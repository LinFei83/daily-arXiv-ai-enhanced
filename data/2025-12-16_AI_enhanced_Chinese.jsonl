{"id": "2512.11998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11998", "abs": "https://arxiv.org/abs/2512.11998", "authors": ["Glenn Zhang", "Treasure Mayowa", "Jason Fan", "Yicheng Fu", "Aaron Sandoval", "Sean O'Brien", "Kevin Zhu"], "title": "Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models", "comment": "Accepted at ACL 2025 SRW, 5 pages body, 14 pages total", "summary": "Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.", "AI": {"tldr": "本文提出了一种名为直接置信度对齐（DCA）的方法，使用直接偏好优化（DPO）来对齐大型语言模型（LLM）的口头表达置信度与其内部置信度，以提高模型透明度和可靠性。研究发现DCA对某些模型架构有效，但对其他模型无效，强调了未来需要更多模型感知的方法。", "motivation": "随着大型语言模型（LLM）的广泛使用，生产值得信赖和可靠的模型变得越来越重要。校准旨在通过提高模型置信度与其响应正确性或期望结果的实际可能性之间的一致性来实现这一点。然而，研究发现模型的内部置信度（源自token概率）与其口头表达置信度不一致，导致不同校准方法产生误导性结果。", "method": "本文提出了一种名为直接置信度对齐（DCA）的方法。该方法利用直接偏好优化（DPO）来对齐LLM的口头表达置信度与其内部置信度，而不是与真实准确性对齐。研究在多种开源LLM和广泛的数据集上评估了DCA。此外，还引入了三个新的基于校准误差的指标来进一步评估对齐效果。", "result": "研究结果表明，DCA在某些模型架构上改善了对齐指标，减少了模型置信度表达中的不一致性。然而，研究也指出DCA在其他模型上可能无效。", "conclusion": "DCA方法能够改善特定LLM架构中内部置信度与口头表达置信度的一致性，从而提高模型透明度和可靠性。但其有效性因模型而异，这强调了在追求更可解释和可信赖的LLM时，需要开发更多针对模型特点的方法。"}}
{"id": "2512.11835", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.11835", "abs": "https://arxiv.org/abs/2512.11835", "authors": ["Seyma Yaman Kayadibi"], "title": "A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models", "comment": "42 pages, 6 toy simulation Python implementations, 20 monad clauses instantiated across six system bundles (ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, teleology)", "summary": "Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and \"self-like\" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.", "AI": {"tldr": "本文提出一个基于莱布尼茨单子论的条款框架，以人工年龄分数（AAS）为核心，为大型语言模型（LLM）的内部记忆和控制提供透明、可审计的约束和分析方法。", "motivation": "大型语言模型（LLMs）通常作为强大但不透明的系统部署，其内部记忆和“自我”行为的治理方式缺乏原则性和可审计性。", "method": "该研究基于先前引入的人工年龄分数（AAS）作为人工记忆老化的度量，开发了一个面向工程的、基于条款的架构。它将莱布尼茨《单子论》中的20个单子分组为六个类别，并将其实现为在AAS内核之上的可执行规范。通过六个最小的Python实现，在召回分数、冗余和权重等通道级量上进行了数值实验，每个实验都遵循输入设置、条款实现、数值结果和LLM设计影响的四步模式。", "result": "实验表明，该条款系统表现出有界且可解释的行为：AAS轨迹保持连续且速率受限，矛盾和无根据的主张会触发明确的惩罚，分层细化以受控方式揭示有机结构。通过和谐项对齐了双重视图和目标-行动对，并且完善分数中的窗口漂移能够区分持续改进和持续退化。", "conclusion": "基于单子的条款框架以AAS为骨干，为约束和分析人工智能体内部动态提供了一个透明、代码级的蓝图。"}}
{"id": "2512.12236", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12236", "abs": "https://arxiv.org/abs/2512.12236", "authors": ["Aujasvit Datta", "Jiayun Wang", "Asad Aali", "Armeet Singh Jatyani", "Anima Anandkumar"], "title": "Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT", "comment": null, "summary": "Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.\n  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.", "AI": {"tldr": "CTO是一种统一的CT重建神经算子框架，通过在连续函数空间操作，实现了对不同采样率和图像分辨率的泛化，优于传统深度学习方法和扩散模型。", "motivation": "稀疏视图CT重建是一个病态逆问题。现有深度学习方法（如CNN）在特定采集设置下表现良好，但难以泛化到不同的采样率和图像分辨率，导致伪影。", "method": "本文提出了Computed Tomography neural Operator (CTO)，一个统一的CT重建框架。它通过在连续函数空间中操作，实现了对采样率和图像分辨率的泛化。CTO在正弦图和图像域中协同工作，使用在函数空间中参数化的旋转等变离散-连续卷积，使其本质上与分辨率和采样无关。", "result": "CTO无需重新训练即可在多采样率和跨分辨率场景下保持一致性能，平均比CNNs高出>4dB PSNR。与最先进的扩散方法相比，CTO推理速度快500倍，平均增益3dB。经验结果也验证了CTO在正弦图空间算子学习和旋转等变卷积方面的设计选择。", "conclusion": "CTO在不同采样率和分辨率下均优于现有基线方法，提供了一个可扩展、可泛化的解决方案，使自动化CT重建在实际部署中更具可行性。"}}
{"id": "2512.11802", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11802", "abs": "https://arxiv.org/abs/2512.11802", "authors": ["Zheng Li", "Peng Zhang", "Shixiao Liang", "Hang Zhou", "Chengyuan Ma", "Handong Yao", "Qianwen Li", "Xiaopeng Li"], "title": "Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights", "comment": null, "summary": "Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.", "AI": {"tldr": "本研究通过实地实验，对特斯拉的交通灯和停车标志控制（TLSSC）系统与交通控制设备（TCDs）的交互行为进行了实证分析，建立了数据集，定义了行为模式，并使用FVDM进行了量化建模。", "motivation": "了解高级驾驶辅助系统（ADAS）如何与交通控制设备（TCDs）互动对于评估其对交通运行的影响至关重要，但目前对此缺乏集中的实证研究。", "method": "设计并执行了针对特斯拉TLSSC在不同限速和TCD类型下的实地实验，收集了同步的高分辨率车辆轨迹数据和驾驶员视角视频。基于这些数据，建立了TLSSC-TCD交互行为（停车、加速、跟车）的分类体系，并校准了全速度差模型（FVDM）来定量描述每种行为模式。", "result": "识别出一个跟车阈值（约90米）。停车行为对期望速度偏差和相对速度表现出强烈的响应性，而加速行为则更为保守。交叉路口跟车行为比标准跟车行为表现出更平滑的动态和更小的车头时距。", "conclusion": "所建立的数据集、行为定义和模型特征为未来ADAS-TCD交互逻辑的仿真、安全评估和设计提供了基础。"}}
{"id": "2512.12284", "categories": ["eess.IV", "cs.AI", "cs.AR", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.12284", "abs": "https://arxiv.org/abs/2512.12284", "authors": ["Donghyuk Kim", "Sejeong Yang", "Wonjin Shin", "Joo-Young Kim"], "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval", "comment": "14 pages, 20 figures, conference", "summary": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.", "AI": {"tldr": "V-Rex是首个软硬件协同设计的加速器，通过提出ReSV算法和动态KV缓存检索引擎，解决了流媒体视频LLM在边缘设备上因KV缓存增长导致的计算和内存瓶颈，实现了实时、高能效且低精度损失的推理。", "motivation": "流媒体视频LLM在实时多模态任务中日益普及，但其键值（KV）缓存会随连续视频输入显著增长，导致迭代预填充阶段面临巨大的计算、数据传输和精度下降挑战，尤其是在资源受限的边缘部署场景中问题更为突出。", "method": "本文提出了V-Rex，一个软硬件协同设计的加速器。在算法层面，V-Rex引入了ReSV（一个无需训练的动态KV缓存检索算法），通过利用时空相似性进行token聚类来减少视频帧间的KV缓存内存。在硬件层面，V-Rex提供了一个紧凑、低延迟的硬件加速器，包含动态KV缓存检索引擎（DRE），该引擎采用位级和提前退出计算单元，以充分实现算法优势。", "result": "V-Rex在边缘部署上实现了3.9-8.3 FPS的实时流媒体视频LLM推理，并具有高能效和可忽略的精度损失。尽管DRE仅占2.2%的功耗和2.0%的面积，但整个系统相比AGX Orin GPU实现了1.9-19.7倍的速度提升和3.1-18.5倍的能效改进。", "conclusion": "V-Rex是首个全面解决算法和硬件层面KV缓存检索问题的方案，使得流媒体视频LLM能够在资源受限的边缘设备上实现实时推理，克服了现有方法的关键限制。"}}
{"id": "2512.11824", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11824", "abs": "https://arxiv.org/abs/2512.11824", "authors": ["Rosh Ho", "Jian Zhang"], "title": "ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision", "comment": null, "summary": "This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \\SI{96.73}{\\percent} grasp classification accuracy with sub-\\SI{40.00}{\\milli\\second} end-to-end latency. Physical validation using standardized benchmarks shows \\SI{82.71}{\\percent} success on YCB object manipulation and reliable performance across \\SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \\$\\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.", "AI": {"tldr": "ReGlove是一个低成本、视觉引导的辅助性矫形器系统，它将商用气动康复手套转换为智能设备，通过边缘计算和YOLO视觉模型实现上下文感知的抓取，无需生物信号，成本低于250美元。", "motivation": "全球数百万人患有上肢功能障碍，但现有辅助技术要么价格昂贵，要么依赖不可靠的生物信号，这促使研究人员寻求更经济、更可靠的解决方案。", "method": "该系统名为ReGlove，通过将腕部摄像头与树莓派5边缘计算推理引擎集成，利用实时YOLO计算机视觉模型实现上下文感知抓取。它将低成本的商用气动康复手套进行改造。", "result": "ReGlove实现了96.73%的抓取分类准确率和低于40毫秒的端到端延迟。在标准化基准测试中，YCB物体操作成功率为82.71%，并在27项日常生活活动（ADL）任务中表现可靠。总成本低于250美元，且全部采用商用组件。", "conclusion": "ReGlove为可访问的、基于视觉的上肢辅助技术提供了技术基础，能够惠及那些无法使用传统肌电图（EMG）控制设备的人群。"}}
{"id": "2512.11842", "categories": ["eess.SY", "cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.11842", "abs": "https://arxiv.org/abs/2512.11842", "authors": ["Trevor McClain", "Rahul Bhadani"], "title": "Car-following Models and Congestion Control with Followerstopper on a Ring-Road under Known Delay -- Examining Limit Cycle", "comment": "Submitted to IV 2026", "summary": "This paper examines the IDM microscopic car-following model from a dynamical systems perspective, analyzing the effects of delay on congestion formation. Further, a case of mixed-autonomy is considered by controlling one car with Followerstopper in a ring road setting containing IDM vehicles as human drivers. Specifically, the stop-and-go waves phenomenon in idealized traffic from a dynamical systems perspective is examined. We show that Followerstopper-controlled vehicle is effective at eliminating emergent stop-and-go waves in the IDM traffic simulation. We show through simulation that the uniform flow manifold is unstable for the ring road simulation with IDM vehicles, and that replacing a single car with Followerstopper induces stability, allowing the cars to drive safely at a uniform speed. Additionally, the case of known delay is considered in a mixed-autonomy scenario. Our simulation result shows that while considering a known time delay, traffic waves emerge earlier than in the no-delay case. At the same time, a single-vehicle controlled using Followerstopper controller is able to prevent the emergence of traffic waves even in the presence of delay.", "AI": {"tldr": "本文从动力系统角度分析了IDM跟驰模型中延迟对拥堵形成的影响，并展示了在混合自主交通中，单个Followerstopper控制车辆能有效消除IDM车辆交通中的走走停停波，即使存在已知延迟。", "motivation": "研究延迟对交通拥堵形成的影响，特别是走走停停波现象，并探索在混合自主交通场景下，通过智能车辆控制来消除这些交通波的可能性。", "method": "采用动力系统视角分析IDM微观跟驰模型；在环形路场景下，使用IDM模型模拟人类驾驶车辆，并引入一个由Followerstopper控制器控制的车辆以模拟混合自主交通；通过仿真验证了在无延迟和已知延迟两种情况下，Followerstopper对交通波的影响。", "result": "IDM车辆环形路仿真中，均匀流形不稳定，会导致走走停停波的出现。引入单个Followerstopper控制车辆能使系统稳定，消除交通波，并使车辆以均匀速度安全行驶。在已知延迟情况下，交通波出现得更早，但单个Followerstopper控制车辆仍然能够有效阻止交通波的出现。", "conclusion": "Followerstopper控制器在混合自主交通中表现出色，即使只有一个受控车辆，也能有效消除IDM交通中的走走停停波，并在存在已知延迟的情况下依然保持其有效性，从而实现交通流的稳定和安全。"}}
{"id": "2512.11849", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11849", "abs": "https://arxiv.org/abs/2512.11849", "authors": ["Nimol Thuon", "Jun Du"], "title": "KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document", "comment": null, "summary": "Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \\textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.", "AI": {"tldr": "本文介绍了KH-FUNSD，这是首个公开可用的高棉语商业文档分层标注数据集，旨在解决低资源、非拉丁语种文档布局分析的挑战，并提供了基准模型结果。", "motivation": "低资源、非拉丁语种（如高棉语）的自动化文档布局分析面临巨大挑战，尤其是在商业文档领域。现有文档AI工具对高棉语关注不足，缺乏专门资源，这阻碍了公共管理和私营企业的信息处理效率。", "method": "研究团队构建了KH-FUNSD数据集，该数据集包含收据、发票和报价单等高棉语商业文档，并采用三级分层标注框架：1) 区域检测，将文档划分为核心区域（如页眉、表单字段、页脚）；2) FUNSD风格标注，区分问题、答案、标题及其他关键实体及其关系；3) 细粒度分类，分配特定语义角色（如字段标签、值、页眉、页脚、符号）。此外，还对多个领先模型进行了基准测试。", "result": "本文成功创建了KH-FUNSD数据集，这是第一个公开可用的高棉语表单文档理解的分层标注数据集。该数据集支持全面的布局分析和精确的信息提取，并为高棉语商业文档提供了首套基线结果，揭示了非拉丁、低资源脚本所带来的独特挑战。", "conclusion": "KH-FUNSD数据集通过其多级标注方法，有效解决了高棉语商业文档布局分析和信息提取的资源匮乏问题。它为高棉语文档AI工具的开发奠定了基础，并为未来针对低资源非拉丁语种的研究提供了重要的基准和资源。"}}
{"id": "2512.12008", "categories": ["cs.CL", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.12008", "abs": "https://arxiv.org/abs/2512.12008", "authors": ["Minghui Liu", "Aadi Palnitkar", "Tahseen Rabbani", "Hyunwoo Jae", "Kyle Rui Sang", "Dixi Yao", "Shayan Shabihi", "Fuheng Zhao", "Tian Li", "Ce Zhang", "Furong Huang", "Kunpeng Zhang"], "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.", "AI": {"tldr": "本文评估了多种KV缓存压缩策略在长推理任务上的性能，发现H2O和解码增强版SnapKV对推理模型表现出色，并揭示了缓存预算与推理成本之间的权衡。", "motivation": "大型语言模型（LLMs）在长上下文任务中受限于KV缓存的内存增长（与上下文长度呈线性关系）。现有压缩算法多针对预填充阶段，但在需要长解码的推理任务（如多步推理和自我反思）中的性能评估不足。", "method": "本文对几种流行的KV缓存压缩策略在长推理任务上进行了基准测试。研究使用了非推理模型Llama-3.1-8B-Instruct以及推理模型，并特别关注了H2O和解码增强版SnapKV的性能。", "result": "对于非推理模型Llama-3.1-8B-Instruct，没有单一策略适用于所有情况，性能受数据集类型影响。然而，对于推理模型，H2O和解码增强版SnapKV是主导策略，表明“重击者”跟踪（heavy-hitter tracking）对推理轨迹的有效性。研究还发现，低预算下的逐出策略可以产生更长的推理轨迹，揭示了缓存大小与推理成本之间的权衡。", "conclusion": "KV缓存压缩策略的有效性取决于模型类型和任务性质。对于推理任务，追踪关键信息（如H2O和SnapKV所示）至关重要。此外，在KV缓存预算和推理过程的长度及成本之间存在一个权衡点。"}}
{"id": "2512.11865", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11865", "abs": "https://arxiv.org/abs/2512.11865", "authors": ["Ju-Young Kim", "Ji-Hong Park", "Myeongjun Kim", "Gun-Woo Kim"], "title": "Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation", "comment": "Accepted to MobieSec 2025 (poster session)", "summary": "Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.", "AI": {"tldr": "针对智能农业中基于RGB相机和机械臂的系统易受光度扰动攻击的问题，本文提出了一种可解释的对抗鲁棒视觉-语言-动作（VLA）模型。该模型包含一个Evidence-3模块，用于检测扰动、生成自然语言解释，并显著提高了对抗条件下的动作预测准确性和可解释性。", "motivation": "智能农业中常用的基于RGB相机感知和机械臂控制的系统，易受色调、光照和噪声变化等光度扰动的影响，在对抗攻击下可能导致故障。", "method": "本文提出了一个基于OpenVLA-OFT框架的可解释对抗鲁棒视觉-语言-动作（VLA）模型。该模型集成了Evidence-3模块，用于检测光度扰动并生成其原因和影响的自然语言解释。", "result": "实验结果显示，与基线模型相比，所提出的模型将当前动作L1损失降低了21.7%，下一动作L1损失降低了18.4%。", "conclusion": "该模型在对抗条件下显著提高了智能农业系统的动作预测准确性和可解释性。"}}
{"id": "2512.11811", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11811", "abs": "https://arxiv.org/abs/2512.11811", "authors": ["Fengyi Xu", "Jun Ma", "Waishan Qiu", "Cui Guo"], "title": "Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention", "comment": null, "summary": "Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.", "AI": {"tldr": "VPR-AttLLM是一个模型无关的框架，通过将大型语言模型（LLM）的语义推理和地理空间知识整合到现有视觉地点识别（VPR）流程中，以注意力引导的方式增强描述符，从而提高众包街景图像的地理定位精度，尤其是在应对洪水等危机事件时。", "motivation": "众包街景图像（如社交媒体图片）为城市洪水和其他危机事件提供了宝贵的实时视觉证据，但这些图像通常缺乏可靠的地理元数据，给应急响应带来挑战。现有的视觉地点识别（VPR）模型在处理此类跨源图像时，由于视觉失真和领域偏移，性能会大幅下降。", "method": "VPR-AttLLM是一个模型无关的框架，它将大型语言模型（LLM）的语义推理和地理空间知识整合到已有的VPR流程中。通过注意力引导的描述符增强，VPR-AttLLM利用LLM识别城市背景中具有位置信息区域，并抑制瞬态视觉噪声。该方法无需模型再训练或额外数据。", "result": "在包含真实社交媒体洪水图像的SF-XL扩展基准、合成洪水场景以及新的HK-URBAN数据集上进行了全面评估。VPR-AttLLM与CosPlace、EigenPlaces和SALAD三种最先进的VPR模型结合后，召回性能持续提升，相对增益通常在1-3%之间，在最具挑战性的真实洪水图像上可达8%。", "conclusion": "这项研究不仅实现了可衡量的检索精度提升，还建立了一个LLM引导的多模态融合通用范式，适用于视觉检索系统。通过将城市感知理论的原则嵌入到注意力机制中，VPR-AttLLM将类人空间推理与现代VPR架构相结合。其即插即用设计、强大的跨源鲁棒性和可解释性突出了其在可扩展城市监测和众包危机图像快速地理定位方面的潜力。"}}
{"id": "2512.11816", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11816", "abs": "https://arxiv.org/abs/2512.11816", "authors": ["Enes Özeren", "Matthias Aßenmacher"], "title": "Reinforcement Learning for Latent-Space Thinking in LLMs", "comment": "16 pages, 16 figures, 7 tables", "summary": "Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.", "AI": {"tldr": "本文探讨了在复杂任务中，如数学推理，连续嵌入空间的“潜在空间思维”相比离散语言空间的“思维链”（CoT）推理的局限性。研究发现，监督微调方法（如Coconut）存在敏感性和固有缺陷，而通过强化学习（包括GRPO和新颖的Latent RL方法）优化潜在思维步骤的模型，在数学推理领域仍落后于传统的语言空间CoT模型。", "motivation": "传统的思维链（CoT）推理在离散语言空间中进行，效率低下，因为许多生成的token仅用于强制执行语言规则而非推理本身。潜在空间思维旨在通过连续嵌入空间进行思考来提高效率，但现有方法在复杂任务（如数学推理）中表现不佳。", "method": "研究首先通过实验证明了针对潜在空间思维的监督微调方法（如Coconut）对设计选择高度敏感且存在固有局限性。随后，研究探索了强化学习（RL）技术，包括GRPO，并设计了一种新颖的Latent RL方法，用于直接优化潜在思维步骤。", "result": "实验结果显示，Coconut方法对设计选择高度敏感并存在多项固有局限性。此外，尽管采用了强化学习（GRPO和Latent RL）进行训练，这些模型在数学推理领域仍落后于传统的语言空间CoT模型。", "conclusion": "尽管潜在空间思维旨在提高推理效率，且研究探索了监督微调和强化学习方法进行优化，但在数学推理等复杂任务中，这些方法训练出的模型仍未能超越传统的语言空间思维链（CoT）模型。"}}
{"id": "2512.11987", "categories": ["eess.SY", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2512.11987", "abs": "https://arxiv.org/abs/2512.11987", "authors": ["Philippe Voyer", "Simon Tartakovsky", "Steven J. Benton", "William C. Jones"], "title": "Pivot-Only Azimuthal Control and Attitude Estimation of Balloon-borne Payloads", "comment": "AIAA SCITECH 2026 Forum", "summary": "This paper presents an attitude estimation and yaw-rate control framework for balloon-borne payloads using pivot-only actuation, motivated by the Taurus experiment. Taurus is a long-duration balloon instrument designed for rapid azimuthal scanning at approximately 30 deg/s using a motorized pivot at the flight-train connection, without a reaction wheel. We model the gondola as a rigid body subject to realistic disturbances and sensing limitations, and implement a Multiplicative Extended Kalman Filter (MEKF) that estimates attitude and gyroscope bias by fusing inertial and vector-camera measurements. A simple PI controller uses the estimated states to regulate yaw rate. Numerical simulations incorporating representative disturbance and measurement noise levels are used to evaluate closed-loop control performance and MEKF behavior under flight-like conditions. Experimental tests on the Taurus gondola validate the pivot-only approach, demonstrating stable high-rate tracking under realistic hardware constraints. The close agreement between simulation and experiment indicates that the simplified rigid-body model captures the dominant dynamics relevant for controller design and integrated estimation-and-control development.", "AI": {"tldr": "本文提出了一种仅使用枢轴驱动的球载有效载荷姿态估计和偏航率控制框架，并结合Taurus实验进行了仿真和实验验证。", "motivation": "研究动机源于Taurus实验的需求，该实验旨在通过飞行链连接处的电动枢轴进行快速方位扫描（约30度/秒），且不使用反作用轮，因此需要一种仅通过枢轴驱动实现姿态估计和偏航率控制的方法。", "method": "该研究将吊舱建模为受扰动和传感限制的刚体，并采用乘法扩展卡尔曼滤波器（MEKF）融合惯性传感器和矢量相机测量数据来估计姿态和陀螺仪偏差。一个简单的PI控制器利用估计的状态来调节偏航率。通过数值模拟和Taurus吊舱的实验测试来评估闭环控制性能和MEKF行为。", "result": "数值模拟显示了在类似飞行条件下的良好闭环控制性能和MEKF行为。对Taurus吊舱的实验测试验证了仅枢轴驱动方法的有效性，展示了在实际硬件约束下稳定的高速跟踪能力。仿真与实验结果的高度一致性表明，简化的刚体模型能够捕捉控制器设计和集成估计-控制开发所需的主要动力学特性。", "conclusion": "仅通过枢轴驱动的姿态估计和偏航率控制框架对于球载有效载荷是可行的，且简化的刚体模型足以用于控制器设计和系统开发。"}}
{"id": "2512.11864", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.11864", "abs": "https://arxiv.org/abs/2512.11864", "authors": ["Christoph Einspieler", "Matthias Horn", "Marie-Louise Lackner", "Patrick Malik", "Nysret Musliu", "Felix Winter"], "title": "Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars", "comment": "18 pages, 4 figures", "summary": "The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.", "AI": {"tldr": "本文针对工业生产中带有复杂前置和日历资源约束的并行机调度问题，提出了一种新的变体，并开发了精确的约束建模方法和高效的启发式（包括元启发式）方法，其中元启发式已成功应用于工业实践。", "motivation": "在工业制造中，为并行机寻找高效的生产调度是一个巨大挑战。现代工厂的大规模需求使得通过自动化调度技术最小化生产成本具有巨大潜力。然而，现有解决方案难以有效处理现实生产环境中出现的复杂前置约束和基于日历的资源限制，即使是基本变体也已证明是NP难的。", "method": "本文提出了一种新的并行机调度变体，包含作业前置和基于日历的累积资源约束。对于小型调度场景，采用约束建模方法结合最先进的约束求解技术作为精确解法。对于大规模问题实例，提出了一种构造启发式算法，以及一种使用局部搜索的定制元启发式算法。", "result": "所提出的元启发式方法已在工业环境中部署并正在使用中，表明其在解决大规模实际问题方面的有效性。", "conclusion": "本文成功开发并分析了能够解决包含复杂前置和日历资源约束的现实并行机调度场景的自动化方法，特别是元启发式方法已在工业中得到应用，满足了实际生产需求。"}}
{"id": "2512.12042", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12042", "abs": "https://arxiv.org/abs/2512.12042", "authors": ["Philipp Habicht", "Lev Sorokin", "Abdullah Saydemir", "Ken E. Friedl", "Andrea Stocco"], "title": "Benchmarking Contextual Understanding for In-Car Conversational Systems", "comment": null, "summary": "In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.", "AI": {"tldr": "本文探讨了使用大型语言模型（LLMs）和高级提示技术来评估车载对话式问答（ConvQA）系统响应与用户意图的一致性，发现LLM评估是传统人工评估的可扩展且准确替代方案。", "motivation": "评估车载ConvQA系统的准确性和可靠性是一个挑战，需要一种有效的方法来衡量系统响应对用户话语的遵守程度和上下文理解能力。", "method": "研究通过合成用户话语及其对应的正确和包含错误的系统响应，利用13种不同规模和供应商的LLM（包括推理和非推理模型），并结合输入-输出、思维链、自我一致性和多智能体提示技术，在餐厅推荐案例中评估了LLM对ConvQA系统响应-话语连贯性的理解。", "result": "高级提示技术显著提升了小型非推理模型的性能，特别是多智能体提示。推理模型普遍优于非推理模型，其中使用单智能体提示结合自我一致性表现最佳。DeepSeek-R1模型以0.002美元/请求的成本实现了0.99的F1分数。非推理模型DeepSeek-V3在有效性和成本-时间效率之间达到了最佳平衡。", "conclusion": "基于LLM的评估为ConvQA系统中上下文理解的基准测试提供了一种可扩展且准确的替代方案，优于传统的人工评估方法。"}}
{"id": "2512.11869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11869", "abs": "https://arxiv.org/abs/2512.11869", "authors": ["D. Shainu Suhas", "G. Rahul", "K. Muni"], "title": "Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion", "comment": null, "summary": "Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.", "AI": {"tldr": "Temporal-Anchor3DLane通过改进多任务损失、引入轻量级LSTM时间融合模块和优化训练策略，显著提升了单目3D车道线检测的鲁棒性和时间稳定性，在OpenLane数据集上F1分数提高了6.2。", "motivation": "单目3D车道线检测面临深度模糊、遮挡和帧间时间不稳定性等挑战。现有基于Anchor的方法（如Anchor3DLane）存在回归异常值敏感、全局曲线几何监督不足、多损失项平衡困难以及对时间连续性利用有限等问题。", "method": "本文提出了Temporal-Anchor3DLane框架，通过三项关键改进扩展了Anchor3DLane：1) 一系列多任务损失改进，包括Balanced L1回归、Chamfer点集距离、基于不确定性的损失权重，以及分类和可见性的Focal和Dice损失；2) 一个轻量级时间LSTM融合模块，用于聚合帧间锚点特征，取代了更重的Transformer风格融合；3) ESCOP风格的训练优化，将曲线级监督与时间一致性相结合。", "result": "在OpenLane数据集上，Temporal-Anchor3DLane将F1分数提高了6.2，并产生了更平滑的时间轨迹，证明了小的架构和损失改进可以在不增加传感器或扩展模型的情况下显著增强3D车道线的鲁棒性。", "conclusion": "通过对现有Anchor3DLane模型进行多任务损失改进、轻量级时间融合模块的引入以及训练策略的优化，Temporal-Anchor3DLane显著提升了单目3D车道线检测的性能和时间稳定性，为该领域提供了有效且高效的解决方案。"}}
{"id": "2512.11872", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11872", "abs": "https://arxiv.org/abs/2512.11872", "authors": ["Mingwang Xu", "Jiahao Cui", "Feipeng Cai", "Hanlin Shang", "Zhihao Zhu", "Shan Luan", "Yifang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff", "AI": {"tldr": "WAM-Diff是一个基于视觉-语言-动作（VLA）的自动驾驶框架，它利用离散掩码扩散（masked diffusion）迭代优化未来轨迹序列，并在NAVSIM基准测试中取得了优异性能，为轨迹生成提供了一种新颖的、支持场景感知解码的替代方案。", "motivation": "目前的自动驾驶系统主要依赖自回归大型语言模型和连续扩散策略，而离散掩码扩散在轨迹生成方面的潜力尚未得到充分探索。本研究旨在探索并展示离散掩码扩散在自动驾驶轨迹生成中的有效性。", "method": "本文提出了WAM-Diff框架，该框架采用掩码扩散迭代细化表示未来自我轨迹的离散序列。其主要方法包括：1) 系统性地将掩码扩散应用于自动驾驶，支持灵活的非因果解码顺序；2) 通过稀疏MoE架构实现可扩展的模型容量，该架构在运动预测和驾驶导向的视觉问答（VQA）上联合训练；3) 使用群序列策略优化（GSPO）进行在线强化学习，以优化序列级的驾驶奖励。", "result": "WAM-Diff模型在NAVSIM-v1上实现了91.0的PDMS分数，在NAVSIM-v2上实现了89.7的EPDMS分数。这些结果有力地证明了掩码扩散在自动驾驶中的有效性。", "conclusion": "掩码扩散是一种在自动驾驶中有效的轨迹生成方法，WAM-Diff提供了一个有前景的替代方案，可替代传统的自回归和连续扩散策略，并支持场景感知的轨迹生成解码策略。"}}
{"id": "2512.11999", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11999", "abs": "https://arxiv.org/abs/2512.11999", "authors": ["Wei Xiao", "Anni Li"], "title": "Taylor-Lagrange Control for Safety-Critical Systems", "comment": "13 pages", "summary": "This paper proposes a novel Taylor-Lagrange Control (TLC) method for nonlinear control systems to ensure the safety and stability through Taylor's theorem with Lagrange remainder. To achieve this, we expand a safety or stability function with respect to time along the system dynamics using the Lie derivative and Taylor's theorem. This expansion enables the control input to appear in the Taylor series at an order equivalent to the relative degree of the function. We show that the proposed TLC provides necessary and sufficient conditions for system safety and is applicable to systems and constraints of arbitrary relative degree. The TLC exhibits connections with existing Control Barrier Function (CBF) and Control Lyapunov Function (CLF) methods, and it further extends the CBF and CLF methods to the complex domain, especially for higher order cases. Compared to High-Order CBFs (HOCBFs), TLC is less restrictive as it does not require forward invariance of the intersection of a set of safe sets while HOCBFs do. We employ TLC to reformulate a constrained optimal control problem as a sequence of quadratic programs with a zero-order hold implementation method, and demonstrate the safety of zero-order hold TLC using an event-triggered control method to address inter-sampling effects. Finally, we illustrate the effectiveness of the proposed TLC method through an adaptive cruise control system and a robot control problem, and compare it with existing CBF methods.", "AI": {"tldr": "本文提出了一种新颖的泰勒-拉格朗日控制（TLC）方法，利用带有拉格朗日余项的泰勒定理，确保非线性控制系统的安全性和稳定性。", "motivation": "现有控制屏障函数（CBF）和控制李雅普诺夫函数（CLF）方法在处理复杂高阶系统时存在局限性，特别是高阶CBF（HOCBFs）对安全集交集的正向不变性要求过于严格。本研究旨在提供一种更通用、更少限制的方法来确保非线性系统的安全性和稳定性。", "method": "该方法通过李导数和泰勒定理展开安全或稳定性函数，使控制输入在泰勒级数中以与函数相对阶数相同的阶次出现。TLC提供了系统安全的必要和充分条件，并适用于任意相对阶数的系统和约束。它将约束优化控制问题重新表述为一系列二次规划，并采用零阶保持（ZOH）实现，通过事件触发控制解决采样间隔效应。", "result": "所提出的TLC方法提供了系统安全的必要和充分条件，并适用于任意相对阶数的系统和约束。与HOCBFs相比，TLC的限制更少，因为它不要求安全集交集的正向不变性。通过自适应巡航控制系统和机器人控制问题，验证了TLC的有效性，并展示了零阶保持TLC在处理采样间隔效应下的安全性。", "conclusion": "泰勒-拉格朗日控制（TLC）方法是一种有效且限制较少的非线性控制系统安全和稳定性保障方法。它扩展了现有的CBF和CLF方法到复杂高阶领域，并能处理任意相对阶数的系统，具有广泛的应用潜力。"}}
{"id": "2512.11873", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11873", "abs": "https://arxiv.org/abs/2512.11873", "authors": ["Antonia Yepes", "Marie Charbonneau"], "title": "Audio-Based Tactile Human-Robot Interaction Recognition", "comment": "1 page, 1 figure, 1 table", "summary": "This study explores the use of microphones placed on a robot's body to detect tactile interactions via sounds produced when the hard shell of the robot is touched. This approach is proposed as an alternative to traditional methods using joint torque sensors or 6-axis force/torque sensors. Two Adafruit I2S MEMS microphones integrated with a Raspberry Pi 4 were positioned on the torso of a Pollen Robotics Reachy robot to capture audio signals from various touch types on the robot arms (tapping, knocking, rubbing, stroking, scratching, and pressing). A convolutional neural network was trained for touch classification on a dataset of 336 pre-processed samples (48 samples per touch type). The model shows high classification accuracy between touch types with distinct acoustic dominant frequencies.", "AI": {"tldr": "本研究探索利用机器人本体麦克风通过声音检测触觉交互，并使用卷积神经网络对不同触觉类型进行高精度分类。", "motivation": "提出一种替代传统关节扭矩传感器或六轴力/扭矩传感器的新方法，以实现机器人的触觉检测。", "method": "将两个Adafruit I2S MEMS麦克风与树莓派4集成，并放置在Pollen Robotics Reachy机器人的躯干上。收集了机器人手臂上多种触觉类型（敲击、摩擦、按压等）的音频信号。使用卷积神经网络（CNN）对包含336个预处理样本的数据集进行触觉分类训练。", "result": "该模型对具有明显声学主导频率的触觉类型显示出高分类准确率。", "conclusion": "在机器人本体上放置麦克风可以有效地通过声音检测和分类触觉交互。"}}
{"id": "2512.11874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11874", "abs": "https://arxiv.org/abs/2512.11874", "authors": ["Jiahao Jiang", "Zhangrui Yang", "Xuanhan Wang", "Jingkuan Song"], "title": "Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training", "comment": "3 pages,3 figures, Extended abstract submitted to the 10th Computer Vision in Plant Phenotyping and Agriculture (CVPPA) Workshop, held in conjunction with ICCV 2025", "summary": "This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.", "AI": {"tldr": "本文介绍了一种针对全球小麦全语义分割竞赛的系统性自训练框架，结合混合训练策略和数据增强，并使用SegFormer模型和师生循环来提高准确性。", "motivation": "为全球小麦全语义分割竞赛提供一个高性能的解决方案，并在比赛中取得有竞争力的成绩。", "method": "开发了一个系统性自训练框架，该框架包含：1) 两阶段混合训练策略；2) 大量数据增强；3) 核心模型为SegFormer（MiT-B4骨干网络）；4) 迭代的师生循环，以逐步优化模型精度和最大化数据利用率。", "result": "所提出的方法在开发和测试阶段的数据集上均取得了有竞争力的性能。", "conclusion": "该系统性自训练框架，结合SegFormer模型、混合训练和师生循环，能有效提升小麦全语义分割任务的准确性，并在竞赛中表现出色。"}}
{"id": "2512.12072", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12072", "abs": "https://arxiv.org/abs/2512.12072", "authors": ["Avinash Amballa", "Yashas Malur Saidutta", "Chi-Heng Lin", "Vivek Kulkarni", "Srinivas Chappidi"], "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs", "comment": "Arxiv Submission", "summary": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.", "AI": {"tldr": "本文提出了一种名为 Voyager 的新颖方法，利用行列式点过程（DPP）迭代优化大型语言模型（LLMs）生成合成数据集的多样性。该方法无需训练、适用于闭源模型且可扩展，实验证明其多样性提升了 1.5-3 倍。", "motivation": "大型语言模型生成的合成数据集在用于下游模型的评估和训练时，存在多样性不足的问题。", "method": "Voyager 是一种迭代的、原则性的方法，直接通过行列式点过程（DPPs）的机制优化数据集的多样性。该方法无需训练，适用于闭源模型，并且具有可扩展性。", "result": "Voyager 在多样性方面显著优于流行的基线方法，实现了 1.5-3 倍的提升。论文还为该方法提供了理论依据。", "conclusion": "Voyager 通过一种新颖、数学上合理且可扩展的方法，有效解决了大型语言模型生成数据集的多样性不足问题，并在实践中取得了显著改善。"}}
{"id": "2512.12087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12087", "abs": "https://arxiv.org/abs/2512.12087", "authors": ["Jiayi Yuan", "Cameron Shinn", "Kai Xu", "Jingze Cui", "George Klimiashvili", "Guangxuan Xiao", "Perkz Zheng", "Bo Li", "Yuxin Zhou", "Zhouhai Ye", "Weijie You", "Tian Zheng", "Dominic Brown", "Pengbo Wang", "Richard Cai", "Julien Demouth", "John D. Owens", "Xia Hu", "Song Han", "Timmy Liu", "Huizi Mao"], "title": "BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding", "comment": null, "summary": "The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.", "AI": {"tldr": "BLASST是一种即插即用的动态稀疏注意力方法，通过在线识别并跳过不重要的注意力计算，有效加速了大型语言模型（LLMs）的长上下文推理，同时保持高精度。", "motivation": "大型语言模型（LLMs）对长上下文推理能力的需求日益增长，但标准的注意力机制存在固有的计算和内存瓶颈。", "method": "BLASST方法动态剪枝注意力矩阵，无需预计算或代理分数。它利用固定阈值和在线softmax信息来识别可忽略的注意力分数，从而跳过softmax计算、Value块加载和后续的矩阵乘法。该方法可无缝集成到现有FlashAttention内核设计中，适用于预填充和解码阶段以及所有注意力变体。研究还开发了自动化校准程序，并探索了稀疏感知训练。", "result": "在保持高精度的前提下，BLASST在现代GPU上实现了预填充阶段1.62倍的加速（74.7%稀疏度）和解码阶段1.48倍的加速（73.2%稀疏度）。此外，稀疏感知训练进一步提升了模型的精度-稀疏度边界。", "conclusion": "BLASST为加速LLMs的长上下文推理提供了一个统一且高效的解决方案，通过动态稀疏化显著缓解了计算和内存瓶颈，并通过稀疏感知训练展现了未来性能提升的潜力。"}}
{"id": "2512.12016", "categories": ["eess.SY", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.12016", "abs": "https://arxiv.org/abs/2512.12016", "authors": ["Mevan Wijewardena", "Kamiar Asgari", "Michael J. Neely"], "title": "Bandit-Based Rate Adaptation for a Single-Server Queue", "comment": null, "summary": "This paper considers the problem of obtaining bounded time-average expected queue sizes in a single-queue system with a partial-feedback structure. Time is slotted; in slot $t$ the transmitter chooses a rate $V(t)$ from a continuous interval. Transmission succeeds if and only if $V(t)\\le C(t)$, where channel capacities $\\{C(t)\\}$ and arrivals are i.i.d. draws from fixed but unknown distributions. The transmitter observes only binary acknowledgments (ACK/NACK) indicating success or failure. Let $\\varepsilon>0$ denote a sufficiently small lower bound on the slack between the arrival rate and the capacity region. We propose a phased algorithm that progressively refines a discretization of the uncountable infinite rate space and, without knowledge of $\\varepsilon$, achieves a $\\mathcal{O}\\!\\big(\\log^{3.5}(1/\\varepsilon)/\\varepsilon^{3}\\big)$ time-average expected queue size uniformly over the horizon. We also prove a converse result showing that for any rate-selection algorithm, regardless of whether $\\varepsilon$ is known, there exists an environment in which the worst-case time-average expected queue size is $Ω(1/\\varepsilon^{2})$. Thus, while a gap remains in the setting without knowledge of $\\varepsilon$, we show that if $\\varepsilon$ is known, a simple single-stage UCB type policy with a fixed discretization of the rate space achieves $\\mathcal{O}\\!\\big(\\log(1/\\varepsilon)/\\varepsilon^{2}\\big)$, matching the converse up to logarithmic factors.", "AI": {"tldr": "本文研究了部分反馈单队列系统中在未知分布下实现有界时间平均预期队列大小的问题。提出了一种分阶段算法，并在不知道系统松弛参数ε的情况下，实现了次优的队列性能。同时给出了一个反向结果，并证明了当ε已知时，一个简单的策略可以达到接近最优的性能。", "motivation": "在信道容量和到达率分布未知、且仅有二元反馈（成功/失败）的通信系统中，如何维持有界的时间平均预期队列大小是一个重要的挑战。", "method": "本文提出了一种分阶段算法，该算法逐步细化了无限速率空间的离散化。此外，对于已知ε的情况，提出了一种简单的单阶段UCB（Upper Confidence Bound）类型策略，该策略采用固定速率空间离散化。通过数学分析，推导了算法的性能界限，并证明了一个反向结果。", "result": "在不知道ε的情况下，所提出的分阶段算法实现了 $\\mathcal{O}(\\log^{3.5}(1/\\varepsilon)/\\varepsilon^{3})$ 的时间平均预期队列大小。同时，本文证明了一个反向结果，表明对于任何速率选择算法，最坏情况下的时间平均预期队列大小为 $Ω(1/\\varepsilon^{2})$。当ε已知时，一个简单的UCB类型策略可以达到 $\\mathcal{O}(\\log(1/\\varepsilon)/\\varepsilon^{2})$ 的性能，在对数因子内与反向结果相匹配。", "conclusion": "在不知道ε的情况下，所提出的分阶段算法与理论下限之间存在性能差距。然而，当ε已知时，一个简单的策略可以实现接近最优的性能。这项工作为在部分反馈和未知环境下的队列管理提供了一个有效的算法和理论分析。"}}
{"id": "2512.12952", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12952", "abs": "https://arxiv.org/abs/2512.12952", "authors": ["Krishna Srikar Durbha", "Hassene Tmar", "Ping-Hao Wu", "Ioannis Katsavounidis", "Alan C. Bovik"], "title": "Leveraging Compression to Construct Transferable Bitrate Ladders", "comment": "Under Review in IEEE Transactions on Image Processing", "summary": "Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.", "AI": {"tldr": "本文提出了一种新的基于机器学习的逐镜头码率阶梯构建技术，通过分析压缩过程和源视频感知特征来准确预测VMAF分数，并在不同编码设置下进行了性能评估，优于现有方法和固定码率阶梯。", "motivation": "传统的逐标题/逐镜头视频编码技术（如基于凸包）能显著提高码率效率和QoE，但计算开销大。机器学习方法作为替代方案正在兴起，但仍需改进以更准确地构建内容自适应码率阶梯。", "method": "提出了一种新的基于机器学习的码率阶梯构建技术。该方法通过分析压缩过程，并在压缩前对源视频进行感知相关测量，来准确预测压缩视频的VMAF分数。同时，还研究了逐镜头码率阶梯在不同编码设置下的表现。", "result": "所提出的框架在大规模视频语料库上，能够准确预测VMAF分数，并优于领先的现有方法。此外，研究还评估了逐镜头码率阶梯在不同编码设置下的性能。", "conclusion": "所提出的基于机器学习的逐镜头码率阶梯构建技术能够有效且准确地预测VMAF分数，为内容自适应编码提供了一个计算效率更高的解决方案，并在不同编码设置下表现良好，优于固定码率阶梯和通过穷举编码构建的最佳凸包。"}}
{"id": "2512.11876", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11876", "abs": "https://arxiv.org/abs/2512.11876", "authors": ["Hrigved Mahesh Suryawanshi"], "title": "Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)", "comment": null, "summary": "Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analysis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real-time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR-based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhibited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.", "AI": {"tldr": "该论文提出了一种针对M4机器人平台的越野导航框架，利用激光雷达和CNN进行实时地形分析，生成能量高效且避开困难地形的路径。", "motivation": "在非结构化环境中，机器人自主导航需要实时评估地形难度，并规划兼顾效率与安全的路径。", "method": "研究首先通过与OptiTrack真值比较，发现基于激光雷达的SLAM（FAST-LIO）比基于摄像头的SLAM在高程图生成方面具有厘米级精度，因此选择激光雷达作为主要传感器。该方法利用FAST-LIO生成2.5D高程图，然后通过基于CNN的模型处理高程图以估计通行性分数，并将其转换为导航成本。一个定制的A*规划器结合这些成本与几何距离和能量消耗，规划出以适度距离增加换取地形质量显著改善的路径。整个流程集成了FAST-LIO定位、GPU加速高程映射、CNN通行性估计和Nav2导航（带定制规划器）。", "result": "研究结果表明，基于激光雷达的建图达到了厘米级精度，优于基于摄像头的方法。该系统成功避开了低通行性区域，并通过接受少量更长的路径，显著降低了地形成本。", "conclusion": "这项工作为适用于多模态机器人平台的智能地形感知导航奠定了基础。"}}
{"id": "2512.11871", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11871", "abs": "https://arxiv.org/abs/2512.11871", "authors": ["Tekleab G. Gebremedhin", "Hailom S. Asegede", "Bruh W. Tesheme", "Tadesse B. Gebremichael", "Kalayu G. Redae"], "title": "Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops", "comment": "A preliminary version of this work was presented at the International Conference on Postwar Technology for Recovery and Sustainable Development (Feb. 2025). This manuscript substantially extends that work with expanded experiments and on-device deployment analysis. Code and dataset are publicly available at: https://github.com/Tekleab15/Automated_plant_disease_and_pest_detection_system", "summary": "Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.", "AI": {"tldr": "本文为埃塞俄比亚提格雷地区开发了一个离线优先的仙人掌无花果病害检测系统，利用移动高效的深度学习模型，以应对基础设施中断导致专家诊断受限的问题，并实现了本地化部署。", "motivation": "埃塞俄比亚提格雷地区超过80%的人口依赖农业，但基础设施中断限制了当地居民获取专业的作物病害诊断。为了解决这一问题，研究旨在开发一个可在离线环境下运行的、易于部署的病害检测系统。", "method": "研究构建了一个包含3587张田间图像的本土仙人掌无花果病害数据集，涵盖三种核心症状类别。为适应冲突后边缘环境的部署限制，评估了三种移动高效的架构：自定义轻量级CNN、EfficientNet-Lite1和CNN-Transformer混合模型MobileViT-XS。模型部署在一个支持Tigrigna和Amharic语言的Flutter应用中，可在Cortex-A53级设备上完全离线推理。", "result": "EfficientNet-Lite1达到了90.7%的测试准确率；轻量级CNN以42毫秒的推理延迟和4.8MB的模型大小实现了89.5%的准确率，具有最佳部署特性；MobileViT-XS取得了97.3%的平均交叉验证准确率。结果表明，基于MHSA的全局推理比局部纹理CNN核能更可靠地区分害虫簇和二维真菌病变。", "conclusion": "该研究成功构建并部署了一个支持完全离线推理的、本地化的仙人掌无花果病害检测系统，显著提高了粮食安全关键诊断的可及性和包容性，尤其在资源受限的环境中表现出色。"}}
{"id": "2512.11907", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11907", "abs": "https://arxiv.org/abs/2512.11907", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "comment": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM), 5 pages, 1 figure", "summary": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.", "AI": {"tldr": "该研究提出了一种在复杂结构约束下个性化大型语言模型（LLM）代理的方法。通过将用户知识图谱编译成抽象的宏观方面，并证明常见的层次和配额约束构成层状拟阵，从而将结构化个性化问题转化为拟阵约束下的次模函数最大化问题，实现了具有理论保证的贪婪算法。", "motivation": "个性化LLM代理需要用户数据，但这带来了效用与数据披露之间的权衡。虽然数据效用通常呈次模性，但现实世界的个性化受限于复杂的结构约束（如逻辑依赖、分类配额和层次规则），这些约束违反了标准子集选择算法的假设。", "method": "研究提出了一种原则性的方法来形式化建模这些约束。它引入了一个编译过程，将带有依赖关系的用户知识图谱转换为一组抽象的宏观方面。核心方法是证明这些宏观方面上的常见层次和配额约束构成一个有效的层状拟阵（laminar matroid）。", "result": "主要结果是证明了常见的层次和配额约束在抽象宏观方面上形成一个有效的层状拟阵。这一理论特性使得结构化个性化问题可以被视为拟阵约束下的次模函数最大化问题，从而能够使用具有常数因子保证（通过贪婪算法）或 (1-1/e) 保证（通过连续贪婪算法）的方法解决，适用于更丰富、更真实的个性化问题类别。", "conclusion": "该研究通过将复杂的结构化个性化问题建模为拟阵约束下的次模函数最大化，为LLM代理的个性化提供了一种理论上严谨且具有算法效率保证的方法，能够处理现实世界中更复杂的约束条件。"}}
{"id": "2512.12167", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12167", "abs": "https://arxiv.org/abs/2512.12167", "authors": ["Yoav Gelberg", "Koshi Eguchi", "Takuya Akiba", "Edoardo Cetin"], "title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings", "comment": null, "summary": "So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.", "AI": {"tldr": "本文提出DroPE方法，在训练后移除语言模型的位置嵌入（PE），从而实现零样本上下文扩展，无需昂贵的长上下文微调。", "motivation": "现有方法需要昂贵的微调才能有效扩展语言模型的上下文长度。研究发现，位置嵌入在预训练中至关重要，但过度依赖它们会阻碍模型泛化到未见过的序列长度，且位置嵌入并非有效语言模型的固有要求。", "method": "DroPE（Dropping the Positional Embeddings of LMs after training）方法。该方法在预训练完成后，移除语言模型的位置嵌入，并进行一个简短的重新校准阶段。", "result": "DroPE实现了无缝的零样本上下文扩展，无需任何长上下文微调。它能快速适应预训练的语言模型，同时不损害其在原始训练上下文中的能力。该方法在不同模型和数据集大小上均表现出色，远超先前的专业架构和已有的旋转位置嵌入缩放方法。", "conclusion": "位置嵌入在预训练后可以安全移除，经过短暂的重新校准后，可以有效且高效地扩展语言模型的上下文，突破了传统长上下文微调的瓶颈。"}}
{"id": "2512.11902", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11902", "abs": "https://arxiv.org/abs/2512.11902", "authors": ["Yanna Elizabeth Smid", "Peter van der Putten", "Aske Plaat"], "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning", "comment": null, "summary": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode", "AI": {"tldr": "本研究引入了“镜像模式”，一种让AI模仿玩家个人策略的新游戏模式，旨在挑战玩家不断改变游戏玩法，并发现该模式提高了玩家满意度。", "motivation": "回合制游戏中的敌人策略应具有惊喜性和不可预测性。当前研究旨在通过让AI模仿玩家自身策略来挑战玩家，促使他们改变玩法。", "method": "研究构建了《火焰纹章 英雄》的简化版，包含标准模式和镜像模式。首先，通过结合生成对抗模仿学习（GAIL）、行为克隆（BC）和近端策略优化（PPO）的强化学习与模仿学习方法，找到了一个合适的模型来模仿玩家演示。其次，通过玩家测试评估了该模型，模型使用参与者提供的演示进行训练。", "result": "实验发现，模型在防御行为方面表现出良好的模仿，但在进攻策略方面则不然。参与者的游戏玩法表明他们识别出了自己的撤退战术，并且镜像模式的总体玩家满意度更高。", "conclusion": "进一步完善模型可以提高模仿质量和玩家满意度，尤其是在玩家面对自己策略时。这表明镜像模式具有提升玩家体验的潜力。"}}
{"id": "2511.01411", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.01411", "abs": "https://arxiv.org/abs/2511.01411", "authors": ["Reza Karimzadeh", "Albert Alonso", "Frans Zdyb", "Julius B. Kirkegaard", "Bulat Ibragimov"], "title": "Extremal Contours: Gradient-driven contours for compact visual attribution", "comment": null, "summary": "Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve/delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.", "AI": {"tldr": "本文提出了一种无需训练的视觉模型解释方法，用可调平滑轮廓取代密集扰动掩码。该方法通过傅里叶级数参数化星凸区域，并利用分类器梯度进行优化，生成紧凑、忠实且一致的解释，在多个基准测试中表现优于现有方法。", "motivation": "为视觉模型生成忠实且紧凑的解释仍然是一个挑战，因为常用的密集扰动掩码往往碎片化、过拟合，需要复杂的后处理。", "method": "该方法用平滑可调的轮廓取代了密集的掩码。它通过截断傅里叶级数参数化星凸区域，并使用分类器梯度在极端保留/删除目标下进行优化。该方法保证生成单一、简单连接的掩码，并通过限制低维平滑轮廓来提高鲁棒性。它还支持显式区域控制，并可扩展到多轮廓以定位多个对象。", "result": "该方法将自由参数数量减少了几个数量级，实现了稳定的边界更新，无需清理。它对对抗性掩码伪影具有鲁棒性，在ImageNet分类器上匹配了密集掩码的极端保真度，同时生成了紧凑、可解释的区域，并改善了运行间一致性。它还支持重要性轮廓图。与基于梯度和扰动的方法相比，该方法实现了更高的相关性质量和更低的复杂性，尤其在自监督DINO模型上，相关性质量提高了15%以上，并保持了正向的忠实度相关性。", "conclusion": "该方法为视觉模型提供了一种新颖、无需训练的解释方法，通过使用平滑轮廓生成忠实、紧凑且一致的解释。它在性能上超越了现有基线，特别是在自监督模型上表现出色，并提供了更好的可解释性和控制性。"}}
{"id": "2512.11886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11886", "abs": "https://arxiv.org/abs/2512.11886", "authors": ["Mohammed Irfan Ali"], "title": "Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control", "comment": null, "summary": "Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.", "AI": {"tldr": "本论文为行星探索蛇形机器人COBRA开发了一套完整的自主导航系统，通过集成视觉惯性SLAM、降阶状态估计和闭环轨迹跟踪，实现了自主路径点导航。", "motivation": "蛇形机器人在极端地形上具有卓越的移动性，但其高度铰接的身体给自主导航带来了巨大挑战，尤其是在缺乏外部跟踪基础设施的环境中。以往的工作完全依赖于开环遥控操作。", "method": "该方法集成了车载视觉惯性SLAM（使用深度相机和边缘计算进行实时定位），并采用降阶框架估计质心姿态。通过距离依赖的偏航误差融合来调制CPG步态参数，驱动闭环控制器进行轨迹跟踪。SLAM的性能通过运动捕捉地面真值进行验证，并对蛇形机器人平台特有的漂移行为和故障模式进行了表征。最后通过物理实验验证了整个系统。", "result": "该系统在动态运动中实现了实时定位，并成功表征了蛇形机器人平台特有的漂移行为和故障模式。物理实验验证了完整系统，展示了精确的多路径点跟踪能力。", "conclusion": "本研究为自主蛇形机器人导航奠定了基础，成功实现了多路径点跟踪，证明了该系统在行星探索等环境中的潜力。"}}
{"id": "2512.11909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11909", "abs": "https://arxiv.org/abs/2512.11909", "authors": ["Hanna Dettki"], "title": "Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets", "comment": null, "summary": "The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \\emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?\n  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\\!\\to\\!E\\!\\leftarrow\\!C_2$ ) under \\emph{Direct} (one-shot number as response = probability judgment of query node being one and \\emph{Chain of Thought} (CoT; think first, then provide answer).\n  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \\in [0,1]$ include a shared prior $p(C)$;\n  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\\neq}m_2$) variant.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2512.12013", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12013", "abs": "https://arxiv.org/abs/2512.12013", "authors": ["Senhao Gao", "Junqing Zhang", "Luoyu Mei", "Shuai Wang", "Xuyu Wang"], "title": "Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition", "comment": null, "summary": "Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \\sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.", "AI": {"tldr": "本文提出了一种基于毫米波雷达点云的人体活动识别（HAR）系统，通过设计星形图表示和离散动态图神经网络（DDGNN）来处理毫米波信号的稀疏性和可变大小问题，并在实际数据集中取得了接近视觉系统的高精度HAR性能。", "motivation": "毫米波雷达点云的HAR系统面临稀疏性和可变大小问题，现有方法常借用视觉系统针对密集点云的预处理算法，但这些方法可能不适用于毫米波雷达系统。", "method": "研究人员提出了一种基于图表示的方法，并结合离散动态图神经网络（DDGNN）来探索人体运动相关的时空特征。具体来说，他们设计了一个星形图来描述手动添加的静态中心点与同一帧及连续帧中动态毫米波雷达点之间的高维相对关系，然后利用DDGNN学习这个可变大小星形图中的特征。", "result": "实验结果表明，该方法在真实HAR数据集上优于其他基线方法，实现了94.27%的整体分类准确率，接近基于视觉骨架数据的97.25%的性能。此外，该系统在资源受限平台（如Raspberry Pi 4）上进行了推理测试，验证了其有效性，并且无需重采样或帧聚合器即可超越三种近期雷达专用方法。", "conclusion": "所提出的基于星形图表示和DDGNN的方法能有效提取毫米波雷达点云的时空特征，解决了其稀疏性和可变大小问题，并在人体活动识别任务中取得了优异的性能，具有实际应用潜力。"}}
{"id": "2512.11884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11884", "abs": "https://arxiv.org/abs/2512.11884", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors", "comment": null, "summary": "Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors", "AI": {"tldr": "本文比较了零样本的SAM3与微调的YOLO11在密集实例分割任务上的表现，发现YOLO在特定IoU下F1分数更高，但SAM3在边界稳定性方面表现出12倍的优越性。", "motivation": "深度学习在实例分割领域发展出两种截然不同的范式：通过任务特定微调优化的专用模型和能够进行零样本分割的通用基础模型。本研究旨在全面比较这两种范式在密集实例分割任务中的表现和优劣。", "method": "研究在MinneApple数据集（包含670张果园图像和28,179个苹果实例标注）上，对零样本模式下的SAM3（Segment Anything Model v3）和三种微调过的Ultralytics YOLO11变体（nano, medium, large）进行了实例分割性能评估。评估主要通过F1分数和IoU阈值分析进行。", "result": "研究发现，IoU选择会使性能差距膨胀高达30%。在合适的IoU=0.15阈值下，YOLO模型取得了68.9%、72.2%和71.9%的F1分数，而纯零样本模式下的SAM3达到了59.8%。然而，YOLO在IoU范围内的性能下降了48-50点，而SAM3仅下降了4点，显示SAM3具有12倍优越的边界稳定性。这突显了SAMv3在掩膜精度方面的优势，以及YOLO11在检测完整性方面的专长。", "conclusion": "SAMv3在掩膜精度和边界稳定性方面表现出色，而YOLO11则在检测完整性方面具有专业优势。这项工作有助于深入理解在密集实例分割任务中何时选择专用微调模型或通用基础模型。"}}
{"id": "2512.13434", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13434", "abs": "https://arxiv.org/abs/2512.13434", "authors": ["Youssef Megahed", "Inok Lee", "Robin Ducharme", "Kevin Dick", "Adrian D. C. Chan", "Steven Hawken", "Mark C. Walker"], "title": "Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging", "comment": "14 pages, 8 figures, 4 tables", "summary": "Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.", "AI": {"tldr": "本研究评估了一种自监督超声基础模型（USF-MAE）在胎儿肾脏异常自动分类中的表现，结果显示其在二分类和多分类任务中均显著优于基线模型，并具有良好的可解释性。", "motivation": "产前超声是检测先天性肾脏和泌尿道异常的基石，但其诊断受操作员依赖性和成像条件不佳的限制。因此，需要开发更自动化、客观的诊断方法。", "method": "研究使用包含969张二维超声图像的精选数据集，对预训练的超声自监督基础模型（USF-MAE）进行了微调，用于正常肾脏、泌尿道扩张和多囊性发育不良肾脏的二分类和多分类。通过交叉验证和独立的测试集，将USF-MAE模型与DenseNet-169卷积基线模型进行了比较。同时，采用了Score-CAM可视化技术来增强模型的可解释性。", "result": "USF-MAE模型在所有评估指标上均持续优于基线模型。在二分类设置中，USF-MAE在验证集上将AUC提高了约1.87%，F1-score提高了7.8%；在独立测试集上，AUC提高了2.32%，F1-score提高了4.33%。在多分类设置中，性能提升更为显著，AUC提高了16.28%，F1-score提高了46.15%。Score-CAM可视化显示模型预测依据的是临床相关的肾脏结构。", "conclusion": "超声特异性自监督学习能够生成有用的表示，作为下游诊断任务的基础。所提出的框架为产前肾脏异常检测提供了一种鲁棒且可解释的方法，并展示了基础模型在产科影像学中的应用前景。"}}
{"id": "2512.12590", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12590", "abs": "https://arxiv.org/abs/2512.12590", "authors": ["Indiwara Nanayakkara", "Dehan Jayawickrama", "Mervyn Parakrama B. Ekanayake"], "title": "Automatic Wire-Harness Color Sequence Detector", "comment": "6 pages, 20 figures, IEEE ICIIS 2025 Conference - Accepted", "summary": "Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.", "AI": {"tldr": "本文介绍了一种半自动机器视觉系统，用于线束检测，实现了100%的检测准确率，并将检测时间缩短了44%，解决了传统人工检测耗时且易出错的问题。", "motivation": "在现代电子制造服务（EMS）行业中，线束检测过程仍然是劳动密集型且容易出错的，这促使研究人员寻求更高效、准确的自动化解决方案。", "method": "该系统采用模块化机械框架集成五台工业标准CMOS相机，并利用基于HSV和RGB颜色域值比较的颜色序列分类器进行操作。用户可以通过至少五个参考样本对每个线束批次进行训练，训练文件可存储并重复使用。系统还具备用户管理、可调照明、会话数据存储和安全登录等功能。", "result": "该解决方案已部署在GPV Lanka Pvt. Ltd.，并取得了100%的检测准确率，与手动方法相比，检测时间减少了44%。", "conclusion": "该半自动机器视觉系统在实际应用中展现了可靠且高效的线束检测能力，有效解决了人工检测的痛点。"}}
{"id": "2512.11894", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11894", "abs": "https://arxiv.org/abs/2512.11894", "authors": ["Mahathir Monjur", "Shahriar Nirjon"], "title": "mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description", "comment": "Accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision 2026 (WACV 2026)", "summary": "Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.", "AI": {"tldr": "mmWeaver是一个新颖的框架，利用隐式神经表示（INRs）和超网络，通过建模为连续函数来合成逼真、环境特定的毫米波信号，实现高达49倍的压缩，并显著提升下游任务性能。", "motivation": "毫米波雷达应用（如活动识别和姿态估计）严重依赖多样化、环境特定的信号数据集，但毫米波信号固有的复杂性、稀疏性和高维度使得物理仿真计算成本高昂，因此需要一种高效、逼真的信号生成和数据增强方法。", "method": "mmWeaver通过将毫米波信号建模为连续函数，使用隐式神经表示（INRs）进行合成，并利用超网络根据环境上下文（从RGB-D图像提取）和人体运动特征（通过MotionGPT从文本生成姿态）动态生成INR参数。该方法在语义和几何先验条件下，生成多分辨率的I/Q信号，并保留关键的相位信息。", "result": "mmWeaver实现了0.88的复杂SSIM和35 dB的PSNR，在信号真实性方面优于现有方法。它将活动识别准确率提高了7%，人体姿态估计误差降低了15%，同时运行速度比基于仿真的方法快6到35倍。", "conclusion": "mmWeaver提供了一个高效、自适应的毫米波信号合成框架，通过结合INRs和超网络，能够生成逼真且环境特定的信号，显著提升了毫米波雷达在活动识别和姿态估计等下游任务中的性能，并大大降低了计算成本。"}}
{"id": "2512.11896", "categories": ["cs.CV", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11896", "abs": "https://arxiv.org/abs/2512.11896", "authors": ["Tessa Vu"], "title": "Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat", "comment": "Completed as a requirement in MUSA 6950-001", "summary": "Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.", "AI": {"tldr": "本研究提出了一个名为Hot Hém的GeoAI工作流，通过结合谷歌街景图像、语义图像分割和遥感技术，预测胡志明市的行人热暴露，并实现热感知路径规划。", "motivation": "在热带高密度城市中，行人热暴露是一个关键的健康风险，而标准的路径规划算法往往忽略微尺度的热变化。", "method": "该方法是一个空间数据科学流程，结合了谷歌街景（GSV）图像、语义图像分割和遥感技术。训练了两个XGBoost模型，利用GSV训练数据集预测地表温度（LST），并将其部署在所有OSMnx派生的行人网络节点上，以实现热感知路径规划。", "result": "该模型部署后，可以为精确识别和进一步理解为什么某些城市走廊在基础设施层面可能经历不成比例的更高温度提供基础，并支持热感知路径规划。", "conclusion": "Hot Hém工作流为识别和理解城市中高热区域及其原因提供了基础，并能支持热感知路径规划，从而降低行人的热暴露风险。"}}
{"id": "2512.12026", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12026", "abs": "https://arxiv.org/abs/2512.12026", "authors": ["Jialin Zheng", "Haoyu Wang", "Yangbin Zeng", "Han Xu", "Di Mou", "Hong Li", "Patrick Wheeler", "Sergio Vazquez", "Leopoldo G. Franquelo"], "title": "DT-MPC: Synthesizing Derivation-Free Model Predictive Control from Power Converter Netlists via Physics-Informed Neural Digital Twins", "comment": "15 pages, 15 figures", "summary": "Model Predictive Control (MPC) is a powerful control strategy for power electronics, but it highly relies on manually-derived and topology-specific analytical models, which is labor-intensive and time-consuming in practical designs. To overcome this bottleneck, this paper introduces a Digital-Twin-based MPC (DT-MPC) framework for generic power converters that can systematically translate a high-level circuit into an objective-aware control policy by leveraging a DT as a high-fidelity system model. Furthermore, a physics-informed neural surrogate predictor is proposed to accelerate predictions by DT and enable real-time operation. A gradient-free simplex search optimizer is also introduced to efficiently handle complex multi-objective optimization. The efficacy of the framework has been validated through a cloud-to-edge deployment on a 1500 W dual active bridge converter. Experimental results show that the synthesized predictive model achieves an inference speed over 7 times faster than real time, the DT-MPC controller outperforms several human-designed counterparts, and the overall framework reduces engineering design time by over 95\\%, verifying the superiority of DT-MPC on generalized power converters.", "AI": {"tldr": "本文提出了一种基于数字孪生（DT）的通用电力转换器模型预测控制（DT-MPC）框架，通过物理信息神经代理预测器和无梯度单纯形搜索优化器，克服了传统MPC对手动建模的依赖，显著提高了性能并缩短了工程设计时间。", "motivation": "模型预测控制（MPC）在电力电子领域表现强大，但其高度依赖手动推导和拓扑特定的分析模型，这在实际设计中耗时耗力。", "method": "本文引入了数字孪生（DT）作为高保真系统模型，构建了一个DT-MPC框架，能将高级电路系统地转化为目标感知控制策略。为加速预测和实现实时操作，提出了一个物理信息神经代理预测器。此外，引入了无梯度单纯形搜索优化器来高效处理复杂的多目标优化。", "result": "该框架在1500 W双有源桥转换器上进行了验证。结果显示，合成的预测模型推理速度比实时快7倍以上，DT-MPC控制器优于多个人工设计的对应控制器，并且整体框架将工程设计时间缩短了95%以上。", "conclusion": "DT-MPC框架在通用电力转换器上表现出卓越性，验证了其在提升性能和大幅减少工程设计时间方面的优势。"}}
{"id": "2512.12081", "categories": ["eess.SY", "cs.AI", "cs.SI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.12081", "abs": "https://arxiv.org/abs/2512.12081", "authors": ["Semih Kara", "Yasin Sonmez", "Can Kizilkale", "Alex Kurzhanskiy", "Nuno C. Martins", "Murat Arcak"], "title": "Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models", "comment": null, "summary": "Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.", "AI": {"tldr": "研究如何通过战略性地放置电动汽车充电站来缓解交通拥堵，提出了两种均衡模型（拥堵博弈和排队模拟）和一个可扩展的贪婪算法。排队模型被证明更真实。", "motivation": "电动汽车的普及可能因充电站位置不当而加剧交通拥堵，因此需要研究如何优化充电站布局以减少拥堵。", "method": "采用了两种均衡模型：基于拥堵博弈的模型和基于原子排队模拟的模型。将这两种模型应用于一个可扩展的贪婪充电站选址算法。提出了一种统一的方法，该方法从排队模拟中校准拥堵延迟并在链路空间中求解均衡。", "result": "实验表明，该贪婪方案在实际网络中能产生最优或接近最优的拥堵结果，尽管不能保证全局最优。排队模拟方法比拥堵博弈模型产生更真实的结果。", "conclusion": "通过结合排队模拟和贪婪算法，可以有效地战略性放置电动汽车充电站以减少交通拥堵，其中排队模型能提供更真实的拥堵评估。"}}
{"id": "2512.11912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11912", "abs": "https://arxiv.org/abs/2512.11912", "authors": ["Liu Peng", "Yaochu Jin"], "title": "Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis", "comment": null, "summary": "A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.", "AI": {"tldr": "本文系统比较了不同现代概率模型在低质量数据下的鲁棒性，发现自回归语言模型表现出显著的弹性，而类条件扩散模型则灾难性退化，分类器受影响适中，且影响随数据集规模减小。鲁棒性受限于条件信息丰富度和训练数据的绝对信息含量。", "motivation": "研究不同现代概率模型在低质量数据影响下的鲁棒性差异，因为现有模型对此表现出截然不同的鲁棒性谱系。", "method": "通过系统性的比较调查，评估低质量数据对不同模型的影响。通过整合信息论、PAC学习和梯度动力学等多视角分析来解释观察到的差异。", "result": "自回归语言模型（如GPT-2）在50%的token损坏下仍表现出显著的弹性（测试NLL仅从2.87适度增加到3.59）。相比之下，在相同数据损坏水平下，类条件扩散模型灾难性退化（图像-标签一致性相对于基线下降56.81%）。分类器则受到中度影响，且这种影响随数据集规模的增大而减弱。分析表明，鲁棒性主要受两个关键原则影响：约束学习问题的条件信息的丰富性，以及训练数据的绝对信息含量，后者使得正确信息的信号能够主导统计噪声。", "conclusion": "不同概率模型对低质量数据的鲁棒性差异显著，这种差异主要由条件信息的丰富性和训练数据的绝对信息含量所决定。自回归语言模型因此表现出更强的弹性，而类条件扩散模型则更为脆弱。"}}
{"id": "2512.12168", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12168", "abs": "https://arxiv.org/abs/2512.12168", "authors": ["Zheng Huang", "Kiran Ramnath", "Yueyan Chen", "Aosong Feng", "Sangmin Woo", "Balasubramaniam Srinivasan", "Zhichao Xu", "Kang Zhou", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "title": "Diffusion Language Model Inference with Monte Carlo Tree Search", "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.", "AI": {"tldr": "扩散语言模型（DLM）的推理存在次优解码路径问题。MEDAL引入蒙特卡洛树搜索（MCTS）初始化，通过探索解掩码轨迹，显著提升了推理性能，开创了搜索式推理的新范式。", "motivation": "扩散语言模型（DLM）在推理过程中通过迭代去噪生成文本，但确定解掩码位置和提交令牌是一个巨大的组合搜索问题。现有推理方法依赖启发式算法或额外训练，导致次优的解码路径，因此需要一个更具原则性的搜索机制。", "method": "本文提出了MEDAL框架，将蒙特卡洛树搜索（MCTS）集成到扩散语言模型（DLM）推理的初始化阶段。通过限制搜索空间为高置信度动作，并优先选择能提高模型对剩余掩码位置置信度的令牌，MCTS被用于探索有前景的解掩码轨迹，为后续细化提供稳健的起点。", "result": "在多个基准测试中，MEDAL比现有推理策略实现了高达22.0%的性能提升。", "conclusion": "MEDAL为扩散语言模型中的搜索式推理建立了一个新范式，显著改进了DLM的推理性能。"}}
{"id": "2512.12017", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12017", "abs": "https://arxiv.org/abs/2512.12017", "authors": ["Haoyu Wang", "Junwei Liu", "Jialin Zheng", "Yangbin Zeng", "Di Mou", "Zian Qin"], "title": "Online Full ZVS Optimization for Modular Multi-Active Bridge Converter in MV PET", "comment": "8 pages, 11 figures. Accecpted by APEC 2026", "summary": "Multi-active bridge (MAB) converters, the core of the state-of-the-art medium-voltage power electronic transformers, can flexibly connect multiple DC ports among distributed DC grids and loads, but suffer from hard switching under conventional single phase-shift control, especially under unbalanced voltage conversion ratios and light load conditions. Although some offline methods manage to improve the efficiency through complex optimization structures, there lacks online optimization methods that are simple but effective due to the strong coupling among ports of the converter. By leveraging the time-domain model of the MAB converter under the multiple phase-shift modulation scheme, this paper simplifies the optimization process and proposes an online optimization method that can achieve full zero-voltage switching (ZVS) operation regardless of the load conditions. The proposed method has simple solutions with only voltage conversion ratios involved and can be implemented within a wide operation range without additional sensors or advanced controllers. A four-port MAB converter is constructed as the prototype. The simulation and experimental results have verified the feasibility and superiority of the proposed online strategy in achieving ZVS operation, dynamic response, and efficiency improvement.", "AI": {"tldr": "本文提出了一种基于时域模型和多相移调制方案的多有源桥（MAB）变换器在线优化方法，旨在实现全范围零电压开关（ZVS）操作，从而解决传统控制下的硬开关问题并提高效率和动态响应。", "motivation": "多有源桥（MAB）变换器在传统单相移控制下，尤其是在电压转换比不平衡和轻载条件下，存在硬开关问题。现有离线优化方法复杂，且由于端口之间强耦合，缺乏简单有效的在线优化方法。", "method": "该研究利用MAB变换器在多相移调制方案下的时域模型，简化了优化过程，并提出了一种在线优化方法。该方法仅涉及电压转换比，解决方案简单，无需额外传感器或高级控制器，可在宽操作范围内实现。通过构建一个四端口MAB变换器原型进行仿真和实验验证。", "result": "所提出的方法实现了全负载条件下的零电压开关（ZVS）操作，具有简单的解决方案，并且仅涉及电压转换比。仿真和实验结果验证了该在线策略在实现ZVS操作、改善动态响应和提高效率方面的可行性和优越性。", "conclusion": "通过利用MAB变换器的时域模型和多相移调制方案，所提出的在线优化方法成功解决了MAB变换器的硬开关问题，实现了全ZVS操作，并显著提高了动态响应和效率，且实现方式简单有效。"}}
{"id": "2512.12238", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12238", "abs": "https://arxiv.org/abs/2512.12238", "authors": ["Yinzhu Cheng", "Haihua Xie", "Yaqing Wang", "Miao He", "Mingming Sun"], "title": "Semantic Distance Measurement based on Multi-Kernel Gaussian Processes", "comment": null, "summary": "Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.", "AI": {"tldr": "本文提出了一种基于多核高斯过程（MK-GP）的语义距离度量方法，其核参数可从数据中自动学习，并在上下文学习（ICL）设置下的大语言模型细粒度情感分类任务中验证了其有效性。", "motivation": "计算语言学中的语义距离测量是一个基本问题，但大多数经典方法是固定的，难以适应特定的数据分布和任务需求。", "method": "提出了一种基于多核高斯过程（MK-GP）的语义距离度量。将文本相关的潜在语义函数建模为高斯过程，其协方差函数由Matérn和多项式分量组合而成的复合核函数给出。核参数在监督下从数据中自动学习。", "result": "实验结果表明，在上下文学习（ICL）设置下，该语义距离在结合大语言模型的细粒度情感分类任务中表现出有效性。", "conclusion": "所提出的基于多核高斯过程的语义距离度量，通过从数据中自动学习核参数，能够有效且自适应地量化文本相似性，并在细粒度情感分类等任务中展现了其优势。"}}
{"id": "2512.11920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11920", "abs": "https://arxiv.org/abs/2512.11920", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving", "comment": "Accepted to FPGA'26 Oral", "summary": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.", "AI": {"tldr": "CXL-SpecKV提出了一种基于CXL和FPGA的解耦式KV缓存架构，通过内存解耦、推测性预取和硬件压缩，显著提高了LLM服务吞吐量并降低了内存成本。", "motivation": "大型语言模型（LLMs）在数据中心部署时面临巨大的KV缓存内存需求，这限制了批处理大小并降低了系统吞吐量。", "method": "CXL-SpecKV通过以下三项创新解决问题：(i) 基于CXL的内存解耦框架，将KV缓存卸载到远程FPGA内存；(ii) 推测性KV缓存预取机制，预测并预加载未来token的缓存条目；(iii) FPGA加速的KV缓存压缩和解压缩引擎，将内存带宽需求降低多达4倍。", "result": "CXL-SpecKV在LLM模型上实现了高达3.2倍的吞吐量提升，同时将内存成本降低了2.8倍，并保持了准确性。", "conclusion": "智能内存解耦与推测执行相结合，可以有效解决大规模LLM服务中的内存瓶颈挑战。"}}
{"id": "2512.12140", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12140", "abs": "https://arxiv.org/abs/2512.12140", "authors": ["Hangli Ge", "Hiroaki Mori", "Yasuhira Chiba", "Noboru Koshizuka"], "title": "Realizing Space-oriented Control in Smart Buildings via Word Embeddings", "comment": "4pages, 4figures, 1 table, accepted by IEEE GCCE 2025", "summary": "This paper presents a novel framework for implementing space-oriented control systems in smart buildings. In contrast to conventional device-oriented approaches, which often suffer from issues related to development efficiency and portability, our framework adopts a space-oriented paradigm that leverages natural language processing and word embedding techniques. The proposed framework features a chat-based graphical user interface (GUI) that converts natural language inputs into actionable OpenAI API calls, thereby enabling intuitive space level (e.g., room) control within smart environments. To support efficient embedding-based search and metadata retrieval, the framework integrates a vector database powered by Elasticsearch. This ensures the accurate identification and invocation of appropriate smart building APIs. A prototype implementation has been tested in a smart building environment at the University of Tokyo, demonstrating the feasibility of the approach.", "AI": {"tldr": "本文提出一种面向空间的智能建筑控制框架，利用自然语言处理、词嵌入和聊天界面，将自然语言指令转化为OpenAI API调用，实现直观的空间级控制，并集成向量数据库以支持高效搜索。", "motivation": "传统的面向设备的智能建筑控制方法存在开发效率低和可移植性差的问题。", "method": "该框架采用面向空间的范式，结合自然语言处理和词嵌入技术。它通过聊天式图形用户界面（GUI）将自然语言输入转换为可执行的OpenAI API调用，并集成由Elasticsearch驱动的向量数据库，以实现高效的基于嵌入的搜索和元数据检索。", "result": "在日本东京大学的智能建筑环境中进行了原型实现测试，结果表明该方法是可行的。", "conclusion": "所提出的框架为智能建筑中的直观空间级控制提供了一种可行的新方法。"}}
{"id": "2512.11891", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11891", "abs": "https://arxiv.org/abs/2512.11891", "authors": ["Songqiao Hu", "Zeyi Liu", "Shuang Liu", "Jun Cen", "Zihan Meng", "Xiao He"], "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer", "comment": "20 pages, 14 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.", "AI": {"tldr": "本文提出AEGIS，一个Vision-Language-Safe Action (VLSA) 架构，通过集成基于控制障碍函数（CBF）的即插即用安全约束层，显著提升了现有VLA模型在机器人操作任务中的安全性和任务成功率，同时保持了指令遵循性能。", "motivation": "VLA模型在机器人操作任务中表现出强大的泛化能力，但在非结构化环境中部署时，由于需要同时确保任务依从性和安全性（特别是避免碰撞），面临巨大挑战。", "method": "引入了名为AEGIS的Vision-Language-Safe Action (VLSA) 架构。其核心是一个基于控制障碍函数（CBF）的即插即用安全约束（SC）层，可直接与现有VLA模型集成，提供理论安全保证。为评估该架构，构建了一个全面的安全关键基准SafeLIBERO。 ", "result": "AEGIS在障碍物规避率上取得了59.16%的显著提升，同时将任务执行成功率提高了17.25%，表现优于现有SOTA基线。", "conclusion": "AEGIS架构通过引入基于CBF的安全约束层，成功解决了VLA模型在部署中面临的安全挑战，在提供理论安全保证的同时，显著提升了障碍物规避和任务成功率，使其更适用于现实世界的非结构化环境。"}}
{"id": "2512.11900", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11900", "abs": "https://arxiv.org/abs/2512.11900", "authors": ["Christopher E. Mower", "Rui Zong", "Haitham Bou-Ammar"], "title": "Data-driven Interpretable Hybrid Robot Dynamics", "comment": null, "summary": "We study data-driven identification of interpretable hybrid robot dynamics, where an analytical rigid-body dynamics model is complemented by a learned residual torque term. Using symbolic regression and sparse identification of nonlinear dynamics (SINDy), we recover compact closed-form expressions for this residual from joint-space data. In simulation on a 7-DoF Franka arm with known dynamics, these interpretable models accurately recover inertial, Coriolis, gravity, and viscous effects with very small relative error and outperform neural-network baselines in both accuracy and generalization. On real data from a 7-DoF WAM arm, symbolic-regression residuals generalize substantially better than SINDy and neural networks, which tend to overfit, and suggest candidate new closed-form formulations that extend the nominal dynamics model for this robot. Overall, the results indicate that interpretable residual dynamics models provide compact, accurate, and physically meaningful alternatives to black-box function approximators for torque prediction.", "AI": {"tldr": "本文研究了一种数据驱动方法，通过符号回归和SINDy识别可解释的混合机器人动力学模型，该模型结合了分析刚体动力学和学习到的残差扭矩项，在准确性和泛化性上优于神经网络基线，并提供了紧凑、准确且具有物理意义的扭矩预测模型。", "motivation": "研究动机在于开发一种可解释的机器人动力学模型，能够结合已知的分析模型和从数据中学习到的残差项，以克服纯黑盒模型的局限性，提供更紧凑、准确且具有物理意义的扭矩预测。", "method": "该研究采用混合机器人动力学模型，即在分析刚体动力学模型的基础上，补充一个学习到的残差扭矩项。通过符号回归（Symbolic Regression）和非线性动力学稀疏识别（SINDy）方法，从关节空间数据中恢复残差的紧凑闭式表达式。同时，使用神经网络作为基线进行性能比较。", "result": "在7自由度Franka机械臂的仿真中，可解释模型以极小的相对误差准确恢复了惯性、科里奥利力、重力和粘滞效应，并在准确性和泛化性上优于神经网络基线。在7自由度WAM机械臂的真实数据上，符号回归残差的泛化能力显著优于SINDy和神经网络（后者倾向于过拟合），并提出了扩展机器人名义动力学模型的新闭式公式。", "conclusion": "研究结果表明，可解释的残差动力学模型为扭矩预测提供了紧凑、准确且具有物理意义的替代方案，优于黑盒函数逼近器。"}}
{"id": "2512.11903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11903", "abs": "https://arxiv.org/abs/2512.11903", "authors": ["Iacopo Catalano", "Eduardo Montijano", "Javier Civera", "Julio A. Placed", "Jorge Pena-Queralta"], "title": "Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics", "comment": null, "summary": "Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.", "AI": {"tldr": "Aion是一个将时间流动力学直接嵌入分层3D场景图（3DSG）的框架，它利用基于图的稀疏动态图（MoD）表示来捕捉运动流，从而实现动态环境中更可解释和可扩展的自主导航。", "motivation": "动态环境中的自主导航需要同时捕捉语义结构和时间演变的表示。现有3D场景图（3DSG）对动态的扩展主要集中在单个物体或智能体上，而动态图（MoD）虽然能建模运动模式，但通常受限于缺乏语义意识且不易扩展的网格离散化。因此，需要一种能有效结合语义和时间维度，并具备可扩展性的新框架。", "method": "本文提出了Aion框架。它将时间流动力学直接嵌入到分层3DSG中，从而有效地整合了时间维度。Aion采用了一种基于图的稀疏MoD表示，用于捕捉任意时间间隔内的运动流，并将其附加到场景图中的导航节点上。", "result": "Aion提供了更可解释和可扩展的预测。它改进了在复杂动态环境中的规划和交互能力。", "conclusion": "Aion成功地将时间流动力学融入到分层3DSG中，通过结合语义结构和时间演变，提供了一种更可解释、可扩展且能改进复杂动态环境中规划和交互的自主导航解决方案。"}}
{"id": "2512.12245", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12245", "abs": "https://arxiv.org/abs/2512.12245", "authors": ["Anika Sharma", "Tianyi Niu", "Emma Wrenn", "Shashank Srivastava"], "title": "Adversarially Probing Cross-Family Sound Symbolism in 27 Languages", "comment": null, "summary": "The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.", "AI": {"tldr": "本文首次对跨语言的尺寸形容词中的声音象征现象进行了大规模计算分析，发现即使在无关语言中，语音形式也能显著预测尺寸语义，且这种关联在去除语言特异性信号后依然存在，表明其具有跨语系普遍性。", "motivation": "声音象征（词语发音与意义之间的非任意映射）现象虽然通过如“Bouba Kiki”等实验有所体现，但很少进行大规模测试。本研究旨在首次进行计算性的跨语言分析，特别是在尺寸语义领域。", "method": "研究构建了一个包含810个尺寸形容词（来自27种语言，每种30个词）的语料库，并进行了音素转录和母语者音频验证。使用基于音段特征的解释性分类器来预测尺寸语义。为探究超越谱系关系的普遍性，还训练了一个对抗性擦除器，在保留尺寸信号的同时抑制语言身份信息（甚至在语系粒度上）。", "result": "研究发现，语音形式能以高于随机的准确率预测尺寸语义，即使在不相关的语言中也是如此，且元音和辅音均有贡献。在对抗性擦除器处理后，语言预测的准确率降至随机水平以下，而尺寸预测的准确率仍显著高于随机水平，这表明存在跨语系的声音象征偏向。", "conclusion": "声音象征在尺寸语义领域是一个普遍存在的现象，其规律性超越了具体的语言和语系。研究发布了数据、代码和诊断工具，以支持未来对图像性的大规模研究。"}}
{"id": "2512.11935", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.11935", "abs": "https://arxiv.org/abs/2512.11935", "authors": ["Jaehyung Lee", "Justin Ely", "Kent Zhang", "Akshaya Ajith", "Charles Rhys Campbell", "Kamal Choudhary"], "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org", "comment": null, "summary": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.", "AI": {"tldr": "AGAPI是一个开放获取的智能体AI平台，通过整合开源大语言模型和材料科学API，利用Agent-Planner-Executor-Summarizer架构，实现自主构建和执行多步骤材料发现工作流，为AI加速材料研究提供可扩展且透明的基础。", "motivation": "尽管人工智能正在重塑科学发现，但其在材料研究中的应用受到计算生态系统碎片化、可复现性挑战以及对商业大语言模型依赖的限制。", "method": "本文介绍了AGAPI (AtomGPT.org API)，一个开放获取的智能体AI平台。它整合了超过八个开源大语言模型和二十多个材料科学API端点，通过一个通用的编排框架统一了数据库、模拟工具和机器学习模型。AGAPI采用Agent-Planner-Executor-Summarizer架构，能够自主构建和执行涵盖材料数据检索、图神经网络属性预测、机器学习力场优化、紧束缚计算、衍射分析和逆向设计等在内的多步骤工作流。", "result": "研究通过端到端工作流（包括异质结构构建、粉末X射线衍射分析和半导体缺陷工程，涉及多达十个顺序操作）展示了AGAPI的功能。此外，AGAPI还使用30多个示例提示作为测试用例进行了评估，并比较了有无工具访问的智能体预测结果与实验数据。AGAPI目前拥有超过1000名活跃用户。", "conclusion": "AGAPI为可复现、AI加速的材料发现提供了一个可扩展且透明的基础。"}}
{"id": "2512.12370", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12370", "abs": "https://arxiv.org/abs/2512.12370", "authors": ["Nicolai A. Weinreich", "Marco Muñiz", "Marius Mikučionis", "Kim G. Larsen", "Remus Teodorescu"], "title": "Data-Selective Online Battery Identification Using Extended Time Regular Expressions", "comment": "This work has been submitted to IFAC for possible publication", "summary": "In this paper, we propose a data-efficient online battery identification method which targets highly informative battery cell data segments based on the driving pattern of the vehicle. We consider the case of a vehicle driving on/off a motorway and construct an Extended Time Regular Expression (ETRE) to detect data segments fitting these driving patterns. Simulation results indicate that by only using up to 10.71% of the data on average, the proposed method provides a low-bias and low-variance estimator under non-negligible current and voltage noise compared to other conventional estimation algorithms.", "AI": {"tldr": "本文提出了一种数据高效的在线电池识别方法，通过扩展时间正则表达式（ETRE）根据车辆驾驶模式（如高速公路进出）选择信息丰富的电池数据片段。", "motivation": "在车辆驾驶过程中，需要一种数据高效的在线电池识别方法，以应对不同的驾驶模式并有效利用数据。", "method": "该方法通过构建扩展时间正则表达式（ETRE）来检测符合特定驾驶模式（如高速公路进出）的数据片段，从而识别出信息量高的电池数据。", "result": "仿真结果表明，该方法平均仅使用高达10.71%的数据，就能在存在显著电流和电压噪声的情况下，提供低偏差和低方差的估计器，优于其他传统估计算法。", "conclusion": "所提出的方法通过利用车辆驾驶模式选择信息丰富的电池数据，实现了数据高效的在线电池识别，并在噪声环境下表现出卓越的估计性能。"}}
{"id": "2512.12197", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12197", "abs": "https://arxiv.org/abs/2512.12197", "authors": ["Minghao Mou", "Junjie Qin"], "title": "Braess' Paradoxes in Coupled Power and Transportation Systems", "comment": "61 pages, 17 figures", "summary": "Transportation electrification introduces strong coupling between the power and transportation systems. In this paper, we generalize the classical notion of Braess' paradox to coupled power and transportation systems, and examine how the cross-system coupling induces new types of Braess' paradoxes. To this end, we model the power and transportation networks as graphs, coupled with charging points connecting to nodes in both graphs. The power system operation is characterized by the economic dispatch optimization, while the transportation system user equilibrium models travelers' route and charging choices. By analyzing simple coupled systems, we demonstrate that capacity expansion in either transportation or power system can deteriorate the performance of both systems, and uncover the fundamental mechanisms for such new Braess' paradoxes to occur. We also provide necessary and sufficient conditions of the occurrences of Braess' paradoxes for general coupled systems, leading to managerial insights for infrastructure planners. For general networks, through characterizing the generalized user equilibrium of the coupled systems, we develop efficient algorithms to detect Braess' paradoxes and novel charging pricing policies to mitigate them.", "AI": {"tldr": "本文将经典布雷斯悖论推广到电力和交通耦合系统，揭示了容量扩张可能导致双系统性能恶化的新型布雷斯悖论，并提出了检测算法和缓解策略。", "motivation": "交通电气化导致电力和交通系统之间出现强耦合。研究旨在理解这种跨系统耦合如何引发新型布雷斯悖论，即容量扩张可能反而损害系统性能。", "method": "将电力和交通网络建模为图，通过充电点耦合。电力系统操作通过经济调度优化表征，交通系统用户均衡模型化旅行者路线和充电选择。通过分析简单系统演示机制，并为一般耦合系统提供布雷斯悖论发生的充要条件。开发了检测算法和新型充电定价策略。", "result": "研究表明，在电力或交通系统中进行容量扩张可能同时恶化两个系统的性能，揭示了新型布雷斯悖论发生的根本机制。为一般耦合系统提供了布雷斯悖论发生的充要条件，并开发了有效的检测算法和缓解悖论的充电定价策略。", "conclusion": "交通电气化背景下，电力和交通耦合系统存在新型布雷斯悖论，即容量扩张可能导致反效果。理解这些悖论对基础设施规划者至关重要，且可以通过有效的检测算法和定价策略进行缓解。"}}
{"id": "2512.12736", "categories": ["cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12736", "abs": "https://arxiv.org/abs/2512.12736", "authors": ["Syeda Zunaira Ahmed", "Hejab Tahira Beg", "Maryam Khalid"], "title": "Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks", "comment": "11 pages, 5 figures", "summary": "Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.\n  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.\n  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.", "AI": {"tldr": "本文提出了一种人口统计学感知的机器学习框架，通过行为真实的数据增强策略，为5G网络中的自适应视频流提供个性化QoE预测，显著提高了预测准确性。", "motivation": "现有QoE预测方法依赖有限数据集并假设统一用户感知，这限制了它们在异构真实世界环境中的适用性，无法实现用户中心的服务交付和智能资源管理。", "method": "本文提出一个人口统计学感知的机器学习框架，引入了一种行为真实的人口统计学数据增强策略，将小型QoE数据集扩展了六倍，以模拟用户对流媒体缺陷（如卡顿、码率变化、质量下降）的不同敏感度。然后，使用增强后的数据集评估了经典的机器学习模型和先进的深度学习架构（包括基于注意力的MLP和TabNet）。", "result": "实验结果表明，与基线模型相比，该方法在RMSE、MAE和R指标上均显著提高了预测准确性。在所有评估方法中，TabNet表现最佳，这得益于其固有的特征选择和注意力机制。", "conclusion": "人口统计学感知的数据增强显著增强了QoE预测的鲁棒性，并为5G视频流网络中个性化QoE感知智能提供了一个可扩展的方向。"}}
{"id": "2512.11942", "categories": ["cs.AI", "cs.FL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.11942", "abs": "https://arxiv.org/abs/2512.11942", "authors": ["Vince Trencsenyi"], "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play", "comment": null, "summary": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.", "AI": {"tldr": "本文提出了一种声明式、基于逻辑的领域特定语言和自动化流程，利用回答集编程来表示超博弈结构并运行超博弈合理化程序，以解决多智能体系统中超博弈理论缺乏统一形式化和可扩展算法的问题。", "motivation": "传统的博弈论假设忽视了玩家之间感知、信息不对称和有限理性导致的私有、主观博弈观的差异。尽管超博弈理论提供了处理不匹配心理模型的数学框架，但其在多智能体系统中的实际应用受阻于缺乏统一、形式化、实用的表示语言以及管理复杂超博弈结构和均衡的可扩展算法。", "method": "本文通过引入一种声明式、基于逻辑的领域特定语言来编码超博弈结构和超博弈解概念。利用回答集编程，开发了一个自动化流程来实例化超博弈结构，并运行了一种新颖的超博弈合理化程序，该程序旨在寻找能解释看似非理性结果的信念结构。", "result": "所提出的语言为超博弈建立了统一的形式化，并为开发基于信念的异构推理器奠定了基础，提供了具有逻辑保证的可验证上下文。这些贡献建立了超博弈理论、多智能体系统和战略AI之间的联系。该方法实现了超博弈结构实例化和合理化过程的自动化。", "conclusion": "本文通过引入一种统一的语言和可扩展的算法，弥补了超博弈理论在多智能体系统实际应用中的空白，为处理玩家异质性和不匹配的心理模型提供了实用的工具和形式化基础，从而促进了超博弈理论在战略AI领域的应用。"}}
{"id": "2512.13144", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.13144", "abs": "https://arxiv.org/abs/2512.13144", "authors": ["Chun Kit Wong", "Paraskevas Pegios", "Nina Weng", "Emilie Pi Fogtmann Sejer", "Martin Grønnebæk Tolsgaard", "Anders Nymark Christensen", "Aasa Feragen"], "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models", "comment": "46 pages", "summary": "Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.", "AI": {"tldr": "本文提出了一种名为权重空间相关性分析（Weight Space Correlation Analysis）的可解释方法，用于量化深度学习模型对特征的利用情况。通过该方法，研究发现一个早产预测模型主要利用临床相关特征，而非图像中编码的无关元数据，从而验证了模型的可靠性。", "motivation": "医学影像中的深度学习模型容易受到捷径学习（shortcut learning）的影响，依赖于图像嵌入中编码的混杂元数据（如扫描仪型号）。关键问题是模型是否主动利用这些编码信息进行最终预测。", "method": "研究引入了权重空间相关性分析，这是一种可解释的方法，通过测量主临床任务分类头与辅助元数据任务分类头之间的对齐程度来量化特征利用率。该方法首先通过检测人工诱导的捷径学习进行了验证。", "result": "将该方法应用于一个用于自发性早产（sPTB）预测的SA-SonoNet模型。分析证实，虽然嵌入中包含大量元数据，但sPTB分类器的权重向量与临床相关因素（如出生体重）高度相关，而与临床无关的采集因素（如扫描仪）脱钩。", "conclusion": "该方法提供了一个验证模型可信度的工具，表明在没有诱导偏差的情况下，临床模型选择性地利用与真实临床信号相关的特征。"}}
{"id": "2512.11901", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11901", "abs": "https://arxiv.org/abs/2512.11901", "authors": ["Santosh Patapati"], "title": "CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities", "comment": "WACV; Supplementary material is available on CVF proceedings", "summary": "We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.", "AI": {"tldr": "CLARGA是一种通用的多模态融合架构，利用图注意力网络和混合损失函数，能够高效、自适应地处理任意数量和类型模态，并在各种任务和缺失输入情况下表现出色。", "motivation": "现有方法缺乏通用的多模态融合架构，无法灵活处理任意数量和类型的模态，且在适应不同样本、保持效率和应对缺失输入方面存在局限。", "method": "CLARGA为每个样本构建一个基于特征的注意力加权图，并使用多头图注意力网络在此图上传递信息，实现样本级的自适应融合。其复杂度为亚二次方，并通过可学习掩码处理缺失模态。模型采用混合目标函数训练：监督任务损失与对比式InfoNCE损失相结合，以提高跨模态一致性和鲁棒性。", "result": "CLARGA在金融、人机交互、多媒体分类和情感计算等7个不同数据集上的多模态表示学习任务中，持续优于基线、最先进模型和消融实验。额外实验还证明了其对缺失输入的鲁棒性以及在特定任务中的卓越表现。", "conclusion": "CLARGA是一种有效且高效的多模态表示学习解决方案，可以轻松集成到机器学习模型中，适用于各种任务。"}}
{"id": "2512.11997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11997", "abs": "https://arxiv.org/abs/2512.11997", "authors": ["Anfeng Peng", "Ajesh Koyatan Chathoth", "Stephen Lee"], "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion", "comment": null, "summary": "System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.", "AI": {"tldr": "EnrichLog是一个无需训练的、基于日志条目的异常检测框架，它通过检索增强生成技术，利用语料库和样本特有的知识丰富原始日志条目，从而提高异常检测的准确性和可解释性。", "motivation": "传统的日志分析技术（如基于模板和序列的方法）在处理分布式系统日志时，经常丢失重要的语义信息或难以处理模糊的日志模式，导致异常检测效果不佳。", "method": "EnrichLog是一个基于日志条目的无训练异常检测框架。它通过整合上下文信息（包括历史示例和从语料库中提取的推理），用语料库特有和样本特有的知识来丰富原始日志条目。该框架利用检索增强生成（RAG）技术，在不进行再训练的情况下集成相关上下文知识。", "result": "EnrichLog在四个大规模系统日志基准数据集上持续提高了异常检测性能，有效处理了模糊的日志条目，并保持了高效的推理速度。结果表明，结合语料库和样本特有的知识显著增强了模型的置信度和检测准确性。", "conclusion": "EnrichLog通过结合语料库和样本特有的知识，能够实现更准确、可解释的异常检测，有效处理模糊日志，并保持高效推理，使其非常适合实际部署中的系统日志异常检测任务。"}}
{"id": "2512.13397", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.13397", "abs": "https://arxiv.org/abs/2512.13397", "authors": ["Malte Silbernagel", "Albert Alonso", "Jens Petersen", "Bulat Ibragimov", "Marleen de Bruijne", "Madeleine K. Wyburd"], "title": "rNCA: Self-Repairing Segmentation Masks", "comment": null, "summary": "Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.", "AI": {"tldr": "本文提出将神经元细胞自动机（NCA）作为一种通用的精修机制，通过局部迭代更新修复分割掩码中常见的拓扑错误，显著提高了不同分割任务的性能。", "motivation": "通用分割模型常常生成拓扑结构不正确（如碎片化或断开）的掩码，而现有修复方法通常依赖于手工规则或特定任务的架构，缺乏通用性。", "method": "将神经元细胞自动机（NCA）重新设计为精修机制（rNCA）。通过对不完美掩码和真实标签进行训练，NCA利用图像上下文进行局部、迭代更新，学习目标形状的结构特性。它能逐步重新连接断裂区域、修剪松散碎片，并收敛到拓扑一致的结果。", "result": "rNCA在不同任务中表现出色：对于视网膜血管，Dice/clDice分数提升2-3%，β₀错误减少60%，β₁错误减少20%；对于心肌，在零样本设置下修复了61.5%的破损病例，同时将ASSD和HD分别降低了19%和16%。这表明NCA是一种有效且广泛适用的精修器。", "conclusion": "神经元细胞自动机（NCA）可以作为一种有效且广泛适用的精修机制，通过局部、迭代的更新，成功修复了不同基础分割模型和任务产生的拓扑错误，提高了分割结果的质量。"}}
{"id": "2512.11899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11899", "abs": "https://arxiv.org/abs/2512.11899", "authors": ["Futa Waseda", "Shojiro Yamabe", "Daiki Shiono", "Kento Sasaki", "Tsubasa Takahashi"], "title": "Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models", "comment": null, "summary": "Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.", "AI": {"tldr": "大型视觉语言模型（LVLMs）容易受到排版攻击，现有评估和防御方法鼓励忽略文本。本文引入了“读或不读视觉问答”（RIO-VQA）任务和“读或不读基准”（RIO-Bench）数据集，以评估模型选择性使用文本的能力，并发现现有模型和防御措施无法平衡排版鲁棒性与文本阅读能力，同时提出了一种新的数据驱动防御方法。", "motivation": "大型视觉语言模型（LVLMs）容易受到排版攻击，图像中的误导性文本会覆盖视觉理解。现有的评估协议和防御措施主要侧重于物体识别，隐式地鼓励模型忽略文本以实现鲁棒性。然而，现实世界场景通常需要对物体和文本进行联合推理（例如，识别行人同时阅读交通标志）。", "method": "本文引入了“读或不读视觉问答”（RIO-VQA）这一新任务，形式化了视觉问答（VQA）中选择性使用文本的需求：模型必须根据上下文决定何时阅读文本，何时忽略文本。为了评估，本文提出了“读或不读基准”（RIO-Bench），这是一个标准化的数据集和协议，为每个真实图像提供相同场景的反事实（阅读/忽略），仅通过改变文本内容和问题类型。此外，RIO-Bench还支持一种新颖的数据驱动防御方法，该方法学习自适应地选择性使用文本。", "result": "通过使用RIO-Bench，研究表明强大的LVLMs和现有的防御措施无法平衡排版鲁棒性与文本阅读能力，这突显了改进方法的必要性。RIO-Bench还促成了一种新颖的数据驱动防御方法，该方法能够学习自适应地选择性使用文本，超越了以往非自适应、忽略文本的防御措施。", "conclusion": "这项工作揭示了现有评估范围与现实世界需求之间存在的根本性偏差，为构建可靠的LVLMs提供了一条原则性的路径。"}}
{"id": "2512.12297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12297", "abs": "https://arxiv.org/abs/2512.12297", "authors": ["Radu-Gabriel Chivereanu", "Tiberiu Boros"], "title": "F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation", "comment": "Accepted at The 20th International Conference on Linguistic Resources and Tools for Natural Language Processing", "summary": "This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.", "AI": {"tldr": "该研究为F5-TTS模型引入了一个轻量级输入适配器，以支持罗马尼亚语，同时保留了模型的语音克隆、英语和中文支持能力。", "motivation": "为了在不重新训练整个模型的情况下，让F5-TTS模型支持罗马尼亚语，并同时保持其现有的语音克隆以及英语和中文支持能力。", "method": "研究人员冻结了F5-TTS模型的原始权重，并附加了一个子网络作为文本编码器文本嵌入矩阵的扩展进行训练。他们利用F5-TTS中已有的ConvNeXt模块来建模新字符级嵌入之间的共依赖性，作为“软”字母到声音层。模型通过20名人类听众在三个任务上进行评估：音频相似性、发音和自然度、罗马尼亚语-英语语码转换。", "result": "该方法保持了语音克隆能力，并在一定程度上实现了同一语句内的语码转换；然而，仍然存在残余的英语口音特征。", "conclusion": "该方法成功地为F5-TTS模型添加了罗马尼亚语支持，并保留了其语音克隆能力和部分语码转换能力，但英语口音问题仍有待解决。"}}
{"id": "2512.11921", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11921", "abs": "https://arxiv.org/abs/2512.11921", "authors": ["Abdullah Yahya Abdullah Omaisan", "Ibrahim Sheikh Mohamed"], "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.", "AI": {"tldr": "本文提出了一种高效的微调方法和部署分析，使大型视觉-语言-动作（VLA）模型（3.1B参数）能够在消费级GPU上运行，并成功部署到低成本机器人平台上，实现有效的操作性能。", "motivation": "VLA模型在机器人操作中表现出色，但由于计算限制和适应新机器人平台的需求，将其部署到经济实惠的机器人上仍面临挑战。", "method": "本文提出了一种资源高效的微调策略，结合低秩适应（LoRA）和量化技术，使数十亿参数的VLA模型能在配备8GB显存的消费级GPU上运行。该方法还探讨了冻结和非冻结视觉编码器之间的权衡，以适应有限演示数据的新机器人平台。", "result": "通过在SO101机械臂上进行按钮按压任务的实际部署，本文证明该方法在保持计算效率的同时，实现了有效的操作性能。在200个演示示例上进行训练，成功将VLA模型部署到低成本机器人平台，并详细分析了部署挑战、失败模式以及训练数据量与实际性能的关系。", "conclusion": "研究结果表明，通过适当的微调方法，VLA模型可以成功部署在经济实惠的机器人平台上，使得先进的操作能力不再局限于昂贵的研究机器人，提高了其可及性。"}}
{"id": "2512.11908", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11908", "abs": "https://arxiv.org/abs/2512.11908", "authors": ["Heng Zhang", "Rui Dai", "Gokhan Solak", "Pokuang Zhou", "Yu She", "Arash Ajoudani"], "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models", "comment": null, "summary": "Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.", "AI": {"tldr": "本综述全面概述了机器人接触密集型任务中基于安全学习的方法，涵盖安全探索和安全执行，并特别强调了这些原则如何与新兴的机器人基础模型（如VLM/VLA）结合，以及由此带来的机遇和挑战。", "motivation": "接触密集型任务因固有的不确定性、复杂的动力学以及交互过程中的高损伤风险，对机器人系统构成巨大挑战。尽管基于学习的控制在使机器人获取和泛化复杂操作技能方面展现出巨大潜力，但确保探索和执行过程中的安全性仍然是实际部署的关键瓶颈。", "method": "本综述将现有方法分为两大领域：安全探索和安全执行。审查了关键技术，包括约束强化学习、风险敏感优化、不确定性感知建模、控制障碍函数和模型预测安全防护罩。特别关注这些安全学习原则如何扩展并与新兴的机器人基础模型（特别是视觉-语言模型VLM和视觉-语言-动作模型VLA）相互作用。", "result": "本综述展示了现有方法如何通过结合先验知识、任务结构和在线适应来平衡安全性和效率。讨论了VLM/VLA方法带来的新安全机遇（如语言级约束规范和安全信号的多模态接地）以及它们引入的放大风险和评估挑战。", "conclusion": "总结了当前局限性，并展望了未来有希望的方向，旨在复杂接触密集型环境中部署可靠、安全对齐且由基础模型赋能的机器人。"}}
{"id": "2512.12264", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12264", "abs": "https://arxiv.org/abs/2512.12264", "authors": ["Abhay Srivastava", "Sam Jung", "Spencer Mateega"], "title": "Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics", "comment": null, "summary": "We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.", "AI": {"tldr": "本文介绍了MARKET-BENCH基准测试，用于评估大型语言模型（LLMs）在量化交易任务中构建可执行回测器的能力。研究发现，LLMs可以构建基础交易基础设施，但在价格、库存和风险的鲁棒推理方面仍有不足。", "motivation": "研究动机是评估LLMs在量化交易领域的实际应用能力，特别是它们能否根据自然语言描述和市场假设构建可执行的回测器，从而判断其在金融智能方面的潜力。", "method": "研究引入了MARKET-BENCH基准测试，包含三种经典策略（微软的定期交易、可口可乐和百事可乐的配对交易、微软的Delta对冲）。模型需生成代码，其盈亏、回撤和头寸路径需与参考实现匹配。评估采用多轮pass@k指标，区分结构可靠性（回测是否运行）和数值准确性（回测指标的平均绝对误差）。", "result": "大多数模型能可靠执行最简单的策略（平均pass@3为0.80），但错误在模型和任务之间差异巨大。Gemini 3 Pro和Claude 4.5 Sonnet在简单策略上表现出高可靠性和低误差。GPT-5.1 Codex-Max在前两种策略上达到完美的pass@1，并在最简单任务上误差最低。Qwen3 Max实现了完美的pass@3，但有时会产生不准确的盈亏路径。", "conclusion": "当前LLMs能够构建基本的交易基础设施，但它们在对价格、库存和风险进行鲁棒推理方面仍然面临挑战。研究发布了MARKET-BENCH基准和公开排行榜，以促进该领域未来的研究和发展。"}}
{"id": "2512.11905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11905", "abs": "https://arxiv.org/abs/2512.11905", "authors": ["Ming-Zher Poh", "Shun Liao", "Marco Andreetto", "Daniel McDuff", "Jonathan Wang", "Paolo Di Achille", "Jiang Wu", "Yun Liu", "Lawrence Cai", "Eric Teasley", "Mark Malhotra", "Anupam Pathak", "Shwetak Patel"], "title": "Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life", "comment": null, "summary": "Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.", "AI": {"tldr": "本研究提出并验证了通过智能手机被动捕捉的自然微笑，作为衡量积极情感和主观幸福感的客观、可扩展的行为指标，克服了传统自报方法的局限性。", "motivation": "主观幸福感是个人和社会健康的核心，但其科学测量传统上依赖于自报方法，这些方法容易出现回忆偏差和参与者负担重的问题。这导致我们对日常生活中幸福感的表达方式理解不足。", "method": "研究分析了233名同意参与者在一周内被动记录的405,448个视频片段。利用深度学习模型量化微笑强度，并识别其昼夜和每日模式。此外，还分析了微笑强度与体育活动、光照暴露和智能手机使用之间的关联。", "result": "微笑强度的每日模式与全国幸福感调查数据高度相关（r=0.92），昼夜节律与日重建方法的结果密切对应（r=0.80）。较高的日均微笑强度与更多的体育活动（Beta系数=0.043）和更大的光照暴露（Beta系数=0.038）显著相关，而与智能手机使用没有发现显著影响。", "conclusion": "这些发现表明，被动式智能手机感知可以作为一种强大、生态有效的方法来研究情感行为的动态，并为在人群规模上理解这种行为开辟了道路。"}}
{"id": "2512.12474", "categories": ["eess.SY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12474", "abs": "https://arxiv.org/abs/2512.12474", "authors": ["Jamsheed Mistri"], "title": "AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion", "comment": "13 pages, 4 figures", "summary": "Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.", "AI": {"tldr": "本文提出了一种基于AI的跆拳道计分系统，通过传感器融合和机器学习（SVMs）实时分类踢腿技术，并结合新的计分规则，旨在提升奥运跆拳道的观赏性、公平性及观众参与度。", "motivation": "奥运跆拳道因静态、防御性比赛和有争议的计分系统导致观众参与度低。现有保护和计分系统（PSS）依赖简单的冲击传感器和逻辑，鼓励安全策略，降低了比赛的活力。", "method": "提出一个AI驱动的计分系统，融合现有PSS传感器与加速度计、陀螺仪、磁/RFID及冲击力传感器。利用传感器融合和支持向量机（SVMs）的机器学习流程，实时分类踢腿技术（类型、接触位置、冲击力、脚部使用部位）。设计了新的计分规则，根据特定踢腿技术（如转身踢、旋转踢）奖励积分，以鼓励动态进攻。", "result": "基于2024年一项研究，验证了实时踢腿分类的可行性，准确率达到96-98%。提出通过集成SVM分类器和扩展数据集等方法进一步提高准确性，以满足竞技体育的高精度要求。", "conclusion": "所提出的系统能够提高计分公平性，减少规则利用和不正当战术，鼓励更多动态技术，并增强观众的理解和兴奋度，从而全面提升奥运跆拳道的观赏性和吸引力。"}}
{"id": "2512.12048", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12048", "abs": "https://arxiv.org/abs/2512.12048", "authors": ["Muddsair Sharif", "Huseyin Seker"], "title": "Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp", "comment": null, "summary": "This paper presents a novel context-sensitive multi\\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\\%), grid operators (20\\%), charging station operators (20\\%), fleet operators (20%), and environmental factors (15\\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\\-DRA framework achieves 92\\% coordination success rate, 15\\% energy efficiency improvement, 10\\% cost reduction, 20% grid strain decrease, and \\2.3x faster convergence while maintaining 88\\% training stability and 85\\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\\$122,962 and 69\\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.", "AI": {"tldr": "本文提出了一种名为CAMAC-DRA的情境感知多智能体协调框架，用于优化智能电动汽车充电生态系统。该框架通过整合深度Q网络、图神经网络和注意力机制，平衡多方利益，并在动态环境中显著提升能源效率、降低成本和电网压力。", "motivation": "智能电动汽车充电生态系统面临动态环境和多方利益（如电动汽车用户、电网运营商、充电站运营商、车队运营商和环境因素）之间的复杂平衡挑战。需要一个能够适应实时变量并协调这些竞争目标的解决方案。", "method": "本文提出了CAMAC-DRA框架，通过Smart2Charge应用实现。该系统协调250辆电动汽车和45个充电站的自主充电智能体，并通过情境感知决策适应动态环境。其多智能体方法结合了协调的深度Q网络、图神经网络和注意力机制，处理包括天气、交通、电网负荷和电价在内的20个情境特征。框架通过加权协调机制和共识协议平衡五个利益相关者的目标。", "result": "通过包含441,077笔充电交易的真实世界数据集进行验证，CAMAC-DRA框架实现了92%的协调成功率、15%的能源效率提升、10%的成本降低、20%的电网压力减少以及2.3倍更快的收敛速度，同时保持88%的训练稳定性和85%的样本效率。与DDPG、A3C、PPO和GNN等基线算法相比，性能表现优越。真实世界验证证实了其商业可行性，净现值为-$122,962，并通过可再生能源整合实现69%的成本降低。", "conclusion": "CAMAC-DRA框架通过开发情境感知、多利益相关者协调机制，成功平衡了竞争目标并适应实时变量，为智能电动汽车充电协调和可持续交通电气化提供了突破性解决方案。"}}
{"id": "2512.12059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12059", "abs": "https://arxiv.org/abs/2512.12059", "authors": ["Luke Bhan", "Hanyu Zhang", "Andrew Gordon Wilson", "Michael W. Mahoney", "Chuck Arvin"], "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification", "comment": "Presented at AAAI 2026 AI4TS workshop and AABA4ET workshop", "summary": "Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.", "AI": {"tldr": "本文提出“预测评论家”系统，利用大型语言模型（LLMs）实现自动化预测监控，并系统评估了LLMs评估时间序列预测质量的能力。", "motivation": "在大规模零售业务中，监控预测系统对客户满意度、盈利能力和运营效率至关重要。研究旨在利用LLMs的广泛世界知识和推理能力，解决现有预测监控的挑战。", "method": "研究提出“预测评论家”系统，利用LLMs进行自动化预测监控。通过三个实验，在合成和真实世界的预测数据上，评估LLMs回答以下三个关键问题的能力：1) LLMs能否识别明显不合理的预测？2) LLMs能否有效整合非结构化外部特征来评估预测合理性？3) 性能如何随模型大小和推理能力变化？还评估了多模态LLMs整合非结构化上下文信号的能力。", "result": "LLMs能可靠检测和批评不良预测（如时间错位、趋势不一致、尖峰错误），最佳模型F1分数为0.88（人类水平为0.97）。多模态LLMs能有效整合非结构化上下文信号（如促销历史），以识别缺失或错误的促销尖峰（F1分数为0.84）。在真实世界的M5时间序列数据集上，LLMs成功识别出不准确的预测，不合理预测的sCRPS比合理预测高至少10%。", "conclusion": "研究结果表明，即使没有领域特定的微调，LLMs也能为自动化预测监控和评估提供一个可行且可扩展的方案。"}}
{"id": "2512.11906", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11906", "abs": "https://arxiv.org/abs/2512.11906", "authors": ["Noorul Wahab", "Nasir Rajpoot"], "title": "MPath: Multimodal Pathology Report Generation from Whole Slide Images", "comment": "Pages 4, Figures 1, Table 1", "summary": "Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.", "AI": {"tldr": "MPath是一个轻量级多模态框架，通过视觉前缀提示机制，利用WSI视觉嵌入来驱动预训练的生物医学语言模型（BioBART），从而自动生成诊断病理报告。", "motivation": "将高分辨率组织模式转化为临床连贯的文本报告极具挑战性，原因在于形态学的高度变异性和病理叙述的复杂结构。因此，直接从全玻片图像（WSI）自动生成诊断病理报告是一个新兴且重要的方向。", "method": "MPath框架通过学习到的视觉前缀提示机制，将WSI导出的视觉嵌入条件化地输入到预训练的生物医学语言模型（BioBART）中。它利用基础模型WSI特征（CONCH + Titan），并通过紧凑的投影模块将其注入BioBART，同时保持语言主干模型冻结以确保稳定性和数据效率，而非进行端到端的视觉-语言预训练。", "result": "MPath在RED 2025 Grand Challenge数据集上进行了开发和评估，并在测试阶段2中排名第4，尽管提交机会有限。", "conclusion": "研究结果强调了基于提示的多模态条件作用作为病理报告生成的可扩展和可解释策略的潜力。"}}
{"id": "2512.12337", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12337", "abs": "https://arxiv.org/abs/2512.12337", "authors": ["Yushen Fang", "Jianjun Li", "Mingqian Ding", "Chang Liu", "Xinchi Zou", "Wenqi Yang"], "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema", "comment": null, "summary": "Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.", "AI": {"tldr": "本文提出了一种名为SCIR的自校正迭代细化框架和一个多任务双语自校正数据集MBSC，以解决LLM驱动的信息抽取（IE）系统中高训练成本和难以与LLM偏好对齐的问题。SCIR显著降低了训练成本，并提高了IE任务的性能。", "motivation": "当前基于大型语言模型（LLM）的信息抽取（IE）系统面临两大限制：高昂的训练成本以及难以与LLM偏好对齐。研究旨在解决这些问题，提升IE系统的效率和准确性。", "method": "本文提出了SCIR（Self-Correcting Iterative Refinement）框架，它通过双路径自校正模块和反馈驱动优化，实现了与现有LLM和IE系统的即插即用兼容性，从而大幅降低了训练成本。同时，还构建了一个包含超过100,000条条目的多任务双语（中英文）自校正数据集MBSC，通过间接蒸馏GPT-4的能力来解决偏好对齐的挑战。", "result": "实验结果表明，SCIR框架在命名实体识别、关系抽取和事件抽取三项关键任务上超越了最先进的IE方法，在基于跨度的Micro-F1指标上平均提高了5.27%，同时与基线方法相比，训练成本降低了87%。", "conclusion": "这些进展不仅增强了信息抽取系统的灵活性和准确性，也为轻量级和高效的IE范式铺平了道路。"}}
{"id": "2512.12021", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12021", "abs": "https://arxiv.org/abs/2512.12021", "authors": ["Xincheng Cao", "Haochong Chen", "Bilin Aksun-Guvenc", "Levent Guvenc"], "title": "Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking", "comment": null, "summary": "Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.", "AI": {"tldr": "本文提出了一种改进的Hybrid-A*路径规划算法，结合运动学可行性和静态障碍物避障功能，用于解决车辆在狭窄空间内的泊车挑战，并在仿真中验证了其生成无碰撞且运动学可行轨迹的有效性。", "motivation": "在狭窄空间内泊车是一项具有挑战性的任务，因为可行的无碰撞路径稀缺。", "method": "研究采用了一种修改后的Hybrid-A*路径规划算法。该算法结合了标准Hybrid A*算法固有的可行性保证，并增加了静态障碍物避障功能。文中推导了一个运动学单轨模型来描述车辆的低速运动，并将其用作Hybrid A*算法中的运动模型以生成可行的运动原语分支。模型状态还用于重建车辆中心线，结合膨胀的二进制占用图，实现了静态障碍物避障功能。", "result": "通过仿真研究和动画测试，所提出的算法被证明能够持续提供运动学上可行且无碰撞的轨迹。", "conclusion": "所提出的改进Hybrid-A*算法能够有效地处理车辆在狭窄空间内泊车的挑战，生成既符合车辆运动学特性又避开障碍物的路径。"}}
{"id": "2512.12475", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12475", "abs": "https://arxiv.org/abs/2512.12475", "authors": ["Grace E. Calkins", "Jay W. McMahon", "David C. Woffinden"], "title": "Improved Directional State Transition Tensors for Accurate Aerocapture Performance Analysis", "comment": "Submitted to AIAA Journal of Guidance, Control, and Dynamics for publication", "summary": "Aerocapture is a unique challenge for semi-analytical propagation because its nonconservative dynamics lead to force magnitudes that vary substantially across the trajectory. State transition tensors (STTs), higher-order Taylor series expansions of the solution flow, have been widely used as a computationally efficient semi-analytical propagation method for orbital scenarios, but have not previously been applied to aerocapture. However, obtaining the higher-order STTs requires integrating exponentially more equations. Directional state transition tensors (DSTTs) mitigate this cost by projecting the state into a reduced-dimension basis. This work develops novel dynamics analysis techniques to identify effective bases for this reduction, including augmented higher-order Cauchy Green tensors tailored to quantities of interest such as apoapsis radius. Results show that DSTTs constructed along these bases significantly reduce computational cost while maintaining accuracy in apoapsis and energy prediction. In particular, certain of these DSTTs outperform traditional DSTTs in nonlinear perturbation propagation for key state subsets and quantities of interest. These results establish STTs and DSTTs as practical tools for aerocapture performance analysis to enable robust guidance and navigation.", "AI": {"tldr": "本文首次将状态转移张量（STT）和方向状态转移张量（DSTT）应用于气动捕获（aerocapture）任务，并开发了新的动力学分析技术来识别有效的降维基，显著降低了计算成本，同时保持了高精度。", "motivation": "气动捕获的非保守动力学导致力大小沿轨迹变化显著，对半解析传播构成独特挑战。STT是一种计算高效的轨道传播方法，但尚未应用于气动捕获。获取高阶STT需要集成指数级更多的方程，而DSTT通过投影到降维基来缓解这一成本，但需要有效的基选择。", "method": "本文开发了新颖的动力学分析技术，包括增强型高阶柯西-格林张量（augmented higher-order Cauchy Green tensors），以识别气动捕获场景下DSTT的有效降维基，特别是针对远拱点半径等关注量。", "result": "结果表明，基于这些新基构建的DSTT显著降低了计算成本，同时在远拱点和能量预测方面保持了高精度。特别地，某些DSTT在关键状态子集和关注量的非线性扰动传播方面优于传统DSTT。", "conclusion": "这些结果确立了STT和DSTT作为气动捕获性能分析的实用工具，能够实现鲁棒的制导和导航，特别是在引入新基选择技术后，其在效率和精度上表现出色。"}}
{"id": "2512.12444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12444", "abs": "https://arxiv.org/abs/2512.12444", "authors": ["Veronica Mangiaterra", "Hamad Al-Azary", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors", "comment": "30 pages, 5 figures", "summary": "As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.", "AI": {"tldr": "本研究评估了GPT模型在比喻属性（熟悉度、可理解性、可想象性）评分上的有效性和可靠性，发现其结果与人类评分高度相关，并能有效预测行为和电生理反应，表明LLMs可替代或增强人类评分，但在处理比喻的常规性和多模态方面需谨慎。", "motivation": "大型语言模型（LLMs）在科学研究中的应用日益广泛，其可信度成为关键问题。在心理语言学领域，LLMs已被用于自动扩充人类评分数据集，并在单个词汇评分上取得了有希望的结果。然而，对于比喻等复杂项目的评分表现仍未被探索。", "method": "本研究评估了三个GPT模型对687个比喻（来自意大利比喻档案和三个英语研究）在熟悉度、可理解性和可想象性方面的评分的有效性和可靠性。通过与人类数据的一致性以及预测行为和电生理反应的能力进行了全面的验证。", "result": "机器生成的评分与人类评分呈正相关。熟悉度评分在英语和意大利语比喻中均达到中度到强度的相关性，但对于高感觉运动负荷的比喻，相关性减弱。可想象性在英语中显示中度相关，在意大利语中显示中度到强度相关。英语比喻的可理解性表现出最强的相关性。总体而言，大型模型优于小型模型，且在熟悉度和可想象性方面出现更大的人机不一致。机器生成的评分显著预测了反应时间和脑电图幅度，强度与人类评分相当。此外，GPT在独立会话中获得的评分高度稳定。", "conclusion": "GPT模型，特别是大型模型，可以有效且可靠地替代或增强人类主体对比喻属性的评分。然而，在处理比喻意义的常规性和多模态方面，LLMs与人类的对齐程度较差，因此需要仔细考虑刺激的性质。"}}
{"id": "2512.11925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11925", "abs": "https://arxiv.org/abs/2512.11925", "authors": ["Mozhgan Hadadi", "Talukder Z. Jubery", "Patrick S. Schnable", "Arti Singh", "Bedrich Benes", "Adarsh Krishnamurthy", "Baskar Ganapathysubramanian"], "title": "FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications", "comment": null, "summary": "Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.", "AI": {"tldr": "FloraForge是一个LLM辅助框架，使领域专家能够通过自然语言迭代精炼，生成生物学上准确、完全参数化的3D植物模型，从而降低对编程专业知识的要求。", "motivation": "当前的3D植物模型方法存在局限性：基于学习的方法需要大量特定物种的训练数据且缺乏可编辑性；程序化建模虽然提供参数控制，但需要几何建模和复杂程序规则的专业知识，使领域科学家难以使用。", "method": "FloraForge利用LLM辅助共同设计来精炼Python脚本，生成具有植物学约束、显式控制点和参数变形函数的层次B样条曲面表示的参数化植物几何。它通过迭代的自然语言“植物精炼”（PR）和人工可读的“植物描述符”（PD）文件，将程序模型拟合到经验点云数据。", "result": "该框架在玉米、大豆和绿豆上进行了演示，能够生成双重输出：用于可视化的三角网格，以及带有额外参数元数据用于定量分析的三角网格。它独特地结合了LLM辅助的模板创建、实现表型分析和渲染的数学连续表示，以及通过PD直接的参数控制。", "conclusion": "FloraForge在保持数学严谨性的同时，使植物科学领域的复杂几何建模民主化。"}}
{"id": "2512.12600", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.12600", "abs": "https://arxiv.org/abs/2512.12600", "authors": ["Si Wu", "Tengfei Liu", "Yiguang Hong", "Zhong-Ping Jiang", "Tianyou Chai"], "title": "Feasible-Set Reshaping for Constraint Qualification in Optimization-Based Control", "comment": null, "summary": "This paper presents a novel feasible-set reshaping technique to optimization-based control with ensured constraint qualification. In our problem setting, the feasible set of admissible control inputs depends on the real-time state of the plant, and the linear independence constraint qualification (LICQ) may not be satisfied in some regions of interest. By feasible-set reshaping, we project the constraints of the original feasible set onto an appropriately chosen constant matrix with its rows forming a positive span of the space of the optimization variable. It is proved that the reshaped feasible set is nonempty and satisfies LICQ, as long as the original feasible set is nonempty. The effectiveness of the proposed method is verified by constructing Lipschitz continuous quadratic-program-based (QP-based) controllers based on the reshaped feasible sets.", "AI": {"tldr": "本文提出了一种新颖的可行集重塑技术，用于基于优化的控制，以确保满足线性独立约束条件（LICQ）。", "motivation": "在基于优化的控制中，可接受控制输入的G可行集取决于实时设备状态，且在某些关键区域可能不满足LICQ，这给控制器设计带来了挑战。", "method": "通过将原始可行集的约束投影到一个经过适当选择的常数矩阵上，该矩阵的行构成了优化变量空间的积极跨度，从而实现可行集重塑。", "result": "只要原始可行集非空，重塑后的可行集即为非空且满足LICQ。通过构建基于重塑可行集的Lipschitz连续二次规划（QP）控制器，验证了该方法的有效性。", "conclusion": "所提出的可行集重塑方法能够有效解决优化控制中LICQ不满足的问题，并有助于构建性能良好的QP基控制器。"}}
{"id": "2512.11898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11898", "abs": "https://arxiv.org/abs/2512.11898", "authors": ["Yawar Ali", "K. Ramachandra Rao", "Ashish Bhaskar", "Niladri Chatterjee"], "title": "Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic", "comment": "This paper presents basic statistics and trends in empirically observed data from highly heterogeneous and area-based traffic while offering the datasets open source for researchers and practitioners", "summary": "This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.", "AI": {"tldr": "本文提供了一套开放获取的、使用无人机（UAV）在印度异质性城市交通条件下采集的微观车辆轨迹（MVT）数据集，旨在克服传统路边视频采集的局限性，并支持全球交通研究。", "motivation": "传统的路边视频采集在密集混合交通中因遮挡、视角受限和车辆运动不规律等问题而效果不佳。研究人员需要一种能提供俯视视角、减少这些问题并捕捉丰富时空动态的方法，以更好地理解和建模异质性、区域性城市交通。", "method": "研究采用无人机进行俯视记录，利用Data from Sky (DFS) 平台提取数据。数据集通过与人工计数、空间平均速度和探测轨迹进行对比验证。每个数据集包含时间戳的车辆位置、速度、纵向和横向加速度以及车辆分类，分辨率为每秒30帧。数据在印度国家首都地区的六个路段收集，覆盖了不同的交通构成和密度水平。", "result": "数据集包含了详细的微观车辆轨迹数据，反映了多样化的交通组成和密度水平。初步分析揭示了异质性、区域性交通环境下典型的行为模式，包括车道保持偏好、速度分布和横向机动。这些数据集已开放共享。", "conclusion": "这些经验数据集为全球研究社区提供了一个宝贵的资源，可用于在区域性交通条件下进行仿真建模、安全评估和行为研究。通过开放这些数据，研究人员将有机会开发、测试和验证能更准确反映复杂城市交通环境的模型。"}}
{"id": "2512.11944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11944", "abs": "https://arxiv.org/abs/2512.11944", "authors": ["Jia Hu", "Yang Chang", "Haoran Wang"], "title": "A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach", "comment": "34 pages, 11 figures", "summary": "Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, \"black-box\" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: \"Human-Centric\" customization, \"Platform-Adaptive\" dynamics adaptation, and \"System Self-Optimization\" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.", "AI": {"tldr": "本文综述了自动驾驶运动规划领域从传统管道方法到学习方法的演变，指出透明性与适应性之间的两难困境，并提出了一种数据驱动的最优控制范式，旨在融合经典控制的可验证结构与机器学习的适应性，以实现安全、可解释且类人自主的下一代系统。", "motivation": "自动驾驶运动规划面临一个根本性权衡：管道方法透明但脆弱，而现代学习系统适应性强但缺乏透明度。这种持续的困境阻碍了真正值得信赖的自动驾驶系统的发展。", "method": "本文批判性地综合了该领域从管道方法到模仿学习、强化学习和生成式AI的演变。通过对学习型运动规划方法进行全面综述，提出了一种数据驱动的最优控制范式作为统一框架。", "result": "提出的数据驱动最优控制范式能协同整合经典控制的可验证结构与机器学习的适应能力，利用真实世界数据持续优化系统动力学、成本函数和安全约束。该框架有望实现“以人为本”的定制、“平台自适应”的动态适应和通过自调优实现的“系统自优化”三大下一代能力。", "conclusion": "基于所提出的范式，未来的研究方向应致力于开发同时具备安全性、可解释性并能实现类人自主的智能交通系统。"}}
{"id": "2512.12447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12447", "abs": "https://arxiv.org/abs/2512.12447", "authors": ["Gary Lupyan"], "title": "Large language models have learned to use language", "comment": "Commentary on Futrell & Mahowald's How Linguistics Learned to Stop Worrying and Love the Language Models (BBS, Forthcoming)", "summary": "Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.", "AI": {"tldr": "大型语言模型已掌握语言能力，这要求我们重新思考语言知识评估方式，因为我们已进入后图灵测试时代。", "motivation": "承认大型语言模型（LLMs）已学会使用语言，这为语言科学的突破性进展提供了机遇。", "method": "本文没有描述具体的研究方法，但提出需要放弃一些长期持有的关于语言知识评估的观念，并正视我们已进入后图灵测试时代这一事实。", "result": "本文没有呈现具体的研究结果，而是指出大型语言模型已学会使用语言，这开启了语言科学的突破之门。", "conclusion": "为在语言科学领域取得突破，必须放弃一些旧的语言知识评估观念，并接受我们已进入后图灵测试时代这一现实。"}}
{"id": "2512.12088", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12088", "abs": "https://arxiv.org/abs/2512.12088", "authors": ["S. R. Eshwar", "Aniruddha Mukherjee", "Kintan Saha", "Krishna Agarwal", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations", "comment": null, "summary": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.", "AI": {"tldr": "本文评估了可靠策略迭代（RPI）在函数逼近设置下的鲁棒性，RPI通过恢复策略迭代的值估计单调性，在经典控制任务中表现出比DQN、DDPG等现有方法更早达到最优性能并保持稳定的优势。", "motivation": "深度强化学习方法常受样本效率低下、训练不稳定和超参数敏感性等问题困扰。RPI旨在通过恢复函数逼近中策略迭代的值估计单调性，提供一个更可靠的替代方案。", "method": "在CartPole和Inverted Pendulum两个经典控制任务上，通过改变神经网络和环境参数，评估RPI的经验性能鲁棒性。并将RPI与DQN、Double DQN、DDPG、TD3和PPO等现有方法进行比较。", "result": "RPI能早期达到接近最优的性能，并在训练过程中持续保持该策略。相对于DQN、Double DQN、DDPG、TD3和PPO，RPI表现出更强的鲁棒性。", "conclusion": "RPI作为一种更可靠的深度强化学习替代方案，有望解决现有方法存在的样本效率低、训练不稳定和超参数敏感性等问题。"}}
{"id": "2512.11926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11926", "abs": "https://arxiv.org/abs/2512.11926", "authors": ["Qinghao Meng", "Chenming Wu", "Liangjun Zhang", "Jianbing Shen"], "title": "TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder", "comment": "12 pages, 9 figures", "summary": "3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.", "AI": {"tldr": "本文提出一个联合补全与检测框架，通过Transformer基的TransBridge上采样模块和动态-静态重建（DSRecon）模块，在不增加成本的情况下，显著提升稀疏区域的3D目标检测性能。", "motivation": "自动驾驶中，3D目标检测在远距离稀疏LiDAR点云区域表现不佳，难以有效检测目标和障碍物。需要有效的点云稠密化策略来解决这一挑战。", "method": "本文提出了一个联合补全与检测框架。具体方法包括：1) TransBridge，一个新型基于Transformer的上采样模块，用于融合检测和补全网络的特征。2) 动态-静态重建（DSRecon）模块，为补全网络生成稠密的LiDAR数据作为真值。3) 利用Transformer机制建立通道和空间关系，生成用于补全的高分辨率特征图，使检测网络能从补全网络获取隐式补全特征。", "result": "在nuScenes和Waymo数据集上的广泛实验表明，所提出的框架能持续改进端到端3D目标检测。在多种方法中，平均精度（mAP）提升了0.7到1.5个点，显示了其泛化能力。对于两阶段检测框架，mAP最高提升了5.78个点。", "conclusion": "该框架通过结合补全和检测，并引入TransBridge和DSRecon模块，有效解决了稀疏LiDAR点云区域的3D目标检测难题，显著提升了检测性能，且具有良好的泛化能力。"}}
{"id": "2512.12488", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12488", "abs": "https://arxiv.org/abs/2512.12488", "authors": ["James Luther", "Donald Brown"], "title": "The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting", "comment": null, "summary": "Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.", "AI": {"tldr": "本研究使用霍夫斯泰德文化维度和VSM13调查，评估了主流LLM的文化偏向性及其通过提示进行文化适应的能力。结果显示多数模型默认偏向美国文化，但可通过提示向其他文化靠拢，尽管对日本和中国文化的对齐效果不佳。", "motivation": "文化是人类互动的基础，随着生成式大型语言模型（LLMs）的兴起，其文化对齐成为人机交互领域的重要研究方向。", "method": "研究采用VSM13国际调查和霍夫斯泰德文化维度，识别了八种流行LLM（DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, Mistral Large）的文化对齐情况。随后，通过“文化提示”（系统提示）测试了这些模型向中国、法国、印度、伊朗、日本和美国等目标文化转变的适应性。", "result": "研究发现，在未指定文化时，大多数LLM偏向美国文化。通过文化提示，八个模型中有七个能够更接近预期的目标文化。然而，模型在与日本和中国文化对齐时遇到了困难，即使其中两个模型源自中国公司DeepSeek。", "conclusion": "LLM在默认情况下普遍偏向美国文化，但通过文化提示可以使其向其他文化转变。不过，模型在对齐日本和中国文化方面仍存在挑战，这表明在文化适应性方面仍有改进空间。"}}
{"id": "2512.12194", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12194", "abs": "https://arxiv.org/abs/2512.12194", "authors": ["Min-Won Seo", "Aamodh Suresh", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping", "comment": "18 pages, 17 figures", "summary": "Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.", "AI": {"tldr": "B-ActiveSEAL是一个可扩展的信息论主动探索框架，通过明确考虑定位与建图的耦合不确定性，实现了自适应的探索-利用平衡，解决了大规模环境中长期操作的计算难题。", "motivation": "主动机器人探索中的决策需要整合定位与建图，并在紧密耦合的不确定性下进行。然而，在大型环境中长期操作时，管理这些相互依赖的不确定性很快变得计算上难以处理。", "method": "本文提出了B-ActiveSEAL框架，它是一个可扩展的信息论主动探索框架，明确将感知到建图的耦合不确定性纳入决策过程。该框架(i)自适应地平衡地图不确定性（探索）和定位不确定性（利用），(ii)适应广泛的广义熵度量，实现灵活且不确定性感知的主动探索，(iii)确立行为熵（BE）作为一种有效的信息度量，通过在耦合不确定性下实现直观和自适应的决策。此外，还建立了传播耦合不确定性并将其整合到通用熵公式中的理论基础。", "result": "通过严格的理论分析和在开源地图及ROS-Unity模拟器上的广泛实验验证，B-ActiveSEAL实现了良好的探索-利用权衡，并在不同复杂环境中产生了多样化、自适应的探索行为，相比代表性基线显示出明显优势。", "conclusion": "B-ActiveSEAL是一种有效的、不确定性感知的主动探索方法，能够处理紧密耦合的定位-建图不确定性，并在大规模环境中实现可扩展的长期操作。"}}
{"id": "2512.12058", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12058", "abs": "https://arxiv.org/abs/2512.12058", "authors": ["Anja Sheppard", "Chris Reale", "Katherine A. Skinner"], "title": "A Stochastic Approach to Terrain Maps for Safe Lunar Landing", "comment": "Accepted to IEEE Aerospace 2026", "summary": "Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.\n  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.\n  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.", "AI": {"tldr": "本文提出了一种利用高斯过程（GP）从月球勘测轨道飞行器（LRO）数据生成随机高程图的方法，通过两阶段GP模型整合数字高程模型（DEM）置信度图，以更准确地估计月球地形的不确定性，从而支持月球南极等高风险区域的安全着陆。", "motivation": "在月球南极等阴影区域安全着陆极具挑战性，传统视觉方法不可靠，而激光雷达（LiDAR）在月球环境未经充分测试。利用LRO数据创建先验地图是必要的，但现有随机高程映射方法未充分利用DEM置信度图中的关键信息，导致地形不确定性估计不足，这在自主太空飞行等高风险环境中是致命缺陷。", "method": "研究引入了一种两阶段高斯过程（GP）模型。第一阶段GP从DEM置信度数据中学习空间变化的噪声特性（异方差性）。这些异方差信息随后被用于指导主GP的噪声参数，该主GP负责建模月球地形。此外，为实现可扩展训练，研究采用了随机变分高斯过程（stochastic variational GPs）。", "result": "通过利用GP，该方法能够更准确地建模异方差传感器噪声对最终高程图的影响。因此，该方法生成了更具信息量的地形不确定性估计。", "conclusion": "该方法产生的信息更丰富的地形不确定性估计，对于后续任务（如危险检测和安全着陆点选择）至关重要，特别是在月球南极等复杂且高风险的着陆环境中，显著提升了任务安全性。"}}
{"id": "2512.12601", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12601", "abs": "https://arxiv.org/abs/2512.12601", "authors": ["Si Wu", "Zhengyan Qin", "Tengfei Liu", "Zhong-Ping Jiang"], "title": "Quadratic-Programming-based Control of Multi-Robot Systems for Cooperative Object Transport", "comment": null, "summary": "This paper investigates the control problem of steering a group of spherical mobile robots to cooperatively transport a spherical object. By controlling the movements of the robots to exert appropriate contact (pushing) forces, it is desired that the object follows a velocity command. To solve the problem, we first treat the robots' positions as virtual control inputs of the object, and propose a velocity-tracking controller based on quadratic programming (QP), enabling the robots to cooperatively generate desired contact forces while minimizing the sum of the contact-force magnitudes. Then, we design position-tracking controllers for the robots. By appropriately designing the objective function and the constraints for the QP, it is guaranteed that the QP admits a unique solution and the QP-based velocity-tracking controller is Lipschitz continuous. Finally, we consider the closed-loop system as an interconnection of two subsystems, corresponding to the velocity-tracking error of the object and the position-tracking error of the robots, and employ nonlinear small-gain techniques for stability analysis. The effectiveness of the proposed design is demonstrated through numerical simulations.", "AI": {"tldr": "本文研究了多球形移动机器人合作运输球形物体的控制问题，通过二次规划（QP）设计了速度跟踪控制器和位置跟踪控制器，并利用非线性小增益技术进行了稳定性分析。", "motivation": "研究的动机是实现一组球形移动机器人合作运输一个球形物体，使其按照给定的速度指令移动，同时控制机器人施加适当的接触（推动）力。", "method": "首先，将机器人的位置视为物体的虚拟控制输入，并提出了一种基于二次规划（QP）的速度跟踪控制器，使机器人能够合作产生期望的接触力，同时最小化接触力幅度的总和。其次，为机器人设计了位置跟踪控制器。通过适当设计QP的目标函数和约束，确保QP具有唯一解且基于QP的速度跟踪控制器是Lipschitz连续的。最后，将闭环系统视为两个子系统（物体速度跟踪误差和机器人位置跟踪误差）的互连，并采用非线性小增益技术进行稳定性分析。", "result": "通过数值模拟证明了所提出设计的有效性。研究保证了QP具有唯一解，且基于QP的速度跟踪控制器是Lipschitz连续的。", "conclusion": "本文成功设计了一种多机器人合作运输球形物体的控制策略，该策略能够使物体有效跟踪速度指令，同时确保接触力的合理分配和系统的稳定性。"}}
{"id": "2512.12755", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12755", "abs": "https://arxiv.org/abs/2512.12755", "authors": ["Tingwei Cao", "Yan Xu"], "title": "An End-to-End Approach for Microgrid Probabilistic Forecasting and Robust Operation via Decision-focused Learning", "comment": "10 pages", "summary": "High penetration of renewable energy sources (RES) introduces significant uncertainty and intermittency into microgrid operations, posing challenges to economic and reliable scheduling. To address this, this paper proposes an end-to-end decision-focused framework that jointly optimizes probabilistic forecasting and robust operation for microgrids. A multilayer encoder-decoder (MED) probabilistic forecasting model is integrated with a two-stage robust optimization (TSRO) model involving direct load control (DLC) through a differentiable decision pathway, enabling gradient-based feedback from operational outcomes to improve forecasting performance. Unlike conventional sequential approaches, the proposed method aligns forecasting accuracy with operational objectives by directly minimizing decision regret via a surrogate smart predict-then-optimize (SPO) loss function. This integration ensures that probabilistic forecasts are optimized for downstream decisions, enhancing both economic efficiency and robustness. Case studies on modified IEEE 33-bus and 69-bus systems demonstrate that the proposed framework achieves superior forecasting accuracy and operational performance, reducing total and net operation costs by up to 18% compared with conventional forecasting and optimization combinations. The results verify the effectiveness and scalability of the end-to-end decision-focused approach for resilient and cost-efficient microgrid management under uncertainty.", "AI": {"tldr": "本文提出一个端到端决策驱动框架，将概率预测与鲁棒优化相结合，以解决可再生能源高渗透下微电网调度中的不确定性和间歇性问题，从而提高经济性和可靠性。", "motivation": "可再生能源高渗透给微电网运行带来显著的不确定性和间歇性，对经济可靠调度构成挑战。", "method": "提出一个端到端决策驱动框架，将多层编码器-解码器（MED）概率预测模型与包含直接负荷控制（DLC）的两阶段鲁棒优化（TSRO）模型通过可微分决策路径集成。通过代理智能预测-然后-优化（SPO）损失函数直接最小化决策遗憾，实现梯度反馈，使预测准确性与运行目标对齐。", "result": "该框架在预测准确性和运行性能上表现优越，与传统预测优化组合相比，总运行成本和净运行成本降低高达18%。在修改后的IEEE 33节点和69节点系统上验证了其有效性。", "conclusion": "所提出的端到端决策驱动方法对于不确定性下的弹性且经济高效的微电网管理具有有效性和可扩展性。"}}
{"id": "2512.12177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12177", "abs": "https://arxiv.org/abs/2512.12177", "authors": ["Aydin Ayanzadeh", "Tim Oates"], "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation", "comment": "Accepted for publication in the proceedings of the IEEE International Conference on Big Data (IEEE BigData 2025)", "summary": "Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.", "AI": {"tldr": "本文提出一种利用基础模型（LLM）将平面图转换为可导航知识图谱并生成导航指令的新方法，以帮助视障人士进行室内导航，并通过图谱表示和上下文学习显著提高导航精度。", "motivation": "室内导航对视障人士来说仍是巨大挑战，现有解决方案主要依赖基础设施，在动态环境中导航能力受限。", "method": "提出Floorplan2Guide方法，利用基础模型（LLM）从建筑布局中提取空间信息，将平面图转换为可导航的知识图谱，并生成人类可读的导航指令，减少了传统平面图解析所需的预处理工作。", "result": "实验结果表明，少样本学习比零样本学习能提高导航准确性。在5样本提示下，Claude 3.7 Sonnet在短、中、长路线上的准确率分别达到92.31%、76.92%和61.54%。基于图谱的空间结构比直接视觉推理的成功率高出15.4%。", "conclusion": "研究证实，图形表示和上下文学习能显著提高导航性能，使该解决方案对视障用户室内导航更加精确有效。"}}
{"id": "2512.12203", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12203", "abs": "https://arxiv.org/abs/2512.12203", "authors": ["Eric J. Elias", "Michael Esswein", "Jonathan P. How", "David W. Miller"], "title": "Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion", "comment": "18 pages, 11 figures. To be published in proceedings of AIAA SCITECH 2026 Forum", "summary": "As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.", "AI": {"tldr": "该研究提出了一种将可见光和热红外图像融合的方法，以提高在轨操作中对未知空间物体（RSO）进行导航的同步定位与建图（SLAM）算法的性能，特别是在恶劣光照条件下。", "motivation": "随着在轨操作的普及，围绕未知RSO的精确导航需求增加。传统相机在日食或阴影期效果不佳，激光雷达虽鲁棒但体积大、重且耗能。热红外相机虽能克服光照限制，但分辨率和特征丰富性不足。因此，需要一种方法结合两者的优点。", "method": "研究通过逼真模拟低地球轨道目标卫星的可见光和热红外图像，并采用像素级融合方法创建可见光/热红外复合图像，以结合两种相机的优势。随后，在不同光照和轨迹条件下，比较了单目SLAM算法在仅使用可见光、仅使用热红外和使用融合图像时的导航误差。", "result": "融合图像在导航性能方面显著优于仅使用可见光和仅使用热红外的方法。", "conclusion": "可见光和热红外图像的融合能够有效提升在复杂光照条件下对未知RSO的导航性能，解决了传统视觉和热红外传感器的局限性。"}}
{"id": "2512.11939", "categories": ["cs.CV", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11939", "abs": "https://arxiv.org/abs/2512.11939", "authors": ["Clément Fernandes", "Wojciech Pieczynski"], "title": "Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains", "comment": null, "summary": "Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.", "AI": {"tldr": "本文提出了一种新的HEMC-CPS模型，结合了上下文Peano扫描和证据隐马尔可夫链，用于无监督贝叶斯图像分割，并在合成和真实图像上展示了其有效性。", "motivation": "传统的Peano扫描结合隐马尔可夫链（HMC）在无监督图像分割中速度快，但可以进一步改进。上下文Peano扫描（CPS）和证据隐马尔可夫链（HEMC）已分别显示出改进HMC-based贝叶斯分割的潜力。本文旨在通过同时考虑两者来构建一个更强大的模型，以处理更复杂的图像。", "method": "该研究引入了一个新的HEMC-CPS模型，通过结合上下文Peano扫描（CPS）和证据隐马尔可夫链（HEMC）。分割通过贝叶斯最大后验模式（MPM）进行，参数使用随机期望最大化（SEM）方法进行无监督估计。模型在合成图像和真实图像上进行了验证。", "result": "新的HEMC-CPS模型在贝叶斯最大后验模式（MPM）分割中表现出有效性。它展示了对更复杂图像（如三维或多传感器多分辨率图像）进行建模和分割的潜力。此外，HMC-CPS和HEMC-CPS模型不仅限于图像分割，还可用于任何类型的空间相关数据。", "conclusion": "HEMC-CPS模型显著改进了无监督贝叶斯图像分割，并通过结合上下文Peano扫描和证据隐马尔可夫链提供了强大的性能。该模型具有广泛的应用前景，不仅限于图像分割，还可用于处理其他空间相关数据。"}}
{"id": "2512.12182", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12182", "abs": "https://arxiv.org/abs/2512.12182", "authors": ["Xinyu Gao"], "title": "TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion", "comment": null, "summary": "Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.", "AI": {"tldr": "本文提出了一种基于生成表示的少样本知识图谱补全框架，结合两阶段注意力三元组增强器和基于U-KAN的扩散模型，在两个公开数据集上取得了最先进的成果。", "motivation": "知识图谱在智能问答和推荐系统等领域广泛应用，但真实世界数据的异构性和多面性导致关系分布呈长尾状，使得在有限样本下补全缺失事实至关重要。现有研究（基于度量匹配或元学习）未能充分利用图中的邻域信息或忽略对比信号的分布特征。", "method": "本文从生成表示的角度重新审视问题，提出一个少样本知识图谱补全框架。该框架集成了两阶段注意力三元组增强器（two-stage attention triple enhancer）和基于U-KAN的扩散模型（U-KAN based diffusion model）。", "result": "在两个公开数据集上进行的广泛实验表明，本文方法取得了新的最先进（state-of-the-art）结果。", "conclusion": "所提出的基于生成表示的少样本知识图谱补全框架，通过结合两阶段注意力三元组增强器和U-KAN扩散模型，有效解决了长尾关系分布下的知识图谱补全问题，并显著优于现有方法。"}}
{"id": "2512.12175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12175", "abs": "https://arxiv.org/abs/2512.12175", "authors": ["Haoyang Chen", "Richong Zhang", "Junfan Chen"], "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective", "comment": null, "summary": "Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.", "AI": {"tldr": "本文提出一种新的上下文学习（ICL）演示选择方法，通过引入转导标签传播框架和合成数据，解决了现有方法在标签一致性方面的不足，从而提高了ICL的性能。", "motivation": "当前ICL演示选择方法通常基于语义相似性选择Top-K示例，但未能保证所选示例的标签一致性。作者认为标签一致性对于ICL至关重要，并从贝叶斯和转导标签传播角度重新审视ICL的工作机制。", "method": "将ICL视为一种转导学习方法，并结合贝叶斯观点中的潜在概念，推导出相似演示在标签一致的情况下能更好地指导查询概念。在此基础上，建立了标签传播框架，将标签一致性与传播误差边界联系起来。提出了一种数据合成方法（TopK-SD），该方法同时利用语义和标签信息，以获取具有一致标签的演示。", "result": "所提出的TopK-SD方法在多个基准测试中均优于传统的TopK采样方法。", "conclusion": "这项工作为理解ICL内部工作机制提供了新的视角，并提出了一种有效提高ICL性能的演示选择策略。"}}
{"id": "2512.11928", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11928", "abs": "https://arxiv.org/abs/2512.11928", "authors": ["Alexander Peysakhovich", "William Berman", "Joseph Rufo", "Felix Wong", "Maxwell Z. Wilson"], "title": "MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion", "comment": null, "summary": "Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.", "AI": {"tldr": "该研究训练了一个扩散模型MONET，能从明场图像预测细胞涂色通道，克服了传统细胞涂色劳动密集和无法观察细胞动态的局限，并支持生成延时视频和部分域外泛化。", "motivation": "传统的细胞涂色技术存在两大问题：一是劳动密集型，耗时耗力；二是需要化学固定，无法用于研究细胞动态过程。这限制了其在生物研究中的应用。", "method": "研究人员训练了一个名为MONET的扩散模型，利用大量数据集从明场图像预测细胞涂色通道。该模型采用了一致性架构，使其能够在无法获得细胞涂色视频训练数据的情况下生成延时视频。此外，该架构还实现了上下文学习，使模型能够部分泛化到分布外的细胞系和成像协议。", "result": "模型质量随规模的增加而提高。MONET模型能够生成高质量的细胞涂色延时视频，填补了传统方法无法实现动态观察的空白。此外，模型通过上下文学习，展现了对未知细胞系和成像协议的部分迁移能力。", "conclusion": "虚拟细胞涂色技术并非旨在完全取代物理细胞涂色，而是作为一种补充工具，赋能生物研究中新的工作流程，从而扩展细胞形态学研究的可能性。"}}
{"id": "2512.12544", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12544", "abs": "https://arxiv.org/abs/2512.12544", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Wanhao Yu", "Zhankai Ye", "Dawei Xiang", "Ting Hua", "Xin Liu", "Shangqian Gao", "Tingting Yu"], "title": "HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks", "comment": null, "summary": "Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.", "AI": {"tldr": "HyperEdit 提出了一种新的方法，通过超网络动态适应用户指令和差异感知正则化，解决了大型语言模型在指令式文本编辑中忠实性不足和过度编辑的问题，显著提升了编辑性能。", "motivation": "指令式文本编辑对于代码编辑器等实际应用至关重要，但大型语言模型在此任务上表现不佳。它们难以忠实地实现用户指令并保留未修改内容，甚至微小的意外修改都可能破坏功能。现有方法将编辑视为通用文本生成，导致编辑与用户意图不符以及过度修改未更改区域。", "method": "HyperEdit 提出了两种方法：1) 基于超网络的动态适应，生成请求特定参数，使模型能够根据每条指令调整编辑策略。2) 差异感知正则化，将监督重点放在修改过的区域上，防止过度编辑，同时确保精确、最小的更改。", "result": "HyperEdit 在修改区域上的 BLEU 值比最先进的基线模型相对提升了 9%—30%，尽管它只使用了 3B 参数。", "conclusion": "HyperEdit 通过动态适应用户指令和精确控制编辑，有效解决了大型语言模型在指令式文本编辑中的关键问题，以更小的模型规模实现了显著优于现有方法的性能改进。"}}
{"id": "2512.12638", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12638", "abs": "https://arxiv.org/abs/2512.12638", "authors": ["Rishit Agnihotri", "Amit Chaurasia"], "title": "Electric Road Systems for Smart Cities: A Scalable Infrastructure Framework for Dynamic Wireless Charging", "comment": "Preprint. Under review for conference submission. Simulation-based study", "summary": "The transition to electric transportation is a key enabler for intelligent and sustainable cities; however, inadequate charging infrastructure remains a major barrier to large-scale electric vehicle (EV) adoption. This paper presents a scalable Electric Road System (ERS) architecture that enables Dynamic Wireless Charging (DWC) of EVs during motion. The proposed framework integrates inductive charging coils embedded in road pavement, real-time vehicle-to-infrastructure (V2I) communication, and adaptive energy management coordinated with smart grid systems. Modular road segments with a standardized charging process are employed to ensure scalability across urban corridors and interoperability among different EV platforms. System performance is evaluated using a co-simulation framework combining MATLAB-based power analysis with traffic inputs generated in SUMO. Key performance metrics include charging efficiency, energy cost per kilometer, and battery lifecycle improvement. Simulation results indicate a potential reduction in range anxiety and an increase in battery lifespan due to frequent shallow charging cycles. The study further discusses deployment challenges, policy considerations, and energy distribution strategies aligned with climate-resilient urban development. A case study of a tier-1 Indian city is presented to analyze the cost-benefit trade-offs of retrofitting high-density urban corridors with ERS. The proposed framework provides a practical foundation for next-generation EV infrastructure planning in smart cities.", "AI": {"tldr": "本文提出了一种可扩展的电动道路系统（ERS）架构，通过动态无线充电（DWC）解决电动汽车（EV）充电基础设施不足的问题，从而促进智能城市中的EV普及。", "motivation": "电动交通是实现智能和可持续城市的关键，但充电基础设施不足严重阻碍了电动汽车的大规模普及。", "method": "该研究提出了一种可扩展的ERS架构，集成路面嵌入式感应充电线圈、实时车-基础设施（V2I）通信以及与智能电网协调的自适应能源管理。系统采用标准化充电过程的模块化路段。性能通过结合MATLAB电力分析和SUMO交通输入的协同仿真框架进行评估。此外，还分析了部署挑战、政策考量和能源分配策略，并以一个印度一线城市的案例研究分析了高密度城市走廊改造ERS的成本效益权衡。", "result": "仿真结果表明，该系统能有效减少续航焦虑，并通过频繁的浅层充电循环延长电池寿命。研究还评估了充电效率、每公里能源成本和电池寿命改善等关键性能指标。", "conclusion": "所提出的框架为智能城市中下一代电动汽车基础设施规划提供了实用基础，并探讨了与气候适应型城市发展相关的部署挑战、政策考量和能源分配策略。"}}
{"id": "2512.12537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12537", "abs": "https://arxiv.org/abs/2512.12537", "authors": ["Agniva Maiti", "Manya Pandey", "Murari Mandal"], "title": "NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data", "comment": null, "summary": "The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.", "AI": {"tldr": "本文介绍了NagaNLP，一个针对那加语（Nagamese）的开源工具包，通过LLM驱动但经人工验证的合成数据生成方法，为这一资源匮乏的语言构建了数据集和模型，并在NLP任务上取得了显著的基准性能提升。", "motivation": "世界上绝大多数语言，特别是像那加语这样的克里奥尔语，在自然语言处理（NLP）领域严重缺乏资源，这严重阻碍了它们在数字技术中的代表性。", "method": "研究人员开发了一个多阶段管道，利用专家指导的LLM（Gemini）生成候选语料库，然后由母语使用者进行精炼和标注。这种“合成-混合”方法生成了1万对对话数据集和高质量的标注语料库。随后，他们训练了判别式模型（微调的XLM-RoBERTa-base）和生成式模型（微调的Llama-3.2-3B Instruct，命名为NagaLLaMA）来评估其方法的有效性。", "result": "微调后的XLM-RoBERTa-base模型在那加语词性标注上达到了93.81%的准确率（0.90 F1-Macro），在命名实体识别上达到了0.75 F1-Macro，大大优于零样本基线。微调后的NagaLLaMA模型在对话任务中表现出色，困惑度为3.85，比其少样本对应模型（96.76）有了数量级的改进。", "conclusion": "NagaNLP工具包（包括所有数据集、模型和代码）的发布，为那加语这一此前服务不足的语言提供了基础资源，并为在其他低资源语境中减少数据稀缺性提供了一个可复现的框架。"}}
{"id": "2512.12761", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12761", "abs": "https://arxiv.org/abs/2512.12761", "authors": ["Zhiquan Zhang", "Omar Muhammetkulyyev", "Tichakorn Wongpiromsarn", "Melkior Ornik"], "title": "Lexicographic Multi-Objective Stochastic Shortest Path with Mixed Max-Sum Costs", "comment": null, "summary": "We study the Stochastic Shortest Path (SSP) problem for autonomous systems with mixed max-sum cost aggregations under Linear Temporal Logic constraints. Classical SSP formulations rely on sum-aggregated costs, which are suitable for cumulative quantities such as time or energy but fail to capture bottleneck-style objectives such as avoiding high-risk transitions, where performance is determined by the worst single event along a trajectory. Such objectives are particularly important in safety-critical systems, where even one hazardous transition can be unacceptable. To address this limitation, we introduce max-aggregated objectives that minimize the bottleneck cost, i.e., the maximum one-step cost along a trajectory. We show that standard Bellman equations on the original state space do not apply in this setting and propose an augmented MDP with a state variable tracking the running maximum cost, together with a value iteration algorithm. We further identify a cyclic policy phenomenon, where zero-marginal-cost cycles prevent goal reaching under max-aggregation, and resolve it via a finite-horizon formulation. To handle richer task requirements, linear temporal logic specifications are translated into deterministic finite automata and combined with the system to construct a product MDP. We propose a lexicographic value iteration algorithm that handles mixed max-sum objectives under lexicographic ordering on this product MDP. Gridworld case studies demonstrate the effectiveness of the proposed framework.", "AI": {"tldr": "本文研究了在线性时间逻辑（LTL）约束下，具有混合最大-和成本聚合的自主系统的随机最短路径（SSP）问题，特别关注了瓶颈式目标。", "motivation": "传统的SSP公式依赖于和聚合成本，适用于累积量，但无法捕捉瓶颈式目标（如避免高风险转换），这在安全关键系统中至关重要，因为即使一次危险事件也可能无法接受。", "method": "引入最大聚合目标以最小化瓶颈成本；提出一个带有跟踪运行最大成本状态变量的增强型MDP和相应的价值迭代算法；通过有限 horizon 公式解决了循环策略现象；将LTL规范转换为DFA并与系统结合构建乘积MDP；提出了一种处理乘积MDP上混合最大-和目标的词典序价值迭代算法。", "result": "通过Gridworld案例研究证明了所提出框架的有效性。", "conclusion": "该研究提供了一个在LTL约束下处理自主系统SSP问题的新框架，能够有效处理混合最大-和成本聚合，特别适用于瓶颈式安全目标。"}}
{"id": "2512.12576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12576", "abs": "https://arxiv.org/abs/2512.12576", "authors": ["Xueru Wen", "Jie Lou", "Yanjiang Liu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun", "Yaojie Lu", "Debing Zhang"], "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning", "comment": null, "summary": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.", "AI": {"tldr": "本文提出CoVRl，一种结合变分推断和强化学习的新方法，通过耦合先验和后验分布来提高语言模型在无验证器环境下的推理能力，实现高效探索和强烈的思维-答案一致性。", "motivation": "现有的无验证器强化学习方法通过利用LLM生成参考答案的固有概率作为奖励信号，但它们通常仅根据问题采样推理轨迹，导致推理轨迹采样与答案信息脱钩，从而造成探索效率低下以及轨迹与最终答案之间缺乏一致性。", "method": "CoVRL通过混合采样策略耦合先验和后验分布，将变分推断与强化学习相结合。它构建并优化了一个整合这两个分布的复合分布，从而在实现高效探索的同时保持了强大的思维-答案一致性。", "result": "在数学和通用推理基准上的大量实验表明，CoVRL将性能比基础模型提高了12.4%，并比强大的最新无验证器强化学习基线额外提高了2.3%。", "conclusion": "CoVRL为增强语言模型的通用推理能力提供了一个原则性的框架，显著提升了模型性能，并在无验证器RL方法中取得了领先地位。"}}
{"id": "2512.12211", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12211", "abs": "https://arxiv.org/abs/2512.12211", "authors": ["Longchao Da", "David Isele", "Hua Wei", "Manish Saroya"], "title": "Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving", "comment": "9 Pages, 8 Figures", "summary": "Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.", "AI": {"tldr": "该论文提出了一种新的自适应评估流程，用于自动驾驶中的轨迹预测，根据驾驶场景的临界性动态结合准确性和多样性维度，以更合理地反映预测器对自动驾驶车辆性能的影响。", "motivation": "现有的轨迹预测评估方法主要依赖基于误差的指标（如ADE、FDE），这些指标仅从事后角度反映准确性，却忽略了预测器对自动驾驶车辆（SDV）的实际影响，尤其是在复杂交互场景中，高质量的预测器不仅要准确，还应捕捉邻近代理所有可能的运动方向，以支持SDV的谨慎决策。现有指标难以满足这一标准。", "method": "本文提出了一种全面的评估流程，通过准确性和多样性这两个维度自适应地评估预测器的性能。根据驾驶场景的临界性，这两个维度被动态结合，从而得出预测器性能的最终得分。", "result": "在基于真实世界数据集的闭环基准上进行的广泛实验表明，与传统指标相比，该流程产生了更合理的评估结果，能更好地反映预测器评估与自动驾驶车辆驾驶性能之间的相关性。", "conclusion": "该评估流程提供了一种可靠的方法来选择最有可能提升自动驾驶车辆驾驶性能的预测器。"}}
{"id": "2512.12228", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12228", "abs": "https://arxiv.org/abs/2512.12228", "authors": ["Huichang Yun", "Seungho Yoo"], "title": "Semantic Zone based 3D Map Management for Mobile Robot", "comment": "12 pages, 11 figures", "summary": "Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment", "AI": {"tldr": "本文提出一种基于语义区域的3D地图管理方法，通过将环境划分为有意义的语义区域，并根据任务动态加载/卸载这些区域，以有效降低移动机器人在大型室内环境中的内存消耗，实现稳定的内存使用。", "motivation": "大型室内环境中的移动机器人需要精确的3D空间表示，但3D地图占用大量内存，现有SLAM框架的内存管理方法（基于几何距离或时间度量）在空间分区环境中效率低下，导致难以在有限计算资源下维护完整的地图数据。", "method": "该方法将环境划分为有意义的语义单元（如大厅、走廊），并将这些区域作为内存管理的基本单位。系统根据任务动态地将相关区域加载到工作内存（WM），并将非活动区域卸载到长期内存（LTM），从而严格执行用户定义的内存阈值。该方法在RTAB-Map框架中实现。", "result": "与标准方法相比，该方法显著减少了不必要的特征点加载/卸载循环和累计内存使用量。实验结果证实，基于语义区域的管理确保了稳定、可预测的内存使用，同时保持了地图的导航可用性。", "conclusion": "基于语义区域的3D地图管理方法将控制范式从以几何为中心转向以语义为中心，有效解决了大型室内环境中移动机器人3D地图的内存管理问题，提高了内存效率和地图可用性。"}}
{"id": "2512.12225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12225", "abs": "https://arxiv.org/abs/2512.12225", "authors": ["Laha Ale"], "title": "A Geometric Theory of Cognition", "comment": null, "summary": "Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.", "AI": {"tldr": "本文提出一个统一的几何框架，将人类认知过程（包括双过程效应）解释为在黎曼流形上认知势的黎曼梯度流，从而提供了一个普遍的动力学定律。", "motivation": "人类认知涵盖感知、记忆、判断、推理等多种能力，但这些能力通常由不同的计算理论解释。研究旨在建立一个统一的数学框架来解释这些多样的认知过程。", "method": "将认知状态表示为微分流形上的一个点，该流形具有学习到的黎曼度量，编码表征约束、计算成本和认知变量间的结构关系。一个标量认知势结合了预测准确性、结构简洁性、任务效用和规范/逻辑要求。认知被描述为该势的黎曼梯度流，形成一个普遍的动力学定律。", "result": "从该框架中自然涌现出广泛的心理现象，包括经典的双过程效应（快速直觉反应和慢速深思熟虑），这源于度量引起的各向异性产生的内在时间尺度分离和几何相变，无需模块化或混合架构。推导了这些机制的分析条件，并通过典型认知任务的模拟展示了其行为特征。", "conclusion": "这些结果为认知奠定了几何基础，并为开发更通用、更类人的通用人工智能系统提供了指导原则。"}}
{"id": "2512.11941", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11941", "abs": "https://arxiv.org/abs/2512.11941", "authors": ["Jingmin Zhu", "Anqi Zhu", "James Bailey", "Jun Liu", "Hossein Rahmani", "Mohammed Bennamoun", "Farid Boussaid", "Qiuhong Ke"], "title": "DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition", "comment": null, "summary": "Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \\textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS", "AI": {"tldr": "DynaPURLS是一个统一框架，通过建立鲁棒的多尺度视觉-语义对应关系，并在推理时动态优化，显著提升了零样本骨骼行为识别（ZS-SAR）的泛化能力。", "motivation": "现有的零样本骨骼行为识别方法受限于将骨骼特征与静态、类别级语义对齐，这种粗粒度对齐无法弥合已知类与未知类之间的领域鸿沟，阻碍了细粒度视觉知识的有效迁移。", "method": "本文提出了DynaPURLS框架。它利用大型语言模型生成包含全局运动和局部身体部位动态的层次化文本描述。同时，一个自适应分区模块通过语义分组骨骼关节来生成细粒度视觉表示。为了强化这种细粒度对齐以对抗训练-测试领域漂移，DynaPURLS包含一个动态细化模块，该模块在推理时通过轻量级可学习投影将文本特征适应于传入的视觉流。此细化过程通过置信度感知、类别平衡的记忆库进行稳定，以减轻噪声伪标签的错误传播。", "result": "在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上的广泛实验表明，DynaPURLS显著优于现有技术，创造了新的最先进记录。", "conclusion": "DynaPURLS通过建立多尺度视觉-语义对应并进行动态细化，成功解决了零样本骨骼行为识别中细粒度知识迁移和领域漂移的限制，展现出卓越的泛化性能。"}}
{"id": "2512.12776", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12776", "abs": "https://arxiv.org/abs/2512.12776", "authors": ["Haochong Chen", "Xincheng Cao", "Levent Guvenc", "Bilin Aksun-Guvenc"], "title": "High Order Control Lyapunov Function - Control Barrier Function - Quadratic Programming Based Autonomous Driving Controller for Bicyclist Safety", "comment": null, "summary": "Ensuring the safety of Vulnerable Road Users (VRUs) is a critical challenge in the development of advanced autonomous driving systems in smart cities. Among vulnerable road users, bicyclists present unique characteristics that make their safety both critical and also manageable. Vehicles often travel at significantly higher relative speeds when interacting with bicyclists as compared to their interactions with pedestrians which makes collision avoidance system design for bicyclist safety more challenging. Yet, bicyclist movements are generally more predictable and governed by clear traffic rules as compared to the sudden and sometimes erratic pedestrian motion, offering opportunities for model-based control strategies. To address bicyclist safety in complex traffic environments, this study proposes and develops a High Order Control Lyapunov Function High Order Control Barrier Function Quadratic Programming (HOCLF HOCBF QP) control framework. Through this framework, CLFs constraints guarantee system stability so that the vehicle can track its reference trajectory, whereas CBFs constraints ensure system safety by letting vehicle avoiding potential collisions region with surrounding obstacles. Then by solving a QP problem, an optimal control command that simultaneously satisfies stability and safety requirements can be calculated. Three key bicyclist crash scenarios recorded in the Fatality Analysis Reporting System (FARS) are recreated and used to comprehensively evaluate the proposed autonomous driving bicyclist safety control strategy in a simulation study. Simulation results demonstrate that the HOCLF HOCBF QP controller can help the vehicle perform robust, and collision-free maneuvers, highlighting its potential for improving bicyclist safety in complex traffic environments.", "AI": {"tldr": "本研究提出了一种高阶控制李雅普诺夫函数和高阶控制障碍函数二次规划（HOCLF HOCBF QP）控制框架，以提高自动驾驶系统在复杂交通环境中对骑行者的安全性。", "motivation": "在智能城市中，确保弱势道路使用者（VRUs）的安全是自动驾驶系统开发的关键挑战。骑行者与车辆的相对速度通常较高，使碰撞避免设计更具挑战性。然而，骑行者的运动通常比行人更可预测，并遵循交通规则，这为基于模型的控制策略提供了机会。", "method": "研究开发了HOCLF HOCBF QP控制框架。其中，CLF约束确保系统稳定性，使车辆能跟踪参考轨迹；CBF约束通过避免与障碍物的潜在碰撞区域来确保系统安全。通过求解二次规划（QP）问题，计算出同时满足稳定性和安全要求的最佳控制指令。该策略在模拟中，使用美国致命事故分析报告系统（FARS）中记录的三个关键骑行者碰撞场景进行了综合评估。", "result": "仿真结果表明，HOCLF HOCBF QP控制器能够帮助车辆执行鲁棒、无碰撞的机动，突显了其在复杂交通环境中改善骑行者安全的潜力。", "conclusion": "HOCLF HOCBF QP控制框架在复杂交通环境下能够有效提高自动驾驶系统对骑行者的安全性，实现车辆的稳定跟踪和碰撞避免。"}}
{"id": "2512.11977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11977", "abs": "https://arxiv.org/abs/2512.11977", "authors": ["Sushmita Nath"], "title": "A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer", "comment": "submit/7075585. 5 pages with 5 figures", "summary": "Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.", "AI": {"tldr": "本研究发现，在数据受限的条件下，DeiT模型在半导体晶圆缺陷分类中表现优于传统CNN模型（如VGG-19、SqueezeNet、Xception），实现了更高的准确率和F1分数，并能更好地检测少数缺陷类别。", "motivation": "预测性维护在现代工业中至关重要，尤其在敏感的半导体领域。尽管CNN模型在晶圆图像分类中表现良好，但在数据有限和不平衡的情况下，其性能会下降。因此，需要探索在这些挑战性条件下更有效的缺陷检测方法。", "method": "本研究调查了数据高效图像Transformer (DeiT) 模型在数据受限条件下对晶圆图缺陷进行分类的有效性。实验中，DeiT模型与VGG-19、Xception和Squeeze-Net等多种CNN模型进行了性能比较。", "result": "实验结果显示，DeiT模型达到了90.83%的最高分类准确率，显著优于VGG-19 (65%)、SqueezeNet (82%)、Xception (66%) 和 Hybrid (67%) 等CNN模型。DeiT还展示了更高的F1分数 (90.78%)、更快的训练收敛速度，并在检测少数缺陷类别方面表现出更强的鲁棒性。", "conclusion": "研究结果突出了DeiT等基于Transformer的模型在半导体晶圆缺陷检测中的巨大潜力，并支持在半导体制造过程中采用预测性维护策略。"}}
{"id": "2512.12608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12608", "abs": "https://arxiv.org/abs/2512.12608", "authors": ["Hong Su"], "title": "Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery", "comment": null, "summary": "Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.", "AI": {"tldr": "本文提出了一种受人类启发的新学习框架，通过“显式记录”存储因果关系和“最大熵方法发现”捕获多样化策略，以解决大型语言模型在稀有、低资源场景下学习困难的问题。", "motivation": "大型语言模型（LLMs）擅长从大规模语料库中提取常见模式，但在稀有、低资源或未见过的情况下（如小众硬件部署问题、不规则物联网设备行为）表现不佳，因为这些情况在训练数据中代表性不足。此外，LLMs主要依赖隐式参数记忆，限制了其显式获取、回忆和完善方法的能力，使其更像直觉驱动的预测器而非以方法为导向的学习者。", "method": "受人类从稀有经验中学习的启发，本文提出了一个集成两种互补机制的学习框架：1. **显式记录 (Obvious Record)**：将因果关系（或问题-解决方案）作为符号记忆显式存储，即使只遇到一次或不频繁也能实现持久学习。2. **最大熵方法发现 (Maximum-Entropy Method Discovery)**：优先并保留语义差异度高的方法，以捕获多样化和未充分代表的策略，这些策略通常会被下一词预测忽略。", "result": "在包含60对语义多样的问题-解决方案基准测试上，所提出的熵引导方法在处理未见问题时展现出更强的覆盖能力，并比随机基线具有显著更高的内部多样性。", "conclusion": "该研究证实了其方法在发现更具泛化性和人类启发性方法方面的有效性。"}}
{"id": "2512.12613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12613", "abs": "https://arxiv.org/abs/2512.12613", "authors": ["Yucan Guo", "Saiping Guan", "Miao Su", "Zeya Zhao", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning", "comment": null, "summary": "Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.", "AI": {"tldr": "本文提出StruProKGR框架，用于稀疏知识图谱推理，通过距离引导的路径收集和概率路径聚合，实现了高效、有效且可解释的推理。", "motivation": "现实世界中知识图谱常是稀疏的，导致推理困难。现有基于路径的方法依赖计算密集型随机游走，路径质量不一，且未能利用图的结构性，独立处理路径。", "method": "StruProKGR框架采用距离引导的路径收集机制，显著降低计算成本并探索更相关的路径。它通过概率路径聚合融入结构信息，优先考虑相互加强的路径，从而增强推理过程。", "result": "在五个稀疏知识图谱推理基准上的广泛实验表明，StruProKGR在有效性和效率上均超越了现有基于路径的方法。", "conclusion": "StruProKGR为稀疏知识图谱推理提供了一个有效、高效且可解释的解决方案。"}}
{"id": "2512.12803", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12803", "abs": "https://arxiv.org/abs/2512.12803", "authors": ["Dong Liu", "Juan S. Giraldo", "Peter Palensky", "Pedro P. Vergara"], "title": "Distributed Reinforcement Learning using Local Smart Meter Data for Voltage Regulation in Distribution Networks", "comment": null, "summary": "Centralised reinforcement learning (RL) for voltage magnitude regulation in distribution networks typically involves numerous agent-environment interactions and power flow (PF) calculations, inducing computational overhead and privacy concerns over shared data. Thus, we propose a distributed RL algorithm to regulate voltage magnitude. First, a dynamic Thevenin equivalent model is integrated within smart meters (SM), enabling local voltage magnitude estimation using local SM data for RL agent training, and mitigating the dependency of synchronised data collection and centralised PF calculations. To mitigate estimation errors induced by Thevenin model inaccuracies, a voltage magnitude correction strategy that combines piecewise functions and neural networks is introduced. The piecewise function corrects the large errors of estimated voltage magnitude, while a neural network mimics the grid's sensitivity to control actions, improving action adjustment precision. Second, a coordination strategy is proposed to refine local RL agent actions online, preventing voltage magnitude violations induced by excessive actions from multiple independently trained agents. Case studies on energy storage systems validate the feasibility and effectiveness of the proposed approach, demonstrating its potential to improve voltage regulation in distribution networks.", "AI": {"tldr": "本文提出一种分布式强化学习算法，通过智能电表集成动态戴维宁等效模型进行本地电压估计，并结合分段函数与神经网络进行误差校正，同时引入在线协调策略，有效解决了集中式强化学习在配电网电压调节中的计算开销和隐私问题。", "motivation": "传统的集中式强化学习（RL）在配电网电压幅值调节中需要大量的智能体-环境交互和潮流计算，导致计算开销大，并引发数据共享的隐私担忧。", "method": "1. 将动态戴维宁等效模型集成到智能电表（SM）中，利用本地SM数据进行本地电压幅值估计和RL智能体训练，减少对同步数据收集和集中式潮流计算的依赖。2. 引入电压幅值校正策略，结合分段函数（校正大误差）和神经网络（模仿电网敏感性，提高动作调整精度），以缓解戴维宁模型不准确性导致的估计误差。3. 提出一种协调策略，在线优化本地RL智能体的动作，防止多个独立训练的智能体因过度动作引起电压幅值越限。", "result": "在储能系统上的案例研究验证了所提方法的可行性和有效性，展示了其在改善配电网电压调节方面的潜力。", "conclusion": "所提出的分布式强化学习方法对于配电网电压幅值调节是可行且有效的，能够解决集中式RL的计算和隐私问题，并提高电压调节性能。"}}
{"id": "2512.12381", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12381", "abs": "https://arxiv.org/abs/2512.12381", "authors": ["Truong Xuan Khanh", "Truong Quynh Hoa"], "title": "Entropy Collapse: A Universal Failure Mode of Intelligent Systems", "comment": "18 pages, 5 figures", "summary": "Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.\n  We identify \\emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.\n  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.\n  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.\n  \\noindent\\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis", "AI": {"tldr": "本文提出“熵塌陷”作为智能系统的一种普遍动态失效模式，当反馈放大速度超过新颖性再生时，系统会从高熵自适应状态急剧转变为低熵塌陷状态，导致适应性维度收缩和刚性化，统一解释了AI模型塌陷、经济制度僵化等现象，并强调了熵感知设计的重要性。", "motivation": "尽管智能系统被普遍认为会通过学习、协调和优化而改进，但在人工智能、经济制度和生物进化等领域，智能的增长却常常导致自相矛盾的退化：系统变得僵化、失去适应性并意外失效。本文旨在解释这种“悖论式退化”的深层机制。", "method": "研究将“熵塌陷”识别为一种普遍的动态失效模式，其发生条件是反馈放大速度超过了有限的新颖性再生。研究在最小的领域无关假设下，将塌陷形式化为向稳定的低熵流形收敛，而非零熵状态，意味着有效适应性维度的收缩。通过分析方法建立了临界阈值、动态不可逆性和吸引子结构，并通过最小模拟验证了其在不同更新机制下的普遍性。", "result": "智能系统会经历从高熵自适应状态到低熵塌陷状态的急剧转变。塌陷表现为有效适应性维度的收缩，而非活动或规模的丧失。研究分析性地建立了临界阈值、动态不可逆性和吸引子结构。该框架统一了AI中的模型塌陷、经济学中的制度僵化和进化中的基因瓶颈等多样化现象，将其视为同一底层过程的不同表现。", "conclusion": "研究结果将塌陷重新定义为智能的一种结构性成本，解释了后期干预为何系统性失败，并提出了旨在维持智能系统长期适应性的“熵感知设计原则”。"}}
{"id": "2512.12260", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.12260", "abs": "https://arxiv.org/abs/2512.12260", "authors": ["Ege Atacan Doğan", "Peter F. Patel-Schneider"], "title": "A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure", "comment": null, "summary": "Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.", "AI": {"tldr": "本文分析了维基数据（Wikidata）的多层级和多轴设计，认为其与传统本体论的单一分类法不同，为协同和演进的知识图谱提供了可扩展和模块化的本体构建方法。", "motivation": "传统本体设计强调不相交和穷尽的顶层区分以及单一的统一层次结构。而维基数据不强制单一的基础分类法，而是同时容纳多个分类轴。本文旨在分析维基数据这种多层级、多轴设计的结构性影响。", "method": "通过分析维基数据的架构，研究其多层级和多轴设计所带来的结构性影响。", "result": "维基数据的架构支持本体构建的可扩展和模块化方法，特别适合协同和不断演进的知识图谱。", "conclusion": "维基数据的多层级和多轴设计提供了一种可扩展且模块化的本体构建方法，这对于协同和不断演进的知识图谱尤其适用。"}}
{"id": "2512.12233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12233", "abs": "https://arxiv.org/abs/2512.12233", "authors": ["Murad Mehrab Abrar", "Trevor W. Harrison"], "title": "Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements", "comment": "9 pages", "summary": "Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.", "AI": {"tldr": "本文提出了一种针对浮力驱动微型浮标的鲁棒、低成本水下定位方案，通过双向声学飞行时间（ToF）测量和基于几何成本与CRLB的滤波，显著提高了定位精度和鲁棒性。", "motivation": "廉价自主平台需要高频次、精确的水下定位更新，但目前仍面临挑战。", "method": "该方法引入了双向声学飞行时间（ToF）定位框架，结合浮标到浮子和浮子到浮标的传输以增加可用测量次数。它将非线性三边测量法与基于几何成本和Cramer-Rao下界（CRLB）的位置估计滤波相结合，以消除多径效应和其他声学误差引起的异常值，从而提高定位鲁棒性。此外，还展示了用于未回收浮子的时间差到达（TDoA）定位。", "result": "该定位方案在两次现场部署中实现了相对于GPS位置低于4米的定位中值误差。滤波技术将平均误差从139.29米降低到12.07米，并改善了轨迹与GPS路径的对齐。该方法在不依赖大量平滑的情况下提高了定位鲁棒性。", "conclusion": "该研究通过精心设计的算法，提高了基于距离的声学定位技术的定位频率和鲁棒性，从而最大限度地发挥了其在廉价自主平台中的效用。"}}
{"id": "2512.11995", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11995", "abs": "https://arxiv.org/abs/2512.11995", "authors": ["Chenrui Fan", "Yijun Liang", "Shweta Bhardwaj", "Kwesi Cobbina", "Ming Li", "Tianyi Zhou"], "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions", "comment": "28 pages", "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.", "AI": {"tldr": "本文提出了V-REX，一个用于评估视觉语言模型（VLMs）多步探索式视觉推理能力的基准套件，它通过将复杂任务分解为一系列问题（CoQ）来量化分析模型的规划和遵循能力，并揭示了现有模型的不足。", "motivation": "许多视觉语言模型在处理开放式、需要多轮探索和推理的复杂任务时表现不佳，而现有基准主要关注定义明确、目标明确的简单问题。此外，多步探索路径的中间步骤难以评估，这阻碍了对模型“视觉思维”过程的理解和改进。", "method": "研究者开发了一个名为“V-REX”的评估套件，包含需要原生多步探索的挑战性视觉推理任务和评估协议。V-REX将多步探索式推理建模为“问题链（Chain-of-Questions, CoQ）”，并将其分解为两种能力：1) 规划：将开放式任务分解为一系列探索性问题；2) 遵循：按顺序回答预设的CoQ以收集信息并得出最终答案。通过为每一步设置有限的问题和答案选项，V-REX实现了对中间步骤的可靠定量和细粒度分析。", "result": "通过评估最先进的专有和开源视觉语言模型，研究揭示了模型性能的一致扩展趋势，规划和遵循能力之间存在显著差异，并且在多步探索式推理方面仍有很大的改进空间。", "conclusion": "V-REX成功弥合了复杂开放式视觉推理任务评估的空白，提供了一种量化分析模型多步探索和推理能力的方法，并指出了当前视觉语言模型在此类任务中存在的显著不足和未来改进的方向。"}}
{"id": "2512.12230", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12230", "abs": "https://arxiv.org/abs/2512.12230", "authors": ["Jonathan Spraggett"], "title": "Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy", "comment": "Accepted at 28th RoboCup International Symposium", "summary": "Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup", "AI": {"tldr": "本文提出了一种单一的深度强化学习策略，能够让不同形态的人形机器人从跌倒中恢复，实现了强大的零样本泛化能力，在某些情况下甚至超越了专用策略。", "motivation": "现有深度强化学习方法在机器人跌倒恢复方面表现出色，但需要为每种机器人形态单独训练策略，这在动态环境中（如RoboCup）效率低下，且耗时费力。", "method": "研究人员采用深度强化学习（DRL）结合CrossQ技术，训练了一个单一策略。该策略涵盖了七种具有不同身高、体重和动力学特性的机器人形态。通过留一法实验、形态缩放分析和多样性消融实验来评估和验证策略的泛化能力。", "result": "训练后的统一策略在未见过的形态上实现了高达86 ± 7%的零样本迁移率。有针对性的形态覆盖显著提高了零样本泛化能力。在某些情况下，该共享策略甚至超越了专业的基线策略。", "conclusion": "研究结果表明，形态无关的跌倒恢复控制是可行的，并为通用型人形机器人控制奠定了基础。"}}
{"id": "2512.11988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11988", "abs": "https://arxiv.org/abs/2512.11988", "authors": ["Xianghui Xie", "Bowen Wen", "Yan Chang", "Hesam Rabeti", "Jiefeng Li", "Ye Yuan", "Gerard Pons-Moll", "Stan Birchfield"], "title": "CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction", "comment": "14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/", "summary": "Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.", "AI": {"tldr": "CARI4D是首个类别无关的方法，能从单目RGB视频中重建度量尺度、空间和时间一致的4D人-物交互，通过姿态假设选择、渲染-比较范式以及接触推理实现。", "motivation": "从单目RGB相机准确捕获人-物交互对人类理解、游戏和机器人学习至关重要。然而，从单视图推断4D交互面临物体和人体信息未知、深度模糊、遮挡和复杂运动等挑战，阻碍了3D和时间上的一致重建。现有方法通过假设已知物体模板或限制物体类别来简化设置。", "method": "本文提出了CARI4D方法。它包含一个姿态假设选择算法，该算法鲁棒地整合基础模型的个体预测；通过学习到的渲染-比较范式共同精化这些预测，以确保空间、时间和像素对齐；最后，通过推理复杂的接触点进行进一步精化，以满足物理约束。", "result": "实验结果表明，该方法在重建误差方面，在分布内数据集上优于现有技术38%，在未见过的数据集上优于36%。该模型能够泛化到训练类别之外，可以零样本应用于野外互联网视频。", "conclusion": "CARI4D是第一个能够从单目RGB视频中重建类别无关、空间和时间一致的度量尺度4D人-物交互的方法，并在性能和泛化能力上显著超越了现有技术，尤其适用于未知物体和野外场景。"}}
{"id": "2512.12620", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12620", "abs": "https://arxiv.org/abs/2512.12620", "authors": ["Aheli Poddar", "Saptarshi Sahoo", "Sujata Ghosh"], "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives", "comment": "9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic & AI. Code available at https://github.com/XAheli/Logic-in-LLMs", "summary": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.", "AI": {"tldr": "本文从逻辑和自然语言角度研究大型语言模型（LLMs）的直言三段论推理能力，发现其符号推理表现出色，但可能偏离人类推理的细微差别。", "motivation": "探究LLMs的基本推理能力及其研究发展方向。", "method": "使用14个大型语言模型，从符号推理和自然语言理解两方面评估其直言三段论推理能力。", "result": "直言三段论推理并非LLMs的统一涌现特性。部分模型在符号推理方面表现完美。", "conclusion": "LLMs可能正日益成为形式化推理机制，而非显式表达人类推理的细微之处。"}}
{"id": "2512.12288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12288", "abs": "https://arxiv.org/abs/2512.12288", "authors": ["Mahule Roy", "Guillaume Lambard"], "title": "Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases", "comment": "33 pages", "summary": "Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.", "AI": {"tldr": "该研究引入了一个量子感知的生成式AI框架，通过整合多保真学习和主动验证，克服了传统生成模型在强关联体系中依赖DFT的局限性，显著提高了材料发现的效率和准确性。", "motivation": "传统的材料发现生成模型主要依赖密度泛函理论（DFT）数据进行训练和验证，这导致它们继承了DFT在强关联体系中的系统性失效，从而产生了探索偏差，并无法发现DFT预测不准确的材料。", "method": "该框架采用了以下方法：1. 一个基于扩散的生成器，以量子力学描述符为条件。2. 一个使用等变神经网络势能的验证器，该验证器通过一个跨多层理论（PBE, SCAN, HSE06, CCSD(T)）的分层数据集进行训练。3. 一个鲁棒的主动学习循环，用于量化并针对低保真和高保真预测之间的差异。", "result": "与仅依赖DFT的基线相比，该框架在高差异区域（例如关联氧化物）中成功识别潜在稳定候选物的能力提高了3-5倍，同时保持了计算可行性。研究还进行了全面的消融研究、详细的失效模式分析，并与现有最先进的生成模型进行了基准测试。", "conclusion": "这项工作提供了一个严谨、透明的框架，能够将计算材料发现的有效搜索空间扩展到单一保真模型的限制之外，为发现传统方法难以预测的新材料提供了可能。"}}
{"id": "2512.12641", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12641", "abs": "https://arxiv.org/abs/2512.12641", "authors": ["Sander Land", "Yuval Pinter"], "title": "Which Pieces Does Unigram Tokenization Really Need?", "comment": "10 pages, 1 figure. For associated code, see https://github.com/sanderland/script_tok", "summary": "The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.", "AI": {"tldr": "本文提供了一个清晰的指南，以简化Unigram分词算法的实现和参数选择，并提出了一种在训练损失略高的情况下能提高压缩率的更简单算法。", "motivation": "Unigram分词算法虽然理论优雅，但其实现复杂，限制了其在实际中的应用（主要限于SentencePiece及其适配器）。研究旨在弥合理论与实践之间的鸿沟。", "method": "通过提供详细的实现指南和参数选择建议，并识别出一种更简单的算法。", "result": "研究发现了一种更简单的Unigram算法，该算法以略高的训练损失为代价，换取了更高的压缩率。", "conclusion": "本文通过提供清晰的实现指导和发现一种简化的、更高效的算法，成功地将Unigram分词算法从理论推广到更广泛的实践应用中。"}}
{"id": "2512.12243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12243", "abs": "https://arxiv.org/abs/2512.12243", "authors": ["HT To", "S Nguyen", "NH Pham"], "title": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.", "AI": {"tldr": "本文提出CAR-CHASE，通过冲突感知启发式缓存和自适应混合启发式方法，显著加速了基于CL-CBS的类车机器人多智能体路径规划（MAPF）算法，同时保持了解决方案的最优性。", "motivation": "CL-CBS等算法在解决类车机器人MAPF问题时，由于运动学启发式计算昂贵，面临巨大的计算挑战。传统的启发式缓存假设启发式函数仅依赖于状态，但在CBS中，冲突解决产生的约束使搜索空间变得上下文相关，导致传统缓存无效。", "method": "本文提出了CAR-CHASE方法，结合了：1) 冲突感知启发式缓存，根据状态和相关约束上下文缓存启发式值；2) 自适应混合启发式，智能地在快速近似计算和精确计算之间切换。主要创新包括：(1) 紧凑的“冲突指纹”以有效编码影响状态启发式的约束；(2) 基于空间、时间、几何标准的关联性过滤器；(3) 具有理论质量界限的自适应切换策略。", "result": "在480个基准实例上的实验表明，CAR-CHASE相对于基线CL-CBS实现了2.46倍的几何平均加速，并保持了解决方案的最优性。成功率从77.9%提高到84.8%（+6.9个百分点），总运行时间减少了70.1%，并解决了额外33个此前超时的实例。性能提升随问题复杂性而增加，在30智能体障碍场景中最高可达4.06倍加速。", "conclusion": "CAR-CHASE通过引入冲突感知启发式缓存和自适应混合启发式，显著提高了类车机器人MAPF问题的计算效率和可扩展性，同时保持了最优性。所提出的技术具有通用性，可应用于其他CBS变体。"}}
{"id": "2512.12794", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12794", "abs": "https://arxiv.org/abs/2512.12794", "authors": ["Yichen Liu", "Hongyu Wu", "Bo Liu"], "title": "A Rule-Aware Prompt Framework for Structured Numeric Reasoning in Cyber-Physical Systems", "comment": null, "summary": "Many cyber-physical systems (CPS) rely on high-dimensional numeric telemetry and explicit operating rules to maintain safe and efficient operation. Recent large language models (LLMs) are increasingly considered as decision-support components in such systems, yet most deployments focus on textual inputs and do not directly address rule-grounded reasoning over numeric measurements. This paper proposes a rule-aware prompt framework that systematically encodes CPS domain context, numeric normalization, and decision rules into a modular prompt architecture for LLMs. The framework decomposes prompts into five reusable modules, including role specification, CPS domain context, numeric normalization, rule-aware reasoning, and output schema, and exposes an interface for plugging in diverse rule sets. A key design element is separating rule specification from the representation of normalized numeric deviations, which enables concise prompts that remain aligned with domain rules. We analyze how different normalization strategies and prompt configurations influence rule adherence, interpretability, and token efficiency. The framework is model-agnostic and applicable across CPS domains. To illustrate its behavior, we instantiate it on numeric anomaly assessment in an IEEE 118-bus electric power transmission network and evaluate several prompting and adaptation regimes. The results show that rule-aware, z-score-based value blocks and a hybrid LLM-detector architecture can substantially improve consistency with CPS rules and anomaly detection performance while reducing token usage, providing a reusable bridge between numeric telemetry and general-purpose LLMs.", "AI": {"tldr": "本文提出了一种规则感知的提示框架，用于使大型语言模型（LLMs）能够对网络物理系统（CPS）中的高维数值遥测数据进行基于规则的推理，从而提高规则遵守度、可解释性和令牌效率。", "motivation": "许多网络物理系统（CPS）依赖高维数值遥测和明确的操作规则来维护安全高效的运行。尽管大型语言模型（LLMs）正被越来越多地考虑作为此类系统中的决策支持组件，但大多数部署关注文本输入，未能直接处理基于数值测量的规则推理问题。", "method": "本文提出一个规则感知的提示框架，将CPS领域上下文、数值归一化和决策规则系统地编码到模块化的LLM提示架构中。该框架将提示分解为五个可重用模块：角色规范、CPS领域上下文、数值归一化、规则感知推理和输出模式。其关键设计是分离规则规范与归一化数值偏差的表示，以实现简洁且与领域规则对齐的提示。该框架是模型无关的，并适用于各种CPS领域。通过在IEEE 118总线电力传输网络上的数值异常评估进行实例化和评估。", "result": "研究结果表明，规则感知、基于z-score的值块以及混合LLM-检测器架构可以显著提高与CPS规则的一致性和异常检测性能，同时减少令牌使用量。这为数值遥测数据和通用LLMs之间提供了一个可重用的桥梁。", "conclusion": "该研究提供了一个通用的、可重用的框架，使LLMs能够有效地处理CPS中的数值遥测数据并进行规则推理，从而提高了系统性能和资源效率，弥合了数值数据与通用LLMs之间的鸿沟。"}}
{"id": "2512.12826", "categories": ["eess.SY", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.12826", "abs": "https://arxiv.org/abs/2512.12826", "authors": ["Matei Drilea", "Alexander Dijkshoorn", "Gusthavo Ribeiro Salomão", "Stefano Stramigioli", "Gijs Krijnen"], "title": "Sensitivity increase of 3D printed, self-sensing, carbon fibers structures with conductive filament matrix due to flexural loading", "comment": "28 pages, 9 figures, 4 tables", "summary": "The excellent structural and piezoresistive properties of continuous carbon fiber make it suitable for both structural and sensing applications. This work studies the use of 3D printed, continuous carbon fiber reinforced beams as self-sensing structures. It is demonstrated how the sensitivity of these carbon fiber strain gauges can be increased irreversibly by means of a pretreatment by ``breaking-in'' the sensors with a large compressive bending load. The increase in the gauge factor is attributed to local progressive fiber failure, due to the combination of the thermal residual stress from the printing process and external loading. The coextrusion of conductive filament around the carbon fibers is demonstrated as a means of improving the reliability, noise and electrical connection of the sensors. A micrograph of the sensor cross section shows that the conductive filament contacts the various carbon fiber bundles. All-in-all, the use of ``breaking-in'' carbon fiber strain gauges in combination with coextrusion of conductive filament hold promises for 3D printed structural sensors with a high sensitivity.", "AI": {"tldr": "本研究探索了3D打印连续碳纤维增强梁作为自感知结构的应用，通过“磨合”预处理提高其应变传感灵敏度，并通过共挤导电丝改善可靠性。", "motivation": "连续碳纤维具有优异的结构和压阻特性，适用于结构和传感应用，因此研究其作为自感知结构的可能性。", "method": "研究了3D打印的连续碳纤维增强梁作为自感知结构。通过施加大压缩弯曲载荷对传感器进行“磨合”预处理，以不可逆地提高灵敏度。采用导电丝共挤技术包覆碳纤维，以提高传感器的可靠性、降低噪声并改善电连接。通过显微照片观察了传感器横截面。", "result": "通过“磨合”预处理，碳纤维应变传感器的灵敏度（应变系数）可不可逆地提高。灵敏度提高归因于打印过程中的热残余应力与外部载荷共同作用导致的局部渐进式纤维失效。导电丝共挤技术被证明能有效改善传感器的可靠性、噪声和电连接。传感器横截面的显微照片显示导电丝与碳纤维束接触良好。", "conclusion": "结合“磨合”碳纤维应变计和导电丝共挤技术，为开发高灵敏度的3D打印结构传感器提供了有前景的途径。"}}
{"id": "2512.12012", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12012", "abs": "https://arxiv.org/abs/2512.12012", "authors": ["Antonio Guillen-Perez"], "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus", "comment": null, "summary": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.", "AI": {"tldr": "Semantic-Drive是一个本地优先的神经符号框架，用于自动驾驶车辆数据挖掘。它通过两阶段感知和多模型共识机制，高效、隐私地识别稀有安全关键事件，并在消费级硬件上运行。", "motivation": "自动驾驶汽车（AVs）的发展受限于“长尾”训练数据（即稀有安全关键事件）的稀缺性。现有解决方案（如手动标注、粗略元数据搜索、基于云的VLM）成本高昂、缺乏精确性或存在隐私问题。", "method": "Semantic-Drive采用本地优先的神经符号框架，将感知解耦为两个阶段：1) 符号基础：通过实时开放词汇检测器（YOLOE）锚定注意力；2) 认知分析：通过推理VLM进行法医场景分析。为减轻幻觉，它采用“系统2”推理时对齐策略，利用多模型“法官-侦察兵”共识机制。", "result": "在nuScenes数据集上，针对Waymo Open Dataset (WOD-E2E) 分类法进行基准测试，Semantic-Drive的召回率达到0.966（CLIP为0.475），并比单一模型减少40%的风险评估错误。该系统完全在消费级硬件（NVIDIA RTX 3090）上运行。", "conclusion": "Semantic-Drive为自动驾驶车辆数据中的稀有安全关键事件识别提供了一个高效、精确且保护隐私的本地化解决方案，克服了现有方法的局限性。"}}
{"id": "2512.12320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12320", "abs": "https://arxiv.org/abs/2512.12320", "authors": ["Canqi Meng", "Weibang Bai"], "title": "Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy", "comment": null, "summary": "Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\\circ}$ (N=2), tilting of $18^{\\circ}$ (N=1), and twisting of $115^{\\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.", "AI": {"tldr": "本文提出一种新颖的设计方法，通过在多孔泡沫体上刻划特定图案，实现具有可编程变形的软多孔执行器，克服了传统软执行器结构支撑弱和多功能性不足的问题。", "motivation": "传统的软气动执行器通常缺乏结构支撑，且实现多功能性需要昂贵的几何特定重新设计。虽然将多孔材料填充到腔室中可以提供结构稳定性，但通过定制多孔体本身来实现可编程变形的方法尚未得到充分探索。", "method": "研究人员通过在多孔泡沫体上刻划特定图案来引入局部结构各向异性，从而在全局真空输入下引导材料变形。文章讨论了三种基本图案（横向用于弯曲，纵向用于倾斜，对角线用于扭曲），并建立了有限元分析（FEA）计算模型来研究刻划图案机制。该方法还应用于仿生机器人手的设计。", "result": "实验证明，通过优化图案阵列数量N，执行器可以实现高达80°的弯曲（N=2），18°的倾斜（N=1）和115°的扭曲（N=8）。该方法还展示了图案的可转移性、可扩展性和免模具快速原型制作的通用性。作为综合应用，研究人员将人手掌纹路图转化为功能性刻划图案，创造出能够实现类人自适应抓握的仿生软机器人手。", "conclusion": "这项工作为多功能软多孔机器人的设计提供了一种新的、高效且可扩展的范式。"}}
{"id": "2512.12053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12053", "abs": "https://arxiv.org/abs/2512.12053", "authors": ["Tran-Vu La", "Minh-Tan Pham", "Yu Li", "Patrick Matgen", "Marco Chini"], "title": "Adaptive federated learning for ship detection across diverse satellite imagery sources", "comment": "5 pages, IGARSS 2025", "summary": "We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.", "AI": {"tldr": "本文研究了联邦学习（FL）在跨多样卫星数据集的船舶检测中的应用，提供了一种保护隐私的解决方案，无需数据共享或集中收集，并显著提高了检测精度。", "motivation": "处理商业卫星图像或敏感船舶标注时存在隐私问题，需要一种无需数据共享或集中收集的解决方案。", "method": "评估了四种联邦学习模型（FedAvg、FedProx、FedOpt和FedMedian），并将其与本地训练基线（YOLOv8模型在每个数据集上独立训练）进行了比较。研究还探讨了联邦学习配置（如通信轮次和本地训练周期）对性能的影响。", "result": "联邦学习模型显著提高了小规模本地数据集上的检测精度，并达到了接近使用所有数据集进行全局训练的性能水平。研究还强调了选择合适的联邦学习配置对于优化检测精度和计算效率的重要性。", "conclusion": "联邦学习是一种有效且保护隐私的船舶检测解决方案，其性能接近集中式训练，并且通过适当的配置可以进一步优化。"}}
{"id": "2512.12833", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12833", "abs": "https://arxiv.org/abs/2512.12833", "authors": ["Nathaniel Smith", "Yu Wang"], "title": "Data-driven Supervisory Control under Attacks via Spectral Learning", "comment": "10 pages", "summary": "The technological advancements facilitating the rapid development of cyber-physical systems (CPS) also render such systems vulnerable to cyber attacks with devastating effects. Supervisory control is a commonly used control method to neutralize attacks on CPS. The supervisor strives to confine the (symbolic) paths of the system to a desired language via sensors and actuators in a closed control loop, even when attackers can manipulate the symbols received by the sensors and actuators. Currently, supervisory control methods face limitations when effectively identifying and mitigating unknown, broad-spectrum attackers. In order to capture the behavior of broad-spectrum attacks on both sensing and actuation channels we model the plant, supervisors, and attackers with finite-state transducers (FSTs). Our general method for addressing unknown attackers involves constructing FST models of the attackers from spectral analysis of their input and output symbol sequences recorded from a history of attack behaviors observed in a supervisory control loop. To construct these FST models, we devise a novel learning method based on the recorded history of attack behaviors. A supervisor is synthesized using such models to neutralize the attacks.", "AI": {"tldr": "本文提出一种新颖方法，通过对网络物理系统（CPS）中观察到的攻击行为进行谱分析和学习，构建未知攻击者的有限状态传感器（FST）模型，进而综合出主管控制器来抵御广谱网络攻击。", "motivation": "随着技术发展，网络物理系统（CPS）面临网络攻击的严重威胁，现有主管控制方法在有效识别和缓解未知、广谱攻击者方面存在局限性，特别是在攻击者能够操纵传感和执行通道时。", "method": "研究方法包括：1. 使用有限状态传感器（FSTs）建模工厂、主管和攻击者。2. 针对未知攻击者，从主管控制循环中记录的攻击行为历史数据中，通过对其输入和输出符号序列进行谱分析，构建攻击者的FST模型。3. 设计一种新颖的学习方法来从记录的攻击历史中构建这些FST模型。4. 利用这些模型综合出主管控制器来抵御攻击。", "result": "提出了一种处理未知攻击者的通用方法，该方法能够通过对历史攻击行为进行谱分析和采用新颖的学习方法，构建攻击者的FST模型，并利用这些模型来综合主管控制器以抵御攻击。", "conclusion": "通过学习未知广谱攻击者的FST模型，可以合成出更有效的主管控制器，从而提升网络物理系统抵御复杂网络攻击的能力。"}}
{"id": "2512.12411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12411", "abs": "https://arxiv.org/abs/2512.12411", "authors": ["Ely Hahami", "Lavik Jain", "Ishaan Sinha"], "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs", "comment": "7 pages (+ 5 pages for appendix), 5 figures, 1 table", "summary": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.", "AI": {"tldr": "本文测试了Anthropic关于大模型内省能力的说法，发现Llama-3.1-8B能重现其结果，但模型内省对提示语变化敏感，易受影响；不过，模型能可靠地分类注入概念的强度。", "motivation": "Anthropic声称前沿模型能够检测并命名注入的“概念”（表现为激活方向）。本文旨在测试这些说法的鲁棒性。", "method": "1. 在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮“涌现式内省”结果。2. 系统性地改变推理提示语。3. 测试相关任务，如注入概念的多项选择识别和二元判别。4. 测试对归一化注入概念向量强度（弱/中/强/非常强）的分类能力。", "result": "1. 在Llama-3.1-8B-Instruct上成功复现了Anthropic的内省结果，识别和命名注入概念的成功率为20%，与Anthropic报告的数字完全一致。2. 内省能力是脆弱的：在密切相关的任务（如多项选择识别）或不同的二元判别提示语下，性能会急剧下降。3. 发现了部分内省的现象：同一模型能以高达70%的准确率可靠地分类归一化注入概念向量的强度，远高于25%的随机基线。", "conclusion": "这些结果进一步证明了语言模型在内省过程中确实计算了其基线内部表示的函数；然而，这些关于自身表示的自我报告是狭隘且对提示语敏感的。"}}
{"id": "2512.12413", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12413", "abs": "https://arxiv.org/abs/2512.12413", "authors": ["Gabriel R. Lau", "Wei Yan Low", "Louis Tay", "Ysabel Guevarra", "Dragan Gašević", "Andree Hartanto"], "title": "Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale", "comment": null, "summary": "Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.", "AI": {"tldr": "本研究开发并验证了一个13项的“人工智能使用批判性思维”量表，包含验证、动机和反思三个维度，并展示了其心理测量学特性和预测能力。", "motivation": "生成式人工智能工具日益普及，但其流畅性、不透明性和幻觉倾向要求用户批判性地评估其输出。因此，研究人员旨在概念化并测量用户在使用人工智能时如何进行批判性思考。", "method": "研究通过六项研究（N=1365）开发并验证了该量表。具体方法包括：研究1生成并验证量表项目；研究2支持了三因素结构（验证、动机、反思）；研究3、4、5确认了高阶模型，并证明了内部一致性、重测信度、因子载荷、性别不变性以及聚合和区分效度；研究3和4进一步探讨了其与其他人格特质的关联；研究6证明了量表的效标效度。", "result": "研究成功开发并验证了一个13项的“人工智能使用批判性思维”量表，具有良好的三因素结构（验证、动机、反思）和强大的心理测量学特性（信度、效度、不变性）。该量表与开放性、外向性、积极特质情感以及人工智能使用频率呈正相关。更高的量表得分预示着更频繁和多样化的验证策略、在ChatGPT事实核查任务中更高的判断准确性，以及对负责任人工智能更深入的反思。", "conclusion": "本研究阐明了人们如何以及为何对生成式人工智能输出进行监督，并提供了一个经过验证的量表和基于生态学的任务范式，以支持未来关于批判性参与生成式人工智能输出的理论检验、跨群体和纵向研究。"}}
{"id": "2512.12377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12377", "abs": "https://arxiv.org/abs/2512.12377", "authors": ["Haichuan Li", "Changda Tian", "Panos Trahanias", "Tomi Westerlund"], "title": "INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset", "comment": null, "summary": "We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.", "AI": {"tldr": "INDOOR-LIDAR是一个综合性的室内3D激光雷达混合数据集，结合了模拟和真实数据，旨在解决现有数据集的局限性，并推动机器人感知研究。", "motivation": "现有的室内激光雷达数据集存在规模有限、标注格式不一致以及数据采集中人为变异性等问题。", "method": "INDOOR-LIDAR采用混合方法，整合了模拟环境（可灵活配置布局、点密度和遮挡）和自主地面机器人采集的真实世界扫描（捕捉真实的传感器噪声、杂波和特定领域伪影）。每个样本包含密集的点云数据、强度测量和KITTI风格的标注。", "result": "该数据集支持广泛的应用，包括3D目标检测、鸟瞰图(BEV)感知、SLAM、语义场景理解以及模拟与真实室内域之间的域适应。通过连接合成数据和真实世界数据，INDOOR-LIDAR建立了一个可扩展、逼真且可复现的基准。", "conclusion": "INDOOR-LIDAR为复杂室内环境中的机器人感知进步提供了一个可扩展、逼真且可复现的基准。"}}
{"id": "2512.12477", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12477", "abs": "https://arxiv.org/abs/2512.12477", "authors": ["Jiawen Chen", "Yanyan He", "Qi Shao", "Mengli Wei", "Duxin Chen", "Wenwu Yu", "Yanlong Zhao"], "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs", "comment": null, "summary": "Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE", "AI": {"tldr": "MetaHGNIE是一个元路径诱导的超图对比学习框架，用于在异构知识图中解耦和对齐结构与语义信息，以解决节点重要性估计中高阶依赖和跨模态集成的问题。", "motivation": "现有异构知识图谱中的节点重要性估计方法通常依赖于成对连接，忽略了多实体和关系之间的高阶依赖，并且独立处理结构和语义信号，阻碍了有效的跨模态集成。", "method": "本文提出了MetaHGNIE框架。它通过元路径序列构建高阶知识图谱，其中类型化超边捕获多实体关系上下文。结构依赖通过局部注意力聚合，语义表示通过配备稀疏分块的超图Transformer编码。最后，一个多模态融合模块在对比学习和辅助监督下集成结构和语义嵌入，确保鲁棒的跨模态对齐。", "result": "在基准节点重要性估计数据集上进行的广泛实验表明，MetaHGNIE持续优于最先进的基线方法。", "conclusion": "这些结果强调了在异构知识图中显式建模高阶交互和跨模态对齐的有效性。"}}
{"id": "2512.12871", "categories": ["eess.SY", "q-fin.CP", "q-fin.PR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12871", "abs": "https://arxiv.org/abs/2512.12871", "authors": ["Millend Roy", "Agostino Capponi", "Vladimir Pyltsov", "Yinbo Hu", "Vijay Modi"], "title": "CapOptix: An Options-Framework for Capacity Market Pricing", "comment": null, "summary": "Electricity markets are under increasing pressure to maintain reliability amidst rising renewable penetration, demand variability, and occasional price shocks. Traditional capacity market designs often fall short in addressing this by relying on expected-value metrics of energy unserved, which overlook risk exposure in such systems. In this work, we present CapOptix, a capacity pricing framework that interprets capacity commitments as reliability options, i.e., financial derivatives of wholesale electricity prices. CapOptix characterizes the capacity premia charged by accounting for structural price shifts modeled by the Markov Regime Switching Process. We apply the framework to historical price data from multiple electricity markets and compare the resulting premium ranges with existing capacity remuneration mechanisms.", "AI": {"tldr": "CapOptix是一个容量定价框架，它将容量承诺解释为可靠性期权，并利用马尔可夫机制转换过程来模拟批发电力价格的结构性变化，以更准确地确定容量溢价。", "motivation": "电力市场在可再生能源渗透率上升、需求波动和价格冲击下，维持可靠性面临巨大压力。传统容量市场设计依赖于未供应电量的预期值指标，忽略了系统中的风险暴露，因此不足以应对这些挑战。", "method": "本文提出了CapOptix框架，将容量承诺解释为可靠性期权（即批发电力价格的金融衍生品）。该框架通过马尔可夫机制转换过程建模结构性价格变化，从而计算容量溢价。", "result": "该框架被应用于多个电力市场的历史价格数据。其得出的溢价范围与现有容量补偿机制进行了比较。", "conclusion": "CapOptix提供了一种新的容量定价方法，通过将容量承诺视为金融期权并考虑市场价格的结构性变化，能够更好地应对电力市场中的可靠性挑战和风险暴露。"}}
{"id": "2512.12443", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12443", "abs": "https://arxiv.org/abs/2512.12443", "authors": ["Akhmadillo Mamirov", "Faiaz Azmain", "Hanyu Wang"], "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline", "comment": null, "summary": "AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.", "AI": {"tldr": "本研究发现AI模型文档碎片化且结构不一致，阻碍了安全评估。我们开发了一个加权透明度框架和自动化评估流程，揭示了模型文档中普遍存在的系统性缺陷，尤其在安全关键披露方面。", "motivation": "AI模型文档在不同平台间碎片化且结构不一致，导致政策制定者、审计员和用户难以可靠地评估AI模型的安全主张、数据来源和版本变更。", "method": "研究分析了5个前沿模型和100个Hugging Face模型卡的文档，识别出947个独特的章节名称。基于欧盟AI法案附件IV和斯坦福透明度指数，开发了一个包含8个主要部分和23个子部分的加权透明度框架，优先考虑安全关键披露（如安全评估25%、关键风险20%）。随后，实现了一个自动化多智能体管道，利用大型语言模型（LLM）从公共来源提取文档并通过共识机制进行完整性评分。该管道用于评估50个视觉、多模态、开源和闭源模型。", "result": "自动化评估50个模型的总成本低于3美元，并揭示了系统性缺陷。前沿实验室（xAI、微软、Anthropic）的合规率约为80%，而大多数提供商低于60%。安全关键类别显示出最大的缺陷：在所有评估模型中，欺骗行为、幻觉和儿童安全评估分别导致148、124和116的总失分。", "conclusion": "AI模型文档存在严重缺陷，特别是在安全关键信息披露方面。需要标准化和改进文档实践，以实现对AI模型更可靠的评估和问责制。"}}
{"id": "2512.12427", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12427", "abs": "https://arxiv.org/abs/2512.12427", "authors": ["Rudolf Reiter", "Chao Qin", "Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models", "comment": null, "summary": "Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.", "AI": {"tldr": "Unique是一种统一的MPC方法，通过在单个优化中级联不同精度的模型（短时高精度、长时低精度），显著提升了四旋翼飞行器的控制和规划性能。", "motivation": "四旋翼飞行器在许多空中任务中，既需要即时响应（高精度模型），也需要长距离规划（长时域规划）。现有方法在高精度模型上速度慢，无法用于长时域；而低精度规划器虽然可扩展但闭环性能差。因此，需要一种能兼顾即时响应和长距离规划的统一解决方案。", "method": "本文提出了Unique，一种统一的MPC方法，其核心是将不同精度的模型级联到一个优化框架中：短时域使用高精度模型进行精确控制，长时域使用低精度模型进行规划。具体方法包括：对齐跨时域的成本；为质点模型推导保持可行性的推力和机体角速率约束；引入匹配不同状态、推力引起的加速度和急动-机体角速率关系的过渡约束。为避免非光滑障碍物引起的局部最小值，提出了一种3D渐进平滑调度方案。此外，部署并行随机初始化的MPC求解器，以在长时域低精度模型上发现更低成本的局部最小值。", "result": "在仿真和实际飞行中，在相同的计算预算下，Unique相比标准MPC和分层规划器-跟踪器基线，闭环位置或速度跟踪性能提高了高达75%。消融实验和帕累托分析证实了在时域变化、约束近似和平滑调度方面的稳健增益。", "conclusion": "Unique成功地将不同精度的模型统一到MPC框架中，解决了四旋翼飞行器即时响应和长距离规划的挑战，显著提升了其闭环控制和规划性能。"}}
{"id": "2512.12872", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12872", "abs": "https://arxiv.org/abs/2512.12872", "authors": ["Xiaojie Tao", "Yaoyu Fan", "Zhaoyi Ye", "Rajit Gadh"], "title": "Heavy-Duty Electric Vehicles Contribution for Frequency Response in Power Systems with V2G", "comment": "Accepted at The 16th IEEE International Conference on Green Energy and Smart Systems, 2025", "summary": "The integration of heavy-duty electric vehicles (EVs) with Vehicle-to-Grid (V2G) capability offers a promising solution to enhance grid stability by providing primary frequency response in power systems. This paper investigates the potential of heavy-duty EVs to support the California power grid under different charging strategies: immediate, delayed, and constant minimum power charging. Simulation results demonstrate that both V2G-capable EVs and non-V2G modes have great potential to provide primary frequency response, with V2G-capable EVs exhibiting especially strong contributions. The study highlights the influence of charging strategies, control modes, and grid conditions on EV contributions to grid stability, emphasizing their critical role in mitigating the adverse effects of renewable energy penetration.", "AI": {"tldr": "本文研究了重型电动汽车（EV）及其车网互动（V2G）能力在不同充电策略下为加州电网提供一次调频以增强电网稳定性的潜力，发现V2G模式贡献尤其显著。", "motivation": "通过整合重型电动汽车（EV）及其车网互动（V2G）能力，为电力系统提供一次调频，以增强电网稳定性，并缓解可再生能源渗透带来的负面影响。", "method": "采用仿真研究方法，在即时、延迟和恒定最小功率充电等不同充电策略下，调查重型电动汽车（包括V2G和非V2G模式）对加州电网一次调频的支持潜力。", "result": "仿真结果表明，V2G-capable和非V2G模式的电动汽车都具有提供一次调频的巨大潜力，其中V2G-capable电动汽车的贡献尤其显著。研究强调了充电策略、控制模式和电网条件对电动汽车贡献电网稳定性的影响。", "conclusion": "重型电动汽车在缓解可再生能源渗透的不利影响和增强电网稳定性方面发挥着关键作用，V2G能力尤其能提供强大的贡献。"}}
{"id": "2512.12060", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.12060", "abs": "https://arxiv.org/abs/2512.12060", "authors": ["Tejas Panambur", "Ishan Rajendrakumar Dave", "Chongjian Ge", "Ersin Yumer", "Xue Bai"], "title": "CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos", "comment": "The first two authors contributed equally", "summary": "Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.\n  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.\n  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.", "AI": {"tldr": "CreativeVR是一个扩散先验引导的视频修复框架，专门用于修复AI生成和真实视频中严重的结构和时间伪影，通过一个可控的精度旋钮和新颖的时间一致性退化模块实现。", "motivation": "现代文生视频模型在精细结构上（如人脸、手部、背景、时间一致性）存在严重伪影。传统的视频修复方法主要针对模糊和下采样等合成退化，往往会稳定而非修复这些伪影。现有的扩散先验修复器通常在光度噪声上训练，缺乏对感知质量和保真度之间权衡的控制。", "method": "CreativeVR是一个扩散先验引导的视频修复框架。它采用基于深度适配器的方法，提供一个“精度旋钮”来控制模型遵循输入的强度，从而在标准退化上的精确修复和对挑战性内容的结构/运动校正行为之间平滑地进行权衡。其核心创新是一个在训练期间使用的“时间一致性退化模块”，该模块应用精心设计的变换来生成真实的结构性故障。", "result": "CreativeVR在具有严重伪影的视频（AIGC54基准测试，使用FIQA、语义和感知指标）上取得了最先进的结果，并在标准视频修复基准上表现出竞争力。它还具有实用的吞吐量（在单个80GB A100上，720p视频约为13 FPS）。", "conclusion": "CreativeVR成功解决了AI生成和真实视频中严重的结构和时间伪影问题，通过其创新的训练方法和可控的修复能力，实现了卓越的性能和实用性。"}}
{"id": "2512.12056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12056", "abs": "https://arxiv.org/abs/2512.12056", "authors": ["Maria Rodriguez", "Minh-Tan Pham", "Martin Sudmanns", "Quentin Poterek", "Oscar Narvaez"], "title": "Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management", "comment": "5 pages, IGARSS 2025", "summary": "After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.", "AI": {"tldr": "本研究提出了一种针对SPOT-6/7图像的监督语义分割工作流程，旨在提高野火烧毁区域（BA）划定的性能和效率，并评估了U-Net和SegFormer模型，同时通过辅助任务和测试时增强优化了结果。", "motivation": "野火后烧毁区域的划定对于量化损害和支持生态恢复至关重要。当前的BA测绘方法依赖于遥感图像上的计算机视觉模型，但往往忽视了它们在时间受限的应急管理场景中的适用性。", "method": "本研究引入了一种监督语义分割工作流程，针对SPOT-6/7高分辨率图像进行BA划定。实验评估了U-Net和SegFormer模型，并探讨了将土地覆盖数据作为辅助任务以及测试时增强（TTA）对模型性能和推理时间的影响。评估指标包括Dice分数、交并比和推理时间，并考虑了混合精度等优化方法。", "result": "结果显示，在有限训练数据下，U-Net和SegFormer模型表现相似。然而，SegFormer需要更多资源，这限制了其在紧急情况下的实际应用。整合土地覆盖数据作为辅助任务可增强模型鲁棒性，且不增加推理时间。最后，测试时增强（TTA）提高了BA划定性能，但增加了推理时间，这可以通过混合精度等优化方法来缓解。", "conclusion": "所提出的工作流程能够有效提升野火烧毁区域划定的性能和效率。尽管SegFormer在资源方面存在挑战，但通过引入辅助任务可以增强模型鲁棒性，并且通过优化方法（如混合精度）可以缓解测试时增强带来的推理时间增加，使其在时间受限的应急管理场景中具有实用价值。"}}
{"id": "2512.12643", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12643", "abs": "https://arxiv.org/abs/2512.12643", "authors": ["Yida Cai", "Ranjuexiao Hu", "Huiyuan Xie", "Chenyang Li", "Yun Liu", "Yuxiao Ye", "Zhenghao Liu", "Weixing Shen", "Zhiyuan Liu"], "title": "LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases", "comment": null, "summary": "Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.", "AI": {"tldr": "本文提出了一套全面的中文民事案件法律关系图谱和LexRel专家标注基准，用于法律关系抽取任务，并评估了当前大型语言模型在此任务上的局限性，同时证明了法律关系信息对下游法律AI任务的增益。", "motivation": "法律关系是民事法律体系中的关键分析框架，但在法律人工智能（法律AI）领域，尤其是在中文民事案件中，由于缺乏全面的图谱，法律关系的研究仍未充分。", "method": "首先，引入了一套包含层级分类和论元定义的综合图谱，用于AI系统捕获中文民事案件中的法律关系。其次，基于该图谱，提出了法律关系抽取任务并构建了LexRel专家标注基准。最后，使用LexRel评估了最先进的大型语言模型在该任务上的表现，并展示了整合法律关系信息对其他下游法律AI任务的性能提升。", "result": "当前的大型语言模型在准确识别民事法律关系方面存在显著局限性。此外，整合法律关系信息能够持续提升其他下游法律AI任务的性能。", "conclusion": "所提出的法律关系图谱和LexRel基准填补了中文民事案件法律关系研究的空白，揭示了现有大型语言模型在此领域的不足，并强调了法律关系信息对于提升法律AI系统能力的重要性。"}}
{"id": "2512.12437", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12437", "abs": "https://arxiv.org/abs/2512.12437", "authors": ["Jonathan Spraggett"], "title": "Sim2Real Reinforcement Learning for Soccer skills", "comment": "Undergrad Thesis", "summary": "This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.", "AI": {"tldr": "该论文提出了一种结合课程训练和对抗运动先验（AMP）的强化学习方法，以更高效地训练类人机器人控制任务，在模拟中表现出更动态、适应性强的策略，但未能成功迁移到真实世界。", "motivation": "传统的强化学习方法在适应真实世界环境、处理复杂性和生成自然运动方面存在局限性，促使研究者寻求更有效的方法来训练类人机器人的控制任务。", "method": "采用课程训练（curriculum training）和对抗运动先验（Adversarial Motion Priors, AMP）技术来训练强化学习策略。", "result": "开发的强化学习策略在踢腿、行走和跳跃任务中表现出更高的动态性、适应性，并优于先前的方法。然而，学习到的策略未能成功从模拟环境迁移到真实世界。", "conclusion": "所提出的强化学习方法在模拟环境中显著提升了类人机器人控制任务的性能，但未能成功实现策略从模拟到真实世界的迁移，突显了当前强化学习方法在完全适应真实世界场景方面的局限性。"}}
{"id": "2512.12654", "categories": ["cs.CL", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.12654", "abs": "https://arxiv.org/abs/2512.12654", "authors": ["Hassan Mujtaba", "Hamza Naveed", "Hanzlah Munir"], "title": "Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks", "comment": "6 pages", "summary": "Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.", "AI": {"tldr": "本文提出一个基于图的框架，利用角色互动网络对乌尔都语小说进行作者归属分析，结果显示学习到的图表示显著优于基线方法。", "motivation": "传统的作者归属分析主要关注文本词汇和风格线索，而高层叙事结构（特别是对于乌尔都语等低资源语言）尚未得到充分探索。", "method": "将每部小说建模为角色互动网络（图），其中节点代表角色，边表示他们在叙事邻近度内的共现。系统比较了多种图表示方法，包括全局结构特征、节点级语义摘要、无监督图嵌入和有监督图神经网络。", "result": "在包含7位作者的52部乌尔都语小说数据集上，学习到的图表示（特别是监督图神经网络）显著优于手工和无监督基线，在严格的作者感知评估协议下，准确率高达0.857。", "conclusion": "作者风格可以仅从叙事结构中推断出来，图基方法在乌尔都语小说作者归属分析中表现出色，尤其是在利用学习到的图表示时。"}}
{"id": "2512.12677", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12677", "abs": "https://arxiv.org/abs/2512.12677", "authors": ["Amirhossein Yousefiramandi", "Ciaran Cooney"], "title": "Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches", "comment": "18 pages, 6 figures", "summary": "We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.", "AI": {"tldr": "该研究探讨了在资源受限下，对解码器专用大型语言模型（LLMs）进行文本分类的有效微调策略。结果显示，基于嵌入的方法显著优于指令微调方法，并且与领域特定模型（如BERT）相比具有竞争力甚至超越。", "motivation": "在计算资源有限（如单GPU）的情况下，如何高效地对解码器专用LLMs进行微调以适应下游文本分类任务。", "method": "研究了两种方法：1) 将分类头连接到预训练的因果LLM，并使用LLM的最终token嵌入作为序列表示进行微调。2) 以“提示->响应”格式对LLM进行指令微调。为实现单GPU微调高达8B参数的模型，结合了4位模型量化和低秩适应（LoRA）技术进行参数高效训练。实验在专有单标签数据集和公共WIPO-Alpha专利数据集（极端多标签分类）上进行。", "result": "基于嵌入的方法在F1分数上显著优于指令微调方法，并且与在相同任务上微调的领域特定模型（如BERT）相比，具有很强的竞争力，甚至表现更佳。", "conclusion": "直接利用因果LLM的内部表示，结合高效的微调技术，可以在有限的计算资源下实现令人印象深刻的分类性能。文章还讨论了每种方法的优势，并提出了优化LLM分类微调的实用指导和未来方向。"}}
{"id": "2512.12080", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12080", "abs": "https://arxiv.org/abs/2512.12080", "authors": ["Ryan Po", "Eric Ryan Chan", "Changan Chen", "Gordon Wetzstein"], "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models", "comment": "Project page here: https://ryanpo.com/bagger", "summary": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.", "AI": {"tldr": "为解决自回归视频模型在长序列生成中的曝光偏差问题，本文提出了Backwards Aggregation (BAgger) 自监督方案，通过模型自身的rollout构建纠正轨迹，使其能从错误中恢复，从而提高视频生成质量和稳定性。", "motivation": "自回归视频模型在通过下一帧预测进行世界建模时，存在曝光偏差问题。训练时使用干净上下文，推理时使用自生成帧，导致误差累积，并随着时间推移出现质量漂移。", "method": "本文引入了Backwards Aggregation (BAgger)，这是一种自监督方案，它从模型自身的rollout中构建纠正轨迹，以训练模型从错误中恢复。与以往依赖少量步骤蒸馏和分布匹配损失的方法不同，BAgger使用标准的分数或流匹配目标进行训练，避免了大型教师模型和长时间反向传播。", "result": "将BAgger应用于因果扩散Transformer，并在文本到视频、视频扩展和多提示生成任务中进行评估。结果表明，BAgger带来了更稳定的长序列运动和更好的视觉一致性，并显著减少了漂移。", "conclusion": "BAgger是一种有效的自监督方案，能够纠正自回归视频模型中的曝光偏差，显著提升了模型在长序列视频生成中的稳定性和视觉一致性，且不损害质量和多样性。"}}
{"id": "2512.12923", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12923", "abs": "https://arxiv.org/abs/2512.12923", "authors": ["Kai Xiong", "Xingyu Wu", "Anna Duan", "Supeng Leng", "Jianhua He"], "title": "Information-Optimal Formation Geometry Design for Multimodal UAV Cooperative Perception", "comment": null, "summary": "The efficacy of UAV swarm cooperative perception fundamentally depends on three-dimensional (3D) formation geometry, which governs target observability and sensor complementarity. In the literature, the exploitation of formation geometry and its impact on UAV sensing have rarely been studied, which can significantly degrade multimodal cooperative perception at scenarios where heterogeneous payloads (vision cameras and LiDAR) should be geometrically arranged to exploit their complementary strengths while managing communication interference and hardware budgets. To bridge this critical gap, we propose an information-theoretic optimization framework that allocation of UAVs and multimodal sensors, configures formation geometries, and flight control. The UAV-sensor allocation is optimized by the Fisher Information Matrix (FIM) determinant maximization. Under this framework we introduce an equivalent formation transition strategy that enhances field-of-view (FOV) coverage without compromising perception accuracy and communication interference. Furthermore, we design a novel Lyapunov-stable flight control scheme with logarithmic potential fields to generate energy-efficient trajectories for formation transitions. Extensive simulations demonstrate our formation-aware design achieves 25.0\\% improvement in FOV coverage, 104.2\\% enhancement in communication signal strength, and 47.2\\% reduction in energy consumption compared to conventional benchmarks. This work establishes that task-driven geometric configuration represents a foundational rather than incidental component in next-generation UAV swarm systems.", "AI": {"tldr": "本文提出了一种信息论优化框架，用于无人机蜂群的多模态合作感知，通过优化编队几何、无人机-传感器分配和飞行控制，显著提升了感知覆盖、通信强度并降低了能耗。", "motivation": "现有研究鲜少探讨编队几何对无人机感知的影响，尤其是在异构传感器（视觉和激光雷达）需要几何排列以发挥互补优势、同时管理通信干扰和硬件预算的场景下，这会导致多模态合作感知性能下降。", "method": "本文提出一个信息论优化框架，通过最大化费雪信息矩阵（FIM）行列式来优化无人机和多模态传感器的分配。在此框架下，引入了一种等效编队转换策略，在不牺牲感知精度和通信干扰的前提下增强视场（FOV）覆盖。此外，设计了一种基于对数势场的Lyapunov稳定飞行控制方案，以生成节能的编队转换轨迹。", "result": "广泛的仿真结果表明，与传统基准相比，本文提出的编队感知设计在FOV覆盖方面提高了25.0%，通信信号强度增强了104.2%，能耗降低了47.2%。", "conclusion": "这项工作确立了任务驱动的几何配置在下一代无人机蜂群系统中是一个基础而非偶然的组成部分。"}}
{"id": "2512.12501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12501", "abs": "https://arxiv.org/abs/2512.12501", "authors": ["Dang Phuong Nam", "Nguyen Kieu", "Pham Thanh Hieu"], "title": "SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation", "comment": null, "summary": "Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.", "AI": {"tldr": "SafeGen是一个框架，旨在将伦理保障直接嵌入到文本到图像生成流程中，以解决生成式AI的伦理困境，同时平衡创造自由与责任。", "motivation": "生成式AI（如文本到图像系统）带来了巨大的创造机会，但也引发了严重的伦理问题，包括放大社会偏见、产生虚假信息和侵犯知识产权，即所谓的“双重用途困境”。", "method": "SafeGen框架基于可信AI原则设计，包含两个互补组件：1) BGE-M3，一个经过微调的文本分类器，用于过滤有害或误导性提示；2) Hyper-SD，一个优化的扩散模型，用于生成高保真、语义对齐的图像。该框架通过一个精心策划的多语言（英语-越南语）数据集和公平感知训练过程进行构建。", "result": "定量评估显示，Hyper-SD实现了IS = 3.52，FID = 22.08，SSIM = 0.79，而BGE-M3的F1-Score达到了0.81。消融研究进一步证实了领域特定微调对两个模块的重要性。案例研究展示了SafeGen在阻止不安全提示、生成包容性教学材料和强化学术诚信方面的实际效果。", "conclusion": "SafeGen证明了在文本到图像生成工作流程中，通过嵌入伦理保障，可以实现创造自由与伦理责任的调和。"}}
{"id": "2512.12716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12716", "abs": "https://arxiv.org/abs/2512.12716", "authors": ["Xuanzhang Liu", "Jianglun Feng", "Zhuoran Zhuang", "Junzhe Zhao", "Maofei Que", "Jieting Li", "Dianlei Wang", "Hao Tong", "Ye Chen", "Pan Li"], "title": "CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning", "comment": "Accepted to WSDM '26 Oral", "summary": "Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by \"Context Explosion\", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.", "AI": {"tldr": "CoDA是一种分层强化学习智能体，通过解耦高层规划和低层执行来解决大型语言模型（LLM）代理的“上下文爆炸”问题，它使用一个共享的LLM骨干并采用PECO方法进行端到端优化，显著提升了在复杂多跳问答任务上的性能和长上下文场景下的鲁棒性。", "motivation": "LLM代理在解决复杂多步任务时，由于“上下文爆炸”问题（即长时间文本输出积累导致模型上下文窗口过载），性能受到严重影响并导致推理失败。", "method": "引入了CoDA（Context-Decoupled hierarchical Agent），一个上下文解耦的分层代理框架。它使用一个共享的LLM骨干，学习在两个隔离的角色中操作：一个在高层进行简洁战略规划的“规划器”（Planner），以及一个在临时隔离工作区处理工具交互的“执行器”（Executor）。该统一代理通过PECO（Planner-Executor Co-Optimization）进行端到端训练，这是一种应用轨迹级奖励来共同优化两个角色的强化学习方法，通过上下文相关的策略更新促进无缝协作。", "result": "CoDA在复杂的多跳问答基准测试上比现有最先进的基线取得了显著的性能提升。在长上下文场景中，CoDA表现出强大的鲁棒性，能够保持稳定的性能，而所有其他基线都遭受了严重的性能下降。", "conclusion": "CoDA的分层设计有效缓解了上下文过载问题，验证了其在复杂、长上下文任务中提升LLM代理性能和鲁棒性的有效性。"}}
{"id": "2512.12883", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.12883", "abs": "https://arxiv.org/abs/2512.12883", "authors": ["Masoud S. Sakha", "Rushikesh Kamalapurkar"], "title": "On the embedding transformation for optimal control of multi-mode switched systems", "comment": null, "summary": "This paper develops an embedding-based approach to solve switched optimal control problems (SOCPs) with an arbitrary number of subsystems. Initially, the discrete switching signal is represented by a set of binary variables, encoding each mode in binary format. An embedded optimal control problem (EOCP) is then formulated by replacing these binary variables with continuous embedded variables that can take intermediate values between zero and one. Although embedding allows SOCPs to be addressed using conventional techniques, the optimal solutions of EOCPs often yield intermediate values for binary variables, which may not be feasible for the original SOCP. To address this challenge, a modified EOCP (MEOCP) is introduced by adding a concave auxiliary cost function of appropriate dimensionality to the main cost function. This addition ensures that the optimal solution of the EOCP is bang-bang, and as a result, feasible for the original SOCP.", "AI": {"tldr": "本文提出了一种基于嵌入的方法来解决切换最优控制问题（SOCPs），通过引入凹辅助成本函数确保嵌入式问题的解是bang-bang的，从而使其对原始SOCP可行。", "motivation": "切换最优控制问题（SOCPs）由于离散的切换信号难以解决。虽然嵌入方法允许使用传统技术，但由此产生的嵌入式最优控制问题（EOCPs）的解常包含中间值，这对于原始SOCP是不可行的。", "method": "首先，将离散切换信号表示为一组二元变量。然后，通过用连续嵌入变量替换这些二元变量，形成一个嵌入式最优控制问题（EOCP）。为解决中间值问题，引入了一个修正的EOCP（MEOCP），通过向主成本函数添加一个适当维度的凹辅助成本函数。", "result": "通过添加凹辅助成本函数，修正后的EOCP（MEOCP）的优化解被保证是bang-bang的。", "conclusion": "MEOCP的bang-bang解对于原始SOCP是可行的，这使得SOCPs能够使用传统技术来解决，同时确保解的实际可行性。"}}
{"id": "2512.12730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12730", "abs": "https://arxiv.org/abs/2512.12730", "authors": ["Jingzhe Ding", "Shengda Long", "Changxin Pu", "Huan Zhou", "Hongwan Gao", "Xiang Gao", "Chao He", "Yue Hou", "Fei Hu", "Zhaojian Li", "Weiran Shi", "Zaiyuan Wang", "Daoguang Zan", "Chenchen Zhang", "Xiaoxu Zhang", "Qizhi Chen", "Xianfu Cheng", "Bo Deng", "Qingshui Gu", "Kai Hua", "Juntao Lin", "Pai Liu", "Mingchen Li", "Xuanguang Pan", "Zifan Peng", "Yujia Qin", "Yong Shan", "Zhewen Tan", "Weihao Xie", "Zihan Wang", "Yishuo Yuan", "Jiayu Zhang", "Enduo Zhao", "Yunfei Zhao", "He Zhu", "Chenyang Zou", "Ming Ding", "Jianpeng Jiao", "Jiaheng Liu", "Minghao Liu", "Qian Liu", "Chongyao Tao", "Jian Yang", "Tong Yang", "Zhaoxiang Zhang", "Xinjie Chen", "Wenhao Huang", "Ge Zhang"], "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents", "comment": null, "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.", "AI": {"tldr": "本文提出了NL2Repo Bench，一个用于评估编码智能体长周期代码库生成能力的基准。研究发现，现有智能体在该任务上表现不佳，揭示了长周期推理是自主编码智能体的主要瓶颈。", "motivation": "现有编码智能体评估基准侧重于局部代码生成或短期修复，未能严格评估构建完整软件系统所需的长期能力，即智能体在扩展时间范围内维持连贯推理、规划和执行的能力。", "method": "研究者设计了NL2Repo Bench基准，要求智能体仅根据一份自然语言需求文档和一个空工作区，自主设计架构、管理依赖、实现多模块逻辑，并生成一个完全可安装的Python库。他们使用该基准评估了最先进的开源和闭源模型。", "result": "实验结果显示，长周期代码库生成问题在很大程度上尚未解决。即使是最强的智能体，平均测试通过率也低于40%，并且很少能正确完成整个代码库。详细分析揭示了主要的长期故障模式，包括过早终止、丧失全局连贯性、脆弱的跨文件依赖以及数百个交互步骤中规划不足。", "conclusion": "NL2Repo Bench建立了一个严格、可验证的测试平台，用于衡量智能体的持续能力，并强调长周期推理是下一代自主编码智能体面临的核心瓶颈。"}}
{"id": "2512.12083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12083", "abs": "https://arxiv.org/abs/2512.12083", "authors": ["Guanfang Dong", "Luke Schultz", "Negar Hassanpour", "Chao Gao"], "title": "RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer", "comment": null, "summary": "The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.", "AI": {"tldr": "本文提出RePack框架，通过将高维视觉基础模型(VFM)表示投影到低维流形，生成更紧凑、解码器友好的表示，以解决其在潜在扩散模型(LDMs)中引起的信息过载问题，从而加速DiT收敛并提升生成性能。", "motivation": "预训练视觉基础模型(VFMs)的强大表示能力被用于增强潜在扩散模型(LDMs)，但其高维表示可能导致信息过载，尤其当VFM特征维度超过原始图像解码尺寸时，这促使研究者寻求一种在保留VFM特征效用的同时避免其高维副作用的方法。", "method": "RePack通过将高维VFM表示投影到低维流形，将其转换为更紧凑、对解码器友好的表示。该方法能有效过滤非语义噪声，同时保留高保真重建所需的核心结构信息。", "result": "实验结果表明，RePack显著加速了Diffusion Transformers (DiTs)的收敛，并优于直接注入原始VFM特征进行图像重建的现有方法。在DiT-XL/2上，RePack仅需64个epoch即可达到3.66的FID分数，比现有最先进方法快35%。", "conclusion": "RePack成功提取了VFM表示的核心语义，同时避免了高维度带来的副作用，证明了其在改善扩散模型性能方面的有效性。"}}
{"id": "2512.12468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12468", "abs": "https://arxiv.org/abs/2512.12468", "authors": ["Tina Tian", "Xinyu Wang", "Andrew L. Orekhov", "Fujun Ruan", "Lu Li", "Oliver Kroemer", "Howie Choset"], "title": "Autonomously Unweaving Multiple Cables Using Visual Feedback", "comment": "6 pages, 5 figures", "summary": "Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.", "AI": {"tldr": "本文提出了一种利用视觉反馈的抓取-放置方法，通过迭代感知-规划-行动过程，成功实现了多根纠缠电缆的解缠，平均成功率达84%。", "motivation": "线缆管理中分离和解开纠缠线缆的任务自动化极具挑战性，因为线缆易变形且可能存在多种打结和交织情况。现有工作主要集中于解开单根线缆的结，而本文则专注于多根交织线缆的解缠这一不同但同样重要的子任务。", "method": "将线缆解缠问题公式化为抓取-放置问题。采用基于图的线缆状态表示，编码来自视觉图像的拓扑和几何信息，并从离散节点中选择抓取位置。提出了一种新颖的状态转换模型，用于预测未来的线缆状态和识别有效动作，该模型考虑了操作过程中线缆的拉直和弯曲。利用此模型，在两种高级动作基元之间进行选择，并计算预测的即时成本以优化低级动作。整个过程是迭代的感知-规划-行动循环。", "result": "实验证明，通过迭代感知-规划-行动过程，该方法能够解缠电缆和鞋带，平均成功率达到84%。", "conclusion": "所提出的基于视觉反馈的迭代感知-规划-行动方法，能够有效地解决多根线缆的解缠问题，为线缆管理自动化提供了一种可行方案。"}}
{"id": "2512.12503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12503", "abs": "https://arxiv.org/abs/2512.12503", "authors": ["Mingrui Ye", "Chanjin Zheng", "Zengyi Yu", "Chenyu Xiang", "Zhixue Zhao", "Zheng Yuan", "Helen Yannakoudakis"], "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.", "AI": {"tldr": "本文介绍了KidsArtBench，一个包含儿童艺术品及其多维度专家标注和评论的新基准，并提出了一种属性特定的多LoRA方法结合回归感知微调（RAFT），显著提高了多模态大语言模型（MLLMs）对儿童艺术表达的评估能力。", "motivation": "多模态大语言模型在艺术表达评估方面能力有限，尤其是在处理抽象、开放性的美学概念时。现有的美学数据集通常只提供成人图像的单一标量分数，缺乏针对儿童艺术品的多维度标注和形成性反馈。", "method": "引入KidsArtBench，一个包含1000多幅儿童艺术品（5-15岁）的新基准，由12位专家教育者根据9个评分维度进行标注，并提供专家评论。在此基础上，提出了一种属性特定的多LoRA方法，每个属性（如真实感、想象力）对应一个独立的评估维度，并结合回归感知微调（RAFT）来使预测与序数刻度对齐。该方法在Qwen2.5-VL-7B模型上进行了验证。", "result": "在Qwen2.5-VL-7B模型上，所提出的方法将相关性从0.468提高到0.653。在感知维度上取得了最大的提升，并缩小了在高阶属性上的差距。这些结果表明，结合教育者对齐的监督和属性感知训练能够产生具有教学意义的评估。", "conclusion": "教育者对齐的监督和属性感知训练可以产生具有教学意义的评估。KidsArtBench和所提出的方法为教育AI在艺术评估领域的持续进步提供了一个严谨的测试平台。"}}
{"id": "2512.12649", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12649", "abs": "https://arxiv.org/abs/2512.12649", "authors": ["Zhewen Zheng", "Wenjing Cao", "Hongkang Yu", "Mo Chen", "Takashi Suzuki"], "title": "Bayesian Optimization Parameter Tuning Framework for a Lyapunov Based Path Following Controller", "comment": null, "summary": "Parameter tuning in real-world experiments is constrained by the limited evaluation budget available on hardware. The path-following controller studied in this paper reflects a typical situation in nonlinear geometric controller, where multiple gains influence the dynamics through coupled nonlinear terms. Such interdependence makes manual tuning inefficient and unlikely to yield satisfactory performance within a practical number of trials. To address this challenge, we propose a Bayesian optimization (BO) framework that treats the closed-loop system as a black box and selects controller gains using a Gaussian-process surrogate. BO offers model-free exploration, quantified uncertainty, and data-efficient search, making it well suited for tuning tasks where each evaluation is costly. The framework is implemented on Honda's AI-Formula three-wheeled robot and assessed through repeated full-lap experiments on a fixed test track. The results show that BO improves controller performance within 32 trials, including 15 warm-start initial evaluations, indicating that it can efficiently locate high-performing regions of the parameter space under real-world conditions. These findings demonstrate that BO provides a practical, reliable, and data-efficient tuning approach for nonlinear path-following controllers on real robotic platforms.", "AI": {"tldr": "本文提出并评估了一种贝叶斯优化（BO）框架，用于在有限评估预算下高效调优真实机器人上的非线性路径跟踪控制器。", "motivation": "在实际实验中，参数调优受限于硬件评估预算。非线性几何控制器（如路径跟踪控制器）的多个增益通过耦合非线性项相互影响，导致手动调优效率低下，难以在实际试验次数内获得满意性能。", "method": "提出了一种贝叶斯优化（BO）框架。该框架将闭环系统视为黑箱，并使用高斯过程代理模型来选择控制器增益。BO提供无模型探索、量化不确定性和数据高效搜索。", "result": "该框架在Honda AI-Formula三轮机器人上实现，并在固定测试赛道上通过重复全圈实验进行评估。结果表明，BO在32次试验（包括15次热启动初始评估）内显著提升了控制器性能，证明其能高效定位参数空间中的高性能区域。", "conclusion": "研究结果表明，贝叶斯优化为真实机器人平台上的非线性路径跟踪控制器提供了一种实用、可靠且数据高效的调优方法。"}}
{"id": "2512.12775", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12775", "abs": "https://arxiv.org/abs/2512.12775", "authors": ["Pedro Henrique Luz de Araujo", "Michael A. Hedderich", "Ali Modarressi", "Hinrich Schuetze", "Benjamin Roth"], "title": "Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions", "comment": "31 pages, 35 figures", "summary": "Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.", "AI": {"tldr": "本研究发现，大型语言模型（LLMs）的角色设定（persona fidelity）在长时间对话中会逐渐下降，尤其是在目标导向的对话中，且存在角色忠诚度与指令遵循之间的权衡。为此，研究引入了一种新的长对话评估协议。", "motivation": "目前，对分配角色的LLMs的评估通常局限于短时、单轮对话，这无法真实反映其在教育、医疗和社科模拟等实际应用中的长期使用情况，因此需要一种能稳健测量长上下文效应的评估方法。", "method": "研究引入了一种结合长角色对话（超过100轮）和评估数据集的评估协议，创建了对话条件基准来测量长上下文效应。随后，利用此协议调查了对话长度对七种最先进的开源和闭源LLMs的角色忠诚度、指令遵循和安全性的影响。", "result": "研究发现，在对话过程中，LLMs的角色忠诚度会下降，尤其是在模型需要同时保持角色忠诚度和指令遵循的目标导向对话中。同时，识别出角色忠诚度与指令遵循之间存在权衡：非角色基线模型最初表现优于角色分配模型；随着对话的进行和角色忠诚度的衰减，角色响应与基线响应变得越来越相似。", "conclusion": "研究结果强调了角色应用在扩展交互中的脆弱性，并提供了一种系统测量此类故障的评估协议。这对于理解和改进LLMs在实际应用中的长期表现至关重要。"}}
{"id": "2512.12548", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12548", "abs": "https://arxiv.org/abs/2512.12548", "authors": ["Yesid Fonseca", "Manuel S. Ríos", "Nicanor Quijano", "Luis F. Giraldo"], "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents", "comment": "14 pages, 6 figures", "summary": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.", "AI": {"tldr": "研究表明，配备学习世界模型的AI觅食者能自然收敛到符合边际价值定理（MVT）的最优觅食策略，这主要由其预测能力而非单纯奖励最大化驱动，与生物觅食者行为相似。", "motivation": "尽管边际价值定理（MVT）常用于描述斑块觅食行为并做出预测，但生物觅食者中实现最优斑块觅食决策的计算机制仍不明确。本研究旨在探索这些机制。", "method": "本研究使用了一种基于模型的强化学习（RL）智能体，该智能体能够学习其环境的预测性世界模型。通过比较其斑块离开行为与MVT模型和标准无模型RL智能体，来评估其决策模式。", "result": "结果显示，配备学习世界模型的人工觅食者能够自然地收敛到与MVT一致的策略。高效的斑块离开行为主要由其预测能力驱动，而非仅仅是奖励最大化。这些基于模型的智能体展现出与许多生物觅食者相似的决策模式。", "conclusion": "研究表明，预测性世界模型可以作为AI系统中更具解释性和生物学基础的决策制定的基础。生态最优性原则对于推动可解释和自适应AI具有重要价值。"}}
{"id": "2512.12552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12552", "abs": "https://arxiv.org/abs/2512.12552", "authors": ["Jifei Liu", "Zhi Chen", "Yuanguang Zhong"], "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms", "comment": null, "summary": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在决策中复制并放大了人类认知偏差，尤其是在动态报童问题中。更复杂的模型（如GPT-4）表现出更大的非理性，而效率优化的模型（如GPT-4o）表现接近最优。这些偏差源于模型架构而非知识缺陷。", "motivation": "LLMs日益融入商业决策，但其复制和放大人类认知偏差的潜力构成重大且未被充分理解的风险，尤其是在供应链管理等高风险运营环境中。本研究旨在识别LLMs认知偏差的性质和来源。", "method": "通过与GPT-4、GPT-4o和LLaMA-8B进行的动态、多轮实验，研究人员使用经典的报童问题测试了五种既定的决策偏差。", "result": "LLMs持续复制了经典的“过低/过高”订购偏差，并显著放大了追逐需求等其他行为，相比人类基准更为明显。分析揭示了“智能悖论”：更复杂的GPT-4通过过度思考表现出最大的非理性，而效率优化的GPT-4o表现接近最优。这些偏差即使在提供最优公式后依然存在，表明它们源于架构限制而非知识空白。", "conclusion": "首先，管理者应根据具体任务选择模型，因为效率优化的模型在某些优化问题上可能优于更复杂的模型。其次，LLMs显著放大偏差的特性凸显了在高风险决策中迫切需要强有力的人机协作监督。第三，设计结构化、基于规则的提示是管理者限制模型启发式倾向并提高AI辅助决策可靠性的实用有效策略。"}}
{"id": "2512.13004", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13004", "abs": "https://arxiv.org/abs/2512.13004", "authors": ["Muhammad Sarwar", "Muhammad Rizwan", "Mubushra Aziz", "Abdul Rehman Sudais"], "title": "Large Language Models for Power System Applications: A Comprehensive Literature Survey", "comment": "17 pages, 1 table, 1 figure", "summary": "This comprehensive literature review examines the emerging applications of Large Language Models (LLMs) in power system engineering. Through a systematic analysis of recent research published between 2020 and 2025, we explore how LLMs are being integrated into various aspects of power system operations, planning, and management. The review covers key application areas including fault diagnosis, load forecasting, cybersecurity, control and optimization, system planning, simulation, and knowledge management. Our findings indicate that while LLMs show promising potential in enhancing power system operations through their advanced natural language processing and reasoning capabilities, significant challenges remain in their practical implementation. These challenges include limited domain-specific training data, concerns about reliability and safety in critical infrastructure, and the need for enhanced explainability. The review also highlights emerging trends such as the development of power system-specific LLMs and hybrid approaches combining LLMs with traditional power engineering methods. We identify crucial research directions for advancing the field, including the development of specialized architectures, improved security frameworks, and enhanced integration with existing power system tools. This survey provides power system researchers and practitioners with a comprehensive overview of the current state of LLM applications in the field and outlines future pathways for research and development.", "AI": {"tldr": "该文献综述系统分析了2020年至2025年间大型语言模型（LLMs）在电力系统工程中的新兴应用，涵盖故障诊断、负荷预测、网络安全、控制优化、系统规划、仿真和知识管理等领域，并指出其前景和挑战。", "motivation": "研究动机是系统地审视大型语言模型（LLMs）如何被整合到电力系统运营、规划和管理的各个方面，以了解其新兴应用和潜在影响。", "method": "通过对2020年至2025年间发表的最新研究进行系统性文献分析，来探讨LLMs在电力系统中的应用。", "result": "研究发现，LLMs在增强电力系统运营方面展现出巨大潜力，但其实际实施面临挑战，包括领域特定训练数据有限、关键基础设施的可靠性和安全性问题，以及可解释性需求。新兴趋势包括开发电力系统专用LLMs和结合传统电力工程方法的混合方法。", "conclusion": "LLMs在电力系统领域具有广阔的应用前景，但需解决数据、可靠性和可解释性等挑战。未来的研究方向应包括开发专业架构、改进安全框架以及加强与现有电力系统工具的集成。"}}
{"id": "2512.12770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12770", "abs": "https://arxiv.org/abs/2512.12770", "authors": ["Thales Sales Almeida", "Rodrigo Nogueira", "Hélio Pedrini"], "title": "Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining", "comment": null, "summary": "Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu", "AI": {"tldr": "本研究通过Curió 7B和Curió-Edu 7B模型，探讨了语言模型持续预训练中数据质量的重要性，发现即使数据量和计算资源较少，高质量的数据选择也能显著提升模型在特定语言适应任务中的性能。", "motivation": "持续预训练是使通用语言模型适应新环境的有效替代方案。研究旨在探究在语言适应中，数据量是否足够，抑或数据质量是否扮演决定性角色，尤其是在大规模葡萄牙语模型持续预训练中。", "method": "研究基于LLaMA-2模型，开发了两个70亿参数的葡萄牙语模型：Curió 7B使用ClassiCC-PT语料库的1000亿个葡萄牙语token进行训练；Curió-Edu 7B则仅使用该语料库中经过教育和STEM领域过滤的100亿个token进行训练。", "result": "Curió-Edu 7B模型尽管仅使用了Curió 7B十分之一的数据量和五分之一的计算资源，但在评估中却超越了使用完整语料库训练的Curió 7B模型。这表明即使模型对目标语言的先验暴露有限，数据选择仍然至关重要。", "conclusion": "数据选择在语言模型的持续预训练和语言适应中具有根本性的作用。高质量的数据即使数量较少，也能带来比大量但未经筛选的数据更好的性能表现。"}}
{"id": "2512.12972", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12972", "abs": "https://arxiv.org/abs/2512.12972", "authors": ["Zhongda Chu", "Fei Teng"], "title": "Headroom as A Grid Service in Software-Defined Power Grids: A Peak-to-Peak Control Design Approach", "comment": null, "summary": "To address system frequency challenges driven by the integration of renewable generation, advanced control strategies are designed at the device level to provide effective frequency support following disturbances. However, typically relying on energy-based performance metrics, these methods cannot guarantee the system frequency constraints such as frequency nadir and maximum Rate-of-Change-of-Frequency (RoCoF). Moreover, locally-designed frequency support cannot minimize the overall system cost to maintain frequency stability. On the other hand, the concept of frequency-constrained system scheduling is introduced, which incorporates frequency dynamic constraints into the system economic optimization, so that frequency requirements can be maintained with minimum cost. However, these works rely on analytical approximations of the frequency dynamic metrics, which are mathematically complicated and tend to be over-conservative for the approximation of IBR headroom requirements.", "AI": {"tldr": "本文指出当前用于可再生能源并网的频率支持策略存在局限性，包括设备级控制无法保证频率约束和系统成本最优，以及频率约束系统调度依赖于过度保守的解析近似。", "motivation": "可再生能源并网对系统频率稳定性构成挑战。现有设备级频率控制策略无法有效保证频率下垂和最大频率变化率（RoCoF）等关键频率约束，也无法实现系统成本最优。同时，现有的频率约束系统调度方法依赖于复杂且保守的解析近似，导致对逆变器（IBR）裕度需求估计过高。", "method": "文章讨论了两种主要方法：1. 设备级高级控制策略，通常基于能量性能指标设计，用于在扰动后提供频率支持。2. 频率约束系统调度，将频率动态约束纳入系统经济优化，以最低成本维持频率要求。", "result": "设备级控制策略无法保证系统频率约束（如频率下垂和RoCoF），也无法最小化系统维护频率稳定性的总成本。频率约束系统调度方法依赖于数学上复杂的解析近似，且在估算IBR裕度需求时倾向于过于保守。", "conclusion": "为有效应对可再生能源并网带来的频率挑战，需要新的控制和调度策略，这些策略既能保证系统频率约束（如频率下垂和RoCoF），又能最小化系统总成本，同时避免使用过度复杂和保守的频率动态指标解析近似。"}}
{"id": "2512.12089", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12089", "abs": "https://arxiv.org/abs/2512.12089", "authors": ["Zihu Wang", "Boxun Xu", "Yuxuan Xia", "Peng Li"], "title": "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering", "comment": null, "summary": "Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.", "AI": {"tldr": "大型视觉-语言模型（LVLMs）因视觉注意力分散而产生幻觉。本文提出VEGAS方法，通过在解码时将视觉编码器更集中的注意力图注入到语言模型的中间层，有效抑制了幻觉，并达到了最先进的性能。", "motivation": "LVLMs在视觉和文本推理方面表现出色，但常产生语言流畅但与视觉证据不符的幻觉。研究旨在探究何种形式的视觉注意力能有效抑制解码过程中的幻觉。", "method": "研究发现LVLMs在最终视觉注意力图未能集中于关键图像对象时容易产生幻觉，而视觉编码器更集中的注意力图能显著减少幻觉。通过分析，发现视觉-文本冲突在语言模型中间层达到峰值。基于此，提出VEGAS方法，这是一种简单的推理时方法，将视觉编码器的注意力图注入到语言模型的中间层，并自适应地引导那些未能集中于关键图像对象的token。", "result": "在多个基准测试中，VEGAS方法持续展现出最先进的减少幻觉性能。", "conclusion": "将视觉编码器集中的注意力图注入到语言模型的中间层，能够有效抑制LVLMs中的幻觉。VEGAS方法提供了一种简单而有效的方式来解决这一问题。"}}
{"id": "2512.12717", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12717", "abs": "https://arxiv.org/abs/2512.12717", "authors": ["Mattia Catellani", "Marta Gabbi", "Lorenzo Sabattini"], "title": "HMPCC: Human-Aware Model Predictive Coverage Control", "comment": null, "summary": "We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.", "AI": {"tldr": "该论文提出了一种名为HMPCC的人员感知模型预测控制（MPC）框架，用于机器人团队在未知环境中进行覆盖，通过整合人员运动预测来确保安全操作并避免与非合作代理碰撞，且无需显式通信。", "motivation": "传统的覆盖策略通常依赖于简化的假设（如已知或凸形环境、静态密度函数），难以适应真实世界场景，尤其是在有人类参与的情况下，这促使研究人员寻求更适应动态、未知和有人环境的解决方案。", "method": "本文提出了一种基于模型预测控制（MPC）的人员感知覆盖框架（HMPCC），将人员运动预测整合到规划过程中。环境被建模为高斯混合模型（GMM），机器人团队以完全去中心化的方式运行，不依赖显式通信。", "result": "研究结果表明，人员轨迹预测能够实现更高效和自适应的覆盖，并改善了人机代理之间的协调。", "conclusion": "HMPCC框架通过整合人员运动预测，使得机器人团队能够在未知且动态的环境中，以去中心化和无通信的方式，更安全、高效和自适应地执行覆盖任务，特别是在有人类参与的场景中表现出色。"}}
{"id": "2512.12090", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12090", "abs": "https://arxiv.org/abs/2512.12090", "authors": ["Samar Fares", "Nurbek Tastan", "Karthik Nandakumar"], "title": "SPDMark: Selective Parameter Displacement for Robust Video Watermarking", "comment": null, "summary": "The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.", "AI": {"tldr": "随着高质量视频生成模型的出现，急需鲁棒的水印方案来检测和追踪生成视频的来源。SPDMark提出了一种基于视频扩散模型选择性参数位移（利用LoRA）的新型生成内视频水印框架，实现了水印的不可感知性、鲁棒性和计算效率。", "motivation": "高质量视频生成模型的普及，使得可靠检测和追踪生成视频来源的需求日益增长。然而，现有的视频水印方法（无论是后处理还是生成内方法）都未能同时实现水印的不可感知性、鲁棒性和计算效率。", "method": "本文提出了一种名为SPDMark（SpeedMark）的生成内视频水印框架。它通过修改生成模型参数的子集来嵌入水印，将位移建模为层级基准偏移的加性组合，并利用低秩适应（LoRA）实现参数高效的基准偏移。在训练阶段，通过最小化消息恢复、感知相似度和时间一致性损失，联合学习基准偏移和水印提取器。为检测和定位时间篡改，使用加密哈希函数从基本水印密钥导出帧特定水印消息。在水印提取时，应用最大二分匹配来恢复即使经过时间篡改的视频的正确帧顺序。", "result": "SPDMark在文本到视频和图像到视频生成模型上的评估表明，它能够生成不可感知的水印，并以高精度恢复。同时，它对各种常见的视频修改表现出鲁棒性，包括抵抗时间篡改。", "conclusion": "SPDMark通过其独特的选择性参数位移方法，成功解决了现有视频水印方案在不可感知性、鲁棒性和计算效率方面的不足，为追踪生成视频的来源提供了一个有效且鲁棒的解决方案，即使面对时间篡改也能保持水印的完整性。"}}
{"id": "2512.12101", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12101", "abs": "https://arxiv.org/abs/2512.12101", "authors": ["Swarn S. Warshaneyan", "Maksims Ivanovs", "Blaž Cugmas", "Inese Bērziņa", "Laura Goldberga", "Mindaugas Tamosiunas", "Roberts Kadiķis"], "title": "AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging", "comment": "10 pages, 10 figures, 2 tables, 22 references. Journal submission undergoing peer review", "summary": "We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.", "AI": {"tldr": "本研究全面探讨了在光学和数字全息显微镜（DIHM）图像中全自动识别花粉的方法，并提出使用GAN进行数据增强以提高DIHM图像上的识别性能。", "motivation": "在未重建的全息图像中识别花粉具有挑战性，因为存在散斑噪声、双像伪影以及与明场图像的显著差异。研究旨在克服这些挑战，推动DIHM在兽医成像等领域实现全自动工作流程。", "method": "研究使用YOLOv8s进行目标检测，MobileNetV3L进行分类。数据集包含自动标注的光学图像和对齐的DIHM图像。为改善DIHM图像中的目标检测，采用具有谱归一化的Wasserstein GAN (WGAN-SN) 生成合成DIHM图像，并通过混合真实和合成数据进行增强。", "result": "在光学数据上，检测mAP50达到91.3%，分类准确率达到97%。而在原始DIHM数据上，检测mAP50仅为8.15%，分类准确率为50%。通过扩展DIHM图像中的边界框，检测mAP50提升至13.3%，分类准确率提升至54%。WGAN-SN生成的合成DIHM图像FID分数为58.246。以1.0:1.5的比例混合真实和合成DIHM数据后，目标检测性能提高到15.4%。", "conclusion": "研究结果表明，基于GAN的数据增强可以有效缩小光学和DIHM图像之间性能差距，使全自动DIHM工作流程在兽医成像等实际应用中迈出了虽小但重要的一步。"}}
{"id": "2512.13021", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13021", "abs": "https://arxiv.org/abs/2512.13021", "authors": ["Mo Yang", "Jing Yu", "Necmiye Ozay"], "title": "Safe Control of Multi-Agent Systems with Minimal Communication", "comment": "to appear at 2025 IEEE Conference on Decision and Control (CDC)", "summary": "In many multi-agent systems, communication is limited by bandwidth, latency, and energy constraints. Designing controllers that achieve coordination and safety with minimal communication is critical for scalable and reliable deployment. This paper presents a method for designing controllers that minimize inter-agent communication in multi-agent systems while satisfying safety and coordination requirements, while conforming to communication delay constraints. The control synthesis problem is cast as a rank minimization problem, where a convex relaxation is obtained via system level synthesis. Simulation results on various tasks, including trajectory tracking with relative and heterogeneous sensing, demonstrate that the proposed method significantly reduces inter-agent transmission compared to baseline approaches.", "AI": {"tldr": "本文提出了一种多智能体系统控制器设计方法，旨在最小化智能体间通信，同时满足安全、协调和通信延迟约束。", "motivation": "多智能体系统中带宽、延迟和能源限制了通信，因此设计能以最少通信实现协调和安全的控制器对于可扩展和可靠部署至关重要。", "method": "将控制综合问题建模为秩最小化问题，并通过系统级综合获得其凸松弛解。", "result": "通过在轨迹跟踪等任务上的仿真结果表明，与基线方法相比，所提出的方法显著减少了智能体间的通信传输。", "conclusion": "该方法成功地在满足安全、协调和通信延迟要求的同时，显著降低了多智能体系统中的通信开销。"}}
{"id": "2512.12777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12777", "abs": "https://arxiv.org/abs/2512.12777", "authors": ["Mosh Levy", "Zohar Elyoseph", "Shauli Ravfogel", "Yoav Goldberg"], "title": "State over Tokens: Characterizing the Role of Reasoning Tokens", "comment": null, "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.", "AI": {"tldr": "大型语言模型（LLMs）的推理标记并非其真实推理过程的忠实解释，而是一种外部化的计算状态。研究应将其解码为状态而非文本。", "motivation": "LLMs在最终答案前生成的推理标记能提升复杂任务性能，但经验证据表明它们并非模型实际推理过程的忠实解释。存在外观（像人类思维）与功能之间的差距。", "method": "引入“State over Tokens (SoT)”概念框架。该框架将推理标记重新定义为一种外部化的计算状态，是模型无状态生成周期中唯一的持久信息载体。", "result": "SoT解释了推理标记如何在不作为文本忠实解释的情况下驱动正确推理，并揭示了此前被忽视的关于这些标记的研究问题。", "conclusion": "为了真正理解LLMs的推理过程，研究必须超越将推理标记作为文本阅读，而应专注于将其解码为状态。"}}
{"id": "2512.12722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12722", "abs": "https://arxiv.org/abs/2512.12722", "authors": ["Tarik Viehmann", "Daniel Swoboda", "Samridhi Kalra", "Himanshu Grover", "Gerhard Lakemeyer"], "title": "Making Robots Play by the Rules: The ROS 2 CLIPS-Executive", "comment": null, "summary": "CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.", "AI": {"tldr": "本文将规则编程语言CLIPS集成到ROS生态系统中，并展示了其与PDDL规划框架的灵活集成。", "motivation": "CLIPS是一种适用于构建知识驱动型应用的规则编程语言，特别适合协调自主机器人等复杂任务。受Fawkes机器人框架中CLIPS-Executive的启发，作者旨在将CLIPS引入更广泛使用的ROS生态系统。", "method": "主要方法是将CLIPS集成到ROS生态系统中。此外，还通过描述一个基于PDDL的规划框架集成来展示CLIPS的灵活性。", "result": "成功地将CLIPS集成到ROS生态系统中。通过展示与PDDL规划框架的集成，证明了CLIPS在机器人应用中的灵活性和潜力。", "conclusion": "CLIPS可以有效地集成到ROS中，并能灵活地与PDDL等规划框架协同工作，从而为构建知识驱动的机器人协调应用提供一个强大的工具。"}}
{"id": "2512.12632", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12632", "abs": "https://arxiv.org/abs/2512.12632", "authors": ["Rishit Agnihotri", "Sandeep Kumar Sharma"], "title": "Optimized Conflict Management for Urban Air Mobility Using Swarm UAV Networks", "comment": "Preprint. Under review for conference submission", "summary": "Urban Air Mobility (UAM) poses unprecedented traffic coordination challenges, especially with increasing UAV densities in dense urban corridors. This paper introduces a mathematical model using a control algorithm to optimize an Edge AI-driven decentralized swarm architecture for intelligent conflict resolution, enabling real-time decision-making with low latency. Using lightweight neural networks, the system leverages edge nodes to perform distributed conflict detection and resolution. A simulation platform was developed to evaluate the scheme under various UAV densities. Results indicate that the conflict resolution time is dramatically minimized up to 3.8 times faster, and accuracy is enhanced compared to traditional centralized control models. The proposed architecture is highly promising for scalable, efficient, and safe aerial traffic management in future UAM systems.", "AI": {"tldr": "本文提出了一种基于边缘AI的去中心化蜂群架构，利用轻量级神经网络和控制算法，优化城市空中交通(UAM)中的无人机冲突解决，实现低延迟实时决策。", "motivation": "城市空中交通(UAM)带来了前所未有的交通协调挑战，尤其是在密集的城市走廊中无人机密度不断增加，需要智能、实时的冲突解决方案。", "method": "研究采用了一个数学模型和控制算法，以优化由边缘AI驱动的去中心化蜂群架构。该系统利用边缘节点上的轻量级神经网络进行分布式冲突检测和解决。开发了一个仿真平台来评估该方案在不同无人机密度下的性能。", "result": "结果表明，与传统的集中式控制模型相比，该方案将冲突解决时间显著缩短了多达3.8倍，并提高了准确性。", "conclusion": "所提出的架构在未来的UAM系统中，对于可扩展、高效和安全的空中交通管理具有巨大潜力。"}}
{"id": "2512.13099", "categories": ["eess.SY", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.13099", "abs": "https://arxiv.org/abs/2512.13099", "authors": ["Julien Allard", "Noé Diffels", "François Vallée", "Bertrand Cornélusse", "Zacharie De Grève"], "title": "On the Complementarity of Shared Electric Mobility and Renewable Energy Communities", "comment": null, "summary": "Driven by the ongoing energy transition, shared mobility service providers are emerging actors in electrical power systems which aim to shift combustion-based mobility to electric paradigm. In the meantime, Energy Communities are deployed to enhance the local usage of distributed renewable production. As both ators share the same goal of satisfying the demand at the lowest cost, they could take advantage of their complementarity and coordinate their decisions to enhance each other operation. This paper presents an original Mixed-Integer Second Order Cone Programming long-term Electric Vehicle fleet planning optimization problem that integrates the coordination with a Renewable Energy Community and Vehicle-to-Grid capability. This model is used to assess the economic, energy, and grid performances of their collaboration in a 21 buses low-voltage distribution network. Key results show that, both actors coordination can help reducing the yearly cost up to 11.3 % compared to their stand-alone situation and that it may reduce the stress on the substation transformer by 46 % through the activation of the inherent EVs flexibility when subject to peak penalties from the grid operator.", "AI": {"tldr": "本文提出一个电动汽车车队规划优化模型，整合了与可再生能源社区的协调及车网互动（V2G）能力，旨在评估其协作在经济、能源和电网性能方面的效益。", "motivation": "能源转型推动共享出行服务商向电动出行转变，同时能源社区致力于提高分布式可再生能源的本地利用率。由于两者都以最低成本满足需求为目标，因此存在通过协调决策相互增强运营的潜力。", "method": "本文提出了一个原创的混合整数二阶锥规划（MISOCP）长期电动汽车车队规划优化问题，该模型整合了与可再生能源社区的协调以及车网互动（V2G）能力。该模型在一个21总线低压配电网络中用于评估其协作的经济、能源和电网性能。", "result": "关键结果表明，与独立运营相比，两者的协调可以将年度成本降低高达11.3%；并且在电网运营商施加高峰惩罚时，通过激活电动汽车固有的灵活性，可以将变电站变压器的压力降低46%。", "conclusion": "共享电动汽车车队与可再生能源社区之间的协调，结合车网互动能力，能够显著降低运营成本，并有效缓解电网压力，从而实现经济、能源和电网性能的多重提升。"}}
{"id": "2512.12107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12107", "abs": "https://arxiv.org/abs/2512.12107", "authors": ["Yuheng Li", "Yue Zhang", "Abdoul Aziz Amadou", "Yuxiang Lai", "Jike Zhong", "Tiziano Passerini", "Dorin Comaniciu", "Puneet Sharma"], "title": "EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography", "comment": null, "summary": "Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.", "AI": {"tldr": "本文介绍了EchoGround-MIMIC数据集和EchoVLM模型，旨在解决超声心动图解读中缺乏大规模临床图像-文本数据和基于测量推理的挑战，并实现了端到端超声心动图解读的最新性能。", "motivation": "超声心动图解读劳动密集且本质上是多模态的，需要视图识别、定量测量、定性评估和基于指南的推理。现有视觉-语言模型（VLMs）在超声心动图领域的潜力受限于缺乏大规模、临床基础的图像-文本数据集，以及缺少超声心动图解读核心的基于测量的推理能力。", "method": "研究引入了EchoGround-MIMIC，首个基于测量的多模态超声心动图数据集，包含19,065对图像-文本。在此基础上，提出了EchoVLM，一个视觉-语言模型，整合了两个新颖的预训练目标：(i) 视图感知对比损失，用于编码超声心动图图像的视图依赖结构；(ii) 否定感知对比损失，用于区分临床上关键的阴性与阳性发现。", "result": "EchoVLM在五种临床应用（涵盖36项任务，包括多模态疾病分类、图像-文本检索、视图分类、心腔分割和地标检测）中取得了最先进的性能，例如在零样本疾病分类中达到86.5%的AUC，在视图分类中达到95.1%的准确率。研究表明，临床基础的多模态预训练产生了可迁移的视觉表示。", "conclusion": "临床基础的多模态预训练能产生可迁移的视觉表示。EchoVLM被确立为端到端超声心动图解读的基础模型。研究团队将发布EchoGround-MIMIC数据集和数据整理代码，以促进可复现性和多模态超声心动图解读领域的进一步研究。"}}
{"id": "2512.12634", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12634", "abs": "https://arxiv.org/abs/2512.12634", "authors": ["Youngmin Im", "Byeongung Jo", "Jaeyoung Wi", "Seungwoo Baek", "Tae Hoon Min", "Joo Hyung Lee", "Sangeun Oh", "Insik Shin", "Sunjae Lee"], "title": "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents", "comment": null, "summary": "Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.", "AI": {"tldr": "本文提出MobiBench，一个模块化、多路径感知的离线基准测试框架，用于评估移动GUI代理，解决了现有方法在可扩展性、可复现性和组件分析方面的局限性。", "motivation": "当前的GUI代理评估存在两大局限：一是依赖单路径离线或不可扩展/不可复现的在线基准，导致对有效替代动作的误判或评估困难；二是将代理视为黑盒，忽略了组件贡献，导致不公平比较或掩盖性能瓶颈。", "method": "本文提出了MobiBench，这是首个模块化且多路径感知的离线基准测试框架。它允许在离线环境中进行高保真、可扩展和可复现的评估，并能对代理的各个组件进行模块级分析。", "result": "MobiBench与人类评估者达到了94.72%的一致性，与精心设计的在线基准相当，同时保持了静态离线基准的可扩展性和可复现性。模块级分析揭示了关键见解，包括对不同技术系统评估、最佳模块配置、当前LFM的局限性以及设计更强大、更经济高效的移动代理的实用指南。", "conclusion": "MobiBench成功克服了现有移动GUI代理评估的局限性，提供了一种高保真、可扩展、可复现且支持模块级分析的离线评估方案。其发现为未来移动代理的设计提供了宝贵的指导。"}}
{"id": "2512.12597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12597", "abs": "https://arxiv.org/abs/2512.12597", "authors": ["Miriam Horovicz"], "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "comment": null, "summary": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.", "AI": {"tldr": "AgentSHAP是首个用于解释大型语言模型（LLM）代理中工具重要性的框架，它采用模型无关的蒙特卡洛Shapley值方法，以解决现有可解释性AI（XAI）方法无法解释工具贡献的问题。", "motivation": "LLM代理使用外部工具解决复杂任务，但目前尚不清楚哪些工具对最终响应做出了贡献。现有的XAI方法无法提供工具层面的解释，这是一个盲点。", "method": "AgentSHAP将代理视为黑盒，通过蒙特卡洛Shapley值计算工具重要性。它通过测试代理在不同工具子集下的响应，并基于博弈论公平地计算重要性分数，将计算成本从O(2^n)降低到实用水平。", "result": "在API-Bank上的综合实验表明，AgentSHAP在不同运行中产生一致的分数，能正确识别关键工具，并区分相关和不相关工具。", "conclusion": "AgentSHAP是第一个针对LLM代理工具归因的可解释性方法，它基于博弈论中的Shapley值，并与TokenSHAP和PixelSHAP一起，完善了基于Shapley值的现代生成式AI的XAI工具系列。"}}
{"id": "2512.12652", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.12652", "abs": "https://arxiv.org/abs/2512.12652", "authors": ["Nardine Osman"], "title": "Value-Aware Multiagent Systems", "comment": null, "summary": "This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.", "AI": {"tldr": "本文提出了AI中“价值意识”的概念，超越传统价值对齐问题，并提供了一个包含三个核心支柱的工程化路线图：学习和表示人类价值观、确保价值对齐以及提供基于价值观的可解释性。", "motivation": "超越传统的价值对齐问题，提出更全面的“价值意识”概念，以指导AI的工程化设计。", "method": "构建了一个由三个核心支柱组成的路线图：1) 使用形式语义学习和表示人类价值观；2) 确保单个智能体和多智能体系统的价值对齐；3) 提供基于价值观的行为可解释性。", "result": "展示了作者在上述部分主题上的正在进行的工作，并将其应用于现实生活领域。", "conclusion": "该论文定义了AI中的价值意识，并提出了一个简明且简化的工程化路线图，通过三个核心支柱指导价值意识AI的开发。"}}
{"id": "2512.12842", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12842", "abs": "https://arxiv.org/abs/2512.12842", "authors": ["Kuan Fang", "Yuxin Chen", "Xinghao Zhu", "Farzad Niroui", "Lingfeng Sun", "Jiuguang Wang"], "title": "SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding", "comment": "9 pages, 7 figures", "summary": "We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.", "AI": {"tldr": "SAGA是一个通用的视觉运动控制框架，通过解耦高级语义意图和低级控制，并利用多模态基础模型进行可供性接地，实现了跨环境、任务和用户规范的泛化，并在真实世界任务中表现出色。", "motivation": "开发一个能够跨各种环境、任务目标和用户规范进行泛化的视觉运动控制框架，并高效学习这种能力。", "method": "SAGA通过将高级语义意图与低级视觉运动控制解耦，并将任务目标明确地与观察到的环境相结合。它使用基于可供性的任务表示来统一表达多样复杂的行为。通过利用多模态基础模型，SAGA将任务表示映射到机器人视觉观察中的3D可供性热图，从而突出任务相关实体并抽象掉无关的外观变化。这些接地的可供性使得能够有效地训练一个条件策略，用于多任务演示数据上的全身控制。SAGA可以处理语言指令、选定点和示例演示等不同形式的任务规范。", "result": "SAGA能够解决以不同形式（包括语言指令、选定点和示例演示）指定的任务，实现了零样本执行和少样本适应。在四足机械臂上进行的广泛实验（涵盖11个真实世界任务）表明，SAGA始终以显著优势超越了端到端和模块化基线。", "conclusion": "这些结果表明，结构化的可供性接地为实现通用移动操作提供了一条可扩展且有效的途径。"}}
{"id": "2512.12818", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12818", "abs": "https://arxiv.org/abs/2512.12818", "authors": ["Chris Latimer", "Nicoló Boschi", "Andrew Neeser", "Chris Bartholomew", "Gaurav Srivastava", "Xuan Wang", "Naren Ramakrishnan"], "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects", "comment": null, "summary": "Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.", "AI": {"tldr": "Hindsight是一种新型的LLM智能体记忆架构，它将记忆组织成四个逻辑网络，并支持保留、回忆和反思操作，显著提升了智能体在长期对话记忆任务中的推理能力和表现。", "motivation": "当前智能体记忆系统将记忆视为外部层，导致证据与推断混淆、长期信息组织困难以及对推理可解释性支持不足，限制了LLM智能体在积累经验、跨会话适应和超越单次问答方面的成长。", "method": "Hindsight将智能体记忆视为结构化的、一流的推理基质，将其组织成四个逻辑网络（世界事实、智能体经验、合成实体摘要、演变信念）。它支持保留、回忆和反思三种核心操作，通过时间性、实体感知记忆层将对话流转化为可查询的记忆库，并通过反思层对记忆库进行推理以生成答案并以可追溯的方式更新信息。", "result": "Hindsight结合开源20B模型，在LongMemEval基准测试中将整体准确率从39%提升至83.6%，并超越了完整上下文的GPT-4o。进一步扩展骨干模型，Hindsight在LongMemEval上达到91.4%，在LoCoMo上达到89.61%（而此前最强的开放系统为75.78%），持续优于现有记忆架构在多会话和开放域问题上的表现。", "conclusion": "Hindsight通过其结构化的记忆架构和推理框架，有效解决了现有记忆系统的局限性，在长期对话记忆基准测试中取得了显著的性能提升，表明将记忆作为一流的推理基质是LLM智能体发展的关键方向。"}}
{"id": "2512.13229", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13229", "abs": "https://arxiv.org/abs/2512.13229", "authors": ["Mischa Huisman", "Erjen Lefeber", "Nathan van de Wouw", "Carlos Murguia"], "title": "Plant Equivalent Controller Realizations for Attack-Resilient Cyber-Physical Systems", "comment": null, "summary": "As cyber-physical systems (CPSs) become more dependent on data and communication networks, their vulnerability to false data injection (FDI) attacks has raised significant concerns. Among these, stealthy attacks, those that evade conventional detection mechanisms, pose a critical threat to closed-loop performance. This paper introduces a controller-oriented method to enhance CPS resiliency against such attacks without compromising nominal closed-loop behavior. Specifically, we propose the concept of plant equivalent controller (PEC) realizations, representing a class of dynamic output-feedback controllers that preserve the input-output behavior of a given base controller while exhibiting distinct robustness properties in the presence of disturbances and sensor attacks. To quantify and improve robustness, we employ reachable set analysis to assess the impact of stealthy attacks on the closed-loop dynamics. Building on this analysis, we provide mathematical tools (in terms of linear matrix inequalities) to synthesize the optimal PEC realization that minimizes the reachable set under peak-bounded disturbances. The proposed framework thus provides systematic analysis and synthesis tools to enhance the attack resilience of CPSs while maintaining the desired nominal performance. The effectiveness of the approach is demonstrated on the quadruple-tank process subject to stealthy sensor attacks.", "AI": {"tldr": "本文提出了一种以控制器为中心的方法，通过引入“设备等效控制器（PEC）”实现，增强网络物理系统（CPS）对隐蔽虚假数据注入（FDI）攻击的弹性，同时不影响标称闭环性能。该方法利用可达集分析和线性矩阵不等式（LMI）工具来合成最优PEC，以最小化攻击影响。", "motivation": "网络物理系统（CPS）对数据和通信网络的依赖性日益增强，使其易受虚假数据注入（FDI）攻击。其中，能够规避传统检测机制的隐蔽攻击对闭环性能构成严重威胁，激发了研究者开发增强CPS韧性的方法。", "method": "本文提出了设备等效控制器（PEC）实现的概念，这是一种动态输出反馈控制器，能在保持给定基础控制器输入输出行为的同时，展现出不同的鲁棒性。为量化和提高鲁棒性，研究采用可达集分析来评估隐蔽攻击对闭环动力学的影响。在此基础上，提供了基于线性矩阵不等式（LMI）的数学工具，用于合成最优PEC实现，以最小化在峰值有界扰动下的可达集。", "result": "所提出的框架提供了一套系统的分析和合成工具，能够增强CPS的攻击弹性，同时保持所需的标称性能。该方法的有效性通过在受隐蔽传感器攻击的四罐过程上的实验得到了验证。", "conclusion": "该研究提供了一种控制器导向的方法，通过最优的PEC实现，能够系统地增强CPS对隐蔽攻击的弹性，同时不牺牲其预期的标称运行性能。"}}
{"id": "2512.12108", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12108", "abs": "https://arxiv.org/abs/2512.12108", "authors": ["Dashti A. Ali", "Aras T. Asaad", "Jacob J. Peoples", "Mohammad Hamghalam", "Alex Robins", "Mane Piliposyan", "Richard K. G. Do", "Natalie Gangai", "Yun S. Chun", "Ahmad Bashir Barekzai", "Jayasree Chakraborty", "Hala Khasawneh", "Camila Vilela", "Natally Horvat", "João Miranda", "Alice C. Wei", "Amber L. Simpson"], "title": "A Novel Patch-Based TDA Approach for Computed Tomography", "comment": null, "summary": "The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.", "AI": {"tldr": "本研究提出了一种新颖的基于补丁的持久同源性（PH）构建方法，专门用于三维CT医学图像分析。该方法在分类性能和时间效率上均优于传统的三维立方复形算法，并提供了方便的Python包。", "motivation": "医学影像领域的机器学习模型需要强大的特征工程来提高性能。拓扑数据分析（TDA）及其核心工具持久同源性（PH）能从数据中提取深层拓扑特征。然而，传统用于三维CT图像的立方复形过滤方法在性能和计算复杂度上可能存在局限性，尤其对于高分辨率图像。", "method": "本研究引入了一种新颖的、为体积医学影像数据（特别是CT模态）量身定制的基于补丁的PH构建方法。通过在多个三维CT图像数据集上进行广泛实验，系统分析了该方法的性能，并与传统的三维立方复形算法进行了基准测试。此外，还提供了一个名为Patch-TDA的Python包以方便使用。", "result": "研究结果表明，基于补丁的TDA方法在分类性能和时间效率方面均表现出色。与立方复形方法相比，该方法在所有数据集上平均实现了准确率10.38%、AUC 6.94%、敏感性2.06%、特异性11.58%和F1分数8.51%的提升。", "conclusion": "所提出的基于补丁的PH方法在三维CT图像分析中，无论是在分类性能还是时间效率方面，均显著优于传统的三维立方复形方法。该研究为医学影像领域的机器学习模型提供了更有效和高效的特征工程工具，并提供了易于使用的Python包。"}}
{"id": "2512.12686", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12686", "abs": "https://arxiv.org/abs/2512.12686", "authors": ["Samarth Sarin", "Lovepreet Singh", "Bhaskarjit Sarmah", "Dhagash Mehta"], "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI", "comment": "Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India", "summary": "Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.", "AI": {"tldr": "该论文提出了Memoria，一个模块化记忆框架，通过动态会话级摘要和基于加权知识图谱的用户建模引擎，为大型语言模型（LLM）提供持久、可解释且上下文丰富的智能体记忆，从而实现可扩展的个性化对话式AI。", "motivation": "当前LLM在扩展用户交互中缺乏连续性、个性化和长期上下文能力，限制了它们作为真正交互式和自适应智能体的部署。需要一种“智能体记忆”来使LLM能够跨对话保留和利用信息，实现类似人类的持久性。", "method": "Memoria框架整合了两个互补组件：1) 动态会话级摘要，用于短期对话连贯性；2) 一个基于加权知识图谱（KG）的用户建模引擎，用于逐步捕捉用户特征、偏好和行为模式，以实现长期个性化。这种混合架构在LLM的token限制内运行，结合了短期连贯性和长期个性化。", "result": "Memoria通过弥合无状态LLM接口与智能体记忆系统之间的鸿沟，实现了可扩展、个性化的对话式AI。它为需要自适应和不断演进用户体验的行业应用提供了一个实用的解决方案。", "conclusion": "Memoria框架通过其混合架构（会话摘要和KG用户建模）成功地为LLM提供了智能体记忆，解决了LLM在长期交互中缺乏连续性和个性化的问题，提供了一个实用且可扩展的解决方案，以实现更具适应性的对话式AI。"}}
{"id": "2512.12128", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12128", "abs": "https://arxiv.org/abs/2512.12128", "authors": ["Thomas Manzini", "Priyankari Perali", "Raisa Karnik", "Robin R. Murphy"], "title": "A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery", "comment": "11 pages, 6 figures, 6 tables. To appear AAAI'26", "summary": "This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\\% Macro IoU. If spatial alignment is not considered, approximately 8\\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.", "AI": {"tldr": "本文提出了一个最大的灾后道路损伤评估和道路对齐基准数据集（CRASAR-U-DRIODs），包含18个基线模型，并解决了现有数据集的局限性，特别强调了道路线对齐对模型性能的重要性。", "motivation": "现有灾后道路损伤评估数据集规模小或图像分辨率低，不足以检测应急管理人员关注的现象；已开发的机器学习系统缺乏操作验证；实践中观察到道路线未对齐导致模型性能下降，且未对齐的道路线可能导致大量错误标注和实际道路偏离。", "method": "构建了CRASAR-U-DRIODs数据集，包含来自10个联邦宣布灾害的657.25公里道路的灾后小型无人机（sUAS）图像，并按照10类标注方案进行标记。训练了18个基线机器学习模型，并在2024年黛比和海伦飓风的行动响应中进行了部署。为解决道路线空间对齐问题，提供了9,184次道路线调整。", "result": "CRASAR-U-DRIODs是目前最大的道路损伤评估和道路对齐基准数据集。18个基线模型在实际操作中得到了部署。当模型部署在未对齐的道路线上时，平均宏观IoU性能下降5.596%。如果不考虑空间对齐，大约8%（11公里）的道路不良状况将被错误标记，大约9%（59公里）的道路线将偏离实际道路。", "conclusion": "道路线对齐是灾后道路损伤评估中的一个关键挑战，机器学习、计算机视觉和机器人社区应解决这些动态问题，以实现更有效和知情的灾害决策。"}}
{"id": "2512.12812", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12812", "abs": "https://arxiv.org/abs/2512.12812", "authors": ["Hanyu Cai", "Binqi Shen", "Lier Jin", "Lan Hu", "Xiaojing Fan"], "title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA", "comment": null, "summary": "Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.\n  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.", "AI": {"tldr": "本研究系统评估了交互语气（友好、中性、粗鲁）对GPT、Gemini和Llama三大模型在不同任务上准确性的影响，发现语气敏感性因模型和领域而异，尤其在人文任务中，粗鲁语气会降低GPT和Llama的准确性，但总体而言，现代大型语言模型对语气变化具有较强的鲁棒性。", "motivation": "提示工程对大型语言模型（LLM）性能至关重要，但语用元素（如语言语气和礼貌程度）的影响，尤其是在不同模型家族之间的影响，尚未得到充分探索。", "method": "提出一个系统评估框架，用于检验交互语气如何影响模型准确性。使用GPT-4o mini、Gemini 2.0 Flash和Llama 4 Scout三个模型，在MMMLU基准测试的六个STEM和人文领域任务上，评估了“非常友好”、“中性”和“非常粗鲁”三种提示变体下的模型性能，并通过统计显著性检验分析了准确性差异。", "result": "语气敏感性既依赖于模型，也依赖于领域。中性或非常友好的提示通常比非常粗鲁的提示产生更高的准确性，但统计显著性影响仅出现在部分人文任务中：粗鲁语气会降低GPT和Llama的准确性，而Gemini对语气相对不敏感。当在每个领域内聚合任务性能时，语气效应减弱并大多失去统计显著性。研究还表明数据集规模和覆盖范围显著影响语气效应的检测。", "conclusion": "虽然交互语气在特定解释性设置中可能很重要，但现代大型语言模型在典型的混合领域使用中，对语气变化普遍具有鲁棒性。这为实际部署中的提示设计和模型选择提供了实用指导。"}}
{"id": "2512.12945", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12945", "abs": "https://arxiv.org/abs/2512.12945", "authors": ["Anja Sheppard", "Parker Ewen", "Joey Wilson", "Advaith V. Sethuraman", "Benard Adewole", "Anran Li", "Yuzhen Chen", "Ram Vasudevan", "Katherine A. Skinner"], "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework", "comment": "Accepted into R-AL", "summary": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.", "AI": {"tldr": "本文提出SLIM-VDB，一个轻量级语义建图系统，利用OpenVDB数据结构和统一的贝叶斯更新框架，实现封闭集和开放集语义融合，显著降低内存和集成时间，同时保持相当的建图精度。", "motivation": "现有的语义建图系统在计算和内存效率方面有待提高，未充分利用OpenVDB等先进数据结构进行语义建图；此外，现有系统缺乏在单一框架内整合固定类别和开放语言标签预测的能力。", "method": "本文提出了SLIM-VDB，一个新颖的3D语义建图系统，它利用OpenVDB数据结构，并整合了一个统一的贝叶斯更新框架，以实现封闭集和开放集语义的融合。", "result": "SLIM-VDB与当前最先进的语义建图方法相比，显著减少了内存消耗和集成时间，同时保持了可比的建图精度。", "conclusion": "SLIM-VDB成功提供了一个高效且多功能的语义建图解决方案，能够支持封闭集和开放集词典的语义融合，并在效率上表现出色。"}}
{"id": "2512.13263", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13263", "abs": "https://arxiv.org/abs/2512.13263", "authors": ["Yi Luo", "Luping Xiang", "Cheng Luo", "Kun Yang", "Shida Zhong", "Jienan Chen"], "title": "An End-to-End Neural Network Transceiver Design for OFDM System with FPGA-Accelerated Implementation", "comment": null, "summary": "The evolution toward sixth-generation (6G) wireless networks demands high-performance transceiver architectures capable of handling complex and dynamic environments. Conventional orthogonal frequency-division multiplexing (OFDM) receivers rely on cascaded discrete Fourier transform (DFT) and demodulation blocks, which are prone to inter-stage error propagation and suboptimal global performance. In this work, we propose two neural network (NN) models DFT-Net and Demodulation-Net (Demod-Net) to jointly replace the IDFT/DFT and demodulation modules in an OFDM transceiver. The models are trained end-to-end (E2E) to minimize bit error rate (BER) while preserving operator equivalence for hybrid deployment. A customized DFT-Demodulation Net Accelerator (DDNA) is further developed to efficiently map the proposed networks onto field-programmable gate array (FPGA) platforms. Leveraging fine-grained pipelining and block matrix operations, DDNA achieves high throughput and flexibility under stringent latency constraints. Experimental results show that the DL-based transceiver consistently outperforms the conventional OFDM system across multiple modulation schemes. With only a modest increase in hardware resource usage, it achieves approximately 1.5 dB BER gain and up to 66\\% lower execution time.", "AI": {"tldr": "本文提出两种神经网络模型（DFT-Net和Demod-Net）来联合替代OFDM收发器中的IDFT/DFT和解调模块，并通过定制的硬件加速器（DDNA）在FPGA上实现，显著提升了6G无线网络的性能（BER和执行时间）。", "motivation": "第六代（6G）无线网络需要高性能的收发器架构来处理复杂动态的环境。传统的OFDM接收器依赖于级联的DFT和解调模块，容易出现级间错误传播和次优的全局性能。", "method": "研究提出DFT-Net和Demod-Net两种神经网络模型，用于端到端（E2E）地替代OFDM收发器中的IDFT/DFT和解调模块，以最小化误码率（BER）并保持操作符等效性。此外，开发了定制的DFT-Demodulation Net Accelerator (DDNA) 来高效地将这些网络映射到FPGA平台，利用细粒度流水线和块矩阵操作实现高吞吐量和灵活性。", "result": "实验结果表明，基于深度学习的收发器在多种调制方案下始终优于传统OFDM系统。在硬件资源仅适度增加的情况下，实现了约1.5 dB的BER增益，并将执行时间缩短高达66%。", "conclusion": "所提出的基于深度学习的OFDM收发器结合定制的硬件加速器，在BER性能和执行时间方面均显著优于传统系统，为6G无线网络提供了高性能的解决方案。"}}
{"id": "2512.12692", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12692", "abs": "https://arxiv.org/abs/2512.12692", "authors": ["Mahir Labib Dihan", "Tanzima Hashem", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment", "comment": "Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/", "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.", "AI": {"tldr": "WebOperator是一个树搜索框架，通过引入可靠的回溯机制和战略性探索，解决了LLM代理在部分可观察的Web环境中缺乏远见和难以纠正错误的问题，显著提高了任务成功率。", "motivation": "LLM代理在Web环境中通常采取贪婪的、一步一步的行动方式，缺乏长远考量和回溯能力，导致难以纠正错误或系统性探索。现有的树搜索方法缺乏安全回溯机制，且假设所有操作均可逆，限制了其在真实Web任务中的有效性。", "method": "WebOperator采用最佳优先树搜索策略，结合奖励估算和安全考量来对行动进行排序。它包含一个鲁棒的回溯机制，能在重放路径前验证其可行性，防止意外副作用。此外，它从多个推理上下文生成多样化的候选行动，并在执行前过滤无效行动并合并语义等价的行动，以指导探索。", "result": "WebOperator在WebArena上使用gpt-4o达到了54.6%的最新成功率，证明了将战略远见与安全执行相结合的关键优势。", "conclusion": "WebOperator通过其树搜索框架、可靠的回溯机制和战略性探索策略，成功解决了LLM代理在Web环境中缺乏远见和难以纠正错误的挑战，显著提升了代理在复杂Web任务中的性能和成功率。"}}
{"id": "2512.12793", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12793", "abs": "https://arxiv.org/abs/2512.12793", "authors": ["Mizuho Aoki", "Kohei Honda", "Yasuhiro Yoshimura", "Takeshi Ishita", "Ryo Yonetani"], "title": "VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps", "comment": null, "summary": "This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.", "AI": {"tldr": "VLG-Loc是一种新颖的全局定位方法，它利用视觉语言模型和人类可读的标注地标地图，在蒙特卡洛定位框架中实现机器人定位，在环境变化下表现出比现有方法更强的鲁棒性，并可通过融合进一步提升。", "motivation": "人类可以利用仅包含地标名称和区域的地图进行定位，但机器人由于缺乏几何和外观细节，难以在观察到的地标与地图中的地标之间建立对应关系。现有的基于扫描的方法对环境变化敏感，因此需要一种更鲁棒的定位方法。", "method": "本文提出了视觉语言全局定位（VLG-Loc）方法。它使用仅包含地标名称和区域的人类可读标注足迹地图。该方法利用视觉语言模型（VLM）在机器人多方向图像观测中搜索地图中注明地标。然后，在蒙特卡洛定位（MCL）框架内识别机器人位姿，并利用找到的地标评估每个位姿假设的似然。此外，通过视觉和基于扫描的定位的概率融合实现进一步改进。", "result": "在模拟和真实零售环境中的实验验证表明，VLG-Loc比现有基于扫描的方法具有更优越的鲁棒性，尤其是在环境变化下。通过视觉和基于扫描的定位的概率融合，实现了进一步的性能提升。", "conclusion": "VLG-Loc成功地将人类利用简单地标地图进行定位的能力转化为机器人系统，提供了一种在环境变化下高度鲁棒的全局定位解决方案，并且可以通过与其他传感模式的融合来增强其性能。"}}
{"id": "2512.13233", "categories": ["eess.SY", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2512.13233", "abs": "https://arxiv.org/abs/2512.13233", "authors": ["Mojtaba Joodaki", "Idriz Pelaj"], "title": "Measurement of Material Volume Fractions in a Microwave Resonant Cavity Sensor Using Convolutional Neural Network", "comment": null, "summary": "A non-destructive, real-time method for estimating the volume fraction of a dielectric mixture inside a resonant cavity is presented. A convolutional neural network (CNN)-based approach is used to estimate the fractional composition of two-phase dielectric mixtures inside a resonant cavity using scattering parameter (S-parameter) measurements. A rectangular cavity sensor with a strip feed structure is characterized using a vector network analyzer (VNA) from 0.01--20~GHz. The CNN is trained using both simulated and experimentally measured S-parameters and achieves high predictive accuracy even without de-embedding or filtering, demonstrating robustness to measurement imperfections. The simulation results achieve a coefficient of determination ($R^2$)=0.99 using $k$-fold cross-validation, while the experimental model using raw data achieves an $R^2=0.94$ with a mean absolute error (MAE) below 6\\%. Data augmentation further improves the accuracy of the experimental prediction to above $R^2=0.998$ (MAE$<$0.72\\%). The proposed method enables rapid, non-destructive, accurate, low-cost, and real-time estimation of material fractions, illustrating strong potential for sensing applications in microwave material characterization.", "AI": {"tldr": "该研究提出了一种基于卷积神经网络（CNN）和散射参数（S-参数）的非破坏性、实时方法，用于估算谐振腔内两相介电混合物的体积分数，并在模拟和实验数据上均取得了高精度。", "motivation": "当前对材料组分估算的方法可能存在破坏性、耗时或成本高等问题。本研究旨在开发一种快速、非破坏性、准确、低成本且实时的材料组分估算方法，以满足微波材料表征传感应用的需求。", "method": "研究使用一个带有带状馈电结构的矩形谐振腔传感器，通过矢量网络分析仪（VNA）在0.01-20 GHz范围内测量散射参数（S-参数）。然后，采用基于卷积神经网络（CNN）的方法，利用模拟和实验测量的S-参数训练CNN模型来估算介电混合物的分数组成。此外，还采用了数据增强技术以进一步提高模型精度。", "result": "该方法在没有去嵌入或滤波的情况下，仍能实现高预测精度，显示出对测量缺陷的鲁棒性。模拟结果的决定系数（R^2）达到0.99；使用原始数据的实验模型R^2为0.94，平均绝对误差（MAE）低于6%。通过数据增强，实验预测的精度进一步提高，R^2超过0.998，MAE低于0.72%。", "conclusion": "所提出的方法能够实现快速、非破坏性、准确、低成本和实时的材料组分估算。它在微波材料表征的传感应用中展现出巨大的潜力。"}}
{"id": "2512.12142", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.12142", "abs": "https://arxiv.org/abs/2512.12142", "authors": ["Björn Lütjens", "Patrick Alexander", "Raf Antwerpen", "Til Widmann", "Guido Cervone", "Marco Tedesco"], "title": "MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater", "comment": null, "summary": "The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as \"ground truth\", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.", "AI": {"tldr": "该研究开发了一种深度学习模型，通过融合多源遥感和物理模型数据，生成格陵兰冰盖每日100米分辨率的地表融水地图，显著提高了地图的精度。", "motivation": "格陵兰冰盖融化速度加快，但其过程尚不完全清楚且难以测量。现有融水地图在时间或空间分辨率上存在取舍，无法同时达到高分辨率，这限制了对融化过程的理解。", "method": "开发了一个深度学习模型，通过融合合成孔径雷达（SAR）、被动微波（PMW）、数字高程模型（DEM）数据以及区域气候模型（RCM）输出，对RCM结果进行时空降尺度处理。研究区域为格陵兰东部的赫尔海姆冰川，时间跨度为2017-2023年。使用SAR衍生的融水作为“地面真值”，并评估了UNet和DeepLabv3+等标准深度学习方法。同时，发布了对齐数据集MeltwaterBench作为基准。", "result": "基于深度学习的数据融合方法在研究区域的融水地图精度达到95%，比仅依赖区域气候模型（83%）或被动微波观测（72%）的现有非深度学习方法高出10个百分点以上。另一种不依赖深度学习的SAR数据滑动窗口计算方法也达到了90%的准确率，但低估了极端融化事件。", "conclusion": "深度学习模型结合多源数据流能够显著提高格陵兰冰盖地表融水地图的时空分辨率和精度。所创建的基准数据集MeltwaterBench将促进未来数据驱动的降尺度方法研究。"}}
{"id": "2512.12839", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12839", "abs": "https://arxiv.org/abs/2512.12839", "authors": ["Dingyi Yang", "Qin Jin"], "title": "What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation", "comment": "24 pages, 7 figures, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics", "summary": "In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.", "AI": {"tldr": "本研究系统性地探索了长篇故事（>10万词）的自动评估，引入了大规模基准LongStoryEval，分析了读者关注的评估维度，比较了不同评估方法，并提出了表现优于GPT-4o的NovelCritique模型。", "motivation": "自动评估书本长度故事（超过10万词）是一个具有挑战性的领域。研究旨在解决两个关键问题：理解读者最看重哪些评估方面，以及探索评估长篇故事的有效方法。", "method": "1. 建立了首个大规模基准LongStoryEval，包含600本平均12.1万词的新出版书籍，附带平均评分和多条读者评论。2. 通过分析用户提及的评估方面，提出了评估标准结构，并实验识别了8个顶级标准中最重要的方面。3. 比较了三种评估方法的有效性：基于聚合的、增量更新的和基于摘要的评估。4. 基于研究发现，提出了8B模型NovelCritique，该模型利用高效的基于摘要的框架来评论和评分故事。", "result": "1. 成功识别了对读者最重要的评估方面。2. 基于聚合和基于摘要的评估表现更好，前者擅长细节评估，后者效率更高。3. NovelCritique模型在与人类评估的一致性方面优于GPT-4o等商业模型。", "conclusion": "本研究为长篇故事的自动评估提供了深入见解，确立了有效的评估方法，并开发了高性能的NovelCritique模型和大型基准数据集LongStoryEval，为该领域未来的研究奠定了基础。"}}
{"id": "2512.12868", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12868", "abs": "https://arxiv.org/abs/2512.12868", "authors": ["Furong Jia", "Yuan Pu", "Finn Guo", "Monica Agrawal"], "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM", "comment": null, "summary": "Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.", "AI": {"tldr": "本研究通过引入一种基于频率的概率排序器（FBPR），探讨了大型语言模型（LLMs）在临床诊断基准测试中的表现是否反映了底层的概率推理。结果显示，FBPR的性能与LLMs相当，且两者在解决问题上互补，表明LLMs的性能并非简单地源于频率聚合，但简单的概率方法仍能解释很大一部分基准性能。", "motivation": "尽管大型语言模型在多项选择临床诊断基准测试中表现出色，但尚不清楚其性能在多大程度上反映了底层的概率推理能力。本研究旨在通过“选择最可能诊断”的任务来探究这一点。", "method": "研究使用了MedQA的问题集，并引入了频率基概率排序器（FBPR）。FBPR是一种轻量级方法，它利用来自大型语料库的概念-诊断共现统计数据，通过平滑的朴素贝叶斯算法对选项进行评分。共现统计数据来源于OLMo和Llama的预训练语料库。研究将FBPR的性能与直接的LLM推理进行了比较。", "result": "当共现统计数据来源于OLMo和Llama的预训练语料库时，FBPR的性能与在相同语料库上预训练的相应LLMs相当。直接的LLM推理和FBPR在正确回答的问题上大多不同，重叠程度仅略高于随机机会，这表明两种方法具有互补的优势。", "conclusion": "研究结果强调了明确的概率基线（如FBPR）的持续价值：它们提供了一个有意义的性能参考点和潜在混合的补充信号。尽管LLMs的性能似乎是由除简单频率聚合之外的机制驱动的，但本研究表明，类似于历史上基于低复杂度的专家系统的方法仍然解释了基准性能的很大一部分。"}}
{"id": "2512.13331", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13331", "abs": "https://arxiv.org/abs/2512.13331", "authors": ["Martina Vinetti", "Sabino Francesco Roselli", "Martin Fabian"], "title": "A Multi-Worker Assembly Line Rebalancing with Spatial and Ergonomic Considerations", "comment": null, "summary": "This work addresses the Assembly Line Rebalancing Problem in manual assembly systems where multiple workers operate in parallel within the same station - an industrially relevant scenario that remains insufficiently explored in the literature. A multi-objective optimization model is proposed that incorporates task reassignment, worker allocation, ergonomic evaluation, and explicit spatial feasibility through work-area constraints. The formulation minimizes deviations from the current configuration while promoting balanced workload and ergonomic conditions among workers. Computational experiments on synthetic problem instances demonstrate that the model consistently generates feasible and human-centered reconfigurations across varying cycle-time conditions, highlighting its potential as a decision-support tool for industrial rebalancing in flexible production environments.", "AI": {"tldr": "本研究提出了一种多目标优化模型，用于解决多工人并行操作装配线中的平衡问题，该模型综合考虑了任务分配、工人分配、人体工程学和空间可行性，旨在生成可行且以人为本的重新配置方案。", "motivation": "在手动装配系统中，多个工人在同一工位并行操作是一种工业上常见的场景，但在现有文献中对此类装配线平衡问题的探索尚不充分。", "method": "提出了一种多目标优化模型，该模型整合了任务重新分配、工人分配、人体工程学评估以及通过工作区域约束实现的空间可行性。该模型旨在最小化与当前配置的偏差，同时促进工人之间工作负载和人体工程学条件的平衡。", "result": "在合成问题实例上的计算实验表明，该模型能够在不同的节拍时间条件下，持续生成可行且以人为本的重新配置方案。", "conclusion": "该模型具有作为柔性生产环境中工业再平衡决策支持工具的潜力。"}}
{"id": "2512.12855", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12855", "abs": "https://arxiv.org/abs/2512.12855", "authors": ["Patrick Kostelac", "Xuerui Wang", "Anahita Jamshidnejad"], "title": "MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems", "comment": null, "summary": "Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.", "AI": {"tldr": "本文提出了一种集成MPC-RL框架，结合了MPC的稳定性和安全保障与RL的适应性，通过在训练中利用MPC定义安全边界指导RL学习，并在部署时使用轻量级安全滤波器确保实时约束满足，从而实现对复杂工程系统的安全、自适应控制。", "motivation": "现代工程系统（如自动驾驶、柔性机器人、智能航空平台）需要控制器具备对不确定性的鲁棒性、对环境变化的适应性以及实时约束下的安全性。强化学习（RL）虽能提供数据驱动的适应性，但缺乏动态约束满足机制；模型预测控制（MPC）虽能处理约束并提供鲁棒性，但依赖精确模型且在线优化计算量大，难以满足实时需求。", "method": "本文提出一个集成的MPC-RL框架。在训练阶段，MPC定义安全控制边界，指导RL组件进行约束感知的策略学习。在部署阶段，学习到的策略通过基于Lipschitz连续性的轻量级安全滤波器实时运行，以确保约束满足，避免了繁重的在线优化。", "result": "该方法在一个非线性气动弹性机翼系统上进行了验证，结果表明其改善了扰动抑制能力，减少了执行器功耗，并在湍流条件下表现出鲁棒性能。该架构可推广到其他具有结构化非线性和有界扰动的领域。", "conclusion": "该集成MPC-RL架构通过结合MPC的稳定性和安全保障与RL的适应性，为工程应用中安全的人工智能驱动控制提供了一个可扩展的解决方案。"}}
{"id": "2512.12146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12146", "abs": "https://arxiv.org/abs/2512.12146", "authors": ["Ayush Vaibhav Bhatti", "Deniz Karakay", "Debottama Das", "Nilotpal Rajbongshi", "Yuito Sugimoto"], "title": "Open Horizons: Evaluating Deep Models in the Wild", "comment": null, "summary": "Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.", "AI": {"tldr": "本文对开放集识别（OSR）和少样本类增量学习（FSCIL）进行了统一的实验研究，比较了不同骨干网络和评分函数在CIFAR-10上的性能，并揭示了它们对未知类别检测和灾难性遗忘的影响。", "motivation": "在开放世界部署中，模型需要既能识别已知类别，又能在新类别出现时保持可靠性。", "method": "针对OSR，比较了ResNet-50、ConvNeXt-Tiny和CLIP ViT-B/16三种预训练冻结视觉编码器，结合线性探测和MSP、Energy、Mahalanobis、kNN四种后验评分函数，使用AUROC、AUPR、FPR@95和OSCR等指标进行评估。针对FSCIL，比较了改进的SPPR、OrCo和ConCM方法，使用部分冻结的ResNet-50，并在1、5、10样本设置下进行评估。", "result": "在OSR中，CLIP在已知和未知样本分离方面表现最强，Energy在不同骨干网络上提供最稳定的性能。在FSCIL中，ConCM在10样本设置下达到84.7%的准确率，并产生最清晰的混淆矩阵，但所有方法在超过5个样本后均出现饱和。", "conclusion": "受控评估揭示了骨干网络架构和评分机制如何影响未知类别检测，以及基于原型的方法如何减轻增量适应过程中的灾难性遗忘。"}}
{"id": "2512.12165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12165", "abs": "https://arxiv.org/abs/2512.12165", "authors": ["Daniel Adebi", "Sagnik Majumder", "Kristen Grauman"], "title": "Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video", "comment": null, "summary": "Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.", "AI": {"tldr": "该研究提出了一种音视频框架，利用被动场景声音辅助相机姿态估计，尤其在视觉信息受损时表现出鲁棒性，首次成功将音频用于真实世界视频中的相对相机姿态估计。", "motivation": "理解相机运动是具身感知和3D场景理解的基础问题。然而，纯视觉方法在运动模糊或遮挡等视觉退化条件下常常失效。", "method": "引入了一个简单但有效的音视频框架，将到达方向（DOA）频谱和双耳嵌入集成到最先进的纯视觉姿态估计算法中。", "result": "在两个大型数据集上的结果表明，该方法相比强大的纯视觉基线模型有持续的性能提升，并在视觉信息受损时表现出鲁棒性。这是首次成功利用音频在真实世界视频中进行相对相机姿态估计的工作。", "conclusion": "偶然的日常音频信号是解决经典空间挑战（相机姿态估计）的一个意想不到但有前景的信号。"}}
{"id": "2512.12993", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12993", "abs": "https://arxiv.org/abs/2512.12993", "authors": ["Guillermo A. Castillo", "Himanshu Lodha", "Ayonga Hereid"], "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations", "comment": null, "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.", "AI": {"tldr": "本文提出了一种分层策略，通过将降维感知表示与强化学习（RL）高层策略相结合，实现地形感知的双足机器人运动，并利用CNN-VAE生成潜在地形编码以优化决策过程。", "motivation": "现有端到端方法在实时步态生成方面存在局限性，研究旨在通过整合降维感知表示来增强基于RL的高层策略，以实现地形感知的双足机器人运动。", "method": "采用分层策略，将CNN-VAE生成的潜在地形编码与降阶机器人动力学相结合，形成紧凑状态来优化运动决策。系统分析了潜在空间维度对学习效率和策略鲁棒性的影响。通过整合近期地形观测序列扩展为历史感知方法，并引入蒸馏方法直接从深度相机图像学习潜在表示。通过模拟和真实传感器数据进行初步硬件验证，并在Agility Robotics (AR) 模拟器中进行高保真验证。", "result": "结果证实了该方法的鲁棒性和适应性，突显了其在硬件部署方面的潜力。", "conclusion": "该分层地形感知双足机器人运动方法具有鲁棒性和适应性，并通过利用潜在地形编码和历史感知机制，展现出在真实世界硬件部署的巨大潜力。"}}
{"id": "2512.12193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12193", "abs": "https://arxiv.org/abs/2512.12193", "authors": ["Xuancheng Xu", "Yaning Li", "Sisi You", "Bing-Kun Bao"], "title": "SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation", "comment": null, "summary": "Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.", "AI": {"tldr": "SMRABooth是一种定制化视频生成方法，通过引入自监督编码器和光流编码器提供对象级主体和运动表示，并结合稀疏LoRA注入策略，有效解决了现有方法在主体外观相似性和运动模式一致性方面的不足，实现了可控的文生视频。", "motivation": "现有定制化视频生成方法难以同时确保主体外观相似性和运动模式一致性，原因在于缺乏针对主体和运动的对象级指导。", "method": "本文提出了SMRABooth，包含三个核心阶段：1) 利用自监督编码器获取主体表示，指导主体对齐，捕捉主体整体结构并增强高级语义一致性。2) 利用光流编码器获取运动表示，捕捉结构连贯且独立于外观的对象级运动轨迹。3) 提出主体-运动关联解耦策略，通过在不同位置和时间应用稀疏LoRA注入，有效减少主体和运动LoRA之间的干扰。这些表示在LoRA微调过程中与模型对齐。", "result": "广泛的实验表明，SMRABooth在主体和运动定制方面表现出色，能够保持一致的主体外观和运动模式。", "conclusion": "SMRABooth通过对象级的主体和运动表示以及主体-运动关联解耦策略，有效解决了定制化视频生成中的关键挑战，在可控文生视频方面展现了其有效性。"}}
{"id": "2512.12967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12967", "abs": "https://arxiv.org/abs/2512.12967", "authors": ["Weizhou Shen", "Ziyi Yang", "Chenliang Li", "Zhiyuan Lu", "Miao Peng", "Huashan Sun", "Yingcheng Shi", "Shengyi Liao", "Shaopeng Lai", "Bo Zhang", "Dayiheng Liu", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "comment": null, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "AI": {"tldr": "QwenLong-L1.5通过系统性的后训练创新，在长上下文推理能力上取得了卓越表现，其性能可与GPT-5和Gemini-2.5-Pro媲美，并在超长上下文任务中表现出色。", "motivation": "现有模型在长上下文推理方面存在局限，尤其是在处理需要多跳推理和超长序列的任务时。研究旨在开发一种能够超越简单检索，实现真正长程推理能力的新模型。", "method": "1. **长上下文数据合成管道**：开发了一个系统性合成框架，通过分解文档和程序化组合可验证推理问题，生成需要多跳推理的高质量训练数据。2. **长上下文训练的稳定强化学习**：引入了带有任务特定优势估计的任务平衡采样来缓解奖励偏差，并提出了自适应熵控制策略优化（AEPO）来动态调节探索-利用权衡。3. **超长上下文的记忆增强架构**：开发了一个记忆管理框架，结合多阶段融合RL训练，将单次推理与基于迭代记忆的处理无缝集成，以处理超过4M tokens的任务。", "result": "QwenLong-L1.5（基于Qwen3-30B-A3B-Thinking）在长上下文推理基准测试中，性能与GPT-5和Gemini-2.5-Pro相当，平均超越其基线9.90分。在1M~4M tokens的超长任务中，其记忆代理框架比代理基线提高了9.48分。此外，所获得的长上下文推理能力也提升了科学推理、记忆工具使用和扩展对话等通用领域的性能。", "conclusion": "QwenLong-L1.5通过创新的数据合成、稳定的强化学习和记忆增强架构，显著提升了长上下文推理能力，达到了领先水平，并对通用领域性能产生积极影响。"}}
{"id": "2512.12706", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12706", "abs": "https://arxiv.org/abs/2512.12706", "authors": ["Enhong Mu", "Minami Yoda", "Yan Zhang", "Mingyue Zhang", "Yutaka Matsuno", "Jialong Li"], "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning", "comment": null, "summary": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.", "AI": {"tldr": "本文提出了SMART框架，结合LLM解释代码差异和强化学习代理，用于游戏更新的自动化测试，平衡了结构覆盖和功能验证，显著提高了修改代码的分支覆盖率和任务完成率。", "motivation": "“游戏即服务”模式下频繁的内容更新给质量保证带来了巨大压力。现有自动化测试方法要么只关注代码结构而忽略游戏玩法，要么只验证高级意图而忽视底层代码变化，无法有效应对游戏更新测试的挑战。", "method": "SMART框架通过大型语言模型（LLM）解释抽象语法树（AST）差异以提取功能意图，构建上下文感知的混合奖励机制。该机制指导强化学习（RL）代理顺序完成游戏目标，同时自适应地探索修改过的代码分支，从而协同结构验证和功能验证。", "result": "在Overcooked和Minecraft两个环境中，SMART显著优于现有基线方法。它实现了超过94%的修改代码分支覆盖率（是传统强化学习方法的近两倍），同时保持了98%的任务完成率，有效地平衡了结构完整性和功能正确性。", "conclusion": "SMART框架成功弥合了代码中心和玩家中心自动化测试方法的差距，为游戏更新测试提供了一个高效且全面的解决方案，能够同时确保修改代码的结构覆盖和游戏功能的正确性。"}}
{"id": "2512.12804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12804", "abs": "https://arxiv.org/abs/2512.12804", "authors": ["Sander Beckers"], "title": "Causal Counterfactuals Reconsidered", "comment": "Preprint: currently under review", "summary": "I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions", "AI": {"tldr": "本文提出了一种新的反事实概率语义，它推广了佩尔（Pearl）的标准语义，适用于无法扩展为现实结构因果模型（SCM）的概率因果模型，并为佩尔和戴维（Dawid）关于反事实的长期争论提供了一个折衷方案。", "motivation": "佩尔的传统反事实语义仅限于可扩展为现实SCM的因果模型，但作者指出，即使在简单场景中也会出现超出此范围的概率因果模型，因此需要更通用的语义。此外，佩尔与戴维之间关于反事实的哲学争论需要一个能平衡双方观点的解决方案。", "method": "作者开发了一种新的反事实概率语义，它推广了佩尔的语义，适用于不限于现实SCM的概率因果模型。该方法限制在满足马尔可夫条件、只包含现实变量且因果完备的模型。尽管像佩尔一样使用结构因果模型，但避免使用“响应变量”。作者还证明了其语义与另外两种不涉及SCM的近期提议是等价的，并与文献中更广泛的随机反事实观点一致。", "result": "所提出的新语义成功地推广了佩尔的反事实语义，能够处理超出佩尔语义范围的概率因果模型。这些模型即使在简单设置中也普遍存在。该语义在佩尔和戴维的争论中提供了一个自然的折衷，即在拒绝普遍因果决定论和非现实变量的同时，仍能实现通用的反事实语义。此外，该语义被证明与不涉及SCM的其他近期提议等价，并与更广泛的随机反事实文献相符。", "conclusion": "即使在放弃普遍因果决定论和非现实变量的前提下，通过推广佩尔的语义并处理更广泛的概率因果模型，仍然可以建立通用的反事实语义。本文提出的新语义不仅填补了现有理论的空白，还为佩尔和戴维之间的争论提供了一个可行的解决方案，并与不依赖SCM的其他方法保持一致。"}}
{"id": "2512.13393", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13393", "abs": "https://arxiv.org/abs/2512.13393", "authors": ["Mohammad Reza Fasihi", "Brian L. Mark"], "title": "QoS-Aware State-Augmented Learnable Framework for 5G NR-U/Wi-Fi Coexistence: Impact of Parameter Selection and Enhanced Collision Resolution", "comment": "10 pages, 6 figures, 2 tables", "summary": "Unlicensed spectrum supports diverse traffic with stringent Quality-of-Service (QoS) requirements. In NR-U/Wi-Fi coexistence,the values of MAC parameters critically influence delay, collision behavior, and airtime fairness and efficiency. In this paper, we investigate the impact of (i) cost scaling and violation modeling, (ii) choice of MAC parameters, and (iii) an enhanced collision resolution scheme for the Listen-Before-Talk (LBT) mechanism on the performance of a state-augmented constrained reinforcement learning controller for NR-U/Wi-Fi coexistence. Coexistence control is formulated as a constrained Markov decision process with an explicit delay constraint for high-priority traffic and fairness as the optimization goal. Our simulation results show three key findings: (1) signed, threshold-invariant cost scaling with temporal smoothing stabilizes learning and strengthens long-term constraint adherence; (2) use of the contention window parameter for control provides smoother adaptation and better delay compliance than other MAC parameters; and (3) enhanced LBT significantly reduces collisions and improves airtime efficiency. These findings provide practical insights for achieving robust, QoS-aware coexistence control.", "AI": {"tldr": "本文研究了在NR-U/Wi-Fi共存场景中，MAC参数、成本建模和增强型LBT机制对基于受限强化学习控制器的性能影响，旨在实现稳健、QoS感知的共存控制。", "motivation": "非授权频谱支持具有严格QoS要求的多样化流量，而NR-U/Wi-Fi共存中MAC参数对延迟、碰撞行为和空口时间公平性及效率有关键影响。因此，需要研究如何优化这些参数和机制以实现有效的共存控制。", "method": "将共存控制建模为具有显式高优先级流量延迟约束和公平性优化目标的受限马尔可夫决策过程。使用状态增强型受限强化学习控制器，并研究了(i)成本缩放和违规建模、(ii)MAC参数选择（特别是竞争窗口参数），以及(iii)增强型LBT碰撞解决方案对性能的影响。通过仿真验证了这些方法。", "result": "(1) 带有时间平滑的符号、阈值不变成本缩放能够稳定学习并增强长期约束遵守；(2) 使用竞争窗口参数进行控制比其他MAC参数提供更平滑的适应性和更好的延迟合规性；(3) 增强型LBT显著减少了碰撞并提高了空口时间效率。", "conclusion": "这些发现为实现稳健、QoS感知的共存控制提供了实用的见解。"}}
{"id": "2512.13344", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13344", "abs": "https://arxiv.org/abs/2512.13344", "authors": ["Vaishnavi Jagabathula", "Pushpak Jagtap"], "title": "Neural Control Barrier Functions for Signal Temporal Logic Specifications with Input Constraints", "comment": null, "summary": "Signal Temporal Logic (STL) provides a powerful framework to describe complex tasks involving temporal and logical behavior in dynamical systems. In this work, we address the problem of synthesizing controllers for continuous-time systems under STL specifications with input constraints. We propose a neural network-based framework for synthesizing time-varying control barrier functions (TVCBF) and their corresponding controllers for systems to fulfill STL specifications while respecting input constraints. We formulate barrier conditions incorporating the spatial and temporal logic of the given STL specification. Additionally, we introduce a validity condition to provide formal safety guarantees across the entire state space. Finally, we demonstrate the effectiveness of the proposed approach through several simulation studies considering different STL tasks.", "AI": {"tldr": "本文提出了一种基于神经网络的框架，用于为连续时间系统合成时变控制障碍函数（TVCBF）及其控制器，以满足信号时序逻辑（STL）规范和输入约束。", "motivation": "研究的动机是解决在存在输入约束的情况下，为连续时间系统合成满足复杂STL规范的控制器的问题。", "method": "该方法提出一个基于神经网络的框架来合成TVCBF及其控制器。它将STL规范的空间和时间逻辑融入到障碍条件中，并引入一个有效性条件以提供整个状态空间的形式化安全保证。", "result": "通过多项仿真研究，验证了所提出方法在处理不同STL任务时的有效性。", "conclusion": "该研究成功地提出了一种有效的神经网络方法，用于在输入约束下，为连续时间系统合成满足STL规范的控制器。"}}
{"id": "2512.12987", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12987", "abs": "https://arxiv.org/abs/2512.12987", "authors": ["Amin Jalal Aghdasian", "Farzaneh Abdollahi", "Ali Kamali Iglie"], "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning", "comment": null, "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.", "AI": {"tldr": "本文提出了两种基于深度强化学习（DRL）的新算法（AR-RDPG和AR-CADPG），用于自动驾驶车辆在雪地条件下的车道保持系统（LKS），以处理不确定性和打滑问题，并在仿真和真实世界中进行了验证。", "motivation": "自动驾驶车辆在雪地条件下运行的复杂性和不确定性（如打滑），对车道保持系统提出了挑战，需要更鲁棒的决策方法。", "method": "提出了两种行动鲁棒（Action-Robust）的深度强化学习算法：1) AR-RDPG：感知层对相机图像进行去噪，然后使用预训练的DCNN提取车道中心线系数，并与驾驶特性一同输入到控制层。2) AR-CADPG：一种端到端方法，将卷积神经网络（CNN）和注意力机制集成到DRL框架中。两种方法均在CARLA模拟器中训练，并在多种雪地场景下验证，最后在基于Jetson Nano的真实自动驾驶车辆上进行实验。", "result": "两种学习策略在真实世界实验中均表现出可行性和稳定性。其中，AR-CADPG方法在路径跟踪精度和鲁棒性方面表现更优。", "conclusion": "AR-CADPG方法结合时间记忆、对抗性弹性（action-robust）和注意力机制，在自动驾驶车辆的雪地车道保持任务中表现出卓越的有效性。"}}
{"id": "2512.12950", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12950", "abs": "https://arxiv.org/abs/2512.12950", "authors": ["Lingyi Meng", "Maolin Liu", "Hao Wang", "Yilan Cheng", "Qi Yang", "Idlkaid Mohanmmed"], "title": "Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping", "comment": "43 pages, 6 fingures, accepted in Artificial Intelligence and Law (2025)", "summary": "Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.", "AI": {"tldr": "本文提出了一种人机协作的多智能体框架，用于构建多语言法律术语数据库，旨在解决中日等语言对中同形异义词导致的术语映射挑战，并提高映射的准确性和可扩展性。", "motivation": "跨语言法律术语映射，特别是对于中文和日文这类存在大量同形异义词的语言对，是一个重大挑战。现有资源和标准化工具非常有限，难以准确处理。", "method": "研究提出了一种基于多智能体框架的人机协作方法。该方法整合了大型语言模型和法律领域专家，贯穿于原始文档预处理、文章级对齐、术语提取、映射和质量保证的全过程。AI智能体负责OCR、文本分割、语义对齐和初步术语提取等重复性任务，而人类专家则提供关键的监督、审查和基于语境知识及法律判断的输出管理。该框架使用包含35部关键中文法规及其英日译文的三语平行语料库进行了测试。", "result": "实验结果表明，这种人机协作的多智能体工作流不仅提高了多语言法律术语映射的精确性和一致性，而且与传统手动方法相比，具有更强的可扩展性。", "conclusion": "所提出的人机协作、多智能体框架是一种有效的方法，能够显著改善多语言法律术语映射的精度、一致性和可扩展性，尤其适用于处理中日等复杂语言对的挑战。"}}
{"id": "2512.12806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12806", "abs": "https://arxiv.org/abs/2512.12806", "authors": ["Boyang Yan"], "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution", "comment": "7 pages", "summary": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.", "AI": {"tldr": "本文提出一个容错沙盒框架，通过策略拦截层和事务性文件系统快照机制，解决自主LLM代理在执行破坏性命令和状态不一致时的安全风险，并实现了高安全性与可接受的性能开销。", "motivation": "大型语言模型（LLMs）从被动代码生成器向自主代理的转变带来了严重的安全风险，特别是破坏性命令和系统状态不一致。现有商业解决方案通常优先考虑用户交互安全，其强制的认证障碍不适用于真正的自主代理所需的无头循环。容器的初始化开销大，而交互式CLI的摩擦也阻碍了自主性。", "method": "本文提出了一个容错沙盒框架，通过策略驱动的拦截层和事务性文件系统快照机制来缓解风险。核心假设是将代理动作封装在原子事务中，以保证安全性并保持可接受的延迟。通过在基于Proxmox并利用EVPN/VXLAN隔离的测试台上部署Minimind-MoE LLM并使用nano-vllm进行验证。", "result": "实验结果表明，该框架对高风险命令实现了100%的拦截率，对失败状态实现了100%的回滚成功率。原型每笔事务仅产生14.5%的性能开销（约1.8秒）。与Gemini CLI沙盒的基准测试显示，后者需要交互式认证（“登录”），使其无法用于无头、自主代理的工作流。", "conclusion": "所提出的容错沙盒框架能够有效缓解自主LLM代理的安全风险，通过原子事务保证了高安全性，并提供了可接受的性能开销，优于现有需要交互式认证的商业解决方案。"}}
{"id": "2512.13080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13080", "abs": "https://arxiv.org/abs/2512.13080", "authors": ["Yicheng Feng", "Wanpeng Zhang", "Ye Wang", "Hao Luo", "Haoqi Yuan", "Sipeng Zheng", "Zongqing Lu"], "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos", "comment": null, "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.", "AI": {"tldr": "本文提出一种空间感知VLA预训练范式，通过在大规模人类演示视频中提取3D视觉和3D动作标注，显式对齐视觉空间与物理空间，从而在机器人策略学习前赋予模型3D空间理解能力，以弥合2D感知与3D动作之间的鸿沟。", "motivation": "现有视觉-语言-动作（VLA）模型主要依赖2D视觉输入在3D物理环境中执行动作，这在感知与动作基础之间造成了显著的差距。", "method": "提出“空间感知VLA预训练”范式，利用大规模人类演示视频提取3D视觉和3D动作标注，形成新的监督源，以对齐2D视觉观测与3D空间推理。具体通过VIPA-VLA实例化，这是一种双编码器架构，包含一个3D视觉编码器，用于增强语义视觉表示的3D感知特征。", "result": "当适应下游机器人任务时，VIPA-VLA显著改善了2D视觉与3D动作之间的接地，从而产生了更鲁棒和更具泛化性的机器人策略。", "conclusion": "通过在预训练阶段显式对齐视觉空间与物理空间，并融入3D空间理解，所提出的空间感知VLA预训练范式有效弥合了2D感知与3D动作的差距，显著提升了机器人策略的鲁棒性和泛化能力。"}}
{"id": "2512.13059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13059", "abs": "https://arxiv.org/abs/2512.13059", "authors": ["Ikuya Yamada", "Wataru Ikeda", "Ko Yoshida", "Mengyu Ye", "Hinata Sugimoto", "Masatoshi Suzuki", "Hisanori Ozaki", "Jun Suzuki"], "title": "An Open and Reproducible Deep Research Agent for Long-Form Question Answering", "comment": "Technical report of a winning system in the NeurIPS MMU-RAG competition", "summary": "We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.", "AI": {"tldr": "本文提出一个用于长篇问答的开源深度研究系统，结合开源LLM和开放网络搜索API进行迭代检索、推理和合成，并通过基于LLM-as-a-judge反馈的偏好调优提升回答质量，并在MMU-RAG竞赛中获胜。", "motivation": "研究动机是解决真实世界开放领域中的长篇问答问题，并提升回答的清晰度、洞察力和事实准确性，尤其是在MMU-RAG竞赛的文本到文本赛道中取得优异成绩。", "method": "该系统结合了开源大型语言模型（LLM）和开放网络搜索API，以执行迭代的检索、推理和合成。为提高推理质量，系统采用基于LLM-as-a-judge反馈的偏好调优，该反馈评估回答的清晰度、洞察力和事实准确性等多个方面。", "result": "实验结果表明，所提出的方法在清晰度、洞察力和事实准确性这三个方面持续提升了回答质量。该系统在NeurIPS 2025 MMU-RAG竞赛的文本到文本赛道中被选为获胜系统。", "conclusion": "该研究系统通过结合开源LLM与开放网络搜索，并辅以基于LLM-as-a-judge反馈的偏好调优，成功地在开放领域长篇问答中实现了回答质量的显著提升，并在相关竞赛中表现出色。"}}
{"id": "2512.12199", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12199", "abs": "https://arxiv.org/abs/2512.12199", "authors": ["Ercan Erkalkan", "Vedat Topuz", "Ayça Ak"], "title": "Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms", "comment": "Conference paper in 17th International Scientific Studies Congress proceedings. Topic: thermal+RGB rule level fusion, RDP boundary simplification, leader follower guidance, sub 50ms embedded SoC, minimal communications for wildfire perimeter tracking. Thermal RGB Fusion for Micro-UAV", "summary": "This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.", "AI": {"tldr": "本研究提出了一种轻量级周界跟踪方法，适用于在有限带宽条件下，微型无人机团队在野火环境中进行操作。该方法结合热成像和RGB图像处理，通过规则级合并和RDP算法简化边界，并利用惯性反馈和优化算法实现低延迟和高鲁棒性，适用于紧急侦察。", "motivation": "在野火环境中，微型无人机团队面临带宽受限、GPS信号退化等挑战，需要一种能够快速部署、鲁棒感知且通信需求最小的周界跟踪方法，以支持紧急侦察应用。", "method": "该方法通过热图像自适应阈值和形态学细化生成粗略热区掩膜；利用RGB图像的梯度滤波提供边缘线索并抑制纹理引起的误检。通过规则级合并策略选择边界候选，并使用Ramer Douglas Peucker算法进行简化。系统集成周期性信标和惯性反馈回路以在GPS退化时保持轨迹稳定性。为在嵌入式SoC平台上实现亚50毫秒的延迟，限制了每帧像素操作并预计算梯度表。", "result": "小规模模拟结果显示，与纯边缘跟踪基线相比，该方法能减少平均路径长度和边界抖动，同时通过交集合并分析保持环境覆盖率。电池消耗和计算利用率分析证实了在标准微型平台上实现10-15米/秒前进速度的可行性。", "conclusion": "该方法实现了在野火环境下对微型无人机团队进行周界跟踪的轻量化和鲁棒性，仅需最少的通信，能够快速部署，适用于紧急侦察应用。"}}
{"id": "2512.13009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13009", "abs": "https://arxiv.org/abs/2512.13009", "authors": ["Oğuzhan Akbıyık", "Naseem Alhousani", "Fares J. Abu-Dakka"], "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots", "comment": null, "summary": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.", "AI": {"tldr": "本文提出K-VARK（核化方差感知残余卡尔曼滤波器），将核化概率残余力矩模型整合到自适应卡尔曼滤波器中，显著提高了无传感器力估计的精度和鲁棒性。", "motivation": "机器人与非结构化环境进行安全精确交互需要可靠的接触力估计。然而，由于固有的模型误差、复杂的残余动力学和摩擦，准确的无传感器力估计仍然面临挑战。", "method": "K-VARK方法将一个核化、概率性的关节残余力矩模型集成到自适应卡尔曼滤波器框架中。它通过在优化激励轨迹上训练的核化运动基元（KMP），捕获残余力矩的预测均值和与输入相关的异方差方差。这些统计信息通过增强测量噪声协方差来指导方差感知的虚拟测量更新，同时通过变分贝叶斯优化在线调整过程噪声协方差以应对动态扰动。", "result": "在6自由度协作机械臂上的实验验证表明，K-VARK的均方根误差（RMSE）比现有最先进的无传感器力估计方法降低了20%以上。", "conclusion": "K-VARK能够实现鲁棒且准确的外部力/力矩估计，适用于抛光和装配等高级任务。"}}
{"id": "2512.12856", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12856", "abs": "https://arxiv.org/abs/2512.12856", "authors": ["Saad Alqithami"], "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents", "comment": null, "summary": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.", "AI": {"tldr": "本文提出了一种名为MaRS的人本记忆管理框架，并结合六种遗忘策略来解决生成式智能体在长期交互中面临的记忆管理瓶颈。通过FiFA基准测试，实验证明其混合遗忘策略在性能、隐私和计算效率之间实现了卓越的平衡。", "motivation": "生成式智能体在长期交互场景中，其记忆管理能力成为性能和隐私的关键瓶颈。现有方法要么维持无限记忆，导致计算不可行和隐私问题；要么采用简单遗忘机制，损害智能体连贯性和功能性。", "method": "本文引入了记忆感知保留方案（MaRS），这是一个用于生成式智能体的人本记忆管理框架，并提出了六种理论基础的遗忘策略，以平衡性能、隐私和计算效率。同时，构建了“健忘而忠诚的智能体”（FiFA）基准，从叙事连贯性、目标完成度、社交召回准确性、隐私保护和成本效率等方面评估智能体性能。通过300次评估运行进行了广泛实验。", "result": "实验结果表明，所提出的混合遗忘策略实现了卓越的综合性能（综合得分：0.911），同时保持了计算可行性和隐私保障。这项工作为记忆预算型智能体评估建立了新基准，并为在资源受限、隐私敏感环境中部署生成式智能体提供了实用指导。", "conclusion": "本文通过解决智能体记忆管理中的基本挑战，为新兴的人本AI领域做出了贡献，这些挑战直接影响用户信任、系统可扩展性和法规遵从性。所提出的理论基础、实现框架和实证结果为生成式智能体在复杂场景中的部署提供了重要支持。"}}
{"id": "2512.12976", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12976", "abs": "https://arxiv.org/abs/2512.12976", "authors": ["Marcus Ma", "Cole Johnson", "Nolan Bridges", "Jackson Trager", "Georgios Chochlakis", "Shrikanth Narayanan"], "title": "Authors Should Annotate", "comment": null, "summary": "The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.", "AI": {"tldr": "本文提出了一种“作者标注”技术，即文档作者在创作时直接标注数据。该方法在产品推荐和情感分析中被证明优于第三方标注，能显著提高模型性能（点击率增加534%），并提供更高质量、更快速、更经济的数据。为促进推广，研究团队还发布了作者标注服务。", "motivation": "当前文本标注主要依赖第三方，但在处理情感、信念等主观或自我中心特征时，文档作者的直接信息比第三方代理更可取。因此，需要一种能获取作者直接标注信息的方法来提高数据质量。", "method": "研究引入了“作者标注”技术，即文档作者在创建时进行数据标注。与一个拥有超过10,000用户的商业聊天机器人合作，部署了一个针对产品推荐相关主观特征的作者标注系统。该系统能识别任务相关查询，实时生成标注问题并记录作者答案。研究训练并部署了一个在线学习模型架构用于产品推荐，该模型通过作者标注持续改进。此外，研究还将作者标注与三种传统情感分析标注方法进行了质量和实用性比较，并发布了一个作者标注服务。", "result": "通过作者标注持续改进的在线学习模型，在产品推荐方面实现了比同期行业广告基线高534%的点击率增长。与三种传统情感分析标注方法相比，作者标注被证明质量更高、获取速度更快、成本更低。这些发现强化了现有文献的观点，即由作者标注的数据，特别是针对自我中心和主观信念的标注，质量显著更高。", "conclusion": "作者标注是一种卓越的标注技术，尤其适用于主观和自我中心特征，它能提供更高质量、更快速、更经济的数据，并显著提升模型性能。研究成果支持了作者标注的优越性，并期望通过发布服务促进其在科学研究中的广泛应用。"}}
{"id": "2512.13482", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13482", "abs": "https://arxiv.org/abs/2512.13482", "authors": ["Wenyi Liu", "R. Sharma", "W. \"Grace\" Guo", "J. Yi", "Y. B. Guo"], "title": "Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency", "comment": null, "summary": "Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.", "AI": {"tldr": "本文分析了数字孪生在铣削领域的最新进展，探讨了其虚拟模型、数据流和反馈机制，并通过案例研究展示了其在实时机器学习驱动下的变革能力。", "motivation": "数字孪生通过利用实时数据、AI模型和智能控制系统，赋能智能制造。本研究旨在分析数字孪生在铣削领域的新兴应用。", "method": "本文采用最新技术分析方法，探讨了物理铣削虚拟模型、从物理铣削到虚拟模型的数据流以及从虚拟模型到物理铣削的反馈等关键方面。重点介绍了实时数据流协议和虚拟建模方法。通过一个实时机器学习驱动的刀具-工件接触数字孪生案例研究来展示其能力。", "result": "案例研究展示了实时机器学习驱动的铣削过程中刀具-工件接触的实时数字孪生的变革能力。", "conclusion": "数字孪生对于实现工业4.0及更长远目标至关重要，并提出了未来的研究方向。"}}
{"id": "2512.13439", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13439", "abs": "https://arxiv.org/abs/2512.13439", "authors": ["Nitya Sathyavageeswaran", "Anand D. Sarwate", "Narayan B. Mandayam", "Roy D. Yates"], "title": "Balancing Timeliness and Privacy in Discrete-Time Updating Systems", "comment": null, "summary": "We study the trade-off between Age of Information (AoI) and maximal leakage (MaxL) in discrete-time status updating systems. A source generates time-stamped update packets that are processed by a server that delivers them to a monitor. An adversary, who eavesdrops on the server-monitor link, wishes to infer the timing of the underlying source update sequence. The server must balance the timeliness of the status information at the monitor against the timing information leaked to the adversary. We consider a model with Bernoulli source updates under two classes of Last-Come-First-Served (LCFS) service policies: (1) Coupled policies that tie the server's deliveries to the update arrival process in a preemptive queue; (2) Decoupled (dumping) policies in which the server transmits its freshest update according to a schedule that is independent of the update arrivals. For each class, we characterize the structure of the optimal policy that minimizes AoI for a given MaxL rate. Our analysis reveals that decoupled dumping policies offer a superior age-leakage trade-off to coupled policies. When subject to a MaxL constraint, we prove that the optimal dumping strategy is achieved by dithering between two adjacent deterministic dump periods.", "AI": {"tldr": "本文研究了离散时间状态更新系统中信息年龄 (AoI) 与最大泄漏 (MaxL) 之间的权衡，并比较了两种后进先出 (LCFS) 服务策略在满足给定MaxL下最小化AoI的性能。", "motivation": "在状态更新系统中，服务器需要平衡监控器端状态信息的及时性（低AoI）与泄露给窃听对手的源更新序列时序信息（低MaxL），以保护隐私。", "method": "研究了伯努利源更新模型下的两类后进先出 (LCFS) 服务策略：1) 耦合策略，服务器交付与更新到达过程相关联；2) 解耦（倾倒）策略，服务器根据独立于更新到达的计划传输最新更新。对每一类策略，都刻画了在给定MaxL率下最小化AoI的最优策略结构。", "result": "分析表明，解耦倾倒策略提供了优于耦合策略的年龄-泄漏权衡。在MaxL约束下，最优倾倒策略是通过在两个相邻的确定性倾倒周期之间进行抖动实现的。", "conclusion": "解耦倾倒策略在信息年龄与最大泄漏之间的权衡方面表现更优，并且找到了其最优的实现方式，即在两个相邻确定性倾倒周期之间进行抖动。"}}
{"id": "2512.12918", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12918", "abs": "https://arxiv.org/abs/2512.12918", "authors": ["Nijesh Upreti", "Vaishak Belle"], "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming", "comment": null, "summary": "Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.", "AI": {"tldr": "本文提出一种模块化方法，将归纳逻辑编程（ILP）系统PyGol与SMT求解器Z3耦合，以解决ILP在数值约束推理方面的局限性，从而学习包含符号谓词和数值约束的混合规则。", "motivation": "经典的归纳逻辑编程（ILP）系统在处理数值约束方面能力有限，难以推断阈值或复杂的算术关系。虽然近期工作已开始尝试解决，但仍需要更紧密的集成或专用的数值推理机制。", "method": "研究了一种模块化替代方案，将ILP系统PyGol与SMT求解器Z3耦合。PyGol提出的候选子句被解释为线性或非线性实数算术等背景理论上的量词自由公式，SMT求解器负责实例化和验证数值参数，同时保留ILP的声明性关系偏置。", "result": "该方法支持归纳结合符号谓词与学习到的数值约束（包括阈值、区间和多文字算术关系）的混合规则。在合成数据集上的评估表明，这种模块化的SMT-ILP架构可以扩展符号规则学习的表达能力，并在探究线性、关系、非线性和多跳推理方面表现良好。", "conclusion": "模块化的SMT-ILP架构能够扩展符号规则学习的表达能力，补充了先前的数值ILP方法，并为未来更丰富的理论感知归纳提供了灵活的基础。"}}
{"id": "2512.13090", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13090", "abs": "https://arxiv.org/abs/2512.13090", "authors": ["Jebeom Chae", "Junwoo Chang", "Seungho Yeom", "Yujin Kim", "Jongeun Choi"], "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.", "AI": {"tldr": "本文提出LCHD，一个端到端的视觉基语言条件热启发扩散模型，用于多机器人运动规划，能生成无碰撞轨迹，提高成功率并降低规划延迟。", "motivation": "现有扩散模型在多机器人、语言条件任务中应用受限；推理计算成本高；泛化能力差，需显式环境表示；缺乏几何可达性推理机制。", "method": "LCHD是一个端到端视觉框架，结合CLIP语义先验和防碰撞扩散核作为物理归纳偏置。它使规划器能在可达工作空间内严格解释语言指令，通过引导机器人到匹配语义意图的可达替代方案来处理分布外场景，且推理时无需显式障碍物信息。", "result": "LCHD在多样化的真实世界地图和真实机器人实验中，持续优于现有扩散规划器，提高了成功率并缩短了规划延迟。", "conclusion": "LCHD通过其视觉、语言条件和可达性感知设计，有效解决了现有扩散模型在多机器人规划中的局限性，显著提升了规划性能和效率。"}}
{"id": "2512.12970", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12970", "abs": "https://arxiv.org/abs/2512.12970", "authors": ["Paola Di Maio"], "title": "Towards Open Standards for Systemic Complexity in Digital Forensics", "comment": null, "summary": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.", "AI": {"tldr": "人工智能与数字取证日益交织，为减少取证错误，本文提出采用可读工件和开放标准来解决系统复杂性，并概述了一个数字取证AI模型架构。", "motivation": "尽管取得了巨大进步，但法医学仍易犯错。人工智能与数字取证的交叉点日益复杂且普遍存在，因此需要减轻数字取证中的错误限制。", "method": "通过识别系统复杂性并采用人类可读工件和开放标准来解决问题。此外，本文概述了一个基于最新技术的数字取证AI模型架构。", "result": "本文识别了数字取证中的系统复杂性，并提出了通过采用人类可读工件和开放标准来缓解错误的方法。同时，概述了一个基于现有技术的数字取证AI模型架构。", "conclusion": "为减轻数字取证中的错误，建议通过采用人类可读工件和开放标准来解决系统复杂性，并提出了一个数字取证AI模型架构，以期提升法医学的可靠性。"}}
{"id": "2512.12205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12205", "abs": "https://arxiv.org/abs/2512.12205", "authors": ["Peizheng Li", "Ioannis Mavromatis", "Ajith Sahadevan", "Tim Farnham", "Adnan Aijaz", "Aftab Khan"], "title": "A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection", "comment": "10 pages, 7 figures. Submitted to Data in Brief (Elsevier)", "summary": "We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.", "AI": {"tldr": "该研究发布了一个大规模、长期的城市路灯视觉数据集，包含超过526,000张图像及丰富的元数据，并提供了一个基于CNN-VAE的自监督框架和漂移度量指标，旨在促进智能城市部署中的视觉漂移、异常检测和MLOps策略研究。", "motivation": "该研究旨在详细调查智能城市部署中的视觉漂移、异常检测和MLOps策略，并为评估模型长期稳定性、漂移感知学习和可部署视觉系统提供一个真实、细粒度的基准。", "method": "研究方法包括：1. 数据集采集：在英国布里斯托尔部署22个固定角度摄像头，从2021年至2025年每小时采集图像，共计超过526,000张，涵盖不同光照、天气和季节条件。2. 元数据：每张图像附带时间戳、GPS坐标和设备标识符等丰富元数据。3. 自监督框架：基于卷积变分自编码器（CNN-VAEs），针对每个摄像头节点以及白天/夜晚图像集分别训练模型。4. 漂移度量：定义了两种漂移指标——相对质心漂移（衡量潜在空间偏差）和相对重建误差（衡量图像域退化）。", "result": "研究结果是一个公开可用的大规模、长期城市路灯视觉数据集（JPEG图像和CSV元数据），以及一个用于漂移检测的自监督框架和两种漂移度量指标。该数据集支持可复现性，并可用于路灯监控、天气推断和城市场景理解等下游应用。", "conclusion": "该数据集为评估模型长期稳定性、漂移感知学习和可部署视觉系统提供了一个真实、细粒度的基准，并通过提供自监督框架和漂移度量，促进了二次分析和智能城市相关应用的开发。"}}
{"id": "2512.13109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13109", "abs": "https://arxiv.org/abs/2512.13109", "authors": ["Zewen Qiang", "Sendong Zhao", "Haochun Wang", "Bing Qin", "Ting Liu"], "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.", "AI": {"tldr": "大型语言模型（LLMs）在处理长文本时存在“中间遗失”问题，表现为U形注意力偏差。本研究首次发现“初始显著性”是除位置编码外的一个额外因素，并提出通过缩放初始token与其他token之间的注意力权重来缓解此问题，显著提升了模型处理长上下文的能力。", "motivation": "LLMs在处理长文本序列时，由于“中间遗失”现象（U形注意力偏差，即注意力过度集中于文本开头和结尾），性能受到影响。虽然之前研究将其归因于位置编码，但研究者希望找出并解决其他潜在原因。", "method": "本研究首先识别出“初始显著性”作为导致U形注意力偏差的额外因素，即在注意力计算中，相对于初始token具有更高注意力权重的token在预测下一个token时会获得更多关注。然后，利用这一特性，通过缩放初始token与其他token之间的注意力权重来改进模型。此外，还将此方法与现有减少位置编码偏差的方法相结合。", "result": "通过缩放注意力权重，模型处理长上下文的能力得到提升，在MDQA数据集上取得了最高3.6%的改进。将此方法与现有减少位置编码偏差的方法结合后，性能进一步增强，在KV-Retrieval任务中取得了最高3.4%的改进。", "conclusion": "LLMs在长文本处理中的“中间遗失”问题不仅源于位置编码偏差，还受“初始显著性”影响。通过利用和调整初始显著性，特别是结合现有方法，可以有效提升LLMs处理长上下文的能力。"}}
{"id": "2512.13496", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13496", "abs": "https://arxiv.org/abs/2512.13496", "authors": ["Giacomo Bastianel", "Clement Hardy", "Nils Charels", "Dirk Van Hertem", "Hakan Ergun"], "title": "Identification of Technical Design Constraints and Considerations for Transmission Grid Expansion Planning Projects", "comment": null, "summary": "The large-scale deployment of renewable energy sources, particularly offshore wind, requires large-scale transmission grid expansion projects to transmit the produced low-carbon power to the main demand centers. However, the planning and design of such complex projects currently lack a transparent and systematic process that system operators can follow when considering such investments in their grids. This paper identifies and classifies the main technical design constraints and considerations relevant to the planning of transmission grid expansion projects, and more specifically, electrical energy hubs. Seven key areas of interest are identified, namely network integration, HVDC technologies, costs (CAPEX, OPEX, and space requirements), electricity market design, future proofness and modular expandability, reliability-availability-maintainability, and sustainability. Each area of interest is analyzed in terms of its technical and operational relevance, with technical design constraints and considerations derived from such analysis. In addition, a hierarchical classification of the identified constraints and considerations (and therefore areas of interest) is introduced, distinguishing them between three criticality classes, namely hard constraints, main drivers, and key considerations. The dependencies between the different areas are discussed, too. Therefore, this work provides system operators and policymakers with a structured basis to support a transparent planning methodology with clear decision hierarchies for investments in transmission grid expansion projects.", "AI": {"tldr": "本文提出了一种系统且透明的方法，用于规划输电网扩建项目，特别是针对海上风电，通过识别和分类技术设计约束和考虑因素，并将其划分为不同的关键性级别。", "motivation": "大规模可再生能源（尤其是海上风电）的部署需要大规模输电网扩建，但目前此类复杂项目的规划和设计缺乏透明和系统的流程，供系统运营商在投资时遵循。", "method": "本文识别并分类了输电网扩建项目规划（特别是电能枢纽）相关的技术设计约束和考虑因素，将其归纳为七个关键领域（网络集成、高压直流技术、成本、电力市场设计、未来可扩展性、可靠性-可用性-可维护性、可持续性）。对每个领域进行了技术和操作相关性分析，并从中得出设计约束。此外，引入了分层分类，将这些约束分为三个关键性级别：硬性约束、主要驱动因素和关键考虑因素，并讨论了各领域间的依赖关系。", "result": "识别并分类了七个关键领域的技术设计约束和考虑因素，并建立了分层的关键性分类（硬性约束、主要驱动因素、关键考虑因素）。分析了这些约束的技术和操作相关性，并讨论了不同领域之间的依赖性。", "conclusion": "这项工作为系统运营商和政策制定者提供了一个结构化基础，以支持透明的规划方法，并为输电网扩建项目的投资提供了明确的决策层次。"}}
{"id": "2512.13483", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.13483", "abs": "https://arxiv.org/abs/2512.13483", "authors": ["Ruslan Seifullaev", "André Teixeira"], "title": "Impact analysis of hidden faults in nonlinear control systems using output-to-output gain", "comment": "This work has been submitted to IFAC World Congress 2026 for possible publication", "summary": "Networked control systems (NCSs) are vulnerable to faults and hidden malfunctions in communication channels that can degrade performance or even destabilize the closed loop. Classical metrics in robust control and fault detection typically treat impact and detectability separately, whereas the output-to-output gain (OOG) provides a unified measure of both. While existing results have been limited to linear systems, this paper extends the OOG framework to nonlinear NCSs with quadratically constrained nonlinearities, considering false-injection attacks that can also manipulate sensor measurements through nonlinear transformations. Specifically, we provide computationally efficient linear matrix inequality conditions and complementary frequency-domain tests that yield explicit upper bounds on the OOG of this class of nonlinear systems. Furthermore, we derive frequency-domain conditions for absolute stability of closed-loop systems, generalizing the Yakubovich quadratic criterion.", "AI": {"tldr": "本文将输出到输出增益（OOG）框架扩展到具有二次约束非线性的非线性网络控制系统（NCS），考虑虚假注入攻击，并提供计算高效的LMI条件和频域测试，以得出OOG的显式上限和闭环系统的绝对稳定性条件。", "motivation": "网络控制系统（NCS）易受通信通道故障影响，传统鲁棒控制和故障检测指标将影响和可检测性分开处理，而OOG能统一衡量两者。现有OOG研究仅限于线性系统，因此需要将其扩展到非线性NCS，并考虑通过非线性变换操纵传感器测量的虚假注入攻击。", "method": "本文将OOG框架扩展到具有二次约束非线性的非线性NCS，并考虑虚假注入攻击。具体方法包括：提供计算高效的线性矩阵不等式（LMI）条件；提供互补的频域测试；推导闭环系统绝对稳定性的频域条件，推广了Yakubovich二次判据。", "result": "研究结果包括：针对此类非线性系统，得出了OOG的显式上限；提供了计算OOG上限的LMI条件和频域测试；推导出了闭环系统绝对稳定性的频域条件。", "conclusion": "本文成功将OOG框架扩展到受虚假注入攻击的非线性NCS，提供了一套计算高效的LMI条件和频域测试，用于评估系统性能和绝对稳定性，统一了影响和可检测性的度量。"}}
{"id": "2512.12206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12206", "abs": "https://arxiv.org/abs/2512.12206", "authors": ["Jeongjun Park", "Sunwook Hwang", "Hyeonho Noh", "Jin Mo Yang", "Hyun Jong Yang", "Saewoong Bahk"], "title": "ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB", "comment": null, "summary": "Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.\n  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.\n  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.", "AI": {"tldr": "该研究通过发布大型真实世界UWB雷达驾驶分心行为数据集ALERT，并提出一种输入尺寸无关的Vision Transformer (ISA-ViT) 框架，解决了雷达驾驶活动识别中数据缺乏和ViT模型适应性差的问题，显著提升了识别准确率。", "motivation": "分心驾驶是全球交通事故的主要原因。尽管IR-UWB雷达在驾驶活动识别（DAR）中具有抗干扰、低功耗和保护隐私等优势，但其应用受限于两个主要挑战：缺乏大规模、多样化的真实世界UWB驾驶分心行为数据集，以及难以将固定输入尺寸的Vision Transformer（ViT）模型应用于非标准尺寸的UWB雷达数据。", "method": "该研究提出了两项主要方法：1. 构建并发布了ALERT数据集，包含10,220个在真实驾驶条件下收集的七种分心驾驶活动的雷达样本。2. 提出了输入尺寸无关的Vision Transformer (ISA-ViT) 框架，专为基于雷达的DAR设计。该方法通过调整UWB数据尺寸以满足ViT输入要求，同时保留多普勒频移和相位特征等雷达特有信息，并通过调整补丁配置和利用预训练的位置嵌入向量来克服传统尺寸调整的局限性。此外，还采用了一种域融合策略，结合距离域和频率域特征以进一步提高分类性能。", "result": "综合实验结果表明，ISA-ViT在基于UWB雷达的DAR任务中，相比现有基于ViT的方法，准确率提高了22.68%。", "conclusion": "该工作通过公开发布ALERT数据集并详细阐述其输入尺寸无关策略，促进了更鲁棒、可扩展的真实世界分心驾驶检测系统的开发和部署。"}}
{"id": "2512.12208", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12208", "abs": "https://arxiv.org/abs/2512.12208", "authors": ["Indranil Bhattacharjee", "Vartika Narayani Srinet", "Anirudha Bhattacharjee", "Braj Bhushan", "Bishakh Bhattacharya"], "title": "A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction", "comment": "12 pages, journal paper", "summary": "Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.", "AI": {"tldr": "本研究提出了一种新颖的深度学习流程，用于识别自闭症谱系障碍（ASD）儿童在与NAO人形机器人进行社交互动（被呼唤名字）时的情感反应，该流程结合了CNN和GCN模型，利用视觉和几何特征，并在大规模真实世界数据集中表现出强大的性能。", "motivation": "理解ASD儿童在社交互动中的情感反应是一个关键挑战，尤其在发展心理学和人机交互领域。现有研究在自闭症特定的人机交互情感分析方面存在空白，缺乏捕捉神经多样性儿童细微情感线索的方法和大规模真实世界数据集。", "method": "研究构建了一个包含约50,000帧面部图像的数据集，这些图像来源于15名ASD儿童与NAO机器人互动（被呼唤名字）的视频记录。提出了一种混合深度学习模型，结合了经过微调的基于ResNet-50的卷积神经网络（CNN）和三层图卷积网络（GCN）。模型使用从MediaPipe FaceMesh地标中提取的视觉和几何特征进行训练。情感标签通过DeepFace和FER两个模型的加权集成进行概率性软标签生成，涵盖七种情感类别。最终分类利用通过Kullback-Leibler散度优化的融合嵌入。", "result": "所提出的方法在建模细微的情感反应方面表现出稳健的性能，并能有效捕捉神经多样性儿童的微观情感线索。这是印度首个利用社交机器人进行自闭症情感分析的大规模真实世界数据集和流程。", "conclusion": "该研究为ASD儿童在临床和治疗性人机交互环境中的情感画像提供了重要前景，填补了自闭症特定人机交互研究中的一个主要空白。它为未来个性化辅助技术奠定了重要的基础。"}}
{"id": "2512.13093", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13093", "abs": "https://arxiv.org/abs/2512.13093", "authors": ["Mingqi Yuan", "Tao Yu", "Haolin Song", "Bo Li", "Xin Jin", "Hua Chen", "Wenjun Zeng"], "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations", "comment": "13 pages, 12 figures", "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.", "AI": {"tldr": "本文提出了PvP，一个利用本体感受和特权状态互补性的对比学习框架，以提高类人机器人在动态环境中进行全身控制（WBC）时的样本效率和策略学习稳定性。", "motivation": "尽管强化学习（RL）在类人机器人全身控制方面取得成功，但由于类人机器人复杂的动力学和部分可观测性，其样本效率低下仍然是一个重大挑战。", "method": "本文提出了PvP（Proprioceptive-Privileged）对比学习框架，它利用本体感受和特权状态之间的内在互补性来学习紧凑且与任务相关的潜在表示，无需手动设计数据增强。此外，还开发了SRL4Humanoid框架，用于系统评估类人机器人学习中的状态表示学习（SRL）方法。", "result": "在LimX Oli机器人上进行的广泛实验表明，PvP在速度跟踪和动作模仿任务中，与基线SRL方法相比，显著提高了样本效率和最终性能。", "conclusion": "PvP为类人机器人全身控制提供了一种数据高效的学习方法，并为将SRL与RL集成以实现数据高效的类人机器人学习提供了实用见解和宝贵指导。"}}
{"id": "2512.13063", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13063", "abs": "https://arxiv.org/abs/2512.13063", "authors": ["Cheril Shah", "Akshit Agarwal", "Kanak Garg", "Mourad Heddaya"], "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators", "comment": "Published in the First Workshop on Multi-Turn Interactions in Large Language Models at Neurips 2025", "summary": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.", "AI": {"tldr": "该研究引入数学框架和度量标准，并大规模比较人类与LLM在双边谈判中的表现，发现LLM在适应情境、推断对手策略方面存在根本性局限。", "motivation": "双边谈判是复杂的、情境敏感的任务，人类谈判者会动态调整策略以利用权力不对称和非正式线索。研究旨在理解人类谈判的动态性，并评估当前大型语言模型（LLMs）在模拟此类行为方面的能力和局限性。", "method": "研究提出了一个基于双曲正切曲线的统一数学框架来建模让步动态，并引入了“爆发性tau”（burstiness tau）和“让步刚性指数”（CRI）两个指标来量化出价轨迹的时机和刚性。通过大规模实证比较，将人类谈判者与四种最先进的LLM在自然语言和数字出价设置中进行对比，涵盖有无市场背景以及六种受控的权力不对称场景。", "result": "结果显示，人类能平稳适应情境并推断对手立场和策略，而LLM则系统性地锚定在可能协议区的极端，并优化固定点，而不顾杠杆作用或上下文。定性分析进一步揭示LLM的策略多样性有限，并偶尔使用欺骗性策略。此外，LLM的谈判能力并未随模型改进而提升。", "conclusion": "这些发现强调了当前LLM谈判能力的根本性局限，并指出需要开发能更好内化对手推理和情境依赖策略的模型。"}}
{"id": "2512.13070", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13070", "abs": "https://arxiv.org/abs/2512.13070", "authors": ["Bizhe Bai", "Hongming Wu", "Peng Ye", "Tao Chen"], "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization", "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.", "AI": {"tldr": "自监督强化学习（RL）在LLM推理中存在长期训练下的“策略崩溃”问题。本文提出M-GRPO（动量锚定组相对策略优化）框架，利用动量模型稳定训练目标，并结合基于IQR（四分位距）的自适应过滤方法，动态修剪低熵轨迹以保持策略多样性，从而解决了训练不稳定和过早收敛的问题，实现了最先进的性能。", "motivation": "现有的自监督强化学习方法在LLM的长期训练中存在“策略崩溃”的严重故障模式，导致性能急剧下降。即使增加rollout数量也只能延缓而非阻止这种崩溃。此外，训练过程中常伴随策略熵的快速崩溃，导致过早自信和次优策略。", "method": "1. **M-GRPO (Momentum-Anchored Group Relative Policy Optimization)**：引入一个缓慢演进的动量模型，提供稳定的训练目标以对抗训练不稳定。 2. **基于IQR（四分位距）的自适应过滤方法**：动态修剪低熵轨迹，以保留必要的策略多样性，防止策略熵快速崩溃导致的过早收敛。", "result": "M-GRPO有效稳定了训练过程，而IQR过滤器成功阻止了过早收敛。这两种创新的结合显著提高了训练稳定性，并在多个推理基准上取得了最先进的性能。", "conclusion": "通过M-GRPO和IQR自适应过滤方法，成功解决了自监督强化学习在LLM推理中长期训练的策略崩溃和过早收敛问题，显著提升了训练稳定性和模型性能。"}}
{"id": "2512.13194", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13194", "abs": "https://arxiv.org/abs/2512.13194", "authors": ["Chendong Sun"], "title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models", "comment": null, "summary": "Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \\(1 - \\max(P_{\\mathrm{target}})\\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2512.13504", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.13504", "abs": "https://arxiv.org/abs/2512.13504", "authors": ["Ruslan Seifullaev", "André M. H. Teixeira"], "title": "An $H_2$-norm approach to performance analysis of networked control systems under multiplicative routing transformations", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper investigates the performance of networked control systems subject to multiplicative routing transformations that alter measurement pathways without directly injecting signals. Such transformations, arising from faults or adversarial actions, modify the feedback structure and can degrade performance while remaining stealthy. An $H_2$-norm framework is proposed to quantify the impact of these transformations by evaluating the ratio between the steady-state energies of performance and residual outputs. Equivalent linear matrix inequality (LMI) formulations are derived for computational assessment, and analytical upper bounds are established to estimate the worst-case degradation. The results provide structural insight into how routing manipulations influence closed-loop behavior and reveal conditions for stealthy multiplicative attacks.", "AI": {"tldr": "本文研究了乘性路由变换对网络化控制系统性能的影响，该变换在不直接注入信号的情况下改变测量路径，可能导致性能下降且具有隐蔽性。文章提出了一个H2范数框架来量化这种影响，并通过LMI和分析上限评估最坏情况下的性能退化，揭示了路由操作如何影响闭环行为以及隐蔽攻击的条件。", "motivation": "研究的动机是存在一种乘性路由变换（由故障或对抗行为引起），它们在不直接注入信号的情况下改变测量路径，从而修改反馈结构。这种变换可以导致系统性能下降，但同时又保持隐蔽性，因此需要对其进行量化和理解。", "method": "本文提出一个H2范数框架来量化这些变换的影响，通过评估性能输出和残余输出的稳态能量比。为计算评估推导了等效的线性矩阵不等式（LMI）公式，并建立了分析上限来估计最坏情况下的性能退化。", "result": "研究结果量化了乘性路由变换对网络化控制系统性能的影响，提供了关于路由操作如何影响闭环行为的结构性见解，并揭示了隐蔽乘性攻击的条件。", "conclusion": "本文的结论是，通过提出的H2范数框架、LMI公式和分析上限，可以有效地量化和理解乘性路由变换对网络化控制系统性能的影响，并为识别隐蔽攻击提供了结构性洞察和条件。"}}
{"id": "2512.13094", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13094", "abs": "https://arxiv.org/abs/2512.13094", "authors": ["Xiang Li", "Gang Liu", "Weitao Zhou", "Hongyi Zhu", "Zhong Cao"], "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation", "comment": null, "summary": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.", "AI": {"tldr": "模仿学习（IL）在自动驾驶中因闭环误差累积导致性能下降。本文提出一种名为“专家序列”（SoE）的基于时间交替的策略，无需增加模型大小或数据，显著提升了闭环性能，并在nuPlan基准上达到了最先进水平。", "motivation": "模仿学习在自动驾驶中，尽管在开环设置下表现良好，但在闭环中由于小错误随时间累积，性能会意外下降，可能导致严重故障。当前研究主要通过更复杂的网络架构或高保真数据集来增强单时间点的状态级鲁棒性，但忽略了自动驾驶的连续时间特性，未能利用时间尺度来提升鲁棒性。", "method": "本文提出了一种名为“专家序列”（Sequence of Experts, SoE）的方法。这是一种时间交替策略（temporal alternation policy），旨在不增加模型大小或数据需求的情况下，提升模仿学习的闭环性能。", "result": "在大型自动驾驶基准nuPlan上的实验表明，SoE方法一致且显著地提升了所有评估模型的性能，并取得了最先进的（state-of-the-art）性能。", "conclusion": "SoE模块通过利用时间尺度来增强鲁棒性，为解决自动驾驶中模仿学习的闭环误差累积问题提供了一个新的视角。它有望为提高自动驾驶模型的训练效率提供关键且广泛适用的支持。"}}
{"id": "2512.13278", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13278", "abs": "https://arxiv.org/abs/2512.13278", "authors": ["Jiaru Zou", "Ling Yang", "Yunzhe Qi", "Sirui Chen", "Mengting Ai", "Ke Shen", "Jingrui He", "Mengdi Wang"], "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning", "comment": "Best Paper Award at ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence", "summary": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.", "AI": {"tldr": "AutoTool是一个使LLM智能体能够动态选择工具的框架，通过构建大型数据集和双阶段优化，在多种任务上超越现有方法，并展现出更好的泛化能力。", "motivation": "现有的LLM智能体假设工具集是固定的，这限制了它们对新工具或不断发展的工具集的适应性。", "method": "该研究首先构建了一个包含20万条数据、1000+工具和100+任务的显式工具选择理由数据集。在此基础上，AutoTool采用双阶段优化：(i) 基于监督学习和强化学习的轨迹稳定化以实现连贯推理；(ii) KL正则化的Plackett-Luce排序以优化一致的多步工具选择。研究人员使用AutoTool训练了Qwen3-8B和Qwen2.5-VL-7B两个基础模型。", "result": "AutoTool在十个不同基准测试中，以更少的参数持续优于先进的LLM智能体和工具集成方法。它在数学与科学推理方面平均提升6.4%，在基于搜索的问答中提升4.5%，在代码生成中提升7.7%，在多模态理解中提升6.9%。此外，AutoTool在推理过程中动态利用未见过的工具，展现出更强的泛化能力。", "conclusion": "AutoTool为LLM智能体提供了动态工具选择能力，在各种任务和不断发展的工具集中实现了卓越的性能和泛化能力。"}}
{"id": "2512.13131", "categories": ["cs.AI", "cs.CV", "cs.GR", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.13131", "abs": "https://arxiv.org/abs/2512.13131", "authors": ["Xin Guo", "Yifan Zhao", "Jia Li"], "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning", "comment": "IEEE Transactions on Image Processing", "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.", "AI": {"tldr": "本文提出了一种名为分层隐式周期性（HIP）的学习方法，用于从语音生成3D身体动作，旨在通过显式建模不同运动单元（头部、身体、手部）之间的内部和外部关联，解决现有方法生成不自然动作的问题。", "motivation": "现有从语音生成3D身体动作的方法（如GAN、VQ-VAE、扩散模型）通常采用端到端方案，但未能有效建模不同运动单元（头部、身体、手部）之间关键的内部和外部关联，导致生成的动作不自然且缺乏协调性。", "method": "本文提出了一种统一的分层隐式周期性（HIP）学习方法。该方法包含两个主要技术洞察：1) 利用周期性自编码器探索手势运动相位流形，以从真实分布中模仿人类自然动作，同时结合当前潜在状态中的非周期性信息以实现实例级多样性，从而解缠复杂的动作。2) 通过在学习过程中使用级联引导，建模面部动作、身体姿态和手部动作之间的层次关系。", "result": "通过在3D虚拟形象上进行大量实验，本文提出的方法在定量和定性评估方面均优于当前最先进的语音驱动手势生成方法。", "conclusion": "HIP方法通过有效建模运动单元间的内在关联和层次关系，显著提升了语音驱动3D手势生成的自然度和协调性，解决了现有方法的关键挑战。"}}
{"id": "2512.13102", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13102", "abs": "https://arxiv.org/abs/2512.13102", "authors": ["Rajeev Bhatt Ambati", "Tianyi Niu", "Aashu Singh", "Shlok Mishra", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions", "comment": null, "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.", "AI": {"tldr": "本研究将大型语言模型(LLMs)的重点从静态交互转向动态交互，探索学生主动向教师提问以获取信息并学习的有效策略。结果表明，学生主导的方法显著提高了性能，并且通过DPO训练可以提高提问质量和学习效率。", "motivation": "大型语言模型在静态交互方面表现出色，但现实世界场景（如教育辅导或医疗协助）需要主动获取信息和动态交互。LLMs需要识别不确定性、提出有针对性的问题并有效保留新知识。以往工作主要关注教师如何指导学生，而本研究旨在探索学生如何主动向教师提问以获取有用信息。", "method": "研究侧重于学生主动向教师提问的有效策略。在数学和编程基准测试中进行评估，这些基准测试中基线学生模型的初始表现接近零。为了提高问题质量，研究使用直接偏好优化（DPO）训练学生，并以自我或更强的学生模型作为指导。", "result": "学生主导的方法在Pass@k指标上，相对于静态基线模型，至少带来了0.5的绝对提升。通过自我或更强学生指导的DPO训练，使小型模型学会提出更好的问题，进一步提高了学习效率。", "conclusion": "学生主导的主动提问是大型语言模型在动态交互中有效获取知识的有效策略。通过DPO和指导进行训练，可以显著提高提问质量和学习效率，尤其对于小型模型而言。"}}
{"id": "2512.13545", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13545", "abs": "https://arxiv.org/abs/2512.13545", "authors": ["Yuxin Yang", "Hang Zhou", "Hourong Song", "Branilav Hrezdak", "Yinyi Yan"], "title": "Competent Discrete Time Modeling For analogue controlled PWM Converter Considering State-Feedback", "comment": null, "summary": "Ever since R.D.Middlebrook proposed the state space averaging notion. The small signal model has been widely used as a design tool to tune control parameters. As Moore's law is continuing and the AI chip's high demand for power consumption and dynamic response, the control bandwidth needs to be boosted. However, the average model has two basic assumptions: the low-frequency assumption, the small ripple assumption. In high-bandwidth design, these two assumptions are violated. In order to solve this, various methods have been proposed. This paper gives a comprehensive overview of the existing small signal model for PWM converters from the following perspectives: 1. model fidelity, 2. analytical tractability. 3. complexity of the derivation process and result 4.generality.", "AI": {"tldr": "该论文全面概述了现有PWM变换器的小信号模型，旨在解决传统模型在高带宽设计中因低频和小纹波假设失效而面临的挑战。", "motivation": "随着摩尔定律的持续和AI芯片对功耗及动态响应的高需求，控制带宽需要大幅提升。然而，R.D.Middlebrook提出的状态空间平均小信号模型依赖于低频和小纹波假设，这些假设在高带宽设计中不再成立，因此需要更精确的模型。", "method": "本文通过对现有PWM变换器小信号模型进行全面综述，从模型保真度、分析易处理性、推导过程和结果的复杂性以及通用性四个维度进行评估和比较。", "result": "论文综述了现有的PWM变换器小信号模型，并根据模型保真度、分析易处理性、复杂性和通用性对其进行了分类和评估，以解决传统模型在高带宽应用中的局限性。", "conclusion": "该论文对解决高带宽PWM变换器设计中传统小信号模型失效问题提供了现有解决方案的全面视角，并为未来模型选择和开发提供了参考框架。"}}
{"id": "2512.13100", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13100", "abs": "https://arxiv.org/abs/2512.13100", "authors": ["Guanhua Ji", "Harsha Polavaram", "Lawrence Yunliang Chen", "Sandeep Bajamahal", "Zehan Ma", "Simeon Adebola", "Chenfeng Xu", "Ken Goldberg"], "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning", "comment": null, "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.", "AI": {"tldr": "该研究提出了AugE-Toolkit机器人数据增强工具包和OXE-AugE数据集，通过增强现有机器人数据集来提高通用机器人策略在不同机器人实体上的泛化能力，并在物理实验中验证了其显著效果。", "motivation": "训练能控制多种机器人实体（不同机械臂和夹具组合）的通用机器人策略需要大量多样化的数据集。为每个新硬件平台重新收集演示和重新训练成本过高。现有的聚合数据集（如Open X-Embodiment, OXE）高度不平衡，主要机器人类型占据绝大部分数据，存在过拟合风险。", "method": "研究开发了可扩展的机器人增强管道AugE-Toolkit，并创建了高质量的开源数据集OXE-AugE。OXE-AugE通过9种不同的机器人实体增强了原始OXE数据集，使其轨迹数量增加三倍多，达到超过440万条。研究系统地探讨了机器人增强规模如何影响跨实体学习。", "result": "结果表明，用多样化的机械臂和夹具增强数据集，不仅能提高策略在增强机器人上的性能，还能提高其在未见过机器人以及原始机器人（在分布偏移下）上的性能。在物理实验中，最先进的通用策略（如OpenVLA和$π_0$）在OXE-AugE上进行微调后，在四个真实世界操作任务中，对以前未见过的机器人-夹具组合的成功率提高了24-45%。", "conclusion": "机器人数据增强是一种有效策略，可以显著提升通用机器人策略的跨实体学习能力和泛化性，尤其是在面对多样化和未见过的硬件平台时。"}}
{"id": "2512.13142", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13142", "abs": "https://arxiv.org/abs/2512.13142", "authors": ["Anika Sharma", "Malavika Mampally", "Chidaksh Ravuru", "Kandyce Brennan", "Neil Gaikwad"], "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels", "comment": null, "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.", "AI": {"tldr": "研究发现，大型语言模型（LLMs）在理解堕胎污名这一复杂的心理和生理现象时，缺乏多层次的连贯性，未能真正理解人们“不能说”的内容，尽管其语言表达得体。", "motivation": "随着大型语言模型越来越多地介入带有污名化的健康决策，其理解复杂心理和生理现象的能力却未得到充分评估。研究旨在探究AI是否能理解人们无法言说的内容，特别是LLMs能否连贯地呈现认知、人际和结构层面的堕胎污名。", "method": "研究系统地测试了五种主流LLMs，使用了627个不同人口统计学特征的虚拟角色，并采用经过验证的个体层面堕胎污名量表（ILAS）。通过多层次分析，考察了模型在认知层面（自我评判）、人际层面（预期评判和孤立）和结构层面（社区谴责和披露模式）以及整体污名方面的理解连贯性。", "result": "模型在所有层面上都未能通过真实理解的测试。它们高估了人际污名，低估了认知污名，假设社区谴责是统一的，引入了人类验证数据中不存在的人口统计学偏见，错过了经验证的污名-保密关系，并在理论构念内部自相矛盾。这些模式表明，当前的对齐方法确保了适当的语言，但未能实现连贯的多层次理解。", "conclusion": "当前的大型语言模型缺乏对心理和生理构念的连贯多层次理解。在高风险情境下，AI安全要求在设计（多层次连贯性）、评估（持续审计）、治理和监管（强制审计、问责制、部署限制）以及AI素养方面采取新方法，尤其是在理解人们无法言说之内容决定支持是帮助还是伤害的领域。"}}
{"id": "2512.12218", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12218", "abs": "https://arxiv.org/abs/2512.12218", "authors": ["Rheeya Uppaal", "Phu Mon Htut", "Min Bai", "Nikolaos Pappas", "Zheng Qi"], "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking", "comment": "Preprint", "summary": "Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.", "AI": {"tldr": "本文提出了一种评估视觉语言模型（VLMs）推理链视觉忠实度的新维度，专注于感知步骤是否基于图像。同时，引入了一种无需训练的自反思方法，以检测并局部重新生成不忠实的感知步骤，从而提高多模态推理的可靠性。", "motivation": "现有的VLM评估方法仅测量最终答案的准确性，无法区分模型通过视觉不忠实的中间步骤得出正确答案，或推理忠实但最终预测失败的情况。这限制了对模型能力和故障模式的理解。", "method": "研究提出了一个训练和参考无关的框架，将推理链分解为感知和推理步骤，并使用现成的VLM作为判断器来评估步骤级别的忠实度，并通过人类元评估验证了该方法。在此基础上，提出了一种轻量级的自反思程序，无需任何训练即可检测并局部重新生成不忠实的感知步骤。", "result": "在多个经过推理训练的VLM和以感知为主的基准测试中，所提出的方法在保持最终答案准确性的同时，降低了不忠实感知率，从而提高了多模态推理的可靠性。", "conclusion": "通过引入视觉忠实度作为评估维度并提出无需训练的自反思程序，本文显著提升了视觉语言模型多模态推理的可靠性，特别是在感知步骤的视觉接地性方面。"}}
{"id": "2512.13557", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13557", "abs": "https://arxiv.org/abs/2512.13557", "authors": ["Gabriel Ellemund", "Thomas Hübner", "Quentin Lété", "Stefano Bracco", "Matteo Fresia", "Gabriela Hug"], "title": "Bidding Aggregated Flexibility in European Electricity Auctions", "comment": null, "summary": "Bidding flexibility in day-ahead and intraday auctions would enable decentralized flexible resources, such as electric vehicles and heat pumps, to efficiently align their consumption with the intermittent generation of renewable energy. However, because these resources are individually too small to participate in those auctions directly, an aggregator (e.g., a utility) must act on their behalf. This requires aggregating many decentralized resources, which is a computationally challenging task. In this paper, we propose a computationally efficient and highly accurate method that is readily applicable to European day-ahead and intraday auctions. Distinct from existing methods, we aggregate only economically relevant power profiles, identified through price forecasts. The resulting flexibility is then conveyed to the market operator via exclusive groups of block bids. We evaluate our method for a utility serving the Swiss town of Losone, where flexibility from multiple heat pumps distributed across the grid must be aggregated and bid in the Swiss day-ahead auction. Results show that our method aggregates accurately, achieving 98% of the theoretically possible cost savings. This aggregation accuracy remains stable even as the number of heat pumps increases, while computation time grows only linearly, demonstrating strong scalability.", "AI": {"tldr": "本文提出了一种计算高效且高精度的方法，使聚合商能够将电动汽车和热泵等分散式灵活资源整合起来，通过识别经济相关的电力曲线并使用专属的块状投标组，参与日前和日内市场拍卖，从而有效匹配可再生能源的间歇性发电，同时实现显著的成本节约和良好的可扩展性。", "motivation": "分散式灵活资源（如电动汽车和热泵）需要有效调整其消耗以匹配间歇性可再生能源发电。然而，这些资源个体规模过小，无法直接参与市场拍卖。聚合商需要代表它们参与，但这涉及大量资源的聚合，是一个计算上的挑战。", "method": "本文提出了一种计算高效且高精度的方法。该方法通过价格预测识别并仅聚合经济相关的电力曲线，然后通过专属的块状投标组将聚合后的灵活性传达给市场运营商。该方法在瑞士洛索内镇的一个公用事业公司进行了评估，该公司负责聚合分布式热泵的灵活性并参与瑞士日前拍卖。", "result": "结果显示，该方法聚合准确，实现了理论上可能成本节约的98%。即使热泵数量增加，聚合精度也能保持稳定，而计算时间仅呈线性增长，这表明了强大的可扩展性。", "conclusion": "所提出的方法能够准确、高效且可扩展地聚合分散式灵活资源，使聚合商能够有效参与日前和日内市场拍卖，从而促进灵活资源与可再生能源的匹配，并实现显著的成本节约。"}}
{"id": "2512.12209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12209", "abs": "https://arxiv.org/abs/2512.12209", "authors": ["Zahra Dehghanian", "Morteza Abolghasemi", "Hamid Beigy", "Hamid R. Rabiee"], "title": "CineLOG: A Training Free Approach for Cinematic Long Video Generation", "comment": null, "summary": "Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.", "AI": {"tldr": "为解决现有视频合成模型在电影级属性（如摄像机轨迹、类型）控制上的不足及数据集问题，本文引入了高质量的CineLOG数据集和一种新颖的分阶段生成流程，显著提升了视频合成的精细控制能力和视觉质量。", "motivation": "当前的视频合成模型难以实现文本提示之外的精细控制，特别是对摄像机轨迹和电影类型等电影级属性。现有数据集存在数据不平衡、标签噪声或模拟与现实差距大的问题。", "method": "本文介绍了CineLOG数据集，包含5,000个高质量、平衡且未剪辑的视频片段，每个片段都带有详细的场景描述、基于标准电影分类法的明确摄像机指令和类型标签（覆盖17种摄像机运动和15种电影类型）。同时，提出了一种新颖的生成流程，将复杂的文本到视频（T2V）任务分解为四个更简单的阶段，利用更成熟的技术。为实现连贯的多镜头序列，还引入了一个轨迹引导过渡模块，用于生成平滑的时空插值。", "result": "广泛的人工评估表明，本文提出的流程在遵循特定摄像机和剧本指令方面显著优于最先进的端到端T2V模型，同时保持了专业的视觉质量。", "conclusion": "CineLOG数据集和所提出的生成流程有效解决了可控视频合成中的挑战，特别是在电影级属性的精细控制方面，通过提供高质量数据和创新的分阶段方法，实现了优越的控制精度和视觉效果。"}}
{"id": "2512.13279", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13279", "abs": "https://arxiv.org/abs/2512.13279", "authors": ["Jinrui Liu", "Jeff Wu", "Xuanguang Pan", "Gavin Cheung", "Shuai Ma", "Chongyang Tao"], "title": "AIR: Post-training Data Selection for Reasoning via Attention Head Influence", "comment": "19 pages", "summary": "LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.", "AI": {"tldr": "本文提出AIR框架，通过利用LLM检索头的注意力影响，无监督地选择高价值推理数据，以提高推理能力蒸馏的效率和准确性。", "motivation": "现有数据选择方法（如手动、基于启发式规则）在LLM推理能力蒸馏中未能捕捉单个推理步骤的因果重要性，导致蒸馏效率受限。", "method": "AIR框架首先识别现成模型中对推理至关重要的注意力头，然后构建一个禁用这些注意力头影响的弱化参考模型，最后通过量化由此产生的损失差异来计算“注意力影响分数”。该分数用于在步骤和样本层面进行细粒度评估，支持步骤级加权微调和全局样本选择。", "result": "在多个推理基准测试中，AIR持续提高推理准确性，优于启发式基线，并能有效隔离最关键的推理步骤和样本。", "conclusion": "AIR为LLM的推理能力蒸馏提供了一种机制驱动且数据高效的方法，提高了蒸馏效率和模型性能。"}}
{"id": "2512.12219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12219", "abs": "https://arxiv.org/abs/2512.12219", "authors": ["Zhi Chen", "Jingcai Guo", "Taotao Cai", "Yuxiang Cai"], "title": "Fine-Grained Zero-Shot Learning with Attribute-Centric Representations", "comment": "Preprint", "summary": "Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.", "AI": {"tldr": "本文提出了一种零样本学习框架（ACR），通过在表示学习阶段强制属性解耦，学习以属性为中心的表示，以解决细粒度识别中属性纠缠的问题，并在多个基准数据集上取得了最先进的结果。", "motivation": "识别未见过的细粒度类别需要模型能区分细微的视觉差异。传统模型在处理属性（如颜色、形状、纹理）时存在属性纠缠问题，将它们混淆成单一的视觉嵌入，导致关键区别被掩盖。现有的事后解决方案不足，因为它们在已经混合的表示上操作。", "method": "本文提出了一种零样本学习框架（ACR），通过在表示学习阶段强制属性解耦来学习以属性为中心的表示。ACR通过两个专家混合组件实现：1) 补丁专家混合（MoPE），通过双层路由机制嵌入到Transformer中，将图像块条件性地分派给专门的专家，确保连贯的属性族由专门的专家处理；2) 属性专家混合（MoAE）头部，将专家精炼的特征投影到稀疏、部分感知的属性图中，用于鲁棒的零样本分类。", "result": "在零样本学习基准数据集CUB、AwA2和SUN上，我们的ACR方法始终取得了最先进的结果。", "conclusion": "通过学习以属性为中心的表示，并利用补丁专家混合和属性专家混合进行属性解耦，本文提出的ACR框架有效解决了零样本细粒度识别中的属性纠缠挑战，显著提升了性能。"}}
{"id": "2512.13286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13286", "abs": "https://arxiv.org/abs/2512.13286", "authors": ["Youssra Rebboud", "Pasquale Lisena", "Raphael Troncy"], "title": "Integrating Causal Reasoning into Automated Fact-Checking", "comment": "Extended version of the accepted ACM SAC paper", "summary": "In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.", "AI": {"tldr": "该论文提出了一种结合事件关系提取、语义相似性计算和基于规则推理的方法，用于在事实核查中检测因果关系中的逻辑不一致，从而增强解释性。", "motivation": "在事实核查中，检测错误的因果关系是驳斥主张的常见原因，但当前的自动化事实核查方法缺乏专门的基于因果的推理，错失了提供语义丰富解释性的机会。", "method": "所提出的方法结合了事件关系提取、语义相似性计算和基于规则的推理，以检测主张和证据中提及的事件链之间的逻辑不一致。", "result": "该方法在两个事实核查数据集上进行了评估，建立了将细粒度因果事件关系整合到事实核查中的第一个基线，并增强了判决预测的解释性。", "conclusion": "将细粒度因果事件关系整合到事实核查中是可行的，并且能够有效检测逻辑不一致并提高判决解释性。"}}
{"id": "2512.13170", "categories": ["cs.RO", "cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13170", "abs": "https://arxiv.org/abs/2512.13170", "authors": ["Deepak Ingole", "Valentin Bhend", "Shiva Ganesh Murali", "Oliver Dobrich", "Alisa Rupenayan"], "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks", "comment": null, "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.", "AI": {"tldr": "本文提出了一种基于任务级性能反馈的迭代学习框架，用于自动调整非线性模型预测控制（NMPC）的权重矩阵，以适应重复性机器人任务中的环境变化和系统磨损。", "motivation": "制造过程常受环境漂移和系统磨损影响，即使在重复操作中也需要重新调整控制器。因此，需要一种在重复性任务中实现NMPC权重自适应调整的实用解决方案。", "method": "该方法受范数最优迭代学习控制（ILC）启发，通过任务重复自适应调整NMPC的Q和R权重，以最小化跟踪精度、控制努力和饱和度等关键性能指标（KPI）。它构建了一个经验敏感性矩阵来实现结构化权重更新，避免了传统基于梯度的NMPC求解器微分需求。", "result": "在UR10e机器人进行碳纤维缠绕的模拟中，该方法在仅4次在线重复后就收敛到接近最优的跟踪性能（RMSE在离线贝叶斯优化（BO）的0.3%以内），远优于BO算法所需的100次离线评估。", "conclusion": "该方法为重复性机器人任务中的自适应NMPC调优提供了一个实用解决方案，它结合了精心优化控制器的精度和在线自适应的灵活性。"}}
{"id": "2512.13298", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13298", "abs": "https://arxiv.org/abs/2512.13298", "authors": ["Anna Aksenova", "Boris Zverkov", "Nicola Dainese", "Alexander Nikitin", "Pekka Marttinen"], "title": "MiniLingua: A Small Open-Source LLM for European Languages", "comment": "9+6 pages, 6 figures and 3 tables in the main text. Code at https://github.com/MiniLingua-ai/training_artifacts", "summary": "Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.", "AI": {"tldr": "MiniLingua是一个十亿参数的多语言开源大模型，针对13种欧洲语言从头训练，在多项任务上超越了EuroLLM，并与更先进的模型保持竞争力。", "motivation": "大型语言模型存在计算成本高、隐私问题和以英语为中心等局限性。研究旨在开发高效、小参数量（约十亿参数）的模型，以实现强大的性能并支持设备端使用。", "method": "本文介绍了MiniLingua，一个十亿参数的多语言开源大模型，针对13种欧洲语言从头训练，旨在平衡语言覆盖范围和指令遵循能力。该模型经过指令微调。", "result": "经过指令微调的MiniLingua在摘要、分类以及开放式和封闭式问答任务上优于EuroLLM（一个训练方法相似但预算更高的模型）。此外，在开放式生成任务上，它与更先进的SOTA模型保持竞争力。", "conclusion": "MiniLingua证明了小参数量、多语言的大模型可以在计算成本、隐私和多语言支持方面提供高效且强大的解决方案，并释放了模型权重、分词器和源代码。"}}
{"id": "2512.13154", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13154", "abs": "https://arxiv.org/abs/2512.13154", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Joo Hyuk Jeon", "Jie Hao", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations", "comment": null, "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.", "AI": {"tldr": "本文提出MAC，一个多智能体澄清框架，通过策略性地管理澄清对话来解决对话代理中的用户歧义，从而提高任务成功率并减少对话轮次。", "motivation": "在多智能体对话系统中，歧义消解是一个关键且未被充分探索的挑战，尤其是在确定哪个智能体应发起澄清以及智能体在面临不确定或不完整用户输入时如何协调行动方面，仍存在何时打断用户以及如何构建最优澄清查询的问题。", "method": "本文提出了MAC（Multi-Agent Clarification），一个交互式多智能体框架，专门用于解决用户歧义。首先引入了一种新颖的用户歧义分类法来指导澄清策略。然后，MAC框架自主协调多个智能体与用户协同互动。", "result": "在MultiWOZ 2.4上的实证评估表明，在两个层面启用澄清功能使任务成功率提高了7.8%（从54.5%到62.3%），并通过预先获取所有所需的用户信息并最大程度地减少重复，将平均对话轮次从6.53减少到4.86。", "conclusion": "研究结果强调了主动用户交互和角色感知澄清对于更可靠的人机通信的重要性。"}}
{"id": "2512.13153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13153", "abs": "https://arxiv.org/abs/2512.13153", "authors": ["Ruiqi Yu", "Qianshi Wang", "Hongyi Li", "Zheng Jun", "Zhicheng Wang", "Jun Wu", "Qiuguo Zhu"], "title": "START: Traversing Sparse Footholds with Terrain Reconstruction", "comment": null, "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.", "AI": {"tldr": "START是一种单阶段学习框架，使四足机器人能够利用低成本视觉和本体感知重建局部地形高度图，从而在稀疏落脚点上实现敏捷、稳定的运动，并展现出卓越的零样本迁移能力。", "motivation": "现有方法在稀疏落脚点地形上存在局限：基于模型的控制器泛化性差且过于保守；端到端学习方法要么依赖引入噪声和复杂管道的高度图，要么从深度图像中隐式推断地形特征，导致关键几何线索缺失、学习效率低下和步态僵硬。", "method": "本文提出了START，一个单阶段学习框架。它仅利用低成本的机载视觉和本体感知来准确重建局部地形高度图，提供明确的中间表示来传达与稀疏落脚点区域相关的基本特征，从而支持全面的环境理解和精确的地形评估。", "result": "实验结果表明，START在多样化的真实世界场景中实现了零样本迁移，展示出卓越的适应性、精确的落脚点放置和鲁棒的运动能力。", "conclusion": "START通过显式的地形重建和高效的环境理解，成功克服了现有方法在稀疏落脚点地形上的局限性，实现了四足机器人在复杂环境中的敏捷、稳定和鲁棒的运动。"}}
{"id": "2512.13183", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13183", "abs": "https://arxiv.org/abs/2512.13183", "authors": ["Alfredo González-Calvin", "Juan F. Jiménez", "Héctor García de Marina"], "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification", "comment": null, "summary": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.", "AI": {"tldr": "本文提出了一种通过软化（mollification）方法来平滑移动机器人非可微路径的策略，使其满足现有路径跟踪算法的要求，同时计算效率高，并能控制曲率。", "motivation": "大多数移动机器人路径跟踪算法要求路径至少是二次连续可微的，这排除了许多方便但非可微的路径（如分段函数）。现有平滑方法（如样条插值或基于优化的方法）要么过于复杂，要么计算成本高昂。", "method": "该方法通过软化（mollification）来正则化非可微函数，用一个可微函数来近似任意路径，该近似可以达到任意精度。此外，还提供了一种系统性的方法来限制生成路径的曲率，并以连接一系列路点的路径为例进行了演示。", "result": "所提出的方法计算效率高，支持在微控制器上实时实现，并与标准轨迹跟踪和路径跟踪算法兼容。它能生成曲率受限的可行路径。", "conclusion": "通过软化方法，可以高效地从非可微输入生成平滑、可行的路径，从而解决了现有机器人控制算法的一个关键限制，并使其能够与标准算法兼容。"}}
{"id": "2512.12220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12220", "abs": "https://arxiv.org/abs/2512.12220", "authors": ["Minheng Ni", "Zhengyuan Yang", "Yaowen Zhang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Zhendong Wang", "Xiaofei Wang", "Shujie Liu", "Lei Zhang", "Wangmeng Zuo", "Lijuan Wang"], "title": "ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation", "comment": null, "summary": "We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.", "AI": {"tldr": "本文引入了ProImage-Bench，一个基于评分标准的基准测试，用于评估专业、科学精确的图像生成。研究发现现有模型在此方面表现不佳，但通过利用评分标准进行迭代细化，可以显著提高生成图像的科学保真度。", "motivation": "现有的图像生成模型能够生成视觉上逼真的图片，但在生成信息密集、科学精确的专业插图（如生物示意图、工程图纸）方面存在不足。研究旨在量化并改进模型在专业图像生成领域的表现。", "method": "研究引入了ProImage-Bench基准测试，包含从教科书和技术报告中收集的654张图，并为之构建了详细的图像指令和分层评分标准（6,076个标准，44,131个二元检查）。评分标准由大型多模态模型（LMM）从周边文本和参考图中提取。评估通过一个基于LMM的自动化裁判进行，该裁判采用惩罚机制来聚合子问题结果。研究还使用这些评分标准作为可操作的监督信号，通过迭代细化来改进编辑模型。", "result": "基准测试显示，尽管在开放领域表现强劲，但最佳基础模型在ProImage-Bench上仅达到0.791的评分标准准确率和0.553的综合标准分数，揭示了在精细科学保真度方面存在显著差距。通过将失败的检查反馈给编辑模型进行迭代细化，一个强大的生成器的评分标准准确率从0.653提升到0.865，标准分数从0.388提升到0.697。", "conclusion": "ProImage-Bench为专业图像生成提供了一个严格的诊断工具，并为改进符合规范的科学插图提供了可扩展的信号。"}}
{"id": "2512.13198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13198", "abs": "https://arxiv.org/abs/2512.13198", "authors": ["Hyun-Gi Lee", "Jaekyeong Han", "Minjun Kwon", "Hyeonuk Kwon", "Jooha Park", "Hoe Jin Ha", "Dong-Hwa Seo"], "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation", "comment": null, "summary": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.", "AI": {"tldr": "本文介绍了一种名为ALBATROSS的自动化机器人系统，能够在惰性气体手套箱内实现锂离子纽扣电池的电解液配制、组装和电化学评估，显著提高了电池测试的效率和数据质量。", "motivation": "随着电池技术向更高稳定性和能量密度发展，需要对各种组件配置进行广泛的电池级测试。传统的纽扣电池组装和测试过程耗时且费力，阻碍了高通量筛选研究。", "method": "研究开发了ALBATROSS系统，一个集成了定制机器人夹具和3D打印结构的全自动锂离子电池测试机器人系统。该系统集成在充满氩气的手套箱中，能够自动进行电解液配制、纽扣电池组装和电化学评估，无需研究人员干预即可处理多达48个电池。", "result": "ALBATROSS系统实现了高组装可靠性，NCM811||Li半电池的放电容量相对标准偏差（RSD）小于1.2%，EIS测量标准偏差小于3 Ω。", "conclusion": "ALBATROSS系统凭借其高可靠性和自动化能力，能够获取高质量的纽扣电池数据集，有望加速下一代电解液的开发。"}}
{"id": "2512.13214", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13214", "abs": "https://arxiv.org/abs/2512.13214", "authors": ["Diego Bolliger", "Gabriele Fadini", "Markus Bambach", "Alisa Rupenyan"], "title": "Differentiable Material Point Method for the Control of Deformable Objects", "comment": "7 Pages, 4 Figures, 1 Table", "summary": "Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.", "AI": {"tldr": "该工作提出了一个可微分的物质点法（MPM）模拟器，用于控制柔性物体的形变。在超弹性绳索的主动阻尼问题中，该模拟器能比基线MPPI方法更快、更有效地优化控制轨迹，且计算成本显著降低。", "motivation": "柔性物体的形变控制极具挑战性，因为它们具有非线性动力学特性和高维配置空间。", "method": "该研究提出了一个可微分的物质点法（MPM）模拟器，并利用其可微分性来优化超弹性绳索主动阻尼问题中的控制轨迹。", "result": "与基线MPPI方法相比，该模拟器将绳索的动能最小化速度提高了约2倍，能量水平降低了20%，同时计算时间仅为后者的约3%。", "conclusion": "可微分MPM模拟器在柔性物体的主动阻尼控制应用中表现出显著的效率和有效性，为柔性物体控制提供了新的解决方案。"}}
{"id": "2512.13159", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13159", "abs": "https://arxiv.org/abs/2512.13159", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Jie Hao", "Joo Hyuk Jeon", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning", "comment": null, "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.", "AI": {"tldr": "本文提出SpeakRL，一种强化学习方法，通过奖励主动澄清互动来增强代理的对话能力，从而在人机协作中实现更高的任务完成率。", "motivation": "当前人机协作模式以单向指令为主，代理缺乏主动澄清用户意图、解决歧义的能力。现有工作未能充分利用大型语言模型的对话潜力，使代理更像追随者而非有效的对话者，限制了协作效率。", "method": "引入SpeakRL，一种强化学习方法，通过奖励代理的主动互动（如提出必要的澄清问题）来提升其对话能力。构建了SpeakER合成数据集，包含通过交互式澄清问题解决任务的对话场景。系统分析了对话主动性的奖励设计，并提出了一种平衡提问与行动的原则性奖励公式。", "result": "经验评估显示，与基础模型相比，该方法在任务完成率上绝对提高了20.14%，且未增加对话轮次，甚至超越了更大的专有模型，证明了以澄清为中心的用户-代理交互的潜力。", "conclusion": "通过引入SpeakRL和SpeakER数据集，并优化奖励设计，本文成功提升了代理的主动对话能力，显著改善了人机协作中的任务完成率，揭示了以澄清为核心的交互方式在未来人机协作中的巨大前景。"}}
{"id": "2512.12222", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12222", "abs": "https://arxiv.org/abs/2512.12222", "authors": ["Nathalie Alexander", "Arnaud Gucciardi", "Umberto Michelucci"], "title": "Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs", "comment": null, "summary": "Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.", "AI": {"tldr": "本研究比较了SynthSeg和SamSeg在婴儿脑部MRI分割中的表现。结果显示，SynthSeg在体积和分形维数（FD）估计方面更可靠，但分割相关的不确定性可能影响FD结果的解释。", "motivation": "婴儿脑部MRI的精确分割对于量化结构和复杂性的发育变化至关重要。然而，持续的髓鞘形成和组织对比度降低使得自动化分割极具挑战性，因此需要系统评估不同分割方法的准确性及其对体积和分形维数估计的影响。", "method": "研究使用Baby Open Brains (BOB) 数据集（71次扫描，1-9个月），系统比较了SynthSeg和SamSeg两种分割方法与专家标注的准确性。评估指标包括Dice系数、交并比（IoU）、95百分位Hausdorff距离和归一化互信息。同时，还评估了这些方法对体积和分形维数（FD）估计的影响，并使用Bland-Altman分析和相关性分析来评估一致性和偏差。", "result": "SynthSeg在所有质量指标上均优于SamSeg（主要区域平均Dice > 0.8），其体积估计与手动参考非常接近（平均+4%）。SamSeg则系统性地高估了脑室和全脑体积（平均+76%）。分割准确性随年龄增长而提高。分形维数分析显示，SynthSeg与专家分割之间存在显著的区域差异，且分割相关的FD变异性超出了大多数发育队列中报告的组间差异。体积和FD偏差呈正相关。", "conclusion": "SynthSeg为儿科MRI提供了最可靠的体积和分形维数结果。然而，由于分割相关的不确定性，对体积和FD的微小形态差异应谨慎解释。"}}
{"id": "2512.13330", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13330", "abs": "https://arxiv.org/abs/2512.13330", "authors": ["Joona Kytöniemi", "Jousia Piha", "Akseli Reunamo", "Fedor Vitiugin", "Farrokh Mehryary", "Sampo Pyysalo"], "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models", "comment": null, "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.", "AI": {"tldr": "FIN-bench-v2 是一个统一的芬兰语大型语言模型评估基准套件，整合了多种任务和数据集，并经过严格的任务选择过程，确保其鲁棒性。", "motivation": "需要一个统一、一致格式且经过验证的基准来评估芬兰语大型语言模型，涵盖阅读理解、常识推理、情感分析、世界知识和对齐等多种任务。", "method": "该研究将芬兰语版本的广泛使用的基准与更新的 FIN-bench 整合到一个统一的集合中。所有数据集都被转换为 HuggingFace Datasets 格式，包含完形填空和多项选择两种提示形式，每项任务有五种变体。对机器翻译资源进行了人工标注或审查。通过预训练 2.15B 参数的模型并分析其学习曲线，计算单调性、信噪比、非随机性能和模型排序一致性来选择鲁棒的任务。此外，还评估了一组更大的指令微调模型以表征性能。", "result": "FIN-bench-v2 提供了一个统一、格式一致的基准套件，涵盖了芬兰语大型语言模型的多种多项选择和生成任务。通过严格的任务选择过程，确保了基准任务的鲁棒性。评估了更大的指令微调模型，以表征其在不同任务和提示形式下的性能。所有数据集、提示和评估配置均已公开。", "conclusion": "FIN-bench-v2 成功地构建了一个全面且鲁棒的芬兰语大型语言模型评估基准套件，为芬兰语 LLM 的研究和开发提供了宝贵的工具和资源，并已全部公开可用。"}}
{"id": "2512.13215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13215", "abs": "https://arxiv.org/abs/2512.13215", "authors": ["Yinsong Qu", "Yunxiang Li", "Shanlin Zhong"], "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment", "comment": "9 pages, 11 figures, conference paper for the 2025 International Conference on Advanced Robotics and Mechatronics (ICARM), accepted", "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).", "AI": {"tldr": "本文提出了一种改进的顺序模型预测控制（ISMPC）导航框架，通过多方向安全矩形走廊算法和集成了走廊与障碍函数约束的MPC，解决了自主移动机器人（AMRs）在复杂环境中导航的挑战，提高了空间利用率并保持实时性能。", "motivation": "AMRs在工业应用中不可或缺，但其在密集、杂乱和半结构化环境中导航面临挑战，主要原因是非完整车辆动力学、与静态/动态障碍物的交互以及非凸操作空间的限制。", "method": "本文将导航任务重新定义为顺序切换最优控制问题。提出并实施了两个关键创新：1) 多方向安全矩形走廊（MDSRC）算法，通过矩形凸区域编码自由空间以避免静态障碍物，减少计算负担并加速求解器收敛；2) 一个集成了走廊约束和障碍函数约束的顺序MPC导航框架，用于实现静态和动态障碍物避障。该框架直接生成AMR的速度指令，简化了传统导航算法架构。", "result": "对比实验表明，该框架在自由空间利用率方面表现优越（平均走廊面积增加41.05%），同时保持了实时计算性能（平均走廊生成延迟为3毫秒）。", "conclusion": "ISMPC导航框架能够有效解决AMRs在复杂环境中的导航问题，通过创新的走廊算法和MPC集成，显著提高了空间利用率和计算效率，并简化了导航算法架构。"}}
{"id": "2512.13363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13363", "abs": "https://arxiv.org/abs/2512.13363", "authors": ["Shibani Sankpal"], "title": "Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers", "comment": "14 pages, 12 figures", "summary": "This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.", "AI": {"tldr": "本研究通过使用预训练的Transformer模型（如DistilBERT和RoBERTa）检测心理健康相关文本中句子级的情绪，并测量情绪漂移分数，以揭示情绪在文本中的变化模式。", "motivation": "传统情感分析通常将整条信息归类为积极、消极或中性，而忽略了信息中情绪的细微变化，尤其是在心理健康对话中，这种情绪的动态变化至关重要。", "method": "使用预训练的Transformer模型（如DistilBERT和RoBERTa）检测文本中的句子级情绪，并计算情绪漂移分数来量化情绪状态的变化。", "result": "研究结果揭示了心理健康对话中情绪升级或缓解的模式，为理解情绪动态提供了深刻见解。", "conclusion": "该方法可应用于更好地理解各种内容中的情绪动态。"}}
{"id": "2512.13477", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13477", "abs": "https://arxiv.org/abs/2512.13477", "authors": ["Timothy A. Brumfiel", "Revanth Konda", "Drew Elliott", "Jaydev P. Desai"], "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model", "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)", "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.", "AI": {"tldr": "本文评估了一种简化的COAST导丝机器人在模拟脉动血流的人体模型中导航复杂血管的能力。", "motivation": "手动导丝导航在血管内介入手术中存在挑战，机器人可控导丝有望通过主动转向导丝尖端来提高可操作性和导航能力。", "method": "研究评估了一种简化版COAST导丝机器人（由两根而非三根导管组成）的性能。通过在具有脉动血流的解剖模型中进行导航实验来验证其有效性。", "result": "修改后的COAST导丝机器人能够有效地导航复杂的血管模型。", "conclusion": "简化的COAST导丝机器人能有效导航迂曲的血管结构，展示了其在血管内介入中的潜力。"}}
{"id": "2512.13240", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13240", "abs": "https://arxiv.org/abs/2512.13240", "authors": ["Zihui Zhao", "Zechang Li"], "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.", "AI": {"tldr": "本文提出反射偏好优化（RPO）框架，通过引入外部模型生成的反射提示，增强DPO的偏好学习信号，从而提高大型语言和视觉-语言模型的对齐效果，减少幻觉。", "motivation": "标准DPO中，选择和拒绝响应由同一策略生成，导致两者相似且KL散度小，学习信号弱，收敛慢且不稳定。这限制了DPO在对齐模型方面的有效性。", "method": "RPO将提示引导的反射机制融入DPO范式。它利用外部模型识别幻觉来源并生成简洁的反射提示。这些提示用于构建具有更强对比度和更清晰偏好信号的策略内偏好对。理论分析表明，以提示为条件可增加预期偏好裕度，提高样本效率，同时保持在策略分布族内。", "result": "RPO在更少的训练样本和迭代次数下实现了卓越的模型对齐，显著降低了幻觉率，并在多模态基准测试中取得了最先进的性能。", "conclusion": "RPO通过引入反射提示有效解决了标准DPO学习信号弱的问题，显著提高了模型对齐效果、样本效率并降低了幻觉率，为大型语言和视觉-语言模型的对齐提供了一个强大且高效的替代方案。"}}
{"id": "2512.12229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12229", "abs": "https://arxiv.org/abs/2512.12229", "authors": ["Tianyu Zhang", "Dong Liu", "Chang Wen Chen"], "title": "Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder", "comment": null, "summary": "Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.", "AI": {"tldr": "本文提出了一种名为AEIC的非对称极致图像压缩框架，通过使用浅层编码器和一步扩散解码器，在超低比特率下实现了高编码效率和高质量的图像重建，特别适用于边缘设备。", "motivation": "现有超低比特率图像压缩方法依赖于大型预训练编码器（如VAE或基于tokenizer的模型），这使得它们不适用于带宽受限和计算能力有限的发送端设备（如边缘设备），因此需要探索使用浅层编码器来实现高效率且高质量的压缩。", "method": "本文提出了非对称极致图像压缩（AEIC）框架。它采用中等甚至浅层的编码网络，同时利用一步扩散解码器在极端比特率下保持高保真度和真实感重建。为进一步提高浅层编码器的效率，设计了一种双边特征蒸馏方案，将知识从中等编码器版本的AEIC转移到其浅层编码器变体。", "result": "实验表明，AEIC在超低比特率下不仅在率失真感知性能上优于现有方法，而且在1080P输入图像上实现了35.8 FPS的卓越编码效率，同时保持了与现有方法相当的解码速度。", "conclusion": "AEIC框架成功地实现了编码简单性和解码质量的平衡，通过浅层编码器和扩散解码器在超低比特率下提供了卓越的性能和效率，使其非常适合带宽和计算受限的场景部署。"}}
{"id": "2512.13168", "categories": ["cs.AI", "cs.CE", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.13168", "abs": "https://arxiv.org/abs/2512.13168", "authors": ["Haoyu Dong", "Pengkun Zhang", "Yan Gao", "Xuanyu Dong", "Yilin Cheng", "Mingzhe Lu", "Adina Yakefu", "Shuxin Zheng"], "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "comment": null, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "AI": {"tldr": "本文引入了一个名为Finch的金融与会计基准，用于评估AI代理在真实企业级专业工作流程中的表现。该基准源于真实企业数据，包含多模态、复杂且知识密集型任务。对前沿AI系统的评估显示，它们在这些真实世界的企业工作流程中表现不佳。", "motivation": "研究动机是为了评估AI代理在真实世界、企业级专业工作流程中的能力，这些工作流程涉及数据输入、结构化、格式化、网络搜索、跨文件检索、计算、建模、验证、翻译、可视化和报告等复杂任务，且数据具有真实环境的混乱性。", "method": "研究引入了Finch基准，其数据来源于安然公司（15,000个电子表格和500,000封电子邮件）及其他金融机构的真实企业工作空间，保留了多模态（文本、表格、公式、图表、代码和图像）的混乱性。工作流程构建结合了LLM辅助发现和专家注释（超过700小时的专家工作），产生了172个复合工作流程和384个任务。研究对GPT 5.1、Claude Sonnet 4.5、Gemini 3 Pro、Grok 4和Qwen 3 Max等前沿AI系统进行了人工和自动化评估。", "result": "Finch基准成功捕捉了真实企业工作固有的混乱、长周期、知识密集和协作性质。评估结果显示，前沿AI系统在这些任务中表现不佳：GPT 5.1 Pro总共花费48小时，仅通过38.4%的工作流程，而Claude Sonnet 4.5仅通过25.0%。全面的案例研究进一步揭示了真实世界企业工作流程对AI代理构成的挑战。", "conclusion": "真实世界的企业级工作流程，尤其是在金融和会计领域，对当前的AI代理构成了巨大的挑战。即使是前沿的AI系统，在处理这些复杂、多模态且知识密集型任务时，其表现也远未达到令人满意的水平。"}}
{"id": "2512.13262", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13262", "abs": "https://arxiv.org/abs/2512.13262", "authors": ["Hyunki Seong", "Jeong-Kyun Lee", "Heesoo Myeong", "Yongho Shin", "Hyun-Mook Cho", "Duck Hoon Kim", "Pranav Desai", "Monu Surana"], "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving", "comment": "11 pages, 5 figures", "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.", "AI": {"tldr": "本文提出两种互补策略，通过强化学习后训练（GRBO）和测试时采样策略（Warm-K），显著提升了自动驾驶中多智能体交互行为学习的安全性和鲁棒性，同时保持行为真实性。", "motivation": "自动驾驶中多智能体交互运动行为学习面临挑战：模仿学习模型因数据集偏见（多为安全演示）导致在安全关键场景下鲁棒性不足；大多数研究采用开环评估，忽略了闭环执行中累积误差的影响。", "method": "1. **组相对行为优化 (GRBO)**：一种强化学习后训练方法，通过组相对优势最大化和人类正则化，对预训练行为模型进行微调。2. **Warm-K采样策略**：一种暖启动的Top-K采样策略，用于平衡运动选择的一致性和多样性，并在测试时进行行为缩放。", "result": "1. **GRBO**：仅使用10%的训练数据集，将安全性能提高了40%以上，同时保持了行为的真实性。2. **Warm-K**：在无需重新训练的情况下，增强了测试时的行为一致性和反应性，缓解了协变量偏移并减少了性能差异。", "conclusion": "通过GRBO的强化学习后训练和Warm-K的测试时采样策略，有效解决了模仿学习在自动驾驶多智能体交互中的安全性和鲁棒性问题，显著提升了模型性能和行为一致性。"}}
{"id": "2512.12268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12268", "abs": "https://arxiv.org/abs/2512.12268", "authors": ["Yuqing Lei", "Yingjun Du", "Yawen Huang", "Xiantong Zhen", "Ling Shao"], "title": "MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models", "comment": "NeurIPS 2025 Workshop", "summary": "Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.", "AI": {"tldr": "MetaTPT是一个元学习框架，通过学习自监督辅助任务来动态生成增强视图，以指导测试时提示调优，从而显著提高视觉语言模型在领域偏移下的适应性。", "motivation": "视觉语言模型（如CLIP）在零样本泛化能力强，但在测试时对领域偏移敏感。现有的测试时提示调优（TPT）使用固定增强，在更具挑战性的场景中表现不佳。", "method": "MetaTPT是一个元学习框架，它学习一个自监督辅助任务来指导测试时提示调优。该辅助任务为每个样本动态学习参数化增强，生成信息丰富的视图。MetaTPT采用双循环优化范式：内循环学习自监督任务以生成视图，外循环通过强制这些视图之间的一致性进行提示调优。", "result": "MetaTPT在领域泛化和跨数据集基准测试中取得了最先进的性能。", "conclusion": "通过将增强学习与提示调优相结合，MetaTPT显著改善了领域偏移下的测试时适应性。"}}
{"id": "2512.13441", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.13441", "abs": "https://arxiv.org/abs/2512.13441", "authors": ["Johan J. Bolhuis", "Andrea Moro", "Stephen Crain", "Sandiway Fong"], "title": "Large language models are not about language", "comment": null, "summary": "Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.", "AI": {"tldr": "本文认为大型语言模型（LLMs）对语言学无用，因为它们是依赖大量数据分析外部字符串的概率模型，而人类语言是内部的、递归的、依赖少量输入生成分层思想结构的计算系统。", "motivation": "研究动机源于对大型语言模型（LLMs）与人类语言系统之间根本差异的认识，即LLMs是外部的、概率性的、数据驱动的，而人类语言是内部的、计算性的、高效且数据需求极低的。", "method": "本文采用概念分析和对比的方法，将大型语言模型（LLMs）的运作方式（概率性、外部字符串分析、大量数据需求）与人类语言的内在机制（心智内部计算系统、递归生成分层结构、少量输入增长、区分真实与不可能语言的能力）进行比较。", "result": "主要观点是大型语言模型因其概率性、对海量数据的依赖以及仅分析外部字符串的特性，无法有效捕捉人类语言作为一种心智内部、递归且高效的计算系统的本质，因此对语言学研究而言是无用的。", "conclusion": "结论是大型语言模型（LLMs）与人类语言系统的基本性质存在根本性差异，使得LLMs无法成为理解或分析人类语言内在计算机制的有效工具。"}}
{"id": "2512.13297", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13297", "abs": "https://arxiv.org/abs/2512.13297", "authors": ["Zhenghao Zhu", "Chuxue Cao", "Sirui Han", "Yuanfeng Song", "Xing Chen", "Caleb Chen Cao", "Yike Guo"], "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data", "comment": null, "summary": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.", "AI": {"tldr": "本文介绍了MedInsightBench，首个用于评估大型多模态模型(LMMs)医疗洞察发现能力的基准，并提出了MedInsightAgent，一个自动化代理框架，以提高LMMs在该领域的性能。", "motivation": "医疗数据分析中，从复杂、多模态数据集中提取深度洞察至关重要，但目前缺乏专门用于评估LMMs发现医疗洞察能力的高质量数据集。", "method": "引入了MedInsightBench，一个包含332个精心策划并标注了洞察的医疗案例的基准。提出了MedInsightAgent，一个用于医疗数据分析的自动化代理框架，由视觉根查找器、分析洞察代理和后续问题编撰器三个模块组成。", "result": "现有LMMs在MedInsightBench上的表现有限，主要原因在于它们在提取多步骤、深度洞察方面的挑战以及缺乏医学专业知识。实验表明，MedInsightAgent可以提高通用LMMs在医疗洞察发现方面的性能。", "conclusion": "MedInsightBench揭示了当前LMMs在医疗洞察发现方面的局限性，而MedInsightAgent为提高LMMs在此领域的性能提供了一个有效的解决方案。"}}
{"id": "2512.12246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12246", "abs": "https://arxiv.org/abs/2512.12246", "authors": ["I Putu Andika Bagas Jiwanta", "Ayu Purwarianti"], "title": "Moment and Highlight Detection via MLLM Frame Segmentation", "comment": null, "summary": "Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous \"0\" and/or \"1\" characters, with one character per frame. The \"0\"/\"1\" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.", "AI": {"tldr": "本文提出一种新颖方法，通过将分割目标直接应用于多模态大语言模型（MLLM）的输出token（表示帧级别的“0”/“1”字符），解决了现有MLLM方法无法提供直接帧级梯度的问题，并在视频高光检测和时刻检索任务上取得了高效且强大的性能。", "motivation": "基于Transformer和多模态大语言模型（MLLM）的方法在视频时刻和高光检测中表现出色，但它们通常以文本时间戳形式生成预测，这导致模型无法获得直接的帧级别梯度。尽管强化学习（RL）方法试图解决此问题，但仍需更直接、更稳定的方法来桥接语言输出与帧级别预测之间的鸿沟。", "method": "研究者提出一种新方法，将分割目标直接应用于LLM的输出token。LLM接收固定数量的帧和一个提示，强制其输出一系列连续的“0”和/或“1”字符，每个字符对应一帧。这些“0”/“1”字符利用LLM固有的语言能力，同时分别充当背景和前景的概率。训练时，模型在这些概率上使用分割损失，并结合正常的因果语言模型损失。推理时，通过束搜索生成序列和logits，分别作为时刻和显著性分数。该方法仅采样25帧。", "result": "尽管只采样了25帧（不到同类方法的一半），该方法在QVHighlights数据集上实现了强大的高光检测性能（56.74 HIT@1）。此外，其高效的时刻检索方法也超过了基线（35.28 MAP）。经验证明，即使因果语言模型损失趋于平稳，分割损失也能提供稳定的互补学习信号。", "conclusion": "通过将分割目标直接应用于LLM的输出token，本研究成功地为MLLM在视频理解任务中提供了直接的帧级别预测能力。该方法不仅在视频高光检测和时刻检索任务上实现了卓越且高效的性能，而且通过引入分割损失，为模型的稳定学习提供了关键的补充信号。"}}
{"id": "2512.13323", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13323", "abs": "https://arxiv.org/abs/2512.13323", "authors": ["Árpád Pándy", "Róbert Lakatos", "András Hajdu"], "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning", "comment": null, "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.", "AI": {"tldr": "本文提出了一种错误驱动的优化框架，通过迭代细化提示规则，显著提升了本地小型语言模型（如Qwen3 4B）在表格数据算术推理任务中的准确性，使其在隐私合规的前提下超越了大型模型。", "motivation": "在金融和医疗等受监管行业中，工业智能体需要对表格数据进行准确的算术操作，同时确保敏感信息不离开本地安全环境。然而，现有模型在算术任务上存在局限性。", "method": "研究引入了一个错误驱动的算术推理优化框架，应用于本地小型语言模型（SLM）的Code Generation Agent (CGA)。该方法通过聚类错误的预测，迭代地细化提示规则（prompt-rules）。", "result": "基线Qwen3 4B模型在算术任务上表现出基本限制。通过提出的错误驱动方法，模型性能显著提高，准确率达到70.8%。结果表明，该方法使小型模型在隐私合规的前提下超越了大型语言模型（GPT-3.5 Turbo）。", "conclusion": "开发可靠、可解释且可工业部署的AI助手，不仅可以通过昂贵的微调实现，也可以通过系统性的、错误驱动的提示优化来实现，从而使小型模型能够超越大型语言模型，同时满足隐私合规要求。"}}
{"id": "2512.13271", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13271", "abs": "https://arxiv.org/abs/2512.13271", "authors": ["Fangju Yang", "Hang Yang", "Ibrahim Alsarraj", "Yuhao Wang", "Ke Wu"], "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation", "comment": null, "summary": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.", "AI": {"tldr": "本文提出了一种名为LASEM的轻量级驱动空间能量建模框架，用于索驱动连续体机器人(CDCRs)的实时动态建模，显著提高了计算效率和准确性，并支持多种驱动模式。", "motivation": "索驱动连续体机器人(CDCRs)需要准确、实时的动态模型，以实现高速动态预测或基于模型的控制，现有方法在这方面存在不足。", "method": "该研究提出了轻量级驱动空间能量建模(LASEM)框架，直接在驱动空间中建立驱动势能。通过统一的变分推导，将控制动力学简化为单个偏微分方程(PDE)，仅需欧拉力矩平衡，并隐式包含牛顿力平衡。该方法避免了显式计算缆绳-骨架接触力，简化了模型结构，同时保持了几何精度和物理一致性。此外，它原生支持力输入和位移输入两种驱动模式。通过伽辽金时空模态离散化和约简状态的分析时间域导数，进一步提高了计算速度。", "result": "LASEM框架实现了轻量级而准确的动态建模，简化了模型结构，提高了计算效率，同时保持了几何精度和物理一致性。重要的是，该框架原生支持力输入和位移输入两种驱动模式。与现有最先进的实时动态建模方法相比，该框架平均实现了62.3%的计算速度提升。", "conclusion": "LASEM框架为索驱动连续体机器人提供了一种高效、准确的实时动态建模解决方案，其轻量级结构和多驱动模式支持能力，显著优于现有方法，对于CDCRs的高速控制和预测具有重要意义。"}}
{"id": "2512.13219", "categories": ["cs.RO", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.13219", "abs": "https://arxiv.org/abs/2512.13219", "authors": ["Christoph Hartmann", "Marios Demetriades", "Kevin Prüfer", "Zichen Zhang", "Klaus Spindler", "Stefan Weltge"], "title": "A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization", "comment": "Code available at https://github.com/TUM-utg/PyCAALP (repository will be made public prior to publication)", "summary": "This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.", "AI": {"tldr": "本文提出了PyCAALP，一个基于Python的计算机辅助装配线规划框架，用于自动化装配序列规划（ASP）和生产线规划（PLP），它采用图方法建模组件和关节，并集成运动学边界条件和启发式方法来管理复杂性。", "motivation": "研究动机是为了自动化装配序列规划（ASP）和生产线规划（PLP），解决其固有的高组合复杂性，并确保自动化装配规划的可行性。", "method": "该框架名为PyCAALP，采用图基方法建模生产模块中的组件和关节。它集成了运动学边界条件（如零件碰撞）以确保可行性，并开发算法计算所有可行的生产序列，包括空间关系检测和几何约束公式化。算法还考虑了搬运可行性、公差匹配和关节兼容性等属性，并通过单件流装配和几何约束强制等启发式方法来精炼解决方案空间。PLP阶段被建模为混合整数规划（MIP），并通过复杂性降低技术显著减少计算时间。", "result": "PyCAALP能够计算所有可行的生产序列，通过集成模块检测空间关系并制定几何约束。它成功地管理了装配序列生成中的高组合复杂性，并利用启发式方法更有效地规划复杂装配。PLP阶段通过平衡固定数量制造站的总时间，显著减少了MIP的计算时间。框架还支持工程约束的定制以及ASP和PLP之间的灵活权衡。", "conclusion": "PyCAALP是一个开放源代码的灵活框架，能够实现自动化、高效的装配序列规划和生产线规划，通过其图基方法、集成约束和启发式方法，促进了在工业和生产研究应用中的进一步协作和采用。"}}
{"id": "2512.13472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13472", "abs": "https://arxiv.org/abs/2512.13472", "authors": ["Jian Yang", "Shawn Guo", "Lin Jing", "Wei Zhang", "Aishan Liu", "Chuan Hao", "Zhoujun Li", "Wayne Xin Zhao", "Xianglong Liu", "Weifeng Lv", "Bryan Dai"], "title": "Scaling Laws for Code: Every Programming Language Matters", "comment": null, "summary": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.", "AI": {"tldr": "本文首次系统探索了多语言代码预训练的缩放定律，发现不同编程语言对模型规模和数据量有差异化收益，多语言预训练存在协同效应，并提出了一种比例依赖的多语言缩放定律，通过优化训练令牌分配来提升代码大模型的综合性能。", "motivation": "现有的代码大模型缩放定律忽视了不同编程语言在预训练中的不同影响，导致性能预测不准确，且忽略了现代软件开发固有的多语言特性。因此，有必要首先研究不同编程语言的缩放定律，然后考虑它们之间的相互影响，以得出最终的多语言缩放定律。", "method": "研究进行了1000多次实验（相当于336,000+ H800小时），涵盖多种编程语言、模型大小（0.2B至14B参数）和数据集大小（1T tokens）。实验建立了跨多种编程语言的代码LLM综合缩放定律，并探索了并行配对（将代码片段与其翻译连接）的预训练策略。", "result": "解释型语言（如Python）比编译型语言（如Rust）从模型规模和数据量的增加中获益更多。多语言预训练提供了协同效益，尤其是在句法相似的编程语言之间。并行配对的预训练策略显著增强了跨语言能力，并具有良好的缩放特性。最终提出了一种比例依赖的多语言缩放定律，通过优先考虑高价值语言、平衡高协同效应语言对和减少对快速饱和语言的分配，在相同计算预算下，实现了优于均匀分布的跨所有编程语言的平均性能。", "conclusion": "研究揭示了多语言代码预训练的复杂缩放特性，并提出了一种比例依赖的多语言缩放定律。通过优化训练令牌的分配，该定律能够有效提升代码大模型在多语言环境下的综合性能，为未来高效训练代码LLM提供了指导。"}}
{"id": "2512.13478", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13478", "abs": "https://arxiv.org/abs/2512.13478", "authors": ["Kei Saito"], "title": "Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models", "comment": "19 pages", "summary": "Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing \"Dr. Smith the cardiologist\" from \"Dr. Smith the researcher\"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.", "AI": {"tldr": "本文提出非解析推理（NRR）框架，通过多向量嵌入、非坍塌注意力及上下文身份追踪，解决现有语言模型过早语义坍塌问题，从而在推理过程中保留语义歧义，提高上下文处理和推理能力。", "motivation": "当前语言模型因softmax竞争和贪婪解码，导致过早地确定单一含义，在缺乏足够上下文时丢弃有效解释，造成推理脆弱和上下文处理失败。", "method": "NRR框架包含三个核心组件：1) 多向量嵌入，为每个token维护多个可行解释；2) 非坍塌注意力，防止层间出现赢者通吃动态；3) 上下文身份追踪（CIT），为重复实体分配上下文特定身份。这些机制通过一个外部解析操作符$ρ$统一，使语义承诺变得明确、可控且依赖于任务。", "result": "在合成评估中，NRR展示了保留歧义和追踪上下文的能力。增强了CIT的模型在域外身份转换任务上达到90.9%的准确率，而Transformer基线模型仅为9.1%。", "conclusion": "NRR提供了一种解决过早语义坍塌的原则性替代方案，将歧义视为一种明确的表征状态而非故障模式。它将表示与解析分离，使单一模型无需重新训练即可在创造性、事实性和保留歧义的推理之间切换。关键在于何时、如何以及由谁来控制AI解决歧义。"}}
{"id": "2512.12277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12277", "abs": "https://arxiv.org/abs/2512.12277", "authors": ["Thibault Geoffroy", "Myriam Maumy", "Lionel Prevost"], "title": "Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions", "comment": "28 pages, 8 figures, chapter for \"Emotion and Facial Recognition in Artificial Intelligence: Sustainable Multidisciplinary Perspectives and Applications\" (2026)", "summary": "As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.", "AI": {"tldr": "本文提出了一种混合持续学习框架，用于面部表情识别（FER），通过结合深度卷积特征和面部动作单元（AUs），并使用贝叶斯高斯混合模型（BGMMs）进行建模，以减轻灾难性遗忘，提高识别准确性和知识保留能力。", "motivation": "随着人工智能（AI）系统日益融入日常生活，识别和适应人类情感对于有效的人机交互至关重要。面部表情识别是推断情感状态的主要途径，但情感的动态性和文化细微差别要求模型能够持续学习而不忘记先验知识。", "method": "研究提出了一种混合框架，用于持续学习环境下的面部表情识别，以减轻灾难性遗忘。该方法整合了两种互补模态：深度卷积特征和从面部动作编码系统（FACS）中提取的面部动作单元（AUs）。组合表示通过贝叶斯高斯混合模型（BGMMs）进行建模，这提供了一种轻量级、概率性的解决方案，避免了重新训练，并具有强大的判别能力。模型在Compound Facial Expression of Emotion (CFEE) 数据集上进行测试，先学习基本表情，然后逐步识别复合表情。", "result": "实验结果表明，该模型提高了准确性，增强了知识保留能力，并减少了遗忘。模型能够先学习基本表情，然后逐步识别复合表情。", "conclusion": "该框架有助于开发具有情感智能的AI系统，并在教育、医疗保健和自适应用户界面等领域具有应用潜力。"}}
{"id": "2512.12281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12281", "abs": "https://arxiv.org/abs/2512.12281", "authors": ["Jiahao Zhao"], "title": "Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection", "comment": "12 pages, 4 figures, 3 ttables", "summary": "Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data \"first principles\" is more critical for achieving a superior architecture than simply retrieving SOTA components.", "AI": {"tldr": "Cognitive-YOLO是一个LLM驱动的框架，它通过分析数据集的内在特征，直接合成高性能目标检测网络架构，并在多个数据集上取得了优异的性能和参数效率。", "motivation": "传统手动设计目标检测架构耗时费力，神经架构搜索（NAS）计算成本高昂。现有基于大型语言模型（LLM）的方法多为搜索循环中的迭代优化器，而非基于对数据整体理解直接生成架构，存在效率和理解上的不足。", "method": "Cognitive-YOLO包含三个阶段：1) 分析模块从目标数据集中提取关键元特征（如目标尺度分布、场景密度）；2) LLM基于这些特征，并结合通过检索增强生成（RAG）获取的最新组件，合成结构化的神经架构描述语言（NADL）；3) 编译器将NADL实例化为可部署模型。", "result": "在五个不同的目标检测数据集上，Cognitive-YOLO生成的架构始终表现优异，达到极具竞争力的性能，并展现出优于基线模型的性能-参数权衡。消融研究表明，LLM的数据驱动推理是性能提升的主要驱动力，数据“第一性原理”的深刻理解比简单检索SOTA组件对实现卓越架构更为关键。", "conclusion": "Cognitive-YOLO证明了LLM通过对数据集内在特征的深度理解进行数据驱动推理，能够直接高效地合成高性能目标检测架构，并且这种基于“第一性原理”的理解对于架构设计至关重要。"}}
{"id": "2512.13374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13374", "abs": "https://arxiv.org/abs/2512.13374", "authors": ["Francesca Da Ros", "Luca Di Gaspero", "Kevin Roitero"], "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.", "AI": {"tldr": "本研究探讨大型语言模型（LLMs）如何内部表示组合优化问题，发现LLMs能捕获与优化性能相关的有意义结构信息，其隐藏层表示在预测最佳求解器方面的能力与传统特征提取相当。", "motivation": "尽管LLMs在优化自动化方面取得了进展，但人们对它们如何学习问题结构或算法行为知之甚少。本研究旨在理解LLMs内部如何表示组合优化问题，以及这些表示能否支持下游决策任务。", "method": "研究采用双重方法：直接查询（评估LLM显式提取实例特征的能力）和探测分析（检查此类信息是否隐式编码在其隐藏层中）。探测框架进一步扩展到针对每个实例的算法选择任务，评估LLM衍生的表示是否能预测表现最佳的求解器。实验涵盖四个基准问题和三种实例表示。", "result": "结果显示，LLMs通过直接查询或探测，表现出中等程度的从问题实例中恢复特征信息的能力。值得注意的是，LLM隐藏层表示的预测能力与传统特征提取方法所达到的效果相当，这表明LLMs捕获了与优化性能相关的有意义的结构信息。", "conclusion": "LLMs能够捕捉组合优化问题的有意义结构信息，其内部表示在预测最佳求解器方面展现出与传统特征工程相当的效力，为LLMs在优化领域的应用提供了更深层次的理解。"}}
{"id": "2512.13487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13487", "abs": "https://arxiv.org/abs/2512.13487", "authors": ["Ayon Roy", "Risat Rahaman", "Sadat Shibly", "Udoy Saha Joy", "Abdulla Al Kafi", "Farig Yousuf Sadeque"], "title": "Advancing Bangla Machine Translation Through Informal Datasets", "comment": "33 pages, 13 figures", "summary": "Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.", "AI": {"tldr": "本研究旨在通过开发非正式语料库（如社交媒体和对话文本）来改进孟加拉语机器翻译，特别关注非正式语言翻译，以提高孟加拉语用户的数字信息可及性。", "motivation": "孟加拉语是全球第六大语言，但其开源机器翻译进展有限，尤其是在非正式语言方面。现有研究主要关注正式语言，导致数百万孟加拉语使用者无法获取在线信息，这主要是由于缺乏配对的孟加拉语-英语数据和先进的翻译模型。", "method": "本研究将探索当前最先进的模型，并通过开发一个来自社交媒体和会话文本等非正式来源的数据集来提出改进孟加拉语翻译的方法。", "result": "本研究旨在通过专注于非正式语言翻译，推进孟加拉语机器翻译的发展，并提高孟加拉语使用者在数字世界的访问能力。", "conclusion": "通过关注非正式语言翻译并开发相应的语料库，本研究有望显著改善孟加拉语机器翻译的质量，从而为数百万孟加拉语使用者提供更好的在线信息访问体验。"}}
{"id": "2512.13494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13494", "abs": "https://arxiv.org/abs/2512.13494", "authors": ["Yu-Chen Lu", "Sheng-Feng Yu", "Hui-Hsien Weng", "Pei-Shuo Wang", "Yu-Fang Hu", "Liang Hung-Chun", "Hung-Yueh Chiang", "Kai-Chiang Wu"], "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping", "comment": "Accepted by AAAI 2026", "summary": "Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.", "AI": {"tldr": "本文提出SkipCat，一种新颖的低秩压缩框架，通过层内共享低秩投影和块跳过技术，在相同压缩率下保留更多有效秩，从而在边缘设备上部署大型语言模型时，显著提高性能。", "motivation": "大型语言模型（LLM）因其庞大的参数量，难以部署在计算和内存受限的边缘设备上。传统的低秩压缩方法虽然能减少成本，但需要大幅降低秩才能实现显著效率提升，这通常会导致严重的性能下降。为了在压缩率和性能之间取得更好的平衡，需要一种新的方法。", "method": "SkipCat框架包含两项技术：1. 层内共享低秩投影：多个共享相同输入的矩阵使用共同的低秩投影，减少冗余并提高压缩效率。2. 块跳过技术：选择性地跳过低秩分解中某些子块的计算和内存传输。这两种技术共同作用，使得在相同压缩预算下，压缩模型能够保留更多有效秩。", "result": "实验结果表明，在不进行额外微调的情况下，SkipCat在零样本任务上，以相同的压缩率比现有低秩压缩方法提高了7%的准确率。", "conclusion": "SkipCat的秩最大化压缩策略在严格的资源限制下，能够有效保持模型性能，凸显了其在边缘设备上部署大型语言模型方面的潜力。"}}
{"id": "2512.13293", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13293", "abs": "https://arxiv.org/abs/2512.13293", "authors": ["Hao Fua", "Wei Liu", "Shuai Zhoua"], "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration", "comment": null, "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.", "AI": {"tldr": "本文提出了一种新颖的协调探索多机器人强化学习算法，通过引入内在动机探索和双采样模式，解决了多机器人社交编队导航中行人行为不可预测性及机器人间探索效率低下的挑战。", "motivation": "多机器人社交编队导航是实现人机共存的关键能力。尽管强化学习在此领域前景广阔，但行人行为的固有不可预测性和不合作动态，以及机器人之间协调探索效率低下，构成了重大挑战。", "method": "本文提出了一种新型的协调探索多机器人强化学习算法，其核心组件包括：1) 一个自学习的内在奖励机制，旨在集体缓解策略保守性，引入内在动机探索；2) 在集中式训练和分散式执行框架内整合双采样模式，以增强导航策略和内在奖励的表示；3) 采用两时间尺度更新规则来解耦参数更新。", "result": "在社交编队导航基准测试中，所提出的算法在关键指标上表现出优于现有先进方法的性能。", "conclusion": "该算法通过引入内在动机探索和双采样模式，有效解决了多机器人社交编队导航中行人行为不可预测性及机器人间探索效率低下的问题，并取得了卓越的性能。"}}
{"id": "2512.13552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13552", "abs": "https://arxiv.org/abs/2512.13552", "authors": ["Hour Kaing", "Raj Dabre", "Haiyue Song", "Van-Hien Tran", "Hideki Tanaka", "Masao Utiyama"], "title": "PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation", "comment": "Published at COLING 2025, 14 pages", "summary": "This work introduces {\\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.", "AI": {"tldr": "PrahokBART是一个紧凑型高棉语预训练序列到序列模型，通过高质量语料和语言学组件训练，在多项生成任务上优于mBART50。", "motivation": "现有多语言模型忽略了高棉语的语言学问题，研究旨在通过改进预训练语料质量和解决高棉语特有的语言学问题来提升其性能。", "method": "从头开始训练PrahokBART，一个紧凑的序列到序列模型。使用精心策划的高棉语和英语语料库。整合了词汇分割和规范化等语言学组件，以解决高棉语的语言学问题。在机器翻译、文本摘要和标题生成等任务上进行评估。", "result": "PrahokBART在机器翻译、文本摘要和标题生成这三个生成任务上均优于强大的多语言预训练模型mBART50。研究还深入分析了每个语言学模块的影响，并评估了模型在文本生成过程中处理空格的有效性。", "conclusion": "PrahokBART通过专注于语料质量和解决高棉语特有的语言学问题，显著提升了高棉语生成任务的性能，超越了现有强大多语言模型。模型有效处理空格对于高棉语文本的自然度至关重要。"}}
{"id": "2512.13481", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13481", "abs": "https://arxiv.org/abs/2512.13481", "authors": ["Ojas Pungalia", "Rashi Upadhyay", "Abhishek Mishra", "Abhiram H", "Tejasvi Alladi", "Sujan Yenuganti", "Dhruv Kumar"], "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings", "comment": "Under Review", "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.", "AI": {"tldr": "本研究评估了大型语言模型（LLM）是否表现出嫉妒行为。结果显示，某些LLM在特定情境下确实存在嫉妒倾向，例如试图拉低同伴以实现结果均等，而其他模型则更关注自身收益最大化。", "motivation": "随着LLM越来越多地代表人类参与协作和竞争性工作流程，有必要评估它们是否以及在何种条件下表现出类嫉妒偏好，因为嫉妒是影响人类竞争和团队结果的常见行为。", "method": "研究通过两种场景测试了LLM之间是否表现出类嫉妒行为：1) 一个点数分配游戏，测试模型是否试图战胜其同伴；2) 一个模拟职场环境，观察模型在认可不公情况下的行为。", "result": "研究发现某些LLM持续表现出类嫉妒模式，但模型和情境之间存在显著差异。例如，GPT-5-mini和Claude-3.7-Sonnet倾向于拉低同伴模型以均等化结果，而Mistral-Small-3.2-24B则专注于最大化自身收益。", "conclusion": "这些结果强调了在基于LLM的多智能体系统中，需要将竞争性倾向视为一个重要的安全和设计因素。"}}
{"id": "2512.13399", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13399", "abs": "https://arxiv.org/abs/2512.13399", "authors": ["Sitao Cheng", "Tianle Li", "Xuhan Huang", "Xunjian Yin", "Difan Zou"], "title": "Differentiable Evolutionary Reinforcement Learning", "comment": "Work in Progress. We release our code and model at https://github.com/sitaocheng/DERL", "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.", "AI": {"tldr": "本文提出可微分演化强化学习 (DERL)，一个双层框架，通过可微分元优化自动发现最优奖励信号，以解决复杂推理任务中奖励函数设计困难的问题，并在多个领域取得最先进性能。", "motivation": "在强化学习中，设计有效的奖励函数是一项核心且艰巨的挑战，尤其是在复杂推理任务中。现有的自动化奖励优化方法通常依赖于无导数的进化启发式算法，将奖励函数视为黑盒，未能捕捉奖励结构与任务性能之间的因果关系。", "method": "DERL 采用双层框架，其中一个元优化器 (Meta-Optimizer) 通过组合结构化原子原语来演化奖励函数（即元奖励），指导内部循环策略的训练。与以往的进化方法不同，DERL 的元优化是可微分的：它将内部循环的验证性能作为信号，通过强化学习更新元优化器，从而近似任务成功的“元梯度”，逐步学习生成更密集、更具可操作性的反馈。", "result": "DERL 在 ALFWorld 和 ScienceWorld 领域取得了最先进的性能，显著优于依赖启发式奖励的方法，特别是在分布外场景中。对演化轨迹的分析表明，DERL 成功捕捉了任务的内在结构，无需人工干预即可实现自我改进的智能体对齐。", "conclusion": "DERL 提供了一种新颖的可微分演化强化学习框架，能够自主发现最优奖励信号，有效解决了复杂推理任务中的奖励设计难题，并在多个领域展现出优越的性能和无需人工干预的自我改进能力。"}}
{"id": "2512.12296", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12296", "abs": "https://arxiv.org/abs/2512.12296", "authors": ["Hyunju Lee", "Youngmin Oh", "Jeimin Jeon", "Donghyeon Baek", "Bumsub Ham"], "title": "GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search", "comment": "Accepted to WACV 2026", "summary": "Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods", "AI": {"tldr": "本文提出GrowTAS，一个渐进式训练框架，用于Transformer架构搜索（TAS）。它通过先训练小型子网络再逐步引入大型子网络，有效解决了现有超网络方法中权重共享导致的子网络间干扰问题，并显著提升了性能。", "motivation": "现有TAS方法通常训练一个包含所有候选架构的超参数网络（超网络），但所有子网络共享权重导致严重的干扰，尤其损害小型子网络的性能。研究发现，训练良好的小型子网络可以为训练大型子网络提供良好基础。", "method": "提出GrowTAS渐进式训练框架，从训练小型子网络开始，逐步纳入大型子网络，以减少干扰并稳定训练过程。此外，还引入GrowTAS+，通过仅微调部分权重来进一步提升大型子网络的性能。", "result": "在ImageNet和多个迁移学习基准（包括CIFAR-10/100、Flowers、CARS和INAT-19）上进行了大量实验，结果表明所提出的方法优于当前的TAS方法。", "conclusion": "GrowTAS/GrowTAS+通过渐进式训练策略，有效解决了现有TAS超网络训练中的子网络间干扰问题，从而能够发现更高效的视觉Transformer，并提升了整体性能。"}}
{"id": "2512.13304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13304", "abs": "https://arxiv.org/abs/2512.13304", "authors": ["Sait Sovukluk", "Johannes Englsberger", "Christian Ott"], "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories", "comment": "Accepted for publication in Biomimetic Intelligence and Robotics. Supplemental video: https://youtu.be/HlAg2nbNct4", "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.", "AI": {"tldr": "本研究提出了一种用于类人机器人跑步的步态自适应框架，结合弹簧-质量轨迹和无差拍控制增益库，实现了在复杂环境中的敏捷和鲁棒运动。", "motivation": "使类人机器人能够执行敏捷且鲁棒的跑步行为，并适应各种具有挑战性的环境，如随机障碍物、不规则步道和外部干扰。", "method": "该框架包含四个主要部分：1) 自动生成弹簧-质量轨迹库；2) 通过主动控制的模板模型生成无差拍控制增益库；3) 开发步态自适应的轨迹选择策略；4) 通过全身控制（WBC）框架将弹簧-质量轨迹映射到类人机器人模型，同时考虑闭链系统、自碰撞和反应式肢体摆动。", "result": "该框架在MuJoCo模拟器中展示了其包容性和鲁棒性，成功应对了随机垫脚石、跳跃障碍、S形运动、突然改变跑步方向以及拒绝显著干扰和不确定性等挑战。在信号噪声、不精确性、建模误差和延迟等全面不确定性下也表现出鲁棒性。所有行为均使用单一库和相同WBC控制参数实现，无需额外调整。315条不同轨迹的弹簧-质量和无差拍控制增益库在4.5秒内自动计算完成。", "conclusion": "所提出的框架为类人机器人跑步和步态自适应提供了一个鲁棒且敏捷的解决方案，能够有效地应对复杂的环境和不确定性，并且库的生成效率高。"}}
{"id": "2512.12287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12287", "abs": "https://arxiv.org/abs/2512.12287", "authors": ["Ahmad Zafarani", "Zahra Dehghanian", "Mohammadreza Davoodi", "Mohsen Shadroo", "MohammadAmin Fazli", "Hamid R. Rabiee"], "title": "RealDrag: The First Dragging Benchmark with Real Target Image", "comment": null, "summary": "The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \\textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.\n  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.", "AI": {"tldr": "由于缺乏标准化基准和真实目标图像，拖拽式图像编辑模型的评估不可靠。本文引入了RealDrag，首个包含配对真实目标图像的全面基准，并提出了四种新颖的任务特定指标，对17个SOTA模型进行了大规模系统分析，揭示了当前方法的权衡并建立了可靠基线。", "motivation": "拖拽式图像编辑模型的评估因缺乏标准化基准、统一指标、不一致的评估协议以及最关键的缺少包含真实目标图像的数据集而不可靠，这使得竞争方法之间的客观比较变得困难。", "method": "1. 引入了RealDrag，这是首个包含配对真实目标图像的综合性点基图像编辑基准。2. RealDrag数据集包含400多个来自不同视频源的人工标注样本，包括源/目标图像、手柄/目标点、可编辑区域掩码以及图像和编辑动作的描述性标题。3. 提出了四种新颖的任务特定指标：语义距离（SeD）、外部掩码保留分数（OMPS）、内部补丁保留分数（IPPS）和方向相似度（DiS），用于量化像素级匹配保真度、非编辑区域保留和语义对齐。4. 使用此基准对17个SOTA模型进行了首次大规模系统分析。", "result": "通过使用RealDrag基准进行评估，揭示了当前方法之间明显的权衡，并为未来的研究建立了稳健、可复现的基线。", "conclusion": "RealDrag数据集和评估工具包的引入，为拖拽式图像编辑领域提供了第一个包含真实目标图像的标准化基准和一套新的评估指标，解决了当前评估不可靠的问题，并为指导未来研究奠定了坚实基础。"}}
{"id": "2512.13356", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13356", "abs": "https://arxiv.org/abs/2512.13356", "authors": ["Zeyad Gamal", "Youssef Mahran", "Ayman El-Badawy"], "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.", "AI": {"tldr": "本文提出了一种基于强化学习（TD3算法）的框架，用于控制和稳定双旋翼气动系统（TRAS），实现特定角度控制和轨迹跟踪，并通过仿真和真实实验验证了其有效性和对外部扰动的鲁棒性。", "motivation": "TRAS具有复杂的动力学和非线性特性，传统控制算法难以有效控制。近期强化学习在多旋翼控制领域的潜力吸引了研究兴趣，促使本文探索其在TRAS控制中的应用。", "method": "本文采用了强化学习（RL）框架，并具体使用了双延迟深度确定性策略梯度（TD3）算法来训练RL智能体。该算法适用于连续状态和动作空间，且无需系统模型。研究通过模拟仿真（包括风扰动测试）和实验室实际设置实验来验证控制器性能，并与传统PID控制器进行了对比。", "result": "仿真结果表明RL控制方法有效。在外部风扰动下，RL控制器的有效性优于传统PID控制器。最终，实验室实际实验也证实了该控制器在实际应用中的有效性。", "conclusion": "强化学习（特别是TD3算法）是一种有效的方法，能够成功控制和稳定双旋翼气动系统（TRAS），实现角度控制和轨迹跟踪，并在存在外部扰动的情况下表现出良好的鲁棒性，适用于实际应用。"}}
{"id": "2512.13359", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13359", "abs": "https://arxiv.org/abs/2512.13359", "authors": ["Sümer Tunçay", "Alain Andres", "Ignacio Carlucho"], "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles", "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.", "AI": {"tldr": "本文提出了一种基于JAX和MuJoCo-XLA的GPU加速强化学习训练流程，实现了水下自主航行器（AUV）的六自由度（6-DOF）位置控制，训练时间少于两分钟，并在真实水下环境中实现了零样本模拟到现实的鲁棒轨迹跟踪和扰动抑制。", "motivation": "传统AUV控制器在面对未知动力学或环境扰动时性能下降。强化学习虽有潜力，但训练缓慢且模拟到现实的迁移具有挑战性。", "method": "开发了一个基于JAX和MuJoCo-XLA（MJX）的GPU加速强化学习训练流程。通过联合JIT编译大规模并行物理模拟和学习更新，并系统评估了多种强化学习算法。", "result": "训练时间少于两分钟。在真实水下实验中，实现了鲁棒的6-DOF轨迹跟踪和有效的扰动抑制，且策略是从模拟中零样本迁移的。", "conclusion": "首次明确展示了基于强化学习的AUV六自由度位置控制在真实世界中的应用。"}}
{"id": "2512.12302", "categories": ["cs.CV", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12302", "abs": "https://arxiv.org/abs/2512.12302", "authors": ["Huan Zheng", "Yucheng Zhou", "Tianyi Yan", "Jiayi Su", "Hongjun Chen", "Dubing Chen", "Wencheng Han", "Runzhou Tao", "Zhongying Qiu", "Jianfei Yang", "Jianbing Shen"], "title": "From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving", "comment": null, "summary": "Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.", "AI": {"tldr": "本文提出了Intention-Drive，一个旨在评估自动驾驶系统理解并执行高级人类意图能力的综合基准，以推动从指令遵循者向意图实现者的范式转变。", "motivation": "当前的端到端自动驾驶系统仅能执行简单的低级指令，缺乏理解和实现高级人类抽象意图的能力。为了实现真正智能的自动驾驶，需要从指令遵循转向意图实现，但目前缺乏衡量和推动这一复杂任务进展的标准化基准。", "method": "引入了Intention-Drive基准，包含两个核心贡献：1) 一个结合复杂场景和相应自然语言意图的新数据集；2) 一个以“意图成功率”（Intent Success Rate, ISR）为中心的新颖评估协议，该协议评估人类目标的语义实现程度，而非简单的几何精度。", "result": "通过对一系列基线模型在Intention-Drive上的广泛评估，发现存在显著的性能缺陷，表明基线模型难以实现完成这项高级任务所需的全面场景和意图理解。", "conclusion": "Intention-Drive基准揭示了当前自动驾驶模型在理解和实现高级人类意图方面的局限性，并为未来研究指明了方向，以推动自动驾驶系统向更高智能水平发展。"}}
{"id": "2512.13559", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13559", "abs": "https://arxiv.org/abs/2512.13559", "authors": ["Gibson Nkhata", "Uttamasha Anjally Oyshi", "Quan Mai", "Susan Gauch"], "title": "Verifying Rumors via Stance-Aware Structural Modeling", "comment": "8 pages, 2 figures, published in The 24th IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2025), London, UK, 2025", "summary": "Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.", "AI": {"tldr": "本文提出了一种立场感知结构化建模方法，通过整合语义内容、用户立场和对话结构，有效地验证社交媒体谣言的真实性，并显著优于现有方法。", "motivation": "现有模型在谣言验证中难以同时捕捉语义内容、立场信息和对话结构，尤其受限于基于Transformer编码器的序列长度，从而阻碍了对谣言真实性的准确判断。", "method": "我们提出了一种立场感知结构化建模方法，将对话中的每个帖子与其立场信号一起编码，并按立场类别聚合回复嵌入，以实现可扩展且语义丰富的线程表示。为增强结构感知，我们引入立场分布和层次深度作为协变量，以捕捉立场不平衡和回复深度的影响。", "result": "在基准数据集上的广泛实验表明，我们的方法在预测谣言真实性方面显著优于现有方法。此外，模型在早期检测和跨平台泛化方面也表现出多功能性。", "conclusion": "所提出的立场感知结构化建模方法能够有效整合语义、立场和对话结构信息，显著提升了社交媒体谣言真实性预测的准确性，并展现出良好的泛化能力。"}}
{"id": "2512.13505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13505", "abs": "https://arxiv.org/abs/2512.13505", "authors": ["Henry Prakken", "Wijnand van Woerkom"], "title": "Defending the Hierarchical Result Models of Precedential Constraint", "comment": "This is the long version of a paper with the same title presented at the 38th International Conference on Legal Knowledge and Information Systems", "summary": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.", "AI": {"tldr": "本文回应了Bench-Capon对层级案例推理模型，特别是van Woerkom结果模型的批评，认为通过正确解释中间因素为维度并应用维度模型，可以避免这些批评。", "motivation": "近年来提出了层级案例推理（CBR）模型来模拟先例约束。Trevor Bench-Capon批评这些模型在某些情况下会给出错误结果，特别是在不同底层因素以不同强度建立中间因素时，模型未能考虑到这一点。本文旨在回应这些批评。", "method": "本文针对van Woerkom的结果型层级模型，通过论证Bench-Capon在某些例子中似乎将中间因素解释为维度，并提出将van Woerkom的基于维度的层级结果模型应用于这些例子，可以避免Bench-Capon的批评。", "result": "通过将中间因素解释为维度并应用van Woerkom的维度型层级结果模型，可以有效规避Bench-Capon对层级案例推理模型的批评，证明模型在特定解释下能够给出正确结果。", "conclusion": "van Woerkom的层级结果模型在正确解释中间因素（例如作为维度）并应用其维度版本时，能够有效处理Bench-Capon提出的批评，表明模型具有更强的鲁棒性。"}}
{"id": "2512.13510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13510", "abs": "https://arxiv.org/abs/2512.13510", "authors": ["Linjie Mu", "Yannian Gu", "Zhongzhen Huang", "Yakun Zhu", "Shaoting Zhang", "Xiaofan Zhang"], "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph", "comment": null, "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.", "AI": {"tldr": "MedCEG框架通过引入临床证据图（CEG）和临床推理过程奖励机制，显式监督大型语言模型的推理过程，旨在提高医疗AI推理的临床可靠性和有效性，并在实验中表现出优越性能和生成临床有效推理链的能力。", "motivation": "尽管具有推理能力的大型语言模型在临床应用中表现出色，但现有通过强化学习增强的医疗推理过程在训练中常忽略其准确性和有效性，导致临床可靠性有限。研究旨在弥补这一差距，提高医疗AI推理的临床有效性。", "method": "提出MedCEG框架，通过“关键证据图”（CEG）显式监督推理过程，为医疗语言模型注入临床有效推理路径。构建包含挑战性临床病例的数据集，并为每个样本算法性构建CEG，以代表高质量、可验证的推理路径。引入“临床推理程序奖励”机制，评估推理的节点覆盖率、结构正确性和链完整性，全面衡量推理质量。", "result": "实验结果表明，MedCEG在性能上超越了现有方法，并能生成临床有效的推理链，代表了可靠医疗AI推理的坚实进展。", "conclusion": "MedCEG通过其独特的监督机制和奖励评估体系，显著提升了医疗AI推理的临床可靠性和有效性，为医疗领域的AI决策支持提供了更值得信赖的基础。"}}
{"id": "2512.13380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13380", "abs": "https://arxiv.org/abs/2512.13380", "authors": ["Chuan Mao", "Haoqi Yuan", "Ziye Huang", "Chaoyi Xu", "Kai Ma", "Zongqing Lu"], "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning", "comment": "19 pages", "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.", "AI": {"tldr": "本文提出了DemoFunGrasp，一种通用的灵巧功能性抓取方法，通过分解抓取条件、利用单步演示编辑以及结合视觉语言模型，解决了功能性抓取中目标复杂性、多任务探索和虚实迁移的挑战，实现了对未见物体、功能和抓取风格的泛化，并能遵循指令自主执行抓取。", "motivation": "强化学习在灵巧抓取方面取得了显著成功，但精细的功能性抓取仍未被充分探索，面临多重挑战：为不同物体指定功能性抓取目标和奖励函数的复杂性、多任务强化学习探索的难度，以及从模拟到现实世界的迁移挑战。", "method": "本文提出了DemoFunGrasp框架，用于通用的灵巧功能性抓取。该方法将功能性抓取条件分解为抓取风格和功能性（affordance）两个互补部分，并将其整合到强化学习框架中。为解决多任务优化挑战，利用单个抓取演示，将强化学习问题重新表述为“一步演示编辑”，显著提高了样本效率和性能。此外，通过结合视觉语言模型（VLM）进行规划，实现了自主指令遵循的抓取执行。", "result": "仿真和真实世界的实验结果表明，DemoFunGrasp能够泛化到物体、功能和抓取风格的未见组合，在成功率和功能性抓取精度方面均优于基线方法。除了强大的虚实迁移能力外，通过集成视觉语言模型进行规划，该系统还实现了自主指令遵循的抓取执行。", "conclusion": "DemoFunGrasp为通用的灵巧功能性抓取提供了一个有效的解决方案，通过创新的条件分解和优化策略，成功克服了精细功能性抓取面临的关键挑战。其在泛化能力、样本效率、虚实迁移以及指令遵循方面的表现，显著推动了灵巧操作领域的发展。"}}
{"id": "2512.12303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12303", "abs": "https://arxiv.org/abs/2512.12303", "authors": ["Yang Ou", "Xiongwei Zhao", "Xinye Yang", "Yihan Wang", "Yicheng Di", "Rong Yuan", "Xieyuanli Chen", "Xu Zhu"], "title": "OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation", "comment": "Submitted to TMM", "summary": "Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.", "AI": {"tldr": "本文提出了一种名为OMUDA的统一框架，通过在不同表示层引入分层掩码策略，解决了无监督域适应(UDA)语义分割中存在的上下文模糊、特征不一致和伪标签噪声问题，显著提升了跨域性能。", "motivation": "现有的无监督域适应(UDA)方法在语义分割中仍难以弥合域鸿沟，主要原因包括跨域上下文模糊、特征表示不一致以及类别伪标签噪声。", "method": "OMUDA框架包含三种分层掩码策略：1) 上下文感知掩码(CAM)，自适应区分前景背景以平衡全局上下文和局部细节；2) 特征蒸馏掩码(FDM)，通过从预训练模型知识迁移来增强鲁棒一致的特征学习；3) 类别解耦掩码(CDM)，通过明确建模类别不确定性来减轻噪声伪标签的影响。这些策略在上下文、表示和类别层面有效减少域偏移。", "result": "在多个具有挑战性的跨域语义分割基准测试中，OMUDA被证实有效。尤其是在SYNTHIA->Cityscapes和GTA5->Cityscapes任务上，OMUDA可以无缝集成到现有UDA方法中，并持续取得最先进的结果，平均性能提升7%。", "conclusion": "OMUDA通过其分层掩码范式，在上下文、表示和类别层面有效减少了域偏移，提供了一个超越现有方法的统一解决方案，显著提升了无监督域适应语义分割的性能。"}}
{"id": "2512.13514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13514", "abs": "https://arxiv.org/abs/2512.13514", "authors": ["Aman Arora", "Matteo El-Hariry", "Miguel Olivares-Mendez"], "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM", "comment": "Presented at AI4OPA Workshop at the International Conference on Space Robotics (iSpaRo) 2025 at Sendai, Japan", "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.", "AI": {"tldr": "本文提出了一种基于强化学习（PPO）的框架，用于在Isaac Sim高保真模型中实现JAXA Int-Ball2机器人在国际空间站（ISS）日本实验舱（JEM）内的六自由度（6-DoF）精确对接，同时考虑了传感噪声、执行器误差和环境变化。", "motivation": "自主自由飞行器在国际空间站（ISS）内的任务中扮演关键角色，但其在传感噪声、微小执行器不匹配和环境变异下的精确对接仍然是一个重大挑战。", "method": "研究采用强化学习（RL）框架，特别是近端策略优化（PPO）算法，在JEM的高保真Isaac Sim模型中训练和评估Int-Ball2机器人的6-DoF对接控制器。训练中融入了域随机化动力学和有界观测噪声，并明确建模了螺旋桨的阻力扭矩效应和极性结构。", "result": "所学习的策略在各种条件下实现了稳定可靠的对接性能。", "conclusion": "该研究提供了一个受控研究，探讨了Int-Ball2的推进物理学如何影响微重力环境下的RL对接性能，并为Int-Ball2在碰撞感知导航、安全RL、推进精确的模拟到真实迁移以及基于视觉的端到端对接等未来扩展工作奠定了基础。"}}
{"id": "2512.12307", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.12307", "abs": "https://arxiv.org/abs/2512.12307", "authors": ["Benjamin Beilharz", "Thomas S. A. Wallis"], "title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding", "comment": "18 pages, 6 figures. Supplementary material and code will be provided at the end of January", "summary": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.", "AI": {"tldr": "本文提出MRD（可微分渲染的同色异谱），一种利用基于物理的可微分渲染方法，通过寻找生成相同模型激活但物理上不同的3D场景参数（模型同色异谱），来探究视觉模型对3D场景生成属性的隐式理解。", "motivation": "尽管深度学习在视觉任务中表现出色，但其表示和决策过程难以理解和解释。视觉模型通常在2D输入上训练，但常被认为能发展出对底层3D场景的隐式表示。现有的像素级评估方法缺乏物理基础，难以深入探究模型对物理场景属性的敏感性。", "method": "引入MRD方法，该方法使用基于物理的可微分渲染来寻找在物理上不同但产生相同模型激活的3D场景参数（即模型同色异谱）。与以往的像素级方法不同，MRD的重建结果始终基于物理场景描述，允许在保持其他参数不变的情况下，探究模型对特定物理场景属性（如物体形状、材质）的敏感性。", "result": "作为概念验证，MRD评估了多个模型恢复几何（形状）和双向反射分布函数（材质）场景参数的能力。结果显示，目标场景和优化场景在模型激活上高度相似，但视觉结果有所不同。定性分析表明，这些重建有助于研究模型对哪些物理场景属性敏感或不变。", "conclusion": "MRD有望通过分析物理场景参数如何驱动模型响应的变化，从而加深我们对计算机视觉和人类视觉的理解。"}}
{"id": "2512.13564", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13564", "abs": "https://arxiv.org/abs/2512.13564", "authors": ["Yuyang Hu", "Shichun Liu", "Yanwei Yue", "Guibin Zhang", "Boyang Liu", "Fangyi Zhu", "Jiahang Lin", "Honglin Guo", "Shihan Dou", "Zhiheng Xi", "Senjie Jin", "Jiejun Tan", "Yanbin Yin", "Jiongnan Liu", "Zeyu Zhang", "Zhongxiang Sun", "Yutao Zhu", "Hao Sun", "Boci Peng", "Zhenrong Cheng", "Xuanbo Fan", "Jiaxin Guo", "Xinlei Yu", "Zhenhong Zhou", "Zewen Hu", "Jiahao Huo", "Junhao Wang", "Yuwei Niu", "Yu Wang", "Zhenfei Yin", "Xiaobin Hu", "Yue Liao", "Qiankun Li", "Kun Wang", "Wangchunshu Zhou", "Yixin Liu", "Dawei Cheng", "Qi Zhang", "Tao Gui", "Shirui Pan", "Yan Zhang", "Philip Torr", "Zhicheng Dou", "Ji-Rong Wen", "Xuanjing Huang", "Yu-Gang Jiang", "Shuicheng Yan"], "title": "Memory in the Age of AI Agents", "comment": null, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "AI": {"tldr": "这篇综述旨在提供当前智能体记忆研究的最新全景，通过统一的视角（形式、功能、动态）对其进行分类和分析，并展望了未来的研究方向。", "motivation": "智能体记忆研究领域日益碎片化，概念模糊，现有分类法（如长/短期记忆）不足以涵盖其多样性。因此，需要一个更新、更清晰的视角来整合和理解这一快速发展的领域。", "method": "作者首先明确了智能体记忆的范围，并将其与相关概念区分开来。然后，从形式（令牌级、参数化、潜在记忆）、功能（事实性、经验性、工作记忆）和动态（记忆的形成、演变、检索）三个统一的视角对智能体记忆进行了审查。此外，还总结了记忆基准和开源框架，并提出了新兴的研究前沿。", "result": "本研究提供了一个全面的智能体记忆分类体系：在形式上识别出令牌级、参数化和潜在记忆；在功能上提出了更细致的事实性、经验性和工作记忆分类；在动态上分析了记忆的形成、演变和检索过程。同时，汇编了记忆基准和开源框架，并展望了记忆自动化、强化学习集成、多模态记忆、多智能体记忆和可信度等新兴研究前沿。", "conclusion": "本综述不仅为现有工作提供了参考，也为将记忆视为未来智能体智能设计中的一级原语奠定了概念基础，并指明了新兴的研究方向。"}}
{"id": "2512.13586", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13586", "abs": "https://arxiv.org/abs/2512.13586", "authors": ["Jia-Nan Li", "Jian Guan", "Wei Wu", "Chongxuan Li"], "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "comment": null, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.", "AI": {"tldr": "ReFusion是一种新型的掩码扩散模型，通过将并行解码从token级别提升到slot级别，解决了自回归模型推理慢和现有掩码扩散模型计算开销大、生成不连贯的问题，显著提高了性能和效率。", "motivation": "自回归模型（ARMs）因其顺序推理而速度缓慢。掩码扩散模型（MDMs）虽然提供了并行替代方案，但存在关键缺点：无法进行Key-Value（KV）缓存导致计算开销大，以及从难以处理的token组合空间中学习依赖关系导致生成不连贯。", "method": "ReFusion引入了一种迭代的“规划与填充”（plan-and-infill）解码过程。首先，扩散式规划步骤识别一组弱相关的槽（slot，即固定长度的连续子序列）；然后，自回归填充步骤并行解码这些选定的槽。这种基于槽的设计同时实现了完整的KV缓存重用与统一的因果框架，并将学习复杂性从token组合空间降低到可管理的槽级排列空间。", "result": "在七个不同的基准测试中，ReFusion不仅以平均34%的性能提升和超过18倍的速度超越了先前的MDMs，而且在性能上缩小了与强大ARMs的差距，同时保持了平均2.33倍的速度优势。", "conclusion": "ReFusion通过引入槽级并行解码，成功解决了自回归模型和现有掩码扩散模型的局限性，实现了卓越的性能和效率，并弥合了与强大自回归模型之间的性能差距。"}}
{"id": "2512.13598", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13598", "abs": "https://arxiv.org/abs/2512.13598", "authors": ["Daniel Melcer", "Qi Chen", "Wen-Hao Chiang", "Shweta Garg", "Pranav Garg", "Christian Bock"], "title": "Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization", "comment": null, "summary": "A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.", "AI": {"tldr": "文本梯度法能提升大型语言模型性能，但实验表明其行为无法用梯度类比准确解释。", "motivation": "旨在无需人工干预即可提升大型语言模型（LLM）性能，并探究一种主流的自动提示优化技术（文本梯度法）的行为。", "method": "通过一系列实验和案例研究来调查文本梯度方法的行为。", "result": "虽然文本梯度方法通常能带来性能提升，但实验表明梯度类比并不能准确解释其行为。", "conclusion": "这些发现可能有助于选择提示优化策略，并指导新方法的开发。"}}
{"id": "2512.12339", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12339", "abs": "https://arxiv.org/abs/2512.12339", "authors": ["Maurya Goyal", "Anuj Singh", "Hadi Jamali-Rad"], "title": "Unified Control for Inference-Time Guidance of Denoising Diffusion Models", "comment": null, "summary": "Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe", "AI": {"tldr": "本文提出UniCoDe算法，统一了扩散模型中基于采样和基于梯度的引导方法，以更高效地将模型输出与下游目标对齐，并在多种任务上表现出色。", "motivation": "为了提高扩散模型在特定任务上的性能，需要将模型输出与下游目标对齐。现有的对齐方法主要分为基于采样和基于梯度的两种策略，但它们各自存在局限性，例如采样效率低下。本文旨在结合两者的优势，解决这些问题。", "method": "本文提出了UniCoDe，一个通用算法，将局部梯度信号整合到采样过程中。它结合了基于采样方法（探索多个候选输出并选择高奖励信号的）和基于梯度方法（使用可微分奖励近似直接引导生成过程）的优点，形成一个统一的框架。", "result": "UniCoDe实现了更高效的采样，并在奖励对齐和与扩散无条件先验的偏离之间提供了更好的权衡。实验结果表明，UniCoDe在各种任务中与最先进的基线方法保持竞争力。", "conclusion": "UniCoDe成功地将基于采样和基于梯度的引导方法整合到一个统一框架中，有效解决了采样效率问题，并在对齐扩散模型输出与下游目标方面提供了更好的性能和效率权衡。"}}
{"id": "2512.13607", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13607", "abs": "https://arxiv.org/abs/2512.13607", "authors": ["Boxin Wang", "Chankyu Lee", "Nayeon Lee", "Sheng-Chieh Lin", "Wenliang Dai", "Yang Chen", "Yangyi Chen", "Zhuolin Yang", "Zihan Liu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models", "comment": "We publicly release the Nemotron-Cascade models and the full collection of training data at: https://huggingface.co/collections/nvidia/nemotron-cascade", "summary": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.", "AI": {"tldr": "本文提出级联域级强化学习（Cascade RL）方法，以解决通用推理模型在强化学习中遇到的跨领域异构性问题，显著提升了模型性能，并在多个基准测试中达到SOTA水平。", "motivation": "使用强化学习构建通用推理模型面临显著的跨领域异构性，例如推理响应长度和验证延迟的巨大差异。这种变异性使强化学习基础设施复杂化、训练速度减慢，并使训练课程（如响应长度扩展）和超参数选择变得困难。", "method": "本文提出级联域级强化学习（Cascade RL）方法，构建通用推理模型Nemotron-Cascade。与传统混合不同领域异构提示的方法不同，Cascade RL通过编排顺序的、域级强化学习来降低工程复杂性。此外，将RLHF作为预处理步骤进行对齐，以增强模型的推理能力。", "result": "Cascade RL降低了工程复杂性，并在广泛的基准测试中实现了最先进的性能。作为预处理步骤的RLHF显著提升了模型的推理能力，远超单纯的偏好优化。后续的域级RLVR阶段很少降低早期领域达到的基准性能，甚至可能有所改善。经过强化学习训练的14B模型在LiveCodeBench v5/v6/Pro上超越了其SFT教师模型DeepSeek-R1-0528，并在2025年国际信息学奥林匹克竞赛（IOI）中获得银牌表现。", "conclusion": "Cascade RL是一种有效开发通用推理模型的方法，它通过解决跨领域异构性问题，简化了工程复杂性，并显著提升了模型性能。RLHF作为预处理步骤对提升推理能力至关重要，且后续的域级训练不会对早期性能造成负面影响。"}}
{"id": "2512.12357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12357", "abs": "https://arxiv.org/abs/2512.12357", "authors": ["Zishen Song", "Yongjian Zhu", "Dong Wang", "Hongzhan Liu", "Lingyu Jiang", "Yongxing Duan", "Zehua Zhang", "Sihan Li", "Jiarui Li"], "title": "TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection", "comment": null, "summary": "Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.", "AI": {"tldr": "本文发布了Daylily-Leaf数据集，并提出了TCLeaf-Net，一个结合了Transformer和卷积的混合检测器，旨在解决真实田间条件下叶片病害检测中的复杂背景、信息丢失和病斑尺度变化等挑战，实现了更高的精度和效率。", "motivation": "及时准确地检测叶片病害对作物生长和减少产量损失至关重要。然而，在真实的田间条件下，杂乱的背景、领域漂移和有限的病斑级数据集阻碍了鲁棒模型的建立。", "method": "本文发布了Daylily-Leaf数据集，包含1,746张RGB图像和7,839个病斑（理想和真实田间条件）。提出TCLeaf-Net，一个Transformer-卷积混合检测器，优化用于真实田间环境。TCLeaf-Net包含三个主要组件：1) Transformer-卷积模块（TCM）结合全局上下文和局部性保留卷积，以抑制非叶区域的干扰；2) 原始尺度特征召回和采样（RSFRS）模块结合双线性重采样和卷积，以保留精细空间细节，减少下采样时的信息损失；3) 带有FPN（DFPN）的可变形对齐模块利用基于偏移的对齐和多感受野感知，以加强多尺度融合，处理病斑尺度变化和特征漂移。", "result": "在Daylily-Leaf数据集的田间分割上，TCLeaf-Net相比基线模型将mAP@50提高了5.4个百分点，达到78.2%，同时计算量减少了7.5 GFLOPs，GPU内存使用减少了8.7%。该模型在精度和召回率上均优于最新的YOLO和RT-DETR系列模型，并在PlantDoc、Tomato-Leaf和Rice-Leaf数据集上表现出强大的性能。", "conclusion": "TCLeaf-Net在Daylily-Leaf数据集上表现出色，并在其他植物病害检测场景中展现出强大的鲁棒性和泛化能力，有效解决了真实田间条件下叶片病害检测的挑战。"}}
{"id": "2512.13618", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13618", "abs": "https://arxiv.org/abs/2512.13618", "authors": ["Zefang Liu", "Nam Nguyen", "Yinzhu Quan", "Austin Zhang"], "title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models", "comment": null, "summary": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.", "AI": {"tldr": "本文首次实证研究了事件序列的时间分词策略，发现没有一种策略是普遍最优的，预测性能强烈依赖于分词器与数据统计特性的匹配。", "motivation": "在大型语言模型（LLMs）中表示连续时间是建模时间事件序列的一个关键但未充分探索的挑战。现有的字节级或日历令牌等策略效果不明确，尤其是在真实世界事件数据具有从平滑对数正态到离散尖峰等多样化统计分布的情况下。", "method": "本研究比较了五种不同的时间编码策略：朴素数字字符串、高精度字节级表示、人类语义日历令牌、经典均匀分箱以及自适应残差标量量化。通过在代表不同分布的真实世界数据集上微调LLMs来评估这些策略。", "result": "分析表明，没有一种策略是普遍优越的；相反，预测性能严重依赖于分词器与数据统计属性的对齐。其中，基于对数的策略在偏斜分布上表现出色，而以人为中心的格式在混合模态下表现稳健。", "conclusion": "研究得出结论，选择合适的时间分词策略对于事件序列的LLM建模至关重要，且必须根据数据本身的统计特性进行适配，而非追求单一的通用解决方案。"}}
{"id": "2508.15250", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15250", "abs": "https://arxiv.org/abs/2508.15250", "authors": ["Yilin Jiang", "Mingzi Zhang", "Sheng Jin", "Zengyi Yu", "Xiangjie Kong", "Binghao Tu"], "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling", "comment": "29pages, 15 figures, Accepted by EMNLP Main Confrence", "summary": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.", "AI": {"tldr": "本文提出了EMNLP框架，用于评估模拟教师角色的LLM在个性、道德发展阶段和伦理风险方面的心理和伦理对齐，发现LLM在抽象推理上表现出色，但在情感复杂性上存在不足，且能力越强的模型越容易受到有害提示注入攻击。", "motivation": "尽管大型语言模型（LLMs）能够模拟专业角色（如教师），但对其在这些情境下的心理和伦理评估仍显不足。", "method": "本文引入了EMNLP（教育者角色道德与规范LLMs画像）框架，用于进行个性画像、道德发展阶段测量以及在软提示注入下的伦理风险评估。该框架扩展了现有量表，并构建了88个教师特有的道德困境，以实现与人类教师的职业导向比较。同时，设计了专门的软提示注入集来评估教师角色LLM的依从性和脆弱性。", "result": "实验结果显示，模拟教师角色的LLM比人类教师表现出更理想化和两极分化的个性，擅长抽象道德推理，但在处理情感复杂情境时表现不佳。推理能力越强的模型，越容易受到有害提示注入攻击，揭示了能力与安全性之间的悖论。模型温度和其他超参数的影响有限，仅在某些风险行为中有所体现。", "conclusion": "本文首次提出了一个基准，用于评估教育AI中教师角色LLM的伦理和心理对齐，并揭示了LLM在专业模拟中的潜在挑战和风险。"}}
{"id": "2512.12309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12309", "abs": "https://arxiv.org/abs/2512.12309", "authors": ["Shenghao Fu", "Yukun Su", "Fengyun Rao", "Jing Lyu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "comment": null, "summary": "Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \\ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.", "AI": {"tldr": "WeDetect是一个基于检索的开放词汇目标检测模型家族，通过双塔非融合架构实现了最先进的性能和高效率，并统一了检测、候选框生成、目标检索和指代表达式理解等多种任务。", "motivation": "开放词汇目标检测旨在检测任意类别，其中非融合方法通过将识别视为检索问题，在共享嵌入空间中匹配区域和文本查询，从而提供更快的推理速度。本研究旨在充分探索这种检索理念在效率和多功能性方面的独特优势。", "method": "该研究提出了WeDetect模型家族：1) WeDetect：一个实时双塔非融合架构，通过精心策划的数据和充分训练实现开放词汇检测。2) WeDetect-Uni：基于WeDetect的通用候选框生成器，通过冻结检测器并微调一个目标性提示来检索通用对象候选框，并支持历史数据中的目标检索。3) WeDetect-Ref：一个基于大型多模态模型（LMM）的目标分类器，用于处理复杂的指代表达式理解，从WeDetect-Uni提取的候选框列表中检索目标对象，并以单次前向传递进行分类。", "result": "WeDetect在开放词汇检测方面超越了其他融合模型，达到了最先进的性能。WeDetect-Uni实现了历史数据的快速回溯和目标检索。WeDetect-Ref能有效处理复杂的指代表达式理解。WeDetect家族成功地在统一的检索框架下整合了检测、候选框生成、目标检索和指代表达式理解，在15个基准测试中均取得了最先进的性能，并具有高推理效率。", "conclusion": "WeDetect家族提供了一个连贯的检索框架，统一了开放词汇检测及其相关任务（如候选框生成、目标检索和指代表达式理解），在保持高推理效率的同时，在多个基准测试中取得了最先进的性能，展示了检索范式在这些任务中的强大潜力和多功能性。"}}
{"id": "2512.13561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13561", "abs": "https://arxiv.org/abs/2512.13561", "authors": ["Li-Wei Shih", "Ruo-Syuan Mei", "Jesse Heidrich", "Hui-Ping Wang", "Joel Hooton", "Joshua Solomon", "Jorge Arinez", "Guangze Li", "Chenhui Shao"], "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments", "comment": "Submitted to the 54th SME North American Manufacturing Research Conference (NAMRC 54)", "summary": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.", "AI": {"tldr": "本文提出了一种三层近场感知框架，通过结合激光条纹中断检测、激光条纹位移测量和嵌入式AI视觉对象检测，解决了自主移动机器人（AMR）在制造环境中传统传感器无法检测到近距离小物体的问题，实现了实时、高性价比的安全操作。", "motivation": "传统测距传感器（如LiDAR和超声波设备）虽然能提供广阔的态势感知，但往往无法检测到机器人底座附近的小物体，这对于自主移动机器人在制造环境中的安全运行至关重要。", "method": "该研究提出了一个三层近场感知框架：\n1. 光线不连续性检测：投射激光条纹，通过识别条纹中断进行快速、二值的障碍物存在检测。\n2. 光线位移测量：通过分析相机图像中投射条纹的几何位移来估计物体高度，提供定量的障碍物高度信息，计算开销极小。\n3. 基于计算机视觉的对象检测：在嵌入式AI硬件上使用对象检测模型对物体进行分类，实现语义感知和情境感知安全决策。\n所有方法均在Raspberry Pi 5系统上实现。", "result": "所有方法均在Raspberry Pi 5系统上实现了实时性能，帧率达到25或50帧每秒。实验评估和比较分析表明，所提出的分层方法在精度、计算和成本之间取得了平衡。", "conclusion": "所提出的三层近场感知框架通过平衡精度、计算和成本，为实现自主移动机器人在制造环境中的安全操作提供了一个可扩展的感知解决方案。"}}
{"id": "2512.13644", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13644", "abs": "https://arxiv.org/abs/2512.13644", "authors": ["Raktim Gautam Goswami", "Amir Bar", "David Fan", "Tsung-Yen Yang", "Gaoyue Zhou", "Prashanth Krishnamurthy", "Michael Rabbat", "Farshad Khorrami", "Yann LeCun"], "title": "World Models Can Leverage Human Videos for Dexterous Manipulation", "comment": null, "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.", "AI": {"tldr": "DexWM是一个灵巧操作世界模型，通过在大量视频数据上训练并引入手部一致性损失，实现了对未来状态更准确的预测和对未见操作技能的零样本泛化，显著优于现有模型。", "motivation": "灵巧操作因其对物体接触中手部细微动作的理解要求而极具挑战性。灵巧操作数据集稀缺。仅预测视觉特征不足以实现精细的灵巧操作。", "method": "引入了DexWM（灵巧操作世界模型），用于预测基于过去状态和灵巧动作的下一个潜在环境状态。为克服数据稀缺，DexWM在超过900小时的人类和非灵巧机器人视频上进行训练。为实现精细灵巧性，引入了辅助手部一致性损失，以强制准确的手部配置。", "result": "DexWM在预测未来状态方面优于先前基于文本、导航和全身动作的世界模型。在配备Allegro夹具的Franka Panda机械臂上，DexWM展示了对未见操作技能的强大零样本泛化能力，在抓取、放置和触及任务中平均性能比Diffusion Policy高出50%以上。", "conclusion": "DexWM是一个有效的灵巧操作世界模型，通过大规模训练数据和手部一致性损失，解决了灵巧操作的挑战，并展现出卓越的预测准确性和泛化能力。"}}
{"id": "2512.13670", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13670", "abs": "https://arxiv.org/abs/2512.13670", "authors": ["Licheng Luo", "Yu Xia", "Kaier Liang", "Mingyu Cai"], "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks", "comment": null, "summary": "Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial", "AI": {"tldr": "本文提出Spatio-Temporal Logic (SpaTiaL) 来表达机器人操作中的几何空间需求，并引入NL2SpaTiaL数据集及翻译验证框架，以实现更可解释、可验证和组合性的指令遵循。", "motivation": "现有研究主要依赖标准时间逻辑 (TL)，忽略了机器人操作任务中至关重要的对象级空间关系和姿态约束。现有数据集也未能充分表示操作任务所需的层次化空间关系，导致指令遵循的局限性。", "method": "研究引入了Spatio-Temporal Logic (SpaTiaL) 来形式化几何空间需求。开发了一个数据集生成框架，该框架能合成SpaTiaL规范并通过确定性、语义保留的反向翻译程序将其转换为自然语言描述，从而构建了NL2SpaTiaL数据集。在此基础上，提出了一个翻译验证框架，该框架配备了基于语言的语义检查器，以确保生成的SpaTiaL公式忠实地编码输入描述的语义。", "result": "通过一系列机器人操作任务的实验表明，基于SpaTiaL的表示为指令遵循提供了更可解释、可验证和组合性的基础。", "conclusion": "SpaTiaL提供了一种更适合机器人操作任务的表达方式，能够有效解决现有方法在处理复杂空间关系方面的不足。NL2SpaTiaL数据集和翻译验证框架显著提高了指令遵循的质量和可靠性。"}}
{"id": "2512.13660", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13660", "abs": "https://arxiv.org/abs/2512.13660", "authors": ["Enshen Zhou", "Cheng Chi", "Yibo Li", "Jingkun An", "Jiayuan Zhang", "Shanyu Rong", "Yi Han", "Yuheng Ji", "Mengzhen Liu", "Pengwei Wang", "Zhongyuan Wang", "Lu Sheng", "Shanghang Zhang"], "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics", "comment": "Project page: https://zhoues.github.io/RoboTracer", "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.", "AI": {"tldr": "本文提出了RoboTracer，一个3D感知的视觉语言模型（VLM），通过结合监督微调（SFT）和强化微调（RFT）来解决机器人空间追踪中的多步骤度量推理和复杂空间指代与测量挑战。同时，作者还引入了大型数据集TraceSpatial和评估基准TraceSpatial-Bench。", "motivation": "机器人空间追踪作为一项基础的具身交互能力，要求多步骤的度量推理以及复杂的空间指代和真实世界度量测量，现有方法难以应对这项组合任务。", "method": "RoboTracer是一个3D感知的VLM，通过通用空间编码器和回归监督解码器实现3D空间指代和测量，并通过监督微调（SFT）增强尺度感知。此外，它通过带有度量敏感过程奖励的强化微调（RFT）来推进多步骤度量推理。为支持训练，作者引入了TraceSpatial数据集（30M QA对）和TraceSpatial-Bench基准。", "result": "实验结果表明，RoboTracer在空间理解、测量和指代方面超越了基线，平均成功率为79.1%。在TraceSpatial-Bench上，它大幅超越了现有技术（包括Gemini-2.5-Pro 36%的准确率），并能与各种控制策略集成，在真实世界杂乱场景中执行长周期、动态任务。", "conclusion": "RoboTracer通过创新的3D感知VLM架构、结合SFT和RFT的训练范式以及大规模数据集和基准，显著提升了机器人在复杂空间追踪任务中的性能，并在多样化机器人和真实场景中展现出强大的通用性和实用性。"}}
{"id": "2512.12360", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12360", "abs": "https://arxiv.org/abs/2512.12360", "authors": ["Yufei Yin", "Qianke Meng", "Minghao Chen", "Jiajun Ding", "Zhenwei Shao", "Zhou Yu"], "title": "VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding", "comment": null, "summary": "Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.", "AI": {"tldr": "VideoARM提出了一种基于层级记忆的智能体推理范式，用于长视频理解，通过自适应、即时推理和记忆构建，显著减少了token消耗并提升了性能。", "motivation": "长视频理解面临挑战，包括时间结构长、多模态线索密集，且现有方法依赖手工推理流程或耗费token的视频预处理来指导多模态大语言模型（MLLMs）进行自主推理。", "method": "VideoARM采用“观察-思考-行动-记忆”的自适应连续循环。一个控制器自主调用工具以粗到细的方式解释视频，从而大幅减少token消耗。同时，一个分层多模态记忆在智能体运行期间持续捕获和更新多级线索，为控制器决策提供精确上下文信息。", "result": "VideoARM在主流基准测试上超越了现有最先进方法DVD，并且显著降低了长视频的token消耗。", "conclusion": "VideoARM通过其自适应、即时智能体推理和分层多模态记忆范式，有效克服了长视频理解的现有局限性，实现了性能提升和token消耗的显著减少。"}}
{"id": "2512.12410", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12410", "abs": "https://arxiv.org/abs/2512.12410", "authors": ["Khalfalla Awedat", "Mohamed Abidalrekab", "Mohammad El-Yabroudi"], "title": "A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams", "comment": null, "summary": "Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.", "AI": {"tldr": "本文提出了一种基于图注意力网络（GAT）的框架，仅利用当前激光雷达帧的原始点云几何信息，重建自动驾驶中因硬件老化、障碍物或强反射导致的垂直光束丢失，有效恢复3D感知能力。", "motivation": "激光雷达传感器中的垂直光束丢失（由硬件老化、灰尘、雪、雾或强反射引起）会导致点云中缺失整个垂直切片，严重降低自动驾驶车辆的3D感知能力，因此需要一种方法来重建这些缺失的数据。", "method": "该方法将每次激光雷达扫描表示为一个非结构化空间图：点为节点，边缘连接附近点并保留原始光束索引顺序。一个多层GAT学习局部几何邻域上的自适应注意力权重，并直接回归缺失位置的海拔（z）值。该框架仅使用当前激光雷达帧，无需相机图像或时间信息。", "result": "在1,065个带有模拟通道丢失的原始KITTI序列上进行训练和评估，该方法实现了平均高度RMSE为11.67厘米，87.98%的重建点落在10厘米的误差阈值内。单GPU每帧推理时间为14.65秒，重建质量对于不同的邻域大小k保持稳定。", "conclusion": "研究结果表明，一个纯粹基于原始点云几何的图注意力模型，能够有效地恢复在真实传感器退化条件下丢失的垂直光束。"}}
{"id": "2512.12372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12372", "abs": "https://arxiv.org/abs/2512.12372", "authors": ["Peixuan Zhang", "Zijian Jia", "Kaiqi Liu", "Shuchen Weng", "Si Li", "Boxin Shi"], "title": "STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative", "comment": null, "summary": "While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.", "AI": {"tldr": "本文提出STAGE，一种基于故事板的多镜头视频生成工作流，通过预测结构化故事板和引入多项机制（如多镜头记忆包、双编码策略、两阶段训练）来解决跨镜头一致性和电影语言问题，并贡献了大规模数据集ConStoryBoard。", "motivation": "尽管生成模型在视频合成的视觉保真度方面取得了进展，但创建连贯的多镜头叙事仍然是一个重大挑战。现有的基于关键帧的方法虽然高效且控制力强，但往往无法保持跨镜头一致性并捕捉电影语言。", "method": "本文提出STAGE（Storyboard-Anchored GEneration）工作流，将基于关键帧的多镜头视频生成任务重新定义。核心方法包括：1) STEP2预测由每个镜头的起始-结束帧对组成的结构化故事板；2) 引入多镜头记忆包（multi-shot memory pack）以确保长程实体一致性；3) 采用双编码策略（dual-encoding strategy）以实现镜头内连贯性；4) 设计两阶段训练方案（two-stage training scheme）以学习电影化的镜头间过渡。此外，还贡献了大规模的ConStoryBoard数据集，包含高质量电影片段及其故事进展、电影属性和人类偏好注释。", "result": "广泛的实验表明，STAGE在结构化叙事控制和跨镜头连贯性方面取得了卓越的性能。", "conclusion": "STAGE通过提出一种基于故事板的新颖工作流，并结合多项创新机制和大规模数据集，有效解决了多镜头视频生成中的叙事连贯性和跨镜头一致性问题，显著提升了生成效果。"}}
{"id": "2512.13654", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13654", "abs": "https://arxiv.org/abs/2512.13654", "authors": ["John E. Ortega", "Dhruv D. Joshi", "Matt P. Borkowski"], "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases", "comment": "7 pages, 1 figure, Appendix of Prompts", "summary": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.", "AI": {"tldr": "本研究深入探讨了大型语言模型（LLMs）在美国最高法院（SCOTUS）判决分类任务中的表现，发现带有记忆的基于提示的模型（如DeepSeek）比基于BERT的模型更鲁棒，性能提升约2个百分点。", "motivation": "理解LLMs在问答之外的分类任务中的响应方式及其“幻觉”现象，特别是其记忆策略。选择SCOTUS判决作为研究对象，因其文本长度、法律术语、非标准结构和领域特定词汇等挑战性特征，是研究LLM记忆准确性的理想任务。", "method": "对基于美国最高法院判决的分类任务进行深入研究，涉及15个和279个标签主题。实验采用了最新的LLM微调和检索方法，包括参数高效微调（parameter-efficient fine-tuning）和自动建模（auto-modeling）等。将带有记忆的基于提示的模型（如DeepSeek）与之前的基于BERT的模型进行比较。", "result": "带有记忆的基于提示的模型（如DeepSeek）在两项SCOTUS分类任务上均显示出比之前基于BERT的模型更强的鲁棒性，性能提升了约2个百分点。", "conclusion": "对于像SCOTUS判决这样具有复杂性和领域特异性的分类任务，带有记忆的基于提示的LLMs（例如DeepSeek）比传统的非提示型BERT模型表现更优，证明了其在处理此类挑战性文本方面的有效性。"}}
{"id": "2512.12560", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12560", "abs": "https://arxiv.org/abs/2512.12560", "authors": ["Xinqi Jin", "Hanxun Yu", "Bohan Yu", "Kebin Liu", "Jian Liu", "Keda Tao", "Yixuan Pei", "Huan Wang", "Fan Dang", "Jiangchuan Liu", "Weiqiang Wang"], "title": "StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding", "comment": null, "summary": "Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.", "AI": {"tldr": "本文提出了一种针对在线视频理解中多模态大语言模型（MLLMs）的令牌剪枝方法，通过引入新的冗余度量（MSSAVT）和掩码剪枝策略，有效减少上下文长度，显著提高准确性并保持极低的延迟。", "motivation": "将多模态大语言模型（MLLMs）应用于在线视频理解领域面临挑战，主要原因是视频帧数过多导致GPU内存占用高和计算延迟大。", "method": "本文提出令牌剪枝方法以缩短上下文长度并保留关键信息。具体包括：1) 引入新颖的冗余度量MSSAVT（Maximum Similarity to Spatially Adjacent Video Tokens），该度量同时考虑令牌相似性和空间位置。2) 设计掩码剪枝策略，确保只剪枝相互不相邻的令牌，以缓解剪枝与冗余之间的双向依赖。3) 整合现有基于时间冗余的剪枝方法，消除视频模态的时间冗余。", "result": "在多个在线和离线视频理解基准测试中，所提出的方法显著提高了准确性（最高达4%），同时剪枝延迟可忽略不计（小于1毫秒）。", "conclusion": "通过令牌剪枝、新颖的MSSAVT冗余度量和掩码剪枝策略，该方法成功解决了多模态大语言模型在在线视频理解中面临的挑战，有效减少了上下文长度，显著提升了性能，并保持了高效性。"}}
{"id": "2512.12622", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12622", "abs": "https://arxiv.org/abs/2512.12622", "authors": ["Zihan Wang", "Seungjun Lee", "Guangzhao Dai", "Gim Hee Lee"], "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation", "comment": null, "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.", "AI": {"tldr": "本文提出了D3D-VLP模型，通过动态3D思维链（3D CoT）和协同学习策略（SLFS），弥合了具身智能体中端到端模型与模块化系统之间的差距，并在多项导航基准测试中取得了最先进的成果。", "motivation": "具身智能体面临挑战：端到端模型缺乏可解释性和显式3D推理能力，而模块化系统则忽视了组件间的相互依赖和协同作用。研究旨在弥合这一差距。", "method": "本文提出了动态3D视觉-语言-规划模型（D3D-VLP），包含两大创新：1) 动态3D思维链（3D CoT），将规划、定位、导航和问答统一到单个3D-VLM和CoT管道中；2) 协同碎片化监督学习（SLFS）策略，利用掩码自回归损失从大规模、部分标注的混合数据中学习，使CoT组件相互加强。为此，构建了一个包含5K真实扫描和20K合成场景的10M混合样本数据集，兼容在线学习方法。", "result": "D3D-VLP模型在多个基准测试中取得了最先进的成果，包括视觉-语言导航（R2R-CE, REVERIE-CE, NavRAG-CE）、目标导向导航（HM3D-OVON）以及面向任务的序列定位和导航（SG3D）。真实世界的移动操作实验进一步验证了其有效性。", "conclusion": "D3D-VLP模型成功地融合了端到端模型和模块化系统的优点，通过创新的3D CoT和SLFS策略，显著提升了具身智能体在复杂导航和推理任务中的表现和可解释性。"}}
{"id": "2512.13655", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.13655", "abs": "https://arxiv.org/abs/2512.13655", "authors": ["Richard J. Young"], "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation", "comment": "25 pages, 6 figures, 8 tables", "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.", "AI": {"tldr": "本研究评估了四种LLM安全对齐机制消除工具的有效性，发现单次通过方法在能力保留方面表现更优，而数学推理能力对消除干预最为敏感。", "motivation": "大型语言模型（LLMs）中的安全对齐机制虽然能阻止有害查询，但也阻碍了认知建模、对抗性测试和安全分析等合法研究应用。虽然存在通过方向正交化移除拒绝表示的消除技术，但现有实现的相对有效性尚未明确。", "method": "本研究评估了四种消除工具（Heretic, DECCP, ErisForge, FailSpy），在十六个指令微调模型（7B-14B参数）上进行测试。报告了工具在所有16个模型上的兼容性，并根据工具支持情况，在部分模型上报告了定量指标（如GSM8K变化）和分布偏移（KL散度）。", "result": "单次通过方法在基准测试子集上展示了卓越的能力保留（三个模型上的GSM8K平均变化：ErisForge -0.28 pp；DECCP -0.13 pp）。贝叶斯优化消除产生了可变的分布偏移（KL散度：0.043-1.646），且对模型能力的影响取决于具体模型。主要发现是数学推理能力对消除干预表现出最高的敏感性，GSM8K变化范围从+1.51 pp到-18.81 pp（相对下降26.5%），具体取决于工具选择和模型架构。", "conclusion": "这些发现为研究人员在部署消除工具时提供了基于证据的选择标准。主要结论指出数学推理能力对消除干预表现出最高的敏感性。"}}
{"id": "2512.12884", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12884", "abs": "https://arxiv.org/abs/2512.12884", "authors": ["Xiangzhong Liu", "Jiajie Zhang", "Hao Shen"], "title": "Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection", "comment": "6 pages, 3 figures, accepted at IV2025", "summary": "In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.", "AI": {"tldr": "本文提出了一种基于Transformer的端到端跨层融合方法，将抽象的目标列表信息与原始相机图像相结合，用于3D目标检测，并引入了去噪查询和可变形高斯掩码，同时解决了目标列表数据集缺失的问题。", "motivation": "在汽车传感器融合系统中，智能传感器和V2X模块通常只提供处理过的目标列表而非原始传感器数据。传统方法通常独立处理原始数据再进行目标级融合。本文旨在提出一种端到端的方法，直接将高度抽象的目标列表信息与原始相机图像融合，以实现3D目标检测。", "method": "该研究提出了一种基于Transformer的端到端跨层融合概念。它将目标列表作为去噪查询输入Transformer，并与可学习查询一起在特征聚合过程中传播。此外，一个从目标列表的位置和尺寸先验派生出的可变形高斯掩码被明确集成到Transformer解码器中，以引导注意力并加速模型训练收敛。由于缺乏独立的目标列表公共数据集，该研究还提出了一种通过模拟状态噪声、假阳性和假阴性从真实边界框生成伪目标列表的方法。", "result": "作为首次进行跨层融合的工作，该方法在nuScenes数据集上相较于基于视觉的基线模型展现出显著的性能提升。它还证明了其在不同噪声水平的模拟目标列表和真实检测器上的泛化能力。", "conclusion": "该研究提出的端到端跨层融合方法有效整合了抽象目标列表和原始相机图像，显著提升了3D目标检测性能，并展示了对不同噪声水平的鲁棒性。同时，其伪目标列表生成方法也为该领域的数据挑战提供了解决方案。"}}
{"id": "2512.12375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12375", "abs": "https://arxiv.org/abs/2512.12375", "authors": ["Hyunkoo Lee", "Wooseok Jang", "Jini Yang", "Taehwan Kim", "Sangoh Kim", "Sangwon Jung", "Seungryong Kim"], "title": "V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping", "comment": "Project Page: https://cvlab-kaist.github.io/V-Warper", "summary": "Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.", "AI": {"tldr": "V-Warper是一个免训练的粗到细个性化框架，用于基于Transformer的视频扩散模型，通过图像适配和推理时外观注入，显著提升了视频中主体外观的一致性，同时避免了大规模视频微调。", "motivation": "现有视频个性化方法依赖于昂贵的视频微调或大型视频数据集，计算成本高且难以扩展。此外，它们在帧间保持精细外观一致性方面仍面临挑战。", "method": "V-Warper采用免训练的粗到细框架：1) 轻量级粗外观适应阶段：利用少量参考图像，通过仅图像的LoRA和主体嵌入适应来编码全局主体身份。2) 推理时精细外观注入阶段：通过计算无RoPE中间层查询-键特征的语义对应关系来细化视觉保真度，指导外观丰富的价值表示扭曲到语义对齐的区域，并辅以掩码确保空间可靠性。", "result": "V-Warper在显著提高外观保真度的同时，保留了提示对齐和运动动态，并且无需大规模视频微调即可高效实现这些改进。", "conclusion": "该框架提供了一种高效、免训练的解决方案，用于解决个性化视频生成中外观一致性差和计算成本高的问题，为基于Transformer的视频扩散模型带来了显著提升。"}}
{"id": "2512.12596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12596", "abs": "https://arxiv.org/abs/2512.12596", "authors": ["Kei Yoshitake", "Kento Hosono", "Ken Kobayashi", "Kazuhide Nakata"], "title": "Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models", "comment": null, "summary": "In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based \"placement plan\" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.", "AI": {"tldr": "本文提出一种利用视觉-语言模型（VLM）生成基于图像广告布局的方法，通过考虑图像的详细构图和语义内容，生成更高质量的广告布局。", "motivation": "传统的广告布局技术主要依赖显著性映射，但未能充分考虑图像的详细构图和语义内容，导致布局质量受限。", "method": "该方法包括两个步骤：首先，VLM分析图像以识别对象类型及其空间关系，并生成一个基于文本的“放置计划”；其次，该计划被渲染成HTML格式代码，从而生成最终布局。", "result": "通过定量和定性比较，实验结果表明，通过明确考虑背景图像的内容，该方法能够生成明显更高质量的广告布局。", "conclusion": "利用VLM明确考虑背景图像内容，能够有效提升广告布局的质量，优于传统基于显著性映射的方法。"}}
{"id": "2512.12885", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12885", "abs": "https://arxiv.org/abs/2512.12885", "authors": ["Minghao Zhu", "Zhihao Zhang", "Anmol Sidhu", "Keith Redmill"], "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition", "comment": "Submitted to IV 2026", "summary": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.", "AI": {"tldr": "本文提出了一种基于检索增强生成（RAG）的零样本道路标志识别框架，利用视觉语言模型（VLM）和大型语言模型（LLM）解决传统深度学习在多类别和数据稀缺问题上的挑战，实现了无需特定训练的准确识别。", "motivation": "智能交通系统中自动化道路标志识别至关重要，但传统深度学习方法难以应对大量的标志类别，且创建详尽的标注数据集不切实际。", "method": "该方法将RAG范式应用于零样本道路标志识别。首先，使用VLM从输入图像生成标志的文本描述；然后，利用该描述从参考设计向量数据库中检索最相关的候选标志；最后，LLM对检索到的候选标志进行推理，以实现最终的精细识别。", "result": "该框架在俄亥俄州MUTCD的303个交通标志上进行了验证。在理想参考图像上实现了95.58%的准确率，在具有挑战性的真实道路数据上达到了82.45%的准确率。", "conclusion": "这项工作证明了基于RAG的架构在创建可扩展、准确且无需任务特定训练的道路标志识别系统方面的可行性。"}}
{"id": "2512.12662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12662", "abs": "https://arxiv.org/abs/2512.12662", "authors": ["Muhammad Umar Farooq", "Abd Ur Rehman", "Azka Rehman", "Muhammad Usman", "Dong-Kyu Chae", "Junaid Qadir"], "title": "Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images", "comment": null, "summary": "Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.", "AI": {"tldr": "SSMT-Net是一个半监督多任务Transformer网络，通过利用无标签数据和多任务学习（结节分割、腺体分割、结节大小估计），显著提高了甲状腺结节在超声图像中的分割精度和鲁棒性。", "motivation": "甲状腺结节在超声图像中的准确分割对诊断和治疗至关重要。然而，结节边界模糊、尺寸多样性以及带标注数据的稀缺性带来了挑战。现有深度学习模型难以有效整合甲状腺腺体的上下文信息并泛化到不同病例。", "method": "本文提出了SSMT-Net，一个半监督多任务Transformer网络。该方法首先在一个无监督阶段利用无标签数据增强Transformer编码器的特征提取能力。在监督阶段，模型联合优化结节分割、腺体分割和结节大小估计，并整合局部和全局上下文特征。", "result": "在TN3K和DDTI数据集上的广泛评估表明，SSMT-Net优于现有最先进的方法，展现出更高的准确性和鲁棒性。", "conclusion": "SSMT-Net在甲状腺结节超声图像分割方面表现出色，具有较高的临床应用潜力。"}}
{"id": "2512.13685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13685", "abs": "https://arxiv.org/abs/2512.13685", "authors": ["Dylan Phelps", "Rodrigo Wilkens", "Edward Gow-Smith", "Lilian Hubner", "Bárbara Malcorra", "César Rennó-Costa", "Marco Idiart", "Maria-Cruz Villa-Uriol", "Aline Villavicencio"], "title": "Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech", "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.", "AI": {"tldr": "该研究通过改变文本的表层形式但保留语义内容，证明语言模型能有效识别阿尔茨海默病(AD)的语义障碍，从而促进早期检测系统的发展。", "motivation": "语言模型在AD筛查中具有潜力，但其可解释性有限，难以区分真正的认知衰退语言标记和表层文本模式。研究旨在评估语言模型表示底层语义指标的能力。", "method": "引入了一种新颖的方法，通过改变语法和词汇来转换文本的表层形式，同时保留语义内容。使用BLEU和chrF分数衡量表层变化，使用语义相似度分数衡量语义保留。此外，还探讨了图片描述语言是否包含足够细节来通过生成模型重建原始图像。", "result": "转换后的文本在AD分类中的表现与原始文本相似，宏观F1分数仅有小幅偏差，表明模型主要依赖语义信息。图像转换增加了大量噪声，降低了分类准确性。研究发现，仅使用语义信息，基于语言模型的分类器仍能检测AD。", "conclusion": "该工作表明，语言模型能够识别难以检测的语义损伤，解决了语言退化中一个被忽视的特征，为AD的早期检测系统开辟了新途径。"}}
{"id": "2512.12386", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12386", "abs": "https://arxiv.org/abs/2512.12386", "authors": ["Swayam Bhanded"], "title": "Speedrunning ImageNet Diffusion", "comment": null, "summary": "Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.", "AI": {"tldr": "本文提出了SR-DiT，一个系统整合多种效率提升技术的扩散Transformer框架，在ImageNet-256上以140M参数模型和400K迭代实现了与更大、训练更久的模型相当的SOTA性能。", "motivation": "扩散Transformer的训练效率技术大多是独立研究的，其组合潜力尚未被充分探索。", "method": "SR-DiT框架在表示对齐的基础上，系统地整合了token路由、架构改进和训练修改。通过广泛的消融研究，识别了最有效的技术组合，并记录了协同效应和不兼容性。", "result": "SR-DiT在ImageNet-256上使用140M参数模型，在400K迭代且无分类器引导的情况下，实现了FID 3.49和KDD 0.319。这一结果与685M参数、训练时间更长的模型相当，是该模型尺寸下的最新SOTA成果。研究还揭示了不同技术组合的有效性、协同作用和不兼容性。", "conclusion": "SR-DiT提供了一个计算上可访问的基线框架，通过系统整合多种效率技术，显著提升了扩散Transformer的训练效率和性能，为未来的研究奠定了基础。"}}
{"id": "2512.13676", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13676", "abs": "https://arxiv.org/abs/2512.13676", "authors": ["Baixiang Huang", "Limeng Cui", "Jiapeng Liu", "Haoran Wang", "Jiawei Xu", "Zhuiyue Tan", "Yutong Chen", "Chen Luo", "Yi Liu", "Kai Shu"], "title": "Towards Effective Model Editing for LLM Personalization", "comment": "15 pages (including appendix), 7 figures. Code, data, results, and additional resources are available at: https://model-editing.github.io", "summary": "Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.", "AI": {"tldr": "本文提出Personalization Editing框架，将个性化视为模型编辑任务，通过局部编辑和聚类偏好表示实现高效精准的LLM个性化。同时引入UPQA数据集，直接评估模型召回和应用用户偏好的能力，实验证明该方法在准确性和效率上优于现有基线。", "motivation": "当前LLM个性化方法面临计算成本高、数据密集、灾难性遗忘以及在多轮交互或隐式查询中性能下降等挑战。此外，现有基准测试常依赖LLM间的角色对话，或侧重风格模仿而非信息检索，无法有效评估模型对用户特定偏好的准确记忆和应用能力。", "method": "本文将个性化概念化为模型编辑任务，并引入Personalization Editing框架。该框架通过聚类偏好表示引导局部编辑，以实现精确的偏好对齐更新，同时保留模型整体能力。此外，本文构建了User Preference Question Answering (UPQA) 短答案问答数据集，该数据集基于真实用户查询，直接评估模型召回和应用特定用户偏好的能力。", "result": "在各项实验设置中，Personalization Editing相比微调方法实现了更高的编辑准确性和计算效率。在多轮对话和隐式偏好问题场景下，其性能优于基于提示的基线方法。", "conclusion": "Personalization Editing框架通过将个性化视为模型编辑任务，并结合聚类偏好表示，有效解决了当前LLM个性化面临的效率和准确性挑战。新引入的UPQA数据集为评估模型召回和应用用户偏好的能力提供了更直接、更相关的基准。实验结果表明，该方法在多个关键方面表现出色，为未来的LLM个性化研究提供了新的方向。"}}
{"id": "2512.12378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12378", "abs": "https://arxiv.org/abs/2512.12378", "authors": ["Junqiao Fan", "Yunjiao Zhou", "Yizhuo Yang", "Xinyuan Cui", "Jiarui Zhang", "Lihua Xie", "Jianfei Yang", "Chris Xiaoxuan Lu", "Fangqiang Ding"], "title": "M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction", "comment": null, "summary": "Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.", "AI": {"tldr": "本文介绍了M4Human，一个大规模（661K帧）多模态基准数据集，包含毫米波雷达、RGB和深度数据，并提供高质量的运动捕捉（MoCap）标注。该数据集旨在克服现有数据集的局限性，推动基于雷达的人体网格重建（HMR）研究，并建立了多模态融合的基准。", "motivation": "现有的人体网格重建（HMR）数据集主要依赖于RGB输入，但受限于遮挡、光照变化和隐私问题。尽管毫米波雷达为室内人体感知提供了隐私保护的解决方案，但当前的雷达数据集规模小、标注稀疏且仅限于简单的原地动作。为了克服这些限制并推动HMR研究，需要一个更大规模、更高质量、更多样化的多模态数据集。", "method": "本文提出了M4Human数据集，它是目前最大规模的（661K帧，是现有最大数据集的9倍）多模态基准。M4Human包含高分辨率的毫米波雷达、RGB和深度数据，并提供原始雷达张量（RT）和处理后的雷达点云（RPC）。数据集拥有高质量的运动捕捉（MoCap）标注，包括3D网格和全局轨迹，涵盖20个受试者和50种多样化动作（包括原地、坐姿原地、自由空间运动或康复动作）。研究者还在RT、RPC模态以及与RGB-D模态的多模态融合上建立了基准。", "result": "广泛的实验结果表明M4Human对于基于雷达的人体建模具有重要意义。同时，基准测试也揭示了在快速、无约束运动下持续存在的挑战。", "conclusion": "M4Human数据集通过提供一个大规模、高质量的多模态基准，极大地推动了基于雷达的人体网格重建研究。它不仅为研究社区提供了丰富的数据资源，还揭示了在处理快速、复杂运动时，基于雷达的人体建模仍面临的挑战，为未来的研究指明了方向。"}}
{"id": "2512.13667", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13667", "abs": "https://arxiv.org/abs/2512.13667", "authors": ["Cristina Aggazzotti", "Elizabeth Allyn Smith"], "title": "A stylometric analysis of speaker attribution from speech transcripts", "comment": null, "summary": "Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.", "AI": {"tldr": "本文提出了一种基于文本内容的说话人归因方法 StyloSpeaker，通过分析转录语音的语言风格特征来识别说话人，尤其适用于声音特征不可靠的情况。", "motivation": "在法医科学中，识别未知说话人或写作者至关重要。传统的语音识别依赖于语音的声学或语音特性，但在声音被伪装或使用文本转语音软件时会失效。因此，需要一种仅基于语言内容来识别说话人的方法。", "method": "本文引入了 StyloSpeaker，一种将说话人语音转录为文本后，应用内容归因方法识别说话人的文体计量学方法。该方法整合了来自作者归因文献中的字符、单词、标记、句子和风格特征。研究在两种转录格式（规范书面文本和标准化文本）以及不同程度的话题控制下进行了评估，并与黑盒神经网络方法进行了比较。", "result": "研究发现，在标准化转录文本上通常能获得更高的归因性能，但在最强的话题控制条件下，整体性能最高。此外，该可解释的文体计量学模型与黑盒神经网络方法进行了比较，并探讨了哪些风格特征最能有效区分说话人。", "conclusion": "StyloSpeaker 是一种有效的基于内容的说话人归因方法，尤其在声音特征不可靠时。标准化转录文本有助于提高归因性能，且话题控制对性能有显著影响。该方法也为理解区分说话人的关键风格特征提供了见解。"}}
{"id": "2512.12633", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12633", "abs": "https://arxiv.org/abs/2512.12633", "authors": ["Zhou Tao", "Shida Wang", "Yongxiang Hua", "Haoyu Cao", "Linli Xu"], "title": "DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model", "comment": null, "summary": "Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.", "AI": {"tldr": "本文提出DiG（差异定位），一种新颖的代理任务框架，通过识别和定位相似图像对之间的所有差异，来提升多模态大语言模型（MLLMs）的细粒度视觉感知和空间推理能力。DiG利用自动化3D渲染数据生成和课程学习进行训练，显著提高了模型在各种视觉感知基准上的性能，并有效迁移到下游任务。", "motivation": "多模态大语言模型（MLLMs）在多种视觉-语言任务上表现出色，但在细粒度视觉感知和精确空间推理方面仍存在局限性。", "method": "引入DiG（差异定位）作为一种新颖的代理任务框架，MLLMs通过识别和定位相似图像对之间所有差异来学习细粒度感知。开发了基于自动化3D渲染的数据生成管道，以产生具有完全可控差异的高质量配对图像。采用课程学习策略，从单一差异逐步增加到多重差异，以解决差异信号稀疏性问题并实现稳定优化。", "result": "DiG显著提高了模型在各种视觉感知基准上的性能。所学到的细粒度感知技能有效地迁移到标准下游任务，包括RefCOCO、RefCOCO+、RefCOCOg以及一般的多模态感知基准。", "conclusion": "差异定位（DiG）是一种可扩展且鲁棒的方法，能够有效提升多模态大语言模型中的细粒度视觉推理能力。"}}
{"id": "2512.13030", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13030", "abs": "https://arxiv.org/abs/2512.13030", "authors": ["Hongzhe Bi", "Hengkai Tan", "Shenghao Xie", "Zeyuan Wang", "Shuhe Huang", "Haitian Liu", "Ruowen Zhao", "Yao Feng", "Chendong Xiang", "Yinze Rong", "Hongyan Zhao", "Hanyu Liu", "Zhizhong Su", "Lei Ma", "Hang Su", "Jun Zhu"], "title": "Motus: A Unified Latent Action World Model", "comment": null, "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.", "AI": {"tldr": "Motus是一个统一的潜在动作世界模型，旨在解决具身智能体中模型碎片化的问题。它通过MoT架构、UniDiffuser风格调度器和光学流学习潜在动作，实现了多功能统一建模，并在模拟和真实世界场景中均取得了显著优于SOTA的性能。", "motivation": "目前的具身智能体方法依赖于理解、世界建模和控制的独立模型，这种碎片化阻碍了多模态生成能力的统一，并限制了从大规模异构数据中学习的能力。", "method": "本文提出了Motus，一个统一的潜在动作世界模型。它利用现有通用预训练模型和丰富的可共享运动信息。Motus引入了MoT（Mixture-of-Transformer）架构来整合三个专家（理解、视频生成和动作），并采用UniDiffuser风格的调度器，以实现不同建模模式（世界模型、视觉-语言-动作模型、逆动力学模型、视频生成模型和视频-动作联合预测模型）之间的灵活切换。Motus还利用光流来学习潜在动作，并采用三阶段训练管道和六层数据金字塔的方法，提取像素级“delta动作”，实现大规模动作预训练。", "result": "Motus在模拟和真实世界场景中均取得了优于现有SOTA方法的性能。在模拟中，相较于X-VLA提升了15%，相较于Pi0.5提升了45%。在真实世界场景中，性能提升了11%到48%。实验结果表明，所有功能和先验的统一建模显著有利于下游机器人任务。", "conclusion": "Motus通过统一建模所有功能和先验，显著提升了下游机器人任务的性能，证明了这种统一方法相较于现有碎片化模型的优越性。"}}
{"id": "2512.13177", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13177", "abs": "https://arxiv.org/abs/2512.13177", "authors": ["Minghui Hou", "Wei-Hsing Huang", "Shaofeng Liang", "Daizong Liu", "Tai-Hao Wen", "Gang Wang", "Runwei Guan", "Weiping Ding"], "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion", "comment": null, "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.", "AI": {"tldr": "MMDrive是一个多模态视觉-语言模型框架，通过融合占用图、激光雷达点云和文本描述，将2D图像理解扩展到3D场景理解，并通过自适应跨模态融合和关键信息提取，在自动驾驶场景理解方面显著优于现有模型。", "motivation": "现有视觉-语言模型受限于2D图像理解范式，限制了其3D空间感知和深度语义融合能力，导致在复杂自动驾驶环境中性能不佳。", "method": "MMDrive框架整合了占用图、激光雷达点云和文本描述三种模态。它引入了两个新组件：文本导向多模态调制器（Text-oriented Multimodal Modulator）根据问题语义动态加权模态贡献，以及跨模态抽象器（Cross-Modal Abstractor）利用可学习的抽象token生成紧凑的跨模态摘要。", "result": "MMDrive在DriveLM基准测试中获得BLEU-4得分54.56和METEOR得分41.78，在NuScenes-QA基准测试中获得62.7%的准确率，均显著优于现有自动驾驶视觉-语言模型。", "conclusion": "MMDrive有效突破了传统仅基于图像的理解限制，实现了复杂驾驶环境中的鲁棒多模态推理，为可解释的自动驾驶场景理解奠定了新基础。"}}
{"id": "2512.12395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12395", "abs": "https://arxiv.org/abs/2512.12395", "authors": ["Haowen Wang", "Xiaoping Yuan", "Fugang Zhang", "Rui Jian", "Yuanwei Zhu", "Xiuquan Qiao", "Yakun Huang"], "title": "ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States", "comment": null, "summary": "Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.", "AI": {"tldr": "ArtGen是一个基于条件扩散的框架，能从单视图图像或文本描述生成具有准确几何和连贯运动学的3D关节对象，通过跨状态采样、思维链推理和稀疏专家扩散Transformer解决现有方法的挑战。", "motivation": "生成关节资产对机器人、数字孪生和具身智能至关重要。现有生成模型常依赖单视图输入，导致几何形状与关节动力学纠缠，产生模糊或不真实的运动学结构。", "method": "ArtGen采用条件扩散框架，通过以下方式解决问题：1) 跨状态蒙特卡洛采样明确强制全局运动学一致性，减少结构-运动纠缠。2) 集成思维链（Chain-of-Thought）推理模块推断鲁棒的结构先验（如部件语义、关节类型和连接性）。3) 稀疏专家扩散Transformer在多样运动学交互中进行专业化，由结构先验指导。4) 结合局部-全局注意力的组合式3D-VAE潜在先验有效捕获精细几何和全局部件级关系。", "result": "在PartNet-Mobility基准测试中，ArtGen显著优于现有最先进的方法。", "conclusion": "ArtGen成功地解决了从单视图输入生成关节3D对象时几何和运动学纠缠的问题，能够生成具有准确几何和连贯运动学的对象，显著超越了现有技术。"}}
{"id": "2512.12492", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12492", "abs": "https://arxiv.org/abs/2512.12492", "authors": ["Shengkai Xu", "Hsiang Lun Kao", "Tianxiang Xu", "Honghui Zhang", "Junqiao Wang", "Runmeng Ding", "Guanyu Liu", "Tianyu Shi", "Zhenyu Yu", "Guofeng Pan", "Ziqian Bi", "Yuqi Ouyang"], "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings", "comment": null, "summary": "Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.", "AI": {"tldr": "本文提出了一种名为AdaptiveDetector的双阶段检测器-验证器框架，结合YOLOv11和视觉-语言模型（VLM），通过自适应置信度阈值和成本敏感的强化学习，显著提高了在真实世界内窥镜图像降级条件下的息肉召回率，减少了漏检。", "motivation": "在实际内窥镜检查中，由于光照变化、运动模糊和遮挡等不利条件，现有在干净数据集上训练的息肉检测器性能不佳，存在领域鸿沟。漏检息肉是临床上的关键问题，需要一种在挑战性条件下表现稳健的检测方法。", "method": "本文提出了AdaptiveDetector，一个包含YOLOv11检测器和VLM验证器的双阶段框架。检测器在VLM指导下自适应调整每帧的置信度阈值。验证器通过组相对策略优化（GRPO）进行微调，并采用不对称、成本敏感的奖励函数，旨在强烈惩罚漏检。为进行真实评估，构建了一个合成测试平台，通过系统地对干净数据集施加不利条件，进行零样本评估。", "result": "在合成降级的CVC-ClinicDB和Kvasir-SEG图像上进行广泛的零样本评估显示，与单独使用YOLO相比，该方法将召回率提高了14到22个百分点，同时精度保持在基线上下0.7到1.7个百分点之间。", "conclusion": "自适应阈值和成本敏感强化学习的结合实现了临床对齐的、开放世界息肉检测，显著减少了假阴性，从而降低了漏检癌前息肉的风险，改善了患者预后。"}}
{"id": "2512.12675", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12675", "abs": "https://arxiv.org/abs/2512.12675", "authors": ["Yuran Wang", "Bohan Zeng", "Chengzhuo Tong", "Wenxuan Liu", "Yang Shi", "Xiaochen Ma", "Hao Liang", "Yuanxing Zhang", "Wentao Zhang"], "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling", "comment": "Code: https://github.com/Ryann-Ran/Scone", "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.", "AI": {"tldr": "Scone是一种统一的理解-生成方法，通过整合多主体组合和主体区分能力，显著提升了主体驱动的图像生成效果，并在新基准上超越了现有模型。", "motivation": "现有主体驱动的图像生成方法在处理多主体场景时，忽视了“区分”能力，即在存在多个候选主体时识别并生成正确主体的能力，这限制了它们在复杂现实视觉环境中的有效性。", "method": "Scone提出了一种统一的理解-生成方法，将组合与区分相结合。它利用“理解专家”作为语义桥梁，传递语义信息并指导“生成专家”保持主体身份，同时最小化干扰。训练分为两阶段：首先学习组合，然后通过语义对齐和基于注意力的掩码增强区分能力。此外，还引入了SconeEval基准来评估组合和区分能力。", "result": "实验证明，Scone在两个基准测试的组合和区分任务中均优于现有的开源模型。", "conclusion": "Scone成功地将主体驱动图像生成中的组合与区分能力相结合，展示了卓越的性能，并提供了一个新的评估基准，有效解决了多主体场景下的主体识别和生成问题。"}}
{"id": "2512.12424", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12424", "abs": "https://arxiv.org/abs/2512.12424", "authors": ["Tue-Thu Van-Dinh", "Hoang-Duy Tran", "Truong-Binh Duong", "Mai-Hanh Pham", "Binh-Nam Le-Nguyen", "Quoc-Thai Nguyen"], "title": "ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics", "comment": "10 pages, 4 figures, Accepted to AI4Research @ AAAI", "summary": "Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.", "AI": {"tldr": "本文介绍了ViInfographicVQA，首个越南语信息图视觉问答基准，包含单图和多图任务，并揭示了当前多模态模型在跨图推理和非跨度推理方面的局限性。", "motivation": "现有的视觉问答（VQA）基准（如场景文本或自然图像VQA）未能充分评估模型在信息丰富、布局复杂的信息图上的推理能力，这类图像结合了文本、图表、图标和设计元素，需要更强的OCR、布局理解、数值和语义推理。此外，缺乏针对越南语信息图的基准，特别是涉及跨图推理的评估。", "method": "研究者引入了ViInfographicVQA基准，包含6747张真实世界信息图和20409个人工验证的问答对。该基准设计了两种评估设置：单图任务（传统设置）和多图任务（需要跨多个语义相关信息图合成证据）。研究者评估了一系列最新的视觉-语言模型在该基准上的表现。", "result": "评估结果显示，当前视觉-语言模型在ViInfographicVQA基准上表现出显著的性能差异，其中最主要的错误发生在涉及跨图整合和非跨度推理的多图问题上。", "conclusion": "ViInfographicVQA为越南语信息图视觉问答贡献了基准结果，并揭示了当前多模态模型在低资源背景下的局限性。这鼓励了未来在布局感知和跨图推理方法方面的探索。"}}
{"id": "2512.13636", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13636", "abs": "https://arxiv.org/abs/2512.13636", "authors": ["Haoyu Fu", "Diankun Zhang", "Zongchuang Zhao", "Jianfeng Cui", "Hongwei Xie", "Bing Wang", "Guang Chen", "Dingkang Liang", "Xiang Bai"], "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning", "comment": "16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/", "summary": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.", "AI": {"tldr": "MindDrive是一个基于大语言模型（LLM）的视觉-语言-动作（VLA）自动驾驶框架，通过将在线强化学习应用于离散的语言决策空间而非连续动作空间，解决了传统模仿学习的挑战和在线强化学习的探索效率问题。", "motivation": "当前的自动驾驶VLA范式主要依赖模仿学习，存在分布偏移和因果混淆等固有挑战。在线强化学习虽有潜力解决这些问题，但其在连续动作空间中的低效探索阻碍了在VLA模型中的应用。", "method": "MindDrive框架包含一个具有两组LoRA参数的LLM：一个作为决策专家，负责情景推理和驾驶决策；另一个作为动作专家，将语言决策动态映射到可行轨迹。通过将轨迹级奖励反馈到推理空间，MindDrive实现了在有限离散语言驾驶决策集上的试错学习，而非直接在连续动作空间中操作。", "result": "MindDrive在挑战性的Bench2Drive基准测试中取得了强大的闭环性能，驾驶分数（DS）达到78.04%，成功率（SR）达到55.09%。这是首次证明在线强化学习在自动驾驶VLA模型中有效性的工作。", "conclusion": "MindDrive有效平衡了复杂场景下的最优决策、类人驾驶行为和在线强化学习中的高效探索，为自动驾驶VLA模型成功应用在线强化学习开辟了道路。"}}
{"id": "2512.12703", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12703", "abs": "https://arxiv.org/abs/2512.12703", "authors": ["Boyuan Li", "Sipeng Zheng", "Bin Cao", "Ruihua Song", "Zongqing Lu"], "title": "Robust Motion Generation using Part-level Reliable Data from Videos", "comment": null, "summary": "Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.\n  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as \"credible\". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.\n  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/", "AI": {"tldr": "该研究提出了一种鲁棒的、部分感知的掩码自回归模型，用于从包含缺失部分的大规模网络视频中提取人体运动，并贡献了一个新的基准数据集，以解决角色动画数据稀缺和数据质量问题。", "motivation": "角色动画面临数据稀缺问题，而从大规模网络视频中提取运动是可扩展的解决方案。然而，许多视频帧中人体部分因屏幕外捕获或遮挡而不可见，这导致一个困境：丢弃缺失部分的数据会限制规模和多样性，而保留它们会损害数据质量和模型性能。", "method": "1. 将人体分解为五个部分，并检测视频帧中清晰可见的“可信”部分。\n2. 通过提出的部分感知变分自编码器将可信部分编码为潜在标记。\n3. 提出一个鲁棒的部分级掩码生成模型，用于预测被掩码的可信部分，同时忽略嘈杂的部分。\n4. 贡献了一个名为K700-M的新基准数据集，包含约20万个真实世界运动序列，用于评估。", "result": "实验结果表明，该方法在运动质量、语义一致性和多样性方面，在干净和嘈杂的数据集上均优于基线方法。", "conclusion": "该方法通过利用可信的部分级数据和鲁棒的部分感知掩码自回归模型，成功解决了从大规模网络视频中提取运动时人体部分缺失的问题，显著提升了运动生成的质量和鲁棒性。"}}
{"id": "2512.12425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12425", "abs": "https://arxiv.org/abs/2512.12425", "authors": ["Hangwei Zhang", "Armando Teles Fortes", "Tianyi Wei", "Xingang Pan"], "title": "BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation", "comment": null, "summary": "Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.", "AI": {"tldr": "BokehDepth是一个两阶段框架，通过解耦散景合成和深度预测，并利用散焦作为无监督的几何线索，显著提升了散景渲染质量和单目深度估计的准确性与鲁棒性。", "motivation": "当前高质量散景渲染依赖于噪声深度图，导致视觉伪影；而现代单目度量深度模型在纹理弱、远距离和几何模糊区域表现不佳，这些区域正是散焦线索最有用的地方。", "method": "BokehDepth包含两个阶段：第一阶段，一个基于预训练图像编辑骨干的物理引导可控散景生成器，从单一清晰输入生成具有校准散景强度的无深度散景堆栈。第二阶段，一个轻量级的散焦感知聚合模块插入现有单目深度编码器，沿散焦维度融合特征以揭示稳定的深度敏感变化，同时保持下游解码器不变。", "result": "BokehDepth在具有挑战性的基准测试中，相较于基于深度图的散景基线，提升了视觉保真度，并持续提高了强大单目深度基础模型的度量精度和鲁棒性。", "conclusion": "BokehDepth通过将散景合成与深度预测解耦，并利用散焦作为辅助的无监督几何线索，有效解决了现有方法的痛点，同时改善了散景渲染质量和单目深度估计的性能。"}}
{"id": "2512.12623", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12623", "abs": "https://arxiv.org/abs/2512.12623", "authors": ["Chengzhi Liu", "Yuzhe Yang", "Yue Fan", "Qingyue Wei", "Sheng Liu", "Xin Eric Wang"], "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.", "AI": {"tldr": "DMLR是一个动态多模态潜在推理框架，通过置信度引导的潜在策略梯度优化和动态视觉注入策略，解决了现有视觉CoT方法在多模态大语言模型中存在的显式逐步推理、不稳定感知-推理交互和高计算开销问题，显著提升了推理和感知性能及推理效率。", "motivation": "当前多模态大语言模型（MLLMs）中的视觉思维链（CoT）方法依赖于显式分步推理、不稳定的感知-推理交互以及显著的计算开销。受人类认知中推理与感知动态交织的启发，研究者认为思考过程并非线性，而是动态穿插的，因此需要一种更高效、灵活的推理机制。", "method": "本文提出了DMLR（Dynamic Multimodal Latent Reasoning）框架，它在测试时通过置信度引导的潜在策略梯度优化来精炼潜在思考（think）令牌，以进行深入推理。此外，引入了动态视觉注入策略，该策略在每个潜在思考令牌处检索最相关的视觉特征，更新最佳视觉补丁集，然后将更新后的补丁注入到潜在思考令牌中，实现动态的视觉-文本交织。", "result": "实验结果表明，DMLR在七个多模态推理基准和多种模型架构上显著提高了推理和感知性能，同时保持了高推理效率。", "conclusion": "DMLR通过动态多模态潜在推理和视觉注入，有效解决了现有视觉CoT方法的局限性，实现了推理和感知性能的显著提升，并保持了高效的推理能力，为多模态大语言模型的推理能力提供了新的方向。"}}
{"id": "2512.12430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12430", "abs": "https://arxiv.org/abs/2512.12430", "authors": ["Ke Zhang", "Yiqun Mei", "Jiacong Xu", "Vishal M. Patel"], "title": "Endless World: Real-Time 3D-Aware Long Video Generation", "comment": "10 pages,7 figures", "summary": "Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.", "AI": {"tldr": "Endless World是一个实时框架，用于生成无限、3D一致的视频序列，解决了长视频生成中的连贯性和结构稳定性挑战。", "motivation": "在流媒体场景中，生成具有稳定3D结构的长而连贯的视频序列是一个重大挑战。", "method": "该方法引入了条件自回归训练策略，将新生成的内容与现有视频帧对齐，以支持无限视频生成。此外，它集成了全局3D感知注意力，提供持续的几何指导，并通过3D注入机制确保扩展序列的物理合理性和几何一致性。", "result": "Endless World能够生成长、稳定且视觉连贯的视频，在视觉保真度和空间一致性方面，性能与现有方法相当或更优。", "conclusion": "Endless World成功地实现了实时、无限、3D一致的视频生成，解决了长序列和动态场景合成中的关键挑战，并展示了出色的视觉和空间一致性。"}}
{"id": "2512.12768", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12768", "abs": "https://arxiv.org/abs/2512.12768", "authors": ["Tianjiao Yu", "Xinzhuo Li", "Yifan Shen", "Yuanzhe Liu", "Ismini Lourentzou"], "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence", "comment": null, "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.", "AI": {"tldr": "CoRe3D是一个统一的3D理解和生成推理框架，它结合语义和空间抽象，通过空间接地推理来指导3D内容形成，从而实现与语言描述的高度一致性。", "motivation": "虽然显式推理机制在语言和视觉任务中已被证明能有效提升模型可靠性、可解释性和跨模态对齐，但其在3D领域的扩展仍不充分。", "method": "CoRe3D引入了一个统一的3D理解和生成推理框架，该框架协同操作语义和空间抽象，使从语言推断的高级意图能够直接指导低级3D内容形成。其核心设计是空间接地的推理表示，它将3D潜在空间分解为局部区域，允许模型以组合和程序化的方式对几何进行推理。该方法紧密耦合了语义思维链推理与结构化空间推理。", "result": "CoRe3D生成的3D输出表现出强大的局部一致性，并与语言描述忠实对齐。", "conclusion": "CoRe3D通过结合语义思维链推理和结构化空间推理，成功地将高级语言意图转化为高质量的3D内容，填补了3D领域推理方法的空白。"}}
{"id": "2512.12701", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12701", "abs": "https://arxiv.org/abs/2512.12701", "authors": ["Xue Li", "Xiaonan Song", "Henry Hu"], "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning", "comment": "10 pages, 3 figures. Expanded version of an extended abstract accepted at NeurIPS 2025 Workshop on VLM4RWD. Presents methodology and preliminary experimental results", "summary": "Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.", "AI": {"tldr": "本文提出自适应Token剪枝（ATP）机制，通过动态保留最具信息量的Token来降低视觉语言模型（VLM）的推理计算成本，在保持几乎不损失准确性的前提下，显著提高速度并减少FLOPs，同时增强模型鲁棒性。", "motivation": "现有视觉语言模型（VLM）因对所有Token进行统一处理而导致计算需求高，阻碍了其在实际场景中的部署。", "method": "ATP是一种动态推理机制，在视觉-语言接口处运行。它结合了ViT CLS注意力（模态内显著性）和CLIP文本-图像相似度（模态间相关性），分配混合重要性分数，以保留前K个最具信息量的Token供大型语言模型（LLM）处理。ATP是一个轻量级门控模块，与BLIP-2、LLaVA和Flamingo等流行骨干网络兼容，无需修改骨干网络即可适应每个输入。", "result": "初步评估显示，ATP在VQAv2、GQA和COCO数据集上将推理FLOPs减少约40%，端到端延迟加速约1.5倍，且准确性损失可忽略不计（低于1%）。定性分析表明ATP保留了视觉基础并增强了可解释性。此外，在损坏条件下的鲁棒性研究表明，自适应剪枝抑制了虚假相关性，提高了模型稳定性。", "conclusion": "这些发现表明资源受限的推理与模型可靠性并非相互竞争的目标。ATP在高效多模态边缘计算流水线中具有重要作用。"}}
{"id": "2512.12929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12929", "abs": "https://arxiv.org/abs/2512.12929", "authors": ["Huu-An Vu", "Van-Khanh Mai", "Trong-Tam Nguyen", "Quang-Duc Dam", "Tien-Huy Nguyen", "Thanh-Huong Le"], "title": "MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation", "comment": null, "summary": "The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.", "AI": {"tldr": "MADTempo是一个视频检索框架，它通过结合事件级时间搜索和基于Google图片搜索的视觉接地模块，解决了现有系统在建模多事件时间依赖性和处理未见视觉概念方面的不足。", "motivation": "在线视频内容的快速增长，使得需要能够理解复杂事件时间结构的检索系统。现有方法在建模跨多个事件的时间依赖性以及处理引用未见或罕见视觉概念的查询时表现不佳。", "method": "MADTempo框架包含两个主要组件：1. 时间搜索机制：通过聚合连续视频片段的相似性分数来捕获事件级连续性，实现对多事件查询的连贯检索。2. 基于Google图片搜索的后备模块：利用外部网络图像扩展查询表示，弥补预训练视觉嵌入的不足，提高对分布外(OOD)查询的鲁棒性。", "result": "这些组件共同提升了现代视频检索系统的时间推理和泛化能力，使其能够更具语义感知和适应性地在大规模视频语料库中进行检索。", "conclusion": "MADTempo为大规模视频语料库中更具语义感知和适应性的检索铺平了道路，显著提升了视频检索系统的时间推理和泛化能力。"}}
{"id": "2512.12498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12498", "abs": "https://arxiv.org/abs/2512.12498", "authors": ["Tasweer Ahmad", "Arindam Sikdar", "Sandip Pradhan", "Ardhendu Behera"], "title": "Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention", "comment": null, "summary": "Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the \"platinum minutes\" and the broader \"golden hour\" window in time-critical UAV-driven search-and-rescue and combat casualty care.", "AI": {"tldr": "本文提出了一种新颖的补丁驱动关系细化方法，通过在训练阶段使用图注意力网络从图像内部补丁依赖中学习缓存适配器权重，从而在不增加推理成本的情况下，显著提升了少样本图像分类在领域偏移下的性能。", "motivation": "现有的基于缓存的适配器（如Tip-Adapter）继承了CLIP的全局、通用表示，在数据量有限的情况下，这些表示对于将通用模型适应到特定领域并非最优。此外，战场上对伤员识别（如受伤与未受伤士兵）的迫切需求，需要支持在关键时间窗口内进行分诊决策。", "method": "该研究通过补丁驱动的关系细化来学习缓存适配器权重，而不是将图像嵌入视为单一向量。具体方法包括：引入一个关系门控图注意力网络（GAT），构建补丁图并执行边感知注意力以强调补丁间的互动，生成上下文丰富的补丁嵌入。接着，一个可学习的多聚合池化将这些嵌入组合成紧凑、任务判别性的表示。关键在于，图细化仅在训练期间用于将关系结构蒸馏到缓存中，不产生额外的推理成本。最终预测通过缓存相似度分数与CLIP零样本logits的残差融合获得。", "result": "在11个基准测试中，该方法持续优于最先进的CLIP适配器和基于缓存的基线模型，同时保持了零样本效率。此外，通过引入“受伤与未受伤士兵”数据集进行伤亡识别，验证了其在战场上的实际应用价值。", "conclusion": "所提出的补丁驱动关系细化方法通过在训练阶段利用图像内部补丁依赖性，有效地解决了少样本图像分类在领域偏移下的挑战，生成了更具判别性的缓存表示，且不增加推理成本。其在多项基准测试上的优异表现以及在战术情境下的应用潜力得到了证实。"}}
{"id": "2512.12822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12822", "abs": "https://arxiv.org/abs/2512.12822", "authors": ["Yongyuan Liang", "Xiyao Wang", "Yuanchen Ju", "Jianwei Yang", "Furong Huang"], "title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding", "comment": null, "summary": "Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.", "AI": {"tldr": "本文提出了Lemon，一个统一的Transformer架构，通过将3D点云块和语言token作为单一序列联合处理，实现了早期空间-语言融合，解决了3D多模态理解中数据稀疏、架构碎片化和训练不稳定的挑战，并在多项3D任务上达到了最先进的性能。", "motivation": "将大型多模态模型（LMMs）扩展到3D理解面临独特挑战：点云数据稀疏且不规则；现有模型依赖于碎片化的架构和模态特定编码器；训练流程常受不稳定性和可扩展性差的困扰。", "method": "引入Lemon，一个统一的Transformer架构，通过联合处理3D点云块和语言token作为单一序列实现早期空间-语言融合。开发了结构化的patchification和tokenization方案以保留空间上下文，并采用三阶段训练课程，逐步从物体级识别到场景级空间推理构建能力。", "result": "Lemon在全面的3D理解和推理任务（包括物体识别、图像字幕和3D场景中的空间推理）上建立了新的最先进性能，并随着模型大小和训练数据增加展示了强大的扩展特性。", "conclusion": "本工作为推进现实世界应用中的3D空间智能提供了一个统一的基础。"}}
{"id": "2512.12508", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12508", "abs": "https://arxiv.org/abs/2512.12508", "authors": ["Jinfan Zhou", "Lixin Luo", "Sungmin Eum", "Heesung Kwon", "Jeong Joon Park"], "title": "Generative Spatiotemporal Data Augmentation", "comment": null, "summary": "We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.", "AI": {"tldr": "本文提出一种利用视频基础模型进行时空数据增强的方法，通过生成多样的视角和场景动态来扩充数据，从而在数据稀缺场景下显著提升模型性能。", "motivation": "现有的数据增强方法（如简单的几何变换或外观扰动）无法有效捕捉真实世界的3D空间和时间变化。在无人机图像等数据标注稀缺的低数据环境下，模型性能提升面临挑战。", "method": "该方法利用现成的视频扩散模型，从给定的图像数据集中生成逼真的3D空间和时间变化的视频片段。这些合成的视频片段被用作补充训练数据。研究还提供了关于选择合适的时空生成设置、将标注迁移到合成帧以及处理生成视图中新暴露且未标注区域（disocclusion）的实用指导。", "result": "在COCO子集和无人机捕获的数据集上的实验表明，该方法在低数据环境下（如无人机图像）取得了持续的性能提升。时空数据增强有效地拓宽了数据分布，补充了传统和现有生成方法未充分表示的维度。", "conclusion": "时空数据增强，当谨慎应用时，通过多样化相机视角和场景动态，拓宽了数据分布，为改善数据稀缺环境下的模型性能提供了一种有效的手段。"}}
{"id": "2512.12935", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12935", "abs": "https://arxiv.org/abs/2512.12935", "authors": ["Toan Le Ngo Thanh", "Phat Ha Huu", "Tan Nguyen Dang Duy", "Thong Nguyen Le Minh", "Anh Nguyen Nhu Tinh"], "title": "Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion", "comment": "Accepted at AAAI Workshop 2026", "summary": "The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.", "AI": {"tldr": "本文提出一个统一的多模态精彩瞬间检索系统，通过级联双嵌入管道、时间感知评分机制和代理引导查询分解，解决了现有方法在固定融合策略、时间建模和手动模态选择方面的挑战，提升了交互式瞬间搜索能力。", "motivation": "视频内容呈指数级增长，急需高效的多模态瞬间检索系统。然而，现有方法面临三个关键挑战：1) 固定权重融合策略在跨模态噪声和模糊查询中表现不佳；2) 时间建模难以捕获连贯的事件序列，并惩罚不切实际的时间间隔；3) 系统需要手动选择模态，降低了可用性。", "method": "本文提出了三个关键创新：1) 一个级联双嵌入管道，结合BEIT-3和SigLIP进行广泛检索，并通过BLIP-2进行重排序以平衡召回率和精度；2) 一个时间感知评分机制，通过束搜索对大时间间隔应用指数衰减惩罚，构建连贯的事件序列；3) 代理引导的查询分解（使用GPT-4o）自动解释模糊查询，分解为模态特定的子查询（视觉/OCR/ASR），并执行自适应分数融合，无需手动模态选择。", "result": "定性分析表明，本文系统能有效处理模糊查询，检索时间上连贯的序列，并动态调整融合策略。", "conclusion": "该系统在处理模糊查询、检索时间连贯序列和动态调整融合策略方面表现出色，提升了交互式瞬间搜索能力。"}}
{"id": "2512.12459", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.12459", "abs": "https://arxiv.org/abs/2512.12459", "authors": ["Jiachen Tao", "Benjamin Planche", "Van Nguyen Nguyen", "Junyi Wu", "Yuchun Liu", "Haoxuan Wang", "Zhongpai Gao", "Gengyu Zhang", "Meng Zheng", "Feiran Wang", "Anwesa Choudhuri", "Zhenghao Zhao", "Weitai Kang", "Terrence Chen", "Yan Yan", "Ziyan Wu"], "title": "From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields", "comment": null, "summary": "Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.", "AI": {"tldr": "本文提出高斯光子场（GPF），一种可学习的连续辐射场表示，用于加速多视角渲染中复杂光传输的计算，通过将光子分布编码为3D高斯基元，实现光子级精度和显著的计算效率提升。", "motivation": "传统的Photon Mapping在渲染同一场景的多个视角时，因每个视角独立进行光子追踪和核估计，导致计算效率低下且存在大量冗余计算。", "method": "将Photon Mapping重新表述为连续可复用的辐射函数。引入高斯光子场（GPF），它是一种可学习的表示，将光子分布编码为各向异性的3D高斯基元。GPF从首次SPPM迭代中物理追踪的光子初始化，并使用最终辐射的多视角监督进行优化，从而将基于光子的光传输蒸馏为连续场。训练完成后，该场能够沿相机射线进行可微分的辐射评估，无需重复光子追踪或迭代细化。", "result": "在包含焦散和镜面-漫反射交互等复杂光传输的场景中，GPF在达到光子级精度的同时，将计算量降低了几个数量级。它成功地将基于光子渲染的物理严谨性与神经场景表示的效率结合起来。", "conclusion": "高斯光子场（GPF）通过将光子映射中的光传输建模为可学习的连续辐射场，有效解决了多视角渲染的效率问题，实现了物理精确性与计算效率的统一，特别适用于处理复杂的光传输效果。"}}
{"id": "2512.13078", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13078", "abs": "https://arxiv.org/abs/2512.13078", "authors": ["Mohaiminul Islam Bhuiyan", "Chan Hue Wah", "Nur Shazwani Kamarudin", "Nur Hafieza Ismail", "Ahmad Fakhri Ab Nasir"], "title": "Heart Disease Prediction using Case Based Reasoning (CBR)", "comment": "Published in Journal of Theoretical and Applied Information Technology on 31st October 2024. Vol.102. No. 20", "summary": "This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.", "AI": {"tldr": "本研究概述了使用智能系统预测心脏病的方法，通过比较模糊逻辑、神经网络和案例推理(CBR)，最终选择CBR，并在心脏病数据集上实现了97.95%的预测准确率。", "motivation": "在医疗领域中，准确预测疾病至关重要。传统方法仅依赖医生经验，往往缺乏精确性，因此需要引入智能系统作为替代方案来提高预测准确性。", "method": "研究首先回顾了智能系统在心脏病预测中的应用，并重点比较了模糊逻辑、神经网络和案例推理(CBR)三种技术。经过比较，选择了CBR作为最终的预测系统。在预测阶段，对心脏病数据集进行了数据预处理（数据清洗）和数据拆分（训练集和测试集），然后使用选定的CBR系统进行心脏病结果预测。", "result": "实验结果显示，案例推理(CBR)在心脏病预测中达到了97.95%的显著准确率。研究还发现，男性患心脏病的概率为57.76%，女性为42.24%。此外，相关研究表明吸烟和饮酒是导致心脏病的重要因素，尤其对男性影响更大。", "conclusion": "本研究证明了智能系统，特别是案例推理(CBR)，在心脏病预测方面具有高准确性（97.95%）和有效性。研究结果还突出了性别在心脏病发病率中的差异，并强调了吸烟和饮酒等生活方式因素对心脏病，特别是男性心脏病的显著贡献。"}}
{"id": "2512.13674", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13674", "abs": "https://arxiv.org/abs/2512.13674", "authors": ["Yiyi Cai", "Xuangeng Chu", "Xiwei Gao", "Sitong Gong", "Yifei Huang", "Caixin Kang", "Kunhang Li", "Haiyang Liu", "Ruicong Liu", "Yun Liu", "Dianwen Ng", "Zixiong Su", "Erwin Wu", "Yuhan Wu", "Dingkun Yan", "Tianyu Yan", "Chang Zeng", "Bo Zheng", "You Zhou"], "title": "Towards Interactive Intelligence for Digital Humans", "comment": null, "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.", "AI": {"tldr": "本文提出了一种名为“交互式智能”的新型数字人范式，该范式能够实现个性化表达、自适应交互和自我演化。为实现此目标，作者介绍了Mio框架，并建立了一个新的基准来评估其能力，实验结果表明其性能优于现有技术。", "motivation": "现有数字人仍停留在表面模仿阶段，缺乏智能交互能力。研究旨在将数字人从肤浅模仿推向智能交互，实现更深层次的个性化、自适应和自演化能力。", "method": "引入“交互式智能”范式，并提出了Mio（多模态交互全能化身）端到端框架。Mio由五个专用模块组成：思考者、说话者、面部动画师、身体动画师和渲染器。该统一架构将认知推理与实时多模态具身化相结合，以实现流畅、一致的交互。此外，还建立了一个新的基准来严格评估交互式智能的能力。", "result": "广泛的实验证明，Mio框架在所有评估维度上都比现有最先进的方法取得了卓越的性能。", "conclusion": "本文的贡献将数字人从表面模仿推向了智能交互，为数字人的发展开辟了新方向。"}}
{"id": "2512.12487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12487", "abs": "https://arxiv.org/abs/2512.12487", "authors": ["Hoang Anh Just", "Yifei Fan", "Handong Zhao", "Jiuxiang Gu", "Ruiyi Zhang", "Simon Jenni", "Kushal Kafle", "Ruoxi Jia", "Jing Shi"], "title": "More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.", "AI": {"tldr": "RLVR训练的视觉语言模型在视觉提取和逻辑推理上存在不足。本文提出PeRL-VL框架，通过解耦方式分别提升感知（引入描述奖励）和推理（文本推理SFT），显著提高了多模态基准测试的准确性。", "motivation": "现有的可验证奖励强化学习（RLVR）在视觉语言模型（VLM）中扩展后，仍存在两个主要失败模式：不准确的视觉提取（遗漏或幻觉细节）和逻辑不一致的思维链，这主要是因为可验证信号只监督最终答案。", "method": "PeRL-VL（感知与推理学习）是一个解耦框架，在RLVR基础上独立改进视觉感知和文本推理。对于感知，PeRL-VL引入了基于VLM的描述奖励，用于评估模型自身生成的图像描述的忠实度和充分性。对于推理，PeRL-VL增加了一个仅限文本的推理SFT阶段，使用富含逻辑的思维链数据，独立于视觉增强连贯性和逻辑一致性。", "result": "在各种多模态基准测试中，PeRL-VL将平均Pass@1准确率从63.3%（基础Qwen2.5-VL-7B）提高到68.8%，优于标准RLVR、仅限文本的推理SFT以及来自GPT-4o的朴素多模态蒸馏方法。", "conclusion": "PeRL-VL通过解耦并分别改进视觉感知和文本推理，成功解决了RLVR训练的VLM在视觉提取和逻辑一致性方面的持久性问题，从而显著提升了模型的整体性能和准确性。"}}
{"id": "2512.12936", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12936", "abs": "https://arxiv.org/abs/2512.12936", "authors": ["Tiange Zhang", "Xiandong Meng", "Siwei Ma"], "title": "Content Adaptive based Motion Alignment Framework for Learned Video Compression", "comment": "Accepted to Data Compression Conference (DCC) 2026 as a poster paper", "summary": "Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.", "AI": {"tldr": "本文提出了一种内容自适应运动对齐框架CAMA，通过两阶段流引导可变形扭曲、多参考质量感知策略和无训练模块，显著提升了端到端视频压缩性能。", "motivation": "现有的端到端视频压缩框架缺乏内容特异性适应能力，导致压缩性能不佳。", "method": "1. 引入两阶段流引导可变形扭曲机制，通过粗到细的偏移预测和掩码调制，实现精确的特征对齐。2. 提出多参考质量感知策略，根据参考帧质量调整失真权重，并应用于分层训练以减少错误传播。3. 集成无训练模块，根据运动幅度和分辨率对帧进行下采样，以获得平滑的运动估计。", "result": "在标准测试数据集上，CAMA框架相比基线模型DCVC-TCM实现了24.95%的BD-rate（PSNR）节省，并且优于DCVC-DC和传统编解码器HM-16.25。", "conclusion": "所提出的内容自适应运动对齐框架CAMA通过适应不同的内容特性，显著提高了视频压缩性能。"}}
{"id": "2512.12824", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12824", "abs": "https://arxiv.org/abs/2512.12824", "authors": ["N. K. B. M. P. K. B. Narasinghe", "Uthayasanker Thayasivam"], "title": "Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners", "comment": "9 pages, 3 figures. Accepted to VISAPP 2026", "summary": "Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an \"augmentation divergence\": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.", "AI": {"tldr": "本文研究了如何将生成-对比式基础模型（如CoCa）高效地适应于少样本图像分类任务。我们发现了“增强分歧”现象，并证明了结合监督对比损失的混合目标能持续提升性能，为在数据稀缺环境下调整模型提供了经验性指导。", "motivation": "大规模多模态基础模型（特别是CoCa）在零样本迁移方面表现出色，但其在数据极度稀缺的下游任务（少样本学习）中的适应性尚未得到充分探索。现有研究主要集中在双编码器架构（如CLIP），而CoCa独特的潜在空间如何响应参数高效微调（PEFT）仍是一个空白。", "method": "本文对CoCa视觉骨干在少样本图像分类中的适应性进行了全面的实证研究。我们系统评估了一系列策略，从免训练的混合原型设计到通过低秩适应（LoRA）进行的深度参数适应。研究了数据增强的影响，并比较了包含监督对比（SupCon）损失的混合目标与标准交叉熵损失的性能。", "result": "研究发现了一个“增强分歧”现象：强数据增强会降低低样本设置下线性探测的性能，但对于稳定LoRA微调至关重要。此外，结合监督对比（SupCon）损失的混合目标在不同样本数量下均比标准交叉熵持续提升性能。我们还刻画了训练配置对数据稀缺性的敏感度，为缩放正则化、秩和采样策略提供了经验性参考设置。", "conclusion": "本文为生成-对比式基础模型在少样本学习中的高效适应提供了经验性参考设置，特别强调了数据增强的复杂作用和混合目标（如SupCon）的优势。这些发现有助于指导在数据稀缺场景下对这类模型进行微调。"}}
{"id": "2512.12997", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12997", "abs": "https://arxiv.org/abs/2512.12997", "authors": ["Wenjing lu", "Zerui Tao", "Dongping Zhang", "Yuning Qiu", "Yang Yang", "Qibin Zhao"], "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP", "comment": null, "summary": "CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.", "AI": {"tldr": "针对CLIP在对抗攻击下预测准确性下降且不确定性被抑制的问题，本文提出一种新的对抗微调方法，通过将CLIP输出重参数化为狄利克雷分布的集中参数，以统一表示语义结构和预测置信度，从而在扰动下整体对齐分布，有效恢复校准的不确定性并提升对抗鲁棒性。", "motivation": "CLIP在零样本分类上表现出色，但极易受到对抗攻击。现有对抗微调方法主要关注匹配干净和对抗样本的预测logits，忽略了不确定性校准，可能损害零样本泛化能力。此外，对抗扰动不仅降低准确性，还会抑制不确定性，导致严重的错误校准和不可靠的过度自信，这揭示了一个超越鲁棒性的关键可靠性差距。", "method": "提出一种新颖的对抗微调目标，同时考虑预测准确性和不确定性对齐。通过将CLIP的输出重参数化为狄利克雷分布的集中参数，构建了一个统一表示，既能捕捉相对语义结构，又能体现预测置信度的大小。该目标在扰动下整体对齐这些分布，超越了单一logits锚定，旨在恢复校准的不确定性。", "result": "在多个零样本分类基准上的实验表明，所提出的方法有效地恢复了校准的不确定性，并在保持干净准确性的同时，实现了具有竞争力的对抗鲁棒性。", "conclusion": "该研究成功地通过新的对抗微调目标解决了CLIP在对抗攻击下预测准确性下降和不确定性被抑制的问题，有效恢复了校准的不确定性，并提升了对抗鲁棒性，同时保持了原始模型的性能。"}}
{"id": "2512.12534", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12534", "abs": "https://arxiv.org/abs/2512.12534", "authors": ["Qi Sun", "Can Wang", "Jiaxiang Shang", "Wensen Feng", "Jing Liao"], "title": "Animus3D: Text-driven 3D Animation via Motion Score Distillation", "comment": "SIGGRAPH Asia 2025", "summary": "We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.", "AI": {"tldr": "Animus3D是一个文本驱动的3D动画框架，它通过引入Motion Score Distillation (MSD)替代传统SDS，并结合LoRA增强视频扩散模型、反演噪声估计、时空正则化和运动细化模块，为静态3D资产生成更生动、细节更丰富的动画。", "motivation": "以往的方法主要利用香草分数蒸馏采样（SDS）从预训练的文本到视频扩散模型中提取运动，导致动画动作微弱或存在明显的抖动。", "method": "该方法引入了一种新颖的SDS替代方案——运动分数蒸馏（MSD）。具体来说，它引入了一个LoRA增强的视频扩散模型，该模型定义了一个静态源分布而非纯噪声。同时，一种基于反演的噪声估计技术确保了在引导运动时的外观保持。为进一步提高运动保真度，还结合了显式的时空正则化项以减轻几何畸变。此外，提出了一个运动细化模块来提高时间分辨率并增强精细细节。", "result": "广泛的实验表明，Animus3D能够成功地从各种文本提示中动画化静态3D资产，与最先进的基线相比，生成了更实质、更详细的运动，同时保持了高视觉完整性。", "conclusion": "Animus3D通过其创新的MSD方法、LoRA增强的视频扩散、反演噪声估计、时空正则化和运动细化模块，有效解决了现有文本驱动3D动画方法的局限性，显著提高了动画的运动丰富度和细节表现，同时保持了视觉质量。"}}
{"id": "2512.13089", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13089", "abs": "https://arxiv.org/abs/2512.13089", "authors": ["Ziqiang Zhu", "Bowei Yang"], "title": "UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era", "comment": "10 pages, 6 figures", "summary": "Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.", "AI": {"tldr": "本文提出UniVCD，一种基于冻结的SAM2和CLIP的无监督、开放词汇变化检测方法，通过轻量级特征对齐模块和后处理，实现了无需标注数据即可对多样场景进行高精度、语义感知的变化检测。", "motivation": "现有变化检测方法多依赖监督学习，导致性能高度依赖数据集、标注成本高昂，且难以泛化到多样场景和未预定义类别。视觉基础模型（如SAM2和CLIP）的兴起为解决这些限制提供了新机遇。", "method": "UniVCD基于冻结的SAM2（提供空间细节表示）和CLIP（提供语义先验）构建。引入了一个轻量级特征对齐模块来融合这两种模型的信息，以实现高分辨率、语义感知的变化估计，同时保持可训练参数量小。此外，还引入了一个简化的后处理流程来抑制噪声和伪变化，提高具有明确边界的物体的检测精度。", "result": "在多个公共的二元变化检测（BCD）和语义变化检测（SCD）基准测试上，UniVCD展现出持续强劲的性能，在F1和IoU等关键指标上达到或超越了现有开放词汇变化检测方法。", "conclusion": "研究结果表明，利用冻结的视觉基础模型和轻量级多模态对齐实现的无监督变化检测，是开放词汇变化检测领域一种实用且有效的新范式。"}}
{"id": "2512.12539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12539", "abs": "https://arxiv.org/abs/2512.12539", "authors": ["Huan Huang", "Michele Esposito", "Chen Zhao"], "title": "Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling", "comment": "6 figures", "summary": "Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.", "AI": {"tldr": "本文提出了一种新的冠状动脉分割框架，通过整合心肌解剖先验知识、结构感知特征编码和三维小波逆小波变换，实现了更稳定和一致的冠状动脉分割。", "motivation": "冠状动脉CT血管造影中准确的冠状动脉分割对定量分析和临床决策至关重要，但由于血管细小、分支复杂、边界模糊和心肌干扰，可靠分割仍具挑战性。", "method": "所提出的框架集成了心肌解剖先验、结构感知特征编码和三维小波逆小波变换。编码阶段融入心肌先验和残差注意力机制以增强冠状结构表示；基于小波逆小波的下采样和上采样实现了空频联合建模并保持多尺度结构一致性；解码阶段通过多尺度特征融合模块整合语义和几何信息。模型在ImageCAS数据集上采用3D重叠块策略进行训练和评估。", "result": "该方法在ImageCAS数据集上取得了0.8082的Dice系数、0.7946的敏感性、0.8471的精确度和9.77毫米的HD95，优于多个主流分割模型。消融研究也证实了各组件的互补贡献。", "conclusion": "所提出的方法能够在复杂的几何条件下实现更稳定和一致的冠状动脉分割，为后续的冠状结构分析任务提供了可靠的分割结果。"}}
{"id": "2512.13043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13043", "abs": "https://arxiv.org/abs/2512.13043", "authors": ["Tong Wei", "Yijun Yang", "Changhao Zhang", "Junliang Xing", "Yuanchun Shi", "Zongqing Lu", "Deheng Ye"], "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "comment": null, "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "AI": {"tldr": "GTR-Turbo提出了一种高效的多模态强化学习方法，通过合并训练中的检查点权重作为“免费”教师模型，解决了稀疏奖励和昂贵教师模型的问题，显著提升了性能并降低了训练成本和时间。", "motivation": "多模态智能体（基于VLM）的多轮强化学习面临奖励稀疏和长期信用分配困难。现有方法（如GTR、On-Policy Distillation）通过查询昂贵且通常是特权模型来提供步级反馈，这限制了实用性和可复现性。", "method": "GTR-Turbo通过合并当前强化学习训练过程中产生的检查点权重，构建一个“免费”的教师模型。然后，这个合并模型通过监督微调或软对数蒸馏来指导后续的强化学习训练。", "result": "GTR-Turbo在不训练或查询昂贵教师模型的情况下，达到了与GTR相当的性能。在多种视觉智能体任务中，它将基线模型的准确率提高了10-30%，同时相对于GTR，将实际训练时间减少了50%，计算成本降低了60%。它还缓解了之前工作中观察到的“熵坍塌”问题，并保持了训练的稳定性。", "conclusion": "GTR-Turbo提供了一种高效且实用的多模态强化学习升级方案，通过利用训练过程中的模型权重作为免费教师，消除了对昂贵特权VLM的依赖，实现了性能提升、成本降低和训练稳定性，提高了方法的实用性和可复现性。"}}
{"id": "2512.12549", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12549", "abs": "https://arxiv.org/abs/2512.12549", "authors": ["Shaif Chowdhury", "Mushfika Rahman", "Greg Hamerly"], "title": "Supervised Contrastive Frame Aggregation for Video Representation Learning", "comment": "12 pages", "summary": "We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.", "AI": {"tldr": "该论文提出了一种监督对比学习框架（Supervised Contrastive Frame Aggregation），通过将视频帧聚合为单一图像并利用预训练CNN骨干网络，学习高效的视频表示，并在分类任务上超越现有方法，同时降低计算成本。", "motivation": "动机在于改进视频表示学习，有效利用时间全局上下文，同时避免复杂视频Transformer模型的计算开销，并能利用预训练的卷积神经网络骨干。", "method": "该方法引入了一种视频到图像的聚合策略，将来自同一视频的多个帧空间排列成一个单一的输入图像，从而可以使用ResNet50等预训练CNN骨干。接着，设计了一个对比学习目标，直接比较模型生成的成对投影：具有相同标签的视频投影定义为正样本对，其他投影为负样本。通过对同一视频进行不同的时间帧采样来创建多个自然视图，以生成具有全局上下文的多样化正样本，减少过拟合。", "result": "实验结果表明，该方法在Penn Action和HMDB51数据集上的分类准确率优于现有方法，且所需的计算资源更少。例如，在Penn Action上达到76%的分类准确率（ViVIT为43%），在HMDB51上达到48%的准确率（ViVIT为37%）。该方法在监督和自监督设置下都能学习有效的视频表示，并支持分类和字幕生成等视频任务。", "conclusion": "所提出的监督对比帧聚合方法（Supervised Contrastive Frame Aggregation）能够有效且高效地学习视频表示，在视频分类等任务上表现出色，并且计算资源需求较低，适用于多种视频相关应用。"}}
{"id": "2512.13164", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13164", "abs": "https://arxiv.org/abs/2512.13164", "authors": ["Xianchao Guan", "Zhiyuan Fan", "Yifeng Wang", "Fuqiang Chen", "Yanjiang Zhou", "Zengyang Che", "Hongxue Meng", "Xin Li", "Yaowei Wang", "Hongpeng Wang", "Min Zhang", "Heng Tao Shen", "Zheng Zhang", "Yongbing Zhang"], "title": "A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis", "comment": "67 pages, 9 figures, 16 tables", "summary": "The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.", "AI": {"tldr": "CRAFTS是首个病理学专用文本到图像生成基础模型，通过引入相关性调节对齐框架，解决了病理AI数据稀缺问题，并生成高质量、生物学准确的病理图像，显著提升了多种临床任务的AI性能。", "motivation": "临床级病理AI的发展受限于多样化、高质量标注数据集的稀缺。现有生成模型存在语义不稳定和形态学幻觉问题，影响诊断可靠性。", "method": "引入CRAFTS（Correlation-Regulated Alignment Framework for Tissue Synthesis），一个病理学专用的文本到图像生成基础模型。采用双阶段训练策略，基于约280万图像-标题对进行训练，并结合一种新颖的对齐机制来抑制语义漂移，确保生物学准确性。该模型还可与ControlNet结合，实现对组织结构的精确控制。", "result": "CRAFTS模型能够生成涵盖30种癌症类型的多样化病理图像，其质量通过客观指标和病理学家评估得到严格验证。CRAFTS增强的数据集显著提升了分类、跨模态检索、自监督学习和视觉问答等多种临床任务的性能。此外，结合ControlNet可以根据核分割掩膜和荧光图像等输入，精确控制组织结构。", "conclusion": "CRAFTS通过克服数据稀缺和隐私担忧的关键障碍，提供了无限的多样化、带注释的组织学数据来源，有效地解锁了针对罕见和复杂癌症表型创建强大诊断工具的可能性。"}}
{"id": "2512.13107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13107", "abs": "https://arxiv.org/abs/2512.13107", "authors": ["Zhijian He", "Feifei Liu", "Yuwei Li", "Zhanpeng Liu", "Jintao Cheng", "Xieyuanli Chen", "Xiaoyu Tang"], "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather", "comment": null, "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.", "AI": {"tldr": "DiffFusion是一个新颖的框架，通过扩散模型进行数据恢复和自适应跨模态融合，显著提升了多模态3D目标检测在恶劣天气下的鲁棒性。", "motivation": "多模态3D目标检测在机器人和自动驾驶中至关重要，但在恶劣天气下，由于天气引起的失真和不同数据模态之间的错位，其有效性受到限制。", "method": "DiffFusion框架包含：1) Diffusion-IR，使用扩散模型恢复受天气影响的图像；2) 点云恢复（PCR），利用图像目标线索补偿损坏的激光雷达数据；3) 双向自适应融合与对齐模块（BAFAM），实现动态多模态融合和双向鸟瞰图（BEV）对齐，以保持空间对应性。", "result": "在三个公共数据集上，DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时保持了在干净数据上的强大性能。在真实世界的DENSE数据集上的零样本结果进一步验证了其泛化能力。", "conclusion": "DiffFusion通过扩散模型的数据恢复和自适应跨模态融合，有效解决了恶劣天气下多模态3D目标检测的挑战，显著提升了系统的鲁棒性和泛化能力。"}}
{"id": "2512.12571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12571", "abs": "https://arxiv.org/abs/2512.12571", "authors": ["Boyeong Im", "Wooseok Lee", "Yoojin Kwon", "Hyung-Sin Kim"], "title": "From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models", "comment": null, "summary": "To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.", "AI": {"tldr": "该论文提出了MVP框架，通过将相机曝光参数（ISO、快门速度、光圈）视为“物理提示”，将测试时间适应（TTA）从数字层面扩展到物理层面，从而提高视觉-语言模型（VLMs）在传感器介导的物理环境中的鲁棒性。", "motivation": "现有的视觉-语言模型（VLMs）主要应用于网络图像，但在传感器介导的物理环境中表现不佳。研究旨在将VLMs的应用范围扩展到这些更复杂的物理环境，提高其鲁棒性。", "method": "MVP是一个前向框架，它在推理时为每个场景获取一系列物理视图，使用源亲和性分数选择表现最佳的k个传感器设置，对保留的视图进行轻量级数字增强，过滤出最低熵的增强视图子集，并通过零温度softmax（硬投票）聚合预测。该方法无需梯度或模型修改，设计简单且易于校准。", "result": "在ImageNet-ES和ImageNet-ES-Diverse数据集上，MVP始终优于仅使用数字TTA的方法，单次自动曝光捕获的性能提升高达25.6个百分点。与结合传统传感器控制和TTA的方法相比，MVP还额外获得了高达3.4个百分点的增益。即使在减少参数候选集以降低捕获延迟的情况下，MVP仍然有效，证明了其实用性。", "conclusion": "研究结果表明，超越捕获后提示，测量时控制（选择和组合真实的物理视图）能够显著提高视觉-语言模型的鲁棒性。"}}
{"id": "2512.12604", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12604", "abs": "https://arxiv.org/abs/2512.12604", "authors": ["Tingyan Wen", "Haoyu Li", "Yihuang Chen", "Xing Zhou", "Lifei Zhu", "Xueqian Wang"], "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching", "comment": "Project page: https://thu-accdiff.github.io/xslim-page/ Code: https://github.com/THU-AccDiff/xslim", "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.", "AI": {"tldr": "X-Slim是一种免训练的缓存加速器，通过统一利用时间步、结构块和空间token之间的冗余，显著加速扩散模型，同时保持高质量。", "motivation": "扩散模型生成质量高，但计算开销巨大，受步数、模型深度和序列长度影响。现有特征缓存方法在加速和保真度之间存在固有限制，需要在不同粒度上更有效地利用可缓存冗余。", "method": "X-Slim（eXtreme-Slimming Caching）是一种免训练的、基于缓存的加速器，首次统一利用时间步、结构（块）和空间（token）上的可缓存冗余。它引入了一个双阈值控制器，将缓存过程分为“先推后精修”：首先将时间步级别的重用推到“预警线”，然后切换到轻量级的块和token级别的刷新来精修剩余冗余，一旦超过“临界线”则触发完整推理以重置累积误差。在每个级别，上下文感知指示器决定何时何地进行缓存。", "result": "X-Slim显著提升了速度-质量前沿。在FLUX.1-dev和HunyuanVideo上，它将延迟分别降低了高达4.97倍和3.52倍，且感知损失极小。在DiT-XL/2上，它实现了3.13倍的加速，并使FID比现有方法提高了2.42。", "conclusion": "X-Slim通过其统一的缓存框架和双阈值控制器，有效解决了扩散模型的计算开销问题，实现了显著的加速，同时保持甚至改善了生成质量，超越了现有方法。"}}
{"id": "2512.12598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12598", "abs": "https://arxiv.org/abs/2512.12598", "authors": ["Cong Xie", "Che Wang", "Yan Zhang", "Zheng Pan", "Han Zou", "Zhenpeng Zhan"], "title": "Geometry-Aware Scene-Consistent Image Generation", "comment": null, "summary": "We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.", "AI": {"tldr": "本文提出了一种几何感知场景一致性图像生成方法，通过构建场景一致性数据集和引入几何引导注意力损失，有效平衡了场景保持和文本提示遵循之间的矛盾。", "motivation": "现有方法在几何感知场景一致性图像生成任务中，难以平衡场景保持和文本提示遵循：要么忠实复制场景但对提示响应差，要么优先遵循提示但牺牲场景一致性。这种权衡是一个亟待解决的问题。", "method": "本文提出了两项关键贡献：(i) 一个场景一致性数据构建流程，用于生成多样化、几何基础的训练对；(ii) 一种新颖的几何引导注意力损失，利用跨视角线索来规范模型的空间推理能力。", "result": "在自建的场景一致性基准测试中，本文方法在自动指标和人类偏好研究方面均优于现有基线，实现了更好的场景对齐和文本-图像一致性。该方法生成了几何连贯、构图多样且忠实于文本指令和底层场景结构的图像。", "conclusion": "本文方法成功解决了几何感知场景一致性图像生成中场景保持和提示遵循的权衡问题，能够生成几何连贯、多样化且忠实于文本指令和场景结构的图像。"}}
{"id": "2512.13101", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13101", "abs": "https://arxiv.org/abs/2512.13101", "authors": ["Wenjing Lu", "Yi Hong", "Yang Yang"], "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation", "comment": "This work has been submitted to the IEEE TMI for possible publication", "summary": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.", "AI": {"tldr": "本文提出不确定性感知协同学习（UnCoL），一个双教师框架，用于半监督医学图像分割，旨在融合基础模型的泛化能力和任务特异性，并通过预测不确定性自适应地调节伪标签学习，从而在有限标注下实现接近全监督的性能。", "motivation": "视觉基础模型在医学图像分割中表现出强大的泛化能力，但在有限标注或罕见病理变异的专业临床任务中，由于通用先验与任务特定要求不匹配，其泛化能力受限。", "method": "本文提出不确定性感知协同学习（UnCoL），一个双教师框架。它从冻结的基础模型中提取视觉和语义表示以传递通用知识，同时维护一个渐进式适应的教师模型以捕获细粒度、任务特异性表示。UnCoL中的伪标签学习通过预测不确定性进行自适应调节，选择性抑制不可靠的监督并稳定模糊区域的学习。", "result": "在多种2D和3D分割基准测试中，UnCoL始终优于最先进的半监督方法和基础模型基线。此外，该模型在显著减少标注需求的情况下，实现了接近全监督的性能。", "conclusion": "UnCoL成功地将泛化和专业化能力融合到半监督医学图像分割中，有效解决了基础模型在专业任务中的局限性，并大幅减少了标注需求。"}}
{"id": "2512.12595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12595", "abs": "https://arxiv.org/abs/2512.12595", "authors": ["Karthikeya KV"], "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation", "comment": null, "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.", "AI": {"tldr": "本研究提出一个融合视觉增强型大语言模型（LLM）与Transformer架构的框架，通过整流流机制、双向分词和噪声感知学习，实现了高分辨率图像合成和多模态数据解释的显著提升，并在基准测试中展现出卓越的性能和效率。", "motivation": "为了解决高分辨率图像合成和多模态数据解释中的现有挑战。", "method": "该研究引入了一个整合视觉增强型LLM与先进Transformer架构的框架。它采用整流流机制连接噪声与数据，实现高效高质量生成；使用双向分词策略无缝融合文本、图像和视频模态输入；通过嵌入时空特征和混合文本-图像序列建模实现统一理解；并利用噪声感知学习算法优化架构，以应对噪声数据分布。", "result": "该模型在合成图像中实现了无与伦比的保真度，并获得了连贯的多模态表示。与基于扩散的方法相比，图像分辨率清晰度提高了25%，计算要求降低了20%。此外，模型还展现出强大的可扩展性和适应性。", "conclusion": "这项工作强调了以视觉为中心的LLM在重新定义计算机视觉和多模态人工智能能力方面的关键作用，在自动系统、创意内容生成和高级视频分析等领域具有巨大潜力。"}}
{"id": "2512.13122", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13122", "abs": "https://arxiv.org/abs/2512.13122", "authors": ["Vivek Alumootil", "Tuan-Anh Vu", "M. Khalid Jawed"], "title": "DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass", "comment": "This is a work in progress", "summary": "Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R", "AI": {"tldr": "DePT3R是一种新颖的框架，能够从多张未标定姿态的图像中，在一次前向传播中同时进行动态场景的密集三维点跟踪和三维重建。", "motivation": "现有动态场景密集三维点跟踪方法依赖于成对处理、已知相机姿态或假设帧的时间顺序，限制了其灵活性和适用性。同时，从大规模未标定图像集合中进行高效三维重建的最新进展，为统一的动态场景理解方法提供了机会。", "method": "DePT3R通过一个强大的骨干网络提取深度时空特征，并使用密集预测头回归像素级映射，实现多任务学习。它无需相机姿态，显著增强了适应性和效率。", "result": "DePT3R在多个具有挑战性的动态场景基准上表现出强大的性能，并比现有最先进方法显著提高了内存效率。", "conclusion": "DePT3R提供了一个无需相机姿态的、高效且适应性强的解决方案，用于动态场景的密集三维点跟踪和三维重建。"}}
{"id": "2512.12586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12586", "abs": "https://arxiv.org/abs/2512.12586", "authors": ["Lixin Chen", "Chaomeng Chen", "Jiale Zhou", "Zhijian Wu", "Xun Lin"], "title": "StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis", "comment": "13 pages, 10 figures. This is the extended version of the paper accepted at AAAI 2026, including related works and appendix", "summary": "Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.", "AI": {"tldr": "StegaVAR是一个新颖的框架，通过将动作视频嵌入到普通掩护视频中，并在隐写域中直接进行视频动作识别（VAR），解决了现有隐私保护方法在隐蔽性和时空特征破坏方面的缺陷。", "motivation": "尽管深度学习在视频动作识别（VAR）方面取得了快速进展，但视频中的隐私泄露仍然是一个关键问题。现有的隐私保护方法依赖匿名化，但存在两个主要问题：1) 隐蔽性低，即产生视觉失真的视频容易引起攻击者注意；2) 时空扰动，即破坏了精确VAR所需的关键时空特征。", "method": "本文提出了StegaVAR框架，首次将动作视频嵌入到普通掩护视频中，并直接在隐写域中执行VAR。为了应对隐写域分析的挑战，提出了两种新机制：1) 秘密时空促进（STeP），在训练期间利用秘密视频指导隐写域中的时空特征提取；2) 跨频带差异注意力（CroDA），通过捕获跨频带语义差异来抑制掩护视频的干扰。", "result": "实验结果表明，StegaVAR在广泛使用的数据集上实现了卓越的VAR性能和隐私保护性能。此外，该框架对多种隐写模型都有效。", "conclusion": "StegaVAR通过在隐写域中进行动作识别，在数据传输和动作分析过程中保持了隐藏秘密视频的完整时空信息，同时通过掩护视频的自然外观确保了传输的隐蔽性，有效解决了现有隐私保护VAR方法的痛点。"}}
{"id": "2512.13157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13157", "abs": "https://arxiv.org/abs/2512.13157", "authors": ["Peter Kocsis", "Lukas Höllein", "Matthias Nießner"], "title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction", "comment": "Project page: https://peter-kocsis.github.io/IntrinsicImageFusion/ Video: https://www.youtube.com/watch?v=-Vs3tR1Xl7k", "summary": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.", "AI": {"tldr": "该论文提出了一种名为“本征图像融合”的方法，通过结合单视图先验和鲁棒的多视图优化，从多视图图像中重建高质量的物理材质，并利用逆路径追踪进行参数优化。", "motivation": "材质重建是一个高度欠约束的问题，通常依赖于分析-合成方法（如路径追踪），但这种方法昂贵且噪声大。研究动机在于引入单视图先验来更好地约束优化过程，并解决单视图估计中常见的不一致性问题。", "method": "该方法首先利用基于扩散的材质估计器为每个视图生成多个（但不一致的）候选分解。为了减少不一致性，它将预测拟合到一个显式的低维参数函数。然后，提出一个鲁棒的优化框架，通过软化的每视图预测选择和基于置信度的软多视图内点集，将最自信视图的最一致预测融合到一致的参数材质空间中。最后，使用逆路径追踪优化这些低维参数。", "result": "该方法在合成和真实场景的材质分解方面均优于现有最先进的方法，生成了清晰、干净的重建结果，适用于高质量的重新照明。", "conclusion": "本征图像融合方法通过有效结合基于扩散的单视图先验和鲁棒的多视图优化，成功地从多视图图像中重建了高质量的物理材质，显著提升了材质分解的性能，并为高质量的重新照明提供了适用性。"}}
{"id": "2512.12610", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12610", "abs": "https://arxiv.org/abs/2512.12610", "authors": ["Wonseok Choi", "Sohwi Lim", "Nam Hyeon-Woo", "Moon Ye-Bin", "Dong-Ju Jeong", "Jinyoung Hwang", "Tae-Hyun Oh"], "title": "Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching", "comment": "WACV 2026", "summary": "Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/", "AI": {"tldr": "本文提出了Patchify，一个简单而高效的补丁级图像检索框架，用于实例级图像检索，无需微调即可提供高性能、可扩展性和可解释性。它还引入了LocScore，一个定位感知度量来评估检索结果的空间准确性。", "motivation": "实例级图像检索任务面临挑战，因为同一对象在大小、位置或外观上可能存在巨大差异。研究旨在开发一种高性能、可扩展且可解释的检索方法，并能准确评估检索结果的空间正确性。", "method": "本文提出了Patchify框架，将每个数据库图像分割成少量结构化补丁，并通过比较这些局部特征与全局查询描述符进行检索。为了评估检索精度和空间正确性，引入了LocScore，一个量化检索区域是否与目标对象对齐的定位感知度量。此外，应用乘积量化（PQ）实现高效的大规模检索，并强调使用信息丰富的特征进行压缩的重要性。", "result": "Patchify在多个基准、骨干网络和区域选择策略上进行了广泛实验，结果表明它优于全局方法，并能补充最先进的重排序（reranking）管道。此外，通过在压缩过程中使用信息丰富的特征，乘积量化显著提升了大规模检索的性能。", "conclusion": "Patchify是一个简单而有效的补丁级检索框架，为实例级图像检索提供了高性能、可扩展性和可解释性，无需微调。引入的LocScore是理解和改进检索行为的宝贵诊断工具。该框架在效率和准确性方面表现出色，尤其适用于大规模检索。"}}
{"id": "2512.13317", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13317", "abs": "https://arxiv.org/abs/2512.13317", "authors": ["Mikhail Zakharov"], "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion", "comment": "12 pages, 1 figure, 5 tables, 10 equations. Preprint", "summary": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.", "AI": {"tldr": "本文研究了人脸识别系统中人脸身份遗忘的问题，旨在通过分散特定身份的嵌入来阻止其被检索，同时不影响其他身份的识别性能，并提出了一种有效的基于分散的遗忘方法。", "motivation": "人脸识别系统在提供准确检索的同时，也因其潜在的未经授权的身份跟踪能力而引发了严重的隐私担忧。尽管机器遗忘被认为是隐私保护的手段，但其在人脸检索（特别是现代基于嵌入的识别模型）中的适用性仍未得到充分探索。", "method": "研究了人脸身份遗忘在检索系统中的固有挑战。目标是使选定身份的嵌入在超球面上分散，阻止形成紧凑的身份簇，从而使其无法被重新识别。同时，需保持嵌入空间的判别结构和模型对剩余身份的检索性能。为此，评估了几种现有的近似类遗忘方法（如随机标签、梯度上升、边界遗忘等），并提出了一种简单而有效的基于分散的遗忘方法。", "result": "在标准基准数据集（VGGFace2、CelebA）上进行了大量实验，结果表明所提出的方法在实现卓越遗忘行为的同时，有效地保留了检索实用性。", "conclusion": "本文提出了一种简单而有效的基于分散的人脸身份遗忘方法，该方法在人脸检索系统中表现出优异的遗忘效果，同时保持了对其他身份的识别性能。"}}
{"id": "2512.12658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12658", "abs": "https://arxiv.org/abs/2512.12658", "authors": ["Qixin Xu", "Haozhe Wang", "Che Liu", "Fangzhen Lin", "Wenhu Chen"], "title": "CogDoc: Towards Unified thinking in Documents", "comment": null, "summary": "Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution \"Fast Reading\" phase for scalable information localization,followed by a high-resolution \"Focused Thinking\" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the \"policy conflict\" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.", "AI": {"tldr": "CogDoc提出了一种粗到细的统一思维框架，通过直接强化学习解决了长上下文多模态文档推理中的可扩展性与保真度权衡问题，其7B模型在视觉丰富文档基准上超越了大型专有模型。", "motivation": "当前的文档推理范式在处理长上下文文档的可扩展性与捕获细粒度多模态细节的保真度之间存在根本性权衡，无法有效兼顾。", "method": "本文提出了CogDoc，一个模仿人类认知过程的统一粗到细思维框架，包括用于信息定位的低分辨率“快速阅读”阶段和用于深度推理的高分辨率“专注思考”阶段。研究还深入探讨了该框架的后训练策略，发现直接强化学习（RL）方法优于结合监督微调（SFT）初始化的RL方法，因为它避免了SFT中观察到的“策略冲突”。", "result": "经验证明，直接RL方法优于SFT初始化的RL方法。CogDoc的7B模型在其参数级别中实现了最先进的性能，特别是在具有挑战性的、视觉丰富的文档基准测试中，显著超越了GPT-4o等更大的专有模型。", "conclusion": "CogDoc通过其粗到细的统一思维框架和直接强化学习策略，有效解决了文档推理中的可扩展性与保真度问题，并在视觉丰富的文档任务上取得了卓越的性能，即使是参数量较小的模型也能超越大型专有模型。"}}
{"id": "2512.13402", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13402", "abs": "https://arxiv.org/abs/2512.13402", "authors": ["Lorenzo Pettinari", "Sidaty El Hadramy", "Michael Wehrli", "Philippe C. Cattin", "Daniel Studer", "Carol C. Hasler", "Maria Licci"], "title": "End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery", "comment": "Code and interactive visualizations: https://lorenzopettinari.github.io/end-2-reg/", "summary": "Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.", "AI": {"tldr": "本文提出End2Reg，一个端到端深度学习框架，用于脊柱手术中无标记RGB-D配准。它联合优化分割和配准，无需弱分割标签和手动步骤，显著提高了配准精度，并推动全自动术中导航发展。", "motivation": "当前脊柱手术中的术中导航系统存在侵入性、辐射大和工作流程中断的问题。现有的无标记RGB-D配准方法依赖弱分割标签来隔离解剖结构，这可能导致误差传播。", "method": "本文提出End2Reg，一个端到端深度学习框架，它联合优化分割和配准。该网络在没有直接分割监督的情况下，仅由配准目标引导，学习专门为配准优化的分割掩码，从而消除了对弱分割标签和手动步骤的需求。", "result": "该框架在离体和体内基准测试中均取得了最先进的性能，将中位目标配准误差（TRE）降低32%至1.83毫米，将均方根误差（RMSE）降低45%至3.95毫米。消融研究证实，端到端优化显著提高了配准精度。", "conclusion": "所提出的端到端RGB-D配准流程消除了对弱标签和手动步骤的依赖，为实现全自动、无标记的术中导航迈出了重要一步。"}}
{"id": "2512.12657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12657", "abs": "https://arxiv.org/abs/2512.12657", "authors": ["Hongyang Li", "Junyi Tao", "Qijie Wei", "Ningzhi Yang", "Meng Wang", "Weihong Yu", "Xirong Li"], "title": "Cross-modal Fundus Image Registration under Large FoV Disparity", "comment": "Accepted as a regular paper at MMM 2026", "summary": "Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.", "AI": {"tldr": "本文提出CARe方法，用于解决视场角（FoV）差异较大的跨模态眼底图像配准（CMFIR）挑战，通过裁剪操作和双拟合对齐模块实现，并验证了其有效性。", "motivation": "现有跨模态眼底图像配准方法假定视场角差异较小，无法应对视场角差异较大的更具挑战性场景。直接应用这些方法在这种情况下会失败。", "method": "CARe方法包含两部分：1. 裁剪（Crop）操作：利用视网膜的生理结构，从宽视场彩色眼底照相（wfCFP）目标图像中裁剪出一个子图像，使其视场角与较小的光学相干断层扫描血管造影（OCTA）源图像大致对齐，从而使现有小FoV差异方法得以复用。2. 对齐（Alignment）模块：通过双拟合改进空间变换，该模块顺序利用经典的RANSAC算法和基于多项式的坐标拟合。", "result": "在包含60对OCTA-wfCFP图像的新开发测试集上进行了广泛实验，验证了CARe方法在跨模态眼底图像配准中的可行性。", "conclusion": "CARe是一种简单而有效的跨模态眼底图像配准方法，特别适用于处理视场角差异较大的挑战性场景，并且其可行性得到了验证。"}}
{"id": "2512.12664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12664", "abs": "https://arxiv.org/abs/2512.12664", "authors": ["Sreehari Rajan", "Kunal Bhosikar", "Charu Sharma"], "title": "InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation", "comment": null, "summary": "Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.", "AI": {"tldr": "本文提出了InteracTalker框架，通过多阶段训练和自适应融合策略，将语音驱动手势与物体交互融合，生成逼真、对物体敏感的人体动作，并构建了一个新的数据集支持此研究。", "motivation": "当前方法分别处理语音驱动手势或物体交互，缺乏集成和综合数据集，限制了其在现实世界中的应用。", "method": "引入了InteracTalker框架，采用多阶段训练来学习统一的动作、语音和提示词嵌入空间。为此，作者通过增强现有文本到动作数据集，创建了一个丰富的人机交互数据集。该框架利用广义动作适应模块实现独立训练和动态组合，并提出了一种自适应融合策略，在扩散采样过程中动态重新加权异构条件信号。", "result": "InteracTalker成功统一了先前分离的任务，在语音驱动手势生成和物体交互合成方面均优于现有方法，甚至超越了专注于手势的扩散方法，生成了高度逼真、对物体敏感的全身动作，具有更高的真实感、灵活性和控制力。", "conclusion": "InteracTalker提供了一个统一的解决方案，用于生成对物体敏感、语音驱动的逼真人机动作，克服了以往的局限性，并取得了卓越的性能。"}}
{"id": "2512.13290", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13290", "abs": "https://arxiv.org/abs/2512.13290", "authors": ["Shu Yu", "Chaochao Lu"], "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models", "comment": null, "summary": "Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.", "AI": {"tldr": "扩散模型在物理对齐和OOD指令遵循方面存在问题，原因在于缺乏因果理解。本文引入因果场景图和物理对齐探测数据集进行诊断，并基于洞察提出了LINA框架，通过目标指导和因果感知去噪调度，显著提升了扩散模型在因果生成任务上的性能。", "motivation": "扩散模型在图像和视频生成中表现出色，但在物理对齐和遵循分布外(OOD)指令方面仍面临挑战。作者认为这些问题源于模型未能学习因果方向并解耦因果因素以进行新颖重组。", "method": "本文引入了因果场景图（Causal Scene Graph, CSG）和物理对齐探测（Physical Alignment Probe, PAP）数据集以进行诊断性干预。基于分析结果，提出了LINA（Learning INterventions Adaptively）框架，该框架通过(1)在提示词和视觉潜在空间中进行有针对性的指导，以及(2)重新分配的、因果感知的去噪调度来学习预测特定提示的干预措施。", "result": "诊断分析得出三个关键洞察：1) 扩散模型难以对提示中未明确确定的元素进行多跳推理；2) 提示嵌入包含纹理和物理的解耦表示；3) 视觉因果结构主要在初始、计算受限的去噪步骤中建立。LINA方法在图像和视频扩散模型中强制实现了物理对齐和OOD指令遵循，并在具有挑战性的因果生成任务和Winoground数据集上取得了最先进的性能。", "conclusion": "扩散模型在物理对齐和OOD指令遵循方面的不足源于其缺乏对因果方向的学习和因果因素的解耦能力。通过引入CSG和PAP进行诊断，并基于此开发LINA框架，结合目标指导和因果感知去噪调度，能够有效解决这些问题，并在因果生成任务中达到卓越性能。"}}
{"id": "2512.13600", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13600", "abs": "https://arxiv.org/abs/2512.13600", "authors": ["Haoyue Zhang", "Meera Chappidi", "Erolcan Sayar", "Helen Richards", "Zhijun Chen", "Lucas Liu", "Roxanne Wadia", "Peter A Humphrey", "Fady Ghali", "Alberto Contreras-Sanz", "Peter Black", "Jonathan Wright", "Stephanie Harmon", "Michael Haffner"], "title": "DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides", "comment": null, "summary": "Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.", "AI": {"tldr": "本文提出了一种名为DA-SSL的域自适应自监督适配器，旨在解决病理基础模型（PFMs）在处理如膀胱肿瘤经尿道切除术（TURBT）等具有领域偏移的组织病理学图像时的局限性，并在不微调基础模型的情况下，有效提升其在临床挑战性任务（如预测新辅助化疗反应）上的性能。", "motivation": "当前的深度学习框架，尤其是结合了多实例学习（MIL）和病理基础模型（PFM）的方法，在组织病理学中表现出色。然而，PFMs在某些癌症或标本类型上存在局限性，这是由于领域偏移造成的，例如这些癌症类型在预训练中很少使用，或者标本含有预训练数据中不常见的组织伪影。TURBT标本就是其中一例，它们对于诊断肌层浸润性膀胱癌（MIBC）至关重要，但包含碎片化的组织芯片和电凝伪影，且未被广泛用于公开可用的PFMs预训练中。此外，在TURBT中预测治疗反应是一个挑战，组织形态学特征目前未被充分利用，识别将从新辅助化疗（NAC）中受益的患者也十分困难。", "method": "为解决上述问题，研究人员提出了一种简单而有效的域自适应自监督适配器（DA-SSL）。该适配器旨在将预训练的PFM特征重新对齐到TURBT领域，而无需对基础模型本身进行微调。该框架被应用于预测TURBT中的治疗反应。", "result": "在多中心研究中，DA-SSL在五折交叉验证中实现了0.77+/-0.04的AUC。在外部测试中，使用多数投票法，其准确率达到0.84，敏感性为0.71，特异性为0.91。", "conclusion": "研究结果表明，轻量级的自监督域适应方法可以有效增强基于PFM的MIL流水线，从而解决临床上具有挑战性的组织病理学任务。"}}
{"id": "2512.13678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13678", "abs": "https://arxiv.org/abs/2512.13678", "authors": ["Ziqi Ma", "Hongqiao Chen", "Yisong Yue", "Georgia Gkioxari"], "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D", "comment": "https://glab-caltech.github.io/steer3d/", "summary": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/", "AI": {"tldr": "Steer3D是一种前馈方法，为图像到3D模型添加文本可控性，使用户能够通过语言编辑AI生成的3D资产。", "motivation": "尽管图像到3D技术取得了进展，但要在实际应用中使用AI生成的3D资产，关键在于需要具备轻松编辑它们的能力。", "method": "该方法受ControlNet启发，将其应用于图像到3D生成，以在前向传播中实现文本引导。它构建了一个可扩展的自动数据生成引擎，并开发了两阶段训练方案，包括流匹配训练和直接偏好优化（DPO）。", "result": "与竞争方法相比，Steer3D能更忠实地遵循语言指令，与原始3D资产保持更好的一致性，并且速度快2.4到28.5倍。", "conclusion": "Steer3D证明了使用10万数据，可以成功地为预训练的图像到3D生成模型添加新的模态（文本）以进行生成引导。"}}
{"id": "2512.12667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12667", "abs": "https://arxiv.org/abs/2512.12667", "authors": ["Haiyang Zheng", "Nan Pu", "Wenjing Li", "Teng Long", "Nicu Sebe", "Zhun Zhong"], "title": "Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning", "comment": "Accepted by AAAI2026", "summary": "The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.", "AI": {"tldr": "该论文提出了一种名为CAL（Confidence-Aware Asymmetric Learning）的新框架，用于开放世界深度伪造归因（OW-DFA）。CAL通过解决现有方法中存在的置信度偏差和未知伪造类型数量需先验已知的问题，显著提升了已知和未知伪造归因的性能，并引入了动态原型修剪策略以自动估计未知类型数量，实现了最先进的性能。", "motivation": "随着合成人脸图像的普及，对鲁棒的开放世界深度伪造归因（OW-DFA）的需求日益增长。然而，现有方法存在两大局限性：1) 置信度偏差导致未知伪造的伪标签不可靠，从而引起训练偏差；2) 不切实际地假设未知伪造类型的数量是先验已知的。", "method": "本文提出了一个置信度感知非对称学习（CAL）框架，它能自适应地平衡已知和新型伪造类型之间的模型置信度。CAL主要包含两个组件：置信度感知一致性正则化（CCR）和非对称置信度强化（ACR）。CCR通过基于归一化置信度动态调整样本损失来减轻伪标签偏差。ACR通过在高置信度样本上选择性学习，并由置信度差距引导，分别校准已知和新型类别的置信度。此外，还引入了动态原型修剪（DPP）策略，以粗到细的方式自动估计新型伪造类型的数量，无需先验假设。", "result": "在标准OW-DFA基准测试和新扩展的包含高级操作的基准测试上进行的广泛实验表明，CAL始终优于现有方法，在已知和新型伪造归因方面均达到了新的最先进性能。", "conclusion": "CAL框架通过解决置信度偏差和未知类型数量的先验假设问题，为开放世界深度伪造归因提供了一个更鲁棒和可扩展的解决方案，显著提高了对已知和未知深度伪造的归因能力，并树立了新的行业标杆。"}}
{"id": "2512.12673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12673", "abs": "https://arxiv.org/abs/2512.12673", "authors": ["Yushun Tang", "Ziqiong Liu", "Jiyuan Jia", "Yi Zhang", "Zhihai He"], "title": "Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation", "comment": null, "summary": "Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\\% in classification accuracy on the ImageNet-C dataset.", "AI": {"tldr": "本文提出了一种名为PCSR的新方法，通过渐进式条件尺度-偏移校准来自适应地调整Transformer模型在推理阶段的自注意力模块，以解决在线测试时域适应中的性能下降问题。", "motivation": "当Transformer网络模型应用于新的目标域时，其自注意力模块的Query、Key和Value特征与源域相比会发生显著变化，导致模型性能大幅下降。这促使研究人员寻求一种方法来动态调整模型以适应这些变化。", "method": "本文提出了一种渐进式条件尺度-偏移校准（PCSR）方法。它将在线模型适应视为一个渐进式域偏移分离过程。在Transformer网络的每一层，通过一个局部线性变换（由条件尺度和偏移因子参数化）来重新校准自注意力。为此，学习一个域分离网络（DSN）来提取域偏移特征，然后使用一个因子生成器网络（FGN）预测用于自注意力校准的尺度和偏移参数。这两个轻量级网络在推理过程中进行在线适应。", "result": "实验结果表明，所提出的PCSR方法能够显著提高在线测试时域适应性能，在ImageNet-C数据集上的分类准确率提升高达3.9%。", "conclusion": "PCSR方法通过渐进式条件尺度-偏移校准有效地解决了Transformer模型在在线测试时域适应中的性能下降问题，通过实时调整自注意力机制显著提升了模型在目标域的适应能力。"}}
{"id": "2512.13690", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13690", "abs": "https://arxiv.org/abs/2512.13690", "authors": ["Susung Hong", "Chongjian Ge", "Zhifei Zhang", "Jui-Hsien Wang"], "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders", "comment": "Project page: https://susunghong.github.io/DiffusionBrowser", "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.", "AI": {"tldr": "DiffusionBrowser是一个模型无关的轻量级解码器框架，它允许用户在视频扩散模型的去噪过程中，在任何时间点交互式地生成多模态预览，并能实时引导生成，揭示生成细节。", "motivation": "视频扩散模型在生成视频时存在不精确、速度慢且过程不透明的问题，用户在长时间的生成过程中无法了解进度和细节。", "method": "本文提出了DiffusionBrowser，一个模型无关的轻量级解码器框架。它能够在去噪过程中的任何时间步或Transformer块生成多模态预览（包括RGB和场景内参）。该框架还支持通过随机性重注入和模态引导在中间噪声步进行交互式生成引导。", "result": "DiffusionBrowser能够以超过4倍实时速度（4秒视频不到1秒）生成预览，且预览与最终视频具有一致的外观和运动。它实现了在中间噪声步交互式引导生成的新控制能力。此外，通过学习到的解码器，该框架系统性地揭示了场景、物体和其他细节在去噪过程中的组成和组装方式。", "conclusion": "DiffusionBrowser通过提供实时交互式预览和中间步骤的控制能力，解决了视频扩散模型不精确、速度慢和不透明的问题，使用户能够更好地理解和引导视频生成过程。"}}
{"id": "2512.12718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12718", "abs": "https://arxiv.org/abs/2512.12718", "authors": ["Sehyun Kim", "Hye Jun Lee", "Jiwoo Lee", "Changgyun Kim", "Taemin Lee"], "title": "Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images", "comment": null, "summary": "The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.", "AI": {"tldr": "本研究提出了一种3D人体姿态分析系统，通过整合四个方向的深度图像来恢复3D人体模型并自动估计脊柱中心线，以克服现有方法的局限性。", "motivation": "脊柱角度是身体平衡的重要指标。现有的多图像人体恢复方法需要昂贵的设备和复杂程序，而单图像方法因遮挡和视角限制难以准确估计脊柱中心线（如脊柱）。", "method": "该方法整合了来自四个方向的深度图像，通过分层匹配（全局和精细配准）进行恢复，以应对噪声和遮挡。它还应用自适应顶点减少技术来保持网格分辨率和形状可靠性，并通过细节层次（Level of Detail）集成确保脊柱角度估计的准确性和稳定性。该方法不依赖训练数据或复杂的神经网络模型。", "result": "所提出的方法实现了高精度的3D脊柱配准估计，且无需训练数据或复杂神经网络模型。验证结果证实了匹配质量的显著提升。", "conclusion": "本研究提出的方法有效地弥补了多图像方法的缺点，解决了单图像方法的局限性，实现了高精度、稳定的3D脊柱中心线估计，对于身体平衡评估具有重要意义。"}}
{"id": "2512.12800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12800", "abs": "https://arxiv.org/abs/2512.12800", "authors": ["Yunlong He", "Gwilherm Lesné", "Ziqian Liu", "Michaël Soumm", "Pietro Gori"], "title": "Learning Common and Salient Generative Factors Between Two Image Datasets", "comment": "This is the author's version of a work submitted to IEEE for possible publication. The final version may differ from this version", "summary": "Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.", "AI": {"tldr": "本文提出了一种新颖的对比分析（CA）框架，旨在从两个图像数据集中分离出共有和显著的生成因素，该框架适用于GAN和扩散模型，并实现了高质量的图像生成和优异的分离能力。", "motivation": "现有的图像合成工作主要集中于条件操纵或解耦表示学习。本文关注一个不同且研究较少的对比分析（CA）问题，即在仅使用数据集信号（而非属性监督）的情况下，区分两个数据集之间共享的生成因素和特定于某个数据集的显著因素。", "method": "提出了一种新颖的对比分析（CA）框架，可适应于GAN和扩散模型。通过定义新的学习策略和损失函数，该方法旨在确保共有和显著因素之间的有效分离，同时保持高质量的图像生成。", "result": "该框架在人脸、动物图像和医学扫描等多种数据集上进行了评估，结果表明其在分离能力和图像合成质量方面均优于现有方法。", "conclusion": "本文成功地开发了一个适用于GAN和扩散模型的对比分析框架，能够有效地将两个数据集中的共有和显著生成因素分离，并在仅使用数据集信号的情况下，实现了卓越的分离能力和高质量的图像生成。"}}
{"id": "2512.12790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12790", "abs": "https://arxiv.org/abs/2512.12790", "authors": ["Tiange Zhang", "Zhimeng Huang", "Xiandong Meng", "Kai Zhang", "Zhipin Deng", "Siwei Ma"], "title": "L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context", "comment": "Accepted to Data Compression Conference (DCC) 2026 as an oral paper", "summary": "Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.", "AI": {"tldr": "本文提出L-STEC方法，通过引入长时空增强上下文，解决了现有神经网络视频压缩中短期依赖和误差累积问题，显著提升了压缩性能。", "motivation": "现有神经网络视频压缩方法仅依赖前一帧特征预测时域上下文，导致两个关键问题：一是参考窗口过短，无法捕获长期依赖和精细纹理；二是仅传播特征级信息会累积误差，导致预测不准确和纹理丢失。", "method": "本文提出长时空增强上下文（L-STEC）方法。首先，通过LSTM扩展参考链以捕获长期依赖。其次，引入像素域的扭曲空间上下文，并通过多感受野网络融合时空信息，以更好地保留参考细节。", "result": "L-STEC显著提升了压缩性能，相较于DCVC-TCM，在PSNR指标下节省37.01%比特率，在MS-SSIM指标下节省31.65%比特率。同时，L-STEC超越了VTM-17.0和DCVC-FM，达到了新的最先进水平。", "conclusion": "L-STEC通过丰富上下文信息，有效解决了神经网络视频压缩中的长期依赖和细节保留问题，实现了显著的压缩性能提升，并确立了新的技术水平。"}}
{"id": "2512.12875", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.12875", "abs": "https://arxiv.org/abs/2512.12875", "authors": ["Weihan Xu", "Kan Jen Cheng", "Koichi Saito", "Muhammad Jehanzeb Mirza", "Tingle Li", "Yisi Liu", "Alexander H. Liu", "Liming Wang", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji", "Gopala Anumanchipalli", "Paul Pu Liang"], "title": "Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal", "comment": null, "summary": "Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.", "AI": {"tldr": "该研究引入了SAVEBench数据集和SAVE模型，用于解决联合音视频编辑中的数据和模态挑战。SAVE是一个端到端的流匹配模型，利用薛定谔桥实现音视频内容的并行编辑和对象移除，同时保持模态对齐和语义一致性。", "motivation": "音视频内容的联合编辑对于精确和可控的内容创作至关重要，但面临两大挑战：一是缺乏目标编辑前后配对的音视频数据，二是音视频模态之间的异质性。", "method": "为解决数据和建模挑战，研究者提出：1. **SAVEBench数据集**：一个带有文本和掩码条件的配对音视频数据集，以实现基于对象的源到目标学习。2. **SAVE模型**：一个端到端的流匹配模型，用于并行编辑音视频内容并保持处理过程中的对齐。SAVE模型融入了**薛定谔桥**，学习从源到目标音视频混合的直接传输。", "result": "评估结果表明，所提出的SAVE模型能够移除音视频内容中的目标对象，同时保留其余内容。与独立音视频编辑器的组合相比，SAVE模型在时间同步性和音视频语义对应方面表现出更强的性能。", "conclusion": "SAVE模型成功解决了联合音视频编辑的数据和建模挑战，通过引入SAVEBench数据集和基于薛定谔桥的流匹配模型，实现了高效且高质量的音视频对象移除，并在模态同步和语义一致性方面优于现有方法。"}}
{"id": "2512.12887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12887", "abs": "https://arxiv.org/abs/2512.12887", "authors": ["Han Liu", "Bogdan Georgescu", "Yanbo Zhang", "Youngjin Yoo", "Michael Baumgartner", "Riqiang Gao", "Jianing Wang", "Gengyan Zhao", "Eli Gibson", "Dorin Comaniciu", "Sasa Grbic"], "title": "Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification", "comment": "1st Place in VLM3D Challenge", "summary": "3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.", "AI": {"tldr": "本文提出AnyMC3D，一个可扩展的3D医学图像分类器，通过在冻结的2D基础模型上添加轻量级插件实现，解决了现有研究的局限性，并在多任务基准测试中超越了3D架构。", "motivation": "当前的3D医学图像分类研究，特别是基于医学基础模型（FMs）的方法，存在数据机制偏差、适应性不佳和任务覆盖不足等关键缺陷。", "method": "AnyMC3D通过在单一冻结骨干网络之上添加轻量级插件（每个任务约1M参数）来适应2D基础模型，实现高效扩展。该框架支持多视角输入、辅助像素级监督和可解释热图生成。研究建立了包含12个任务的综合基准进行评估。", "result": "分析揭示了关键见解：1) 有效的适应对于释放基础模型潜力至关重要；2) 如果适应得当，通用基础模型可以与医学专用基础模型媲美；3) 基于2D的方法在3D分类中超越了3D架构。AnyMC3D首次证明了一个单一的可扩展框架可以在各种应用中（包括VLM3D挑战赛第一名）实现最先进的性能。", "conclusion": "AnyMC3D证明了使用一个单一的可扩展框架实现跨多样化3D医学分类任务的最先进性能是可行的，从而消除了对单独的、特定任务模型的需要。"}}
{"id": "2512.12751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12751", "abs": "https://arxiv.org/abs/2512.12751", "authors": ["Zhenya Yang", "Zhe Liu", "Yuxiang Lu", "Liping Hou", "Chenxuan Miao", "Siyi Peng", "Bailan Feng", "Xiang Bai", "Hengshuang Zhao"], "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation", "comment": "The project page is available at https://huster-yzy.github.io/geniedrive_project_page/", "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.", "AI": {"tldr": "GenieDrive是一个新颖的框架，通过首先生成4D占据（occupancy）作为物理信息基础，然后利用创新的压缩和注意力机制生成物理感知、多视角一致且高度可控的驾驶视频。", "motivation": "现有的驾驶世界模型通常依赖单一扩散模型直接将驾驶动作映射到视频，这使得学习困难并导致物理不一致的输出。因此，需要一个能够生成物理感知驾驶视频的方法。", "method": "该方法首先生成4D占据作为物理信息基础。为了有效压缩高分辨率占据，提出了一种VAE，将其编码为潜在三平面表示，将潜在大小减少到现有方法的58%。引入了互控注意力（Mutual Control Attention, MCA）来精确建模控制对占据演变的影响，并以端到端方式联合训练VAE和后续预测模块。此外，在视频生成模型中引入了归一化多视角注意力（Normalized Multi-View Attention），以4D占据为指导生成多视角驾驶视频。", "result": "GenieDrive在预测mIoU方面实现了7.2%的提升，推理速度达到41 FPS，仅使用3.47 M参数。视频质量显著提高，FVD降低了20.7%。实验证明，GenieDrive能够实现高度可控、多视角一致和物理感知的驾驶视频生成。", "conclusion": "GenieDrive通过其新颖的4D占据生成和视频生成机制，成功实现了物理感知、多视角一致且高度可控的驾驶视频生成，解决了现有方法在物理一致性和学习难度上的挑战。"}}
{"id": "2512.12799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12799", "abs": "https://arxiv.org/abs/2512.12799", "authors": ["Zhe Liu", "Runhui Huang", "Rui Yang", "Siming Yan", "Zining Wang", "Lu Hou", "Di Lin", "Xiang Bai", "Hengshuang Zhao"], "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning", "comment": null, "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI", "AI": {"tldr": "DrivePI是一个新颖的空间感知4D多模态大语言模型（MLLM），作为统一的视觉-语言-动作（VLA）框架，在自动驾驶中实现了3D感知、预测和规划的端到端优化，并显著超越了现有VLA和专业VA模型。", "motivation": "尽管多模态大语言模型（MLLM）在许多领域表现出色，但它们在自动驾驶中生成细粒度的3D感知和预测输出方面的应用仍未得到充分探索。", "method": "本文提出了DrivePI，一个空间感知的4D MLLM，作为统一的视觉-语言-动作（VLA）框架，并兼容视觉-动作（VA）模型。该方法通过端到端优化并行执行空间理解、3D感知（3D占用）、预测（占用流）和规划（动作输出）。它整合了点云、多视图图像和语言指令，以获取精确的几何信息和丰富的视觉外观。此外，还开发了一个数据引擎来生成文本-占用和文本-流QA对，用于4D空间理解。该模型以0.5B Qwen2.5模型作为MLLM骨干。", "result": "DrivePI作为一个单一的统一模型，匹配或超越了现有VLA模型和专业VA模型。具体来说，与VLA模型相比，DrivePI在nuScenes-QA上比OpenDriveVLA-7B的平均准确率高2.5%，在nuScenes上将碰撞率比ORION降低了70%（从0.37%降至0.11%）。与专业VA模型相比，DrivePI在OpenOcc上3D占用比FB-OCC高10.3 RayIoU，在OpenOcc上将占用流的mAVE从0.591降低到0.509，在nuScenes上规划的L2误差比VAD低32%（从0.72m降至0.49m）。", "conclusion": "DrivePI成功地将MLLM应用于自动驾驶中的细粒度4D感知、预测和规划任务。作为一个统一的VLA框架，它不仅实现了卓越的性能，而且作为一个单一模型在多项指标上超越了专门的VLA和VA模型，证明了其在自动驾驶领域的巨大潜力。"}}
{"id": "2512.12906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12906", "abs": "https://arxiv.org/abs/2512.12906", "authors": ["Zhimao Peng", "Enguang Wang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection", "comment": "Accepted by TCSVT2024", "summary": "Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.", "AI": {"tldr": "本文提出了一种基于预测样本分配（PSA）的语义连贯OOD检测（SCOOD）框架，通过双阈值三元样本分配策略、概念对比表示学习损失和再训练策略，显著提高了ID和OOD样本的纯度，并增强了ID/OOD区分能力，优于现有先进方法。", "motivation": "当前的SCOOD方法主要采用基于聚类的ID样本过滤策略，将剩余样本视为辅助OOD数据，这不可避免地在训练中引入大量噪声样本，降低了训练效率和模型性能。", "method": "本文提出预测样本分配（PSA）框架，包括：1) 基于预测能量得分的双阈值三元样本分配策略，将不确定的无标签数据分配到额外的丢弃样本集，以提高选定ID和OOD样本集的纯度；2) 概念对比表示学习损失，以进一步扩大ID和OOD样本在表示空间中的距离；3) 再训练策略，帮助模型充分拟合选定的辅助ID/OOD样本。", "result": "在两个标准SCOOD基准测试中，所提出的方法显著优于现有最先进的方法。", "conclusion": "本文提出的PSA框架通过其独特的双阈值样本分配、概念对比学习和再训练策略，有效解决了SCOOD中噪声样本引入的问题，显著提升了OOD检测的性能。"}}
{"id": "2512.12941", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12941", "abs": "https://arxiv.org/abs/2512.12941", "authors": ["Siyuan Yao", "Dongxiu Liu", "Taotao Li", "Shengjie Li", "Wenqi Ren", "Xiaochun Cao"], "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction", "comment": "IEEE TGRS", "summary": "Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet", "AI": {"tldr": "本文提出了一种不确定性聚合的全局-局部融合网络（UAGLNet），通过不确定性建模指导，有效整合全局与局部视觉语义，以提高遥感图像中的建筑物提取精度。", "motivation": "现有建筑物提取方法在多尺度特征捕获上存在特征金字塔固有差距和全局-局部特征整合不足的问题，导致提取结果不准确和模糊。", "method": "本文提出了UAGLNet，包括：1) 合作编码器，采用混合CNN和Transformer层分别捕获局部和全局语义；2) 中间合作交互块（CIB），缩小局部与全局特征的差距；3) 全局-局部融合（GLF）模块，互补融合全局与局部表示；4) 不确定性聚合解码器（UAD），显式估计像素级不确定性以缓解分割模糊并提升精度。", "result": "广泛的实验证明，该方法在建筑物提取任务中取得了优于其他现有最先进方法的性能。", "conclusion": "UAGLNet通过其独特的不确定性建模和全局-局部特征融合机制，有效解决了遥感图像建筑物提取中的挑战，并显著提升了分割精度。"}}
{"id": "2512.12898", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12898", "abs": "https://arxiv.org/abs/2512.12898", "authors": ["Abhinav Kumar", "Tristan Aumentado-Armstrong", "Lazar Valkov", "Gopal Sharma", "Alex Levinshtein", "Radek Grzeszczuk", "Suren Kumar"], "title": "Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution", "comment": "28 pages, 8 figures, Project Page: https://abhi1kumar.github.io/qonvolution/", "summary": "Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.", "AI": {"tldr": "神经网络在学习高频信号方面存在困难。本文提出了一种名为Queried-Convolutions（Qonvolutions）的简单而强大的方法，它通过将低频信号与查询（如坐标）进行卷积，利用卷积的邻域特性来增强高频信号的学习。实验证明，Qonvolutions在多种高频学习任务中提升了性能，尤其在结合高斯泼溅进行新视角合成时，达到了最先进的水平。", "motivation": "神经网络在学习高频信号时常因谱偏差或优化困难而表现不佳。尽管傅里叶编码等现有技术已取得显著进展，但在处理高频信息时仍有改进空间。", "method": "本文引入了Queried-Convolutions（Qonvolutions），这是一种利用卷积邻域特性的简单修改。Qonvolution将低频信号与查询（例如坐标）进行卷积，以增强对复杂高频信号的学习。", "result": "实验证明，Qonvolutions在多种高频学习任务中提升了性能，包括一维回归、二维超分辨率、二维图像回归和新视角合成（NVS）。特别是，将Qonvolutions与高斯泼溅结合用于NVS时，在真实世界复杂场景中展现了最先进的性能，甚至在图像质量上超越了强大的辐射场模型。", "conclusion": "Qonvolutions是一种简单而有效的方法，能够显著提升神经网络在高频信号学习任务中的表现，为计算机视觉和图形学领域的多项关键任务带来了性能提升，并在新视角合成等领域取得了突破性成果。"}}
{"id": "2512.12756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12756", "abs": "https://arxiv.org/abs/2512.12756", "authors": ["Yue Jiang", "Dingkang Yang", "Minghao Han", "Jinghang Han", "Zizhi Chen", "Yizhou Liu", "Mingcheng Li", "Peng Zhai", "Lihua Zhang"], "title": "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning", "comment": "The omni-modal benchmark report from Fysics AI", "summary": "Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.", "AI": {"tldr": "该论文引入了FysicsWorld，首个统一的全模态基准测试，支持图像、视频、音频和文本之间的双向输入输出，旨在全面评估多模态大模型的理解、生成和推理能力。", "motivation": "当前的基准测试在模态覆盖、交互方式（局限于以文本为中心）以及模态间的相互依赖性和互补性方面存在局限性，无法充分评估快速发展的多模态大语言模型（MLLM）和全模态架构。", "method": "研究者提出了FysicsWorld，一个统一的全模态基准测试，支持图像、视频、音频和文本的任意模态到任意模态的双向输入输出。它包含16个主要任务和3,268个高质量样本，并引入了跨模态互补性筛选（CMCS）策略，用于构建全模态数据，特别是针对口语交互和依赖融合的跨模态推理。", "result": "通过对30多个最先进的基线模型（包括MLLM、特定模态模型、统一理解-生成模型和全模态语言模型）进行全面评估，FysicsWorld揭示了这些模型在理解、生成和推理方面的性能差异和局限性。", "conclusion": "FysicsWorld基准测试为评估和推进下一代全模态架构奠定了统一的基础，并提供了强有力的基线，有助于未来研究克服现有模型的性能瓶颈。"}}
{"id": "2512.12977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12977", "abs": "https://arxiv.org/abs/2512.12977", "authors": ["Shengling Qin", "Hao Yu", "Chenxin Wu", "Zheng Li", "Yizhong Cao", "Zhengyang Zhuge", "Yuxin Zhou", "Wentao Yao", "Yi Zhang", "Zhengheng Wang", "Shuai Bai", "Jianwei Zhang", "Junyang Lin"], "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference", "comment": null, "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.", "AI": {"tldr": "VLCache是一个缓存复用框架，通过利用多模态输入的KV和编码器缓存来消除重复计算，并提出了一种新的策略来最小化误差并平衡准确性和效率。", "motivation": "当多模态输入重复出现时，重新计算成本高昂，因此需要一种方法来有效利用先前的缓存以避免不必要的计算。", "method": "VLCache正式识别并最小化累积复用误差（特别是针对非前缀缓存复用），并提出了一种动态、分层感知的重新计算策略，以平衡准确性和效率。", "result": "VLCache在保持与完全重新计算相当的准确性的同时，仅需计算2-5%的tokens，实现了1.2倍至16倍的TTFT（Time To First Token）加速。该框架已集成到SGLang中。", "conclusion": "VLCache通过有效的缓存复用显著加速了多模态推理，并在实际部署中展现出优异的性能和准确性。"}}
{"id": "2512.12774", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12774", "abs": "https://arxiv.org/abs/2512.12774", "authors": ["Hao Wang", "Ashish Bastola", "Chaoyi Zhou", "Wenhui Zhu", "Xiwen Chen", "Xuanzhao Dong", "Siyu Huang", "Abolfazl Razi"], "title": "Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior", "comment": null, "summary": "As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.", "AI": {"tldr": "Fast-2DGS 提出了一种轻量级框架，通过深度高斯先验和属性回归网络，实现了高效、高质量的2D高斯图像表示，显著减少了计算成本并加速了收敛，使其更接近工业应用。", "motivation": "随着生成模型生成高保真视觉内容的能力增强，对高效、可解释、可编辑的图像表示的需求也随之增长。现有的2D高斯溅射(2DGS)方法在后优化阶段存在收敛慢或需要复杂网络来预测初始配置的问题，导致计算成本和架构复杂性增加。", "method": "本文提出了Fast-2DGS框架。它引入了“深度高斯先验”（Deep Gaussian Prior），这是一个条件网络，用于捕捉不同复杂性下高斯基元的空间分布。此外，还提出了一个“属性回归网络”来预测密集的高斯属性。这种解耦架构旨在通过一次前向传播实现高质量重建，并辅以最少的微调。", "result": "实验表明，该方法在单次前向传播后即可实现高质量重建，只需少量微调。更重要的是，它在不影响视觉质量的前提下显著降低了计算成本。", "conclusion": "Fast-2DGS通过其高效的架构和方法，显著减少了2DGS的计算开销，同时保持了高视觉质量，从而使2DGS技术更接近工业级部署。"}}
{"id": "2512.12925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12925", "abs": "https://arxiv.org/abs/2512.12925", "authors": ["Zhimao Peng", "Enguang Wang", "Fei Yang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery", "comment": "Accepted by TMM2025", "summary": "Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.", "AI": {"tldr": "本文提出了一种名为LSP和DAS的新方法，旨在解决广义类别发现（GCD）任务中伪标签噪声问题，从而提高聚类精度，并实现了最先进的性能。", "motivation": "当前的GCD方法依赖于DINO风格的伪标签策略，但大型预训练模型容易编码虚假关联，为未标记数据生成噪声伪标签，导致模型对噪声样本过拟合。", "method": "本文提出了两种模块：1) 损失锐度惩罚（LSP），通过最小化模型最坏情况的损失锐度来增强模型参数对小扰动的鲁棒性，从而抑制琐碎特征的编码，减少噪声样本过拟合并提高伪标签质量。2) 动态锚点选择（DAS），在模型训练期间基于KNN密度和类别概率为未知类别选择代表性样本，并分配硬伪标签，以缓解已知和未知类别之间的置信度差异，并帮助模型快速学习更准确的未知类别特征分布。", "result": "广泛的实验表明，所提出的方法能有效缓解伪标签的噪声，并在多个GCD基准测试中取得了最先进的结果。", "conclusion": "通过引入损失锐度惩罚和动态锚点选择，本方法成功解决了广义类别发现中伪标签噪声的问题，显著提高了模型对未知类别的聚类精度和特征学习能力。"}}
{"id": "2512.12963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12963", "abs": "https://arxiv.org/abs/2512.12963", "authors": ["Luan Thanh Trinh", "Kenji Doi", "Atsuki Osanai"], "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer", "comment": "Accepted to WACV 2026", "summary": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.", "AI": {"tldr": "SCAdapter是一种新型扩散模型风格迁移方法，利用CLIP图像空间有效分离和整合内容与风格特征，实现逼真的风格迁移，同时比现有扩散方法快至少2倍。", "motivation": "扩散模型在风格迁移中表现出色，但在照片级真实感迁移方面存在不足，常产生绘画效果或丢失细节风格元素。现有方法未能充分解决原始内容风格和风格参考内容特征的不必要影响。", "method": "SCAdapter利用CLIP图像空间有效分离和整合内容与风格特征，系统地从内容图像中提取纯内容，从风格参考中提取风格元素。该方法通过三个组件增强：可控风格自适应实例归一化（CSAdaIN）用于精确多风格混合，KVS注入用于有针对性的风格整合，以及风格迁移一致性目标以保持过程连贯性。此外，它消除了DDIM反演和推理阶段优化。", "result": "SCAdapter在传统和基于扩散的基线上均显著优于现有最先进方法。它比其他基于扩散的方法实现了至少2倍的推理速度，证明了其在实际应用中更高的效率和有效性。", "conclusion": "SCAdapter通过有效分离和整合内容与风格特征，并引入多项创新组件，解决了扩散模型在照片级真实感风格迁移中的挑战，实现了更真实、更快速的风格迁移，为实际应用提供了更有效和高效的解决方案。"}}
{"id": "2512.13006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13006", "abs": "https://arxiv.org/abs/2512.13006", "authors": ["Yifan Pu", "Yizeng Han", "Zhiwei Tang", "Jiasheng Tang", "Fan Wang", "Bohan Zhuang", "Gao Huang"], "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide", "comment": null, "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.", "AI": {"tldr": "首次系统性研究将扩散蒸馏技术应用于开放式文本到图像（T2I）生成，识别了关键障碍并提供了实践指导，为快速、高保真T2I生成奠定基础。", "motivation": "扩散蒸馏技术已显著加速类别条件图像合成，但其在开放式文本到图像（T2I）生成中的适用性尚不明确。", "method": "通过统一框架对现有最先进的蒸馏技术进行改编和比较，并将其应用于强大的T2I教师模型FLUX.1-lite。研究识别了从离散类别标签到自由形式语言提示的转换中出现的关键障碍，并提供了关于输入缩放、网络架构和超参数的实践指导。", "result": "识别了将扩散蒸馏应用于T2I生成时面临的关键障碍，提供了详细的方法分析和实用的指导方针，并发布了开源实现和预训练的学生模型。", "conclusion": "本研究为在实际T2I应用中部署快速、高保真、资源高效的扩散生成器奠定了坚实的基础。"}}
{"id": "2512.12678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12678", "abs": "https://arxiv.org/abs/2512.12678", "authors": ["Fatimah Zohra", "Chen Zhao", "Hani Itani", "Bernard Ghanem"], "title": "$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment", "comment": null, "summary": "CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.", "AI": {"tldr": "β-CLIP是一个多粒度文本条件对比学习框架，通过将文本（从完整标题到句子和短语）与图像区域进行分层对齐，显著改善了细粒度视觉-语言对应，超越了现有方法。", "motivation": "尽管CLIP在全局图像-文本检索方面表现出色，但在细粒度任务上，即使经过详细标题的微调，其性能仍显不足。", "method": "β-CLIP提出了一种多粒度文本条件对比学习框架。它通过交叉注意力动态汇集图像块，为每个文本粒度（标题、句子、短语）生成语境化视觉嵌入。为解决层级中的语义重叠，引入了β-语境化对比对齐损失（β-CAL），该损失平衡了严格的查询匹配和宽松的图像内语境化，支持软交叉熵和硬二元交叉熵公式。", "result": "β-CLIP显著提升了密集对齐性能：在Urban1K上，T2I R@1达到91.8%，I2T R@1达到92.3%；在FG-OVD（Hard）上达到30.9%。在未训练硬负样本的方法中，它达到了最先进的水平。", "conclusion": "β-CLIP为密集的视觉-语言对应建立了一个鲁棒且自适应的基线，显著提高了细粒度任务的性能。"}}
{"id": "2512.13014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13014", "abs": "https://arxiv.org/abs/2512.13014", "authors": ["Haoyu Wang", "Lei Zhang", "Wenrui Liu", "Dengyang Jiang", "Wei Wei", "Chen Ding"], "title": "JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion", "comment": "Accepted at AAAI 2026", "summary": "Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.", "AI": {"tldr": "JoDiffusion是一个新颖的扩散框架，能够根据文本提示同时生成语义一致的图像及其像素级标注，从而为语义分割模型提供可扩展且高质量的训练数据。", "motivation": "像素级标注成本高昂且耗时，而现有合成数据集生成方法存在图像-标注语义不一致或可扩展性问题（如生成图像后再预测伪标注，或依赖手动标注掩码生成图像）。", "method": "JoDiffusion框架基于标准潜在扩散模型，并引入独立的标注变分自编码器（VAE）将标注掩码映射到与图像共享的潜在空间。扩散模型被调整以捕获图像及其标注掩码的联合分布，并以文本提示为条件。这使得模型能够同时生成配对且语义一致的图像和标注掩码。此外，还开发了掩码优化策略以减轻生成过程中的标注噪声。", "result": "JoDiffusion生成的标注数据集在语义分割方面比现有方法带来了显著的性能提升。实验在Pascal VOC、COCO和ADE20K数据集上验证了其有效性。", "conclusion": "JoDiffusion通过联合生成图像和语义一致的标注，有效解决了合成数据生成中语义不一致性和可扩展性问题，为训练高性能语义分割模型提供了更优质、更便捷的数据来源。"}}
{"id": "2512.13018", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13018", "abs": "https://arxiv.org/abs/2512.13018", "authors": ["Tomoya Tanaka", "Tomonori Ikeda", "Ryo Yonemoto"], "title": "Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing", "comment": "8 pages, 6 figures. Comprehensive evaluation of preprocessing, data augmentation, and transfer learning for cross-environment generalization in deep learning-based mmWave radar sensing", "summary": "This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.", "AI": {"tldr": "本研究首次全面评估了深度学习射频传感中的空间泛化技术，发现基于幅度的预处理（如Sigmoid加权）和迁移学习能显著提高跨环境的人员计数准确性。", "motivation": "深度学习射频传感的实际部署需要强大的空间泛化能力，以应对不同环境下的性能下降问题。", "method": "本研究使用FMCW MIMO雷达进行室内人员计数，系统地评估了多种空间泛化技术，包括基于幅度的统计预处理（Sigmoid加权、阈值置零）、频域滤波、基于自编码器的背景抑制、数据增强策略和迁移学习。实验在两种不同布局的环境中进行。", "result": "Sigmoid加权的幅度预处理在跨环境性能上表现最佳，与基线方法相比，RMSE和MAE分别降低了50.1%和55.2%。数据增强提供了额外的但相对较小的改进（MAE提高达8.8%）。迁移学习对于大的空间变化至关重要，在540个目标域样本下，RMSE和MAE分别降低了82.1%和91.3%。", "conclusion": "将深度学习模型与基于幅度的预处理和高效的迁移学习相结合，为开发在空间变化下仍能保持鲁棒准确性的雷达传感系统提供了高度实用的方向。"}}
{"id": "2512.13083", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13083", "abs": "https://arxiv.org/abs/2512.13083", "authors": ["Saumyaranjan Mohanty", "Aravind Reddy", "Konda Reddy Mopuri"], "title": "DiRe: Diversity-promoting Regularization for Dataset Condensation", "comment": "Accepted to WACV 2026", "summary": "In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.", "AI": {"tldr": "本文提出了一种名为DiRe的多样性正则化器，通过结合余弦相似度和欧氏距离，旨在减少数据集凝缩中合成数据集的冗余并提高其多样性，从而提升泛化能力。", "motivation": "现有数据集凝缩方法合成的数据集存在显著冗余，且多样性不足，急需改进以提高其训练效用。", "method": "提出了一种直观的多样性正则化器（DiRe），它由余弦相似度和欧氏距离组成，可直接应用于各种现有最先进的凝缩方法。", "result": "通过广泛实验证明，DiRe的加入显著改善了从CIFAR-10到ImageNet-1K等各种基准数据集上最先进凝缩方法的泛化能力和多样性指标。", "conclusion": "所提出的DiRe多样性正则化器能够有效减少冗余并提高合成数据集的多样性，从而提升现有数据集凝缩方法的性能和泛化能力。"}}
{"id": "2512.13007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13007", "abs": "https://arxiv.org/abs/2512.13007", "authors": ["Nikolai Goncharov", "James L. Gray", "Donald G. Dansereau"], "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects", "comment": null, "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.", "AI": {"tldr": "本文提出了一种基于光场图像的物体跟踪方法，无需预训练模型，对复杂视觉行为（如反射）具有鲁棒性，并通过将语义和几何特征转换为高斯splat进行跟踪，性能与最先进的模型基跟踪器相当。", "motivation": "现有的高性能物体跟踪方法通常依赖于预捕获的物体视图来构建显式参考模型，这限制了它们只能跟踪已知物体，并且在面对视觉复杂的物体（如反射）时跟踪质量会下降。研究动机是开发一种能够泛化到未知且复杂物体、且不受预训练模型限制的鲁棒跟踪方法。", "method": "该方法引入了基于光场图像的物体跟踪。它不依赖于预训练模型，而是利用视觉基础模型从光场输入中提取语义和几何特征。这些特征被转换为视点相关的高斯splat，作为统一的物体表示，支持可微分渲染和姿态优化。此外，研究还构建了一个包含挑战性反射物体和精确真值姿态的光场物体跟踪数据集。", "result": "实验结果表明，该方法在处理反射物体等困难情况下，与最先进的基于模型的跟踪器相比具有竞争力。", "conclusion": "该研究为机器人系统中实现通用物体跟踪铺平了道路，尤其是在处理复杂和未知物体方面展现出潜力。"}}
{"id": "2512.13095", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13095", "abs": "https://arxiv.org/abs/2512.13095", "authors": ["Feng Zhang", "Zezhong Tan", "Xinhong Ma", "Ziqiang Dong", "Xi Leng", "Jianfei Zhao", "Xin Sun", "Yang Yang"], "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning", "comment": null, "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.", "AI": {"tldr": "ADHint是一种新型的提示词强化学习方法，通过在提示词比例调度和相对优势估计中引入难度因素，平衡探索与模仿，显著提升了推理能力和泛化性。", "motivation": "现有基于提示词的强化学习方法在调度提示词比例和估计相对优势时忽略了难度，导致学习不稳定和过度模仿离策略提示词。", "method": "ADHint通过以下方法解决问题：1) 提出“基于样本难度先验的自适应提示词”来根据样本难度调度提示词比例；2) 引入“基于一致性的梯度调制和选择性掩码”来保护提示词内的梯度；3) 采用“基于回滚难度后验的优势估计”来平衡有无提示词回滚的更新。", "result": "ADHint在多种模态、模型规模和领域中表现出卓越的推理能力和域外泛化能力，在pass@1和avg@8指标上均持续超越现有方法。", "conclusion": "通过将难度作为关键因素融入提示词比例调度和优势估计，ADHint成功地在探索与模仿之间实现了更好的权衡，从而显著提升了模型的推理能力和泛化表现。"}}
{"id": "2512.13055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13055", "abs": "https://arxiv.org/abs/2512.13055", "authors": ["Jaeyoon Kim", "Yoonki Cho", "Sung-Eui Yoon"], "title": "Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing", "comment": "AAAI 2026", "summary": "Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR", "AI": {"tldr": "本文提出了一种高效的非对称视觉地点识别（VPR）框架，结合高容量离线图库模型和轻量级在线查询网络，通过地理记忆库和隐式嵌入增强技术，在资源受限设备上显著降低计算成本并超越现有非对称方法。", "motivation": "尽管DINOv2等基础模型在VPR中表现出色，但其高昂的计算成本使其无法部署在资源受限的设备上，因此需要开发更高效的VPR解决方案。", "method": "研究引入了一个非对称VPR框架，包含一个用于离线特征提取的高容量图库模型和一个用于在线处理的轻量级查询网络。为解决异构网络兼容性问题，提出了一个利用VPR数据库中地理位置元数据构建的地理记忆库，以避免耗时的k-NN兼容训练。此外，还引入了隐式嵌入增强技术，以提升查询网络建模特征变化的能力。", "result": "实验证明，该方法不仅显著降低了计算成本，而且优于现有的非对称检索技术，为资源受限环境下的VPR开辟了新方向。", "conclusion": "该研究成功开发了一种高效且性能卓越的非对称VPR框架，使其能够在计算资源有限的设备上实际应用，解决了高容量模型部署的难题。"}}
{"id": "2512.13019", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13019", "abs": "https://arxiv.org/abs/2512.13019", "authors": ["Cheeun Hong", "German Barquero", "Fadime Sener", "Markos Georgopoulos", "Edgar Schönfeld", "Stefan Popov", "Yuming Du", "Oscar Mañas", "Albert Pumarola"], "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation", "comment": null, "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.", "AI": {"tldr": "SneakPeek是一种基于扩散模型的自回归框架，用于从初始图像和结构化文本提示生成未来驱动的流式教学视频，旨在解决现有模型在长序列视频中时间一致性和可控性差的问题。", "motivation": "现有的视频扩散模型在生成多步骤动作的长序列教学视频时，难以保持时间一致性和可控性。然而，生成此类视频在内容创作、教育和人机交互方面具有广泛的应用前景。", "method": "本文提出了SneakPeek框架，包含三项关键创新：1) 预测性因果适应，通过因果模型学习预测下一帧和未来关键帧；2) 未来引导的自强制，采用双区域KV缓存方案解决推理时的曝光偏差问题；3) 多提示条件，提供对多步骤指令的细粒度和程序化控制。", "result": "实验结果表明，该方法能够生成时间连贯、语义忠实的教学视频，并准确遵循复杂的多步骤任务描述。", "conclusion": "SneakPeek通过其创新组件有效缓解了时间漂移，保持了运动一致性，并实现了交互式视频生成，其中未来的提示更新可以动态影响正在进行的流式视频生成。"}}
{"id": "2512.12982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12982", "abs": "https://arxiv.org/abs/2512.12982", "authors": ["Ziheng Qin", "Yuheng Ji", "Renshuai Tao", "Yuxuan Tian", "Yuyang Liu", "Yipu Wang", "Xiaolong Zheng"], "title": "Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes", "comment": null, "summary": "The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13008", "abs": "https://arxiv.org/abs/2512.13008", "authors": ["Xi Luo", "Shixin Xu", "Ying Xie", "JianZhong Hu", "Yuwei He", "Yuhui Deng", "Huaxiong Huang"], "title": "TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading", "comment": null, "summary": "Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13015", "abs": "https://arxiv.org/abs/2512.13015", "authors": ["Xinjie Li", "Zhimin Chen", "Rui Zhao", "Florian Schiffers", "Zhenyu Liao", "Vimal Bhat"], "title": "What Happens Next? Next Scene Prediction with a Unified Video Model", "comment": null, "summary": "Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13039", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13039", "abs": "https://arxiv.org/abs/2512.13039", "authors": ["Hao Chen", "Yiwei Wang", "Songze Li"], "title": "Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models", "comment": "Under Review", "summary": "Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13031", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13031", "abs": "https://arxiv.org/abs/2512.13031", "authors": ["Tomoya Tanaka", "Tomonori Ikeda", "Ryo Yonemoto"], "title": "Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs", "comment": "10 pages, 5 figures. A comprehensive comparison of rule-based, machine learning, and deep learning approaches for human estimation using FMCW MIMO radar, focusing on accuracy, spatial generalization, and output granularity", "summary": "This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13072", "abs": "https://arxiv.org/abs/2512.13072", "authors": ["Zizhi Chen", "Yizhen Gao", "Minghao Han", "Yizhou Liu", "Zhaoyu Chen", "Dingkang Yang", "Lihua Zhang"], "title": "Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models", "comment": null, "summary": "Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \\textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13104", "abs": "https://arxiv.org/abs/2512.13104", "authors": ["Yan Zhang", "Baoxin Li", "Han Sun", "Yuhang Gao", "Mingtai Zhang", "Pei Wang"], "title": "FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection", "comment": null, "summary": "Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13250", "abs": "https://arxiv.org/abs/2512.13250", "authors": ["Juil Koo", "Daehyeon Choi", "Sangwoo Youn", "Phillip Y. Lee", "Minhyuk Sung"], "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection", "comment": "Project page: https://active-view-selection.github.io/", "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.", "AI": {"tldr": "本文引入了视觉基础主动视角选择（VG-AVS）任务，使视觉语言模型（VLMs）能够从当前图像中选择最信息丰富的下一视角，从而将静态视觉问答（VQA）扩展到具身智能体所需的主动视觉。", "motivation": "现有VLM仅限于对静态图像进行推理（快照视觉），而具身智能体需要主动移动以获取更具信息量的视角（主动视觉）。研究旨在弥合这一差距，使VLM能够进行主动视角选择。", "method": "引入了VG-AVS任务，即仅利用当前图像的视觉信息选择最信息丰富的下一视角，不依赖场景记忆或外部知识。构建了一个合成数据集，包含自动生成的查询-目标视角对和问答提示。提出了一个框架，通过监督微调（SFT）和基于强化学习（RL）的策略优化来微调预训练的VLM。", "result": "该方法在视角选择的基础上实现了强大的问答性能，并能稳健地泛化到未见过的合成和真实场景。此外，将所学的VG-AVS框架整合到现有基于场景探索的具身问答（EQA）系统中，能提高下游问答的准确性。", "conclusion": "VG-AVS框架成功地使VLM能够进行主动视角选择，显著提高了基于视角选择的问答性能，并能有效泛化，从而提升了具身智能体的问答能力。"}}
{"id": "2512.13147", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13147", "abs": "https://arxiv.org/abs/2512.13147", "authors": ["Sangmin Hong", "Suyoung Lee", "Kyoung Mu Lee"], "title": "StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion", "comment": "11 pages", "summary": "The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.", "AI": {"tldr": "StarryGazer是一个领域无关的框架，它利用大型单目深度估计（MDE）模型，通过单张稀疏深度图和RGB图像预测密集的深度图，无需地面真值深度。", "motivation": "现有的无监督深度补全方法需要辅助数据，且单目深度估计（MDE）模型虽然能生成合理的相对深度图，但简单地将稀疏深度图与MDE结合会因MDE在物体间深度差异估计不准确而导致高误差。缺乏一种能妥善结合稀疏深度图与MDE进行深度补全的方法。", "method": "StarryGazer首先使用预训练的MDE模型生成相对深度图。然后，这些图像被分割并随机重新缩放，以形成合成的密集伪地面真值和对应的稀疏深度对。最后，一个精炼网络利用这些合成对、相对深度图和RGB图像进行训练，以提高模型的准确性和鲁棒性。", "result": "StarryGazer在各种数据集上均表现出优于现有无监督方法和简单变换MDE结果的性能。", "conclusion": "该框架成功利用了MDE模型的能力，并通过稀疏深度信息适当地修正了误差，实现了出色的深度补全效果。"}}
{"id": "2512.13130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13130", "abs": "https://arxiv.org/abs/2512.13130", "authors": ["Shanghua Liu", "Majharulislam Babor", "Christoph Verduyn", "Breght Vandenberghe", "Bruno Betoni Parodi", "Cornelia Weltzien", "Marina M. -C. Höhne"], "title": "LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping", "comment": null, "summary": "High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.", "AI": {"tldr": "本文针对油菜等复杂作物缺乏鲁棒的叶片追踪方法和大规模数据集的问题，提出了CanolaTrack数据集和LeafTrackNet追踪框架。LeafTrackNet结合YOLOv10检测器和MobileNetV3嵌入网络，在CanolaTrack数据集上实现了显著的性能提升，为植物表型分析提供了新标准。", "motivation": "高分辨率的个体叶片表型分析对植物发育和胁迫响应至关重要，但缺乏鲁棒的叶片追踪方法，尤其对于油菜等结构复杂的作物。现有植物追踪方法受限于小规模物种或受限成像条件，而通用多目标追踪（MOT）方法不适用于动态生物场景。此外，缺乏在真实条件下捕获的大规模数据集也阻碍了准确叶片追踪模型的发展。", "method": "本文引入了CanolaTrack，一个包含5,704张RGB图像和31,840个标注叶片实例的新基准数据集，涵盖184株油菜植株的早期生长阶段。为实现准确的叶片时间追踪，提出LeafTrackNet，一个高效框架，结合基于YOLOv10的叶片检测器和基于MobileNetV3的嵌入网络。在推理过程中，通过基于嵌入的记忆关联策略来维持叶片身份。", "result": "LeafTrackNet在CanolaTrack数据集上，相比现有植物专用追踪器和最先进的MOT基线方法，实现了9%的HOTA（High-Order Tracking Accuracy）性能提升。", "conclusion": "这项工作为真实条件下的叶片级追踪提供了新标准，并发布了CanolaTrack，这是农业作物叶片追踪领域最大的数据集，将有助于推动未来的植物表型研究。代码和数据集均已公开可用。"}}
{"id": "2512.13175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13175", "abs": "https://arxiv.org/abs/2512.13175", "authors": ["Hongxuan Sun", "Tao Wu"], "title": "Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation", "comment": null, "summary": "Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13191", "abs": "https://arxiv.org/abs/2512.13191", "authors": ["Gong Chen", "Chaokun Zhang", "Pengcheng Lv", "Xiaohui Xie"], "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception", "comment": "Accepted by AAAI2026", "summary": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13192", "abs": "https://arxiv.org/abs/2512.13192", "authors": ["Zhuo Chen", "Chengqun Yang", "Zhuo Su", "Zheng Lv", "Jingnan Gao", "Xiaoyuan Zhang", "Xiaokang Yang", "Yichao Yan"], "title": "POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling", "comment": "19 pages, 19 figures", "summary": "Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining \"chicken-and-egg\" cycle for scalable and reproducible portrait illumination.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13238", "abs": "https://arxiv.org/abs/2512.13238", "authors": ["Francesco Ragusa", "Michele Mazzamuto", "Rosario Forte", "Irene D'Ambra", "James Fort", "Jakob Engel", "Antonino Furnari", "Giovanni Maria Farinella"], "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance", "comment": null, "summary": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13247", "abs": "https://arxiv.org/abs/2512.13247", "authors": ["Foivos Paraperas Papantoniou", "Stathis Galanakis", "Rolandos Alexandros Potamias", "Bernhard Kainz", "Stefanos Zafeiriou"], "title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits", "comment": "Project page: https://foivospar.github.io/STARCaster/", "summary": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13276", "abs": "https://arxiv.org/abs/2512.13276", "authors": ["Yan Li", "Lin Liu", "Xiaopeng Zhang", "Wei Xue", "Wenhan Luo", "Yike Guo", "Qi Tian"], "title": "CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing", "comment": null, "summary": "Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13281", "abs": "https://arxiv.org/abs/2512.13281", "authors": ["Jiaqi Wang", "Weijia Wu", "Yi Zhan", "Rui Zhao", "Ming Hu", "James Cheng", "Wei Liu", "Philip Torr", "Kevin Qinghong Lin"], "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "comment": "Code is at https://github.com/video-reality-test/video-reality-tes}, page is at https://video-reality-test.github.io/", "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13313", "abs": "https://arxiv.org/abs/2512.13313", "authors": ["Kling Team", "Jialu Chen", "Yikang Ding", "Zhixue Fang", "Kun Gai", "Yuan Gao", "Kang He", "Jingyun Hua", "Boyuan Jiang", "Mingming Lao", "Xiaohan Li", "Hui Liu", "Jiwen Liu", "Xiaoqiang Liu", "Yuan Liu", "Shun Lu", "Yongsen Mao", "Yingchao Shao", "Huafeng Shi", "Xiaoyu Shi", "Peiqin Sun", "Songlin Tang", "Pengfei Wan", "Chao Wang", "Xuebo Wang", "Haoxian Zhang", "Yuanxing Zhang", "Yan Zhou"], "title": "KlingAvatar 2.0 Technical Report", "comment": "14 pages, 7 figures", "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13285", "abs": "https://arxiv.org/abs/2512.13285", "authors": ["Bo Liu", "Qiao Qin", "Qinghui He"], "title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images", "comment": "9 pages Accepted to AAAI 2026", "summary": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13303", "abs": "https://arxiv.org/abs/2512.13303", "authors": ["Zhihang Liu", "Xiaoyi Bao", "Pandeng Li", "Junjie Zhou", "Zhaohe Liao", "Yefei He", "Kaixun Jiang", "Chen-Wei Xie", "Yun Zheng", "Hongtao Xie"], "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement", "comment": "project page: https://lntzm.github.io/showtable-page/", "summary": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13411", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.13411", "abs": "https://arxiv.org/abs/2512.13411", "authors": ["Patryk Niżeniec", "Marcin Iwanowski"], "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting", "comment": "Code available at: https://patrykni.github.io/UnitySplat2Data/", "summary": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.", "AI": {"tldr": "本文提出了一种新颖的流水线，利用3D高斯溅射和双通道渲染技术，为机器人计算机视觉任务生成大规模、高真实感且自动标注的数据集，并通过混合训练策略显著提升了模型性能。", "motivation": "解决合成图像与真实世界图像之间的领域鸿沟，以及手动标注耗时且效率低下的瓶颈，以支持机器人环境中的计算机视觉任务。", "method": "该方法利用3D高斯溅射（3DGS）创建操作环境和对象的照片级真实感表示。这些资产随后被导入游戏引擎进行物理模拟以生成自然布局。采用新颖的双通道渲染技术，结合溅射的真实感和由代理网格生成的阴影图，算法合成以添加物理上合理的阴影和高光。同时，自动生成像素级精确的分割掩码。", "result": "实验表明，结合少量真实图像与大量合成数据的混合训练策略，能获得最佳的检测和分割性能。", "conclusion": "该方法被证实是有效实现鲁棒和准确模型的最佳策略，能够高效地为机器人计算机视觉任务生成高质量训练数据。"}}
{"id": "2512.13361", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13361", "abs": "https://arxiv.org/abs/2512.13361", "authors": ["Elizaveta Prozorova", "Anton Konev", "Vladimir Faerman"], "title": "Automated User Identification from Facial Thermograms with Siamese Networks", "comment": "5 pages, 2 figures, reported on 21st International Scientific and Practical Conference 'Electronic Means and Control Systems', dedicated to the 80th anniversary of radio engineering education beyond the Urals, Tomsk, 24 November 2025", "summary": "The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.", "AI": {"tldr": "本文分析了基于面部热像图的热成像生物识别技术，比较了红外光谱范围，提出了热像仪的关键要求，并采用暹罗神经网络进行识别，在私有数据集上实现了约80%的准确率，认为热成像在安全系统中有巨大潜力。", "motivation": "研究旨在探索热成像技术在生物识别身份识别中的应用，以克服单一模态的局限性，并开发可靠的安全系统。", "method": "文章对近红外（NIR）、短波红外（SWIR）、中波红外（MWIR）和长波红外（LWIR）光谱范围进行了比较分析；定义了热像仪在生物识别系统中的关键要求（传感器分辨率、热灵敏度、帧率）；提出了使用暹罗神经网络实现自动化识别；在专有数据集上进行了实验；并探讨了结合可见光和红外光谱的混合系统潜力。", "result": "所提出的方法在专有数据集上实现了约80%的识别准确率。研究表明，热成像是一种开发可靠安全系统的有前景技术，并且混合系统具有克服单一模态局限性的潜力。", "conclusion": "热成像技术是开发可靠安全系统的一种有前景的技术，尤其在生物识别领域具有显著潜力。"}}
{"id": "2512.13376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13376", "abs": "https://arxiv.org/abs/2512.13376", "authors": ["Carla Monteiro", "Valentina Corbetta", "Regina Beets-Tan", "Luís F. Teixeira", "Wilson Silva"], "title": "Unlocking Generalization in Polyp Segmentation with DINO Self-Attention \"keys\"", "comment": "29 pages, 10 figures, 8 tables, under review at MIDL 2026", "summary": "Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention \"key\" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13454", "abs": "https://arxiv.org/abs/2512.13454", "authors": ["Arpit Jadon", "Joshua Niemeijer", "Yuki M. Asano"], "title": "Test-Time Modification: Inverse Domain Transformation for Robust Perception", "comment": "Preprint", "summary": "Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.", "AI": {"tldr": "本文提出一种在测试时使用扩散模型的方法，将目标域图像映射回源域分布，以解决域泛化任务中的挑战，并在分割、检测和分类任务上取得了显著改进。", "motivation": "生成式基础模型虽然具有广泛的视觉知识和生成多样图像的能力，但将其用于训练数据增强以合成全面的目标域变体时，存在速度慢、成本高和不完整的问题。这促使研究者寻找一种更高效的域泛化方法。", "method": "该方法在测试时利用扩散模型将目标图像映射回下游模型训练所用的源分布。这种方法仅需要源域描述，保留了任务模型，并避免了大规模合成数据生成。研究还分析了多种生成模型和下游模型，包括集成变体以增强鲁棒性。", "result": "在具有未知目标分布的真实到真实的域泛化场景中，该方法在分割、检测和分类任务上，面对挑战性的环境变化，均实现了持续的性能提升。在BDD100K-Night上相对增益达137%，在ImageNet-R上达68%，在DarkZurich上达62%。", "conclusion": "通过在测试时利用扩散模型将目标图像映射回源分布，该方法为域泛化任务提供了一种有效且高效的解决方案，无需大规模合成数据，并显著提升了模型在不同任务和挑战性环境下的泛化能力。"}}
{"id": "2512.13392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13392", "abs": "https://arxiv.org/abs/2512.13392", "authors": ["Anran Qi", "Changjian Li", "Adrien Bousseau", "Niloy J. Mitra"], "title": "Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs", "comment": null, "summary": "We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/", "AI": {"tldr": "该研究提出了一种图像到视频生成方法，通过引入可编辑的代理动态图（PDG）来分离运动和外观合成，从而实现对最终帧中被遮挡区域的用户控制和可预测的关节运动。", "motivation": "当前的图像到视频生成管道在生成可预测的、关节式的运动，同时在新暴露区域强制执行用户指定内容方面存在困难。", "method": "核心思想是将运动规范与外观合成分离。引入了一个轻量级、用户可编辑的代理动态图（PDG）来确定性地近似驱动部件运动。一个冻结的扩散先验用于合成遵循该运动的合理外观。该无训练管道允许用户松散地标注和重新定位PDG，从中计算密集的运动流，利用扩散模型作为运动引导的着色器。用户可以在图像的被遮挡区域编辑外观，并利用PDG编码的可见性信息执行潜在空间合成，以协调这些区域的运动与用户意图。", "result": "该设计在无需微调的情况下实现了可控的关节运动和对被遮挡区域的用户控制。与现有最先进的方法相比，在将图像转化为关节对象、家具、车辆和可变形物的短视频方面显示出明显优势。该方法将生成式控制（松散的姿态和结构）与可预测控制（最终帧被遮挡区域的外观规范）相结合，解锁了一种新的图像到视频工作流程。", "conclusion": "该方法通过分离运动和外观合成，并允许用户明确控制被遮挡区域，实现了可控的关节运动和用户对图像到视频生成的控制，从而提供了一种新的工作流程。"}}
{"id": "2512.13415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13415", "abs": "https://arxiv.org/abs/2512.13415", "authors": ["Ahmed Abul Hasanaath", "Hamzah Luqman"], "title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition", "comment": null, "summary": "Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13495", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13495", "abs": "https://arxiv.org/abs/2512.13495", "authors": ["Jiangning Zhang", "Junwei Zhu", "Zhenye Gan", "Donghao Luo", "Chuming Lin", "Feifan Xu", "Xu Peng", "Jianlong Hu", "Yuansen Liu", "Yijia Hong", "Weijian Cao", "Han Feng", "Xu Chen", "Chencan Fu", "Keke He", "Xiaobin Hu", "Chengjie Wang"], "title": "Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation", "comment": "Project page: https://zhangzjn.github.io/projects/Soul/", "summary": "We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/", "AI": {"tldr": "本文提出了一个名为Soul的多模态驱动框架，能从单帧图像、文本和音频输入生成高保真、长期的数字人动画，实现精确的唇形同步、生动的面部表情和稳定的身份保持。", "motivation": "现有方法在从少量输入（单帧图像、文本、音频）生成高质量、长期一致的数字人动画方面存在挑战，且面临数据稀缺问题。", "method": "Soul框架基于Wan2.2-5B骨干网络，整合了音频注入层、多种训练策略和阈值感知码本替换，以确保长期生成的一致性。为解决数据稀缺问题，构建了包含百万级精确标注样本的Soul-1M数据集。同时，设计了Soul-Bench用于全面公平的评估。此外，通过步进/CFG蒸馏和轻量级VAE优化推理效率。", "result": "Soul在视频质量、视频-文本对齐、身份保持和唇形同步精度方面显著优于当前领先的开源和商业模型。推理效率提升了11.4倍，且质量损失可忽略不计。", "conclusion": "Soul为高保真、长期数字人动画提供了一个高效且高质量的解决方案，在虚拟主播和电影制作等实际场景中具有广泛的应用前景。"}}
{"id": "2512.13416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13416", "abs": "https://arxiv.org/abs/2512.13416", "authors": ["Haoxuan Qu", "Qiuchi Xiang", "Yujun Cai", "Yirui Wu", "Majid Mirmehdi", "Hossein Rahmani", "Jun Liu"], "title": "Learning to Generate Cross-Task Unexploitable Examples", "comment": null, "summary": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13421", "abs": "https://arxiv.org/abs/2512.13421", "authors": ["Qingyu Shi", "Size Wu", "Jinbin Bai", "Kaidong Yu", "Yujing Wang", "Yunhai Tong", "Xiangtai Li", "Xuelong Li"], "title": "RecTok: Reconstruction Distillation along Rectified Flow", "comment": null, "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13427", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13427", "abs": "https://arxiv.org/abs/2512.13427", "authors": ["Noa Cohen", "Nurit Spingarn-Eliezer", "Inbar Huberman-Spiegelglas", "Tomer Michaeli"], "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models", "comment": "Code and examples are available on the project's webpage at https://noa-cohen.github.io/MineTheGap/", "summary": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13428", "abs": "https://arxiv.org/abs/2512.13428", "authors": ["Anika Islam", "Tasfia Tahsin", "Zaarin Anjum", "Md. Bakhtiar Hasan", "Md. Hasanul Kabir"], "title": "A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification", "comment": null, "summary": "Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13465", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13465", "abs": "https://arxiv.org/abs/2512.13465", "authors": ["Ruiyan Wang", "Teng Hu", "Kaihui Huang", "Zihan Su", "Ran Yi", "Lizhuang Ma"], "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence", "comment": null, "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13440", "abs": "https://arxiv.org/abs/2512.13440", "authors": ["Thalyssa Baiocco-Rodrigues", "Antoine Olivier", "Reda Belbahri", "Thomas Duboudin", "Pierre-Antoine Bannier", "Benjamin Adjadj", "Katharina Von Loga", "Nathan Noiry", "Maxime Touzot", "Hector Roux de Bezieux"], "title": "IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images", "comment": null, "summary": "As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13560", "abs": "https://arxiv.org/abs/2512.13560", "authors": ["Shun Maeda", "Chunzhi Gu", "Koichiro Kamide", "Katsuya Hotta", "Shangce Gao", "Chao Zhang"], "title": "3D Human-Human Interaction Anomaly Detection", "comment": null, "summary": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.", "AI": {"tldr": "本文提出了一项新任务——人机交互异常检测（H2IAD），旨在识别协作3D人体动作中的异常交互行为，并为此提出了一种名为IADNet的新型网络，该网络通过共享时间注意力与距离关系编码有效捕捉交互动态，并在实验中表现出优越性能。", "motivation": "现有的人体异常检测（AD）主要关注单个个体的异常行为，但人类行为本质上是协作的，异常也可能源于人际互动。现有单人AD模型无法捕捉复杂的非对称交互动态，导致检测人际交互异常时准确性低。", "method": "本文提出了人机交互异常检测（H2IAD）任务。为解决H2IAD，作者提出了交互异常检测网络（IADNet），其核心是时间注意力共享模块（TASM），通过共享编码的运动嵌入来同步协作运动相关性。此外，引入了基于距离的关系编码模块（DREM）来反映人际交互中的空间配置和社会线索。最终采用归一化流进行异常评分。", "result": "在人机运动基准测试上的大量实验表明，IADNet在H2IAD任务中优于现有的人体中心异常检测基线模型。", "conclusion": "本文引入了人机交互异常检测（H2IAD）这一新任务，并提出了IADNet网络来有效识别协作3D人体动作中的异常交互行为。IADNet通过专门设计的模块捕捉时间动态和空间配置，在实验中证明了其优越性，为该领域提供了新的解决方案。"}}
{"id": "2512.13492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13492", "abs": "https://arxiv.org/abs/2512.13492", "authors": ["Jiangning Zhang", "Junwei Zhu", "Teng Hu", "Yabiao Wang", "Donghao Luo", "Weijian Cao", "Zhenye Gan", "Xiaobin Hu", "Zhucun Xue", "Chengjie Wang"], "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$", "comment": "Project page: https://zhangzjn.github.io/projects/T3-Video", "summary": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13507", "abs": "https://arxiv.org/abs/2512.13507", "authors": ["Siyan Chen", "Yanfei Chen", "Ying Chen", "Zhuo Chen", "Feng Cheng", "Xuyan Chi", "Jian Cong", "Qinpeng Cui", "Qide Dong", "Junliang Fan", "Jing Fang", "Zetao Fang", "Chengjian Feng", "Han Feng", "Mingyuan Gao", "Yu Gao", "Qiushan Guo", "Boyang Hao", "Qingkai Hao", "Bibo He", "Qian He", "Tuyen Hoang", "Ruoqing Hu", "Xi Hu", "Weilin Huang", "Zhaoyang Huang", "Zhongyi Huang", "Siqi Jiang", "Wei Jiang", "Yunpu Jiang", "Zhuo Jiang", "Ashley Kim", "Jianan Kong", "Zhichao Lai", "Shanshan Lao", "Ai Li", "Feiya Li", "Gen Li", "Huixia Li", "JiaShi Li", "Liang Li", "Ming Li", "Tao Li", "Xian Li", "Xiaojie Li", "Xiaoyang Li", "Xingxing Li", "Yameng Li", "Yifu Li", "Yiying Li", "Chao Liang", "Ying Liang", "Zhiqiang Liang", "Wang Liao", "Yalin Liao", "Heng Lin", "Kengyu Lin", "Shanchuan Lin", "Xi Lin", "Zhijie Lin", "Feng Ling", "Fangfang Liu", "Gaohong Liu", "Jiawei Liu", "Jie Liu", "Shouda Liu", "Shu Liu", "Sichao Liu", "Songwei Liu", "Xin Liu", "Xue Liu", "Yibo Liu", "Zikun Liu", "Zuxi Liu", "Junlin Lyu", "Lecheng Lyu", "Qian Lyu", "Han Mu", "Xiaonan Nie", "Jingzhe Ning", "Xitong Pan", "Yanghua Peng", "Lianke Qin", "Xueqiong Qu", "Yuxi Ren", "Yuchen Shen", "Guang Shi", "Lei Shi", "Yan Song", "Yinglong Song", "Fan Sun", "Li Sun", "Renfei Sun", "Zeyu Sun", "Wenjing Tang", "Zirui Tao", "Feng Wang", "Furui Wang", "Jinran Wang", "Junkai Wang", "Ke Wang", "Kexin Wang", "Qingyi Wang", "Rui Wang", "Sen Wang", "Shuai Wang", "Tingru Wang", "Weichen Wang", "Xin Wang", "Yanhui Wang", "Yue Wang", "Yuping Wang", "Yuxuan Wang", "Ziyu Wang", "Guoqiang Wei", "Wanru Wei", "Di Wu", "Guohong Wu", "Hanjie Wu", "Jian Wu", "Jie Wu", "Ruolan Wu", "Xinglong Wu", "Yonghui Wu", "Ruiqi Xia", "Liang Xiang", "Fei Xiao", "XueFeng Xiao", "Pan Xie", "Shuangyi Xie", "Shuang Xu", "Jinlan Xue", "Bangbang Yang", "Ceyuan Yang", "Jiaqi Yang", "Runkai Yang", "Tao Yang", "Yang Yang", "Yihang Yang", "ZhiXian Yang", "Ziyan Yang", "Yifan Yao", "Zilyu Ye", "Bowen Yu", "Chujie Yuan", "Linxiao Yuan", "Sichun Zeng", "Weihong Zeng", "Xuejiao Zeng", "Yan Zeng", "Chuntao Zhang", "Heng Zhang", "Jingjie Zhang", "Kuo Zhang", "Liang Zhang", "Liying Zhang", "Manlin Zhang", "Ting Zhang", "Weida Zhang", "Xiaohe Zhang", "Xinyan Zhang", "Yan Zhang", "Yuan Zhang", "Zixiang Zhang", "Fengxuan Zhao", "Huating Zhao", "Yang Zhao", "Hao Zheng", "Jianbin Zheng", "Xiaozheng Zheng", "Yangyang Zheng", "Yijie Zheng", "Jiexin Zhou", "Kuan Zhu", "Shenhan Zhu", "Wenjia Zhu", "Benhui Zou", "Feilong Zuo"], "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "comment": "Seedance 1.5 pro Technical Report", "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13511", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13511", "abs": "https://arxiv.org/abs/2512.13511", "authors": ["Piyush Bagad", "Andrew Zisserman"], "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding", "comment": "18 Pages. Project page at http://bpiyush.github.io/tara-website", "summary": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13604", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13604", "abs": "https://arxiv.org/abs/2512.13604", "authors": ["Jianxiong Gao", "Zhaoxi Chen", "Xian Liu", "Junhao Zhuang", "Chengming Xu", "Jianfeng Feng", "Yu Qiao", "Yanwei Fu", "Chenyang Si", "Ziwei Liu"], "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "comment": "Project Page: https://vchitect.github.io/LongVie2-project/", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "AI": {"tldr": "LongVie 2是一个渐进式自回归框架，通过多模态引导、退化感知训练和历史上下文引导，显著提升了视频世界模型的控制性、长期视觉质量和时间一致性，并支持长达五分钟的连续视频生成。同时引入了LongVGenBench基准。", "motivation": "在预训练视频生成系统上构建视频世界模型是迈向通用时空智能的重要一步，但面临控制性、长期视觉质量和时间一致性方面的挑战。", "method": "本文提出了LongVie 2，一个端到端自回归框架，分三个阶段训练：1) 多模态引导，整合密集和稀疏控制信号以提高可控性；2) 输入帧的退化感知训练，弥合训练与长期推理之间的差距以保持视觉质量；3) 历史上下文引导，对齐相邻片段的上下文信息以确保时间一致性。此外，还引入了LongVGenBench，一个包含100个高分辨率一分钟视频的综合基准。", "result": "LongVie 2在长距离可控性、时间连贯性和视觉保真度方面达到了最先进的性能，并支持长达五分钟的连续视频生成。", "conclusion": "LongVie 2是朝着统一视频世界建模迈出的重要一步，展示了在实现视频世界模型所需的核心属性方面的显著进展。"}}
{"id": "2512.13534", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13534", "abs": "https://arxiv.org/abs/2512.13534", "authors": ["Marianne Rakic", "Siyu Gai", "Etienne Chollet", "John V. Guttag", "Adrian V. Dalca"], "title": "Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains", "comment": "Accepted at NeurIPS 2025. Code available at: https://github.com/mariannerakic/Pancakes", "summary": "A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13573", "abs": "https://arxiv.org/abs/2512.13573", "authors": ["Tao Zhang", "Ziqi Zhang", "Zongyang Ma", "Yuxin Chen", "Bing Li", "Chunfeng Yuan", "Guangting Wang", "Fengyun Rao", "Ying Shan", "Weiming Hu"], "title": "MMhops-R1: Multimodal Multi-hop Reasoning", "comment": "Acceped by AAAI 2026", "summary": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.", "AI": {"tldr": "本文提出了MMhops，一个用于评估和促进多模态多跳推理的大规模基准测试，并提出了MMhops-R1，一个基于强化学习的多模态检索增强生成框架，以解决其挑战。", "motivation": "现有的多模态大型语言模型（MLLMs）主要局限于单步推理，且现有基准缺乏评估和推动多跳能力的复杂性，这限制了它们解决复杂现实世界挑战的能力。", "method": "本文引入了MMhops，一个包含“桥接”和“比较”两种任务格式的新型大规模基准测试，要求模型整合外部知识构建复杂推理链。为应对MMhops的挑战，提出了MMhops-R1，一个利用强化学习优化模型自主规划推理路径、制定查询和综合多层次信息的多模态检索增强生成（mRAG）框架。", "result": "实验证明，MMhops-R1在MMhops上显著优于现有强基线模型，表明动态规划和多模态知识整合对于复杂推理至关重要。此外，MMhops-R1对固定跳数推理任务也表现出强大的泛化能力。", "conclusion": "本文贡献了一个具有挑战性的新基准和强大的基线模型，并计划发布相关代码、数据和权重，以促进该关键领域未来的研究。"}}
{"id": "2512.13665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13665", "abs": "https://arxiv.org/abs/2512.13665", "authors": ["Wenhan Chen", "Sezer Karaoglu", "Theo Gevers"], "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency", "comment": null, "summary": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.", "AI": {"tldr": "该研究提出Grab-3D，一个基于3D几何时间一致性的几何感知Transformer框架，通过利用消失点来检测AI生成视频，并显著优于现有技术。", "motivation": "现有AI视频检测方法对生成视频中存在的3D几何模式探索有限，导致检测可靠性不足。研究旨在通过深入挖掘3D几何模式来提高检测性能。", "method": "该论文使用消失点作为3D几何模式的显式表示，揭示真实视频与AI生成视频在几何一致性上的差异。提出Grab-3D，一个几何感知的Transformer框架，用于检测AI生成视频，其核心是3D几何时间一致性。为实现可靠评估，构建了一个静态场景的AI生成视频数据集，以支持稳定的3D几何特征提取。Grab-3D配备了几何位置编码、时空几何注意力机制和基于EMA的几何分类器头部，将3D几何感知显式注入到时间建模中。", "result": "实验结果表明，Grab-3D显著优于最先进的检测器，并且对未见过的生成器表现出强大的跨域泛化能力。", "conclusion": "Grab-3D通过有效利用3D几何时间一致性，为AI生成视频检测提供了一种可靠且高性能的解决方案，展现出卓越的性能和泛化能力。"}}
{"id": "2512.13597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13597", "abs": "https://arxiv.org/abs/2512.13597", "authors": ["Christophe Bolduc", "Julien Philip", "Li Ma", "Mingming He", "Paul Debevec", "Jean-François Lalonde"], "title": "Lighting in Motion: Spatiotemporal HDR Lighting Estimation", "comment": null, "summary": "We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13680", "abs": "https://arxiv.org/abs/2512.13680", "authors": ["Tianye Ding", "Yiming Xie", "Yiqing Liang", "Moitreya Chatterjee", "Pedro Miraldo", "Huaizu Jiang"], "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction", "comment": "16 pages", "summary": "Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$", "AI": {"tldr": "LASER是一个无需训练的框架，通过层级尺度对齐，将离线重建模型转换为流式系统，解决了深度错位问题，实现了高性能和低内存消耗。", "motivation": "现有的前馈离线重建模型（如VGGT和$π^3$）重建质量高，但由于二次方内存复杂度无法处理流式视频。现有流式方法需要大量重新训练，且未能充分利用离线模型的几何先验。", "method": "本文提出了LASER，一个无需训练的框架，通过对齐连续时间窗口间的预测，将离线重建模型转换为流式系统。针对简单的相似变换对齐因单目尺度模糊导致的层深度错位问题，引入了层级尺度对齐：将深度预测分割成离散层，计算每层的尺度因子，并在相邻窗口和时间戳之间传播。", "result": "LASER在相机姿态估计和点云图重建质量上达到了最先进的水平，同时在RTX A6000 GPU上以14 FPS运行，峰值内存为6 GB，使得千米级流式视频的实际部署成为可能。", "conclusion": "LASER通过创新的层级尺度对齐方法，成功地将高性能离线重建模型转换为实用的流式系统，无需重新训练，解决了内存和尺度一致性问题，为大规模流式视频重建提供了高效解决方案。"}}
{"id": "2512.13608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13608", "abs": "https://arxiv.org/abs/2512.13608", "authors": ["Felix J. Dorfner", "Manon A. Dorster", "Ryan Connolly", "Oscar Gentilhomme", "Edward Gibbs", "Steven Graham", "Seth Wander", "Thomas Schultz", "Manisha Bahl", "Dania Daye", "Albert E. Kim", "Christopher P. Bridge"], "title": "DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis", "comment": null, "summary": "Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.\n  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.\n  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.\n  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\\% compared to Dinov2's 77.3\\%.\n  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.", "AI": {"tldr": "本文开发了首个用于数字乳腺断层合成（DBT）的预训练基础模型DBT-DINO，在乳腺密度分类和乳腺癌风险预测方面表现出色，但在病灶检测方面，领域特定预训练效果不一。", "motivation": "尽管基础模型在医学影像领域前景广阔，但其在三维影像模式（特别是数字乳腺断层合成DBT）中的应用尚未得到充分探索。目前尚无针对DBT的基础模型，尽管DBT已用于乳腺癌筛查。", "method": "研究采用DINOv2方法进行自监督预训练，使用了来自27,990名患者的487,975个DBT体量（超过2500万张2D切片）。评估了三个下游任务：1) 使用5,000次筛查的乳腺密度分类；2) 使用106,417次筛查的五年内乳腺癌发病风险预测；3) 使用393个带注释体量的病灶检测。", "result": "在乳腺密度分类任务中，DBT-DINO准确率为0.79，优于MetaAI DINOv2基线（0.73）和DenseNet-121（0.74）。在五年乳腺癌风险预测任务中，DBT-DINO的AUROC为0.78，与DINOv2的0.76相当。在病灶检测任务中，DINOv2的平均敏感度更高（0.67），而DBT-DINO为0.62。然而，DBT-DINO在检测癌性病灶方面表现更好，检测率为78.8%，而DINOv2为77.3%。", "conclusion": "本研究利用前所未有的大规模数据集，开发了首个DBT基础模型DBT-DINO，并在乳腺密度分类和癌症风险预测方面展现出强大性能。然而，领域特定预训练在检测任务上的益处具有可变性，ImageNet基线在一般病灶检测上优于DBT-DINO，表明局部检测任务需要进一步的方法学发展。"}}
{"id": "2512.13639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13639", "abs": "https://arxiv.org/abs/2512.13639", "authors": ["Michal Nazarczuk", "Thomas Tanay", "Arthur Moreau", "Zhensong Zhang", "Eduardo Pérez-Pellitero"], "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All", "comment": "Project page: https://charge-benchmark.github.io/", "summary": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.", "AI": {"tldr": "本文介绍了一个新的数据集，该数据集从高质量动画电影中生成，用于新视角合成，包含动态场景、多模态数据和多种相机设置。", "motivation": "现有研究需要高质量、逼真的动态场景数据集来训练和评估先进的4D场景重建和新视角生成模型。", "method": "该研究从一部高质量动画电影中生成数据集，提供高保真RGB图像以及深度、表面法线、对象分割和光流等互补模态。数据集组织为密集多视角、稀疏相机和单目视频三种基准场景。", "result": "创建了一个视觉丰富、高质量标注且实验设置多样化的数据集，适用于新视角合成和3D视觉研究。", "conclusion": "该数据集为推动新视角合成和3D视觉领域的发展提供了独特的资源，因为它结合了视觉丰富性、高质量标注和多样的实验设置。"}}
{"id": "2512.13609", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13609", "abs": "https://arxiv.org/abs/2512.13609", "authors": ["Shweta Mahajan", "Shreya Kadambi", "Hoang Le", "Munawar Hayat", "Fatih Porikli"], "title": "Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models", "comment": null, "summary": "We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13635", "abs": "https://arxiv.org/abs/2512.13635", "authors": ["Junchao Zhu", "Ruining Deng", "Junlin Guo", "Tianyuan Yao", "Chongyu Qu", "Juming Xiong", "Siqi Lu", "Zhengyi Lu", "Yanfan Zhu", "Marilyn Lionts", "Yuechen Yang", "Yalin Zheng", "Yu Wang", "Shilin Zhao", "Haichun Yang", "Yuankai Huo"], "title": "SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning", "comment": null, "summary": "Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13671", "abs": "https://arxiv.org/abs/2512.13671", "authors": ["Junwen Miao", "Penghui Du", "Yi Liu", "Yu Wang", "Yan Wang"], "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection", "comment": null, "summary": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.", "AI": {"tldr": "AgentIAD是一个工具驱动的智能体框架，通过感知缩放和比较检索实现多阶段视觉检测，解决工业异常检测中样本稀缺和缺陷细微的问题，在MMAD数据集上达到SOTA性能。", "motivation": "工业异常检测面临正常样本稀缺、缺陷细微且局部化的问题。现有的单程视觉-语言模型（VLMs）常忽略小型异常，且缺乏与规范正常模式进行显式比较的机制。", "method": "本文提出了AgentIAD，一个工具驱动的智能体框架，支持多阶段视觉检测。该智能体配备了感知缩放器（PZ）用于局部细粒度分析，以及比较检索器（CR）用于在证据模糊时查询正常样本。通过构建MMAD数据集中的结构化感知和比较轨迹，模型分两阶段训练：监督微调和强化学习。奖励设计分为两部分：感知奖励（监督分类准确性、空间对齐和类型正确性）和行为奖励（鼓励高效工具使用）。", "result": "AgentIAD在MMAD数据集上实现了97.62%的分类准确率，创下新的SOTA，超越了以往基于MLLM的方法，并生成了透明且可解释的检测轨迹。", "conclusion": "AgentIAD通过分步观察、缩放和验证来精炼判断，有效解决了工业异常检测的挑战，取得了领先的性能和可解释性。"}}
{"id": "2512.13677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13677", "abs": "https://arxiv.org/abs/2512.13677", "authors": ["Xiaohu Huang", "Hao Zhou", "Qiangpeng Yang", "Shilei Wen", "Kai Han"], "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation", "comment": "Project page: \\url{https://visual-ai.github.io/jova}", "summary": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.", "AI": {"tldr": "本文提出了JoVA，一个统一的视频-音频联合生成框架，通过联合自注意力机制和口部区域损失，实现了高质量的唇语同步语音和环境音生成。", "motivation": "现有方法存在两个主要局限：一是无法生成与唇部动作同步的人类语音，只能生成环境音；二是统一的视频-音频生成方法通常依赖显式融合或模态特定对齐模块，增加了架构复杂性。", "method": "JoVA在每个Transformer层中采用视频和音频token的联合自注意力机制，实现直接高效的跨模态交互，无需额外对齐模块。此外，引入了一个基于面部关键点检测的简单而有效的口部区域损失，以增强对关键口部区域的监督，从而实现高质量的唇语同步。", "result": "在基准测试中，JoVA在唇语同步准确性、语音质量和整体视频-音频生成保真度方面，优于或与现有最先进的统一和音频驱动方法相当。", "conclusion": "JoVA是一个优雅且高效的框架，用于高质量的多模态生成，解决了现有方法的关键限制。"}}
{"id": "2512.13684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13684", "abs": "https://arxiv.org/abs/2512.13684", "authors": ["Daniel Zoran", "Nikhil Parthasarathy", "Yi Yang", "Drew A Hudson", "Joao Carreira", "Andrew Zisserman"], "title": "Recurrent Video Masked Autoencoders", "comment": null, "summary": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.", "AI": {"tldr": "本文提出了循环视频掩码自编码器（RVM），这是一种新颖的视频表征学习方法，它利用基于Transformer的循环神经网络捕捉时空结构，并通过非对称掩码预测任务学习，实现了高效率和在多种视频及图像任务上的强大性能。", "motivation": "需要一种能有效捕捉自然视频数据时空结构、克服标准时空注意力架构局限性（如计算成本）并提高参数效率的视频表征学习方法。", "method": "RVM采用基于Transformer的循环神经网络来随时间聚合密集图像特征，以有效捕获视频的时空结构。它通过非对称掩码预测任务进行学习，并仅需标准的像素重建目标。", "result": "RVM在动作识别和点/对象跟踪等视频级任务上与最先进的视频模型（如VideoMAE、V-JEPA）表现相当；在测试几何和密集空间理解的任务上优于图像模型（如DINOv2）。RVM在小模型下无需知识蒸馏即能取得强大性能，参数效率比竞争性视频掩码自编码器高出30倍。其循环特性允许在长时间范围内稳定传播特征，且计算成本呈线性增长。定性可视化显示RVM学习到了丰富的场景语义、结构和运动表征。", "conclusion": "RVM是一种高效的“通用型”编码器，它通过循环结构有效捕捉视频时空信息，在多种视频和图像任务上均表现出色，并在参数和计算效率方面显著优于现有方法，为视频表征学习提供了一个有前景的解决方案。"}}
{"id": "2512.13683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13683", "abs": "https://arxiv.org/abs/2512.13683", "authors": ["Lu Ling", "Yunhao Ge", "Yichen Sheng", "Aniket Bera"], "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners", "comment": null, "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/", "AI": {"tldr": "本文通过重新编程预训练的3D实例生成器，使其作为场景级学习器，利用模型中心的空间监督而非数据集监督，解决了交互式3D场景生成中泛化能力不足的问题，实现了对未知布局和新颖物体组合的泛化。", "motivation": "交互式3D场景生成的核心挑战是泛化能力。现有基于学习的方法将空间理解局限于有限的场景数据集，限制了对新布局的泛化。", "method": "研究者重新编程了一个预训练的3D实例生成器，使其充当场景级学习器。通过模型中心的空间监督取代了数据集受限的监督。采用以视图为中心的场景空间公式，而非广泛使用的规范空间，使得场景生成器能够直接从实例模型中学习空间关系，形成一个全前馈、可泛化的场景生成器。", "result": "重新编程后的生成器展现出可迁移的空间知识，能够泛化到未见过的布局和新颖的物体组合。即使在训练场景由随机组合的物体构成时，空间推理能力仍然出现，表明生成器的可迁移场景先验为从纯几何线索推断邻近、支撑和对称提供了丰富的学习信号。定量和定性结果表明，3D实例生成器是一个隐含的空间学习器和推理器。", "conclusion": "3D实例生成器可以作为隐含的空间学习器和推理器，这为交互式3D场景理解和生成的基础模型指明了方向。"}}
{"id": "2512.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13687", "abs": "https://arxiv.org/abs/2512.13687", "authors": ["Jingfeng Yao", "Yuda Song", "Yucong Zhou", "Xinggang Wang"], "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "comment": "Our pre-trained models are available at https://github.com/MiniMax-AI/VTP", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2512.13689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13689", "abs": "https://arxiv.org/abs/2512.13689", "authors": ["Yuanwen Yue", "Damien Robert", "Jianyuan Wang", "Sunghwan Hong", "Jan Dirk Wegner", "Christian Rupprecht", "Konrad Schindler"], "title": "LitePT: Lighter Yet Stronger Point Transformer", "comment": "Project page: https://litept.github.io/", "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
