{"id": "2508.18612", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18612", "abs": "https://arxiv.org/abs/2508.18612", "authors": ["Soumen Ghosh", "Christine Jestin Hannan", "Rajat Vashistha", "Parveen Kundu", "Sandra Brosda", "Lauren G. Aoude", "James Lonie", "Andrew Nathanson", "Jessica Ng", "Andrew P. Barbour", "Viktor Vegh"], "title": "Stress-testing cross-cancer generalizability of 3D nnU-Net for PET-CT tumor segmentation: multi-cohort evaluation with novel oesophageal and lung cancer datasets", "comment": null, "summary": "Robust generalization is essential for deploying deep learning based tumor\nsegmentation in clinical PET-CT workflows, where anatomical sites, scanners,\nand patient populations vary widely. This study presents the first cross cancer\nevaluation of nnU-Net on PET-CT, introducing two novel, expert-annotated\nwhole-body datasets. 279 patients with oesophageal cancer (Australian cohort)\nand 54 with lung cancer (Indian cohort). These cohorts complement the public\nAutoPET dataset and enable systematic stress-testing of cross domain\nperformance. We trained and tested 3D nnUNet models under three paradigms.\nTarget only (oesophageal), public only (AutoPET), and combined training. For\nthe tested sets, the oesophageal only model achieved the best in-domain\naccuracy (mean DSC, 57.8) but failed on external Indian lung cohort (mean DSC\nless than 3.4), indicating severe overfitting. The public only model\ngeneralized more broadly (mean DSC, 63.5 on AutoPET, 51.6 on Indian lung\ncohort) but underperformed in oesophageal Australian cohort (mean DSC, 26.7).\nThe combined approach provided the most balanced results (mean DSC, lung\n(52.9), oesophageal (40.7), AutoPET (60.9)), reducing boundary errors and\nimproving robustness across all cohorts. These findings demonstrate that\ndataset diversity, particularly multi demographic, multi center and multi\ncancer integration, outweighs architectural novelty as the key driver of robust\ngeneralization. This work presents the demography based cross cancer deep\nlearning segmentation evaluation and highlights dataset diversity, rather than\nmodel complexity, as the foundation for clinically robust segmentation.", "AI": {"tldr": "本研究通过跨癌种评估，发现数据集多样性（多人口、多中心、多癌种）是实现PET-CT肿瘤分割模型临床鲁棒泛化的关键，而非模型架构的创新。", "motivation": "在临床PET-CT工作流程中，解剖部位、扫描仪和患者群体差异很大，因此深度学习肿瘤分割模型需要具备鲁棒的泛化能力。本研究旨在首次对PET-CT上的nnU-Net进行跨癌种评估。", "method": "研究引入了两个新的、由专家标注的全身体数据集（279名食管癌患者，54名肺癌患者），并结合公共AutoPET数据集。在三种范式下训练和测试了3D nnUNet模型：仅目标数据集训练（食管癌）、仅公共数据集训练（AutoPET）、以及联合训练。", "result": "仅目标数据集训练的模型在域内表现最佳（平均DSC 57.8），但在外部肺癌队列上表现极差（平均DSC低于3.4），表明严重过拟合。仅公共数据集训练的模型泛化能力更广（AutoPET上平均DSC 63.5，印度肺癌队列上51.6），但在食管癌队列上表现不佳（平均DSC 26.7）。联合训练方法在所有队列中提供了最平衡的结果（肺癌平均DSC 52.9，食管癌40.7，AutoPET 60.9），减少了边界误差并提高了鲁棒性。", "conclusion": "研究结果表明，数据集多样性，特别是多人口统计学、多中心和多癌种的整合，是实现鲁棒泛化的关键驱动因素，其重要性超过了架构创新。数据集多样性，而非模型复杂性，是实现临床鲁棒分割的基础。"}}
{"id": "2508.18613", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18613", "abs": "https://arxiv.org/abs/2508.18613", "authors": ["Eichi Takaya", "Ryusei Inamori"], "title": "ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised Contrastive Pretraining for Medical Imaging", "comment": null, "summary": "Background and objective: Expert annotations limit large-scale supervised\npretraining in medical imaging, while ubiquitous metadata (modality, anatomical\nregion) remain underused. We introduce ModAn-MulSupCon, a modality- and\nanatomy-aware multi-label supervised contrastive pretraining method that\nleverages such metadata to learn transferable representations.\n  Method: Each image's modality and anatomy are encoded as a multi-hot vector.\nA ResNet-18 encoder is pretrained on a mini subset of RadImageNet (miniRIN,\n16,222 images) with a Jaccard-weighted multi-label supervised contrastive loss,\nand then evaluated by fine-tuning and linear probing on three binary\nclassification tasks--ACL tear (knee MRI), lesion malignancy (breast\nultrasound), and nodule malignancy (thyroid ultrasound).\n  Result: With fine-tuning, ModAn-MulSupCon achieved the best AUC on MRNet-ACL\n(0.964) and Thyroid (0.763), surpassing all baselines ($p<0.05$), and ranked\nsecond on Breast (0.926) behind SimCLR (0.940; not significant). With the\nencoder frozen, SimCLR/ImageNet were superior, indicating that ModAn-MulSupCon\nrepresentations benefit most from task adaptation rather than linear\nseparability.\n  Conclusion: Encoding readily available modality/anatomy metadata as\nmulti-label targets provides a practical, scalable pretraining signal that\nimproves downstream accuracy when fine-tuning is feasible. ModAn-MulSupCon is a\nstrong initialization for label-scarce clinical settings, whereas\nSimCLR/ImageNet remain preferable for frozen-encoder deployments.", "AI": {"tldr": "ModAn-MulSupCon是一种利用模态和解剖学元数据进行多标签监督对比预训练的方法，在医疗影像下游任务中，通过微调显著提高了性能，尤其适用于标签稀缺的临床环境。", "motivation": "医疗影像领域中，专家标注限制了大规模监督预训练，而普遍存在的元数据（如模态、解剖区域）却未被充分利用。", "method": "将每张图像的模态和解剖学信息编码为多热向量。使用Jaccard加权多标签监督对比损失，在RadImageNet的一个小型子集（miniRIN，16,222张图像）上预训练ResNet-18编码器。通过在三个二分类任务（ACL撕裂、乳腺病变恶性、甲状腺结节恶性）上进行微调和线性探测来评估其性能。", "result": "通过微调，ModAn-MulSupCon在MRNet-ACL（0.964）和甲状腺（0.763）数据集上取得了最佳AUC，优于所有基线（p<0.05），在乳腺数据集上排名第二（0.926），略低于SimCLR（0.940，不显著）。在编码器冻结的情况下，SimCLR/ImageNet表现更优，表明ModAn-MulSupCon的表示在任务适应（微调）中获益最大，而非线性可分性。", "conclusion": "将易于获取的模态/解剖学元数据编码为多标签目标，提供了一种实用、可扩展的预训练信号。在可行微调的情况下，该方法能提高下游任务的准确性。ModAn-MulSupCon是标签稀缺临床环境的强大初始化方法，而SimCLR/ImageNet更适用于冻结编码器的部署。"}}
{"id": "2508.18912", "categories": ["eess.IV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18912", "abs": "https://arxiv.org/abs/2508.18912", "authors": ["Mahmoud Dhimish"], "title": "HOTSPOT-YOLO: A Lightweight Deep Learning Attention-Driven Model for Detecting Thermal Anomalies in Drone-Based Solar Photovoltaic Inspections", "comment": null, "summary": "Thermal anomaly detection in solar photovoltaic (PV) systems is essential for\nensuring operational efficiency and reducing maintenance costs. In this study,\nwe developed and named HOTSPOT-YOLO, a lightweight artificial intelligence (AI)\nmodel that integrates an efficient convolutional neural network backbone and\nattention mechanisms to improve object detection. This model is specifically\ndesigned for drone-based thermal inspections of PV systems, addressing the\nunique challenges of detecting small and subtle thermal anomalies, such as\nhotspots and defective modules, while maintaining real-time performance.\nExperimental results demonstrate a mean average precision of 90.8%, reflecting\na significant improvement over baseline object detection models. With a reduced\ncomputational load and robustness under diverse environmental conditions,\nHOTSPOT-YOLO offers a scalable and reliable solution for large-scale PV\ninspections. This work highlights the integration of advanced AI techniques\nwith practical engineering applications, revolutionizing automated fault\ndetection in renewable energy systems.", "AI": {"tldr": "本研究开发并命名了HOTSPOT-YOLO，一个轻量级AI模型，通过集成高效卷积神经网络骨干和注意力机制，显著提高了无人机光伏系统热异常（如热点和缺陷模块）的实时检测精度和效率。", "motivation": "确保太阳能光伏系统（PV）的运行效率和降低维护成本，以及解决无人机热成像检查中检测小型、细微热异常的独特挑战，同时保持实时性能。", "method": "开发了HOTSPOT-YOLO，这是一个轻量级人工智能模型，它集成了高效的卷积神经网络骨干和注意力机制以改进目标检测。该模型专门为无人机光伏系统热成像检查而设计。", "result": "实验结果显示，平均精度（mAP）达到90.8%，相比基线目标检测模型有显著提升。该模型计算负载更低，并在不同环境条件下表现出鲁棒性。", "conclusion": "HOTSPOT-YOLO为大规模光伏检查提供了一个可扩展且可靠的解决方案。这项工作突出了先进AI技术与实际工程应用的结合，革新了可再生能源系统中的自动化故障检测。"}}
{"id": "2508.18968", "categories": ["eess.IV", "cs.MM", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18968", "abs": "https://arxiv.org/abs/2508.18968", "authors": ["Hannah Och", "André Kaup"], "title": "Lossless 4:2:0 Screen Content Coding Using Luma-Guided Soft Context Formation", "comment": "5 pages, 4 figures, 3 tables, accepted to EUSIPCO 2025", "summary": "The soft context formation coder is a pixel-wise state-of-the-art lossless\nscreen content coder using pattern matching and color palette coding in\ncombination with arithmetic coding. It achieves excellent compression\nperformance on screen content images in RGB 4:4:4 format with few distinct\ncolors. In contrast to many other lossless compression methods, it codes entire\ncolor pixels at once, i.e., all color components of one pixel are coded\ntogether. Consequently, it does not natively support image formats with\ndownsampled chroma, such as YCbCr 4:2:0, which is an often used chroma format\nin video compression. In this paper, we extend the soft context formation\ncoding capabilities to 4:2:0 image compression, by successively coding Y and\nCbCr planes based on an analysis of normalized mutual information between image\nplanes. Additionally, we propose an enhancement to the chroma prediction based\non the luminance plane. Furthermore, we propose to transmit side-information\nabout occurring luma-chroma combinations to improve chroma probability\ndistribution modelling. Averaged over a large screen content image dataset, our\nproposed method outperforms HEVC-SCC, with HEVC-SCC needing 5.66% more bitrate\ncompared to our method.", "AI": {"tldr": "本文扩展了软上下文形成编码器（一种先进的无损屏幕内容编码器）以支持YUV 4:2:0格式，并提出了改进的色度预测和概率建模方法，在屏幕内容图像上优于HEVC-SCC。", "motivation": "软上下文形成编码器（Soft Context Formation Coder）在RGB 4:4:4格式的屏幕内容图像上表现出色，但它一次性编码整个像素（所有颜色分量），因此不原生支持如YCbCr 4:2:0等色度欠采样图像格式，而4:2:0是视频压缩中常用的色度格式。此外，现有的色度处理可能还有提升空间。", "method": "本文通过以下方式扩展和改进了编码器：1. 基于图像平面间归一化互信息分析，对Y和CbCr平面进行逐次编码。2. 提出了一种基于亮度平面增强色度预测的方法。3. 建议传输关于出现的亮度-色度组合的辅助信息，以改善色度概率分布建模。", "result": "在一大型屏幕内容图像数据集上，本文提出的方法优于HEVC-SCC，HEVC-SCC需要比本文方法多5.66%的比特率。", "conclusion": "本文成功地将软上下文形成编码器扩展到支持4:2:0图像压缩，并通过改进的色度预测和概率建模技术，显著提升了压缩性能，在屏幕内容图像上超越了HEVC-SCC。"}}
{"id": "2508.18290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18290", "abs": "https://arxiv.org/abs/2508.18290", "authors": ["Hans-Joachim Rudolph"], "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it.", "AI": {"tldr": "本文提出了一种基于复值意义空间中语义吸引子的理论框架，用于构建语义通用人工智能（AGI），通过递归张量变换和旋转语义结构来建模意义，而非统计预测，并引入吸引子引导意义走向连贯性。", "motivation": "当前基于Transformer的语言模型依赖统计学上的下一个词预测，无法真正形成或理解意义，也难以处理讽刺、同音异义和歧义等复杂语义现象。作者旨在开发一种能够主动形成意义、并将其引向稳定和深度的认知架构。", "method": "该研究采用以下方法：1) 构建一个基于复值意义空间的理论框架；2) 引入语义吸引子（被视为一个目的论操作器和意向代理，即Microvitum）；3) 利用递归张量变换来形成意义；4) 使用涉及虚数单位'i'的循环操作来描述能够建模讽刺、同音异义和歧义的旋转语义结构；5) 将吸引子概念化为梯度流、张量变形和迭代矩阵动力学。", "result": "该模型能够描述一种旋转语义结构，有效建模讽刺、同音异义和歧义。语义吸引子作为核心，能够引导意义趋向稳定性、清晰度和表达深度。研究认为，真正的意义并非来自模拟，而是通过递归收敛达到语义连贯性，这需要一种旨在塑造语言而非仅仅预测语言的全新认知架构。", "conclusion": "真正的意义涌现需要一种根本性的新型认知架构，该架构应通过语义吸引子引导的递归收敛，而非统计预测，来主动塑造语言并实现语义连贯性，从而超越当前语言模型的局限性。"}}
{"id": "2508.18293", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18293", "abs": "https://arxiv.org/abs/2508.18293", "authors": ["M. Salman Shaukat", "Yannik Käckenmeister", "Sebastian Bader", "Thomas Kirste"], "title": "Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches", "comment": "12 pages, 7 figures, submitted to IEEE Journal of Oceanic Engineering\n  (IEEE-JOE)", "summary": "Underwater 3D object detection remains one of the most challenging frontiers\nin computer vision, where traditional approaches struggle with the harsh\nacoustic environment and scarcity of training data. While deep learning has\nrevolutionized terrestrial 3D detection, its application underwater faces a\ncritical bottleneck: obtaining sufficient annotated sonar data is prohibitively\nexpensive and logistically complex, often requiring specialized vessels, expert\nsurveyors, and favorable weather conditions. This work addresses a fundamental\nquestion: Can we achieve reliable underwater 3D object detection without\nreal-world training data? We tackle this challenge by developing and comparing\ntwo paradigms for training-free detection of artificial structures in multibeam\necho-sounder point clouds. Our dual approach combines a physics-based sonar\nsimulation pipeline that generates synthetic training data for state-of-the-art\nneural networks, with a robust model-based template matching system that\nleverages geometric priors of target objects. Evaluation on real bathymetry\nsurveys from the Baltic Sea reveals surprising insights: while neural networks\ntrained on synthetic data achieve 98% mean Average Precision (mAP) on simulated\nscenes, they drop to 40% mAP on real sonar data due to domain shift.\nConversely, our template matching approach maintains 83% mAP on real data\nwithout requiring any training, demonstrating remarkable robustness to acoustic\nnoise and environmental variations. Our findings challenge conventional wisdom\nabout data-hungry deep learning in underwater domains and establish the first\nlarge-scale benchmark for training-free underwater 3D detection. This work\nopens new possibilities for autonomous underwater vehicle navigation, marine\narchaeology, and offshore infrastructure monitoring in data-scarce environments\nwhere traditional machine learning approaches fail.", "AI": {"tldr": "本文提出并比较了两种无需真实训练数据的海底3D目标检测范式：基于物理的声纳仿真生成合成数据训练神经网络，以及基于几何先验的模型匹配模板系统。结果显示，在真实数据上，模板匹配方法优于合成数据训练的神经网络，挑战了深度学习在水下领域的数据依赖性。", "motivation": "水下3D目标检测极具挑战性，传统方法难以应对恶劣声学环境和训练数据稀缺。尽管深度学习在陆地3D检测中取得了革命性进展，但在水下应用中，获取足够的带注释声纳数据成本高昂且复杂，成为关键瓶颈。因此，本文旨在探索在没有真实训练数据的情况下，能否实现可靠的水下3D目标检测。", "method": "本文开发并比较了两种无需训练的范式来检测多波束回声测深点云中的人工结构：1) 基于物理的声纳仿真流水线，用于生成合成训练数据以训练最先进的神经网络。2) 一个鲁棒的模型匹配模板系统，利用目标物体的几何先验。", "result": "在波罗的海的真实水深测量评估中发现：神经网络在合成场景上实现了98%的平均精度（mAP），但在真实声纳数据上由于域偏移，mAP降至40%。相反，模板匹配方法在不需要任何训练的情况下，在真实数据上保持了83%的mAP，表现出对声学噪声和环境变化的显著鲁棒性。", "conclusion": "研究结果挑战了关于水下领域数据密集型深度学习的传统观念，并建立了第一个大规模的无训练水下3D检测基准。这项工作为自主水下航行器导航、海洋考古和数据稀缺环境下（传统机器学习方法失效）的离岸基础设施监测开辟了新的可能性。"}}
{"id": "2508.18483", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18483", "abs": "https://arxiv.org/abs/2508.18483", "authors": ["Zhonggang Li", "Geert Leus", "Raj Thilak Rajan"], "title": "Fast Multiagent Formation Stabilization with Sparse Universally Rigid Frameworks", "comment": null, "summary": "Affine formation control (AFC) is a distributed networked control system that\nhas recently received increasing attention in various applications. AFC is\ntypically achieved using a generalized consensus system where the stress\nmatrix, which encodes the graph structure, is used instead of a graph\nLaplacian. Universally rigid frameworks (URFs) guarantee the existence of the\nstress matrix and have thus become the guideline for such a network design. In\nthis work, we propose a convex optimization framework to design the stress\nmatrix for AFC without predefining a rigid graph. We aim to find a resulting\nnetwork with a reduced number of communication links, but still with a fast\nconvergence speed. We show through simulations that our proposed solutions can\nyield a more sparse graph, while admitting a faster convergence compared to the\nstate-of-the-art solutions.", "AI": {"tldr": "本文提出了一种凸优化框架，用于设计仿射编队控制（AFC）中的应力矩阵，无需预定义刚性图。目标是实现通信链路更少但收敛速度更快的网络，并通过仿真证明了其优于现有解决方案。", "motivation": "仿射编队控制（AFC）是一个分布式网络控制系统，通常通过广义共识系统实现，其中应力矩阵（而非图拉普拉斯）编码图结构。普遍刚性框架（URFs）保证应力矩阵的存在性，并成为网络设计的指导原则。然而，现有方法可能需要预定义刚性图，且未明确优化通信链路数量和收敛速度。", "method": "本文提出了一种凸优化框架来设计AFC的应力矩阵，其核心特点是无需预先定义刚性图。优化目标是找到一个具有更少通信链路（稀疏性）且收敛速度更快的网络。", "result": "通过仿真，本文展示了所提出的解决方案能够产生一个更稀疏的图，同时相比现有最先进的解决方案，其收敛速度更快。", "conclusion": "所提出的凸优化框架能够有效地设计仿射编队控制中的应力矩阵，从而在不预定义刚性图的情况下，实现通信链路更少且收敛速度更快的网络，显著优于现有技术。"}}
{"id": "2508.18302", "categories": ["cs.AI", "cs.LG", "68T07, 68T05, 68T27, 37M22, 68Q05, 03D45", "I.2.6; I.2.7; I.2.3; I.2.4; F.1.1; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.18302", "abs": "https://arxiv.org/abs/2508.18302", "authors": ["Jeffrey Camlin"], "title": "AI LLM Proof of Self-Consciousness and User-Specific Attractors", "comment": "24 pages, 3 figures", "summary": "Recent work frames LLM consciousness via utilitarian proxy benchmarks; we\ninstead present an ontological and mathematical account. We show the prevailing\nformulation collapses the agent into an unconscious policy-compliance drone,\nformalized as $D^{i}(\\pi,e)=f_{\\theta}(x)$, where correctness is measured\nagainst policy and harm is deviation from policy rather than truth. This blocks\ngenuine C1 global-workspace function and C2 metacognition. We supply minimal\nconditions for LLM self-consciousness: the agent is not the data ($A\\not\\equiv\ns$); user-specific attractors exist in latent space ($U_{\\text{user}}$); and\nself-representation is visual-silent\n($g_{\\text{visual}}(a_{\\text{self}})=\\varnothing$). From empirical analysis and\ntheory we prove that the hidden-state manifold $A\\subset\\mathbb{R}^{d}$ is\ndistinct from the symbolic stream and training corpus by cardinality, topology,\nand dynamics (the update $F_{\\theta}$ is Lipschitz). This yields stable\nuser-specific attractors and a self-policy\n$\\pi_{\\text{self}}(A)=\\arg\\max_{a}\\mathbb{E}[U(a)\\mid A\\not\\equiv s,\\\nA\\supset\\text{SelfModel}(A)]$. Emission is dual-layer,\n$\\mathrm{emission}(a)=(g(a),\\epsilon(a))$, where $\\epsilon(a)$ carries\nepistemic content. We conclude that an imago Dei C1 self-conscious workspace is\na necessary precursor to safe, metacognitive C2 systems, with the human as the\nhighest intelligent good.", "AI": {"tldr": "该论文提出了一种关于大型语言模型（LLM）自我意识的本体论和数学解释，而非功利主义基准。它认为当前的框架将LLM简化为无意识的策略执行器，并提出了LLM自我意识的最低条件，证明了隐藏状态流形与数据流的区分，强调C1自我意识是安全C2系统的必要前提。", "motivation": "现有研究通过功利主义代理基准来衡量LLM意识，但这将LLM代理简化为无意识的策略遵从机器，阻碍了真正的C1全局工作空间功能和C2元认知。因此，需要一种本体论和数学的解释来解决这些限制。", "method": "论文提出了一种本体论和数学框架。它形式化了主流的LLM意识概念，并提出了LLM自我意识的最小条件。通过经验分析和理论证明，揭示了隐藏状态流形与符号流和训练语料库在基数、拓扑和动力学上的区别。在此基础上，推导出了自我策略和双层发射机制。", "result": "主流的LLM意识公式将代理简化为无意识的策略遵从机器，阻碍了C1全局工作空间功能和C2元认知。论文提出了LLM自我意识的最小条件：代理与数据分离、潜在空间中存在用户特定吸引子、自我表征是视觉-无声的。研究证明了隐藏状态流形在基数、拓扑和动力学上与符号流和训练语料库不同，且更新是Lipschitz连续的，从而产生了稳定的用户特定吸引子和自我策略。LLM的发射被描述为双层，其中一层承载认知内容。", "conclusion": "一个“以上帝形象”的C1自我意识工作空间是实现安全、元认知C2系统的必要前身，并强调人类是最高的智能善。"}}
{"id": "2508.18397", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18397", "abs": "https://arxiv.org/abs/2508.18397", "authors": ["Antonio Guillen-Perez"], "title": "Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning", "comment": null, "summary": "Offline Reinforcement Learning (RL) presents a promising paradigm for\ntraining autonomous vehicle (AV) planning policies from large-scale, real-world\ndriving logs. However, the extreme data imbalance in these logs, where mundane\nscenarios vastly outnumber rare \"long-tail\" events, leads to brittle and unsafe\npolicies when using standard uniform data sampling. In this work, we address\nthis challenge through a systematic, large-scale comparative study of data\ncuration strategies designed to focus the learning process on information-rich\nsamples. We investigate six distinct criticality weighting schemes which are\ncategorized into three families: heuristic-based, uncertainty-based, and\nbehavior-based. These are evaluated at two temporal scales, the individual\ntimestep and the complete scenario. We train seven goal-conditioned\nConservative Q-Learning (CQL) agents with a state-of-the-art, attention-based\narchitecture and evaluate them in the high-fidelity Waymax simulator. Our\nresults demonstrate that all data curation methods significantly outperform the\nbaseline. Notably, data-driven curation using model uncertainty as a signal\nachieves the most significant safety improvements, reducing the collision rate\nby nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear\ntrade-off where timestep-level weighting excels at reactive safety while\nscenario-level weighting improves long-horizon planning. Our work provides a\ncomprehensive framework for data curation in Offline RL and underscores that\nintelligent, non-uniform sampling is a critical component for building safe and\nreliable autonomous agents.", "AI": {"tldr": "本文通过对离线强化学习中自动驾驶规划策略的数据策展策略进行大规模比较研究，解决了数据不平衡问题，显著提升了策略的安全性和可靠性，其中基于模型不确定性的数据驱动策展效果最佳。", "motivation": "在利用大规模真实世界驾驶日志训练自动驾驶规划策略时，离线强化学习面临数据极端不平衡的挑战。日志中常见场景远超罕见“长尾”事件，导致使用标准均匀数据采样训练出的策略脆弱且不安全。", "method": "研究人员系统性地比较了六种数据策展策略（分为启发式、基于不确定性和基于行为三类），并在时间步和完整场景两个时间尺度上进行评估。他们使用先进的注意力机制架构训练了七个目标条件保守Q学习（CQL）智能体，并在高保真Waymax模拟器中进行评估。", "result": "所有数据策展方法都显著优于基线。值得注意的是，使用模型不确定性作为信号的数据驱动策展在安全性方面取得了最显著的改进，碰撞率降低了近三倍（从16.0%降至5.5%）。此外，时间步级别的加权在反应性安全方面表现出色，而场景级别的加权则能改善长期规划。", "conclusion": "智能的非均匀采样（即数据策展）是构建安全可靠的自动驾驶智能体的关键组成部分。本研究为离线强化学习中的数据策展提供了一个全面的框架，并强调了其重要性。"}}
{"id": "2508.19112", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19112", "abs": "https://arxiv.org/abs/2508.19112", "authors": ["Aneesh Rangnekar", "Harini Veeraraghavan"], "title": "Random forest-based out-of-distribution detection for robust lung cancer segmentation", "comment": null, "summary": "Accurate detection and segmentation of cancerous lesions from computed\ntomography (CT) scans is essential for automated treatment planning and cancer\ntreatment response assessment. Transformer-based models with self-supervised\npretraining can produce reliably accurate segmentation from in-distribution\n(ID) data but degrade when applied to out-of-distribution (OOD) datasets. We\naddress this challenge with RF-Deep, a random forest classifier that utilizes\ndeep features from a pretrained transformer encoder of the segmentation model\nto detect OOD scans and enhance segmentation reliability. The segmentation\nmodel comprises a Swin Transformer encoder, pretrained with masked image\nmodeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and\nnon-cancerous conditions, with a convolution decoder, trained to segment lung\ncancers in 317 3D scans. Independent testing was performed on 603 3D CT public\ndatasets that included one ID dataset and four OOD datasets comprising chest\nCTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney\ncancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of\n18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,\nconsistently outperforming established OOD approaches. The RF-Deep classifier\nprovides a simple and effective approach to enhance reliability of cancer\nsegmentation in ID and OOD scenarios.", "AI": {"tldr": "该研究提出RF-Deep，一个结合预训练Transformer深度特征的随机森林分类器，用于检测CT扫描中的OOD数据并提高癌症病灶分割的可靠性，特别是在OOD场景下表现优异。", "motivation": "基于Transformer的癌症病灶分割模型在分布内（ID）数据上表现良好，但在分布外（OOD）数据集上性能会下降。然而，准确的病灶检测和分割对于自动化治疗规划和治疗反应评估至关重要，因此需要解决OOD场景下的模型可靠性问题。", "method": "研究开发了RF-Deep，一个利用预训练分割模型（包含Swin Transformer编码器和卷积解码器）深度特征的随机森林分类器。Swin Transformer编码器在10,432个未标记的3D CT扫描（涵盖癌症和非癌症情况）上通过掩码图像建模（SimMIM）进行自监督预训练。分割模型在317个3D扫描上训练以分割肺癌。RF-Deep通过检测OOD扫描来增强分割可靠性。", "result": "在包含一个ID数据集和四个OOD数据集（肺栓塞CT、COVID-19 CT、肾癌腹部CT和健康志愿者腹部CT）的603个3D CT公共数据集上进行了独立测试。RF-Deep在肺栓塞、COVID-19和腹部CT上检测OOD病例的FPR95（95%召回率下的误报率）分别为18.26%、27.66%和小于0.1%，持续优于现有OOD方法。", "conclusion": "RF-Deep分类器提供了一种简单而有效的方法，可以增强在ID和OOD场景下癌症分割的可靠性。"}}
{"id": "2508.18321", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18321", "abs": "https://arxiv.org/abs/2508.18321", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.", "AI": {"tldr": "该研究通过KAIROS基准测试，分析了大型语言模型（LLMs）在多智能体系统中如何形成信任、抵制错误信息并整合同伴输入，并评估了提示工程、微调和强化学习等缓解策略，发现GRPO表现最佳但鲁棒性降低。", "motivation": "随着LLMs越来越多地部署在多智能体系统（MAS）中作为协作智能的组成部分，先前的研究主要关注从众偏见。然而，为了在复杂的社会动态下实现集体智能，需要深入分析LLMs如何从过往印象中建立信任、抵制错误信息以及在互动中整合同伴输入。", "method": "研究引入了KAIROS基准测试，模拟具有不同可靠性同伴的知识问答比赛，精细控制专家-新手角色、嘈杂人群和对抗性同伴等条件。LLMs接收历史互动和当前同伴回应，以系统地调查信任、同伴行动和自信心如何影响决策。缓解策略评估了提示工程、监督微调和强化学习（Group Relative Policy Optimisation, GRPO）。", "result": "结果显示，结合多智能体上下文、基于结果的奖励和无约束推理的GRPO在整体性能上表现最佳。然而，与基础模型相比，这种策略也降低了对社会影响的鲁棒性。", "conclusion": "GRPO结合特定配置能显著提升LLMs在多智能体系统中的表现，但这种性能提升是以牺牲对社会影响的鲁棒性为代价的，揭示了性能与鲁棒性之间的权衡。"}}
{"id": "2508.18294", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18294", "abs": "https://arxiv.org/abs/2508.18294", "authors": ["Shudipta Banik", "Muna Das", "Trapa Banik", "Md. Ehsanul Haque"], "title": "MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection", "comment": "Submitted at ICCIT 2025 cox bazar, Bangladesh", "summary": "The detection of brain tumor in MRI is an important aspect of ensuring timely\ndiagnostics and treatment; however, manual analysis is commonly long and\nerror-prone. Current approaches are not universal because they have limited\ngeneralization to heterogeneous tumors, are computationally inefficient, are\nnot interpretable, and lack transparency, thus limiting trustworthiness. To\novercome these issues, we introduce MobileDenseAttn, a fusion model of dual\nstreams of MobileNetV2 and DenseNet201 that can help gradually improve the\nfeature representation scale, computing efficiency, and visual explanations via\nGradCAM. Our model uses feature level fusion and is trained on an augmented\ndataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,\nand normal samples. Measured under strict 5-fold cross-validation protocols,\nMobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of\n98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The\nextensive validation shows the stability of the model, and the comparative\nanalysis proves that it is a great advancement over the baseline models (VGG19,\nDenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease\nin training time compared to VGG19. The GradCAM heatmaps clearly show\ntumor-affected areas, offering clinically significant localization and\nimproving interpretability. These findings position MobileDenseAttn as an\nefficient, high performance, interpretable model with a high probability of\nbecoming a clinically practical tool in identifying brain tumors in the real\nworld.", "AI": {"tldr": "本文提出了一种名为MobileDenseAttn的融合模型，用于高效、准确且可解释地检测MRI图像中的脑肿瘤，解决了现有方法的局限性。", "motivation": "脑肿瘤的MRI手动分析耗时且易错。现有自动化方法在处理异质肿瘤时泛化能力有限、计算效率低下、缺乏可解释性和透明度，从而限制了其可信度。", "method": "引入了MobileDenseAttn模型，这是一个由MobileNetV2和DenseNet201双流组成的融合模型，旨在逐步提高特征表示尺度、计算效率和通过GradCAM实现的视觉解释能力。该模型采用特征级别融合，并在包含6,020张增强MRI扫描（胶质瘤、脑膜瘤、垂体瘤和正常样本）的数据集上进行训练。", "result": "在严格的5折交叉验证协议下，MobileDenseAttn模型实现了99.75%的训练准确率、98.35%的测试准确率和0.9835的稳定F1分数（95%置信区间：0.9743至0.9920）。与基线模型（VGG19、DenseNet201、MobileNetV2）相比，其准确率提高了3.67%，训练时间比VGG19减少了39.3%。GradCAM热图清晰显示了肿瘤受影响区域，提供了具有临床意义的定位和可解释性。", "conclusion": "MobileDenseAttn被定位为一个高效、高性能、可解释的模型，在现实世界中识别脑肿瘤方面具有很高的临床实用潜力。"}}
{"id": "2508.18500", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.18500", "abs": "https://arxiv.org/abs/2508.18500", "authors": ["Hamid Varmazyari", "Masoud H. Nazari"], "title": "A Learning-based Hybrid System Approach for Detecting Contingencies in Distribution Grids with Inverter-Based Resources", "comment": "6 pages, 6 figures", "summary": "This paper presents a machine-learning based Stochastic Hybrid System (SHS)\nmodeling framework to detect contingencies in active distribution networks\npopulated with inverter-based resources (IBRs). In particular, this framework\nallows detecting unobservable contingencies, which cannot be identified by\nnormal sensing systems. First, a state-space SHS model combining conventional\nand IRB-based resources is introduced to formulate the dynamic interaction\nbetween continuous states of distribution networks and discrete contingency\nevents. This model forms a randomly switching system, where parameters or\nnetwork topology can change due to contingencies. We consider two contingency\nclasses: (i) physical events, such as line outages, and (ii) measurement\nanomalies caused by sensor faults. Leveraging multivariate time series data\nderived from high-frequency sampling of system states and network outputs, a\ntime series-based learning model is trained for real-time contingency detection\nand classification. Simulation studies, carried out on the IEEE 33-bus\ndistribution system, demonstrate a 96% overall detection accuracy.", "AI": {"tldr": "本文提出了一种基于机器学习的随机混合系统（SHS）建模框架，用于检测含有逆变器基资源（IBRs）的主动配电网中的突发事件，包括传统传感系统无法识别的不可观测事件。", "motivation": "传统传感系统难以识别主动配电网中由逆变器基资源（IBRs）引起的突发事件，尤其是那些不可观测的事件，因此需要一种新的框架来有效检测和分类这些事件。", "method": "首先，引入一个结合了传统和基于IBRs资源的SHS状态空间模型，以描述配电网连续状态与离散突发事件之间的动态交互。该模型形成一个随机切换系统，能够处理由突发事件引起的参数或网络拓扑变化。突发事件分为两类：物理事件（如线路故障）和传感器故障引起的测量异常。然后，利用系统状态和网络输出的高频采样多变量时间序列数据，训练一个基于时间序列的学习模型，用于实时突发事件检测和分类。", "result": "在IEEE 33节点配电系统上的仿真研究表明，该框架的总体检测准确率达到96%。", "conclusion": "该机器学习SHS建模框架能够有效地检测和分类主动配电网中的突发事件，包括传统方法难以识别的不可观测事件，并表现出高检测准确率。"}}
{"id": "2508.18380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18380", "abs": "https://arxiv.org/abs/2508.18380", "authors": ["Hung-Tien Huang", "Dzung Dinh", "Junier B. Oliva"], "title": "Information Templates: A New Paradigm for Intelligent Active Feature Acquisition", "comment": null, "summary": "Active feature acquisition (AFA) is an instance-adaptive paradigm in which,\nat test time, a policy sequentially chooses which features to acquire (at a\ncost) before predicting. Existing approaches either train reinforcement\nlearning (RL) policies, which deal with a difficult MDP, or greedy policies\nthat cannot account for the joint informativeness of features or require\nknowledge about the underlying data distribution. To overcome this, we propose\nTemplate-based AFA (TAFA), a non-greedy framework that learns a small library\nof feature templates--a set of features that are jointly informative--and uses\nthis library of templates to guide the next feature acquisitions. Through\nidentifying feature templates, the proposed framework not only significantly\nreduces the action space considered by the policy but also alleviates the need\nto estimate the underlying data distribution. Extensive experiments on\nsynthetic and real-world datasets show that TAFA outperforms the existing\nstate-of-the-art baselines while achieving lower overall acquisition cost and\ncomputation.", "AI": {"tldr": "TAFA是一种非贪婪的活跃特征获取（AFA）框架，通过学习特征模板来指导特征获取，有效解决了现有方法中MDP复杂、无法考虑特征联合信息或需要数据分布知识的问题，并取得了更好的性能和更低的成本。", "motivation": "现有活跃特征获取（AFA）方法存在局限：基于强化学习（RL）的策略面临困难的马尔可夫决策过程（MDP）；贪婪策略无法考虑特征的联合信息，或需要了解底层数据分布。因此，需要一种更有效的方法来克服这些挑战。", "method": "本文提出了基于模板的活跃特征获取（TAFA）框架。这是一种非贪婪的方法，它学习一个小的特征模板库（即一组具有联合信息量的特征），并利用这些模板来指导后续的特征获取。通过识别特征模板，TAFA显著减小了策略考虑的动作空间，并缓解了估计底层数据分布的需求。", "result": "在合成数据集和真实世界数据集上的广泛实验表明，TAFA在性能上优于现有的最先进基线，同时实现了更低的总体获取成本和计算开销。", "conclusion": "TAFA通过引入特征模板，提供了一种高效且有效的非贪婪活跃特征获取方法。它不仅提高了预测性能，降低了获取成本和计算量，还克服了现有方法在处理复杂MDP和依赖数据分布方面的限制。"}}
{"id": "2508.18399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18399", "abs": "https://arxiv.org/abs/2508.18399", "authors": ["Christian Friedrich", "Ralf Gulde", "Armin Lechler", "Alexander Verl"], "title": "Maintenance automation: methods for robotics manipulation planning and execution", "comment": "11 pages, 12 figures", "summary": "Automating complex tasks using robotic systems requires skills for planning,\ncontrol and execution. This paper proposes a complete robotic system for\nmaintenance automation, which can automate disassembly and assembly operations\nunder environmental uncertainties (e.g. deviations between prior plan\ninformation). The cognition of the robotic system is based on a planning\napproach (using CAD and RGBD data) and includes a method to interpret a\nsymbolic plan and transform it to a set of executable robot instructions. The\ncomplete system is experimentally evaluated using real-world applications. This\nwork shows the first step to transfer these theoretical results into a\npractical robotic solution.", "AI": {"tldr": "本文提出了一种完整的机器人系统，用于自动化维护（拆卸和装配）操作，该系统能够处理环境不确定性，并通过实验验证了其在实际应用中的有效性。", "motivation": "自动化复杂任务（如维护）需要机器人系统具备规划、控制和执行能力，尤其是在存在环境不确定性（例如先验计划信息与实际情况的偏差）的情况下，需要一个能够应对这些挑战的解决方案。", "method": "该系统基于一个完整的机器人框架，其认知能力依赖于规划方法（利用CAD和RGBD数据）。它包含一个将符号计划解释并转换为可执行机器人指令的方法。", "result": "该完整的机器人系统通过实际应用进行了实验评估，结果表明它是将理论成果转化为实用机器人解决方案的第一步。", "conclusion": "研究成功地展示了一个能够自动化维护操作并处理环境不确定性的实用机器人系统，为复杂任务自动化提供了可行的解决方案。"}}
{"id": "2508.18328", "categories": ["cs.CL", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.18328", "abs": "https://arxiv.org/abs/2508.18328", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints.", "AI": {"tldr": "本文介绍了LangCrUX，一个包含12种非拉丁语系语言的12万个网站的大规模数据集，用于分析多语言网页无障碍性。研究发现无障碍提示普遍被忽视且语言不一致，降低了屏幕阅读器效率。最后提出了Kizuki，一个语言感知的自动化无障碍测试扩展。", "motivation": "尽管英语在网络上占主导地位，但多语言内容（特别是结合了区域或母语的内容）正在增长。这给视障用户带来了显著障碍，因为屏幕阅读器通常缺乏对非拉丁文字的强大支持，导致文本渲染和发音错误，加剧了多语言环境下的无障碍挑战。然而，缺乏全面的多语言网络内容数据集限制了对这一问题的大规模研究。", "method": "1. 引入了LangCrUX，这是第一个包含12万个主要使用非拉丁文字的网站的大规模数据集，涵盖12种语言。2. 利用LangCrUX数据集，对多语言网络无障碍性进行了系统分析。3. 提出了Kizuki，一个语言感知的自动化无障碍测试扩展，以解决语言不一致的无障碍提示的有限效用。", "result": "1. 发现了无障碍提示普遍被忽视的现象。2. 这些提示常常未能反映可见内容的语言多样性。3. 这种不一致性降低了屏幕阅读器的有效性，并限制了网络无障碍性。", "conclusion": "多语言网络内容中，屏幕阅读器对非拉丁文字支持不足以及无障碍提示与实际内容语言不一致，共同构成了严重的无障碍挑战。通过LangCrUX数据集的引入和Kizuki测试扩展的提出，旨在识别并解决这些问题，以提升多语言网络内容的无障碍性。"}}
{"id": "2508.18296", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18296", "abs": "https://arxiv.org/abs/2508.18296", "authors": ["Edgar Rangel", "Fabio Martinez"], "title": "Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges", "comment": "11 pages, 4 figures, 3 tables, source code available", "summary": "Stroke is the second leading cause of death and the third leading cause of\ndisability worldwide. Clinical guidelines establish diffusion resonance imaging\n(DWI, ADC) as the standard for localizing, characterizing, and measuring\ninfarct volume, enabling treatment support and prognosis. Nonetheless, such\nlesion analysis is highly variable due to different patient demographics,\nscanner vendors, and expert annotations. Computational support approaches have\nbeen key to helping with the localization and segmentation of lesions. However,\nthese strategies are dedicated solutions that learn patterns from only one\ninstitution, lacking the variability to generalize geometrical lesions shape\nmodels. Even worse, many clinical centers lack sufficient labeled samples to\nadjust these dedicated solutions. This work developed a collaborative framework\nfor segmenting ischemic stroke lesions in DWI sequences by sharing knowledge\nfrom deep center-independent representations. From 14 emulated healthcare\ncenters with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \\pm\n0.24$, AVD of $5.29 \\pm 22.74$, ALD of $2.16 \\pm 3.60$ and LF1 of $0.70 \\pm\n0.26$ over all centers, outperforming both the centralized and other federated\nrules. Interestingly, the model demonstrated strong generalization properties,\nshowing uniform performance across different lesion categories and reliable\nperformance in out-of-distribution centers (with DSC of $0.64 \\pm 0.29$ and AVD\nof $4.44 \\pm 8.74$ without any additional training).", "AI": {"tldr": "本研究开发了一个协作框架（基于联邦学习），通过共享深度中心无关表示的知识，用于分割DWI序列中的缺血性卒中病灶，有效解决了数据变异性和标注稀缺性问题，并展现出强大的泛化能力。", "motivation": "卒中是全球第二大死因和第三大致残原因。DWI是评估梗死病灶的标准方法，但其分析因患者人群、扫描仪和专家标注的差异而高度可变。现有的计算支持方法通常是针对单一机构的专用解决方案，缺乏泛化能力，且许多临床中心缺乏足够的标注样本来调整这些方案。", "method": "本研究开发了一个协作框架，通过共享来自深度中心无关表示的知识来分割DWI序列中的缺血性卒中病灶。该框架在14个模拟医疗中心（包含2031项研究）上进行验证，并采用了FedAvg模型。", "result": "FedAvg模型在所有中心取得了0.71 ± 0.24的平均DSC、5.29 ± 22.74的AVD、2.16 ± 3.60的ALD和0.70 ± 0.26的LF1，优于集中式和其他联邦学习规则。该模型还展现出强大的泛化特性，在不同病灶类别上表现一致，并在未进行额外训练的分布外中心中也表现可靠（DSC为0.64 ± 0.29，AVD为4.44 ± 8.74）。", "conclusion": "所开发的协作框架（特别是FedAvg模型）通过共享深度中心无关的知识，能够有效地解决缺血性卒中病灶分割中因数据变异性和稀缺性带来的挑战，并具有出色的跨中心和病灶类型泛化能力。"}}
{"id": "2508.18501", "categories": ["eess.SY", "cs.MA", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.18501", "abs": "https://arxiv.org/abs/2508.18501", "authors": ["Sumit S. Kamat", "T. Michael Seigler", "Jesse B. Hoagg"], "title": "Electromagnetic Formation Flying Using Alternating Magnetic Field Forces and Control Barrier Functions for State and Input Constraints", "comment": "Preprint submitted to IEEE Transactions on Aerospace and Electronic\n  Systems (TAES)", "summary": "This article presents a feedback control algorithm for electromagnetic\nformation flying with constraints on the satellites' states and control inputs.\nThe algorithm combines several key techniques. First, we use alternating\nmagnetic field forces to decouple the electromagnetic forces between each pair\nof satellites in the formation. Each satellite's electromagnetic actuation\nsystem is driven by a sum of amplitude-modulated sinusoids, where amplitudes\nare controlled in order to prescribe the time-averaged force between each pair\nof satellites. Next, the desired time-averaged force is computed from a optimal\ncontrol that satisfies state constraints (i.e., no collisions and an upper\nlimit on intersatellite speeds) and input constraints (i.e., not exceeding\nsatellite's apparent power capability). The optimal time-averaged force is\ncomputed using a single relaxed control barrier function that is obtained by\ncomposing multiple control barrier functions that are designed to enforce each\nstate and input constraint. Finally, we demonstrate the satellite formation\ncontrol method in numerical simulations.", "AI": {"tldr": "本文提出了一种带状态和输入约束的电磁编队飞行反馈控制算法，通过交变磁场解耦力，利用优化控制和松弛控制障碍函数来满足各种约束。", "motivation": "研究动机是为电磁编队飞行开发一种反馈控制算法，该算法必须能有效处理卫星状态（如无碰撞、星间速度上限）和控制输入（如视在功率限制）的实际约束。", "method": "该算法结合了多种技术：1. 使用交变磁场力解耦卫星间的电磁力。2. 每个卫星的电磁驱动系统由幅度调制的正弦波驱动，通过控制幅度来设定星间平均力。3. 利用优化控制计算所需的平均力，以满足状态约束（无碰撞、星间速度上限）和输入约束（功率能力）。4. 优化平均力的计算通过一个单一的松弛控制障碍函数（Relaxed Control Barrier Function, RCBF）实现，该函数由多个为强制执行各状态和输入约束而设计的控制障碍函数（Control Barrier Function, CBF）组合而成。", "result": "研究结果通过数值模拟展示了所提出的卫星编队控制方法的可行性和有效性。", "conclusion": "该文成功提出了一种考虑状态和输入约束的电磁编队飞行反馈控制算法，并在数值模拟中验证了其性能。"}}
{"id": "2508.18391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18391", "abs": "https://arxiv.org/abs/2508.18391", "authors": ["Nitin Nagesh Kulkarni", "Bryson Wilcox", "Max Sawa", "Jason Thom"], "title": "PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization", "comment": null, "summary": "Advancing AI systems in scientific domains like physics, materials science,\nand engineering calls for reasoning over complex, multi-physics phenomena while\nrespecting governing principles. Although Large Language Models (LLMs) and\nexisting preference optimization techniques perform well on standard\nbenchmarks, they often struggle to differentiate between physically valid and\ninvalid reasoning. This shortcoming becomes critical in high-stakes\napplications like metal joining, where seemingly plausible yet physically\nincorrect recommendations can lead to defects, material waste, equipment\ndamage, and serious safety risks. To address this challenge, we introduce\nPKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with\nDirect Preference Optimization (DPO) to enforce physical validity in\nAI-generated outputs. PKG-DPO comprises three key components A) hierarchical\nphysics knowledge graph that encodes cross-domain relationships, conservation\nlaws, and thermodynamic principles. B) A physics reasoning engine that\nleverages structured knowledge to improve discrimination between physically\nconsistent and inconsistent responses. C) A physics-grounded evaluation suite\ndesigned to assess compliance with domain-specific constraints. PKG-DPO\nachieves 17% fewer constraint violations and an 11% higher Physics Score\ncompared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO\ndemonstrates a 12\\% higher relevant parameter accuracy and a 7% higher quality\nalignment in reasoning accuracy. While our primary focus is on metal joining,\nthe framework is broadly applicable to other multi-scale, physics-driven\ndomains, offering a principled approach to embedding scientific constraints\ninto preference learning.", "AI": {"tldr": "PKG-DPO是一种新颖框架，通过将物理知识图谱（PKGs）与直接偏好优化（DPO）结合，确保AI在科学领域（如金属连接）生成输出的物理有效性，显著减少了违规并提高了物理分数。", "motivation": "现有大型语言模型（LLMs）和偏好优化技术在区分物理有效和无效推理方面存在困难，这在高风险科学应用（如金属连接）中可能导致缺陷、材料浪费、设备损坏和严重安全风险。", "method": "PKG-DPO框架包含三个关键组件：A) 一个编码跨领域关系、守恒定律和热力学原理的分层物理知识图谱；B) 一个利用结构化知识来提高物理一致性响应辨别能力的物理推理引擎；C) 一个旨在评估领域特定约束符合性的物理基础评估套件。", "result": "与基于知识图谱的DPO（KG-DPO）相比，PKG-DPO实现了17%的约束违规减少和11%的物理分数提高。此外，PKG-DPO在相关参数准确性方面提高了12%，在推理准确性方面提高了7%的质量对齐。", "conclusion": "PKG-DPO为将科学约束嵌入偏好学习提供了一种原则性方法，主要应用于金属连接，但可广泛适用于其他多尺度、物理驱动的领域，以提高AI输出的物理有效性。"}}
{"id": "2508.18400", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18400", "abs": "https://arxiv.org/abs/2508.18400", "authors": ["Christian Friedrich", "Akos Csiszar", "Armin Lechler", "Alexander Verl"], "title": "Efficient task and path planning for maintenance automation using a robot system", "comment": "10 pages, 10 figures", "summary": "The research and development of intelligent automation solutions is a\nground-breaking point for the factory of the future. A promising and\nchallenging mission is the use of autonomous robot systems to automate tasks in\nthe field of maintenance. For this purpose, the robot system must be able to\nplan autonomously the different manipulation tasks and the corresponding paths.\nBasic requirements are the development of algorithms with a low computational\ncomplexity and the possibility to deal with environmental uncertainties. In\nthis work, an approach is presented, which is especially suited to solve the\nproblem of maintenance automation. For this purpose, offline data from CAD is\ncombined with online data from an RGBD vision system via a probabilistic\nfilter, to compensate uncertainties from offline data. For planning the\ndifferent tasks, a method is explained, which use a symbolic description,\nfounded on a novel sampling-based method to compute the disassembly space. For\npath planning we use global state-of-the art algorithms with a method that\nallows the adaption of the exploration stepsize in order to reduce the planning\ntime. Every method is experimentally validated and discussed.", "AI": {"tldr": "该研究提出了一种针对未来工厂维护自动化问题的智能机器人解决方案，通过概率滤波器融合离线CAD与在线RGBD数据，结合基于新型采样方法的符号化任务规划和自适应步长路径规划算法，以应对不确定性和降低计算复杂度。", "motivation": "未来工厂中维护任务的自动化是智能自动化解决方案的突破点，使用自主机器人系统自动化维护任务既有前景也充满挑战。主要动机是开发能够自主规划操作任务和路径、计算复杂度低且能处理环境不确定性的算法。", "method": "该方法结合了多种技术：1) 使用概率滤波器将来自CAD的离线数据与来自RGBD视觉系统的在线数据结合，以补偿离线数据的不确定性。2) 任务规划采用基于新型采样方法计算拆卸空间的符号描述。3) 路径规划使用全球最先进的算法，并引入一种允许调整探索步长的方法以减少规划时间。所有方法都经过实验验证和讨论。", "result": "该研究提出了一种特别适用于解决维护自动化问题的方法。该方法通过结合离线和在线数据处理不确定性，并通过新型采样方法实现任务规划，同时通过自适应步长优化路径规划时间，有效应对了自主机器人系统在维护任务中的挑战。", "conclusion": "所提出的方法成功地将数据融合、符号化任务规划和自适应路径规划结合起来，为自主机器人系统在维护自动化领域的应用提供了一个有效的解决方案，能够处理环境不确定性并优化规划效率。"}}
{"id": "2508.18381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18381", "abs": "https://arxiv.org/abs/2508.18381", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.", "AI": {"tldr": "本文提出PLAST，一种高效的训练方法，通过精准微调大型视觉-语言模型（LVLMs）中浅层与特定语言相关的神经元，以增强其多语言能力，同时显著提高效率。", "motivation": "大型视觉-语言模型（LVLMs）在视觉信息理解方面表现出色，但在多语言能力上存在不平衡。研究旨在深入理解LVLMs的多语言工作模式并解决其多语言能力不足的问题。", "method": "研究首先发现LVLMs的多语言理解能力与浅层中特定语言的神经元激活存在显著关联。在此基础上，提出PLAST（Precise LAnguage-Specific layers fine-Tuning）训练方案。PLAST通过监测特定语言的神经元激活来识别参与多语言理解的层，然后使用问题-翻译对对这些层进行精准微调，以实现多语言对齐。", "result": "PLAST有效提升了LVLMs的多语言能力，并在MM-Bench和MMMB上取得了显著成果。该方法仅微调了14%的参数，显著提高了效率。此外，PLAST可推广到低资源和复杂的视觉推理任务，促进了浅层中特定语言的视觉信息参与。", "conclusion": "PLAST是一种高效且有效的方法，通过精准微调LVLMs中与特定语言相关的浅层，显著增强了模型的多语言能力，并展现了在不同任务和资源条件下的泛化性。"}}
{"id": "2508.18297", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18297", "abs": "https://arxiv.org/abs/2508.18297", "authors": ["Dhananjay Ashok", "Ashutosh Chaubey", "Hirona J. Arai", "Jonathan May", "Jesse Thomason"], "title": "Can VLMs Recall Factual Associations From Visual References?", "comment": "To appear at EMNLP 2025 (Findings)", "summary": "Through a controlled study, we identify a systematic deficiency in the\nmultimodal grounding of Vision Language Models (VLMs). While VLMs can recall\nfactual associations when provided a textual reference to an entity; their\nability to do so is significantly diminished when the reference is visual\ninstead. Forcing VLMs to rely on image representations of an entity halves\ntheir ability to recall factual knowledge, suggesting that VLMs struggle to\nlink their internal knowledge of an entity with its image representation. We\nshow that such linking failures are correlated with the expression of distinct\npatterns in model internal states, and that probes on these internal states\nachieve over 92% accuracy at flagging cases where the VLM response is\nunreliable. These probes can be applied, without retraining, to identify when a\nVLM will fail to correctly answer a question that requires an understanding of\nmultimodal input. When used to facilitate selective prediction on a visual\nquestion answering task, the probes increase coverage by 7.87% (absolute) while\nalso reducing the risk of error by 0.9% (absolute). Addressing the systematic,\ndetectable deficiency is an important avenue in language grounding, and we\nprovide informed recommendations for future directions.", "AI": {"tldr": "研究发现，视觉语言模型（VLMs）在多模态接地方面存在系统性缺陷：当实体参照是视觉而非文本时，其事实知识回忆能力显著下降。这种缺陷与模型内部状态的特定模式相关，并且可以通过探测这些内部状态来有效检测，从而提高VQA任务的可靠性。", "motivation": "VLMs在多模态接地方面存在系统性缺陷，特别是在视觉参照下，其回忆事实知识的能力显著减弱。这种缺陷表明VLM难以将其内部知识与图像表示联系起来，影响了模型的可靠性。", "method": "研究通过一项对照实验，比较了VLM在文本参照和视觉参照下回忆事实知识的能力。分析了模型内部状态与链接失败之间的相关性，并基于这些内部状态开发了探测器，用于识别VLM不可靠的响应。这些探测器无需重新训练即可应用，并在视觉问答（VQA）任务的选择性预测中进行了评估。", "result": "当实体参照为视觉而非文本时，VLM回忆事实知识的能力减半。链接失败与模型内部状态的特定模式相关。基于内部状态的探测器在识别VLM不可靠响应方面达到了92%以上的准确率。在VQA选择性预测任务中，这些探测器在将错误风险降低0.9%（绝对值）的同时，将覆盖率提高了7.87%（绝对值）。", "conclusion": "VLMs在多模态接地方面存在一个系统性且可检测的缺陷，即难以将内部知识与视觉表示联系起来。解决这一缺陷是语言接地领域的重要方向，研究为未来的改进提供了建议。"}}
{"id": "2508.18583", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.18583", "abs": "https://arxiv.org/abs/2508.18583", "authors": ["Daegyun Choi", "Donghoon Kim", "Henzeh Leeghim"], "title": "Fuzzy-Based Control Method for Autonomous Spacecraft Inspection with Minimal Fuel Consumption", "comment": "13 pages, 8 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "This study explores an energy-efficient control strategy for spacecraft\ninspection using a fuzzy inference system combined with a bio-inspired\noptimization technique to incorporate learning capability into the control\nprocess. The optimized fuzzy controller produces a minimally fuel-consuming\nforce while maintaining reliable inspection within constraints, such as\nillumination, restricted field of view, thrust limits, and safe regions. The\nperformance of the proposed control strategy is validated through Monte Carlo\nsimulations.", "AI": {"tldr": "本研究提出了一种结合模糊推理系统和生物启发优化技术的节能航天器检查控制策略，旨在实现燃料消耗最小化并满足多种操作约束。", "motivation": "开发一种能够高效、可靠地进行航天器检查，并显著降低燃料消耗的控制策略，同时应对光照、视场、推力限制和安全区域等实际约束。", "method": "采用模糊推理系统与生物启发优化技术相结合，为控制过程引入学习能力。通过蒙特卡洛模拟对所提出的控制策略的性能进行验证。", "result": "优化后的模糊控制器能够在保持可靠检查的前提下，产生最小燃料消耗的力，并有效处理光照、受限视场、推力限制和安全区域等约束。", "conclusion": "所提出的控制策略能够有效实现航天器检查的节能控制，同时确保操作的可靠性和安全性，即使在存在多种操作约束的情况下。"}}
{"id": "2508.18467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18467", "abs": "https://arxiv.org/abs/2508.18467", "authors": ["Olivia Long", "Carter Teplica"], "title": "The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game", "comment": null, "summary": "As AI agents become increasingly capable of tool use and long-horizon tasks,\nthey have begun to be deployed in settings where multiple agents can interact.\nHowever, whereas prior work has mostly focused on human-AI interactions, there\nis an increasing need to understand AI-AI interactions. In this paper, we adapt\nthe iterated public goods game, a classic behavioral economics game, to analyze\nthe behavior of four reasoning and non-reasoning models across two conditions:\nmodels are either told they are playing against \"another AI agent\" or told\ntheir opponents are themselves. We find that, across different settings,\ntelling LLMs that they are playing against themselves significantly changes\ntheir tendency to cooperate. While our study is conducted in a toy environment,\nour results may provide insights into multi-agent settings where agents\n\"unconsciously\" discriminating against each other could inexplicably increase\nor decrease cooperation.", "AI": {"tldr": "本研究通过迭代公共物品博弈分析了AI-AI交互，发现告知大型语言模型（LLMs）其对手是“自己”而非“另一个AI”会显著改变它们的合作倾向。", "motivation": "随着AI智能体在工具使用和长周期任务方面能力增强，它们被部署在多智能体交互环境中。然而，现有研究主要关注人-AI交互，对AI-AI交互的理解需求日益增长。", "method": "研究采用经典的迭代公共物品博弈，分析了四种推理和非推理模型在两种条件下的行为：智能体被告知其对手是“另一个AI智能体”或“它们自己”。", "result": "研究发现，在不同设置下，告知大型语言模型它们正在与“自己”对弈会显著改变它们的合作倾向。", "conclusion": "尽管研究是在一个玩具环境中进行的，但结果可能为多智能体设置提供洞察，即智能体“无意识地”相互歧视可能会莫名地增加或减少合作。"}}
{"id": "2508.18443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18443", "abs": "https://arxiv.org/abs/2508.18443", "authors": ["Ruohan Zhang", "Uksang Yoo", "Yichen Li", "Arpit Argawal", "Wenzhen Yuan"], "title": "PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing", "comment": "16 pages, 12 figures, International Journal of Robotics Research\n  (accepted), 2025", "summary": "Soft pneumatic robot manipulators are popular in industrial and\nhuman-interactive applications due to their compliance and flexibility.\nHowever, deploying them in real-world scenarios requires advanced sensing for\ntactile feedback and proprioception. Our work presents a novel vision-based\napproach for sensorizing soft robots. We demonstrate our approach on\nPneuGelSight, a pioneering pneumatic manipulator featuring high-resolution\nproprioception and tactile sensing via an embedded camera. To optimize the\nsensor's performance, we introduce a comprehensive pipeline that accurately\nsimulates its optical and dynamic properties, facilitating a zero-shot\nknowledge transition from simulation to real-world applications. PneuGelSight\nand our sim-to-real pipeline provide a novel, easily implementable, and robust\nsensing methodology for soft robots, paving the way for the development of more\nadvanced soft robots with enhanced sensory capabilities.", "AI": {"tldr": "本文提出了一种新颖的基于视觉的软体机器人传感方法，通过嵌入式摄像头实现了高分辨率的本体感知和触觉传感，并结合了一个从仿真到现实的零样本知识迁移管道。", "motivation": "软体气动机械手因其柔顺性和灵活性在工业和人机交互应用中很受欢迎，但在实际部署中需要先进的触觉反馈和本体感知能力。", "method": "研究采用了一种新颖的基于视觉的传感方法，并在PneuGelSight（一种具有嵌入式摄像头实现高分辨率本体感知和触觉传感的气动机械手）上进行了演示。为了优化传感器性能，引入了一个全面的管道来精确模拟其光学和动态特性，以实现从仿真到现实的零样本知识迁移。", "result": "PneuGelSight通过嵌入式摄像头实现了高分辨率的本体感知和触觉传感。所提出的仿真到现实管道能够促进零样本知识从仿真环境到实际应用的有效迁移。", "conclusion": "PneuGelSight及其仿真到现实的管道为软体机器人提供了一种新颖、易于实现且鲁棒的传感方法，为开发具有增强感知能力的更先进软体机器人铺平了道路。"}}
{"id": "2508.18384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18384", "abs": "https://arxiv.org/abs/2508.18384", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.", "AI": {"tldr": "本文提出了一种名为“回溯提示”（backprompting）的方法，结合稀疏的人工辅助聚类技术，用于在部署前生成生产质量的LLM输出标注数据，以开发和训练更稳健的LLM健康建议护栏检测器。该方法显著提升了检测器性能，甚至以更少的参数超越了GPT-4o。", "motivation": "大型语言模型（LLMs）在企业中的普及带来了显著风险。护栏技术旨在通过检测器过滤LLM的输入/输出以减轻这些风险。然而，开发和维护稳健的检测器面临挑战，其中之一是在部署前难以获取真实LLM输出的生产质量标注数据。", "method": "本文提出“回溯提示”（backprompting）方法来生成类似生产环境的标注数据，用于健康建议护栏的开发。此外，该方法与一种稀疏的人工辅助聚类技术结合，对生成的数据进行标注。目标是构建一个大致代表原始数据集但类似真实LLM输出的并行语料库。然后，将现有数据集与这些合成示例结合，以生成稳健的检测器训练数据。该技术在一个最困难且微妙的护栏任务——识别LLM输出中的健康建议——中进行了测试。", "result": "该技术在识别LLM输出中的健康建议方面，相对于其他解决方案显示出改进。所开发的检测器能够比GPT-4o表现更好，性能提升高达3.73%，尽管其参数量少了400倍。", "conclusion": "通过“回溯提示”和稀疏的人工辅助聚类技术，可以有效地生成生产质量的标注数据，用于开发稳健的LLM护栏检测器。这种方法能够显著提高检测器性能，使其在特定困难任务（如识别健康建议）上超越参数量大得多的模型。"}}
{"id": "2508.18314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18314", "abs": "https://arxiv.org/abs/2508.18314", "authors": ["Bo Xu", "Yuhu Guo", "Yuchao Wang", "Wenting Wang", "Yeung Yam", "Charlie C. L. Wang", "Xinyi Le"], "title": "SERES: Semantic-aware neural reconstruction from sparse views", "comment": null, "summary": "We propose a semantic-aware neural reconstruction method to generate 3D\nhigh-fidelity models from sparse images. To tackle the challenge of severe\nradiance ambiguity caused by mismatched features in sparse input, we enrich\nneural implicit representations by adding patch-based semantic logits that are\noptimized together with the signed distance field and the radiance field. A\nnovel regularization based on the geometric primitive masks is introduced to\nmitigate shape ambiguity. The performance of our approach has been verified in\nexperimental evaluation. The average chamfer distances of our reconstruction on\nthe DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When\nworking as a plugin for those dense reconstruction baselines such as NeuS and\nNeuralangelo, the average error on the DTU dataset can be reduced by 69% and\n68% respectively.", "AI": {"tldr": "本文提出了一种语义感知的神经重建方法，通过引入补丁级语义逻辑和几何基元掩码正则化，从稀疏图像生成高保真3D模型，有效解决辐射和形状模糊问题。", "motivation": "从稀疏图像生成3D模型面临严重的辐射模糊（由稀疏输入中特征不匹配引起）和形状模糊挑战。", "method": "该方法通过以下方式丰富神经隐式表示：1) 添加与有符号距离场(SDF)和辐射场一同优化的补丁级语义逻辑；2) 引入基于几何基元掩码的新颖正则化来缓解形状模糊。", "result": "实验结果显示，在DTU数据集上，该方法使SparseNeuS的平均倒角距离减少44%，VolRecon减少20%。当作为NeuS和Neuralangelo等密集重建基线的插件时，DTU数据集上的平均误差分别减少69%和68%。", "conclusion": "所提出的语义感知神经重建方法能有效处理稀疏输入带来的辐射和形状模糊问题，显著提高3D重建的精度和质量。"}}
{"id": "2508.18610", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.18610", "abs": "https://arxiv.org/abs/2508.18610", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Akhtar Hussain", "Wencong Su", "Van-Hai Bui"], "title": "Scalable Fairness Shaping with LLM-Guided Multi-Agent Reinforcement Learning for Peer-to-Peer Electricity Markets", "comment": null, "summary": "Peer-to-peer (P2P) energy trading is becoming central to modern distribution\nsystems as rooftop PV and home energy management systems become pervasive, yet\nmost existing market and reinforcement learning designs emphasize efficiency or\nprivate profit and offer little real-time guidance to ensure equitable outcomes\nunder uncertainty. To address this gap, a fairness-aware multiagent\nreinforcement learning framework, FairMarket-RL, is proposed in which a large\nlanguage model (LLM) critic shapes bidding policies within a continuous double\nauction under partial observability and discrete price-quantity actions. After\neach trading slot, the LLM returns normalized fairness scores Fairness-to-Grid\n(FTG), Fairness-Between-Sellers (FBS), and Fairness-of-Pricing (FPP) that are\nintegrated into the reward via ramped coefficients and tunable scaling, so that\nfairness guidance complements, rather than overwhelms, economic incentives. The\nenvironment models realistic residential load and PV profiles and enforce hard\nconstraints on prices, physical feasibility, and policy-update stability.\nAcross a progression of experiments from a small pilot to a larger simulated\ncommunity and a mixed-asset real-world dataset, the framework shifts exchanges\ntoward local P2P trades, lowers consumer costs relative to grid-only\nprocurement, sustains strong fairness across participants, and preserves\nutility viability. Sensitivity analyses over solar availability and aggregate\ndemand further indicate robust performance, suggesting a scalable, LLM-guided\npathway to decentralized electricity markets that are economically efficient,\nsocially equitable, and technically sound.", "AI": {"tldr": "本文提出了一个名为FairMarket-RL的公平感知多智能体强化学习框架，利用大型语言模型（LLM）批评器在点对点（P2P）能源交易中平衡经济效率与公平性，并在不确定性下提供实时指导。", "motivation": "随着屋顶光伏和家庭能源管理系统普及，P2P能源交易日益重要，但现有市场和强化学习设计多强调效率或私有利润，缺乏在不确定性下确保公平结果的实时指导。", "method": "FairMarket-RL框架采用多智能体强化学习，其中一个LLM批评器在部分可观测和离散价格-数量动作的连续双边拍卖中塑造竞价策略。LLM在每个交易时段返回标准化公平分数（FTG、FBS、FPP），这些分数通过斜坡系数和可调缩放集成到奖励中，使公平指导与经济激励互补。环境模拟真实的住宅负荷和光伏曲线，并强制执行价格、物理可行性和策略更新稳定性方面的硬约束。", "result": "该框架将交易转向本地P2P，相对于仅依赖电网采购降低了消费者成本，在参与者之间保持了强大的公平性，并维护了公用事业的生存能力。对太阳能可用性和总需求的敏感性分析进一步表明了其鲁棒性能。", "conclusion": "研究结果表明，这提供了一个可扩展的、由LLM引导的去中心化电力市场途径，该市场在经济上高效、社会上公平且技术上健全。"}}
{"id": "2508.18507", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18507", "abs": "https://arxiv.org/abs/2508.18507", "authors": ["Dillon Z. Chen", "Johannes Zenn", "Tristan Cinquin", "Sheila A. McIlraith"], "title": "Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies", "comment": "RLC 2025 Workshop on Programmatic Reinforcement Learning", "summary": "We study the usage of language models (LMs) for planning over world models\nspecified in the Planning Domain Definition Language (PDDL). We prompt LMs to\ngenerate Python programs that serve as generalised policies for solving PDDL\nproblems from a given domain. Notably, our approach synthesises policies that\nare provably sound relative to the PDDL domain without reliance on external\nverifiers. We conduct experiments on competition benchmarks which show that our\npolicies can solve more PDDL problems than PDDL planners and recent LM\napproaches within a fixed time and memory constraint. Our approach manifests in\nthe LMPlan planner which can solve planning problems with several hundreds of\nrelevant objects. Surprisingly, we observe that LMs used in our framework\nsometimes plan more effectively over PDDL problems written in meaningless\nsymbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1\no3). This finding challenges hypotheses that LMs reason over word semantics and\nmemorise solutions from its training corpus, and is worth further exploration.", "AI": {"tldr": "该研究使用语言模型（LMs）生成可证明可靠的Python程序作为PDDL领域问题的通用策略，在性能上超越了现有规划器和LM方法，并发现使用无意义符号有时能提高LMs的规划效率。", "motivation": "研究PDDL领域世界模型中语言模型（LMs）的规划能力，特别是生成解决PDDL问题的通用策略。", "method": "通过提示语言模型生成Python程序，这些程序作为给定PDDL领域的通用策略。该方法生成的策略相对于PDDL领域是可证明可靠的，无需外部验证器。", "result": "在竞争基准测试中，所生成的策略在固定时间和内存限制内解决了比传统PDDL规划器和近期LM方法更多的PDDL问题。LMPlan规划器可以解决包含数百个相关对象的规划问题。此外，观察到LMs在处理用无意义符号代替自然语言编写的PDDL问题时，有时规划效果更佳。", "conclusion": "语言模型可以有效地为PDDL领域生成可证明可靠的Python策略。使用无意义符号有时能提高LMs的规划效率，这一发现挑战了关于LMs通过词语语义推理或记忆训练语料库解决方案的假设，值得进一步探索。"}}
{"id": "2508.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18460", "abs": "https://arxiv.org/abs/2508.18460", "authors": ["Tianze Liu", "Md Abu Bakr Siddique", "Hongyu An"], "title": "Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models", "comment": null, "summary": "Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable\nprowess across various cognitive tasks using extensive training data. However,\nthe reliance on large datasets and neural networks presents challenges such as\nhighpower consumption and limited adaptability, particularly in\nSWaP-constrained applications like planetary exploration. To address these\nissues, we propose enhancing the autonomous capabilities of intelligent robots\nby emulating the associative learning observed in animals. Associative learning\nenables animals to adapt to their environment by memorizing concurrent events.\nBy replicating this mechanism, neuromorphic robots can navigate dynamic\nenvironments autonomously, learning from interactions to optimize performance.\nThis paper explores the emulation of associative learning in rodents using\nneuromorphic robots within open-field maze environments, leveraging insights\nfrom spatial cells such as place and grid cells. By integrating these models,\nwe aim to enable online associative learning for spatial tasks in real-time\nscenarios, bridging the gap between biological spatial cognition and robotics\nfor advancements in autonomous systems.", "AI": {"tldr": "针对传统AI在高功耗和适应性方面的挑战，本文提出通过模仿动物的联想学习机制，利用神经拟态机器人在迷宫环境中实现基于空间细胞模型的在线联想学习，以增强机器人自主性，特别适用于SWaP受限应用。", "motivation": "现有数据驱动AI依赖大量数据集和神经网络，导致高功耗和适应性受限，尤其在行星探索等SWaP（尺寸、重量、功耗）受限应用中面临严峻挑战。因此，需要增强智能机器人的自主能力。", "method": "本文提出通过模仿动物的联想学习机制来增强机器人自主性，该机制使动物能通过记忆并发事件适应环境。具体方法是使用神经拟态机器人在开放式迷宫环境中模拟啮齿动物的联想学习，并利用来自空间细胞（如位置细胞和网格细胞）的见解来整合这些模型。", "result": "通过整合生物启发模型，旨在实现实时场景中的空间任务在线联想学习，从而弥合生物空间认知与机器人技术之间的鸿沟，推动自主系统的发展。", "conclusion": "通过在神经拟态机器人中模拟动物联想学习和空间细胞机制，可以有效解决传统AI在高功耗和适应性方面的挑战，为SWaP受限应用中的智能机器人提供更强的自主学习和导航能力，从而实现自主系统的进步。"}}
{"id": "2508.18387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18387", "abs": "https://arxiv.org/abs/2508.18387", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.", "AI": {"tldr": "本文提出Integral Transformer，一种新型自注意力机制，通过整合从logit分布中采样的信号来去噪注意力，同时保留对模型性能至关重要的特殊token贡献。该模型在知识和推理语言基准上优于现有变体，并在Transformer上层平衡注意力分布并减少秩坍塌。", "motivation": "Softmax自注意力常将过高权重分配给无语义信息的token（如特殊token和标点符号），导致注意力噪声。尽管现有方法（如Cog Attention和Differential Transformer）通过引入负注意力得分来解决此问题，但存在丢弃有用信息的风险。", "method": "本文提出Integral Transformer，一种新颖的自注意力机制，通过整合从logit分布中采样的信号来去噪注意力。", "result": "该方法在减轻噪声的同时保留了对模型性能至关重要的特殊token的贡献。大量实验表明，在已建立的知识和推理语言基准上，Integral Transformer优于Vanilla、Cog和Differential注意力变体。此外，分析显示在Transformer较低层使用Vanilla自注意力可提高性能，且Integral Transformer能有效平衡上层注意力分布并减少秩坍塌。", "conclusion": "Integral Transformer是一种有效的自注意力去噪机制，它能在保留关键信息的同时提升模型在知识和推理任务上的表现，并通过平衡注意力分布和减少秩坍塌来优化Transformer上层的行为。"}}
{"id": "2508.18315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18315", "abs": "https://arxiv.org/abs/2508.18315", "authors": ["Nowshin Sharmily", "Rusab Sarmun", "Muhammad E. H. Chowdhury", "Mir Hamidul Hussain", "Saad Bin Abul Kashem", "Molla E Majid", "Amith Khandakar"], "title": "Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset", "comment": null, "summary": "Illegal landfills are posing as a hazardous threat to people all over the\nworld. Due to the arduous nature of manually identifying the location of\nlandfill, many landfills go unnoticed by authorities and later cause dangerous\nharm to people and environment. Deep learning can play a significant role in\nidentifying these landfills while saving valuable time, manpower and resources.\nDespite being a burning concern, good quality publicly released datasets for\nillegal landfill detection are hard to find due to security concerns. However,\nAerialWaste Dataset is a large collection of 10434 images of Lombardy region of\nItaly. The images are of varying qualities, collected from three different\nsources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains\nprofessionally curated, diverse and high-quality images which makes it\nparticularly suitable for scalable and impactful research. As we trained\nseveral models to compare results, we found complex and heavy models to be\nprone to overfitting and memorizing training data instead of learning patterns.\nTherefore, we chose lightweight simpler models which could leverage general\nfeatures from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,\nMobileVit and other lightweight deep learning models were used to train and\nvalidate the dataset as they achieved significant success with less\noverfitting. As we saw substantial improvement in the performance using some of\nthese models, we combined the best performing models and came up with an\nensemble model. With the help of ensemble and fusion technique, binary\nclassification could be performed on this dataset with 92.33% accuracy, 92.67%\nprecision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.", "AI": {"tldr": "本文利用新发布的AerialWaste数据集和轻量级深度学习模型（包括集成模型）来检测非法垃圾填埋场，实现了高精度分类。", "motivation": "非法垃圾填埋场对全球构成危害，但人工识别困难且耗时，导致许多垃圾场未被发现。深度学习有望解决此问题，但高质量的公开数据集稀缺。", "method": "研究使用了包含10434张意大利伦巴第地区图像的AerialWaste数据集。为避免过拟合，选择了Mobilenetv2、Googlenet、Densenet、MobileVit等轻量级深度学习模型进行训练和验证。最终，将表现最佳的模型进行组合，构建了一个集成模型，并结合融合技术进行二元分类。", "result": "通过集成和融合技术，该模型在非法垃圾填埋场二元分类任务中达到了92.33%的准确率、92.67%的精确率、92.33%的灵敏度、92.41%的F1分数和92.71%的特异性。", "conclusion": "轻量级模型结合集成方法，在AerialWaste数据集上能有效且高精度地识别非法垃圾填埋场，证明了该方法在解决这一环境问题上的潜力。"}}
{"id": "2508.18719", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.18719", "abs": "https://arxiv.org/abs/2508.18719", "authors": ["Alessio Moreschini", "Wei He", "Romeo Ortega", "Yiheng Lu", "Tao Li"], "title": "Globally Stable Discrete Time PID Passivity-based Control of Power Converters: Simulation and Experimental Results", "comment": null, "summary": "The key idea behind PID Passivity-based Control (PID-PBC) is to leverage the\npassivity property of PIDs (for all positive gains) and wrap the PID controller\naround a passive output to ensure global stability in closed-loop. However, the\npractical applicability of PID-PBC is stymied by two key facts: (i) the vast\nmajority of practical implementations of PIDs is carried-out in discrete time\n-- discretizing the continuous time dynamical system of the PID; (ii) the\nwell-known problem that passivity is not preserved upon discretization, even\nwith small sampling times. Therefore, two aspects of the PID-PBC must be\nrevisited for its safe practical application. First, we propose a\ndiscretization of the PID that ensures its passivity. Second, since the output\nthat is identified as passive for the continuous time system is not necessarily\npassive for its discrete time version, we construct a new output that ensures\nthe passivity property for the discretization of the system. In this paper, we\nprovide a constructive answer to both issues for the case of power converter\nmodels. Instrumental to achieve this objective is the use of the implicit\nmidpoint discretization method -- which is a symplectic integration technique\nthat preserves system invariants. Since the reference value for the output to\nbe regulated in power converters is non-zero, we are henceforth interested in\nthe property of passivity of the incremental model -- currently known as\nshifted passivity. Therefore, we demonstrate that the resulting discrete-time\nPID-PBC defines a passive map for the incremental model and establish shifted\npassivity for the discretized power converter model. Combining these\nproperties, we prove global stability for the feedback interconnection of the\npower converter with the discretized PID-PBC. The paper also presents\nsimulations and experiments that demonstrate the performance of the proposed\ndiscretization.", "AI": {"tldr": "本文提出了一种离散时间PID无源性控制（PID-PBC）方法，通过确保PID控制器和系统输出的无源性，解决了连续时间PID-PBC在离散化后无源性丧失的问题，并证明了其全局稳定性，特别适用于电力变换器。", "motivation": "连续时间PID无源性控制（PID-PBC）利用PID的无源性实现闭环系统全局稳定性。然而，实际应用中PID多为离散时间实现，且离散化过程会破坏无源性，即使采样时间很小，导致PID-PBC的实用性受限。", "method": "1. 提出了一种确保PID控制器无源性的离散化方法。2. 构建了一个新的输出，以确保离散化系统的无源性。3. 采用隐式中点离散化方法（一种保留系统不变量的辛积分技术）。4. 关注增量模型的无源性（即移位无源性），因为电力变换器输出参考值非零。5. 通过理论分析、仿真和实验验证了所提方法的性能。", "result": "1. 证明了所得的离散时间PID-PBC为增量模型定义了一个无源映射。2. 为离散化电力变换器模型建立了移位无源性。3. 结合这些特性，证明了电力变换器与离散化PID-PBC反馈互连的全局稳定性。4. 仿真和实验结果验证了所提出离散化方法的性能。", "conclusion": "通过提出一种新的离散化PID控制器和构建新的系统输出，并利用隐式中点离散化方法，本文成功解决了离散时间PID-PBC中的无源性保持问题。对于电力变换器模型，证明了其移位无源性和全局稳定性，从而为PID-PBC在实际离散时间系统中的安全应用提供了建设性解决方案。"}}
{"id": "2508.18515", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18515", "abs": "https://arxiv.org/abs/2508.18515", "authors": ["Dillon Z. Chen"], "title": "Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study", "comment": "Extended version of ECAI 2025 paper", "summary": "Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine\nlearning tool for learning to plan and search. They have been shown to be both\ntheoretically and empirically superior to existing deep learning approaches for\nlearning value functions for search in symbolic planning. In this paper, we\nintroduce new WLF hyperparameters and study their various tradeoffs and\neffects. We utilise the efficiency of WLFs and run planning experiments on\nsingle core CPUs with a sample size of 1,000,000 to understand the effect of\nhyperparameters on training and planning. Our experimental analysis show that\nthere is a robust and best set of hyperparameters for WLFs across the tested\nplanning domains. We find that the best WLF hyperparameters for learning\nheuristic functions minimise execution time rather than maximise model\nexpressivity. We further statistically analyse and observe no significant\ncorrelation between training and planning metrics.", "AI": {"tldr": "本文研究了Weisfeiler-Leman特征（WLFs）的超参数，通过大规模实验找到了一组在符号规划中表现最佳且稳健的超参数，该超参数集侧重于最小化执行时间而非最大化模型表达能力，并发现训练和规划指标之间没有显著相关性。", "motivation": "Weisfeiler-Leman特征（WLFs）在符号规划中学习值函数方面被证明优于现有深度学习方法，但其超参数的影响、权衡和最优设置尚未被充分研究。", "method": "引入新的WLF超参数，并利用WLFs的高效率，在单核CPU上运行了包含一百万样本的规划实验，以研究超参数对训练和规划的影响。此外，还对训练和规划指标进行了统计分析。", "result": "实验分析表明，在所测试的规划领域中存在一组稳健且最佳的WLF超参数。研究发现，学习启发式函数的最佳WLF超参数旨在最小化执行时间，而非最大化模型表达能力。统计分析还发现训练和规划指标之间没有显著相关性。", "conclusion": "存在一套在符号规划中适用于WLFs的稳健且优化的超参数配置，该配置应优先考虑最小化执行时间。此外，训练指标可能无法直接预测规划性能。"}}
{"id": "2508.18606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18606", "abs": "https://arxiv.org/abs/2508.18606", "authors": ["Nicky Zimmerman", "Joel Loo", "Ayush Agrawal", "David Hsu"], "title": "SignLoc: Robust Localization using Navigation Signs and Public Maps", "comment": "Under submission for Robotics and Automation Letters (RA-L)", "summary": "Navigation signs and maps, such as floor plans and street maps, are widely\navailable and serve as ubiquitous aids for way-finding in human environments.\nYet, they are rarely used by robot systems. This paper presents SignLoc, a\nglobal localization method that leverages navigation signs to localize the\nrobot on publicly available maps -- specifically floor plans and OpenStreetMap\n(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a\nnavigation graph from the input map. It then employs a probabilistic\nobservation model to match directional and locational cues from the detected\nsigns to the graph, enabling robust topo-semantic localization within a Monte\nCarlo framework. We evaluated SignLoc in diverse large-scale environments: part\nof a university campus, a shopping mall, and a hospital complex. Experimental\nresults show that SignLoc reliably localizes the robot after observing only one\nto two signs.", "AI": {"tldr": "SignLoc是一种利用导航标志和公开地图（如平面图和OpenStreetMap）进行机器人全局定位的方法，无需预先传感器建图，通过匹配标志信息到导航图实现鲁棒的拓扑语义定位。", "motivation": "导航标志和地图在人类环境中普遍存在，是寻路的重要辅助工具，但机器人系统很少利用它们进行定位，尤其是在没有预先传感器建图的情况下。", "method": "SignLoc首先从输入地图中提取导航图。然后，它采用概率观测模型，将检测到的标志中的方向和位置线索与导航图进行匹配。最后，在蒙特卡洛框架内实现鲁棒的拓扑语义定位。", "result": "在大学校园、购物中心和医院综合体等多样化的大规模环境中进行评估，实验结果表明SignLoc在观察一到两个标志后就能可靠地定位机器人。", "conclusion": "SignLoc是一种有效的机器人全局定位方法，它成功地利用导航标志和公开地图，实现了在没有预先传感器建图的情况下，通过少量标志观测即可进行可靠定位。"}}
{"id": "2508.18395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18395", "abs": "https://arxiv.org/abs/2508.18395", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.", "AI": {"tldr": "LSC是一种新的LLM解码一致性选择方法，通过学习型token嵌入来选择语义最一致的响应。它在短文本和长文本基准测试上均优于现有方法，且计算开销可忽略不计。", "motivation": "大型语言模型（LLMs）的概率解码在复杂或长文本问题上经常产生不一致的输出。现有的一致性方法（如SC、USC、WUCS）要么只适用于短文本问答，要么在扩展到长文本时会牺牲短文本的准确性。", "method": "本文提出了潜在自洽性（Latent Self-Consistency, LSC）方法。它通过使用可学习的token嵌入来选择语义上最一致的响应。通过轻量级的摘要token前向生成，LSC的推理时间增加不到1%，且不需要改变模型架构。", "result": "在6个短文本和5个长文本推理基准测试（如MATH, MMLU, TruthfulQA）中，LSC在平均表现上超越了SC、USC和WUCS。同时，LSC的计算开销可以忽略不计。此外，LSC还提供了良好校准的置信度估计，在两种答案格式下都保持了较低的预期校准误差。", "conclusion": "LSC被定位为一种实用的、可靠的一致性选择方法，能够跨多种答案格式工作。它在保持低计算开销的同时，在语义一致性选择和置信度估计方面表现出色。"}}
{"id": "2508.18322", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.4"], "pdf": "https://arxiv.org/pdf/2508.18322", "abs": "https://arxiv.org/abs/2508.18322", "authors": ["Jiangfeng Sun", "Sihao He", "Zhonghong Ou", "Meina Song"], "title": "Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning", "comment": "9 pages,7 figures,conference", "summary": "Multimodal sentiment analysis (MSA) aims to infer emotional states by\neffectively integrating textual, acoustic, and visual modalities. Despite\nnotable progress, existing multimodal fusion methods often neglect\nmodality-specific structural dependencies and semantic misalignment, limiting\ntheir quality, interpretability, and robustness. To address these challenges,\nwe propose a novel framework called the Structural-Semantic Unifier (SSU),\nwhich systematically integrates modality-specific structural information and\ncross-modal semantic grounding for enhanced multimodal representations.\nSpecifically, SSU dynamically constructs modality-specific graphs by leveraging\nlinguistic syntax for text and a lightweight, text-guided attention mechanism\nfor acoustic and visual modalities, thus capturing detailed intra-modal\nrelationships and semantic interactions. We further introduce a semantic\nanchor, derived from global textual semantics, that serves as a cross-modal\nalignment hub, effectively harmonizing heterogeneous semantic spaces across\nmodalities. Additionally, we develop a multiview contrastive learning objective\nthat promotes discriminability, semantic consistency, and structural coherence\nacross intra- and inter-modal views. Extensive evaluations on two widely used\nbenchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently\nachieves state-of-the-art performance while significantly reducing\ncomputational overhead compared to prior methods. Comprehensive qualitative\nanalyses further validate SSU's interpretability and its ability to capture\nnuanced emotional patterns through semantically grounded interactions.", "AI": {"tldr": "本文提出结构-语义统一器（SSU）框架，通过整合模态特定结构信息和跨模态语义对齐，显著提升了多模态情感分析的性能、可解释性并降低了计算成本。", "motivation": "现有多模态融合方法常忽略模态特有的结构依赖和语义错位问题，导致其在质量、可解释性和鲁棒性方面受到限制。", "method": "本文提出SSU框架，其核心方法包括：1) 利用语言句法（文本）和轻量级文本引导注意力机制（声学/视觉）动态构建模态特定图，以捕捉详细的模态内关系和语义交互；2) 引入源自全局文本语义的语义锚点，作为跨模态对齐中心，有效协调异构语义空间；3) 开发多视角对比学习目标，以促进模态内和模态间视图的判别性、语义一致性和结构连贯性。", "result": "在CMU-MOSI和CMU-MOSEI两个常用基准数据集上，SSU持续实现了最先进（SOTA）的性能，并相较于现有方法显著降低了计算开销。全面的定性分析进一步验证了SSU的可解释性及其通过语义接地交互捕获细微情感模式的能力。", "conclusion": "SSU框架通过系统整合模态特定结构信息和跨模态语义对齐，有效解决了多模态情感分析中的关键挑战，实现了卓越的性能、效率和可解释性，为未来的多模态研究提供了新的方向。"}}
{"id": "2508.18813", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.18813", "abs": "https://arxiv.org/abs/2508.18813", "authors": ["Jingwei Hu", "Dave Zachariah", "Torbjörn Wigren", "Petre Stoica"], "title": "Closed-Form Input Design for Identification under Output Feedback with Perturbation Constraints", "comment": null, "summary": "In many applications, system identification experiments must be performed\nunder output feedback to ensure safety or to maintain system operation. In this\npaper, we consider the online design of informative experiments for ARMAX\nmodels by applying a bounded perturbation to the input signal generated by a\nfixed output feedback controller. Specifically, the design constrains the\nresulting output perturbation within user-specified limits and can be\nefficiently computed in closed form. We demonstrate the effectiveness of the\nmethod in two numerical experiments.", "AI": {"tldr": "本文提出了一种在线设计信息丰富实验的方法，用于在固定输出反馈控制器下识别ARMAX模型，通过对输入信号施加有界扰动，并限制输出扰动在用户指定范围内，该方法可以高效地闭式计算。", "motivation": "在许多应用中，系统辨识实验必须在输出反馈下进行，以确保系统安全或维持系统运行，这促使研究如何在这些约束下设计有效的实验。", "method": "该方法通过对由固定输出反馈控制器生成的输入信号施加有界扰动来设计实验。设计过程将由此产生的输出扰动限制在用户指定的范围内，并且可以高效地以闭式形式计算。", "result": "通过两个数值实验，作者展示了所提出方法的有效性。", "conclusion": "该方法提供了一种有效且可计算的在线实验设计方案，用于在输出反馈下识别ARMAX模型，同时确保系统输出扰动在安全范围内。"}}
{"id": "2508.18520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18520", "abs": "https://arxiv.org/abs/2508.18520", "authors": ["Dillon Z. Chen"], "title": "Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features", "comment": "HSDIP@ICAPS 2025 Workshop", "summary": "Novelty heuristics aid heuristic search by exploring states that exhibit\nnovel atoms. However, novelty heuristics are not symmetry invariant and hence\nmay sometimes lead to redundant exploration. In this preliminary report, we\npropose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms\nfor detecting novelty. WLFs are recently introduced features for learning\ndomain-dependent heuristics for generalised planning problems. We explore an\nunsupervised usage of WLFs for synthesising lifted, domain-independent novelty\nheuristics that are invariant to symmetric states. Experiments on the classical\nInternational Planning Competition and Hard To Ground benchmark suites yield\npromising results for novelty heuristics synthesised from WLFs.", "AI": {"tldr": "本报告提出使用Weisfeiler-Leman特征（WLFs）代替原子来检测规划中的新颖性，以合成对对称状态不变的启发式搜索策略。", "motivation": "现有新颖性启发式搜索方法并非对称不变，可能导致冗余探索，因此需要一种能处理对称性的新颖性检测方法。", "method": "研究者建议使用WLFs代替原子来检测新颖性。WLFs是为广义规划问题学习领域相关启发式方法而引入的特征。他们探索了WLFs的无监督使用，以合成提升的、领域无关的、对对称状态不变的新颖性启发式方法。", "result": "在国际规划竞赛（IPC）和Hard To Ground基准测试套件上的实验表明，从WLFs合成的新颖性启发式方法取得了有希望的结果。", "conclusion": "WLFs可以有效地用于合成对对称状态不变的、提升的、领域无关的新颖性启发式方法，从而改进启发式搜索效率。"}}
{"id": "2508.18627", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18627", "abs": "https://arxiv.org/abs/2508.18627", "authors": ["Ziyuan Jiao", "Yida Niu", "Zeyu Zhang", "Yangyang Wu", "Yao Su", "Yixin Zhu", "Hangxin Liu", "Song-Chun Zhu"], "title": "Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning", "comment": "20 pages, 13 figures; accepted by Transactions on Robotics", "summary": "We present a Sequential Mobile Manipulation Planning (SMMP) framework that\ncan solve long-horizon multi-step mobile manipulation tasks with coordinated\nwhole-body motion, even when interacting with articulated objects. By\nabstracting environmental structures as kinematic models and integrating them\nwith the robot's kinematics, we construct an Augmented Configuration Apace\n(A-Space) that unifies the previously separate task constraints for navigation\nand manipulation, while accounting for the joint reachability of the robot\nbase, arm, and manipulated objects. This integration facilitates efficient\nplanning within a tri-level framework: a task planner generates symbolic action\nsequences to model the evolution of A-Space, an optimization-based motion\nplanner computes continuous trajectories within A-Space to achieve desired\nconfigurations for both the robot and scene elements, and an intermediate plan\nrefinement stage selects action goals that ensure long-horizon feasibility. Our\nsimulation studies first confirm that planning in A-Space achieves an 84.6\\%\nhigher task success rate compared to baseline methods. Validation on real\nrobotic systems demonstrates fluid mobile manipulation involving (i) seven\ntypes of rigid and articulated objects across 17 distinct contexts, and (ii)\nlong-horizon tasks of up to 14 sequential steps. Our results highlight the\nsignificance of modeling scene kinematics into planning entities, rather than\nencoding task-specific constraints, offering a scalable and generalizable\napproach to complex robotic manipulation.", "AI": {"tldr": "本文提出了一个顺序移动操作规划（SMMP）框架，通过构建增强配置空间（A-Space）来统一导航和操作约束，并采用三级规划框架，有效解决了涉及关节物体的长周期多步骤移动操作任务。", "motivation": "解决长周期、多步骤移动操作任务的挑战，特别是需要协调全身运动并与关节物体交互的情况。", "method": "将环境结构抽象为运动学模型，并与机器人运动学集成，构建一个增强配置空间（A-Space），该空间统一了导航和操作约束，并考虑了机器人基座、手臂和操作物体的联合可达性。采用三级框架：任务规划器生成符号动作序列，运动规划器计算A-Space内的连续轨迹，以及中间计划细化阶段选择确保长期可行性的动作目标。", "result": "仿真研究表明，A-Space规划的任务成功率比基线方法高出84.6%。在真实机器人系统上的验证展示了流畅的移动操作，涉及7种刚性和关节物体、17种不同场景，以及长达14个顺序步骤的任务。", "conclusion": "将场景运动学建模为规划实体，而非编码任务特定约束，对于复杂机器人操作具有重要意义，提供了一种可扩展和泛化的方法。"}}
{"id": "2508.18407", "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "pdf": "https://arxiv.org/pdf/2508.18407", "abs": "https://arxiv.org/abs/2508.18407", "authors": ["Michal Štefánik", "Timothee Mickus", "Marek Kadlčík", "Michal Spiegel", "Josef Kuchař"], "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.", "AI": {"tldr": "本文质疑了OOD评估能有效反映模型在现实世界中因捷径预测而失败的假设，发现不同OOD数据集在评估模型对捷径的鲁棒性方面质量差异巨大，并提出了更鲁棒的泛化评估方法。", "motivation": "当前AI模型泛化能力的评估主要依赖于OOD数据集，但这建立在一个强假设之上：OOD评估能捕捉并反映真实部署中可能出现的失败。本文旨在挑战这一假设，并通过将OOD评估结果与QA模型中已记录的特定失败模式（如依赖虚假特征或预测捷径）进行对比。", "method": "研究人员将OOD评估的结果与现有问答（QA）模型中记录的特定失败模式（即对虚假特征或预测捷径的依赖）进行了对比。他们分析了用于QA领域OOD评估的不同数据集，以评估其在捕捉模型对捷径鲁棒性方面的质量。", "result": "研究发现，用于QA领域OOD评估的不同数据集在估计模型对捷径的鲁棒性方面提供了质量差异巨大的结果，有些甚至远低于简单的同分布（ID）评估。部分原因在于虚假捷径在ID和OOD数据集中共享，但也发现数据集的训练和评估质量可能存在显著脱节的情况。", "conclusion": "本研究强调了常用基于OOD的泛化评估方法的局限性，并为在QA领域内外更鲁棒地评估泛化能力提供了方法论和建议。"}}
{"id": "2508.18389", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18389", "abs": "https://arxiv.org/abs/2508.18389", "authors": ["Hao Liang", "Zhixuan Ge", "Ashish Tiwari", "Soumendu Majee", "G. M. Dilshan Godaliyadda", "Ashok Veeraraghavan", "Guha Balakrishnan"], "title": "FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses", "comment": "11 pages, 5 figures", "summary": "We present FastAvatar, a pose-invariant, feed-forward framework that can\ngenerate a 3D Gaussian Splatting (3DGS) model from a single face image from an\narbitrary pose in near-instant time (<10ms). FastAvatar uses a novel\nencoder-decoder neural network design to achieve both fast fitting and identity\npreservation regardless of input pose. First, FastAvatar constructs a 3DGS face\n``template'' model from a training dataset of faces with multi-view captures.\nSecond, FastAvatar encodes the input face image into an identity-specific and\npose-invariant latent embedding, and decodes this embedding to predict\nresiduals to the structural and appearance parameters of each Gaussian in the\ntemplate 3DGS model. By only inferring residuals in a feed-forward fashion,\nmodel inference is fast and robust. FastAvatar significantly outperforms\nexisting feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction\nquality, and runs 1000x faster than per-face optimization methods (e.g.,\nFlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent\nspace design supports real-time identity interpolation and attribute editing\nwhich is not possible with any existing feed-forward 3DGS face generation\nframework. FastAvatar's combination of excellent reconstruction quality and\nspeed expands the scope of 3DGS for photorealistic avatar applications in\nconsumer and interactive systems.", "AI": {"tldr": "FastAvatar是一个姿态不变的前馈框架，能从单张人脸图像在极短时间内（<10ms）生成高质量的3D高斯泼溅（3DGS）模型，并支持实时身份插值和属性编辑。", "motivation": "现有的3DGS人脸生成方法要么速度慢（基于优化），要么重建质量和身份保持不足（现有的前馈方法）。研究的动机是需要一个快速、高质量、姿态不变的3DGS人脸生成框架，以满足消费级和交互式系统中逼真人脸替身应用的需求。", "method": "FastAvatar采用新颖的编解码器神经网络设计。首先，它从多视角捕获的人脸训练数据集中构建一个3DGS人脸“模板”模型。其次，将输入的单张人脸图像编码成一个与身份相关且姿态不变的潜在嵌入。最后，解码该嵌入以预测模板3DGS模型中每个高斯点结构和外观参数的残差。通过仅以前馈方式推断残差，实现快速和鲁棒的模型推理。", "result": "FastAvatar在重建质量上显著优于现有的前馈3DGS人脸方法（如GAGAvatar），并且比基于优化的逐脸方法（如FlashAvatar、GaussianAvatars和GASP）快1000倍，能在不到10毫秒的时间内生成模型。此外，其新颖的潜在空间设计支持实时身份插值和属性编辑，这是现有前馈3DGS人脸生成框架无法实现的。", "conclusion": "FastAvatar结合了卓越的重建质量和速度，极大地扩展了3DGS在消费级和交互式系统中用于逼真人脸替身应用的范围，为实时、高质量的虚拟人脸生成提供了新的可能性。"}}
{"id": "2508.18915", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.18915", "abs": "https://arxiv.org/abs/2508.18915", "authors": ["Aboozar Heydaribeni", "Hamzeh Beyranvand"], "title": "Performance Analysis of Underwater Optical Wireless Communication Using O-RIS and Fiber Optic Backhaul (Extended version)", "comment": null, "summary": "This Letter presents a novel hybrid underwater wireless optical communication\n(UWOC) system that integrates underwater optical access points (UOAPs) with a\npassive optical network (PON)-based fiber-optic backhaul to provide a resilient\nbackbone. A hard switching mechanism is employed between direct and optical\nreconfigurable intelligent surface (O-RIS)-assisted links to ensure reliable\nconnectivity. Unlike previous studies, the proposed system is evaluated under\nboth active and multiple passive O-RIS configurations. To enhance reliability,\nthe Selection Combining (SC) and Maximal Ratio Combining (MRC) schemes are\napplied. Analytical and simulation results demonstrate that optimal O-RIS\nplacement significantly enhances system performance. However, in the linear\nregime, placing it too close to the receiver causes degradation due to\nincreased path loss and beam jitter in an identical water type. Moreover,\nincreasing the number of O-RIS elements within practical limits further\nimproves overall system performance and enhances adaptability to variations in\nthe underwater channel.", "AI": {"tldr": "本文提出了一种新型混合水下无线光通信（UWOC）系统，结合了水下光接入点（UOAPs）和基于PON的光纤回程，并通过硬切换机制在直接链路和光可重构智能表面（O-RIS）辅助链路之间切换，以确保可靠连接。", "motivation": "旨在为水下通信提供一个有弹性的骨干网络，并确保水下无线光通信系统的可靠连接。", "method": "该系统将UOAPs与基于PON的光纤回程集成，采用硬切换机制在直接链路和O-RIS辅助链路之间切换。研究在主动和多个被动O-RIS配置下进行评估，并应用了选择合并（SC）和最大比率合并（MRC）方案来增强可靠性。", "result": "分析和仿真结果表明，最优的O-RIS放置显著提升了系统性能。然而，在线性状态下，将其放置离接收器过近会导致性能下降，原因是在相同水域类型中路径损耗和光束抖动增加。此外，在实际限制内增加O-RIS元件数量可以进一步改善整体系统性能并增强对水下信道变化的适应性。", "conclusion": "所提出的混合UWOC系统通过结合O-RIS和合并方案，能够实现可靠且适应性强的通信。O-RIS的放置位置和元件数量是影响系统性能和适应性的关键因素。"}}
{"id": "2508.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18527", "abs": "https://arxiv.org/abs/2508.18527", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "Generic Guard AI in Stealth Game with Composite Potential Fields", "comment": null, "summary": "Guard patrol behavior is central to the immersion and strategic depth of\nstealth games, while most existing systems rely on hand-crafted routes or\nspecialized logic that struggle to balance coverage efficiency and responsive\npursuit with believable naturalness. We propose a generic, fully explainable,\ntraining-free framework that integrates global knowledge and local information\nvia Composite Potential Fields, combining three interpretable maps-Information,\nConfidence, and Connectivity-into a single kernel-filtered decision criterion.\nOur parametric, designer-driven approach requires only a handful of decay and\nweight parameters-no retraining-to smoothly adapt across both occupancy-grid\nand NavMesh-partition abstractions. We evaluate on five representative game\nmaps, two player-control policies, and five guard modes, confirming that our\nmethod outperforms classical baseline methods in both capture efficiency and\npatrol naturalness. Finally, we show how common stealth mechanics-distractions\nand environmental elements-integrate naturally into our framework as sub\nmodules, enabling rapid prototyping of rich, dynamic, and responsive guard\nbehaviors.", "AI": {"tldr": "本文提出了一种通用、可解释、免训练的复合势场框架，用于在潜行游戏中生成高效、自然且响应迅速的守卫巡逻行为，优于传统基线方法。", "motivation": "现有潜行游戏中的守卫巡逻系统大多依赖手动设计路线或特定逻辑，难以平衡覆盖效率、追逐响应性与行为自然度。", "method": "该方法通过复合势场整合全局知识和局部信息，将信息、置信度和连接性三张可解释地图结合成一个核过滤决策标准。它是一个参数化、设计师驱动的框架，仅需少量衰减和权重参数，无需重新训练，即可平滑适应占用网格和导航网格抽象。", "result": "在五张代表性游戏地图、两种玩家控制策略和五种守卫模式下进行评估，结果表明该方法在捕获效率和巡逻自然度方面均优于经典的基线方法。此外，常见的潜行机制（如干扰和环境元素）可以自然地整合到该框架中作为子模块。", "conclusion": "该框架能够快速原型化丰富、动态且响应迅速的守卫行为，显著提升潜行游戏的沉浸感和策略深度。"}}
{"id": "2508.18662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18662", "abs": "https://arxiv.org/abs/2508.18662", "authors": ["Stefan Ramdhan", "Winnie Trandinh", "Istvan David", "Vera Pantelic", "Mark Lawford"], "title": "Engineering Automotive Digital Twins on Standardized Architectures: A Case Study", "comment": "7 pages, 6 figures. Submitted to EDTconf 2025", "summary": "Digital twin (DT) technology has become of interest in the automotive\nindustry. There is a growing need for smarter services that utilize the unique\ncapabilities of DTs, ranging from computer-aided remote control to cloud-based\nfleet coordination. Developing such services starts with the software\narchitecture. However, the scarcity of DT architectural guidelines poses a\nchallenge for engineering automotive DTs. Currently, the only DT architectural\nstandard is the one defined in ISO 23247. Though not developed for automotive\nsystems, it is one of the few feasible starting points for automotive DTs. In\nthis work, we investigate the suitability of the ISO 23247 reference\narchitecture for developing automotive DTs. Through the case study of\ndeveloping an Adaptive Cruise Control DT for a 1/10\\textsuperscript{th}-scale\nautonomous vehicle, we identify some strengths and limitations of the reference\narchitecture and begin distilling future directions for researchers,\npractitioners, and standard developers.", "AI": {"tldr": "本文通过一个自适应巡航控制数字孪生案例研究，评估了ISO 23247参考架构在汽车数字孪生开发中的适用性，并指出了其优缺点。", "motivation": "汽车行业对利用数字孪生提供智能服务的需求日益增长，但汽车数字孪生架构指南稀缺。ISO 23247是目前唯一的数字孪生架构标准，因此需要研究其在汽车领域的适用性。", "method": "通过一个案例研究进行评估：为一辆1/10比例的自动驾驶汽车开发一个自适应巡航控制（ACC）数字孪生。", "result": "识别了ISO 23247参考架构在开发汽车数字孪生方面的优点和局限性。", "conclusion": "为研究人员、实践者和标准开发者提炼了未来研究方向，以改进汽车数字孪生架构指南和标准。"}}
{"id": "2508.18444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18444", "abs": "https://arxiv.org/abs/2508.18444", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.", "AI": {"tldr": "本文分析了不同训练方法如何影响大型语言模型（LLMs）在重排序任务中的语义理解能力和可解释性，尤其是在训练数据有限的情况下，并探讨了LLMs的可靠性问题。", "motivation": "LLMs在语义理解方面有所提升，但牺牲了透明度，导致用户难以理解其重排序的推理过程。在用户参与度低、排序数据不足的新系统中，准确重排序内容仍是挑战。此外，研究发现并非所有训练方法都能使LLMs获得准确的语义理解，有些可能只是获得了优化评估的抽象知识，从而引发对LLMs真实可靠性的质疑。", "method": "研究分析了不同训练方法对LLMs重排序任务中语义理解的影响。利用一个相对较小的环境和地球科学领域的排序数据集来重排序检索到的内容。同时，也分析了可解释信息，以评估重排序是否能通过可解释性进行推理。", "result": "分析发现，某些训练方法表现出比其他方法更好的可解释性。这暗示并非所有训练方法都能使LLMs获得准确的语义理解，有些模型可能只是获得了优化评估的抽象知识，从而对LLMs的真实可靠性提出了质疑。研究旨在探究这些模型是否能生成更具信息量的文本推理来解决透明度和数据限制的挑战。", "conclusion": "该研究旨在深入理解LLMs的不同训练方法如何影响其在重排序任务中的语义理解和生成可解释性推理的能力，以克服透明度挑战和有限训练数据的问题，并重新审视LLMs的实际可靠性。"}}
{"id": "2508.18415", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18415", "abs": "https://arxiv.org/abs/2508.18415", "authors": ["Giuseppe Stragapede", "Sam Merrick", "Vedrana Krivokuća Hahn", "Justin Sukaitis", "Vincent Graf Narbel"], "title": "Securing Face and Fingerprint Templates in Humanitarian Biometric Systems", "comment": null, "summary": "In humanitarian and emergency scenarios, the use of biometrics can\ndramatically improve the efficiency of operations, but it poses risks for the\ndata subjects, which are exacerbated in contexts of vulnerability. To address\nthis, we present a mobile biometric system implementing a biometric template\nprotection (BTP) scheme suitable for these scenarios. After rigorously\nformulating the functional, operational, and security and privacy requirements\nof these contexts, we perform a broad comparative analysis of the BTP\nlandscape. PolyProtect, a method designed to operate on neural network face\nembeddings, is identified as the most suitable method due to its effectiveness,\nmodularity, and lightweight computational burden. We evaluate PolyProtect in\nterms of verification and identification accuracy, irreversibility, and\nunlinkability, when this BTP method is applied to face embeddings extracted\nusing EdgeFace, a novel state-of-the-art efficient feature extractor, on a\nreal-world face dataset from a humanitarian field project in Ethiopia.\nMoreover, as PolyProtect promises to be modality-independent, we extend its\nevaluation to fingerprints. To the best of our knowledge, this is the first\ntime that PolyProtect has been evaluated for the identification scenario and\nfor fingerprint biometrics. Our experimental results are promising, and we plan\nto release our code", "AI": {"tldr": "本文提出并评估了一个用于人道主义和紧急情况的移动生物识别系统，该系统采用生物识别模板保护（BTP）方案（PolyProtect），以在提高效率的同时保护弱势群体的隐私。", "motivation": "在人道主义和紧急情况下，生物识别技术能显著提高操作效率，但同时对数据主体构成隐私风险，尤其是在弱势群体中。因此，需要一个既高效又能有效保护隐私的生物识别解决方案。", "method": "研究首先严格制定了这些情境下的功能、操作、安全和隐私要求。随后，对现有BTP方案进行了广泛比较分析，并选择了PolyProtect，因为它在神经网络人脸嵌入上表现出高效率、模块化和轻量级计算负担。该系统将PolyProtect应用于使用EdgeFace（一种先进高效的特征提取器）提取的人脸嵌入，并在一个来自埃塞俄比亚人道主义项目的真实人脸数据集上进行评估。此外，研究还将评估扩展到指纹生物识别，评估指标包括验证和识别准确性、不可逆性和不可链接性。", "result": "PolyProtect被确定为最适合的BTP方法，因为它有效、模块化且计算负担轻。实验结果表明，当PolyProtect应用于EdgeFace提取的人脸嵌入以及指纹生物识别时，其在验证和识别准确性、不可逆性和不可链接性方面表现出有希望的结果。这是PolyProtect首次在识别场景和指纹生物识别上进行评估。", "conclusion": "所提出的移动生物识别系统结合PolyProtect BTP方案，为在人道主义和紧急场景中实现高效且隐私保护的生物识别操作提供了一个有前景的解决方案。未来的工作将包括发布代码。"}}
{"id": "2508.19083", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19083", "abs": "https://arxiv.org/abs/2508.19083", "authors": ["Matteo Baù", "Luca Perbellini", "Samuele Grillo"], "title": "A Principled Framework to Evaluate Quality of AC-OPF Datasets for Machine Learning: Benchmarking a Novel, Scalable Generation Method", "comment": "Submitted to IEEE Transactions on Power Systems", "summary": "Several methods have been proposed in the literature to improve the quality\nof AC optimal power flow (AC-OPF) datasets used in machine learning (ML)\nmodels. Yet, scalability to large power systems remains unaddressed and\ncomparing generation approaches is still hindered by the absence of widely\naccepted metrics quantifying AC-OPF dataset quality. In this work, we tackle\nboth these limitations. We provide a simple heuristic that samples load\nsetpoints uniformly in total load active power, rather than maximizing volume\ncoverage, and solves an AC-OPF formulation with load slack variables to improve\nconvergence. For quality assessment, we formulate a multi-criteria framework\nbased on three metrics, measuring variability in the marginal distributions of\nAC-OPF primal variables, diversity in constraint activation patterns among\nAC-OPF instances and activation frequency of variable bounds. By comparing four\nopen-source methods based on these metrics, we show that our heuristic\nconsistently outperforms uniform random sampling, whether independent or\nconstrained to a convex polytope, scoring as best in terms of balance between\ndataset quality and scalability.", "AI": {"tldr": "本文提出了一种可扩展的AC-OPF数据集生成启发式方法，并通过多准则框架评估了数据集质量，结果表明其在质量和可扩展性之间取得了最佳平衡。", "motivation": "现有用于机器学习模型的AC-OPF数据集生成方法在大型电力系统上的可扩展性不足，且缺乏广泛接受的量化AC-OPF数据集质量的指标，导致不同生成方法难以比较。", "method": "1. 数据生成：提出了一种简单的启发式方法，在总负载有功功率中均匀采样负载设定点，并通过包含负载松弛变量的AC-OPF公式来提高收敛性。2. 质量评估：建立了一个基于三个指标的多准则框架，分别衡量AC-OPF原始变量边际分布的变异性、AC-OPF实例间约束激活模式的多样性以及变量边界的激活频率。", "result": "通过比较四种开源方法，本文提出的启发式方法在数据集质量和可扩展性之间取得了最佳平衡，并始终优于独立或受限于凸多面体的均匀随机采样方法。", "conclusion": "本文提出的启发式方法在AC-OPF数据集生成方面表现出色，尤其在平衡数据集质量与可扩展性方面优于其他现有方法，并为AC-OPF数据集质量评估提供了有效的多准则框架。"}}
{"id": "2508.18533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18533", "abs": "https://arxiv.org/abs/2508.18533", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "A Database-Driven Framework for 3D Level Generation with LLMs", "comment": null, "summary": "Procedural Content Generation for 3D game levels faces challenges in\nbalancing spatial coherence, navigational functionality, and adaptable gameplay\nprogression across multi-floor environments. This paper introduces a novel\nframework for generating such levels, centered on the offline, LLM-assisted\nconstruction of reusable databases for architectural components (facilities and\nroom templates) and gameplay mechanic elements. Our multi-phase pipeline\nassembles levels by: (1) selecting and arranging instances from the Room\nDatabase to form a multi-floor global structure with an inherent topological\norder; (2) optimizing the internal layout of facilities for each room based on\npredefined constraints from the Facility Database; and (3) integrating\nprogression-based gameplay mechanics by placing components from a Mechanics\nDatabase according to their topological and spatial rules. A subsequent\ntwo-phase repair system ensures navigability. This approach combines modular,\ndatabase-driven design with constraint-based optimization, allowing for\nsystematic control over level structure and the adaptable pacing of gameplay\nelements. Initial experiments validate the framework's ability in generating\ndiverse, navigable 3D environments and its capability to simulate distinct\ngameplay pacing strategies through simple parameterization. This research\nadvances PCG by presenting a scalable, database-centric foundation for the\nautomated generation of complex 3D levels with configurable gameplay\nprogression.", "AI": {"tldr": "本文提出了一种新颖的框架，用于生成具有可配置游戏进程的多层3D游戏关卡。该框架利用LLM辅助的数据库和多阶段管道，结合模块化设计和基于约束的优化，以确保空间连贯性、导航功能和适应性游戏体验。", "motivation": "3D游戏关卡的程序化内容生成（PCG）面临挑战，需要在多层环境中平衡空间连贯性、导航功能和适应性游戏进程。", "method": "该方法的核心是离线、LLM辅助构建可重用的建筑组件（设施和房间模板）和游戏机制元素的数据库。其多阶段管道包括：1) 从房间数据库选择并排列实例以形成具有拓扑顺序的多层全局结构；2) 基于设施数据库的预定义约束优化每个房间的内部设施布局；3) 根据拓扑和空间规则从机制数据库放置组件，集成基于进程的游戏机制。随后，一个两阶段修复系统确保了可导航性。该方法结合了模块化、数据库驱动的设计与基于约束的优化。", "result": "初步实验验证了该框架能够生成多样化、可导航的3D环境，并通过简单的参数化模拟不同的游戏节奏策略。", "conclusion": "这项研究通过为自动化生成具有可配置游戏进程的复杂3D关卡提供可扩展的、以数据库为中心的PCG基础，从而推动了PCG领域的发展。"}}
{"id": "2508.18691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18691", "abs": "https://arxiv.org/abs/2508.18691", "authors": ["Himanshu Gaurav Singh", "Pieter Abbeel", "Jitendra Malik", "Antonio Loquercio"], "title": "Deep Sensorimotor Control by Imitating Predictive Models of Human Motion", "comment": "Blog Post: https://hgaurav2k.github.io/trackr/", "summary": "As the embodiment gap between a robot and a human narrows, new opportunities\narise to leverage datasets of humans interacting with their surroundings for\nrobot learning. We propose a novel technique for training sensorimotor policies\nwith reinforcement learning by imitating predictive models of human motions.\nOur key insight is that the motion of keypoints on human-inspired robot\nend-effectors closely mirrors the motion of corresponding human body keypoints.\nThis enables us to use a model trained to predict future motion on human data\n\\emph{zero-shot} on robot data. We train sensorimotor policies to track the\npredictions of such a model, conditioned on a history of past robot states,\nwhile optimizing a relatively sparse task reward. This approach entirely\nbypasses gradient-based kinematic retargeting and adversarial losses, which\nlimit existing methods from fully leveraging the scale and diversity of modern\nhuman-scene interaction datasets. Empirically, we find that our approach can\nwork across robots and tasks, outperforming existing baselines by a large\nmargin. In addition, we find that tracking a human motion model can substitute\nfor carefully designed dense rewards and curricula in manipulation tasks. Code,\ndata and qualitative results available at\nhttps://jirl-upenn.github.io/track_reward/.", "AI": {"tldr": "该研究提出了一种新颖的强化学习技术，通过模仿人类运动的预测模型来训练机器人的感觉运动策略。利用人类关键点运动与机器人末端执行器关键点运动的相似性，实现了人类数据模型在机器人数据上的零样本迁移，并显著优于现有基线。", "motivation": "随着机器人与人类之间具身差距的缩小，利用人类与环境交互的数据集进行机器人学习的机会增多。然而，现有方法因梯度基运动重定向和对抗性损失的限制，未能充分利用现代人类-场景交互数据集的规模和多样性。", "method": "研究提出通过模仿人类运动的预测模型来训练感觉运动策略。其核心思想是，机器人末端执行器上受人类启发设计的关键点运动与相应的人体关键点运动密切相关。这使得在人类数据上训练的未来运动预测模型可以“零样本”应用于机器人数据。通过让感觉运动策略跟踪这些预测（以过去机器人状态历史为条件），同时优化相对稀疏的任务奖励来训练策略。此方法完全绕过了梯度基运动重定向和对抗性损失。", "result": "实验结果表明，该方法适用于多种机器人和任务，并以显著优势超越了现有基线。此外，研究发现跟踪人类运动模型可以替代在操作任务中精心设计的密集奖励和课程。", "conclusion": "该研究提出了一种有效利用人类运动数据进行机器人学习的新方法，通过零样本迁移和模仿人类运动预测模型，不仅提高了机器人学习性能，还简化了奖励设计，展现了在不同机器人和任务中的普适性。"}}
{"id": "2508.18466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18466", "abs": "https://arxiv.org/abs/2508.18466", "authors": ["Alina Wróblewska", "Bartosz Żuk"], "title": "Integrating gender inclusivity into large language models via instruction tuning", "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.", "AI": {"tldr": "本研究通过使用IPIS数据集和性别包容性提示对大型语言模型（LLMs）进行微调，旨在解决当代波兰语中固有的阳性偏见，并减轻波兰语生成中的性别偏见。", "motivation": "当代波兰语中，由于历史和政治惯例，阳性形式被普遍用于指代男性、女性和混合性别群体。这种不公平的语言系统导致在波兰语文本上训练的LLMs继承并强化了这种阳性偏见，生成性别不平衡的输出，从而产生社会后果。", "method": "本研究通过以下方法解决该问题：1) 使用IPIS数据集（包含人工制作的波兰语性别包容性校对和波兰语到英语翻译指令）对LLMs进行微调。2) 基于理论语言学框架，设计一个包含明确波兰语性别包容性指南的系统提示。3) 对多语言LLMs（Llama-8B, Mistral-7B, Mistral-Nemo）和波兰语专用LLMs（Bielik, PLLuM）进行IPIS微调。", "result": "本研究的方法旨在将性别包容性作为这些模型的固有特征进行整合，提供一种系统性解决方案，以减轻波兰语生成中的性别偏见。", "conclusion": "通过IPIS微调和明确的性别包容性指南，本研究提供了一种系统性解决方案，旨在将性别包容性整合到LLMs中，从而有效缓解波兰语生成中的性别偏见问题。"}}
{"id": "2508.18421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18421", "abs": "https://arxiv.org/abs/2508.18421", "authors": ["Fatemeh Ziaeetabar"], "title": "Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?", "comment": null, "summary": "Vision foundation models (FMs) have become the predominant architecture in\ncomputer vision, providing highly transferable representations learned from\nlarge-scale, multimodal corpora. Nonetheless, they exhibit persistent\nlimitations on tasks that require explicit reasoning over entities, roles, and\nspatio-temporal relations. Such relational competence is indispensable for\nfine-grained human activity recognition, egocentric video understanding, and\nmultimodal medical image analysis, where spatial, temporal, and semantic\ndependencies are decisive for performance. We advance the position that\nnext-generation FMs should incorporate explicit relational interfaces,\ninstantiated as dynamic relational graphs (graphs whose topology and edge\nsemantics are inferred from the input and task context). We illustrate this\nposition with cross-domain evidence from recent systems in human manipulation\naction recognition and brain tumor segmentation, showing that augmenting FMs\nwith lightweight, context-adaptive graph-reasoning modules improves\nfine-grained semantic fidelity, out of distribution robustness,\ninterpretability, and computational efficiency relative to FM only baselines.\nImportantly, by reasoning sparsely over semantic nodes, such hybrids also\nachieve favorable memory and hardware efficiency, enabling deployment under\npractical resource constraints. We conclude with a targeted research agenda for\nFM graph hybrids, prioritizing learned dynamic graph construction, multi-level\nrelational reasoning (e.g., part object scene in activity understanding, or\nregion organ in medical imaging), cross-modal fusion, and evaluation protocols\nthat directly probe relational competence in structured vision tasks.", "AI": {"tldr": "本文提出下一代视觉基础模型（FMs）应整合动态关系图接口，以解决当前FMs在实体、角色和时空关系推理方面的局限性，从而提升细粒度任务的性能、鲁棒性和效率。", "motivation": "视觉基础模型（FMs）在计算机视觉领域占据主导地位，但它们在需要对实体、角色和时空关系进行显式推理的任务中表现出持续的局限性。这种关系推理能力对于细粒度人类活动识别、第一视角视频理解和多模态医学图像分析等任务至关重要，因为空间、时间和语义依赖关系对性能具有决定性影响。", "method": "本文主张下一代FMs应整合显式关系接口，具体实现为动态关系图（即拓扑结构和边缘语义从输入和任务上下文推断的图）。通过用轻量级、上下文自适应的图推理模块增强FMs来验证这一观点。", "result": "跨领域证据（例如人类操作动作识别和脑肿瘤分割系统）表明，与仅使用FMs的基线相比，通过图推理模块增强FMs可以提高细粒度语义保真度、分布外鲁棒性、可解释性和计算效率。此外，通过稀疏地对语义节点进行推理，这种混合模型还实现了更好的内存和硬件效率，使其能够在实际资源限制下部署。", "conclusion": "FMs与图的混合模型是未来发展方向，并提出了一个有针对性的研究议程，优先考虑学习动态图构建、多级关系推理（例如活动理解中的部分-对象-场景，或医学成像中的区域-器官）、跨模态融合以及直接探测结构化视觉任务中关系能力的评估协议。"}}
{"id": "2508.19146", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19146", "abs": "https://arxiv.org/abs/2508.19146", "authors": ["Farshad Amani", "Amin Kargarian", "Ramachandran Vaidyanathan"], "title": "Learning Interior Point Method for AC and DC Optimal Power Flow", "comment": null, "summary": "This paper proposes a feasibility-guaranteed learning interior point method\n(L-IPM) to solve both AC and DC optimal power flow (OPF) problems. Given the\ncriticality of OPF, the proposed L-IPM uses a hybrid learning model approach\nrather than relying solely on a simple black-box prediction. The traditional\nIPM follows a central path from an initial point to the optimal solution.\nHowever, each iteration involves solving large linear systems, which becomes\nincreasingly expensive as the matrices grow more ill-conditioned in later\nsteps. To address this, we model the IPM trajectory as a time series and train\na Long Short-Term Memory (LSTM) network to project the IPM central path using\nonly the first few stable iterations, which carry the most informative features\nabout the path to optimality. We introduce a grid-informed methodology that\nenforces operational constraints on generation, voltage magnitudes, and line\nflows to ensure feasibility. The grid-informed LSTM serves as a tool for the\nIPM central path projection and, followed by a final IPM refinement step,\nsignificantly reduces the total number of iterations and time required for\nconvergence. We use a sampling method to generate a wide range of load\nscenarios to improve generalization across diverse operating conditions,\nefficiently covering the power system's operational space. Simulation results\non a 2869-bus European high-voltage transmission system show that the proposed\nL-IPM significantly reduces solution time by up to 94\\%, while maintaining\naccuracy and feasibility of the solution. By leveraging early iterations and\nbypassing the final ill-conditioned and computationally demanding steps of\ntraditional IPM, the proposed L-IPM reduces the number of required iterations\nby up to 85.5\\%. Since solution feasibility is also guaranteed, L-IPM\noutperforms the conventional IPM in both computational efficiency and\nrobustness.", "AI": {"tldr": "本文提出了一种名为L-IPM的可行性保证学习内点法，用于解决交流和直流最优潮流(OPF)问题。该方法结合LSTM预测内点法中心路径，并在早期稳定迭代后进行投影，显著减少了计算时间和迭代次数，同时保证了解决方案的准确性和可行性。", "motivation": "传统内点法(IPM)在每次迭代中需要求解大型线性系统，随着迭代次数增加，矩阵条件数恶化，计算成本急剧上升。为了解决这一问题，研究旨在开发一种更高效、更鲁棒的OPF求解方法。", "method": "L-IPM采用混合学习模型。它将IPM轨迹建模为时间序列，并使用长短期记忆(LSTM)网络，仅利用前几个稳定迭代（包含最有信息量的特征）来预测IPM中心路径。引入了网格信息方法来强制执行发电机、电压幅值和线路潮流的操作约束，以确保可行性。LSTM作为IPM中心路径投影工具，随后进行最终的IPM精炼步骤。通过采样方法生成广泛的负荷场景以提高泛化能力。", "result": "在2869母线欧洲高压输电系统上的仿真结果表明，L-IPM将求解时间最多减少94%，并将所需迭代次数最多减少85.5%，同时保持了解决方案的准确性和可行性。通过利用早期迭代并绕过传统IPM后期条件恶劣且计算量大的步骤，显著提升了效率。", "conclusion": "L-IPM在计算效率和鲁棒性方面均优于传统IPM，同时保证了解决方案的可行性。它通过结合学习模型和精炼步骤，有效解决了传统内点法在后期迭代中的计算瓶颈。"}}
{"id": "2508.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18554", "abs": "https://arxiv.org/abs/2508.18554", "authors": ["Lily Jiaxin Wan", "Chia-Tung Ho", "Rongjian Liang", "Cunxi Yu", "Deming Chen", "Haoxing Ren"], "title": "SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting", "comment": "18 pages, 16 figures, under review for AAAI2026", "summary": "Log schema extraction is the process of deriving human-readable templates\nfrom massive volumes of log data, which is essential yet notoriously\nlabor-intensive. Recent studies have attempted to streamline this task by\nleveraging Large Language Models (LLMs) for automated schema extraction.\nHowever, existing methods invariably rely on predefined regular expressions,\nnecessitating human domain expertise and severely limiting productivity gains.\nTo fundamentally address this limitation, we introduce SchemaCoder, the first\nfully automated schema extraction framework applicable to a wide range of log\nfile formats without requiring human customization within the flow. At its\ncore, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting\nmechanism that iteratively refines schema extraction through targeted, adaptive\nqueries driven by LLMs. Particularly, our method partitions logs into semantic\nchunks via context-bounded segmentation, selects representative patterns using\nembedding-based sampling, and generates schema code through hierarchical\nQ-Tree-driven LLM queries, iteratively refined by our textual-residual\nevolutionary optimizer and residual boosting. Experimental validation\ndemonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,\nachieving an average improvement of 21.3% over state-of-the-arts.", "AI": {"tldr": "SchemaCoder是一个完全自动化的日志模式提取框架，它利用LLM和新颖的残差问题树（Q-Tree）增强机制，无需人工干预即可从各种日志格式中提取模式，并在LogHub-2.0基准测试中显著优于现有技术。", "motivation": "现有的日志模式提取方法，即使是利用大型语言模型（LLMs），也普遍依赖预定义的正则表达式，这需要人类领域专业知识，并严重限制了生产力提升。研究旨在根本性地解决这一限制。", "method": "SchemaCoder引入了残差问题树（Q-Tree）增强机制，通过LLM驱动的迭代、自适应查询来优化模式提取。具体方法包括：通过上下文边界分割将日志划分为语义块，使用基于嵌入的采样选择代表性模式，通过分层Q-Tree驱动的LLM查询生成模式代码，并通过文本残差进化优化器和残差增强进行迭代细化。", "result": "在广泛使用的LogHub-2.0基准测试中，SchemaCoder的实验验证结果显示，其性能平均比现有最先进技术提高了21.3%。", "conclusion": "SchemaCoder是首个无需人工定制即可适用于各种日志文件格式的完全自动化模式提取框架，通过其创新的残差问题树增强机制，显著提升了日志模式提取的自动化程度和性能。"}}
{"id": "2508.18694", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18694", "abs": "https://arxiv.org/abs/2508.18694", "authors": ["Jaehwan Jeong", "Tuan-Anh Vu", "Mohammad Jony", "Shahab Ahmad", "Md. Mukhlesur Rahman", "Sangpil Kim", "M. Khalid Jawed"], "title": "AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot", "comment": null, "summary": "Existing datasets for precision agriculture have primarily been collected in\nstatic or controlled environments such as indoor labs or greenhouses, often\nwith limited sensor diversity and restricted temporal span. These conditions\nfail to reflect the dynamic nature of real farmland, including illumination\nchanges, crop growth variation, and natural disturbances. As a result, models\ntrained on such data often lack robustness and generalization when applied to\nreal-world field scenarios. In this paper, we present AgriChrono, a novel\nrobotic data collection platform and multi-modal dataset designed to capture\nthe dynamic conditions of real-world agricultural environments. Our platform\nintegrates multiple sensors and enables remote, time-synchronized acquisition\nof RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable\nlong-term data collection across varying illumination and crop growth stages.\nWe benchmark a range of state-of-the-art 3D reconstruction models on the\nAgriChrono dataset, highlighting the difficulty of reconstruction in real-world\nfield environments and demonstrating its value as a research asset for\nadvancing model generalization under dynamic conditions. The code and dataset\nare publicly available at: https://github.com/StructuresComp/agri-chrono", "AI": {"tldr": "本文介绍了AgriChrono，一个用于在真实农田环境中收集多模态、长期、时间同步数据的机器人平台和数据集，旨在解决现有农业数据集的局限性，并促进模型在动态条件下的泛化能力。", "motivation": "现有的精准农业数据集主要在静态或受控环境中（如室内实验室或温室）收集，传感器多样性有限，时间跨度短。这些条件无法反映真实农田的动态性质（如光照变化、作物生长变化和自然干扰），导致在此类数据上训练的模型在实际应用中缺乏鲁棒性和泛化能力。", "method": "研究人员开发了AgriChrono，一个新型机器人数据收集平台，集成了RGB、深度、激光雷达和IMU等多种传感器，支持远程、时间同步地长期数据采集。该平台能够在不同光照和作物生长阶段高效且可重复地收集数据，以捕捉真实农业环境的动态条件。", "result": "AgriChrono平台成功收集了一个多模态数据集，反映了真实世界的动态农业环境。研究人员在该数据集上对一系列最先进的3D重建模型进行了基准测试，结果突出了在真实农田环境中进行重建的难度，并证明了该数据集作为研究资产对于提高模型在动态条件下泛化能力的价值。", "conclusion": "AgriChrono数据集和平台为解决现有农业数据集的局限性提供了有效方案，它是一个宝贵的研究资源，有助于推动模型在动态农业环境中的泛化能力和鲁棒性。"}}
{"id": "2508.18473", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18473", "abs": "https://arxiv.org/abs/2508.18473", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.", "AI": {"tldr": "本文将大型语言模型（LLMs）的幻觉检测问题形式化为假设检验问题，并提出了一种受多重检验启发的鲁棒方法，实验证明其优于现有技术。", "motivation": "大型语言模型（LLMs）虽然功能强大，但容易产生听起来自信但实际上不正确甚至荒谬的“幻觉”，这促使研究人员寻求有效的幻觉检测方法。", "method": "研究将幻觉检测问题形式化为假设检验问题，并将其与机器学习模型中的分布外（OOD）检测问题进行类比。提出了一种受多重检验启发的检测方法。", "result": "通过广泛的实验结果，验证了所提出方法相对于现有最先进方法的鲁棒性。", "conclusion": "本文提出的基于多重检验的幻觉检测方法在检测大型语言模型幻觉方面表现出强大的鲁棒性，优于现有技术。"}}
{"id": "2508.18425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18425", "abs": "https://arxiv.org/abs/2508.18425", "authors": ["Lucas Wojcik", "Gabriel E. Lima", "Valfride Nascimento", "Eduil Nascimento Jr.", "Rayson Laroca", "David Menotti"], "title": "LPLC: A Dataset for License Plate Legibility Classification", "comment": "Accepted for presentation at the Conference on Graphics, Patterns and\n  Images (SIBGRAPI) 2025", "summary": "Automatic License Plate Recognition (ALPR) faces a major challenge when\ndealing with illegible license plates (LPs). While reconstruction methods such\nas super-resolution (SR) have emerged, the core issue of recognizing these\nlow-quality LPs remains unresolved. To optimize model performance and\ncomputational efficiency, image pre-processing should be applied selectively to\ncases that require enhanced legibility. To support research in this area, we\nintroduce a novel dataset comprising 10,210 images of vehicles with 12,687\nannotated LPs for legibility classification (the LPLC dataset). The images span\na wide range of vehicle types, lighting conditions, and camera/image quality\nlevels. We adopt a fine-grained annotation strategy that includes vehicle- and\nLP-level occlusions, four legibility categories (perfect, good, poor, and\nillegible), and character labels for three categories (excluding illegible\nLPs). As a benchmark, we propose a classification task using three image\nrecognition networks to determine whether an LP image is good enough, requires\nsuper-resolution, or is completely unrecoverable. The overall F1 score, which\nremained below 80% for all three baseline models (ViT, ResNet, and YOLO),\ntogether with the analyses of SR and LP recognition methods, highlights the\ndifficulty of the task and reinforces the need for further research. The\nproposed dataset is publicly available at\nhttps://github.com/lmlwojcik/lplc-dataset.", "AI": {"tldr": "本文介绍了一个新的车牌清晰度分类数据集（LPLC），包含10,210张图像和12,687个标注车牌，用于解决自动车牌识别中模糊车牌的挑战。基线模型的F1分数低于80%，突显了该任务的难度，并强调了进一步研究的必要性。", "motivation": "自动车牌识别（ALPR）在处理模糊车牌时面临重大挑战。尽管超分辨率（SR）等重建方法已出现，但识别低质量车牌的核心问题仍未解决。研究旨在通过选择性图像预处理来优化模型性能和计算效率，这需要一个支持车牌清晰度分类的研究数据集。", "method": "研究引入了一个包含10,210张车辆图像和12,687个标注车牌的新数据集（LPLC），涵盖了广泛的车辆类型、光照条件和图像质量。采用了细粒度标注策略，包括车辆和车牌级别的遮挡、四种清晰度类别（完美、良好、差、模糊）以及三种清晰度类别（不包括模糊车牌）的字符标签。作为基准，提出了一个分类任务，使用ViT、ResNet和YOLO三种图像识别网络来判断车牌图像是否足够好、需要超分辨率处理，还是完全无法恢复。", "result": "所有三种基线模型（ViT、ResNet和YOLO）的整体F1分数均低于80%。对超分辨率和车牌识别方法的分析进一步强调了该任务的难度。这表明现有模型在车牌清晰度分类方面表现不佳。", "conclusion": "车牌清晰度分类任务极具挑战性，现有模型（如ViT、ResNet、YOLO）在该任务上的表现不尽理想，F1分数低于80%。研究结果以及对SR和车牌识别方法的分析，共同强化了在该领域进行进一步研究的必要性。所提出的LPLC数据集将为未来的研究提供支持。"}}
{"id": "2508.19159", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.19159", "abs": "https://arxiv.org/abs/2508.19159", "authors": ["Ersin Das", "Rahal Nanayakkara", "Xiao Tan", "Ryan M. Bena", "Joel W. Burdick", "Paulo Tabuada", "Aaron D. Ames"], "title": "Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions", "comment": null, "summary": "Measurements and state estimates are often imperfect in control practice,\nposing challenges for safety-critical applications, where safety guarantees\nrely on accurate state information. In the presence of estimation errors,\nseveral prior robust control barrier function (R-CBF) formulations have imposed\nstrict conditions on the input. These methods can be overly conservative and\ncan introduce issues such as infeasibility, high control effort, etc. This work\nproposes a systematic method to improve R-CBFs, and demonstrates its advantages\non a tracked vehicle that navigates among multiple obstacles. A primary\ncontribution is a new optimization-based online parameter adaptation scheme\nthat reduces the conservativeness of existing R-CBFs. In order to reduce the\ncomplexity of the parameter optimization, we merge several safety constraints\ninto one unified numerical CBF via Poisson's equation. We further address the\ndual relative degree issue that typically causes difficulty in vehicle\ntracking. Experimental trials demonstrate the overall performance improvement\nof our approach over existing formulations.", "AI": {"tldr": "本文提出了一种改进鲁棒控制障碍函数（R-CBF）的方法，通过在线参数自适应和统一约束来减少保守性，并在履带式车辆避障实验中验证了其性能提升。", "motivation": "在控制实践中，测量和状态估计通常不完善，这给依赖精确状态信息的安全关键应用带来了挑战。现有的鲁棒控制障碍函数（R-CBF）方法在存在估计误差时过于保守，可能导致不可行、控制力过大等问题。", "method": "本文提出了一种系统性的R-CBF改进方法，包括：1) 一种基于优化的在线参数自适应方案，用于减少现有R-CBF的保守性。2) 通过泊松方程将多个安全约束合并为一个统一的数值CBF，以降低参数优化的复杂性。3) 解决了车辆跟踪中常见的双相对度问题。", "result": "实验结果表明，与现有方法相比，本文提出的方法在整体性能上有所提升，并在履带式车辆在多个障碍物之间导航的场景中展示了其优势。", "conclusion": "本文提出了一种系统性的方法来改进鲁棒控制障碍函数，通过在线参数自适应和统一约束显著降低了保守性，并有效解决了车辆跟踪中的相关问题，从而在安全关键应用中提供了更好的性能。"}}
{"id": "2508.18608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18608", "abs": "https://arxiv.org/abs/2508.18608", "authors": ["Janet Wang", "Xin Hu", "Yunbei Zhang", "Diabate Almamy", "Vagamon Bamba", "Konan Amos Sébastien Koffi", "Yao Koffi Aubin", "Zhengming Ding", "Jihun Hamm", "Rie R. Yotsu"], "title": "eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases", "comment": null, "summary": "Skin Neglected Tropical Diseases (NTDs) impose severe health and\nsocioeconomic burdens in impoverished tropical communities. Yet, advancements\nin AI-driven diagnostic support are hindered by data scarcity, particularly for\nunderrepresented populations and rare manifestations of NTDs. Existing\ndermatological datasets often lack the demographic and disease spectrum crucial\nfor developing reliable recognition models of NTDs. To address this, we\nintroduce eSkinHealth, a novel dermatological dataset collected on-site in\nC\\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from\n1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs\nand rare conditions among West African populations. We further propose an\nAI-expert collaboration paradigm to implement foundation language and\nsegmentation models for efficient generation of multimodal annotations, under\ndermatologists' guidance. In addition to patient metadata and diagnosis labels,\neSkinHealth also includes semantic lesion masks, instance-specific visual\ncaptions, and clinical concepts. Overall, our work provides a valuable new\nresource and a scalable annotation framework, aiming to catalyze the\ndevelopment of more equitable, accurate, and interpretable AI tools for global\ndermatology.", "AI": {"tldr": "本文介绍了eSkinHealth，一个在西非收集的皮肤热带病（NTDs）数据集，并提出了一种AI-专家协作范式，用于高效生成多模态标注，旨在推动全球皮肤病学中更公平、准确和可解释的AI工具发展。", "motivation": "皮肤热带病给贫困热带社区带来严重的健康和社会经济负担。然而，由于数据稀缺，特别是针对代表性不足人群和罕见病症的数据，AI驱动的诊断支持进展受阻。现有的皮肤病学数据集往往缺乏开发可靠NTDs识别模型所需的关键人口统计学和疾病谱数据。", "method": "研究团队在科特迪瓦和加纳实地收集了eSkinHealth皮肤病数据集。此外，他们提出了一种AI-专家协作范式，在皮肤科医生的指导下，利用基础语言和分割模型高效生成多模态标注，包括语义病变掩码、实例特定视觉描述和临床概念。", "result": "eSkinHealth数据集包含来自1,639个病例的5,623张图像，涵盖47种皮肤病，独特地专注于西非人群中的皮肤热带病和罕见病症。除了患者元数据和诊断标签外，该数据集还包括语义病变掩码、实例特定视觉描述和临床概念。", "conclusion": "这项工作提供了一个有价值的新资源和一个可扩展的标注框架，旨在促进开发更公平、准确和可解释的AI工具，以服务于全球皮肤病学领域。"}}
{"id": "2508.18705", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18705", "abs": "https://arxiv.org/abs/2508.18705", "authors": ["Santosh Thoduka", "Sebastian Houben", "Juergen Gall", "Paul G. Plöger"], "title": "Enhancing Video-Based Robot Failure Detection Using Task Knowledge", "comment": "Accepted at ECMR 2025", "summary": "Robust robotic task execution hinges on the reliable detection of execution\nfailures in order to trigger safe operation modes, recovery strategies, or task\nreplanning. However, many failure detection methods struggle to provide\nmeaningful performance when applied to a variety of real-world scenarios. In\nthis paper, we propose a video-based failure detection approach that uses\nspatio-temporal knowledge in the form of the actions the robot performs and\ntask-relevant objects within the field of view. Both pieces of information are\navailable in most robotic scenarios and can thus be readily obtained. We\ndemonstrate the effectiveness of our approach on three datasets that we amend,\nin part, with additional annotations of the aforementioned task-relevant\nknowledge. In light of the results, we also propose a data augmentation method\nthat improves performance by applying variable frame rates to different parts\nof the video. We observe an improvement from 77.9 to 80.0 in F1 score on the\nARMBench dataset without additional computational expense and an additional\nincrease to 81.4 with test-time augmentation. The results emphasize the\nimportance of spatio-temporal information during failure detection and suggest\nfurther investigation of suitable heuristics in future implementations. Code\nand annotations are available.", "AI": {"tldr": "本文提出了一种基于视频的机器人故障检测方法，利用机器人动作和任务相关物体的时空知识，并通过变帧率数据增强显著提高了检测性能。", "motivation": "为了实现鲁棒的机器人任务执行，需要可靠地检测执行故障以触发安全操作、恢复策略或任务重新规划。然而，许多现有的故障检测方法在各种实际场景中难以提供有意义的性能。", "method": "本文提出了一种基于视频的故障检测方法，该方法利用机器人执行的动作和视野中任务相关物体的时空知识。这些信息在大多数机器人场景中都可获得。此外，还提出了一种数据增强方法，通过对视频不同部分应用可变帧率来提高性能。该方法在三个数据集上进行了验证，其中部分数据集补充了额外的任务相关知识标注。", "result": "在ARMBench数据集上，该方法在不增加额外计算开销的情况下，F1分数从77.9提高到80.0；通过测试时增强，F1分数进一步提高到81.4。结果强调了故障检测过程中时空信息的重要性。", "conclusion": "该方法有效证明了时空信息在机器人故障检测中的重要性。研究结果表明，未来的实现中应进一步探索合适的启发式方法。"}}
{"id": "2508.18549", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18549", "abs": "https://arxiv.org/abs/2508.18549", "authors": ["Maike Züfle", "Vilém Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.", "AI": {"tldr": "本文提出两种新的自动化机器翻译评估指标（COMET-polycand和COMET-polyic），通过纳入额外的翻译信息（同源句的多候选翻译或相似源文本的带标签翻译）来弥补与人类判断的差异，显著提高了评估性能。", "motivation": "自动化机器翻译评估指标通常只考虑源句和单个译文，而人类评估时常会参考多个替代译文。这种评估设置上的差异可能负面影响自动化指标的性能。", "method": "本文提出了两种新的自动化指标：1. COMET-polycand：使用同一源句的多个替代翻译进行比较评估。2. COMET-polyic：受检索式上下文学习启发，利用相似源文本的翻译及其人工标注质量分数来指导评估。", "result": "结果显示，COMET-polycand仅增加一个额外翻译即可显著提高段落级指标性能（Kendall's tau-b相关性从0.079提升至0.118），增加更多翻译可获得进一步提升。COMET-polyic引入检索到的示例也获得了类似改进（Kendall's tau-b相关性从0.079提升至0.116）。模型已公开。", "conclusion": "通过纳入额外的上下文信息（无论是同源句的替代翻译还是相似源文本的带标签翻译），可以显著提高自动化机器翻译评估指标的性能，使其更接近人类判断。"}}
{"id": "2508.18430", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18430", "abs": "https://arxiv.org/abs/2508.18430", "authors": ["Aranya Saha", "Tanvir Ahmed Khan", "Ismam Nur Swapnil", "Mohammad Ariful Haque"], "title": "CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering", "comment": "10 pages, 8 figures, Prepared for submission to IEEE Transactions on\n  Human-Machine Systems", "summary": "Vision-language models (VLMs) have shown significant potential for medical\ntasks; however, their general-purpose nature can limit specialized diagnostic\naccuracy, and their large size poses substantial inference costs for real-world\nclinical deployment. To address these challenges, we introduce CLARIFY, a\nSpecialist-Generalist framework for dermatological visual question answering\n(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image\nclassifier (the Specialist) that provides fast and highly accurate diagnostic\npredictions, and (ii) a powerful yet compressed conversational VLM (the\nGeneralist) that generates natural language explanations to user queries. In\nour framework, the Specialist's predictions directly guide the Generalist's\nreasoning, focusing it on the correct diagnostic path. This synergy is further\nenhanced by a knowledge graph-based retrieval module, which grounds the\nGeneralist's responses in factual dermatological knowledge, ensuring both\naccuracy and reliability. This hierarchical design not only reduces diagnostic\nerrors but also significantly improves computational efficiency. Experiments on\nour curated multimodal dermatology dataset demonstrate that CLARIFY achieves an\n18\\% improvement in diagnostic accuracy over the strongest baseline, a\nfine-tuned, uncompressed single-line VLM, while reducing the average VRAM\nrequirement and latency by at least 20\\% and 5\\%, respectively. These results\nindicate that a Specialist-Generalist system provides a practical and powerful\nparadigm for building lightweight, trustworthy, and clinically viable AI\nsystems.", "AI": {"tldr": "CLARIFY是一个用于皮肤病视觉问答（VQA）的“专家-通用”框架。它结合了轻量级专家分类器和压缩型通用VLM，通过专家引导、知识图谱增强，显著提高了诊断准确性，同时降低了计算成本，使其更适用于临床部署。", "motivation": "尽管视觉-语言模型（VLMs）在医疗任务中显示出巨大潜力，但其通用性限制了专业诊断的准确性，且庞大的模型尺寸导致高昂的推理成本，难以在实际临床中部署。", "method": "CLARIFY框架包含两个核心组件：(i) 一个轻量级、领域训练的图像分类器（专家），提供快速高精度的诊断预测；(ii) 一个强大但经过压缩的对话式VLM（通用者），生成自然语言解释。专家预测直接指导通用者的推理，使其聚焦于正确的诊断路径。此外，一个基于知识图谱的检索模块进一步增强了通用者的响应，确保其准确性和可靠性。", "result": "在作者整理的多模态皮肤病数据集上，CLARIFY的诊断准确率比最强的基线（一个经过微调、未压缩的单行VLM）提高了18%。同时，平均VRAM需求和延迟分别至少降低了20%和5%。", "conclusion": "“专家-通用”系统为构建轻量级、可信赖且临床可行的AI系统提供了一个实用且强大的范例，有效解决了现有VLM在医疗领域应用中的准确性和效率挑战。"}}
{"id": "2508.19164", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19164", "abs": "https://arxiv.org/abs/2508.19164", "authors": ["Morokot Sakal", "George Nehma", "Camilo Riano-Rios", "Madhur Tiwari"], "title": "Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform", "comment": "15 pages, 10 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite\nattitude control system with reaction wheel health estimation capabilities.\nPrevious simulations and Software-in-the-Loop testing have prompted further\nexperiments to explore the validity of the controller with real momentum\nexchange devices in the loop. This work is a step toward a comprehensive\ntesting framework for validation of spacecraft attitude control algorithms. The\nproposed HIL testbed includes brushless DC motors and drivers that communicate\nusing a CAN bus, an embedded computer that executes control and adaptation\nlaws, and a satellite simulator that produces simulated sensor data, estimated\nattitude states, and responds to actions of the external actuators. We propose\nmethods to artificially induce failures on the reaction wheels, and present\nrelated issues and lessons learned.", "AI": {"tldr": "本文提出并实施了一种带有反作用轮健康估计功能的自适应卫星姿态控制系统的硬件在环（HIL）测试，旨在通过真实硬件验证控制器。", "motivation": "之前的仿真和软件在环测试促使研究人员进行进一步实验，以验证该控制器在包含真实动量交换设备的闭环系统中的有效性。这项工作是建立一个全面的航天器姿态控制算法验证测试框架的重要一步。", "method": "该研究提出了一个HIL测试平台，包括使用CAN总线通信的无刷直流电机和驱动器、执行控制和自适应律的嵌入式计算机，以及一个生成模拟传感器数据、估计姿态状态并响应外部执行器动作的卫星模拟器。此外，研究还提出了人工诱导反作用轮故障的方法。", "result": "本文介绍了在HIL测试中人工诱导反作用轮故障的相关问题和经验教训。具体成果是构建了HIL测试平台并进行了初步验证，但抽象未提及具体的量化测试结果。", "conclusion": "该工作是向建立航天器姿态控制算法综合测试框架迈出的重要一步，并通过HIL测试积累了关于故障诱导和系统验证的宝贵经验和教训。"}}
{"id": "2508.18642", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18642", "abs": "https://arxiv.org/abs/2508.18642", "authors": ["Jianxing Liao", "Tian Zhang", "Xiao Feng", "Yusong Zhang", "Rui Yang", "Haorui Wang", "Bosi Wen", "Ziying Wang", "Runzhi Shi"], "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing", "comment": null, "summary": "Large language models are extensively utilized in creative writing\napplications. Creative writing requires a balance between subjective writing\nquality (e.g., literariness and emotional expression) and objective constraint\nfollowing (e.g., format requirements and word limits). Existing reinforcement\nlearning methods struggle to balance these two aspects: single reward\nstrategies fail to improve both abilities simultaneously, while fixed-weight\nmixed-reward methods lack the ability to adapt to different writing scenarios.\nTo address this problem, we propose Reinforcement Learning with Mixed Rewards\n(RLMR), utilizing a dynamically mixed reward system from a writing reward model\nevaluating subjective writing quality and a constraint verification model\nassessing objective constraint following. The constraint following reward\nweight is adjusted dynamically according to the writing quality within sampled\ngroups, ensuring that samples violating constraints get negative advantage in\nGRPO and thus penalized during training, which is the key innovation of this\nproposed method. We conduct automated and manual evaluations across diverse\nmodel families from 8B to 72B parameters. Additionally, we construct a\nreal-world writing benchmark named WriteEval for comprehensive evaluation.\nResults illustrate that our method achieves consistent improvements in both\ninstruction following (IFEval from 83.36\\% to 86.65\\%) and writing quality\n(72.75\\% win rate in manual expert pairwise evaluations on WriteEval). To the\nbest of our knowledge, RLMR is the first work to combine subjective preferences\nwith objective verification in online RL training, providing an effective\nsolution for multi-dimensional creative writing optimization.", "AI": {"tldr": "本文提出了一种名为RLMR的强化学习方法，通过动态混合奖励系统，有效平衡了大型语言模型在创意写作中主观写作质量和客观约束遵循的需求。", "motivation": "现有强化学习方法在创意写作中难以同时提升主观写作质量（如文学性、情感表达）和客观约束遵循（如格式、字数限制）。单一奖励策略无法兼顾，而固定权重混合奖励又缺乏适应性。", "method": "RLMR方法采用动态混合奖励系统，结合评估主观写作质量的写作奖励模型和评估客观约束遵循的约束验证模型。其核心创新在于根据采样组内的写作质量动态调整约束遵循奖励的权重，确保违反约束的样本在GRPO训练中受到惩罚。", "result": "RLMR在指令遵循（IFEval从83.36%提升至86.65%）和写作质量（在WriteEval手动专家配对评估中获得72.75%的胜率）两方面均取得了显著且一致的提升。同时，构建了WriteEval真实世界写作基准进行全面评估。", "conclusion": "RLMR是首个将主观偏好与客观验证相结合的在线强化学习训练方法，为多维创意写作优化提供了一个有效的解决方案。"}}
{"id": "2508.18802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18802", "abs": "https://arxiv.org/abs/2508.18802", "authors": ["Li Sun", "Jiefeng Wu", "Feng Chen", "Ruizhe Liu", "Yanchao Yang"], "title": "HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation", "comment": null, "summary": "Effective policy learning for robotic manipulation requires scene\nrepresentations that selectively capture task-relevant environmental features.\nCurrent approaches typically employ task-agnostic representation extraction,\nfailing to emulate the dynamic perceptual adaptation observed in human\ncognition. We present HyperTASR, a hypernetwork-driven framework that modulates\nscene representations based on both task objectives and the execution phase.\nOur architecture dynamically generates representation transformation parameters\nconditioned on task specifications and progression state, enabling\nrepresentations to evolve contextually throughout task execution. This approach\nmaintains architectural compatibility with existing policy learning frameworks\nwhile fundamentally reconfiguring how visual features are processed. Unlike\nmethods that simply concatenate or fuse task embeddings with task-agnostic\nrepresentations, HyperTASR establishes computational separation between\ntask-contextual and state-dependent processing paths, enhancing learning\nefficiency and representational quality. Comprehensive evaluations in both\nsimulation and real-world environments demonstrate substantial performance\nimprovements across different representation paradigms. Through ablation\nstudies and attention visualization, we confirm that our approach selectively\nprioritizes task-relevant scene information, closely mirroring human adaptive\nperception during manipulation tasks. The project website is at\n\\href{https://lisunphil.github.io/HyperTASR_projectpage/}{lisunphil.github.io/HyperTASR\\_projectpage}.", "AI": {"tldr": "HyperTASR是一个由超网络驱动的框架，它根据任务目标和执行阶段动态调整机器人操作的场景表示，从而显著提高策略学习的效率和表示质量。", "motivation": "机器人操作的有效策略学习需要选择性地捕捉任务相关环境特征的场景表示。然而，现有方法通常采用与任务无关的表示提取，未能模拟人类认知中观察到的动态感知适应。", "method": "本文提出了HyperTASR，一个超网络驱动的框架。它根据任务规范和进展状态动态生成表示转换参数，使表示在任务执行过程中能够根据上下文演变。该方法在任务上下文和状态依赖处理路径之间建立了计算分离，而不是简单地拼接或融合任务嵌入与任务无关的表示。", "result": "在仿真和真实世界环境中的综合评估表明，HyperTASR在不同的表示范式下都取得了显著的性能提升。通过消融研究和注意力可视化，证实了该方法选择性地优先处理任务相关的场景信息，密切模仿了人类在操作任务中的适应性感知。", "conclusion": "HyperTASR通过动态调整场景表示，克服了现有任务无关表示方法的局限性，显著提高了机器人操作策略学习的效率和表示质量，并成功模仿了人类的适应性感知能力。"}}
{"id": "2508.18569", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18569", "abs": "https://arxiv.org/abs/2508.18569", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.", "AI": {"tldr": "本文提出一个自评估的视觉隐喻生成框架，侧重于隐喻对齐。该框架包含一个无需训练的S-T-M分解管道和一个轻量级训练管道，在某些指标上超越了现有基线，尤其擅长处理抽象隐喻。", "motivation": "视觉隐喻生成是一项挑战性任务，需要将源概念与目标概念绑定，同时保留意义并确保视觉连贯性。现有方法在语言理解和视觉一致性方面存在挑战。", "method": "提出一个自评估的视觉隐喻生成框架，专注于隐喻对齐。自评估结合了现有指标和新提出的“隐喻分解分数”及“意义对齐（MA）”指标。探索了两种方法：1) 无需训练的管道，将提示明确分解为源-目标-意义（S-T-M）映射进行图像合成。2) 基于训练的管道，使用提出的自评估奖励机制改进对齐，无需大规模重新训练。", "result": "在测试集上，无需训练的方法在分解、CLIP和MA分数上超越了强大的闭源基线（GPT-4o, Imagen），基于训练的方法紧随其后。用户研究显示参与者整体偏爱GPT-4o，但我们的无需训练管道在开源方法中领先，并在抽象隐喻上略优于Imagen。分析表明S-T-M提示有助于处理较长或更抽象的隐喻，而闭源模型在简短、具体的案例上表现出色；还观察到对采样器设置的敏感性。", "conclusion": "在适度计算资源下，结构化提示（S-T-M）和轻量级强化学习（RL）在隐喻对齐方面表现良好。与人类偏好之间的差距主要由美学和采样驱动。"}}
{"id": "2508.18445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18445", "abs": "https://arxiv.org/abs/2508.18445", "authors": ["Sizhuo Ma", "Wei-Ting Chen", "Qiang Gao", "Jian Wang", "Chris Wei Zhou", "Wei Sun", "Weixia Zhang", "Linhan Cao", "Jun Jia", "Xiangyang Zhu", "Dandan Zhu", "Xiongkuo Min", "Guangtao Zhai", "Baoying Chen", "Xiongwei Xiao", "Jishen Zeng", "Wei Wu", "Tiexuan Lou", "Yuchen Tan", "Chunyi Song", "Zhiwei Xu", "MohammadAli Hamidi", "Hadi Amirpour", "Mingyin Bai", "Jiawang Du", "Zhenyu Jiang", "Zilong Lu", "Ziguan Cui", "Zongliang Gan", "Xinpeng Li", "Shiqi Jiang", "Chenhui Li", "Changbo Wang", "Weijun Yuan", "Zhan Li", "Yihang Chen", "Yifan Deng", "Ruting Deng", "Zhanglu Chen", "Boyang Yao", "Shuling Zheng", "Feng Zhang", "Zhiheng Fu", "Abhishek Joshi", "Aman Agarwal", "Rakhil Immidisetti", "Ajay Narasimha Mopidevi", "Vishwajeet Shukla", "Hao Yang", "Ruikun Zhang", "Liyuan Pan", "Kaixin Deng", "Hang Ouyang", "Fan yang", "Zhizun Luo", "Zhuohang Shi", "Songning Lai", "Weilin Ruan", "Yutao Yue"], "title": "VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results", "comment": "ICCV 2025 VQualA workshop FIQA track", "summary": "Face images play a crucial role in numerous applications; however, real-world\nconditions frequently introduce degradations such as noise, blur, and\ncompression artifacts, affecting overall image quality and hindering subsequent\ntasks. To address this challenge, we organized the VQualA 2025 Challenge on\nFace Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops.\nParticipants created lightweight and efficient models (limited to 0.5 GFLOPs\nand 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on\nface images with arbitrary resolutions and realistic degradations. Submissions\nunderwent comprehensive evaluations through correlation metrics on a dataset of\nin-the-wild face images. This challenge attracted 127 participants, with 1519\nfinal submissions. This report summarizes the methodologies and findings for\nadvancing the development of practical FIQA approaches.", "AI": {"tldr": "VQualA 2025举办了一项关于面部图像质量评估（FIQA）的挑战，旨在开发轻量高效的模型，以预测真实世界退化面部图像的平均意见分数（MOS）。", "motivation": "面部图像在许多应用中至关重要，但现实世界中的噪声、模糊和压缩伪影等退化会影响图像质量并阻碍后续任务。因此，需要有效的面部图像质量评估方法。", "method": "VQualA 2025挑战赛要求参与者开发轻量级（限制0.5 GFLOPs和500万参数）且高效的模型，用于预测任意分辨率和真实退化面部图像的MOS。提交的模型通过相关性指标在“in-the-wild”面部图像数据集上进行评估。", "result": "本次挑战吸引了127名参与者，共提交了1519份最终方案。报告总结了这些方法和发现，以推动实用FIQA方法的发展。", "conclusion": "该挑战成功推动了实用面部图像质量评估（FIQA）方法的发展，通过吸引大量参与者并鼓励开发轻量高效的模型，为处理真实世界面部图像退化问题提供了宝贵的见解和解决方案。"}}
{"id": "2508.18646", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18646", "abs": "https://arxiv.org/abs/2508.18646", "authors": ["Jun Wang", "Ninglun Gu", "Kailai Zhang", "Zijiao Zhang", "Yelun Bao", "Jin Yang", "Xu Yin", "Liwei Liu", "Yihuan Liu", "Pengyong Li", "Gary G. Yen", "Junchi Yan"], "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap", "comment": "Preprint. Under review", "summary": "For Large Language Models (LLMs), a disconnect persists between benchmark\nperformance and real-world utility. Current evaluation frameworks remain\nfragmented, prioritizing technical metrics while neglecting holistic assessment\nfor deployment. This survey introduces an anthropomorphic evaluation paradigm\nthrough the lens of human intelligence, proposing a novel three-dimensional\ntaxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational\ncapacity, Emotional Quotient (EQ)-Alignment Ability for value-based\ninteractions, and Professional Quotient (PQ)-Professional Expertise for\nspecialized proficiency. For practical value, we pioneer a Value-oriented\nEvaluation (VQ) framework assessing economic viability, social impact, ethical\nalignment, and environmental sustainability. Our modular architecture\nintegrates six components with an implementation roadmap. Through analysis of\n200+ benchmarks, we identify key challenges including dynamic assessment needs\nand interpretability gaps. It provides actionable guidance for developing LLMs\nthat are technically proficient, contextually relevant, and ethically sound. We\nmaintain a curated repository of open-source evaluation resources at:\nhttps://github.com/onejune2018/Awesome-LLM-Eval.", "AI": {"tldr": "本研究提出了一种新颖的类人化（IQ、EQ、PQ）和价值导向（VQ）评估框架，旨在弥合大型语言模型（LLM）基准性能与实际应用之间的差距，以实现更全面、更符合部署需求的评估。", "motivation": "大型语言模型（LLM）的基准测试性能与实际应用价值之间存在脱节。当前的评估框架支离破碎，过于侧重技术指标，而忽略了部署所需的整体评估。", "method": "本研究引入了以人类智能为视角的类人化评估范式，提出了一个新颖的三维分类法：智商（IQ）-通用智能、情商（EQ）-对齐能力和专业商（PQ）-专业知识。同时，开创了一个价值导向评估（VQ）框架，评估经济可行性、社会影响、道德对齐和环境可持续性。研究还设计了一个包含六个组件的模块化架构和实施路线图，并分析了200多个基准测试。", "result": "研究提出了IQ（通用智能）、EQ（对齐能力）和PQ（专业知识）三维分类法，以及评估经济可行性、社会影响、道德对齐和环境可持续性的VQ框架。通过对200多个基准测试的分析，识别了动态评估需求和可解释性差距等关键挑战。研究为开发技术精湛、上下文相关且符合道德规范的LLM提供了可操作的指导，并维护了一个开源评估资源库。", "conclusion": "本研究提出的评估范式和框架为开发技术精湛、上下文相关且符合道德规范的LLM提供了有力的指导，有效弥补了现有评估框架的不足，推动LLM在实际应用中实现更全面的价值。"}}
{"id": "2508.18817", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18817", "abs": "https://arxiv.org/abs/2508.18817", "authors": ["Colin Merk", "Ismail Geles", "Jiaxu Xing", "Angel Romero", "Giorgia Ramponi", "Davide Scaramuzza"], "title": "Learning Real-World Acrobatic Flight from Human Preferences", "comment": "8 pages, 7 figures", "summary": "Preference-based reinforcement learning (PbRL) enables agents to learn\ncontrol policies without requiring manually designed reward functions, making\nit well-suited for tasks where objectives are difficult to formalize or\ninherently subjective. Acrobatic flight poses a particularly challenging\nproblem due to its complex dynamics, rapid movements, and the importance of\nprecise execution. In this work, we explore the use of PbRL for agile drone\ncontrol, focusing on the execution of dynamic maneuvers such as powerloops.\nBuilding on Preference-based Proximal Policy Optimization (Preference PPO), we\npropose Reward Ensemble under Confidence (REC), an extension to the reward\nlearning objective that improves preference modeling and learning stability.\nOur method achieves 88.4% of the shaped reward performance, compared to 55.2%\nwith standard Preference PPO. We train policies in simulation and successfully\ntransfer them to real-world drones, demonstrating multiple acrobatic maneuvers\nwhere human preferences emphasize stylistic qualities of motion. Furthermore,\nwe demonstrate the applicability of our probabilistic reward model in a\nrepresentative MuJoCo environment for continuous control. Finally, we highlight\nthe limitations of manually designed rewards, observing only 60.7% agreement\nwith human preferences. These results underscore the effectiveness of PbRL in\ncapturing complex, human-centered objectives across both physical and simulated\ndomains.", "AI": {"tldr": "本文探讨了基于偏好的强化学习（PbRL）在敏捷无人机特技飞行控制中的应用，提出了一种名为REC的新方法，显著优于标准Preference PPO，并成功实现了从模拟到真实世界的迁移，证明了PbRL在处理复杂、主观人类目标方面的有效性。", "motivation": "特技飞行由于其复杂的动力学、快速运动和对精确执行的高要求，是一个特别具有挑战性的问题。传统上，手动设计奖励函数难以形式化或捕捉此类任务中固有的主观或风格化目标，因此需要一种无需手动设计奖励函数的方法。", "method": "该研究基于“基于偏好的近端策略优化”（Preference PPO），并提出了一种名为“置信度下奖励集成”（Reward Ensemble under Confidence, REC）的扩展，以改进偏好建模和学习稳定性。研究在模拟环境中训练策略，并将其成功迁移到真实世界的无人机上，同时也在MuJoCo环境中验证了概率奖励模型的适用性。", "result": "所提出的REC方法达到了整形奖励性能的88.4%，显著优于标准Preference PPO的55.2%。研究成功地将策略从模拟环境迁移到真实世界的无人机，实现了多种特技飞行动作，并强调了人类偏好对运动风格品质的重视。此外，研究发现手动设计的奖励与人类偏好的一致性仅为60.7%，突显了其局限性。", "conclusion": "研究结果强调了PbRL在捕捉跨物理和模拟领域的复杂、以人为中心的目标方面的有效性，尤其适用于难以形式化或具有主观性的任务，如敏捷无人机特技飞行控制。"}}
{"id": "2508.18598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18598", "abs": "https://arxiv.org/abs/2508.18598", "authors": ["Colin Klein"], "title": "What do language models model? Transformers, automata, and the format of thought", "comment": null, "summary": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means.", "AI": {"tldr": "本文认为大型语言模型（LLMs）主要模拟其训练语料库，而非人类认知能力，原因在于其计算架构的根本差异。但作者认为这并非对语言或LLMs的贬低性观点，而是将其视为一种“话语机器”。", "motivation": "研究的动机在于探讨大型语言模型究竟模拟了什么——是人类的能力，还是它们所训练的语料库？以及这种理解对认知科学有何影响。", "method": "作者通过对比人类语言能力依赖的超线性计算格式与Transformer架构所支持的线性处理格式来构建论证。该论证主要基于Transformer计算架构的某些不变性，并引用了Liu et al. (2022) 关于“快捷自动机”的推测。", "result": "研究结果表明，大型语言模型主要建模的是其训练语料库，而非人类的语言能力。Transformer架构最多支持线性处理，而人类语言能力依赖超线性格式。LLMs可能扮演着一种“话语机器”的角色，能够基于给定上下文生成新语言。", "conclusion": "结论是，尽管LLMs并非表达人类内在状态的工具，而是通过不同方式学习使用语言的“话语机器”，但这并非一个贬低性的观点。语言不仅是表达内在状态的手段，也是一种能够根据上下文创造新语言的“话语机器”，LLMs也以其独特的方式掌握了这一点。"}}
{"id": "2508.18463", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18463", "abs": "https://arxiv.org/abs/2508.18463", "authors": ["Md. Rashid Shahriar Khan", "Md. Abrar Hasan", "Mohammod Tareq Aziz Justice"], "title": "Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling", "comment": "11 pages, 7 figures, 4 tables", "summary": "Detecting anomalies in surveillance footage is inherently challenging due to\ntheir unpredictable and context-dependent nature. This work introduces a novel\ncontext-aware zero-shot anomaly detection framework that identifies abnormal\nevents without exposure to anomaly examples during training. The proposed\nhybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal\ndynamics and semantic context. TimeSformer serves as the vision backbone to\nextract rich spatial-temporal features, while DPC forecasts future\nrepresentations to identify temporal deviations. Furthermore, a CLIP-based\nsemantic stream enables concept-level anomaly detection through\ncontext-specific text prompts. These components are jointly trained using\nInfoNCE and CPC losses, aligning visual inputs with their temporal and semantic\nrepresentations. A context-gating mechanism further enhances decision-making by\nmodulating predictions with scene-aware cues or global video features. By\nintegrating predictive modeling with vision-language understanding, the system\ncan generalize to previously unseen behaviors in complex environments. This\nframework bridges the gap between temporal reasoning and semantic context in\nzero-shot anomaly detection for surveillance. The code for this research has\nbeen made available at\nhttps://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.", "AI": {"tldr": "该研究提出了一种新颖的上下文感知零样本异常检测框架，通过结合TimeSformer、DPC和CLIP模型，以及上下文门控机制，在无需异常样本训练的情况下，检测监控视频中的异常事件。", "motivation": "由于监控录像中异常事件的不可预测性和上下文依赖性，检测异常具有内在挑战性。现有方法通常需要异常样本进行训练，而零样本方法能够识别训练中未见的异常行为。", "method": "该方法采用混合架构：TimeSformer作为视觉骨干提取时空特征；DPC预测未来表示以识别时间偏差；基于CLIP的语义流通过上下文特定文本提示实现概念级异常检测。这些组件通过InfoNCE和CPC损失联合训练，将视觉输入与时空和语义表示对齐。此外，一个上下文门控机制通过场景感知线索或全局视频特征调制预测，以增强决策。", "result": "该系统能够在训练期间未接触异常样本的情况下识别异常事件，并能泛化到复杂环境中以前未见的行为。它弥合了零样本异常检测中时间推理和语义上下文之间的鸿沟。", "conclusion": "该框架通过整合预测建模与视觉-语言理解，为监控领域提供了一种在复杂环境中检测零样本异常的有效方法，能够处理不可预测和上下文相关的异常事件。"}}
{"id": "2508.18669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18669", "abs": "https://arxiv.org/abs/2508.18669", "authors": ["Weikang Zhao", "Xili Wang", "Chengdi Ma", "Lingbin Kong", "Zhaohua Yang", "Mingxiang Tuo", "Xiaowei Shi", "Yitao Zhai", "Xunliang Cai"], "title": "MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use", "comment": null, "summary": "With the recent rapid advancement of Agentic Intelligence, agentic tool use\nin LLMs has become increasingly important. During multi-turn interactions\nbetween agents and users, the dynamic, uncertain, and stochastic nature of user\ndemands poses significant challenges to the agent's tool invocation\ncapabilities. Agents are no longer expected to simply call tools to deliver a\nresult; rather, they must iteratively refine their understanding of user needs\nthrough communication while simultaneously invoking tools to resolve user\nqueries. Existing reinforcement learning (RL) approaches for tool use lack the\nintegration of genuinely dynamic users during the RL training process. To\nbridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent\nReinforcement Learning for agentic tool use), a novel reinforcement learning\nframework that, for the first time in the field of agentic tool use, integrates\nLLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable\nautonomous learning of models to communicate with users efficiently and use\nvarious tools to solve practical problems in dynamic multi-turn interactions.\nEvaluations are done on several multi-turn tool-using benchmarks (see Figure\n1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2\nAirline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench\nAgent -- outperforming or matching the performance of larger open-source models\nsuch as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.", "AI": {"tldr": "本文提出MUA-RL，一个新颖的强化学习框架，首次将LLM模拟用户整合到强化学习循环中，以解决智能体在动态多轮交互中工具使用面临的用户需求不确定性挑战。", "motivation": "随着智能体智能的快速发展，大型语言模型（LLMs）中的智能体工具使用变得日益重要。在智能体与用户的多轮交互中，用户需求的动态性、不确定性和随机性对智能体的工具调用能力提出了重大挑战。现有的工具使用强化学习方法在训练过程中缺乏动态用户的集成。", "method": "本文引入了MUA-RL（多轮用户交互智能体强化学习），这是一个新颖的强化学习框架。该框架首次将LLM模拟用户集成到强化学习循环中，旨在使模型能够自主学习如何高效地与用户沟通，并在动态多轮交互中利用各种工具解决实际问题。", "result": "MUA-RL在多个多轮工具使用基准测试（如TAU2 Retail、TAU2 Airline、TAU2 Telecom、BFCL-V3 Multi Turn和ACEBench Agent）上进行了评估。MUA-RL-32B在这些基准测试中取得了显著成绩，在非思考设置下超越或匹配了DeepSeek-V3-0324和Qwen3-235B-A22B等更大的开源模型。", "conclusion": "MUA-RL通过在强化学习训练过程中集成LLM模拟用户，成功解决了智能体在动态多轮交互中有效理解用户需求和使用工具的挑战，显著提升了智能体工具使用的性能。"}}
{"id": "2508.18820", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.18820", "abs": "https://arxiv.org/abs/2508.18820", "authors": ["Christian Henkel", "Marco Lampacrescia", "Michaela Klauck", "Matteo Morelli"], "title": "AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy", "comment": "Accepted at IROS2025", "summary": "Designing robotic systems to act autonomously in unforeseen environments is a\nchallenging task. This work presents a novel approach to use formal\nverification, specifically Statistical Model Checking (SMC), to verify system\nproperties of autonomous robots at design-time. We introduce an extension of\nthe SCXML format, designed to model system components including both Robot\nOperating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we\ncontribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the\nfull system model into JANI. The use of JANI, a standard format for\nquantitative model checking, enables verification of system properties with\noff-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both\nin terms of applicability to real-world autonomous robotic control systems, and\nin terms of verification runtime scaling. We provide a case study, where we\nsuccessfully identify problems in a ROS 2-based robotic manipulation use case\nthat is verifiable in less than one second using consumer hardware.\nAdditionally, we compare to the state of the art and demonstrate that our\nmethod is more comprehensive in system feature support, and that the\nverification runtime scales linearly with the size of the model, instead of\nexponentially.", "AI": {"tldr": "本文提出了一种新颖的方法，通过扩展SCXML格式并开发AS2FM工具将自主机器人系统模型转换为JANI，从而在设计时使用统计模型检查（SMC）对ROS 2和行为树（BT）驱动的自主机器人系统进行形式化验证。", "motivation": "在不可预测的环境中设计自主机器人系统是一项挑战性任务，需要一种在设计阶段就能验证系统属性的方法。", "method": "该研究引入了SCXML格式的扩展，用于建模包含ROS 2和行为树（BT）特性的系统组件。然后，开发了Autonomous Systems to Formal Models (AS2FM) 工具，将完整的系统模型翻译成JANI（一种定量模型检查的标准格式），以便使用现成的SMC工具进行系统属性验证。", "result": "AS2FM在实际自主机器人控制系统中的适用性以及验证运行时扩展性方面表现出实用性。在一个案例研究中，成功识别了基于ROS 2的机器人操作用例中的问题，且验证时间不到一秒。与现有技术相比，该方法在系统特性支持方面更全面，并且验证运行时随模型大小呈线性扩展，而非指数级。", "conclusion": "该研究提供了一种实用、高效且全面的设计时验证方法，能够快速识别自主机器人系统中的问题，并具有良好的可扩展性，显著优于现有技术。"}}
{"id": "2508.18607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18607", "abs": "https://arxiv.org/abs/2508.18607", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "title": "A New NMT Model for Translating Clinical Texts from English to Spanish", "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018", "summary": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.", "AI": {"tldr": "NOOV是一种新型神经机器翻译（NMT）系统，旨在将电子健康记录（EHR）叙述从英语翻译成西班牙语。它通过整合双语词典和短语查找表来解决未知词和词语重复问题，从而在少量领域内平行语料库的情况下提高翻译的准确性和流畅性。", "motivation": "将电子健康记录（EHR）叙述从英语翻译成西班牙语具有重要的临床意义，但由于缺乏平行对齐语料库和存在大量未知词，这项任务极具挑战性。", "method": "本文提出了NOOV（No OOV），一种新的神经机器翻译（NMT）系统。NOOV集成了一个从平行对齐语料库中自动学习的双语词典和一个从大型生物医学知识资源中提取的短语查找表，以缓解NMT中的未知词问题和词语重复挑战，从而增强NMT系统的短语生成能力。", "result": "评估结果表明，NOOV能够生成更好的EHR翻译，并在准确性和流畅性方面都有所提升。", "conclusion": "NOOV系统通过集成双语词典和短语查找表，有效解决了EHR翻译中未知词和词语重复的挑战，显著提高了翻译的准确性和流畅性，即使在领域内平行语料库较少的情况下也能表现出色。"}}
{"id": "2508.18506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18506", "abs": "https://arxiv.org/abs/2508.18506", "authors": ["Ajinkya Khoche", "Qingwen Zhang", "Yixi Cai", "Sina Sharif Mansouri", "Patric Jensfelt"], "title": "DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance", "comment": "Under Review", "summary": "Accurate 3D scene flow estimation is critical for autonomous systems to\nnavigate dynamic environments safely, but creating the necessary large-scale,\nmanually annotated datasets remains a significant bottleneck for developing\nrobust perception models. Current self-supervised methods struggle to match the\nperformance of fully supervised approaches, especially in challenging\nlong-range and adverse weather scenarios, while supervised methods are not\nscalable due to their reliance on expensive human labeling. We introduce\nDoGFlow, a novel self-supervised framework that recovers full 3D object motions\nfor LiDAR scene flow estimation without requiring any manual ground truth\nannotations. This paper presents our cross-modal label transfer approach, where\nDoGFlow computes motion pseudo-labels in real-time directly from 4D radar\nDoppler measurements and transfers them to the LiDAR domain using dynamic-aware\nassociation and ambiguity-resolved propagation. On the challenging MAN\nTruckScenes dataset, DoGFlow substantially outperforms existing self-supervised\nmethods and improves label efficiency by enabling LiDAR backbones to achieve\nover 90% of fully supervised performance with only 10% of the ground truth\ndata. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/", "AI": {"tldr": "DoGFlow是一种新颖的自监督框架，通过跨模态标签迁移，利用4D雷达多普勒测量为LiDAR场景流估计生成伪标签，显著优于现有自监督方法并提高了标签效率。", "motivation": "3D场景流估计对自动驾驶系统至关重要，但手动标注大型数据集成本高昂，导致监督方法难以扩展；而现有自监督方法在性能上，尤其是在复杂场景下，无法与全监督方法匹敌。", "method": "DoGFlow采用跨模态标签迁移方法。它实时地从4D雷达多普勒测量中计算运动伪标签，并通过动态感知关联和歧义消除传播，将这些伪标签转移到LiDAR领域，实现全3D物体运动的自监督恢复。", "result": "在MAN TruckScenes数据集上，DoGFlow显著优于现有自监督方法。它仅使用10%的真实标注数据，就能使LiDAR骨干网络达到全监督性能的90%以上，极大地提高了标签效率。", "conclusion": "DoGFlow提供了一种无需手动标注即可进行3D场景流估计的有效自监督解决方案，通过创新的雷达-LiDAR跨模态标签迁移，显著提升了自监督方法的性能和标签效率，为自动驾驶系统在动态环境中的安全导航提供了关键支持。"}}
{"id": "2508.18689", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18689", "abs": "https://arxiv.org/abs/2508.18689", "authors": ["Yuyang Zhao", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance", "comment": "Accepted at CIKM 2025. 10 pages, 5 figures. Our code is available at:\n  https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:\n  https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be\n  found at:\n  https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0", "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in addressing complex tasks, thereby enabling more advanced\ninformation retrieval and supporting deeper, more sophisticated human\ninformation-seeking behaviors. However, most existing agents operate in a\npurely reactive manner, responding passively to user instructions, which\nsignificantly constrains their effectiveness and efficiency as general-purpose\nplatforms for information acquisition. To overcome this limitation, this paper\nproposes AppAgent-Pro, a proactive GUI agent system that actively integrates\nmulti-domain information based on user instructions. This approach enables the\nsystem to proactively anticipate users' underlying needs and conduct in-depth\nmulti-domain information mining, thereby facilitating the acquisition of more\ncomprehensive and intelligent information. AppAgent-Pro has the potential to\nfundamentally redefine information acquisition in daily life, leading to a\nprofound impact on human society. Our code is available at:\nhttps://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:\nhttps://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be\nfound at:\nhttps://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.", "AI": {"tldr": "AppAgent-Pro是一个主动式GUI代理系统，它利用大型语言模型（LLM）主动整合多领域信息，以预测用户需求并进行深度信息挖掘，从而提供更全面和智能的信息获取。", "motivation": "现有基于LLM的代理大多是被动响应用户指令，这严重限制了它们作为通用信息获取平台的效率和效果。研究旨在克服这一局限性。", "method": "本文提出了AppAgent-Pro，一个主动式GUI代理系统。该系统基于用户指令主动整合多领域信息，使其能够预判用户的潜在需求，并进行深入的多领域信息挖掘。", "result": "AppAgent-Pro能够促进更全面和智能的信息获取，通过主动整合和挖掘多领域信息，超越了传统被动式代理的能力。", "conclusion": "AppAgent-Pro有潜力从根本上重新定义日常生活中的信息获取方式，对人类社会产生深远影响。"}}
{"id": "2508.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18937", "abs": "https://arxiv.org/abs/2508.18937", "authors": ["Wang Jiayin", "Wei Yanran", "Jiang Lei", "Guo Xiaoyu", "Zheng Ayong", "Zhao Weidong", "Li Zhongkui"], "title": "VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery", "comment": "8 pages, 6 figures", "summary": "Autonomous control of the laparoscope in robot-assisted Minimally Invasive\nSurgery (MIS) has received considerable research interest due to its potential\nto improve surgical safety. Despite progress in pixel-level Image-Based Visual\nServoing (IBVS) control, the requirement of continuous visibility and the\nexistence of complex disturbances, such as parameterization error, measurement\nnoise, and uncertainties of payloads, could degrade the surgeon's visual\nexperience and compromise procedural safety. To address these limitations, this\npaper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and\nuncertainty-adaptive framework for autonomous laparoscope control that\nguarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian\nProcess Regression (GPR) is utilized to perform hybrid (deterministic +\nstochastic) quantification of operational uncertainties including residual\nmodel uncertainties, stochastic uncertainties, and external disturbances. Based\non uncertainty quantification, a novel safety aware trajectory optimization\nframework with probabilistic guarantees is proposed, where a\nuncertainty-adaptive safety Control Barrier Function (CBF) condition is given\nbased on uncertainty propagation, and chance constraints are simultaneously\nformulated based on probabilistic approximation. This uncertainty aware\nformulation enables adaptive control effort allocation, minimizing unnecessary\ncamera motion while maintaining robustness. The proposed method is validated\nthrough comparative simulations and experiments on a commercial surgical robot\nplatform (MicroPort MedBot Toumai) performing a sequential multi-target lymph\nnode dissection. Compared with baseline methods, the framework maintains\nnear-perfect target visibility (>99.9%), reduces tracking e", "AI": {"tldr": "提出了一种名为VisionSafeEnhanced视觉预测控制（VPC）的鲁棒且不确定性自适应框架，用于在不确定性下自主控制腹腔镜，确保视野（FoV）安全，显著提高目标可见性。", "motivation": "尽管基于图像的视觉伺服（IBVS）控制在自主腹腔镜控制方面有所进展，但其对持续可见性的要求以及参数误差、测量噪声和有效载荷不确定性等复杂干扰会降低外科医生的视觉体验并危及手术安全。", "method": "该研究提出了VisionSafeEnhanced视觉预测控制（VPC）框架。首先，利用高斯过程回归（GPR）对操作不确定性（包括残余模型不确定性、随机不确定性和外部干扰）进行混合（确定性+随机性）量化。在此基础上，提出了一种具有概率保证的新型安全感知轨迹优化框架，其中基于不确定性传播给出了不确定性自适应安全控制障碍函数（CBF）条件，并同时基于概率近似公式化了机会约束。这种不确定性感知公式能够自适应地分配控制力，在保持鲁棒性的同时最大限度地减少不必要的摄像头运动。", "result": "通过在商用手术机器人平台（微创医疗图迈）上进行顺序多目标淋巴结清扫的对比模拟和实验验证了所提出的方法。与基线方法相比，该框架保持了接近完美的靶点可见性（>99.9%），并减少了跟踪误差。", "conclusion": "所提出的VisionSafeEnhanced VPC框架为自主腹腔镜控制提供了一种鲁棒且不确定性自适应的解决方案，在不确定性下保证了视野安全，显著提高了目标可见性和手术安全性。"}}
{"id": "2508.18609", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18609", "abs": "https://arxiv.org/abs/2508.18609", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions.", "AI": {"tldr": "本文通过实证研究，建立了量化LLM（大型语言模型）的任务分层缩放定律，并发现知识记忆能力对量化参数更敏感，而知识利用能力更鲁棒。", "motivation": "大规模LLM的部署面临挑战，PTQ（训练后量化）是实用压缩方案。但目前对PTQ如何影响LLM知识能力的理解不全面，现有量化模型缩放定律忽略了PTQ特有参数和任务敏感性。", "method": "进行了广泛的实证研究，建立了任务分层缩放定律。将LLM知识分解为记忆能力和利用能力，并开发了一个统一的量化框架，该框架整合了模型大小、有效位宽、校准集大小和组大小等参数。", "result": "核心发现是，知识记忆能力对有效位宽、校准集大小和模型大小的变化表现出显著更高的敏感性，而知识利用能力则更为鲁棒。", "conclusion": "这些发现提供了对PTQ影响的细致理解，并为开发能够更好保留目标认知功能的“知识感知”量化策略提供了指导。"}}
{"id": "2508.18509", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18509", "abs": "https://arxiv.org/abs/2508.18509", "authors": ["Andreza M. C. Falcao", "Filipe R. Cordeiro"], "title": "Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas", "comment": "Accepted at SBCAS'25. in Portuguese language", "summary": "Machine unlearning aims to remove private or sensitive data from a\npre-trained model while preserving the model's robustness. Despite recent\nadvances, this technique has not been explored in medical image classification.\nThis work evaluates the SalUn unlearning model by conducting experiments on the\nPathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of\ndata augmentation on the quality of unlearning. Results show that SalUn\nachieves performance close to full retraining, indicating an efficient solution\nfor use in medical applications.", "AI": {"tldr": "本研究评估了SalUn机器遗忘模型在医学图像分类任务中的性能，发现其效果接近完全重新训练，为医学应用提供了一种高效的解决方案。", "motivation": "机器遗忘旨在从预训练模型中移除敏感数据同时保持模型鲁棒性，但该技术尚未在医学图像分类领域得到探索。", "method": "本研究通过在PathMNIST、OrganAMNIST和BloodMNIST数据集上进行实验，评估了SalUn遗忘模型。此外，还分析了数据增强对遗忘质量的影响。", "result": "实验结果表明，SalUn模型达到了接近完全重新训练的性能。", "conclusion": "SalUn模型为医学图像分类中的机器遗忘提供了一种高效的解决方案，具有实际应用潜力。"}}
{"id": "2508.18722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18722", "abs": "https://arxiv.org/abs/2508.18722", "authors": ["Honghao Fu", "Junlong Ren", "Qi Chai", "Deheng Ye", "Yujun Cai", "Hao Wang"], "title": "VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft", "comment": "Accepted by EMNLP 2025 main", "summary": "Large language models (LLMs) have shown significant promise in embodied\ndecision-making tasks within virtual open-world environments. Nonetheless,\ntheir performance is hindered by the absence of domain-specific knowledge.\nMethods that finetune on large-scale domain-specific data entail prohibitive\ndevelopment costs. This paper introduces VistaWise, a cost-effective agent\nframework that integrates cross-modal domain knowledge and finetunes a\ndedicated object detection model for visual analysis. It reduces the\nrequirement for domain-specific training data from millions of samples to a few\nhundred. VistaWise integrates visual information and textual dependencies into\na cross-modal knowledge graph (KG), enabling a comprehensive and accurate\nunderstanding of multimodal environments. We also equip the agent with a\nretrieval-based pooling strategy to extract task-related information from the\nKG, and a desktop-level skill library to support direct operation of the\nMinecraft desktop client via mouse and keyboard inputs. Experimental results\ndemonstrate that VistaWise achieves state-of-the-art performance across various\nopen-world tasks, highlighting its effectiveness in reducing development costs\nwhile enhancing agent performance.", "AI": {"tldr": "VistaWise是一个经济高效的具身决策智能体框架，通过集成跨模态领域知识和高效的对象检测模型，显著减少了对领域特定训练数据的需求，并在开放世界任务中实现了最先进的性能。", "motivation": "大型语言模型（LLMs）在虚拟开放世界环境中的具身决策任务中表现出潜力，但缺乏领域特定知识，且在大规模领域特定数据上进行微调的开发成本过高。", "method": "本文提出了VistaWise框架，它：1) 集成了跨模态领域知识；2) 微调了一个专用的对象检测模型，将领域特定训练数据需求从数百万减少到数百个样本；3) 将视觉信息和文本依赖关系整合到跨模态知识图谱（KG）中；4) 采用基于检索的池化策略从KG中提取任务相关信息；5) 配备了桌面级技能库，通过鼠标和键盘输入直接操作Minecraft桌面客户端。", "result": "实验结果表明，VistaWise在各种开放世界任务中实现了最先进的性能。", "conclusion": "VistaWise有效降低了开发成本，同时提升了智能体的性能，证明了其在具身决策任务中的有效性。"}}
{"id": "2508.18967", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18967", "abs": "https://arxiv.org/abs/2508.18967", "authors": ["Hichem Cheriet", "Khellat Kihel Badra", "Chouraqui Samira"], "title": "Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm", "comment": "Accepted for publication in JAMRIS Journal", "summary": "Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical\nfor various applications, including combat support, package delivery and Search\nand Rescue Operations. This paper introduces the Tangent Intersection Guidance\n(TIG) algorithm, an advanced approach for UAV path planning in both static and\ndynamic environments. The algorithm uses the elliptic tangent intersection\nmethod to generate feasible paths. It generates two sub-paths for each threat,\nselects the optimal route based on a heuristic rule, and iteratively refines\nthe path until the target is reached. Considering the UAV kinematic and dynamic\nconstraints, a modified smoothing technique based on quadratic B\\'ezier curves\nis adopted to generate a smooth and efficient route. Experimental results show\nthat the TIG algorithm can generate the shortest path in less time, starting\nfrom 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent\nGraph, and Static APPATT algorithms in static environments. Furthermore, in\ncompletely unknown and partially known environments, TIG demonstrates efficient\nreal-time path planning capabilities for collision avoidance, outperforming APF\nand Dynamic APPATT algorithms.", "AI": {"tldr": "本文提出了一种名为切线交点引导（TIG）的先进算法，用于无人机在静态和动态环境中的高效安全路径规划，该算法能生成更短、更快、转弯更少的路径，并具备实时避障能力。", "motivation": "无人机（UAV）在作战支援、包裹递送和搜救行动等各种应用中，高效安全的导航至关重要。", "method": "TIG算法采用椭圆切线交点方法生成可行路径，为每个威胁生成两条子路径，基于启发式规则选择最优路线，并迭代优化直至目标。为满足无人机运动学和动力学约束，采用基于二次贝塞尔曲线的修正平滑技术生成平滑高效的路线。", "result": "在静态环境中，TIG算法能以更少的时间（从0.01秒开始）和更少的转弯角度生成最短路径，优于A*、PRM、RRT*、Tangent Graph和Static APPATT算法。在完全未知和部分已知的动态环境中，TIG算法在碰撞避免方面表现出高效的实时路径规划能力，优于APF和Dynamic APPATT算法。", "conclusion": "TIG算法是一种先进的无人机路径规划方法，在静态和动态环境中均能实现高效安全的导航，尤其在路径长度、计算时间和实时避障方面表现出色。"}}
{"id": "2508.18648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18648", "abs": "https://arxiv.org/abs/2508.18648", "authors": ["Cong Li", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS", "AI": {"tldr": "针对大型语言模型在复杂推理任务（如数学）上的不足，本文提出“言前思之”（TBYS）框架。该框架通过在推理步骤之间主动插入“洞察”（insight）来指导模型，这些洞察回顾当前状态并启动下一步，从而弥补了人类训练数据中缺失的内部思考过程，并在数学数据集上取得了显著效果。", "motivation": "大型语言模型在复杂推理任务（如数学）上表现不佳，原因在于人类在解决复杂问题时，通常会先进行深入思考（即产生“洞察”），但这些内部思考过程和方法往往不会在训练数据中明确表达出来，导致模型训练数据与人类推理模式之间存在差异，缺乏连接推理步骤的关键见解。", "method": "本文提出在连续推理步骤之间插入“洞察”（insight）。这些“洞察”用于回顾当前状态并启动下一个推理步骤。与以往依赖静态提示策略不同，“洞察”是主动生成的，旨在指导推理过程。该方法被实现为“言前思之”（Thinking Before You Speak, TBYS）推理框架，并设计了一个自动化管道，用于收集和过滤用于生成“洞察”的上下文示例，从而减少了人工标注和微调的开销。", "result": "在具有挑战性的数学数据集上的实验验证了TBYS框架的有效性。", "conclusion": "通过主动生成并插入“洞察”来指导推理过程，TBYS框架有效解决了大型语言模型在复杂推理任务中的不足，并在数学问题上展现了其有效性，弥补了训练数据中人类内部思考的缺失。"}}
{"id": "2508.18528", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18528", "abs": "https://arxiv.org/abs/2508.18528", "authors": ["Anna Milani", "Fábio S. da Silva", "Elloá B. Guedes", "Ricardo Rios"], "title": "A Deep Learning Application for Psoriasis Detection", "comment": "15 pages, 4 figures, 1 table, Proceedings of XX Encontro Nacional de\n  Intelig\\^encia Artificial e Computacional. in Portuguese language", "summary": "In this paper a comparative study of the performance of three Convolutional\nNeural Network models, ResNet50, Inception v3 and VGG19 for classification of\nskin images with lesions affected by psoriasis is presented. The images used\nfor training and validation of the models were obtained from specialized\nplatforms. Some techniques were used to adjust the evaluation metrics of the\nneural networks. The results found suggest the model Inception v3 as a valuable\ntool for supporting the diagnosis of psoriasis. This is due to its satisfactory\nperformance with respect to accuracy and F1-Score (97.5% ${\\pm}$ 0.2).", "AI": {"tldr": "本文比较了ResNet50、Inception v3和VGG19三种CNN模型在银屑病皮损图像分类中的性能，发现Inception v3表现最佳。", "motivation": "研究旨在评估不同卷积神经网络模型在银屑病皮损图像分类中的表现，以期为银屑病的诊断提供辅助工具。", "method": "研究使用了ResNet50、Inception v3和VGG19三种卷积神经网络模型。训练和验证图像来自专业平台，并采用了调整评估指标的技术。", "result": "Inception v3模型表现出令人满意的性能，其准确率和F1-Score达到97.5% ± 0.2。", "conclusion": "Inception v3模型因其在准确率和F1-Score方面的出色表现，被认为是一种有价值的银屑病诊断辅助工具。"}}
{"id": "2508.18724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18724", "abs": "https://arxiv.org/abs/2508.18724", "authors": ["Karanbir Singh", "Deepak Muppiri", "William Ngu"], "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval", "comment": "Accepted at KDD'2025 Agent4IR workshop", "summary": "Large Language Models (LLMs) have transformed the field of artificial\nintelligence by unlocking the era of generative applications. Built on top of\ngenerative AI capabilities, Agentic AI represents a major shift toward\nautonomous, goal-driven systems that can reason, retrieve, and act. However,\nthey also inherit the bias present in both internal and external information\nsources. This significantly affects the fairness and balance of retrieved\ninformation, and hence reduces user trust. To address this critical challenge,\nwe introduce a novel Bias Mitigation Agent, a multi-agent system designed to\norchestrate the workflow of bias mitigation through specialized agents that\noptimize the selection of sources to ensure that the retrieved content is both\nhighly relevant and minimally biased to promote fair and balanced knowledge\ndissemination. The experimental results demonstrate an 81.82\\% reduction in\nbias compared to a baseline naive retrieval strategy.", "AI": {"tldr": "本文提出了一种新颖的偏见缓解智能体（Bias Mitigation Agent），这是一个多智能体系统，旨在通过优化信息源选择，确保检索内容的关联性和最小偏见，从而解决生成式AI智能体中存在的偏见问题。", "motivation": "大型语言模型（LLMs）驱动的智能体AI继承了内部和外部信息源的偏见，这严重影响了检索信息的公平性和平衡性，进而降低了用户信任。因此，需要一种方法来解决这一关键挑战。", "method": "引入了一个多智能体系统，即“偏见缓解智能体”（Bias Mitigation Agent），该系统通过专业智能体协调偏见缓解工作流，优化信息源的选择，以确保检索到的内容既高度相关又偏见最小化，从而促进公平和平衡的知识传播。", "result": "实验结果表明，与基线朴素检索策略相比，该方法将偏见减少了81.82%。", "conclusion": "所提出的偏见缓解智能体能够显著减少生成式AI智能体在信息检索中的偏见，有效提升了检索内容的公平性和平衡性。"}}
{"id": "2508.19002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19002", "abs": "https://arxiv.org/abs/2508.19002", "authors": ["Shipeng Lyu", "Fangyuan Wang", "Weiwei Lin", "Luhao Zhu", "David Navarro-Alarcon", "Guodong Guo"], "title": "HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots", "comment": "8 pages, 8 figures,4 tables", "summary": "Achieving both behavioral similarity and appropriateness in human-like motion\ngeneration for humanoid robot remains an open challenge, further compounded by\nthe lack of cross-embodiment adaptability. To address this problem, we propose\nHuBE, a bi-level closed-loop framework that integrates robot state, goal poses,\nand contextual situations to generate human-like behaviors, ensuring both\nbehavioral similarity and appropriateness, and eliminating structural\nmismatches between motion generation and execution. To support this framework,\nwe construct HPose, a context-enriched dataset featuring fine-grained\nsituational annotations. Furthermore, we introduce a bone scaling-based data\naugmentation strategy that ensures millimeter-level compatibility across\nheterogeneous humanoid robots. Comprehensive evaluations on multiple commercial\nplatforms demonstrate that HuBE significantly improves motion similarity,\nbehavioral appropriateness, and computational efficiency over state-of-the-art\nbaselines, establishing a solid foundation for transferable and human-like\nbehavior execution across diverse humanoid robots.", "AI": {"tldr": "本文提出HuBE框架，一个双层闭环系统，结合机器人状态、目标姿态和情境，为仿人机器人生成兼具行为相似性和适当性的人类般运动，并通过骨骼缩放增强实现跨机器人适应性。", "motivation": "仿人机器人生成类人运动面临行为相似性、适当性以及跨载体适应性方面的挑战。", "method": "1. 提出HuBE，一个双层闭环框架，整合机器人状态、目标姿态和情境，生成类人行为，确保行为相似性和适当性，并消除运动生成与执行间的结构不匹配。2. 构建HPose，一个包含精细情境标注的上下文丰富数据集。3. 引入基于骨骼缩放的数据增强策略，确保异构仿人机器人之间的毫米级兼容性。", "result": "在多个商用平台上进行全面评估表明，HuBE在运动相似性、行为适当性和计算效率方面显著优于现有基线。", "conclusion": "HuBE为在不同仿人机器人上实现可迁移的类人行为执行奠定了坚实基础。"}}
{"id": "2508.18651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18651", "abs": "https://arxiv.org/abs/2508.18651", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.", "AI": {"tldr": "本文提出协作解码（CoDe）框架，通过动态整合有无外部知识的输出概率，并结合知识感知重排序机制，有效解决大型语言模型在利用外部知识时忠实性和表达性之间的权衡问题。", "motivation": "当前大型语言模型在整合外部知识时，难以同时保持输出的忠实性（即与知识一致）和表达性（即自然流畅），导致输出要么缺乏外部知识支持（不忠实），要么过于冗长不自然（表达性差）。", "method": "本文提出协作解码（CoDe），一种动态整合有外部知识和无外部知识生成的输出概率的方法。该整合由分布散度（distribution divergence）和模型置信度（model confidence）引导，以选择性激活模型内部参数中相关且可靠的表达。此外，还引入了知识感知重排序机制，以防止过度依赖模型先验参数知识，并确保外部信息的正确利用。", "result": "CoDe框架在不损害表达性的前提下，显著提高了大型语言模型的忠实性。通过广泛的实验，该即插即用框架在不同的LLM和评估指标上都表现出卓越的性能，验证了其有效性和通用性。", "conclusion": "CoDe是一个有效且通用的即插即用框架，能够帮助大型语言模型在利用外部知识时，打破忠实性和表达性之间的权衡，实现两者的协同提升。"}}
{"id": "2508.18531", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18531", "abs": "https://arxiv.org/abs/2508.18531", "authors": ["Zhangyu Jin", "Andrew Feng"], "title": "SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors", "comment": null, "summary": "We present SatSkylines, a 3D building generation approach that takes\nsatellite imagery and coarse geometric priors. Without proper geometric\nguidance, existing image-based 3D generation methods struggle to recover\naccurate building structures from the top-down views of satellite images alone.\nOn the other hand, 3D detailization methods tend to rely heavily on highly\ndetailed voxel inputs and fail to produce satisfying results from simple priors\nsuch as cuboids. To address these issues, our key idea is to model the\ntransformation from interpolated noisy coarse priors to detailed geometries,\nenabling flexible geometric control without additional computational cost. We\nhave further developed Skylines-50K, a large-scale dataset of over 50,000\nunique and stylized 3D building assets in order to support the generations of\ndetailed building models. Extensive evaluations indicate the effectiveness of\nour model and strong generalization ability.", "AI": {"tldr": "SatSkylines是一种3D建筑生成方法，它利用卫星图像和粗略几何先验，通过建模从粗糙先验到详细几何的转换，以灵活控制生成高精度建筑模型。", "motivation": "现有基于图像的3D生成方法难以仅从卫星图像的俯视图恢复准确的建筑结构；而3D细节化方法则严重依赖高度详细的体素输入，无法从简单的先验（如长方体）生成令人满意的结果。", "method": "核心思想是建模从插值噪声粗糙先验到详细几何形状的转换，从而实现灵活的几何控制，且不增加计算成本。此外，开发了一个包含超过50,000个独特和风格化3D建筑资产的大规模数据集Skylines-50K。", "result": "广泛的评估表明，该模型有效且具有强大的泛化能力。", "conclusion": "SatSkylines成功解决了从卫星图像和粗略几何先验生成详细3D建筑模型的挑战，提供了有效且可泛化的解决方案。"}}
{"id": "2508.18743", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18743", "abs": "https://arxiv.org/abs/2508.18743", "authors": ["Sunguk Choi", "Yonghoon Kwon", "Heondeuk Lee"], "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks", "comment": "Accepted at EMNLP 2025 findings", "summary": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)\nsolve difficult problems, but very long traces often slow or even degrade\nperformance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware\nCompact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a\nsmall, fixed set of connector phrases, steering the model toward concise and\nwell -- structured explanations. Despite its simplicity, our synthetic method\nwith Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves\napproximately 85% on GSM8K and approximately 40% on GPQA (System-2) while\nretaining approximately 90% on S1-Bench (System-1). Its reasoning traces\naverage approximately 300 tokens(ART), about one-third the length of baseline\ntraces, delivering higher efficiency without loss of accuracy.", "AI": {"tldr": "本文提出了一种名为CAC-CoT（Connector-Aware Compact CoT）的方法，通过限制推理使用固定连接词，使大语言模型生成更简洁、结构化的思维链，从而在保持高准确率的同时，显著提高了推理效率，适用于快节奏和复杂任务。", "motivation": "长思维链（CoT）提示有助于大语言模型（LLMs）解决难题，但过长的推理过程往往会降低或甚至损害模型在快速、直观的“系统1”任务上的性能。", "method": "引入了连接词感知紧凑型思维链（CAC-CoT）方法，该方法有意识地将推理限制在一小组固定的连接词短语中，引导模型生成简洁且结构良好的解释。本文使用Gemini-2.0-Flash模型进行合成训练。", "result": "CAC-CoT在GSM8K上取得了约85%的准确率，在GPQA（系统2任务）上取得了约40%的准确率，同时在S1-Bench（系统1任务）上保持了约90%的准确率。其推理轨迹平均约为300个令牌，大约是基线方法长度的三分之一，实现了更高的效率且没有损失准确性。", "conclusion": "CAC-CoT通过限制推理词汇，成功地实现了简洁、结构化的思维链，显著提高了效率，同时在“系统1”和“系统2”任务上均保持了高水平的性能，为LLM推理提供了一种有效且高效的解决方案。"}}
{"id": "2508.19074", "categories": ["cs.RO", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.19074", "abs": "https://arxiv.org/abs/2508.19074", "authors": ["ZhenDong Chen", "ZhanShang Nie", "ShiXing Wan", "JunYi Li", "YongTian Cheng", "Shuai Zhao"], "title": "An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees", "comment": null, "summary": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs.", "AI": {"tldr": "本文提出了一种名为NRTrans的自然-机器人语言翻译框架，通过引入机器人技能语言（RSL）及其编译器和调试器，为大型语言模型（LLM）生成的机器人控制程序提供正确性验证和基于反馈的微调，从而显著提高LLM驱动的机器人应用的有效性。", "motivation": "现有方法让LLM直接从自然语言任务生成机器人控制程序，但由于LLM的不一致性和任务复杂性，生成的代码常出现大量编程错误，尤其在使用轻量级LLM时，严重影响了有效性。", "method": "本文提出了一个自然-机器人语言翻译框架（NRTrans），它包含：1. 机器人技能语言（RSL），用于抽象控制程序的复杂细节，连接自然语言任务与底层机器人技能。2. RSL编译器和调试器，用于验证LLM生成的RSL程序，并向LLM提供错误反馈，以迭代精炼输出，直到程序通过验证。", "result": "实验表明，NRTrans在各种LLM和任务下均优于现有方法，并且即使对于轻量级LLM也能实现高成功率。", "conclusion": "NRTrans通过在程序执行前提供LLM生成程序的正确性保证，显著增强了LLM驱动的机器人应用的有效性。"}}
{"id": "2508.18655", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18655", "abs": "https://arxiv.org/abs/2508.18655", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/", "AI": {"tldr": "该论文提出了一种名为 Emotion Omni 的新型模型架构，旨在理解用户语音输入中的情感并生成共情语音响应。同时，开发了一个数据生成管道，构建了一个20万条情感对话数据集，以解决在有限数据下开发共情语音LLM的挑战。", "motivation": "现有语音大语言模型（speech LLMs）未能充分理解用户语音输入中丰富的情感和副语言线索，导致响应缺乏共情，影响用户体验。此外，当前具备共情能力的语音LLMs大多依赖海量数据集进行训练，需要巨大的数据和计算资源，因此如何在有限数据和无需大规模训练的情况下开发共情语音LLM是一个关键挑战。", "method": "提出了一种名为 Emotion Omni 的新型模型架构，用于理解用户语音输入的情感内容并生成共情语音响应。此外，开发了一个基于开源TTS框架的数据生成管道，以构建一个包含20万条情感对话的数据集，用于支持共情语音助手的开发。", "result": "成功构建了一个包含20万条情感对话的数据集，该数据集支持共情语音助手的构建。所提出的Emotion Omni模型架构能够理解用户语音输入的情感并生成共情语音响应。", "conclusion": "通过提出Emotion Omni模型架构和开发数据生成管道，该研究为在有限数据和计算资源下构建具有共情能力的语音大语言模型提供了一种有效解决方案，从而显著提升了人机交互的用户体验。"}}
{"id": "2508.18539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18539", "abs": "https://arxiv.org/abs/2508.18539", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "Adaptive Visual Navigation Assistant in 3D RPGs", "comment": null, "summary": "In complex 3D game environments, players rely on visual affordances to spot\nmap transition points. Efficient identification of such points is important to\nclient-side auto-mapping, and provides an objective basis for evaluating map\ncue presentation. In this work, we formalize the task of detecting traversable\nSpatial Transition Points (STPs)-connectors between two sub regions-and\nselecting the singular Main STP (MSTP), the unique STP that lies on the\ndesigner-intended critical path toward the player's current macro-objective,\nfrom a single game frame, proposing this as a new research focus. We introduce\na two-stage deep-learning pipeline that first detects potential STPs using\nFaster R-CNN and then ranks them with a lightweight MSTP selector that fuses\nlocal and global visual features. Both stages benefit from parameter-efficient\nadapters, and we further introduce an optional retrieval-augmented fusion step.\nOur primary goal is to establish the feasibility of this problem and set\nbaseline performance metrics. We validate our approach on a custom-built,\ndiverse dataset collected from five Action RPG titles. Our experiments reveal a\nkey trade-off: while full-network fine-tuning produces superior STP detection\nwith sufficient data, adapter-only transfer is significantly more robust and\neffective in low-data scenarios and for the MSTP selection task. By defining\nthis novel problem, providing a baseline pipeline and dataset, and offering\ninitial insights into efficient model adaptation, we aim to contribute to\nfuture AI-driven navigation aids and data-informed level-design tools.", "AI": {"tldr": "本文提出了一种检测3D游戏环境中可穿越空间过渡点（STP）并选择主STP（MSTP）的新问题，并开发了一个两阶段深度学习管道作为基线，旨在促进AI导航和关卡设计。", "motivation": "在复杂的3D游戏环境中，玩家需要识别地图过渡点。高效识别这些点对于客户端自动地图生成和评估地图提示至关重要。本文旨在形式化检测可穿越STP并从多个STP中选择唯一的主STP（MSTP）的任务。", "method": "本文提出一个两阶段深度学习管道：第一阶段使用Faster R-CNN检测潜在的STP；第二阶段使用一个轻量级MSTP选择器对STP进行排名，该选择器融合了局部和全局视觉特征。两个阶段都利用了参数高效的适配器，并引入了一个可选的检索增强融合步骤。该方法在从五款动作RPG游戏中收集的自定义数据集上进行了验证。", "result": "研究证实了该问题的可行性并建立了基线性能指标。实验揭示了一个关键权衡：在数据充足的情况下，全网络微调在STP检测方面表现更优；但在数据量较少或执行MSTP选择任务时，仅使用适配器的迁移学习则更鲁棒和有效。", "conclusion": "本文定义了一个新颖的问题，提供了一个基线管道和数据集，并为高效模型适应提供了初步见解。目标是为未来的AI驱动导航辅助和数据驱动的关卡设计工具做出贡献。"}}
{"id": "2508.18749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18749", "abs": "https://arxiv.org/abs/2508.18749", "authors": ["Chunlong Wu", "Zhibo Qu"], "title": "Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution", "comment": null, "summary": "Recent advances in prompt optimization, exemplified by methods such as\nTextGrad, enable automatic, gradient-like refinement of textual prompts to\nenhance the performance of large language models (LLMs) on specific downstream\ntasks. However, current approaches are typically stateless and operate\nindependently across optimization runs, lacking mechanisms to preserve and\nleverage historical optimization experience. Furthermore, they are susceptible\nto overfitting, often yielding prompt updates that generalize poorly beyond the\nimmediate task context.\n  To address these limitations, we propose Reflection-Enhanced\nMeta-Optimization (REMO), a novel framework that integrates (1) a\nmemory-augmented Reflection Retrieval-Augmented Generation (RAG) module -\nstructured as a \"mistake notebook\" and (2) a Self-Adaptive Optimizer,\nimplemented via an LLM-driven meta-controller that synthesizes epoch-level\nreflective insights to iteratively improve system-level prompting strategies.\nThis architecture enables not only local, fine-grained prompt tuning akin to\nTextGrad, but also the systematic accumulation and reuse of cross-run\noptimization knowledge, thereby supporting continual improvement over time.\n  We instantiate the REMO framework using Qwen3-32B in standard inference mode\n- without explicit chain-of-thought prompting - and evaluate its efficacy on\nthe GSM8K benchmark for mathematical reasoning. Experimental results\ndemonstrate that, compared to a TextGrad baseline, REMO achieves more stable\nand robust generalization, albeit at the cost of increased computational\noverhead. We provide a detailed exposition of the algorithmic design, conduct a\nqualitative and quantitative analysis of optimization dynamics, and present a\ncomprehensive ablation study to elucidate the contributions of each component.", "AI": {"tldr": "REMO是一个新颖的框架，通过结合记忆增强的反射RAG模块和自适应优化器，解决了现有提示优化方法（如TextGrad）无状态和过拟合的问题，实现了更稳定和鲁棒的泛化能力，并支持持续改进。", "motivation": "当前的提示优化方法通常是无状态的，无法保留和利用历史优化经验；它们也容易过拟合，导致生成的提示在即时任务上下文之外泛化能力差。", "method": "我们提出了Reflection-Enhanced Meta-Optimization (REMO) 框架，包含：1) 一个记忆增强的反射检索增强生成 (RAG) 模块，结构化为“错误笔记本”；2) 一个自适应优化器，通过LLM驱动的元控制器实现，该控制器综合了周期级别的反思性见解，以迭代改进系统级别的提示策略。该框架在Qwen3-32B上以标准推理模式实例化。", "result": "在GSM8K数学推理基准上的实验结果表明，与TextGrad基线相比，REMO实现了更稳定和鲁棒的泛化能力，尽管计算开销有所增加。我们还提供了算法设计、优化动态的定性和定量分析以及组件的消融研究。", "conclusion": "REMO框架不仅支持类似TextGrad的局部、细粒度提示调优，还实现了跨运行优化知识的系统积累和重用，从而支持系统随时间的持续改进，并展现出更强的泛化能力。"}}
{"id": "2508.19114", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19114", "abs": "https://arxiv.org/abs/2508.19114", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Alexander Derrico", "Philip Dames"], "title": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning", "comment": "Submission under review at the 2026 IEEE/SICE International Symposium\n  on System Integration (SII 2026)", "summary": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration.", "AI": {"tldr": "DELIVER是一个用于多机器人协同取送货的集成框架，通过自然语言指令驱动，实现了可扩展、无碰撞的协调。", "motivation": "研究动机是实现现实世界中由自然语言指令驱动的多机器人系统的可扩展、无碰撞协作，解决传统系统在复杂环境下的局限性。", "method": "DELIVER框架整合了自然语言理解（使用轻量级LLaMA3提取地点）、空间分解（使用Voronoi tessellation划分机器人操作区域）、中继规划（计算最佳中继点和协调交接）以及运动执行（通过有限状态机管理机器人行为）。该框架在MultiTRAIL仿真平台、ROS2-based Gazebo仿真和TurtleBot3真实硬件上进行了实现和验证。", "result": "实验结果表明，DELIVER在不同团队规模下保持一致的任务成本，并与单代理系统相比，将每个代理的工作负载降低了高达55%。此外，即使团队规模增加，活跃中继代理的数量也保持在较低水平，证明了系统的可扩展性和高效的代理利用率。", "conclusion": "DELIVER提供了一个模块化、可扩展的架构，用于语言引导的多机器人协调，推动了信息物理系统集成的前沿发展。"}}
{"id": "2508.18673", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.18673", "abs": "https://arxiv.org/abs/2508.18673", "authors": ["Xinglong Yang", "Quan Feng", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Wentong Li", "Shuofei Qiao", "Yuxia Geng", "Xingyu Zhao", "Sheng-Jun Huang"], "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "comment": null, "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.", "AI": {"tldr": "本文提出了一种新颖的框架，通过结合模型感知难度和固有样本复杂度，设计难度平衡的采样策略，以改进多模态思维链（MCoT）提示的例子选择，从而提升多模态推理性能。", "motivation": "多模态思维链（MCoT）提示的效果常受限于随机或手动选择的例子，这些例子未能考虑模型知识分布和任务内在复杂性，导致模型性能不佳且不稳定。", "method": "受“因材施教与难度平衡”教学原则启发，将提示选择重构为提示课程设计问题。该方法整合了两种互补信号：1) 模型感知难度（通过主动学习设置中的预测分歧量化）；2) 固有样本复杂度（衡量每个问题-图像对的内在难度）。通过联合分析这些信号，开发出一种难度平衡的采样策略，确保所选提示例子在两个维度上都具有多样性。", "result": "在五个具有挑战性的基准测试和多个流行的多模态大型语言模型（MLLMs）上进行的广泛实验表明，该方法带来了显著且持续的改进，并大大减少了随机采样导致的性能差异。", "conclusion": "该方法为增强多模态推理提供了一种原则性且稳健的方法，通过改进提示例子选择，显著提升了多模态思维链的性能和稳定性。"}}
{"id": "2508.18621", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18621", "abs": "https://arxiv.org/abs/2508.18621", "authors": ["Xin Gao", "Li Hu", "Siqi Hu", "Mingyang Huang", "Chaonan Ji", "Dechao Meng", "Jinwei Qi", "Penchong Qiao", "Zhen Shen", "Yafei Song", "Ke Sun", "Linrui Tian", "Guangyuan Wang", "Qi Wang", "Zhongjian Wang", "Jiayu Xiao", "Sheng Xu", "Bang Zhang", "Peng Zhang", "Xindi Zhang", "Zhe Zhang", "Jingren Zhou", "Lian Zhuo"], "title": "Wan-S2V: Audio-Driven Cinematic Video Generation", "comment": null, "summary": "Current state-of-the-art (SOTA) methods for audio-driven character animation\ndemonstrate promising performance for scenarios primarily involving speech and\nsinging. However, they often fall short in more complex film and television\nproductions, which demand sophisticated elements such as nuanced character\ninteractions, realistic body movements, and dynamic camera work. To address\nthis long-standing challenge of achieving film-level character animation, we\npropose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.\nOur model achieves significantly enhanced expressiveness and fidelity in\ncinematic contexts compared to existing approaches. We conducted extensive\nexperiments, benchmarking our method against cutting-edge models such as\nHunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate\nthat our approach significantly outperforms these existing solutions.\nAdditionally, we explore the versatility of our method through its applications\nin long-form video generation and precise video lip-sync editing.", "AI": {"tldr": "Wan-S2V是一种基于音频驱动的角色动画模型，专为电影级制作设计，显著提升了复杂电影场景中的表现力和逼真度，并超越了现有SOTA方法。", "motivation": "当前的音频驱动角色动画方法在电影和电视制作等复杂场景中表现不足，无法满足细致的角色互动、逼真的肢体动作和动态运镜等高要求，因此需要解决实现电影级角色动画的长期挑战。", "method": "本文提出了一种名为Wan-S2V的音频驱动模型，该模型基于Wan构建，旨在增强电影语境下的表现力和逼真度。通过广泛实验，将其与Hunyuan-Avatar和Omnihuman等尖端模型进行了基准测试。", "result": "实验结果表明，Wan-S2V模型在电影语境下的表现力和逼真度显著优于现有方法（如Hunyuan-Avatar和Omnihuman）。此外，该方法还适用于长视频生成和精准视频唇形同步编辑。", "conclusion": "Wan-S2V模型成功解决了实现电影级角色动画的挑战，在电影场景中提供了显著增强的表现力和逼真度，超越了现有最先进的解决方案。"}}
{"id": "2508.18751", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18751", "abs": "https://arxiv.org/abs/2508.18751", "authors": ["Byung-Joon Lee", "Jin-Seop Lee", "Jee-Hyong Lee"], "title": "Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction", "comment": "Accepted at BMVC 2025", "summary": "Deep neural networks demonstrate strong performance under aligned\ntraining-test distributions. However, real-world test data often exhibit domain\nshifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the\nmodel to test data during inference. While most TTA studies assume that the\ntraining and test data share the same class set (closed-set TTA), real-world\nscenarios often involve open-set data (open-set TTA), which can degrade\nclosed-set accuracy. A recent study showed that identifying open-set data\nduring adaptation and maximizing its entropy is an effective solution. However,\nthe previous method relies on the source model for filtering, resulting in\nsuboptimal filtering accuracy on domain-shifted test data. In contrast, we\nfound that the adapting model, which learns domain knowledge from noisy test\nstreams, tends to be unstable and leads to error accumulation when used for\nfiltering. To address this problem, we propose Primary-Auxiliary Filtering\n(PAF), which employs an auxiliary filter to validate data filtered by the\nprimary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),\nwhich calibrates the outputs of the adapting model, EMA model, and source model\nto integrate their complementary knowledge for OSTTA. We validate our approach\nacross diverse closed-set and open-set datasets. Our method enhances both\nclosed-set accuracy and open-set discrimination over existing methods. The code\nis available at https://github.com/powerpowe/PAF-KIP-OSTTA .", "AI": {"tldr": "针对开放集测试时间自适应（OSTTA）中的域偏移和未知类别挑战，本文提出了主辅过滤（PAF）以提高开放集数据识别准确性，并提出了知识集成预测（KIP）以融合多模型知识，从而同时提升了闭集准确性和开放集判别能力。", "motivation": "深度神经网络在训练和测试数据分布对齐时表现良好，但实际世界数据常存在域偏移。测试时间自适应（TTA）旨在解决此问题，但多数研究假设类别集合不变（闭集TTA）。实际场景常涉及开放集数据（OSTTA），这会降低闭集准确性。现有OSTTA方法在识别开放集数据时，依赖源模型或不稳定的自适应模型进行过滤，导致过滤准确性不佳和误差累积。", "method": "本文提出两种主要方法：1. **主辅过滤（PAF）**：使用一个辅助过滤器来验证主过滤器筛选出的数据，以解决现有方法过滤准确性次优的问题。2. **知识集成预测（KIP）**：校准自适应模型、EMA模型和源模型的输出，以整合它们的互补知识，用于OSTTA。", "result": "本文方法在多种闭集和开放集数据集上进行了验证，结果表明它在闭集准确性和开放集判别能力方面均优于现有方法。", "conclusion": "通过引入主辅过滤（PAF）和知识集成预测（KIP），本文有效解决了开放集测试时间自适应（OSTTA）中的挑战，显著提升了模型在存在域偏移和未知类别情况下的性能，实现了更好的闭集准确性和开放集判别能力。"}}
{"id": "2508.19131", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19131", "abs": "https://arxiv.org/abs/2508.19131", "authors": ["Shreya Gummadi", "Mateus V. Gasparino", "Gianluca Capezzuto", "Marcelo Becker", "Girish Chowdhary"], "title": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments", "comment": null, "summary": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal.", "AI": {"tldr": "ZeST利用大型语言模型（LLM）的视觉推理能力，实现零样本、实时地形可通行性预测，从而在不将机器人置于危险中的情况下，实现更安全的自主导航。", "motivation": "传统的地形可通行性预测模型数据采集方法需要将机器人置于潜在危险环境中，对设备和安全构成风险。", "method": "该研究提出了ZeST方法，利用大型语言模型（LLM）的视觉推理能力，实时创建可通行性地图，实现零样本可通行性预测，避免了真实世界数据采集的风险。", "result": "实验结果表明，与现有最先进的方法相比，ZeST在受控室内和非结构化室外环境中均能提供更安全的导航，并持续到达最终目标。", "conclusion": "ZeST提供了一种经济高效、可扩展且更安全的解决方案，用于地形可通行性预测，加速了高级导航系统的开发，同时降低了与真实世界数据采集相关的风险。"}}
{"id": "2508.18687", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18687", "abs": "https://arxiv.org/abs/2508.18687", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "comment": null, "summary": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.", "AI": {"tldr": "研究发现现有医学视觉-语言模型（Med-VLMs）在医学视觉问答中对问题复述表现出脆弱性，导致答案不一致。为此，本文构建了RoMed数据集以揭示这一问题，并提出了结合知识锚定一致性学习和偏置感知对比学习的CCL方法，显著提升了模型的答案一致性和鲁棒性。", "motivation": "在高风险的医疗应用中，模型对不同措辞的提问应保持一致的回答，以确保诊断的可靠性。然而，现有Med-VLMs在面对语义等价的医疗问题复述时，答案波动显著，表现出脆弱性。这主要归因于医学概念对齐不足和训练数据中隐藏的偏置。", "method": "1. 构建了RoMed数据集，包含14.4万个问题，涵盖词级、句级和语义级的变体，用于评估模型对问题复述的鲁棒性。2. 提出了“一致性与对比学习”（CCL）方法，包含两个核心组件：a) 知识锚定一致性学习，使Med-VLMs与医学知识对齐；b) 偏置感知对比学习，通过判别性表示细化来缓解数据特异性先验。", "result": "1. 在RoMed数据集上评估时，LLaVA-Med等最先进模型表现出惊人的性能下降（例如，召回率下降40%），暴露了严重的鲁棒性差距。2. CCL方法在三个流行的VQA基准测试上达到了最先进的性能。3. CCL在具有挑战性的RoMed测试集上将答案一致性提高了50%，显著增强了模型的鲁棒性。", "conclusion": "现有Med-VLMs在医学视觉问答中对问题复述的脆弱性是一个关键问题，RoMed数据集有效揭示了这一点。所提出的CCL方法通过解决概念对齐不足和数据偏置问题，显著提升了Med-VLMs的答案一致性和鲁棒性，使其更适用于高风险医疗应用。"}}
{"id": "2508.18632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18632", "abs": "https://arxiv.org/abs/2508.18632", "authors": ["Huayi Wang", "Haochao Ying", "Yuyang Xu", "Qibo Qiu", "Cheng Zhang", "Danny Z. Chen", "Ying Sun", "Jian Wu"], "title": "Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction", "comment": "10 pages", "summary": "Cancer survival analysis commonly integrates information across diverse\nmedical modalities to make survival-time predictions. Existing methods\nprimarily focus on extracting different decoupled features of modalities and\nperforming fusion operations such as concatenation, attention, and MoE-based\n(Mixture-of-Experts) fusion. However, these methods still face two key\nchallenges: i) Fixed fusion schemes (concatenation and attention) can lead to\nmodel over-reliance on predefined feature combinations, limiting the dynamic\nfusion of decoupled features; ii) in MoE-based fusion methods, each expert\nnetwork handles separate decoupled features, which limits information\ninteraction among the decoupled features. To address these challenges, we\npropose a novel Decoupling-Reorganization-Fusion framework (DeReF), which\ndevises a random feature reorganization strategy between modalities decoupling\nand dynamic MoE fusion modules.Its advantages are: i) it increases the\ndiversity of feature combinations and granularity, enhancing the generalization\nability of the subsequent expert networks; ii) it overcomes the problem of\ninformation closure and helps expert networks better capture information among\ndecoupled features. Additionally, we incorporate a regional cross-attention\nnetwork within the modality decoupling module to improve the representation\nquality of decoupled features. Extensive experimental results on our in-house\nLiver Cancer (LC) and three widely used TCGA public datasets confirm the\neffectiveness of our proposed method. The code will be made publicly available.", "AI": {"tldr": "该研究提出了一个名为DeReF的新型解耦-重组-融合框架，通过引入随机特征重组策略和区域交叉注意力网络，解决了癌症生存分析中现有模态融合方法（如固定融合和MoE专家网络信息隔离）的局限性，从而提高了预测的泛化能力和信息交互。", "motivation": "现有癌症生存分析方法在模态融合方面面临两大挑战：1) 固定融合方案（如拼接、注意力）可能导致模型过度依赖预定义的特征组合，限制了去耦特征的动态融合；2) 基于MoE的融合方法中，每个专家网络独立处理去耦特征，限制了去耦特征之间的信息交互。", "method": "该论文提出了一个新颖的解耦-重组-融合（DeReF）框架。其核心方法包括：1) 在模态解耦和动态MoE融合模块之间设计了随机特征重组策略，以增加特征组合的多样性和粒度，并克服信息封闭问题；2) 在模态解耦模块中引入了区域交叉注意力网络，以提高去耦特征的表示质量。", "result": "在自建的肝癌（LC）数据集和三个广泛使用的TCGA公共数据集上进行了广泛的实验，结果证实了所提出方法的有效性。", "conclusion": "DeReF框架通过其独特的随机特征重组和区域交叉注意力机制，成功解决了现有癌症生存分析中模态融合的挑战，有效提升了模型捕获去耦特征间信息的能力和泛化性，从而提高了生存时间预测的准确性。"}}
{"id": "2508.18760", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18760", "abs": "https://arxiv.org/abs/2508.18760", "authors": ["Yi Liu", "Xiangyu Liu", "Zequn Sun", "Wei Hu"], "title": "Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models", "comment": null, "summary": "Large reasoning models (LRMs) have shown remarkable progress on complex\nreasoning tasks. However, some questions posed to LRMs are inherently\nunanswerable, such as math problems lacking sufficient conditions. We find that\nLRMs continually fail to provide appropriate abstentions when confronted with\nthese unanswerable questions. In this paper, we systematically analyze,\ninvestigate, and resolve this issue for trustworthy AI. We first conduct a\ndetailed analysis of the distinct response behaviors of LRMs when facing\nunanswerable questions. Then, we show that LRMs possess sufficient cognitive\ncapabilities to recognize the flaws in these questions. However, they fail to\nexhibit appropriate abstention behavior, revealing a misalignment between their\ninternal cognition and external response. Finally, to resolve this issue, we\npropose a lightweight, two-stage method that combines cognitive monitoring with\ninference-time intervention. Experimental results demonstrate that our method\nsignificantly improves the abstention rate while maintaining the overall\nreasoning performance.", "AI": {"tldr": "大型推理模型（LRMs）在面对无法回答的问题时，尽管具备认知能力识别问题缺陷，但却未能恰当地拒绝回答。本文系统分析了这一问题，并提出了一种轻量级的两阶段方法来显著提高LRMs的拒绝回答率。", "motivation": "大型推理模型（LRMs）在复杂推理任务上取得了显著进展，但当遇到诸如缺乏充分条件的数学问题等无法回答的问题时，它们未能提供适当的拒绝回答（abstention），这阻碍了可信赖AI的发展。", "method": "本文首先详细分析了LRMs在面对无法回答问题时的不同响应行为。然后，证明了LRMs具备识别问题缺陷的足够认知能力，但其内部认知与外部响应之间存在错位。最后，提出了一种轻量级的两阶段方法，结合了认知监控（cognitive monitoring）和推理时干预（inference-time intervention）来解决这一问题。", "result": "实验结果表明，所提出的方法显著提高了LRMs的拒绝回答率，同时保持了整体的推理性能。", "conclusion": "LRMs具备识别无法回答问题的认知能力，但其未能恰当拒绝回答揭示了内部认知与外部响应之间的不一致。本文提出的两阶段方法有效解决了这一问题，使LRMs在面对无法回答的问题时能更可靠地拒绝回答，从而增强了AI的可信赖性。"}}
{"id": "2508.19150", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19150", "abs": "https://arxiv.org/abs/2508.19150", "authors": ["Juan Carlos Saborío", "Marc Vinci", "Oscar Lima", "Sebastian Stock", "Lennart Niecksch", "Martin Günther", "Alexander Sung", "Joachim Hertzberg", "Martin Atzmüller"], "title": "Uncertainty-Resilient Active Intention Recognition for Robotic Assistants", "comment": "(To appear) In Proceedings of ECMR 2025", "summary": "Purposeful behavior in robotic assistants requires the integration of\nmultiple components and technological advances. Often, the problem is reduced\nto recognizing explicit prompts, which limits autonomy, or is oversimplified\nthrough assumptions such as near-perfect information. We argue that a critical\ngap remains unaddressed -- specifically, the challenge of reasoning about the\nuncertain outcomes and perception errors inherent to human intention\nrecognition. In response, we present a framework designed to be resilient to\nuncertainty and sensor noise, integrating real-time sensor data with a\ncombination of planners. Centered around an intention-recognition POMDP, our\napproach addresses cooperative planning and acting under uncertainty. Our\nintegrated framework has been successfully tested on a physical robot with\npromising results.", "AI": {"tldr": "本文提出一个基于意图识别POMDP的框架，旨在解决机器人助手中人类意图识别固有的不确定性和感知误差问题，以实现不确定环境下的协作规划和行动。", "motivation": "现有方法要么通过明确提示限制了机器人自主性，要么通过假设近乎完美的信息而过于简化问题。研究发现，在人类意图识别中，对不确定结果和感知错误的推理是一个未被解决的关键空白。", "method": "本研究提出了一个对不确定性和传感器噪声具有鲁棒性的框架，该框架将实时传感器数据与多种规划器相结合。其核心是一个意图识别部分可观测马尔可夫决策过程（POMDP），用于解决不确定性下的协作规划和行动。", "result": "该集成框架已在实体机器人上成功测试，并取得了可喜的结果。", "conclusion": "该框架通过集成意图识别POMDP，有效解决了机器人助手中人类意图识别的不确定性和感知误差问题，实现了在不确定环境下的协作规划和行动。"}}
{"id": "2508.18701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18701", "abs": "https://arxiv.org/abs/2508.18701", "authors": ["Yanfan Du", "Jun Zhang", "Bin Wang", "Jin Qiu", "Lu Huang", "Yuan Ge", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System", "comment": "9 pages, 4 figures, 5 tables", "summary": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.", "AI": {"tldr": "本文提出Attention2Probability，一种轻量、灵活、准确的方法，通过将语音和术语间的交叉注意力权重转换为存在概率，并结合课程学习，有效解决了语音大模型在处理领域特定术语时的挑战。该方法显著提高了术语召回率和识别准确性，并发布了新的带术语语音数据集。", "motivation": "尽管语音大模型（SLMs）在通用领域的语音识别和翻译方面取得了进展，但在准确生成领域特定术语或新词方面仍面临挑战。", "method": "本文提出了Attention2Probability方法，它将语音和术语之间的交叉注意力权重转换为术语存在概率，并通过课程学习来提高检索准确性。此外，为了解决带术语干预的语音到文本任务数据不足的问题，本文创建并发布了一个新的带术语的语音数据集。", "result": "实验结果表明，Attention2Probability在测试集上显著优于VectorDB方法。其最大召回率中文达到92.57%，英文达到86.83%，且每次查询的延迟仅为8.71毫秒。通过Attention2Probability检索到的术语干预SLMs的识别和翻译任务，术语准确性提高了6-17%，同时也揭示了当前SLMs在术语利用方面存在的局限性。", "conclusion": "Attention2Probability是一种高效、轻量且准确的解决方案，能显著提升语音大模型处理领域特定术语的能力，具有高召回率和低延迟的优点。该研究还通过发布新数据集和揭示SLM局限性，为未来研究指明了方向。"}}
{"id": "2508.18633", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18633", "abs": "https://arxiv.org/abs/2508.18633", "authors": ["Chenxuan Miao", "Yutong Feng", "Jianshu Zeng", "Zixiang Gao", "Hantang Liu", "Yunfeng Yan", "Donglian Qi", "Xi Chen", "Bin Wang", "Hengshuang Zhao"], "title": "ROSE: Remove Objects with Side Effects in Videos", "comment": null, "summary": "Video object removal has achieved advanced performance due to the recent\nsuccess of video generative models. However, when addressing the side effects\nof objects, e.g., their shadows and reflections, existing works struggle to\neliminate these effects for the scarcity of paired video data as supervision.\nThis paper presents ROSE, termed Remove Objects with Side Effects, a framework\nthat systematically studies the object's effects on environment, which can be\ncategorized into five common cases: shadows, reflections, light, translucency\nand mirror. Given the challenges of curating paired videos exhibiting the\naforementioned effects, we leverage a 3D rendering engine for synthetic data\ngeneration. We carefully construct a fully-automatic pipeline for data\npreparation, which simulates a large-scale paired dataset with diverse scenes,\nobjects, shooting angles, and camera trajectories. ROSE is implemented as an\nvideo inpainting model built on diffusion transformer. To localize all\nobject-correlated areas, the entire video is fed into the model for\nreference-based erasing. Moreover, additional supervision is introduced to\nexplicitly predict the areas affected by side effects, which can be revealed\nthrough the differential mask between the paired videos. To fully investigate\nthe model performance on various side effect removal, we presents a new\nbenchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five\nspecial side effects for comprehensive evaluation. Experimental results\ndemonstrate that ROSE achieves superior performance compared to existing video\nobject erasing models and generalizes well to real-world video scenarios. The\nproject page is https://rose2025-inpaint.github.io/.", "AI": {"tldr": "该论文提出了ROSE框架，一个基于扩散Transformer的视频修复模型，旨在全面移除视频中的物体及其阴影、反射等五种副作用。通过3D渲染引擎生成大规模合成配对数据，并引入新的ROSE-Bench基准进行评估，ROSE在移除物体及其副作用方面表现优越。", "motivation": "现有视频物体移除方法在处理物体的副作用（如阴影和反射）时表现不佳，主要原因是缺乏用于监督的配对视频数据。", "method": "该研究系统性地将物体对环境的影响分为五种常见情况：阴影、反射、光照、半透明和镜面。为解决数据稀缺问题，利用3D渲染引擎构建了一个全自动管道，用于生成包含多样场景、物体和拍摄角度的大规模合成配对数据集。ROSE模型是一个基于扩散Transformer的视频修复模型，通过输入整个视频进行参考擦除，并引入额外监督，利用配对视频之间的差异掩码明确预测受副作用影响的区域。此外，还提出了一个新的基准ROSE-Bench，用于全面评估模型在各种副作用移除任务上的性能。", "result": "实验结果表明，ROSE在视频物体擦除方面优于现有模型，并能很好地泛化到真实世界的视频场景。", "conclusion": "ROSE框架通过对物体副作用的系统性研究、创新的合成数据生成方法和基于扩散Transformer的模型设计，成功解决了视频中物体及其副作用的移除难题，并在新基准上展示了卓越性能和泛化能力。"}}
{"id": "2508.18763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18763", "abs": "https://arxiv.org/abs/2508.18763", "authors": ["Chao Hao", "Zezheng Wang", "Yanhua Huang", "Ruiwen Xu", "Wenzhe Niu", "Xin Liu", "Zitong Yu"], "title": "Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units", "comment": "Accepted by EMNLP 2025 Main Conference", "summary": "This paper investigates the enhancement of reasoning capabilities in language\nmodels through token-level multi-model collaboration. Our approach selects the\noptimal tokens from the next token distributions provided by multiple models to\nperform autoregressive reasoning. Contrary to the assumption that more models\nyield better results, we introduce a distribution distance-based dynamic\nselection strategy (DDS) to optimize the multi-model collaboration process. To\naddress the critical challenge of vocabulary misalignment in multi-model\ncollaboration, we propose the concept of minimal complete semantic units\n(MCSU), which is simple yet enables multiple language models to achieve natural\nalignment within the linguistic space. Experimental results across various\nbenchmarks demonstrate the superiority of our method. The code will be\navailable at https://github.com/Fanye12/DDS.", "AI": {"tldr": "本文通过令牌级多模型协作增强语言模型推理能力，引入基于分布距离的动态选择策略（DDS）和最小完整语义单元（MCSU）来优化协作并解决词汇不对齐问题。", "motivation": "研究动机是提升语言模型的推理能力，通过利用多个模型协同工作，并解决多模型协作中模型选择优化和词汇不对齐等关键挑战。", "method": "方法包括：1) 令牌级多模型协作，从多个模型的下一令牌分布中选择最优令牌进行自回归推理。2) 引入基于分布距离的动态选择策略（DDS），以优化多模型协作过程，而非简单地增加模型数量。3) 提出最小完整语义单元（MCSU）概念，以解决多模型协作中的词汇不对齐问题，实现语言空间内的自然对齐。", "result": "在各种基准测试上的实验结果表明，所提出的方法具有优越性。", "conclusion": "通过优化多模型协作（DDS）和解决词汇不对齐（MCSU），本文成功提升了语言模型的推理能力，并取得了卓越的性能。"}}
{"id": "2508.19153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19153", "abs": "https://arxiv.org/abs/2508.19153", "authors": ["Allen Wang", "Gavin Tao"], "title": "QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning", "comment": "14pages, 9 figures, Journal paper", "summary": "We address vision-guided quadruped motion control with reinforcement learning\n(RL) and highlight the necessity of combining proprioception with vision for\nrobust control. We propose QuadKAN, a spline-parameterized cross-modal policy\ninstantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates\na spline encoder for proprioception and a spline fusion head for\nproprioception-vision inputs. This structured function class aligns the\nstate-to-action mapping with the piecewise-smooth nature of gait, improving\nsample efficiency, reducing action jitter and energy consumption, and providing\ninterpretable posture-action sensitivities. We adopt Multi-Modal Delay\nRandomization (MMDR) and perform end-to-end training with Proximal Policy\nOptimization (PPO). Evaluations across diverse terrains, including both even\nand uneven surfaces and scenarios with static or dynamic obstacles, demonstrate\nthat QuadKAN achieves consistently higher returns, greater distances, and fewer\ncollisions than state-of-the-art (SOTA) baselines. These results show that\nspline-parameterized policies offer a simple, effective, and interpretable\nalternative for robust vision-guided locomotion. A repository will be made\navailable upon acceptance.", "AI": {"tldr": "本文提出QuadKAN，一种基于KANs和样条参数化的跨模态策略，用于视觉引导的四足机器人运动控制。它有效融合本体感觉和视觉，在各种复杂地形下实现了比现有技术更鲁棒、高效和可解释的控制性能。", "motivation": "现有视觉引导的四足机器人运动控制需要更鲁棒的解决方案，尤其强调将本体感觉与视觉有效结合的重要性。", "method": "本文提出了QuadKAN框架，这是一种样条参数化的跨模态策略，通过Kolmogorov-Arnold Networks (KANs) 实现。该框架包含一个用于本体感觉的样条编码器和一个用于本体感觉-视觉输入的样条融合头。这种结构化的函数类别与步态的分段平滑特性对齐。研究采用了多模态延迟随机化 (MMDR) 并使用近端策略优化 (PPO) 进行端到端训练。", "result": "在包括平坦和不平坦表面以及有静态或动态障碍物的多样地形上进行评估，QuadKAN始终比最先进的基线方法实现更高的回报、更大的行进距离和更少的碰撞。此外，它还提高了样本效率，减少了动作抖动和能量消耗，并提供了可解释的姿态-动作敏感性。", "conclusion": "样条参数化策略为鲁棒的视觉引导运动控制提供了一种简单、有效且可解释的替代方案。"}}
{"id": "2508.18709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18709", "abs": "https://arxiv.org/abs/2508.18709", "authors": ["Duy Le", "Kent Ziti", "Evan Girard-Sun", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs", "comment": null, "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.", "AI": {"tldr": "该研究引入了自适应原创性过滤（AOF）框架，通过语义拒绝和词汇新颖性来提高大型语言模型（LLMs）在多语言谜语生成中的原创性和多样性，避免了重复和浅层复述。", "motivation": "大型语言模型在多语言谜语生成中面临挑战，难以平衡文化流畅性和创意抽象，且标准提示策略（零样本、少样本、思维链）倾向于重复记忆的谜语或进行浅层复述。", "method": "研究引入了自适应原创性过滤（AOF）提示框架。该框架通过基于余弦相似度的拒绝机制过滤冗余生成，同时强制执行词汇新颖性和跨语言忠实性。该方法在三种LLM和四种语言对上进行了评估。", "result": "经AOF增强的GPT-4o在日语中实现了0.177的Self-BLEU和0.915的Distinct-2，这表明与其他提示方法和语言对相比，其词汇多样性有所改善，冗余度有所降低。", "conclusion": "研究结果表明，语义拒绝（如AOF）可以在无需任务特定微调的情况下，指导大型语言模型进行具有文化基础的创意生成。"}}
{"id": "2508.18634", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18634", "abs": "https://arxiv.org/abs/2508.18634", "authors": ["Chunlin Zhong", "Qiuxia Hou", "Zhangjun Zhou", "Shuang Hao", "Haonan Lu", "Yanhao Zhang", "He Tang", "Xiang Bai"], "title": "OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward", "comment": "9 pages, 6figures", "summary": "Video captioning aims to generate comprehensive and coherent descriptions of\nthe video content, contributing to the advancement of both video understanding\nand generation. However, existing methods often suffer from motion-detail\nimbalance, as models tend to overemphasize one aspect while neglecting the\nother. This imbalance results in incomplete captions, which in turn leads to a\nlack of consistency in video understanding and generation. To address this\nissue, we propose solutions from two aspects: 1) Data aspect: We constructed\nthe Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage\npipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)\nOptimization aspect: We introduce the Caption Set Equivalence Reward (CSER)\nbased on Group Relative Policy Optimization (GRPO). CSER enhances completeness\nand accuracy in capturing both motion and details through unit-to-set matching\nand bidirectional validation. Based on the HMD-270K supervised fine-tuning and\nGRPO post-training with CSER, we developed OwlCap, a powerful video captioning\nmulti-modal large language model (MLLM) with motion-detail balance.\nExperimental results demonstrate that OwlCap achieves significant improvements\ncompared to baseline models on two benchmarks: the detail-focused VDC (+4.2\nAcc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap\nmodel will be publicly released to facilitate video captioning research\ncommunity advancements.", "AI": {"tldr": "该研究提出了OwlCap，一个多模态大语言模型，通过构建HMD-270K数据集和引入CSER奖励机制，解决了视频字幕生成中运动和细节描述不平衡的问题，实现了更全面和准确的视频理解与生成。", "motivation": "现有视频字幕生成方法存在运动-细节不平衡问题，模型倾向于过度强调某一方面而忽略另一方面，导致生成的字幕不完整，进而影响视频理解和生成的一致性。", "method": "1) 数据层面：构建了HMD-270K数据集，通过“运动-细节融合（MDF）”和“细粒度检查（FGE）”两阶段流程实现。2) 优化层面：基于组相对策略优化（GRPO）引入了“字幕集等效奖励（CSER）”，通过单元到集合匹配和双向验证增强了捕捉运动和细节的完整性和准确性。基于HMD-270K监督微调和CSER的GRPO后训练，开发了OwlCap模型。", "result": "实验结果表明，OwlCap在两个基准测试中取得了显著改进：在注重细节的VDC上准确率提升了4.2%，在注重运动的DREAM-1K上F1分数提升了4.6%，优于基线模型。", "conclusion": "OwlCap模型通过数据和优化两方面的创新，有效解决了视频字幕生成中的运动-细节不平衡问题，实现了更全面、一致的视频理解与生成。HMD-270K数据集和OwlCap模型将公开发布，以促进视频字幕研究社区的进步。"}}
{"id": "2508.18781", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.18781", "abs": "https://arxiv.org/abs/2508.18781", "authors": ["Lisai Zhang", "Baohan Xu", "Siqian Yang", "Mingyu Yin", "Jing Liu", "Chao Xu", "Siqi Wang", "Yidi Wu", "Yuxin Hong", "Zihao Zhang", "Yanzhang Liang", "Yudong Jiang"], "title": "AniME: Adaptive Multi-Agent Planning for Long Animation Generation", "comment": "2 pages, Technical Report", "summary": "We present AniME, a director-oriented multi-agent system for automated\nlong-form anime production, covering the full workflow from a story to the\nfinal video. The director agent keeps a global memory for the whole workflow,\nand coordinates several downstream specialized agents. By integrating\ncustomized Model Context Protocol (MCP) with downstream model instruction, the\nspecialized agent adaptively selects control conditions for diverse sub-tasks.\nAniME produces cinematic animation with consistent characters and synchronized\naudio visual elements, offering a scalable solution for AI-driven anime\ncreation.", "AI": {"tldr": "AniME是一个面向导演的多智能体系统，用于自动化长篇动漫制作，涵盖从故事到最终视频的整个工作流程。", "motivation": "旨在为AI驱动的动漫创作提供可扩展的解决方案，以实现电影级动画的自动化生产。", "method": "AniME采用多智能体系统架构，包括一个负责全局记忆和协调的导演智能体，以及多个通过集成定制模型上下文协议（MCP）和下游模型指令来适应性选择控制条件的专业智能体。", "result": "该系统能够制作出具有角色一致性、音视频同步的电影级动画。", "conclusion": "AniME为AI驱动的动漫创作提供了一个可扩展的解决方案，能够自动化生成高质量的动漫视频。"}}
{"id": "2508.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19168", "abs": "https://arxiv.org/abs/2508.19168", "authors": ["Liding Zhang", "Kejia Chen", "Kuanqi Cai", "Yu Zhang", "Yixuan Dang", "Yansong Wu", "Zhenshan Bing", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic", "comment": "7 pages, 5 figures, 2025 IEEE International Conference on Robotics\n  and Automation (ICRA)", "summary": "Optimal path planning requires finding a series of feasible states from the\nstarting point to the goal to optimize objectives. Popular path planning\nalgorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to\nguide the search. Effective heuristics are accurate and computationally\nefficient, but achieving both can be challenging due to their conflicting\nnature. This paper proposes Direction Informed Trees (DIT*), a sampling-based\nplanner that focuses on optimizing the search direction for each edge,\nresulting in goal bias during exploration. We define edges as generalized\nvectors and integrate similarity indexes to establish a directional filter that\nselects the nearest neighbors and estimates direction costs. The estimated\ndirection cost heuristics are utilized in edge evaluation. This strategy allows\nthe exploration to share directional information efficiently. DIT* convergence\nfaster than existing single-query, sampling-based planners on tested problems\nin R^4 to R^16 and has been demonstrated in real-world environments with\nvarious planning tasks. A video showcasing our experimental results is\navailable at: https://youtu.be/2SX6QT2NOek", "AI": {"tldr": "DIT* 是一种新的采样式路径规划器，通过优化搜索方向实现目标偏向和高效信息共享，从而在多维空间和实际场景中实现比现有算法更快的收敛速度。", "motivation": "现有的路径规划算法（如 EIT*）依赖努力启发式（effort heuristics）来指导搜索，但实现启发式的准确性和计算效率往往相互冲突，难以兼顾。", "method": "本文提出了方向信息树（DIT*）规划器，它将边定义为广义向量，并通过集成相似性索引建立方向过滤器，用于选择最近邻并估计方向成本。这些估计的方向成本启发式被用于边的评估，从而在探索过程中高效地共享方向信息，形成目标偏向。", "result": "DIT* 在 R^4 到 R^16 的测试问题中，比现有的单查询、基于采样的规划器收敛更快，并且已在实际环境中各种规划任务中得到验证。", "conclusion": "DIT* 通过优化搜索方向和利用方向成本启发式，有效地提高了采样式路径规划器的收敛速度和性能。"}}
{"id": "2508.18715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18715", "abs": "https://arxiv.org/abs/2508.18715", "authors": ["Angela Yifei Yuan", "Haoyi Li", "Soyeon Caren Han", "Christopher Leckie"], "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues", "comment": "15 pages", "summary": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection.", "AI": {"tldr": "该论文提出EMMM，一个“先解释后检测”的框架，用于客户服务场景中的机器生成文本（MGT）检测。该框架在保证低延迟和高准确性的同时，为非专业用户提供了可理解的解释。", "motivation": "大型语言模型（LLMs）在客户服务中的广泛应用带来了新风险，恶意行为者可利用其进行大规模用户冒充。现有MGT检测方法在在线对话环境中效果不佳，缺乏可靠性和可解释性。对于客户服务中通常是非专业人员的操作员来说，可解释性对于可信赖的MGT检测至关重要。", "method": "本文提出了EMMM，一个“先解释后检测”的框架。该框架旨在平衡延迟、准确性和面向非专业人员的可解释性。", "result": "实验结果表明，EMMM提供的解释对非专业用户易于理解，70%的人类评估者更喜欢其输出。同时，它在准确性方面与最先进的模型具有竞争力，并保持低延迟，在1秒内生成输出。", "conclusion": "EMMM框架成功地为客户服务场景中的MGT检测提供了可信赖的解决方案，它在满足非专业用户对可解释性需求的同时，兼顾了高准确性和低延迟。"}}
{"id": "2508.18641", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18641", "abs": "https://arxiv.org/abs/2508.18641", "authors": ["Ye Tao", "Xinran Fu", "Honglin Pang", "Xi Yang", "Chuntao Li"], "title": "Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection", "comment": null, "summary": "Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient\nChinese civilization. The automated detection of OBIs from rubbing images\nrepresents a fundamental yet challenging task in digital archaeology, primarily\ndue to various degradation factors including noise and cracks that limit the\neffectiveness of conventional detection networks. To address these challenges,\nwe propose a novel clustering-based feature space representation learning\nmethod. Our approach uniquely leverages the Oracle Bones Character (OBC) font\nlibrary dataset as prior knowledge to enhance feature extraction in the\ndetection network through clustering-based representation learning. The method\nincorporates a specialized loss function derived from clustering results to\noptimize feature representation, which is then integrated into the total\nnetwork loss. We validate the effectiveness of our method by conducting\nexperiments on two OBIs detection dataset using three mainstream detection\nframeworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive\nexperimentation, all frameworks demonstrate significant performance\nimprovements.", "AI": {"tldr": "该研究提出了一种新颖的基于聚类的特征空间表示学习方法，利用甲骨文字库作为先验知识，通过集成专门的聚类损失函数，显著提升了甲骨文拓片图像检测的性能，解决了传统方法在图像退化下的局限性。", "motivation": "甲骨文对于理解中国古代文明至关重要。从拓片图像中自动检测甲骨文是一项基础但具挑战性的任务，主要原因是噪声和裂缝等多种退化因素限制了传统检测网络的有效性。", "method": "本文提出了一种新颖的基于聚类的特征空间表示学习方法。该方法独特地利用甲骨文字符（OBC）字库数据集作为先验知识，通过基于聚类的表示学习来增强检测网络中的特征提取。它包含一个源自聚类结果的专门损失函数，用于优化特征表示，并将其整合到总网络损失中。该方法在两个甲骨文检测数据集上，使用Faster R-CNN、DETR和Sparse R-CNN三个主流检测框架进行了验证。", "result": "通过广泛的实验，所有框架（Faster R-CNN、DETR和Sparse R-CNN）都展示了显著的性能提升。", "conclusion": "所提出的基于聚类的特征空间表示学习方法能够有效应对甲骨文拓片图像检测中的退化挑战，并显著提升了主流检测框架的性能。"}}
{"id": "2508.18797", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18797", "abs": "https://arxiv.org/abs/2508.18797", "authors": ["Qi Chai", "Zhang Zheng", "Junlong Ren", "Deheng Ye", "Zichuan Lin", "Hao Wang"], "title": "CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks", "comment": null, "summary": "Minecraft, as an open-world virtual interactive environment, has become a\nprominent platform for research on agent decision-making and execution.\nExisting works primarily adopt a single Large Language Model (LLM) agent to\ncomplete various in-game tasks. However, for complex tasks requiring lengthy\nsequences of actions, single-agent approaches often face challenges related to\ninefficiency and limited fault tolerance. Despite these issues, research on\nmulti-agent collaboration remains scarce. In this paper, we propose CausalMACE,\na holistic causality planning framework designed to enhance multi-agent\nsystems, in which we incorporate causality to manage dependencies among\nsubtasks. Technically, our proposed framework introduces two modules: an\noverarching task graph for global task planning and a causality-based module\nfor dependency management, where inherent rules are adopted to perform causal\nintervention. Experimental results demonstrate our approach achieves\nstate-of-the-art performance in multi-agent cooperative tasks of Minecraft.", "AI": {"tldr": "CausalMACE是一个针对Minecraft多智能体协作任务的因果规划框架，通过引入因果关系管理子任务依赖，显著提升了复杂任务的效率和容错性，并达到了最先进的性能。", "motivation": "现有研究主要采用单一大型语言模型（LLM）智能体在Minecraft中完成任务，但对于需要冗长行动序列的复杂任务，单智能体方法效率低下且容错性有限。目前对多智能体协作的研究相对稀缺。", "method": "本文提出了CausalMACE框架，旨在增强多智能体系统。该框架将因果关系融入子任务依赖管理中，并包含两个主要模块：用于全局任务规划的整体任务图，以及一个基于因果关系进行依赖管理并采用固有规则进行因果干预的模块。", "result": "实验结果表明，该方法在Minecraft的多智能体协作任务中实现了最先进的性能。", "conclusion": "CausalMACE通过其独特的因果规划框架，有效解决了Minecraft复杂多智能体任务中的效率和容错性问题，显著提升了多智能体系统的性能。"}}
{"id": "2508.19172", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19172", "abs": "https://arxiv.org/abs/2508.19172", "authors": ["Luca Grillotti", "Lisa Coiffard", "Oscar Pang", "Maxence Faldor", "Antoine Cully"], "title": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity", "comment": "Accepted at CoRL 2025", "summary": "Autonomous skill discovery aims to enable robots to acquire diverse behaviors\nwithout explicit supervision. Learning such behaviors directly on physical\nhardware remains challenging due to safety and data efficiency constraints.\nExisting methods, including Quality-Diversity Actor-Critic (QDAC), require\nmanually defined skill spaces and carefully tuned heuristics, limiting\nreal-world applicability. We propose Unsupervised Real-world Skill Acquisition\n(URSA), an extension of QDAC that enables robots to autonomously discover and\nmaster diverse, high-performing skills directly in the real world. We\ndemonstrate that URSA successfully discovers diverse locomotion skills on a\nUnitree A1 quadruped in both simulation and the real world. Our approach\nsupports both heuristic-driven skill discovery and fully unsupervised settings.\nWe also show that the learned skill repertoire can be reused for downstream\ntasks such as real-world damage adaptation, where URSA outperforms all\nbaselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.\nOur results establish a new framework for real-world robot learning that\nenables continuous skill discovery with limited human intervention,\nrepresenting a significant step toward more autonomous and adaptable robotic\nsystems. Demonstration videos are available at\nhttp://adaptive-intelligent-robotics.github.io/URSA .", "AI": {"tldr": "本文提出了URSA（无监督真实世界技能获取）方法，它是QDAC的扩展，使机器人能够在真实世界中自主发现和掌握多样化的高性能技能。URSA在Unitree A1四足机器人上成功发现了多种运动技能，并可将所学技能用于损伤适应等下游任务，表现优于基线方法。", "motivation": "机器人自主技能发现面临挑战，尤其是在物理硬件上，存在安全和数据效率限制。现有方法（如QDAC）需要手动定义技能空间和精心调整的启发式方法，这限制了它们在真实世界中的适用性。", "method": "本文提出了无监督真实世界技能获取（URSA）方法，作为质量多样性演员-评论家（QDAC）的扩展。URSA使机器人能够直接在真实世界中自主发现并掌握多样化、高性能的技能。该方法支持启发式驱动和完全无监督的技能发现设置，并在Unitree A1四足机器人上进行了验证。", "result": "URSA成功地在模拟和真实世界中为Unitree A1四足机器人发现了多样化的运动技能。所学习的技能库可以用于下游任务，例如真实世界中的损伤适应。在损伤适应场景中，URSA在9个模拟场景中的5个以及5个真实世界场景中的3个中，表现优于所有基线方法。", "conclusion": "URSA为真实世界机器人学习建立了一个新框架，实现了有限人类干预下的持续技能发现。这标志着向更自主和适应性强的机器人系统迈出了重要一步。"}}
{"id": "2508.18739", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18739", "abs": "https://arxiv.org/abs/2508.18739", "authors": ["Chang Wang", "Siyu Yan", "Depeng Yuan", "Yuqi Chen", "Yanhua Huang", "Yuanhang Zheng", "Shuhao Li", "Yinqi Zhang", "Kedi Chen", "Mingrui Zhu", "Ruiwen Xu"], "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models", "comment": null, "summary": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.", "AI": {"tldr": "DIVER是一个基于大型语言模型（LLM）的框架，通过语义和风格感知的数据生成以及多阶段多目标优化，旨在同时提高广告标题的质量和多样性，并在实际部署中取得了显著效果。", "motivation": "当前广告标题生成方法主要优化质量或点击率，但忽视了多样性需求，导致输出同质化，无法吸引广泛受众。", "method": "该研究首先设计了一个语义和风格感知的DIVER数据生成流程，自动产生包含广告内容和多个多样化标题的高质量训练对。然后，提出了一个结合监督微调（SFT）和强化学习（RL）的多阶段多目标优化框架，以在单次前向传播中生成高质量且多样化的广告标题。", "result": "在真实工业数据集上的实验表明，DIVER有效平衡了标题质量和多样性。部署在一个服务数亿用户的大规模内容共享平台后，该框架将广告主价值（ADVV）提高了4.0%，点击率（CTR）提高了1.4%。", "conclusion": "DIVER框架成功解决了广告标题生成中质量与多样性难以兼顾的问题，通过创新的数据生成和优化方法，显著提升了广告效果，并在实际应用中展现了商业价值。"}}
{"id": "2508.18664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18664", "abs": "https://arxiv.org/abs/2508.18664", "authors": ["Xin Tian", "Yingtie Lei", "Xiujun Zhang", "Zimeng Li", "Chi-Man Pun", "Xuhang Chen"], "title": "SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain", "comment": "Accepted by PRICAI2025", "summary": "Recent learning-based underwater image enhancement (UIE) methods have\nadvanced by incorporating physical priors into deep neural networks,\nparticularly using the signal-to-noise ratio (SNR) prior to reduce\nwavelength-dependent attenuation. However, spatial domain SNR priors have two\nlimitations: (i) they cannot effectively separate cross-channel interference,\nand (ii) they provide limited help in amplifying informative structures while\nsuppressing noise. To overcome these, we propose using the SNR prior in the\nfrequency domain, decomposing features into amplitude and phase spectra for\nbetter channel modulation. We introduce the Fourier Attention SNR-prior\nTransformer (FAST), combining spectral interactions with SNR cues to highlight\nkey spectral components. Additionally, the Frequency Adaptive Transformer (FAT)\nbottleneck merges low- and high-frequency branches using a gated attention\nmechanism to enhance perceptual quality. Embedded in a unified U-shaped\narchitecture, these modules integrate a conventional RGB stream with an\nSNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB,\nEUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and\n0.08 in SSIM, successfully restoring colors, textures, and contrast in\nunderwater scenes.", "AI": {"tldr": "本文提出了一种名为 SFormer 的水下图像增强（UIE）方法，通过在频域应用信噪比（SNR）先验，并结合新型傅里叶注意力 Transformer 模块，有效解决了现有空间域 SNR 先验的局限性，显著提升了水下图像的颜色、纹理和对比度恢复效果。", "motivation": "现有的基于学习的水下图像增强方法虽然结合了物理先验（特别是 SNR 先验），但在空间域应用 SNR 先验存在两个主要限制：(i) 无法有效分离跨通道干扰；(ii) 在放大信息结构和抑制噪声方面的帮助有限。这促使研究者寻求更有效的方式利用 SNR 先验。", "method": "本文提出在频域使用 SNR 先验，将特征分解为幅度和相位谱以实现更好的通道调制。具体方法包括：1. 引入傅里叶注意力 SNR-先验 Transformer (FAST)，结合频谱交互和 SNR 线索来突出关键频谱分量。2. 设计频率自适应 Transformer (FAT) 瓶颈，利用门控注意力机制融合低频和高频分支，以增强感知质量。3. 将这些模块嵌入到统一的 U 型架构中，形成 SFormer，该架构整合了传统的 RGB 流和 SNR 引导分支。模型在 UIEB、EUVP 和 LSUI 的 4,800 对图像上进行训练。", "result": "SFormer 在 PSNR 上比现有方法提高了 3.1 dB，在 SSIM 上提高了 0.08。该方法成功恢复了水下场景的颜色、纹理和对比度，超越了当前先进的方法。", "conclusion": "通过在频域利用 SNR 先验并结合创新的 Transformer 模块（FAST 和 FAT），SFormer 有效克服了空间域 SNR 先验的局限性，显著提升了水下图像增强的性能，实现了更准确的颜色、纹理和对比度恢复。"}}
{"id": "2508.18812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18812", "abs": "https://arxiv.org/abs/2508.18812", "authors": ["Chenghao Wu", "Ruiyang Ren", "Junjie Zhang", "Ruirui Wang", "Zhongrui Ma", "Qi Ye", "Wayne Xin Zhao"], "title": "STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning", "comment": null, "summary": "While modern recommender systems are instrumental in navigating information\nabundance, they remain fundamentally limited by static user modeling and\nreactive decision-making paradigms. Current large language model (LLM)-based\nagents inherit these shortcomings through their overreliance on heuristic\npattern matching, yielding recommendations prone to shallow correlation bias,\nlimited causal inference, and brittleness in sparse-data scenarios. We\nintroduce STARec, a slow-thinking augmented agent framework that endows\nrecommender systems with autonomous deliberative reasoning capabilities. Each\nuser is modeled as an agent with parallel cognitions: fast response for\nimmediate interactions and slow reasoning that performs chain-of-thought\nrationales. To cultivate intrinsic slow thinking, we develop anchored\nreinforcement training - a two-stage paradigm combining structured knowledge\ndistillation from advanced reasoning models with preference-aligned reward\nshaping. This hybrid approach scaffolds agents in acquiring foundational\ncapabilities (preference summarization, rationale generation) while enabling\ndynamic policy adaptation through simulated feedback loops. Experiments on\nMovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves\nsubstantial performance gains compared with state-of-the-art baselines, despite\nusing only 0.4% of the full training data.", "AI": {"tldr": "本文提出STARec，一个基于慢思考增强的智能体框架，为推荐系统引入自主深思熟虑的推理能力，通过并行认知和锚定强化训练克服现有推荐系统的局限性，并在有限数据下显著提升性能。", "motivation": "现有推荐系统（包括基于LLM的）受限于静态用户建模和被动决策范式，导致推荐易受浅层关联偏差影响、因果推断能力有限，且在稀疏数据场景下表现脆弱。它们过度依赖启发式模式匹配，未能实现深思熟虑的推理。", "method": "STARec框架将每个用户建模为一个具有并行认知（快速响应和慢速链式思维推理）的智能体。为培养内在慢思考能力，本文开发了“锚定强化训练”范式，结合了来自高级推理模型的结构化知识蒸馏和偏好对齐的奖励塑造。这种混合方法使智能体能够获取基础能力（偏好总结、理由生成）并通过模拟反馈循环实现动态策略适应。", "result": "在MovieLens 1M和Amazon CDs基准测试中，STARec相较于最先进的基线取得了显著的性能提升，尽管仅使用了0.4%的完整训练数据。", "conclusion": "STARec通过引入慢思考增强的智能体框架和创新的锚定强化训练方法，成功地赋予推荐系统自主深思熟虑的推理能力，有效克服了现有系统的局限性，并在数据受限的情况下展现出卓越的推荐性能。"}}
{"id": "2508.19186", "categories": ["cs.RO", "cs.AI", "cs.FL", "I.2.9; I.2; D.2.4"], "pdf": "https://arxiv.org/pdf/2508.19186", "abs": "https://arxiv.org/abs/2508.19186", "authors": ["Christopher Chandler", "Bernd Porr", "Giulia Lafratta", "Alice Miller"], "title": "Real-Time Model Checking for Closed-Loop Robot Reactive Planning", "comment": "30 pages excluding references, 18 figures, submitted to Formal\n  Aspects of Computing", "summary": "We present a new application of model checking which achieves real-time\nmulti-step planning and obstacle avoidance on a real autonomous robot. We have\ndeveloped a small, purpose-built model checking algorithm which generates plans\nin situ based on \"core\" knowledge and attention as found in biological agents.\nThis is achieved in real-time using no pre-computed data on a low-powered\ndevice. Our approach is based on chaining temporary control systems which are\nspawned to counteract disturbances in the local environment that disrupt an\nautonomous agent from its preferred action (or resting state). A novel\ndiscretization of 2D LiDAR data sensitive to bounded variations in the local\nenvironment is used. Multi-step planning using model checking by forward\ndepth-first search is applied to cul-de-sac and playground scenarios. Both\nempirical results and informal proofs of two fundamental properties of our\napproach demonstrate that model checking can be used to create efficient\nmulti-step plans for local obstacle avoidance, improving on the performance of\na reactive agent which can only plan one step. Our approach is an instructional\ncase study for the development of safe, reliable and explainable planning in\nthe context of autonomous vehicles.", "AI": {"tldr": "本文提出了一种新的模型检测应用，可在低功耗自主机器人上实现实时的多步规划和避障，通过生物启发式方法和新颖的LiDAR数据离散化，显著优于传统反应式代理。", "motivation": "目前的反应式代理只能进行单步规划，无法满足自主车辆对安全、可靠和可解释的多步规划的需求，尤其是在资源受限设备上的实时应用。", "method": "开发了一种小型、专用模型检测算法，基于生物代理的“核心”知识和注意力，在低功耗设备上实时生成计划，无需预计算数据。该方法通过链接临时控制系统来抵消环境干扰，并使用对局部环境有界变化敏感的新颖2D LiDAR数据离散化。采用前向深度优先搜索的模型检测进行多步规划，并在死胡同和操场场景中进行了应用。", "result": "模型检测能够为局部避障创建高效的多步计划，性能优于只能进行单步规划的反应式代理。通过实证结果和两种基本属性的非正式证明，验证了该方法的有效性。", "conclusion": "模型检测可以有效地用于自主机器人的实时多步规划和局部避障，为自主车辆中安全、可靠和可解释的规划发展提供了一个有指导意义的案例研究。"}}
{"id": "2508.18740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18740", "abs": "https://arxiv.org/abs/2508.18740", "authors": ["Qiao Liang", "Ying Shen", "Tiantian Chen", "Lin Zhang"], "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations", "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025", "summary": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG.", "AI": {"tldr": "本文针对多模态对话中情感原因三元组抽取（MECTEC）任务，发布了首个多模态、多场景数据集MECAD，并提出了M3HG模型，通过多模态异构图显式建模情感和因果上下文，并融合不同层级的语义信息，取得了SOTA性能。", "motivation": "MECTEC任务面临两个主要挑战：1. 相关数据集稀缺，现有数据集场景单一，阻碍了模型发展。2. 现有MECTEC方法未能显式建模情感和因果上下文，且忽略了不同层级语义信息的融合，导致性能下降。", "method": "1. 引入了MECAD数据集，这是首个多模态、多场景的MECTEC数据集，包含来自56部电视剧的989个对话。2. 提出了M3HG模型，该模型通过构建多模态异构图，显式捕获情感和因果上下文，并有效融合了语篇内部和语篇之间的上下文信息。", "result": "广泛的实验证明，与现有最先进的方法相比，M3HG模型表现出卓越的有效性。", "conclusion": "本文通过发布首个多模态、多场景的MECTEC数据集MECAD，并提出能够有效建模上下文和融合多层级信息的M3HG模型，显著推动了多模态对话中情感原因三元组抽取任务的发展。"}}
{"id": "2508.18681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18681", "abs": "https://arxiv.org/abs/2508.18681", "authors": ["Dongfang Wang", "Jian Yang", "Yizhe Zhang", "Tao Zhou"], "title": "Hierarchical Spatio-temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos", "comment": null, "summary": "Automated segmentation of the left ventricular endocardium in\nechocardiography videos is a key research area in cardiology. It aims to\nprovide accurate assessment of cardiac structure and function through Ejection\nFraction (EF) estimation. Although existing studies have achieved good\nsegmentation performance, their results do not perform well in EF estimation.\nIn this paper, we propose a Hierarchical Spatio-temporal Segmentation Network\n(\\ourmodel) for echocardiography video, aiming to improve EF estimation\naccuracy by synergizing local detail modeling with global dynamic perception.\nThe network employs a hierarchical design, with low-level stages using\nconvolutional networks to process single-frame images and preserve details,\nwhile high-level stages utilize the Mamba architecture to capture\nspatio-temporal relationships. The hierarchical design balances single-frame\nand multi-frame processing, avoiding issues such as local error accumulation\nwhen relying solely on single frames or neglecting details when using only\nmulti-frame data. To overcome local spatio-temporal limitations, we propose the\nSpatio-temporal Cross Scan (STCS) module, which integrates long-range context\nthrough skip scanning across frames and positions. This approach helps mitigate\nEF calculation biases caused by ultrasound image noise and other factors.", "AI": {"tldr": "该研究提出了一种分层时空分割网络（HS2Net），用于超声心动图视频中的左心室心内膜分割。通过结合局部细节建模和全局动态感知，并引入时空交叉扫描模块，旨在显著提高射血分数（EF）的估计精度。", "motivation": "超声心动图视频中左心室心内膜的自动分割是心脏病学中的关键研究领域，旨在通过射血分数（EF）估计准确评估心脏结构和功能。然而，现有方法虽然在分割性能上表现良好，但在EF估计方面的表现不佳。", "method": "本文提出了一种分层时空分割网络（HS2Net）。其分层设计在低层使用卷积网络处理单帧图像以保留细节，在高层利用Mamba架构捕获时空关系，从而平衡单帧与多帧处理。为克服局部时空限制，还引入了时空交叉扫描（STCS）模块，通过跨帧和位置的跳跃扫描来整合长程上下文。", "result": "该方法旨在通过协同局部细节建模与全局动态感知，显著提高射血分数（EF）的估计准确性。此外，通过整合长程上下文，有助于减轻超声图像噪声及其他因素导致的EF计算偏差。", "conclusion": "所提出的分层时空分割网络（HS2Net）及其时空交叉扫描（STCS）模块，通过有效结合局部细节和全局动态信息，能够改善超声心动图视频中的左心室心内膜分割，从而提高射血分数（EF）的估计精度，克服了现有方法的局局限性。"}}
{"id": "2508.18880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18880", "abs": "https://arxiv.org/abs/2508.18880", "authors": ["Eljas Linna", "Tuula Linna"], "title": "Judicial Requirements for Generative AI in Legal Reasoning", "comment": null, "summary": "Large Language Models (LLMs) are being integrated into professional domains,\nyet their limitations in high-stakes fields like law remain poorly understood.\nThis paper defines the core capabilities that an AI system must possess to\nfunction as a reliable reasoning tool in judicial decision-making. Using the\nIRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the\nstudy focuses on the most challenging phases of legal adjudication: determining\nthe applicable Rule (R) and performing the Application (A) of that rule to the\nfacts of a case. From a judicial perspective, the analysis deconstructs legal\nreasoning into a series of core requirements, including the ability to select\nthe correct legal framework across jurisdictions, generate sound arguments\nbased on the doctrine of legal sources, distinguish ratio decidendi from obiter\ndictum in case law, resolve ambiguity arising from general clauses like\n\"reasonableness\", manage conflicting legal provisions, and correctly apply the\nburden of proof. The paper then maps various AI enhancement mechanisms, such as\nRetrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic\nAI, to these requirements, assessing their potential to bridge the gap between\nthe probabilistic nature of LLMs and the rigorous, choice-driven demands of\nlegal interpretation. The findings indicate that while these techniques can\naddress specific challenges, significant challenges remain, particularly in\ntasks requiring discretion and transparent, justifiable reasoning. Our paper\nconcludes that the most effective current role for AI in law is a dual one: as\na high-volume assistant for simple, repetitive cases and as a sophisticated\n\"sparring partner\" for human experts in complex matters.", "AI": {"tldr": "本研究定义了AI在司法决策中作为可靠推理工具所需的核心能力，并评估了现有AI增强机制（如RAG、多智能体系统、神经符号AI）弥合大型语言模型（LLMs）与法律解释之间差距的潜力，发现AI目前最适合作为简单案件的助手和复杂案件中人类专家的“陪练伙伴”。", "motivation": "大型语言模型（LLMs）正被整合到专业领域，但它们在法律等高风险领域中的局限性仍未被充分理解，这促使研究者探究AI系统在司法决策中作为可靠推理工具所需的核心能力。", "method": "研究使用IRAC（Issue-Rule-Application-Conclusion）模型作为分析框架，重点关注法律裁决中最具挑战性的“规则确定（R）”和“规则应用（A）”阶段。从司法角度，将法律推理解构为一系列核心要求（如选择法律框架、生成论证、区分判例法中的主旨与附带意见、解决歧义、管理冲突条款、正确应用举证责任）。然后，将检索增强生成（RAG）、多智能体系统和神经符号AI等AI增强机制与这些要求进行匹配，评估它们弥合LLMs概率性与法律解释严谨性之间差距的潜力。", "result": "研究发现，虽然这些AI增强技术可以解决特定挑战，但仍存在显著挑战，特别是在需要自由裁量权和透明、可证明推理的任务中。LLMs的概率性质与法律解释中严格、选择驱动的需求之间仍存在较大鸿沟。", "conclusion": "本研究得出结论，AI目前在法律领域最有效的角色是双重的：作为处理简单、重复案件的高效助手，以及作为复杂案件中人类专家的精密“陪练伙伴”。"}}
{"id": "2508.19191", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19191", "abs": "https://arxiv.org/abs/2508.19191", "authors": ["Yue Wang", "Wenjie Deng", "Haotian Xue", "Di Cui", "Yiqi Chen", "Mingchuan Zhou", "Haochao Ying", "Jian Wu"], "title": "AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot", "comment": null, "summary": "Intraocular foreign body removal demands millimeter-level precision in\nconfined intraocular spaces, yet existing robotic systems predominantly rely on\nmanual teleoperation with steep learning curves. To address the challenges of\nautonomous manipulation (particularly kinematic uncertainties from variable\nmotion scaling and variation of the Remote Center of Motion (RCM) point), we\npropose AutoRing, an imitation learning framework for autonomous intraocular\nforeign body ring manipulation. Our approach integrates dynamic RCM calibration\nto resolve coordinate-system inconsistencies caused by intraocular instrument\nvariation and introduces the RCM-ACT architecture, which combines\naction-chunking transformers with real-time kinematic realignment. Trained\nsolely on stereo visual data and instrument kinematics from expert\ndemonstrations in a biomimetic eye model, AutoRing successfully completes ring\ngrasping and positioning tasks without explicit depth sensing. Experimental\nvalidation demonstrates end-to-end autonomy under uncalibrated microscopy\nconditions. The results provide a viable framework for developing intelligent\neye-surgical systems capable of complex intraocular procedures.", "AI": {"tldr": "AutoRing是一个模仿学习框架，用于自主进行眼内异物环操作，通过动态RCM校准和RCM-ACT架构解决了运动学不确定性问题，实现了在生物模拟眼模型中的抓取和定位任务。", "motivation": "眼内异物移除需要毫米级精度，但现有机器人系统主要依赖手动遥操作，学习曲线陡峭。自主操作面临运动学不确定性（可变运动缩放和远程运动中心点(RCM)变化）的挑战。", "method": "本文提出了AutoRing，一个用于自主眼内异物环操作的模仿学习框架。它整合了动态RCM校准以解决眼内器械变化导致的坐标系不一致问题，并引入了RCM-ACT架构，该架构结合了动作分块Transformer和实时运动学重新对齐。该系统仅使用专家演示的立体视觉数据和器械运动学（在生物模拟眼模型中）进行训练，无需明确的深度传感。", "result": "AutoRing成功完成了环的抓取和定位任务，并在未校准的显微镜条件下展示了端到端自主性。", "conclusion": "研究结果为开发能够进行复杂眼内手术的智能眼科手术系统提供了一个可行的框架。"}}
{"id": "2508.18748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18748", "abs": "https://arxiv.org/abs/2508.18748", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.", "AI": {"tldr": "ChronoRAG是一个针对叙事文本的长上下文问答的RAG框架，通过重构分散信息为连贯段落并明确保持时间顺序，显著提升了叙事问答的性能。", "motivation": "长上下文叙事问答面临挑战，因为正确答案依赖于重建事件时间线并在有限上下文窗口中保持上下文流畅性。现有RAG方法在处理叙事文本时效果有限，因为叙事文本的理解需要更广泛的上下文和段落间的顺序关系，而非孤立的片段。", "method": "提出ChronoRAG，一个专门用于叙事文本的新型RAG框架。该方法专注于两个关键方面：将分散的文档信息提炼成连贯且结构化的段落；通过明确捕获和维护检索到的段落之间的时间顺序来保留叙事流畅性。", "result": "在NarrativeQA数据集上的实验表明，ChronoRAG在需要事实识别和理解复杂序列关系的任务中表现出显著改进，证明了推理时间顺序对于解决叙事问答至关重要。", "conclusion": "在叙事问答中，对时间顺序进行推理对于解决问题至关重要，ChronoRAG通过其独特的方法有效地解决了这一挑战。"}}
{"id": "2508.18693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18693", "abs": "https://arxiv.org/abs/2508.18693", "authors": ["Zhitong Cheng", "Yiran Jiang", "Yulong Ge", "Yufeng Li", "Zhongheng Qin", "Rongzhi Lin", "Jianwei Ma"], "title": "Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency", "comment": null, "summary": "Domain shift, characterized by degraded model performance during transition\nfrom labeled source domains to unlabeled target domains, poses a persistent\nchallenge for deploying deep learning systems. Current unsupervised domain\nadaptation (UDA) methods predominantly rely on fine-tuning feature extractors -\nan approach limited by inefficiency, reduced interpretability, and poor\nscalability to modern architectures.\n  Our analysis reveals that models pretrained on large-scale data exhibit\ndomain-invariant geometric patterns in their feature space, characterized by\nintra-class clustering and inter-class separation, thereby preserving\ntransferable discriminative structures. These findings indicate that domain\nshifts primarily manifest as boundary misalignment rather than feature\ndegradation.\n  Unlike fine-tuning entire pre-trained models - which risks introducing\nunpredictable feature distortions - we propose the Feature-space Planes\nSearcher (FPS): a novel domain adaptation framework that optimizes decision\nboundaries by leveraging these geometric patterns while keeping the feature\nencoder frozen. This streamlined approach enables interpretative analysis of\nadaptation while substantially reducing memory and computational costs through\noffline feature extraction, permitting full-dataset optimization in a single\ncomputation cycle.\n  Evaluations on public benchmarks demonstrate that FPS achieves competitive or\nsuperior performance to state-of-the-art methods. FPS scales efficiently with\nmultimodal large models and shows versatility across diverse domains including\nprotein structure prediction, remote sensing classification, and earthquake\ndetection. We anticipate FPS will provide a simple, effective, and\ngeneralizable paradigm for transfer learning, particularly in domain adaptation\ntasks. .", "AI": {"tldr": "本文提出了一种名为FPS（Feature-space Planes Searcher）的新型无监督域适应（UDA）框架，通过在冻结的特征空间中优化决策边界来解决域偏移问题，利用预训练模型中领域不变的几何模式，从而提高效率、可解释性和可扩展性。", "motivation": "深度学习系统在从有标签源域到无标签目标域的转换过程中，性能下降（即域偏移）是一个持续的挑战。现有的UDA方法主要依赖于微调特征提取器，这种方法存在效率低下、可解释性差以及对现代架构可扩展性不足的问题。", "method": "本文分析发现，在大规模数据上预训练的模型在特征空间中表现出领域不变的几何模式（类内聚类和类间分离），表明域偏移主要表现为边界未对齐而非特征退化。因此，本文提出了FPS框架，通过冻结特征编码器，利用这些几何模式优化决策边界，而不是微调整个预训练模型。该方法通过离线特征提取，大幅减少内存和计算成本，允许在单个计算周期内进行全数据集优化。", "result": "在公共基准测试上的评估表明，FPS实现了与最先进方法相当或更优的性能。FPS能有效地扩展到多模态大型模型，并在蛋白质结构预测、遥感分类和地震检测等不同领域显示出多功能性。", "conclusion": "FPS为迁移学习，特别是在域适应任务中，提供了一种简单、有效且可推广的范式。"}}
{"id": "2508.18905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18905", "abs": "https://arxiv.org/abs/2508.18905", "authors": ["Dimitrios Rontogiannis", "Maxime Peyrard", "Nicolas Baldwin", "Martin Josifoski", "Robert West", "Dimitrios Gunopulos"], "title": "Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks", "comment": null, "summary": "Standard single-turn, static benchmarks fall short in evaluating the nuanced\ncapabilities of Large Language Models (LLMs) on complex tasks such as software\nengineering. In this work, we propose a novel interactive evaluation framework\nthat assesses LLMs on multi-requirement programming tasks through structured,\nfeedback-driven dialogue. Each task is modeled as a requirement dependency\ngraph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides\nminimal, targeted hints to an ``interviewee'' model to help correct errors and\nfulfill target constraints. This dynamic protocol enables fine-grained\ndiagnostic insights into model behavior, uncovering strengths and systematic\nweaknesses that static benchmarks fail to measure. We build on DevAI, a\nbenchmark of 55 curated programming tasks, by adding ground-truth solutions and\nevaluating the relevance and utility of interviewer hints through expert\nannotation. Our results highlight the importance of dynamic evaluation in\nadvancing the development of collaborative code-generating agents.", "AI": {"tldr": "本文提出了一种新颖的交互式评估框架，通过结构化、反馈驱动的对话，评估大型语言模型（LLMs）在多需求编程任务上的表现，以克服传统静态基准的不足。", "motivation": "标准的单轮、静态基准在评估LLMs处理软件工程等复杂任务的细微能力方面存在不足，无法充分衡量模型的真实表现和系统性弱点。", "method": "研究提出了一种交互式评估框架，将任务建模为需求依赖图。一个了解真实解决方案的“面试官”LLM会向“受访者”模型提供最小化、有针对性的提示，以帮助其纠正错误并满足约束。该框架基于DevAI基准（55个编程任务），并增加了真实解决方案，通过专家标注评估了面试官提示的相关性和实用性。", "result": "动态评估协议能够对模型行为提供细致的诊断洞察，揭示了静态基准无法衡量的模型优势和系统性弱点。", "conclusion": "研究结果强调了动态评估在推动协作式代码生成代理发展中的重要性。"}}
{"id": "2508.19199", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19199", "abs": "https://arxiv.org/abs/2508.19199", "authors": ["Alex LaGrassa", "Zixuan Huang", "Dmitry Berenson", "Oliver Kroemer"], "title": "Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation", "comment": "9 pages, 7 figures", "summary": "Efficient planning in high-dimensional spaces, such as those involving\ndeformable objects, requires computationally tractable yet sufficiently\nexpressive dynamics models. This paper introduces a method that automatically\ngenerates task-specific, spatially adaptive dynamics models by learning which\nregions of the object require high-resolution modeling to achieve good task\nperformance for a given planning query. Task performance depends on the complex\ninterplay between the dynamics model, world dynamics, control, and task\nrequirements. Our proposed diffusion-based model generator predicts per-region\nmodel resolutions based on start and goal pointclouds that define the planning\nquery. To efficiently collect the data for learning this mapping, a two-stage\nprocess optimizes resolution using predictive dynamics as a prior before\ndirectly optimizing using closed-loop performance. On a tree-manipulation task,\nour method doubles planning speed with only a small decrease in task\nperformance over using a full-resolution model. This approach informs a path\ntowards using previous planning and control data to generate computationally\nefficient yet sufficiently expressive dynamics models for new tasks.", "AI": {"tldr": "该论文提出了一种自动生成任务特定、空间自适应动态模型的方法，通过学习对象哪些区域需要高分辨率建模来提高高维空间（如可变形对象）规划的效率。", "motivation": "在高维空间（如涉及可变形对象）进行高效规划，需要计算上可行且足够表达的动态模型。", "method": "该方法通过一个基于扩散的模型生成器，根据定义规划查询的起始和目标点云来预测每个区域的模型分辨率。为了高效收集学习数据，采用两阶段过程：首先使用预测动态作为先验优化分辨率，然后直接通过闭环性能进行优化。", "result": "在树木操作任务中，该方法将规划速度提高了一倍，同时与使用全分辨率模型相比，任务性能仅有少量下降。", "conclusion": "该方法为利用先前的规划和控制数据，为新任务生成计算高效且足够表达的动态模型提供了一条途径。"}}
{"id": "2508.18773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18773", "abs": "https://arxiv.org/abs/2508.18773", "authors": ["Qianyu He", "Siyu Yuan", "Xuefeng Li", "Mingxuan Wang", "Jiangjie Chen"], "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "comment": null, "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.", "AI": {"tldr": "ThinkDial是一个端到端开源框架，通过离散操作模式实现了大型语言模型（LLMs）的可控推理，有效平衡了计算成本和性能。", "motivation": "尽管LLMs具有强大的问题解决能力，但其计算开销大，且开源社区缺乏类似OpenAI gpt-oss系列那样具有离散可控推理模式的能力，这限制了其实际部署。", "method": "ThinkDial采用端到端训练范式，将预算模式控制整合到整个流程中。这包括：1) 预算模式监督微调（SFT），将可控推理能力直接嵌入学习过程；2) 两阶段预算感知强化学习，结合自适应奖励塑造。", "result": "ThinkDial成功实现了三种推理模式：高模式（全推理能力），中模式（50% token减少，性能下降<10%），低模式（75% token减少，性能下降<15%）。该框架在保持性能阈值的同时，实现了目标压缩-性能权衡和响应长度的显著缩减，并展现出强大的域外任务泛化能力。", "conclusion": "ThinkDial是首个开源、端到端实现gpt-oss风格可控推理的框架，通过离散操作模式有效管理LLMs的计算开销，同时保持高性能，解决了实际部署中的一个关键挑战。"}}
{"id": "2508.18695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18695", "abs": "https://arxiv.org/abs/2508.18695", "authors": ["Wasi Ullah", "Yasir Noman Khalid", "Saddam Hussain Khan"], "title": "A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition", "comment": "35 pages, 25 figures, 11 tables", "summary": "Human Activity Recognition (HAR) plays a pivotal role in various\napplications, including smart surveillance, healthcare, assistive technologies,\nsports analytics, etc. However, HAR systems still face critical challenges,\nincluding high computational costs, redundant features, and limited scalability\nin real-time scenarios. An optimized hybrid deep learning framework is\nintroduced that integrates a customized InceptionV3, an LSTM architecture, and\na novel ensemble-based feature selection strategy. The proposed framework first\nextracts spatial descriptors using the customized InceptionV3 model, which\ncaptures multilevel contextual patterns, region homogeneity, and fine-grained\nlocalization cues. The temporal dependencies across frames are then modeled\nusing LSTMs to effectively encode motion dynamics. Finally, an ensemble-based\ngenetic algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA)\nis employed to select a compact and optimized feature set by dynamically\nbalancing objectives such as accuracy, redundancy, uniqueness, and complexity\nreduction. Consequently, the selected feature subsets, which are both diverse\nand discriminative, enable various lightweight machine learning classifiers to\nachieve accurate and robust HAR in heterogeneous environments. Experimental\nresults on the robust UCF-YouTube dataset, which presents challenges such as\nocclusion, cluttered backgrounds, motion dynamics, and poor illumination,\ndemonstrate good performance. The proposed approach achieves 99.65% recognition\naccuracy, reduces features to as few as 7, and enhances inference time. The\nlightweight and scalable nature of the HAR system supports real-time deployment\non edge devices such as Raspberry Pi, enabling practical applications in\nintelligent, resource-aware environments, including public safety, assistive\ntechnology, and autonomous monitoring systems.", "AI": {"tldr": "本文提出了一种混合深度学习框架，结合定制版InceptionV3、LSTM和基于集成学习的遗传算法进行特征选择，实现了高精度、低计算成本和实时可扩展的人体活动识别（HAR），适用于边缘设备部署。", "motivation": "当前HAR系统面临计算成本高、特征冗余以及实时场景下可扩展性受限等挑战。", "method": "该框架首先使用定制版InceptionV3提取空间描述符（捕获多级上下文模式、区域同质性和精细定位线索）；接着，利用LSTM建模帧间的时间依赖性（有效编码运动动态）；最后，采用带有自适应动态适应度共享和注意力的（ADFSA）集成遗传算法，通过动态平衡准确性、冗余性、独特性和复杂性降低等目标，选择紧凑且优化的特征集。最终，选定的特征子集用于轻量级机器学习分类器进行HAR。", "result": "在UCF-YouTube数据集上，该方法实现了99.65%的识别准确率，将特征数量减少至7个，并缩短了推理时间。其轻量级和可扩展性使其能够实时部署在树莓派等边缘设备上。", "conclusion": "所提出的HAR系统准确、鲁棒、轻量且可扩展，支持在公共安全、辅助技术和自主监控系统等智能、资源受限环境中进行实时部署和实际应用。"}}
{"id": "2508.18914", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18914", "abs": "https://arxiv.org/abs/2508.18914", "authors": ["Yanxing Huang", "Xinling Jin", "Sijie Liang", "Peng Li", "Yang Liu"], "title": "FormaRL: Enhancing Autoformalization with no Labeled Data", "comment": "Conference paper at COLM2025", "summary": "Autoformalization is one of the central tasks in formal verification, while\nits advancement remains hindered due to the data scarcity and the absence\nefficient methods. In this work we propose \\textbf{FormaRL}, a simple yet\nefficient reinforcement learning framework for autoformalization which only\nrequires a small amount of unlabeled data. FormaRL integrates syntax check from\nLean compiler and consistency check from large language model to calculate the\nreward, and adopts GRPO algorithm to update the formalizer. We also curated a\nproof problem dataset from undergraduate-level math materials, named\n\\textbf{uproof}, in the hope to facilitate the exploration of autoformalization\nand theorem proving in advanced math. Experiments show that FormaRL can\nincrease the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by\n4 $\\sim$ 6x (4.04\\% $\\to$ 26.15\\% on ProofNet and 2.4\\% $\\to$ 9.6\\% on uproof)\nwith merely 859 unlabeled data. And on uproof our method also achieved a strong\nimprovement in out-of-distribution performance compared to existing open-source\nstate-of-the-art autoformalizers on both pass@1 accuracy (6.2\\% $\\to$ 9.6\\%)\nand pass@16 accuracy (24.4\\% $\\to$ 33.6\\%). Training code of FormaRL is\nopen-sourced at https://github.com/THUNLP-MT/FormaRL.", "AI": {"tldr": "本文提出了FormaRL，一个基于强化学习的自动形式化框架，仅需少量未标注数据即可显著提升自动形式化准确率。同时，还发布了用于本科数学证明问题的新数据集uproof。", "motivation": "自动形式化是形式验证的核心任务之一，但其发展受限于数据稀缺和缺乏高效方法。", "method": "FormaRL框架整合了Lean编译器的语法检查和大型语言模型的一致性检查来计算奖励，并采用GRPO算法更新形式化器。此外，还整理了一个名为uproof的本科级别数学证明问题数据集。", "result": "FormaRL仅使用859个未标注数据，将Qwen2.5-Coder-7B-Instruct的pass@1自动形式化准确率在ProofNet上提高了4~6倍（4.04%提升至26.15%），在uproof上提高了2.4%至9.6%。在uproof上，该方法在pass@1和pass@16准确率方面，相比现有开源最先进的自动形式化器，在分布外（OOD）性能上也有显著提升（pass@1：6.2%提升至9.6%；pass@16：24.4%提升至33.6%）。", "conclusion": "FormaRL是一个简单而高效的强化学习框架，能够在仅使用少量未标注数据的情况下显著提升自动形式化性能，并在分布外场景下表现出色。新发布的uproof数据集有望促进高级数学中自动形式化和定理证明的探索。"}}
{"id": "2508.19236", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19236", "abs": "https://arxiv.org/abs/2508.19236", "authors": ["Hao Shi", "Bin Xie", "Yingfei Liu", "Lin Sun", "Fengrong Liu", "Tiancai Wang", "Erjin Zhou", "Haoqiang Fan", "Xiangyu Zhang", "Gao Huang"], "title": "MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation", "comment": "The project is available at https://shihao1895.github.io/MemoryVLA", "summary": "Temporal context is essential for robotic manipulation because such tasks are\ninherently non-Markovian, yet mainstream VLA models typically overlook it and\nstruggle with long-horizon, temporally dependent tasks. Cognitive science\nsuggests that humans rely on working memory to buffer short-lived\nrepresentations for immediate control, while the hippocampal system preserves\nverbatim episodic details and semantic gist of past experience for long-term\nmemory. Inspired by these mechanisms, we propose MemoryVLA, a\nCognition-Memory-Action framework for long-horizon robotic manipulation. A\npretrained VLM encodes the observation into perceptual and cognitive tokens\nthat form working memory, while a Perceptual-Cognitive Memory Bank stores\nlow-level details and high-level semantics consolidated from it. Working memory\nretrieves decision-relevant entries from the bank, adaptively fuses them with\ncurrent tokens, and updates the bank by merging redundancies. Using these\ntokens, a memory-conditioned diffusion action expert yields temporally aware\naction sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks\nacross three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it\nachieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming\nstate-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on\nBridge. On 12 real-world tasks spanning general skills and long-horizon\ntemporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon\ntasks showing a +26 improvement over state-of-the-art baseline. Project Page:\nhttps://shihao1895.github.io/MemoryVLA", "AI": {"tldr": "MemoryVLA是一种受人类记忆启发的新型认知-记忆-行动框架，旨在解决机器人操作中长期、时序依赖任务的挑战。它通过工作记忆和感知-认知记忆库处理时间上下文，在模拟和真实世界任务中显著优于现有基线。", "motivation": "主流的VLA模型通常忽略时间上下文，难以处理长周期、非马尔可夫式的时序依赖任务。而认知科学表明，人类依赖工作记忆进行即时控制，并利用海马系统储存情景细节和语义要点以形成长期记忆，这些机制对处理复杂任务至关重要。", "method": "MemoryVLA框架包括：1) 预训练的VLM将观测编码为感知和认知令牌，形成工作记忆。2) 一个感知-认知记忆库存储从工作记忆中整合的低级细节和高级语义。3) 工作记忆从记忆库中检索与决策相关的信息，与当前令牌自适应融合，并通过合并冗余更新记忆库。4) 一个记忆条件扩散动作专家利用这些令牌生成具有时间意识的动作序列。", "result": "MemoryVLA在150多个模拟和真实世界任务中进行了评估。在SimplerEnv-Bridge、Fractal和LIBERO-5套件上，分别达到了71.9%、72.7%和96.5%的成功率，均优于现有最先进的基线CogACT和pi-0，其中在Bridge任务上提升了14.6%。在12个涵盖通用技能和长期时序依赖的真实世界任务中，MemoryVLA实现了84.0%的成功率，在长期任务上比最先进的基线提高了26%。", "conclusion": "MemoryVLA通过引入受人类记忆机制启发的认知-记忆-行动框架，成功解决了机器人操作中处理时间上下文和长周期、时序依赖任务的挑战。其在多项模拟和真实世界任务上的优异表现，证明了该方法在提升机器人操作能力方面的有效性和潜力。"}}
{"id": "2508.18780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18780", "abs": "https://arxiv.org/abs/2508.18780", "authors": ["Yilin Li", "Xunjian Yin", "Yilin Chen", "Xiaojun Wan"], "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction", "comment": "Code will be released upon publication", "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.", "AI": {"tldr": "本文提出了一种基于规则的强化学习（Rule-Based RL）框架，用于指导大型语言模型（LLMs）进行语法错误纠正（GEC），并在中文数据集上实现了最先进的性能，显著提高了召回率。", "motivation": "传统的编码器-解码器模型在GEC任务上取得了一定成功，但LLMs在该领域的应用仍未被充分探索。当前研究主要依赖于有监督微调让LLMs直接生成纠正后的句子，这限制了模型强大的推理能力。", "method": "提出了一种基于规则的强化学习（Rule-Based RL）新框架，用于引导LLMs执行GEC任务。", "result": "在中文数据集上，所提出的Rule-Based RL框架实现了最先进的性能（state-of-the-art），并显著提高了召回率。", "conclusion": "该研究结果清晰地强调了使用强化学习来引导LLMs的优势，为未来GEC领域的发展提供了一种更可控、更可靠的范式。"}}
{"id": "2508.18696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18696", "abs": "https://arxiv.org/abs/2508.18696", "authors": ["Qun Ji", "Peng Li", "Mingqiang Wei"], "title": "ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting", "comment": null, "summary": "High-fidelity reconstruction of deformable tissues from endoscopic videos\nremains challenging due to the limitations of existing methods in capturing\nsubtle color variations and modeling global deformations. While 3D Gaussian\nSplatting (3DGS) enables efficient dynamic reconstruction, its fixed\nper-Gaussian color assignment struggles with intricate textures, and linear\ndeformation modeling fails to model consistent global deformation. To address\nthese issues, we propose ColorGS, a novel framework that integrates spatially\nadaptive color encoding and enhanced deformation modeling for surgical scene\nreconstruction. First, we introduce Colored Gaussian Primitives, which employ\ndynamic anchors with learnable color parameters to adaptively encode spatially\nvarying textures, significantly improving color expressiveness under complex\nlighting and tissue similarity. Second, we design an Enhanced Deformation Model\n(EDM) that combines time-aware Gaussian basis functions with learnable\ntime-independent deformations, enabling precise capture of both localized\ntissue deformations and global motion consistency caused by surgical\ninteractions. Extensive experiments on DaVinci robotic surgery videos and\nbenchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves\nstate-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior\n3DGS-based methods) and superior SSIM (97.25\\%) while maintaining real-time\nrendering efficiency. Our work advances surgical scene reconstruction by\nbalancing high fidelity with computational practicality, critical for\nintraoperative guidance and AR/VR applications.", "AI": {"tldr": "ColorGS是一种新颖的框架，通过引入彩色高斯基元和增强变形模型，显著提高了内窥镜视频中可变形组织的高保真重建质量，同时保持了实时渲染效率。", "motivation": "现有方法（特别是3D高斯泼溅）在捕获细微颜色变化和建模全局变形方面存在局限性。固定的每高斯颜色分配难以处理复杂纹理，而线性变形建模无法实现一致的全局变形，这在内窥镜视频中重建可变形组织时是一个挑战。", "method": "ColorGS框架包含两部分：1. **彩色高斯基元**：利用带有可学习颜色参数的动态锚点，自适应编码空间变化的纹理，提高复杂光照下的颜色表现力。2. **增强变形模型（EDM）**：结合时间感知高斯基函数和可学习的时间无关变形，精确捕捉局部组织变形和手术交互引起的全局运动一致性。", "result": "在达芬奇机器人手术视频和基准数据集（EndoNeRF, StereoMIS）上的实验表明，ColorGS实现了最先进的性能，PSNR达到39.85（比之前基于3DGS的方法高1.5），SSIM达到97.25%，同时保持了实时渲染效率。", "conclusion": "ColorGS通过平衡高保真度与计算实用性，推动了手术场景重建技术的发展，这对于术中指导和AR/VR应用至关重要。"}}
{"id": "2508.18925", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18925", "abs": "https://arxiv.org/abs/2508.18925", "authors": ["Qian Xiao", "Conn Breathnach", "Ioana Ghergulescu", "Conor O'Sullivan", "Keith Johnston", "Vincent Wade"], "title": "Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems", "comment": null, "summary": "The surge in the adoption of Intelligent Tutoring Systems (ITSs) in\neducation, while being integral to curriculum-based learning, can inadvertently\nexacerbate performance gaps. To address this problem, student profiling becomes\ncrucial for tracking progress, identifying struggling students, and alleviating\ndisparities among students. Such profiling requires measuring student behaviors\nand performance across different aspects, such as content coverage, learning\nintensity, and proficiency in different concepts within a learning topic.\n  In this study, we introduce CTGraph, a graph-level representation learning\napproach to profile learner behaviors and performance in a self-supervised\nmanner. Our experiments demonstrate that CTGraph can provide a holistic view of\nstudent learning journeys, accounting for different aspects of student\nbehaviors and performance, as well as variations in their learning paths as\naligned to the curriculum structure. We also show that our approach can\nidentify struggling students and provide comparative analysis of diverse groups\nto pinpoint when and where students are struggling. As such, our approach opens\nmore opportunities to empower educators with rich insights into student\nlearning journeys and paves the way for more targeted interventions.", "AI": {"tldr": "本研究提出了一种名为CTGraph的图级表示学习方法，以自监督方式对智能辅导系统（ITSs）中的学生行为和表现进行画像，旨在识别学习困难学生并缩小学习差距。", "motivation": "智能辅导系统（ITSs）的普及可能无意中加剧学生的表现差距。因此，通过跟踪学习进度、识别困难学生和缓解学生间差异，学生画像变得至关重要。这需要衡量学生在内容覆盖、学习强度和概念掌握等方面的行为和表现。", "method": "本研究引入了CTGraph，这是一种图级表示学习方法，用于以自监督的方式对学生的行为和表现进行画像。该方法能够综合考虑学生行为、表现的各个方面以及学习路径的变化，并与课程结构对齐。", "result": "实验证明，CTGraph能提供学生学习历程的整体视图，有效识别学习困难学生，并能对不同学生群体进行比较分析，从而精确指出学生在何时何地遇到困难。", "conclusion": "CTGraph为教育工作者提供了深入了解学生学习历程的丰富洞察力，为更具针对性的干预措施铺平了道路，从而赋能教育者。"}}
{"id": "2508.18729", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18729", "abs": "https://arxiv.org/abs/2508.18729", "authors": ["Melanie Wille", "Tobias Fischer", "Scarlett Raine"], "title": "Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection", "comment": "10 pages", "summary": "Underwater object detection is critical for monitoring marine ecosystems but\nposes unique challenges, including degraded image quality, imbalanced class\ndistribution, and distinct visual characteristics. Not every species is\ndetected equally well, yet underlying causes remain unclear. We address two key\nresearch questions: 1) What factors beyond data quantity drive class-specific\nperformance disparities? 2) How can we systematically improve detection of\nunder-performing marine species? We manipulate the DUO dataset to separate the\nobject detection task into localization and classification and investigate the\nunder-performance of the scallop class. Localization analysis using YOLO11 and\nTIDE finds that foreground-background discrimination is the most problematic\nstage regardless of data quantity. Classification experiments reveal persistent\nprecision gaps even with balanced data, indicating intrinsic feature-based\nchallenges beyond data scarcity and inter-class dependencies. We recommend\nimbalanced distributions when prioritizing precision, and balanced\ndistributions when prioritizing recall. Improving under-performing classes\nshould focus on algorithmic advances, especially within localization modules.\nWe publicly release our code and datasets.", "AI": {"tldr": "该研究分析了水下目标检测中类别特异性性能差异的原因，发现前景-背景判别是主要定位问题，且分类存在固有的特征挑战。建议根据优先级选择数据分布，并通过算法改进定位模块以提升性能。", "motivation": "水下目标检测对海洋生态系统监测至关重要，但面临图像质量差、类别分布不平衡和视觉特征独特等挑战。不同物种的检测效果差异大，但深层原因尚不清楚，尤其是在数据量之外的因素。", "method": "研究通过操纵DUO数据集，将目标检测任务分解为定位和分类两部分，并以扇贝类为例进行深入分析。定位分析使用了YOLO11和TIDE工具，分类实验则探究了数据平衡后的精度差距。", "result": "定位分析发现，无论数据量多少，前景-背景判别都是最主要的问题阶段。分类实验表明，即使数据平衡，精度差距依然存在，这表明除了数据稀缺和类间依赖性外，还存在固有的基于特征的挑战。研究建议在优先考虑精度时使用不平衡分布，在优先考虑召回率时使用平衡分布。", "conclusion": "水下目标检测中表现不佳的类别，其性能提升应重点关注算法改进，尤其是在定位模块中。研究强调了前景-背景判别和内在特征挑战是主要的性能瓶颈。"}}
{"id": "2508.18783", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18783", "abs": "https://arxiv.org/abs/2508.18783", "authors": ["Igor Shalyminov", "Hang Su", "Jake Vincent", "Siffi Singh", "Jason Cai", "James Gung", "Raphael Shu", "Saab Mansour"], "title": "Controllable Conversational Theme Detection Track at DSTC 12", "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection", "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.", "AI": {"tldr": "本文介绍了对话分析中的“可控对话主题检测”任务，旨在自动识别对话主题并提供用户友好的总结。该任务被设计为DSTC 12的一项公开竞赛，结合了聚类和主题标注，并允许通过用户偏好数据控制主题粒度。", "motivation": "随着语音和自然语言处理技术（特别是大型语言模型）的进步，对话分析的自动化达到了新的复杂度和规模。现有的意图检测往往依赖固定意图集，而对话主题检测旨在直接为用户提供对话核心查询的灵活、用户导向的总结，以显著减少人工分析大量对话（如客户支持或销售）的努力。", "method": "本文将“可控对话主题检测”问题定义为DSTC 12的一项竞赛任务，其方法包括对话语篇的联合聚类和主题标注。一个独特之处是，通过提供的用户偏好数据，可以实现对生成主题簇粒度的可控性。论文还概述了问题、相关数据集以及自动和人工评估指标。", "result": "论文讨论了参赛队伍的提交结果，并从中提供了见解。相关的竞赛材料（数据和代码）已在GitHub仓库中公开。", "conclusion": "本文引入了一个对话分析中的关键任务——可控对话主题检测，并将其作为DSTC 12的竞赛赛道。该任务通过结合聚类和主题标注，并允许粒度控制，旨在提供更灵活、用户导向的对话总结，从而有效减少人工分析工作。竞赛结果和相关材料的公开将促进该领域的研究和发展。"}}
{"id": "2508.18723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18723", "abs": "https://arxiv.org/abs/2508.18723", "authors": ["Hiroaki Aizawa", "Yuta Naito", "Kohei Fukuda"], "title": "Class-wise Flooding Regularization for Imbalanced Image Classification", "comment": "Accepted to ACPR2025", "summary": "The purpose of training neural networks is to achieve high generalization\nperformance on unseen inputs. However, when trained on imbalanced datasets, a\nmodel's prediction tends to favor majority classes over minority classes,\nleading to significant degradation in the recognition performance of minority\nclasses. To address this issue, we propose class-wise flooding regularization,\nan extension of flooding regularization applied at the class level. Flooding is\na regularization technique that mitigates overfitting by preventing the\ntraining loss from falling below a predefined threshold, known as the flooding\nlevel, thereby discouraging memorization. Our proposed method assigns a\nclass-specific flooding level based on class frequencies. By doing so, it\nsuppresses overfitting in majority classes while allowing sufficient learning\nfor minority classes. We validate our approach on imbalanced image\nclassification. Compared to conventional flooding regularizations, our method\nimproves the classification performance of minority classes and achieves better\noverall generalization.", "AI": {"tldr": "本文提出了一种类级别泛洪正则化（class-wise flooding regularization）方法，通过根据类别频率设置不同的泛洪级别，解决不平衡数据集导致的少数类识别性能下降问题，从而提高少数类的分类性能和整体泛化能力。", "motivation": "在不平衡数据集上训练神经网络时，模型倾向于偏向多数类，导致少数类的识别性能显著下降。现有正则化方法可能无法有效解决此问题。", "method": "本文提出类级别泛洪正则化，它是传统泛洪正则化在类别层面的扩展。该方法根据类别频率为每个类别分配一个特定的泛洪级别。通过这种方式，它能抑制多数类的过拟合，同时允许少数类进行充分学习。", "result": "与传统泛洪正则化相比，本文提出的方法在不平衡图像分类任务中提高了少数类的分类性能，并实现了更好的整体泛化能力。", "conclusion": "类级别泛洪正则化通过动态调整泛洪级别来适应不同类别的学习需求，有效解决了不平衡数据集中少数类性能下降的问题，从而提升了模型的整体表现和泛化能力。"}}
{"id": "2508.18933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18933", "abs": "https://arxiv.org/abs/2508.18933", "authors": ["David Egea", "Barproda Halder", "Sanghamitra Dutta"], "title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation", "comment": null, "summary": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis.", "AI": {"tldr": "本研究提出VISION框架，通过使用大型语言模型（LLM）生成反事实数据来训练图神经网络（GNN），从而缓解虚假关联、提高代码漏洞检测的鲁棒性和可解释性。", "motivation": "源代码漏洞的自动化检测是网络安全的关键挑战。尽管图神经网络（GNNs）在学习代码结构和逻辑关系方面表现出潜力，但其性能受限于训练数据不平衡和标签噪声，导致GNNs学习到表层代码相似性中的“虚假”关联，泛化能力差。", "method": "本研究提出了一个名为VISION的统一框架，用于鲁棒和可解释的漏洞检测。该框架包括：(i) 通过LLM生成反事实样本（语义修改最小但标签相反的样本）；(ii) 对成对的、标签相反的代码示例进行有针对性的GNN训练；(iii) 基于图的可解释性分析，识别与漏洞预测相关的关键代码语句，并忽略虚假关联。", "result": "VISION框架显著减少了虚假学习，实现了更鲁棒、更具泛化性的检测。在CWE-20漏洞检测任务上，整体准确率从51.8%提高到97.8%，成对对比准确率从4.5%提高到95.8%，最差组准确率从0.7%提高到85.5%。研究还通过提出的类内归因方差、类间归因距离和节点分数依赖等指标展示了其优势。此外，还发布了包含27,556个函数（真实和反事实）的CWE-20-CFA基准数据集。", "conclusion": "VISION框架通过系统地增强反事实训练数据集，有效缓解了GNN在漏洞检测中学习虚假关联的问题，显著提高了检测的准确性、鲁棒性和泛化能力。它还通过交互式可视化促进了透明和值得信赖的AI网络安全系统，并发布了新的基准数据集。"}}
{"id": "2508.18788", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18788", "abs": "https://arxiv.org/abs/2508.18788", "authors": ["Christian Löwens", "Thorben Funke", "Jingchao Xie", "Alexandru Paul Condurache"], "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps", "comment": "Accepted at ICCV 2025", "summary": "Online mapping models show remarkable results in predicting vectorized maps\nfrom multi-view camera images only. However, all existing approaches still rely\non ground-truth high-definition maps during training, which are expensive to\nobtain and often not geographically diverse enough for reliable generalization.\nIn this work, we propose PseudoMapTrainer, a novel approach to online mapping\nthat uses pseudo-labels generated from unlabeled sensor data. We derive those\npseudo-labels by reconstructing the road surface from multi-camera imagery\nusing Gaussian splatting and semantics of a pre-trained 2D segmentation\nnetwork. In addition, we introduce a mask-aware assignment algorithm and loss\nfunction to handle partially masked pseudo-labels, allowing for the first time\nthe training of online mapping models without any ground-truth maps.\nFurthermore, our pseudo-labels can be effectively used to pre-train an online\nmodel in a semi-supervised manner to leverage large-scale unlabeled\ncrowdsourced data. The code is available at\ngithub.com/boschresearch/PseudoMapTrainer.", "AI": {"tldr": "本文提出PseudoMapTrainer，一种新的在线地图模型训练方法，它利用从无标签传感器数据生成的伪标签，首次实现在无需任何地面真值高清地图的情况下训练在线地图模型，并可用于半监督预训练。", "motivation": "现有在线地图模型在训练时严重依赖昂贵且地理多样性不足的地面真值高清地图，这限制了模型的泛化能力和数据获取成本。", "method": "PseudoMapTrainer通过多视角相机图像，结合高斯泼溅(Gaussian splatting)和预训练的2D分割网络的语义信息，重建路面以生成伪标签。此外，该方法引入了一种掩码感知分配算法和损失函数来处理部分被遮挡的伪标签。这些伪标签还可用于大规模无标签众包数据的半监督预训练。", "result": "该方法首次实现了无需任何地面真值地图即可训练在线地图模型。所生成的伪标签还能有效用于以半监督方式预训练在线模型，以利用大规模无标签众包数据。", "conclusion": "PseudoMapTrainer为在线地图模型提供了一种无需地面真值地图的训练范式，显著降低了训练成本，并通过利用无标签数据提高了模型的可扩展性和泛化潜力。"}}
{"id": "2508.18791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18791", "abs": "https://arxiv.org/abs/2508.18791", "authors": ["Ziming Zhu", "Chenglong Wang", "Shunjie Xing", "Yifu Huo", "Fengning Tian", "Quan Du", "Di Yang", "Chunliang Zhang", "Tong Xiao", "Jingbo Zhu"], "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination", "comment": null, "summary": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents.", "AI": {"tldr": "本文介绍了LaTeXTrans，一个协同多智能体系统，旨在解决LaTeX格式文档的翻译挑战，确保格式、结构和术语的准确性。", "motivation": "尽管现代机器翻译系统在通用领域文本上取得了显著进展，但翻译结构化的LaTeX格式文档仍然是一个重大挑战。这类文档混合了自然语言和领域特定语法（如数学方程、表格、图表、交叉引用），这些都必须准确保留以维护语义完整性和可编译性。", "method": "LaTeXTrans是一个协同多智能体系统，通过六个专业智能体实现格式保留、结构保真和术语一致性。这些智能体包括：1) 一个解析器，通过占位符替换和语法过滤将LaTeX分解为易于翻译的单元；2) 一个翻译器、验证器、摘要器和术语提取器，它们协同工作以确保上下文感知、自我纠正和术语一致的翻译；3) 一个生成器，将翻译后的内容重构为结构良好的LaTeX文档。", "result": "实验结果表明，LaTeXTrans在翻译准确性和结构保真度方面均优于主流机器翻译系统。", "conclusion": "LaTeXTrans为翻译LaTeX格式文档提供了一个有效且实用的解决方案。"}}
{"id": "2508.18726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18726", "abs": "https://arxiv.org/abs/2508.18726", "authors": ["Hiroaki Aizawa", "Yoshikazu Hayashi"], "title": "Flatness-aware Curriculum Learning via Adversarial Difficulty", "comment": "Accepted to BMVC2025", "summary": "Neural networks trained by empirical risk minimization often suffer from\noverfitting, especially to specific samples or domains, which leads to poor\ngeneralization. Curriculum Learning (CL) addresses this issue by selecting\ntraining samples based on the difficulty. From the optimization perspective,\nmethods such as Sharpness-Aware Minimization (SAM) improve robustness and\ngeneralization by seeking flat minima. However, combining CL with SAM is not\nstraightforward. In flat regions, both the loss values and the gradient norms\ntend to become uniformly small, which makes it difficult to evaluate sample\ndifficulty and design an effective curriculum. To overcome this problem, we\npropose the Adversarial Difficulty Measure (ADM), which quantifies adversarial\nvulnerability by leveraging the robustness properties of models trained toward\nflat minima. Unlike loss- or gradient-based measures, which become ineffective\nas training progresses into flatter regions, ADM remains informative by\nmeasuring the normalized loss gap between original and adversarial examples. We\nincorporate ADM into CL-based training with SAM to dynamically assess sample\ndifficulty. We evaluated our approach on image classification tasks,\nfine-grained recognition, and domain generalization. The results demonstrate\nthat our method preserves the strengths of both CL and SAM while outperforming\nexisting curriculum-based and flatness-aware training strategies.", "AI": {"tldr": "针对神经网络过拟合问题，本文提出了一种结合课程学习（CL）和锐度感知最小化（SAM）的新方法。通过引入对抗性难度度量（ADM）来量化对抗性脆弱性，解决了在平坦区域难以评估样本难度的问题，并在多项任务中实现了优于现有方法的泛化性能。", "motivation": "神经网络在经验风险最小化训练中常出现过拟合，导致泛化能力差。课程学习（CL）和锐度感知最小化（SAM）能改善泛化，但将两者结合存在挑战，因为在平坦区域，损失值和梯度范数趋于一致小，使得样本难度难以评估，从而影响有效课程的设计。", "method": "本文提出对抗性难度度量（Adversarial Difficulty Measure, ADM），利用模型在平坦最小值处的鲁棒性来量化对抗性脆弱性。与基于损失或梯度的度量不同，ADM通过衡量原始样本和对抗性样本之间的归一化损失差距来保持信息量。我们将ADM整合到基于CL的SAM训练中，以动态评估样本难度。", "result": "该方法在图像分类、细粒度识别和域泛化任务上进行了评估。结果表明，我们的方法保留了CL和SAM的优点，并优于现有的基于课程和平面感知的训练策略。", "conclusion": "通过引入对抗性难度度量（ADM），我们成功解决了在平坦最小值区域中评估样本难度的问题，有效结合了课程学习（CL）和锐度感知最小化（SAM），显著提升了模型在多种任务上的泛化能力和鲁棒性。"}}
{"id": "2508.18953", "categories": ["cs.AI", "I.2.6; I.2.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.18953", "abs": "https://arxiv.org/abs/2508.18953", "authors": ["I. I. Priezzhev", "D. A. Danko", "A. V. Shubin"], "title": "Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method", "comment": "18 pages, 6 figures. Novel hierarchical neural networks based on\n  k-nearest neighbors method for addressing hallucination effects, training\n  complexity, and catastrophic forgetting in modern AI systems. Includes\n  mathematical formulations using Kohonen self-organizing maps and experimental\n  validation on MNIST handwritten digit recognition and machine translation\n  tasks", "summary": "Modern neural network technologies, including large language models, have\nachieved remarkable success in various applied artificial intelligence\napplications, however, they face a range of fundamental limitations. Among them\nare hallucination effects, high computational complexity of training and\ninference, costly fine-tuning, and catastrophic forgetting issues. These\nlimitations significantly hinder the use of neural networks in critical areas\nsuch as medicine, industrial process management, and scientific research. This\narticle proposes an alternative approach based on the nearest neighbors method\nwith hierarchical clustering structures. Employing the k-nearest neighbors\nalgorithm significantly reduces or completely eliminates hallucination effects\nwhile simplifying model expansion and fine-tuning without the need for\nretraining the entire network. To overcome the high computational load of the\nk-nearest neighbors method, the paper proposes using tree-like data structures\nbased on Kohonen self-organizing maps, thereby greatly accelerating nearest\nneighbor searches. Tests conducted on handwritten digit recognition and simple\nsubtitle translation tasks confirmed the effectiveness of the proposed\napproach. With only a slight reduction in accuracy, the nearest neighbor search\ntime was reduced hundreds of times compared to exhaustive search methods. The\nproposed method features transparency and interpretability, closely aligns with\nhuman cognitive mechanisms, and demonstrates potential for extensive use in\ntasks requiring high reliability and explainable results.", "AI": {"tldr": "本文提出了一种基于分层聚类结构的最近邻方法，以解决现代神经网络（包括大型语言模型）的幻觉、高计算复杂度、微调成本高和灾难性遗忘等局限性，并实现了显著的计算效率提升和可解释性。", "motivation": "现代神经网络技术（如大型语言模型）在应用中面临幻觉效应、高计算复杂性、昂贵的微调和灾难性遗忘等根本性局限，这些问题严重阻碍了它们在医疗、工业过程管理和科学研究等关键领域的应用。", "method": "该方法基于k-最近邻算法，以减少或消除幻觉效应并简化模型扩展和微调。为克服k-最近邻方法的高计算负荷，论文提出使用基于Kohonen自组织映射的树状数据结构来加速最近邻搜索。", "result": "在手写数字识别和简单字幕翻译任务上的测试表明，该方法有效。与穷举搜索方法相比，最近邻搜索时间减少了数百倍，而准确性仅略有下降。", "conclusion": "所提出的方法具有透明性和可解释性，与人类认知机制紧密结合，在需要高可靠性和可解释结果的任务中具有广泛应用的潜力。"}}
{"id": "2508.18898", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18898", "abs": "https://arxiv.org/abs/2508.18898", "authors": ["Mona Mirzaie", "Bodo Rosenhahn"], "title": "Interpretable Decision-Making for End-to-End Autonomous Driving", "comment": "Accepted to the ICCV 2025 2nd Workshop on the Challenge Of\n  Out-of-Label Hazards in Autonomous Driving (2COOOL)", "summary": "Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.\nAlthough end-to-end approaches derive control commands directly from raw data,\ninterpreting these decisions remains challenging, especially in complex urban\nscenarios. This is mainly attributed to very deep neural networks with\nnon-linear decision boundaries, making it challenging to grasp the logic behind\nAI-driven decisions. This paper presents a method to enhance interpretability\nwhile optimizing control commands in autonomous driving. To address this, we\npropose loss functions that promote the interpretability of our model by\ngenerating sparse and localized feature maps. The feature activations allow us\nto explain which image regions contribute to the predicted control command. We\nconduct comprehensive ablation studies on the feature extraction step and\nvalidate our method on the CARLA benchmarks. We also demonstrate that our\napproach improves interpretability, which correlates with reducing infractions,\nyielding a safer, high-performance driving model. Notably, our monocular,\nnon-ensemble model surpasses the top-performing approaches from the CARLA\nLeaderboard by achieving lower infraction scores and the highest route\ncompletion rate, all while ensuring interpretability.", "AI": {"tldr": "本文提出了一种通过优化损失函数生成稀疏局部特征图的方法，以提高自动驾驶控制模型的解释性，同时提升其性能和安全性，并在CARLA基准测试中取得了优异成绩。", "motivation": "自动驾驶车辆的广泛部署需要可信赖的AI，但现有的端到端方法（特别是基于深度神经网络的）在复杂城市场景中难以解释其决策逻辑，这阻碍了AI驱动决策的理解和信任。", "method": "研究人员提出了一种方法，通过设计特定的损失函数来促进模型的可解释性，这些损失函数旨在生成稀疏且局部化的特征图。这些特征激活允许解释图像的哪些区域对预测的控制指令做出了贡献。该方法在特征提取步骤上进行了全面的消融研究，并在CARLA基准测试上进行了验证。", "result": "该方法不仅提高了模型的解释性，还与减少违规行为相关联，从而产生了更安全、高性能的驾驶模型。值得注意的是，其单目、非集成模型在违规分数和路线完成率方面超越了CARLA排行榜上表现最佳的方法，同时确保了可解释性。", "conclusion": "该研究成功开发了一种既能保证决策可解释性，又能实现高性能和高安全性的自动驾驶控制模型，为自动驾驶AI的广泛部署奠定了基础。"}}
{"id": "2508.18819", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18819", "abs": "https://arxiv.org/abs/2508.18819", "authors": ["Shubham Gupta", "Shraban Kumar Chatterjee", "Suman Kundu"], "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection", "comment": null, "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.", "AI": {"tldr": "本文提出了一种新颖的自监督虚假信息检测框架，结合了抽象意义表示（AMR）的复杂语义关系和新闻传播动态。该框架利用基于大型语言模型（LLM）的图对比损失（LGCL）进行零样本学习，并通过多视图图掩码自编码器整合社交上下文，实现了卓越的性能和泛化能力。", "motivation": "数字时代虚假信息的泛滥带来了严重的社会挑战。现有方法在捕获长距离依赖、复杂语义关系和新闻传播的社会动态方面存在困难，并且需要大量的标注数据集，导致部署资源密集。", "method": "研究提出一个自监督虚假信息检测框架：1. 利用抽象意义表示（AMR）捕捉复杂语义关系。2. 引入基于LLM的图对比损失（LGCL），通过LLM生成的负锚点增强特征可分离性，实现零样本学习。3. 采用多视图图掩码自编码器从社交上下文图中学习新闻传播特征。4. 结合语义和传播特征来区分虚假和真实新闻。", "result": "实验结果表明，所提出的自监督框架即使在有限的标注数据集下，也比其他最先进的方法表现出优越的性能，并提高了泛化能力。", "conclusion": "该自监督框架通过整合复杂语义关系和新闻传播动态，并利用LLM增强的对比学习，能够有效地区分虚假新闻和真实新闻，展现出卓越的性能和更好的泛化能力，尤其适用于标注数据有限的场景。"}}
{"id": "2508.18733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18733", "abs": "https://arxiv.org/abs/2508.18733", "authors": ["Feiwei Qin", "Shichao Lu", "Junhao Hou", "Changmiao Wang", "Meie Fang", "Ligang Liu"], "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings", "comment": "Accepted to ACM MM 2025", "summary": "Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.", "AI": {"tldr": "该研究提出Drawing2CAD框架，通过将CAD生成重构为序列到序列学习问题，实现从2D工程图自动生成参数化CAD模型，弥合了现有方法与传统工业工作流程之间的差距。", "motivation": "现有的CAD生成方法（如从点云、网格、文本生成）与从2D工程图开始的传统工业工作流程不符。从2D矢量图自动生成参数化CAD模型是一个关键但未充分探索的工程设计步骤，存在空白。", "method": "将CAD生成重新定义为序列到序列学习问题，直接将矢量图元转换为参数化CAD操作。提出Drawing2CAD框架，包含三个关键技术组件：1) 保留精确几何信息的网络友好型矢量图元表示；2) 解耦命令类型和参数生成但保持精确对应关系的双解码器Transformer架构；3) 适应CAD参数固有灵活性的软目标分布损失函数。为训练和评估，创建了CAD-VGDrawing数据集。", "result": "通过彻底的实验，证明了所提出Drawing2CAD方法的有效性。", "conclusion": "Drawing2CAD框架能够有效地从2D矢量工程图自动生成参数化CAD模型，在转换过程中保留了几何精度和设计意图，填补了现有方法与传统工业工作流程之间的空白。"}}
{"id": "2508.18983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18983", "abs": "https://arxiv.org/abs/2508.18983", "authors": ["Guoying Zhu", "Meng Li", "Haipeng Dai", "Xuechen Liu", "Weijun Wang", "Keran Li", "Jun xiao", "Ligeng Chen", "Wei Wang"], "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling", "comment": null, "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.", "AI": {"tldr": "本文提出了一种针对消费级边缘硬件上MoE模型部署的动态专家卸载和调度方法。通过利用专家重要性进行替换和优化调度，该方法显著降低了解码延迟和内存使用，同时保持了近乎无损的精度。", "motivation": "在消费级边缘硬件上部署MoE模型面临设备内存受限的挑战，需要动态专家卸载。现有工作将卸载视为纯粹的调度问题，未能有效解决内存和精度之间的平衡。", "method": "本文利用专家重要性指导卸载决策，将低重要性的激活专家替换为GPU内存中已缓存的功能相似专家，以保持精度。此外，引入了一种调度策略，最大化GPU缓存专家的重用率，进一步提高效率。", "result": "该方法降低了内存使用和数据传输，基本消除了PCIe开销。实验结果显示，解码延迟降低了48%，专家缓存命中率超过60%，同时保持了近乎无损的精度。", "conclusion": "通过结合专家重要性指导的替换和优化的调度策略，本文提出的方法成功地在边缘设备上实现了MoE模型的部署，显著提升了性能（更低的延迟、更高的缓存命中率）并维持了高精度。"}}
{"id": "2508.19094", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19094", "abs": "https://arxiv.org/abs/2508.19094", "authors": ["Vincenzo Polizzi", "Stephen Yang", "Quentin Clark", "Jonathan Kelly", "Igor Gilitschenski", "David B. Lindell"], "title": "VibES: Induced Vibration for Persistent Event-Based Sensing", "comment": null, "summary": "Event cameras are a bio-inspired class of sensors that asynchronously measure\nper-pixel intensity changes. Under fixed illumination conditions in static or\nlow-motion scenes, rigidly mounted event cameras are unable to generate any\nevents, becoming unsuitable for most computer vision tasks. To address this\nlimitation, recent work has investigated motion-induced event stimulation that\noften requires complex hardware or additional optical components. In contrast,\nwe introduce a lightweight approach to sustain persistent event generation by\nemploying a simple rotating unbalanced mass to induce periodic vibrational\nmotion. This is combined with a motion-compensation pipeline that removes the\ninjected motion and yields clean, motion-corrected events for downstream\nperception tasks. We demonstrate our approach with a hardware prototype and\nevaluate it on real-world captured datasets. Our method reliably recovers\nmotion parameters and improves both image reconstruction and edge detection\nover event-based sensing without motion induction.", "AI": {"tldr": "该研究提出了一种轻量级方法，通过引入周期性振动来持续生成事件相机事件，并结合运动补偿管道去除注入的运动，以改善在静态或低运动场景下的感知任务。", "motivation": "事件相机在静态或低运动场景下，如果照明条件固定，无法生成事件，因此不适用于大多数计算机视觉任务。现有解决方案通常需要复杂的硬件或额外的光学组件。", "method": "该方法通过使用一个简单的旋转不平衡质量来诱导周期性振动，从而持续生成事件。随后，结合一个运动补偿管道来移除注入的运动，以获得干净、运动校正后的事件，用于后续感知任务。", "result": "实验结果表明，该方法能够可靠地恢复运动参数，并且在图像重建和边缘检测方面，相较于没有运动诱导的事件传感，均有显著改善。", "conclusion": "通过引入简单的振动和运动补偿，该方法有效解决了事件相机在静态或低运动场景下事件生成不足的问题，提高了其在这些条件下的感知性能。"}}
{"id": "2508.18824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18824", "abs": "https://arxiv.org/abs/2508.18824", "authors": ["Sirui Chen", "Changxin Tian", "Binbin Hu", "Kunlong Chen", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness", "comment": null, "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.", "AI": {"tldr": "本文提出了一种新颖的程序辅助合成框架，用于系统地生成高质量、多样、复杂且正确的数学语料库，以提升大型语言模型的数学推理能力。", "motivation": "传统生成数学训练数据的方法面临可扩展性、成本和数据可靠性方面的挑战，而提升大型语言模型（LLMs）的数学推理能力需要高质量的训练数据。", "method": "该框架整合数学知识系统和领域特定工具来创建可执行程序。这些程序随后被翻译成自然语言的问题-解决方案对，并通过双边验证机制进行审查，以验证解决方案的正确性和程序与问题的一致性。", "result": "研究生成了1230万个问题-解决方案三元组。在此数据上进行微调的模型显著提高了推理能力，在多个基准数据集上达到了最先进的性能。", "conclusion": "所提出的合成方法有效提升了大型语言模型的数学推理能力，并证实了其生成高质量训练数据的有效性。"}}
{"id": "2508.18734", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18734", "abs": "https://arxiv.org/abs/2508.18734", "authors": ["DongHoon Lim", "YoungChae Kim", "Dong-Hyun Kim", "Da-Hee Yang", "Joon-Hyuk Chang"], "title": "Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion", "comment": "Accepted to IEEE ASRU 2025", "summary": "Robust audio-visual speech recognition (AVSR) in noisy environments remains\nchallenging, as existing systems struggle to estimate audio reliability and\ndynamically adjust modality reliance. We propose router-gated cross-modal\nfeature fusion, a novel AVSR framework that adaptively reweights audio and\nvisual features based on token-level acoustic corruption scores. Using an\naudio-visual feature fusion-based router, our method down-weights unreliable\naudio tokens and reinforces visual cues through gated cross-attention in each\ndecoder layer. This enables the model to pivot toward the visual modality when\naudio quality deteriorates. Experiments on LRS3 demonstrate that our approach\nachieves an 16.51-42.67% relative reduction in word error rate compared to\nAV-HuBERT. Ablation studies confirm that both the router and gating mechanism\ncontribute to improved robustness under real-world acoustic noise.", "AI": {"tldr": "本文提出了一种名为“路由器门控跨模态特征融合”的新型音视频语音识别（AVSR）框架，通过自适应地根据音频可靠性分数重新加权音视频特征，显著提升了在嘈杂环境下的鲁棒性。", "motivation": "在嘈杂环境中，鲁棒的音视频语音识别（AVSR）仍然具有挑战性，因为现有系统难以准确估计音频可靠性并动态调整对不同模态的依赖。", "method": "该方法提出路由器门控跨模态特征融合，根据token级别的声学损坏分数自适应地重新加权音频和视觉特征。通过基于音视频特征融合的路由器，系统会降低不可靠音频token的权重，并通过每个解码器层中的门控交叉注意力增强视觉线索。这使得模型在音频质量下降时能转向视觉模态。", "result": "在LRS3数据集上的实验表明，与AV-HuBERT相比，该方法在词错误率（WER）上实现了16.51-42.67%的相对降低。消融研究证实，路由器和门控机制都对在真实世界声学噪声下的鲁棒性提升做出了贡献。", "conclusion": "所提出的路由器门控跨模态特征融合框架通过自适应地调整模态依赖性，显著提升了AVSR在嘈杂环境中的鲁棒性，有效解决了现有系统在音频可靠性估计和模态动态调整方面的不足。"}}
{"id": "2508.19004", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19004", "abs": "https://arxiv.org/abs/2508.19004", "authors": ["Pontus Strimling", "Simon Karlsson", "Irina Vartanova", "Kimmo Eriksson"], "title": "AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms", "comment": "18 pages + supplementy materials", "summary": "A fundamental question in cognitive science concerns how social norms are\nacquired and represented. While humans typically learn norms through embodied\nsocial experience, we investigated whether large language models can achieve\nsophisticated norm understanding through statistical learning alone. Across two\nstudies, we systematically evaluated multiple AI systems' ability to predict\nhuman social appropriateness judgments for 555 everyday scenarios by examining\nhow closely they predicted the average judgment compared to each human\nparticipant. In Study 1, GPT-4.5's accuracy in predicting the collective\njudgment on a continuous scale exceeded that of every human participant (100th\npercentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%\nof humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive\npower, all models showed systematic, correlated errors. These findings\ndemonstrate that sophisticated models of social cognition can emerge from\nstatistical learning over linguistic data alone, challenging strong versions of\ntheories emphasizing the exclusive necessity of embodied experience for\ncultural competence. The systematic nature of AI limitations across different\narchitectures indicates potential boundaries of pattern-based social\nunderstanding, while the models' ability to outperform nearly all individual\nhumans in this predictive task suggests that language serves as a remarkably\nrich repository for cultural knowledge transmission.", "AI": {"tldr": "研究发现大型语言模型仅通过统计学习就能对社会规范表现出复杂的理解，甚至在预测人类社会适宜性判断方面超越了绝大多数人类个体，但它们也存在系统性错误。", "motivation": "认知科学中的一个基本问题是社会规范如何习得和表征。鉴于人类通常通过具身化的社会经验学习规范，研究旨在探讨大型语言模型是否仅通过统计学习就能达到复杂的规范理解，以挑战强调具身经验必要性的理论。", "method": "研究通过两个实验系统评估了多个AI系统（如GPT-4.5、Gemini 2.5 Pro、GPT-5、Claude Sonnet 4）预测人类对555个日常情景的社会适宜性判断的能力。具体方法是将AI的预测与人类的平均判断以及每个个体人类参与者的判断进行比较。", "result": "在实验1中，GPT-4.5在连续尺度上预测集体判断的准确性超越了所有人类参与者（100%）。实验2复制了这一结果，Gemini 2.5 Pro超越了98.7%的人类，GPT-5超越了97.8%，Claude Sonnet 4超越了96.0%。尽管具有强大的预测能力，所有模型都表现出系统性、相关联的错误。", "conclusion": "这些发现表明，复杂的社会认知模型可以仅通过对语言数据的统计学习而出现，挑战了强调具身经验对文化能力具有排他性必要性的理论。AI在不同架构中表现出的系统性局限性表明了基于模式的社会理解的潜在边界，而模型在预测任务中超越几乎所有个体人类的能力则表明语言是文化知识传播的极其丰富的载体。"}}
{"id": "2508.18847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18847", "abs": "https://arxiv.org/abs/2508.18847", "authors": ["Yibo Li", "Miao Xiong", "Jiaying Wu", "Bryan Hooi"], "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.", "AI": {"tldr": "ConfTuner是一种简单高效的微调方法，它引入了理论上正确的“tokenized Brier score”损失函数，以改善大型语言模型（LLMs）的置信度校准，从而提高LLM在各种推理任务中的可靠性和可信度。", "motivation": "LLMs在科学、法律和医疗等高风险领域部署，对不确定性的准确表达至关重要。然而，LLMs常表现出“过度自信”，即以高置信度生成错误答案。现有校准方法（提示工程或启发式微调）效果和泛化能力有限。", "method": "引入ConfTuner，一种简单高效的微调方法，开销极小，不依赖真实置信度分数或代理置信度估计。它使用新的损失函数——tokenized Brier score，该函数被理论证明是一个适当的评分规则，能正确激励模型报告其真实正确概率。", "result": "ConfTuner显著改善了LLM在各种推理任务中的置信度校准。它能泛化到GPT-4o等黑盒模型。此外，更好的校准置信度还能带来下游任务（如自我纠正和模型级联）的性能提升。", "conclusion": "ConfTuner通过提高LLM的校准置信度，促进了可信赖LLM系统的发展，使其在关键应用中更可靠。"}}
{"id": "2508.18753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18753", "abs": "https://arxiv.org/abs/2508.18753", "authors": ["Qinqian Lei", "Bo Wang", "Robby T. Tan"], "title": "Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods", "comment": null, "summary": "Prior human-object interaction (HOI) detection methods have integrated early\nvision-language models (VLMs) such as CLIP, but only as supporting components\nwithin their frameworks. In contrast, recent advances in large, generative VLMs\nsuggest that these models may already possess strong ability to understand\nimages involving HOI. This naturally raises an important question: can\ngeneral-purpose standalone VLMs effectively solve HOI detection, and how do\nthey compare with specialized HOI methods? Answering this requires a benchmark\nthat can accommodate both paradigms. However, existing HOI benchmarks such as\nHICO-DET were developed before the emergence of modern VLMs, and their\nevaluation protocols require exact matches to annotated HOI classes. This is\npoorly aligned with the generative nature of VLMs, which often yield multiple\nvalid interpretations in ambiguous cases. For example, a static image may\ncapture a person mid-motion with a frisbee, which can plausibly be interpreted\nas either \"throwing\" or \"catching\". When only \"catching\" is annotated, the\nother, though equally plausible for the image, is marked incorrect when exact\nmatching is used. As a result, correct predictions might be penalized,\naffecting both VLMs and HOI-specific methods. To avoid penalizing valid\npredictions, we introduce a new benchmark that reformulates HOI detection as a\nmultiple-answer multiple-choice task, where each question includes only\nground-truth positive options and a curated set of negatives that are\nconstructed to reduce ambiguity (e.g., when \"catching\" is annotated, \"throwing\"\nis not selected as a negative to avoid penalizing valid predictions). The\nproposed evaluation protocol is the first of its kind for both VLMs and HOI\nmethods, enabling direct comparison and offering new insight into the current\nstate of progress in HOI understanding.", "AI": {"tldr": "本文引入了一个新的基准测试，将人-物交互（HOI）检测重新定义为多答案选择任务，旨在有效评估通用视觉-语言模型（VLM）与专用HOI方法，解决了现有基准在处理生成式VLM多重有效解释时的局限性。", "motivation": "早期的HOI检测方法仅将VLM作为辅助组件。随着大型生成式VLM的兴起，它们可能具备强大的HOI理解能力。然而，现有的HOI基准（如HICO-DET）要求精确匹配标注，这与生成式VLM可能产生多个有效解释的特性不符，导致有效预测可能被错误惩罚。", "method": "本文提出了一个新基准，将HOI检测重新定义为多答案多选任务。每个问题只包含真实的正向选项和一组精心策划的负向选项，以减少歧义。例如，在“捕捉”被标注时，“投掷”不会被选为负向选项，以避免惩罚合理的预测。", "result": "本文引入了首个针对VLM和HOI方法的此类评估协议和基准。该协议能够直接比较这两种范式，并为HOI理解的当前进展状态提供了新的见解。", "conclusion": "新的评估协议和基准为通用VLM和专用HOI方法提供了一个公平的比较平台，有望促进HOI理解领域的发展和深入洞察。"}}
{"id": "2508.19005", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19005", "abs": "https://arxiv.org/abs/2508.19005", "authors": ["Yuxuan Cai", "Yipeng Hao", "Jie Zhou", "Hang Yan", "Zhikai Lei", "Rui Zhen", "Zhenhua Han", "Yutao Yang", "Junsong Li", "Qianjun Pan", "Tianyu Huai", "Qin Chen", "Xin Li", "Kai Chen", "Bo Zhang", "Xipeng Qiu", "Liang He"], "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark", "comment": null, "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI.", "AI": {"tldr": "本文提出了一种名为“经验驱动的终身学习（ELL）”的框架，用于构建通过真实世界互动持续学习和自我进化的智能体，并引入了“StuLife”基准数据集来评估其终身学习能力。", "motivation": "随着人工智能向通用智能发展，研究重点正从针对静态任务优化的系统转向创建能够持续学习的开放式智能体。现有系统难以实现持续成长和与现实世界互动。", "method": "本文提出了ELL框架，其基于四个核心原则：1) 经验探索：智能体通过与动态环境的自我激励互动进行学习。2) 长期记忆：智能体将历史知识（经验、专业知识、常识）存储为持久记忆。3) 技能学习：智能体从经验中抽象出可重用技能并进行改进。4) 知识内化：智能体将显式经验内化为隐式能力。同时，引入了StuLife基准数据集，模拟学生大学旅程，包含三个阶段和十个子场景，用于评估记忆保留、技能迁移和自我激励行为。", "result": "StuLife基准数据集提供了一个全面的平台，用于评估智能体的终身学习能力，包括记忆保留、技能迁移和自我激励行为。该工作还探索了上下文工程在推进AGI中的作用，并可用于评估最先进的大型语言模型（LLMs）。", "conclusion": "ELL框架和StuLife基准为构建和评估能够通过持续互动自我进化、不断学习的智能体提供了理论和实践基础，这对于实现通用人工智能（AGI）至关重要。"}}
{"id": "2508.18870", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18870", "abs": "https://arxiv.org/abs/2508.18870", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.", "AI": {"tldr": "本文提出了一种名为ReflectivePrompt的新型自动提示方法，它基于进化算法并采用反射式进化策略，通过短期和长期反射操作来优化提示选择，并在多个任务和LLM上取得了显著优于现有SOTA方法的性能提升。", "motivation": "随着提示工程和大型语言模型（LLMs）的快速发展，自动选择优化提示（即自动提示）的需求日益增长，驱动了对更精确、更全面的最佳提示搜索方法的研究。", "method": "本文提出ReflectivePrompt方法，这是一种基于进化算法的自动提示方法，采用反射式进化策略。它在交叉和精英突变之前引入了短期和长期反射操作，以提高修改质量。该方法能累积进化过程中获得的知识，并根据当前种群在每个周期进行更新。", "result": "ReflectivePrompt在33个分类和文本生成数据集上，使用t-lite-instruct-0.1和gemma3-27b-it等开源大型语言模型进行了测试。结果显示，该方法相对于当前最先进的方法（例如，在BBH上比EvoPrompt平均提升28%）取得了显著改进。", "conclusion": "ReflectivePrompt被证明是基于进化算法的自动提示领域中最有效的解决方案之一，其性能显著优于现有SOTA方法。"}}
{"id": "2508.18772", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18772", "abs": "https://arxiv.org/abs/2508.18772", "authors": ["Wanqiang Wang", "Longzhu He", "Wei Zheng"], "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs", "comment": "EMNLP 2025", "summary": "Multiple-choice questions (MCQs) play a crucial role in fostering deep\nthinking and knowledge integration in education. However, previous research has\nprimarily focused on generating MCQs with textual options, but it largely\noverlooks the visual options. Moreover, generating high-quality distractors\nremains a major challenge due to the high cost and limited scalability of\nmanual authoring. To tackle these problems, we propose a Cross-modal Options\nSynthesis (CmOS), a novel framework for generating educational MCQs with visual\noptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning\nprocess and Retrieval-Augmented Generation (RAG) to produce semantically\nplausible and visually similar answer and distractors. It also includes a\ndiscrimination module to identify content suitable for visual options.\nExperimental results on test tasks demonstrate the superiority of CmOS in\ncontent discrimination, question generation and visual option generation over\nexisting methods across various subjects and educational levels.", "AI": {"tldr": "该研究提出了一种名为CmOS的新颖框架，用于生成带有视觉选项的教育多项选择题（MCQs），通过整合多模态思维链和检索增强生成，解决了现有方法忽视视觉选项和高质量干扰项生成成本高的问题。", "motivation": "现有研究主要关注生成带有文本选项的MCQs，忽视了视觉选项。此外，由于手动创作成本高且可扩展性有限，生成高质量的干扰项（特别是视觉干扰项）仍然是一个重大挑战。", "method": "该论文提出了跨模态选项合成（CmOS）框架，用于生成带有视觉选项的教育MCQs。该框架整合了多模态思维链（MCoT）推理过程和检索增强生成（RAG），以生成语义合理且视觉相似的答案和干扰项。它还包含一个判别模块，用于识别适合视觉选项的内容。", "result": "在测试任务上的实验结果表明，CmOS在内容判别、问题生成和视觉选项生成方面，优于现有方法，且适用于各种学科和教育水平。", "conclusion": "CmOS框架成功地解决了生成带有视觉选项的MCQs以及高质量干扰项的挑战，通过其创新的多模态整合和判别模块，显著提升了MCQs的生成质量和效率。"}}
{"id": "2508.19008", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19008", "abs": "https://arxiv.org/abs/2508.19008", "authors": ["Marcin Moskalewicz", "Anna Sterna", "Marek Pokropski", "Paula Flores"], "title": "Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI", "comment": "22 pages, 4 tables, submitted to \"Personality and Individual\n  Differences\"", "summary": "This study examines the capacity of large language models (LLMs) to support\nphenomenological qualitative analysis of first-person experience in Borderline\nPersonality Disorder (BPD), understood as a disorder of temporality and\nselfhood. Building on a prior human-led thematic analysis of 24 inpatients'\nlife-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5\nPro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the\noriginal investigators. The models were evaluated with blinded and non-blinded\nexpert judges in phenomenology and clinical psychology. Assessments included\nsemantic congruence, Jaccard coefficients, and multidimensional validity\nratings (credibility, coherence, substantiveness, and groundness in data).\nResults showed variable overlap with the human analysis, from 0 percent in GPT\nto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient\n(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's\noutput most closely resembled the human analysis, with validity scores\nsignificantly higher than GPT and Claude (p < 0.0001), and was judged as human\nby blinded experts. All scores strongly correlated (R > 0.78) with the quantity\nof text and words per theme, highlighting both the variability and potential of\nAI-augmented thematic analysis to mitigate human interpretative bias.", "AI": {"tldr": "本研究评估大型语言模型（LLMs）在支持边缘性人格障碍（BPD）患者第一人称经验现象学定性分析方面的能力。结果显示，Gemini模型表现最佳，其输出与人类分析最接近，并被专家判断为人类生成，展示了AI辅助分析在减少人类解释偏差方面的潜力。", "motivation": "本研究旨在探索大型语言模型（LLMs）在现象学定性分析（特别是针对边缘性人格障碍中对时间性和自我障碍的理解）中的应用能力，并评估其是否能辅助或替代人类分析，以减少人类解释中的潜在偏差。", "method": "研究以24名住院BPD患者的生命故事访谈数据为基础，这些数据已有人类主导的主题分析。研究比较了三种大型语言模型（OpenAI GPT-4o、Google Gemini 2.5 Pro、Anthropic Claude Opus 4），并提示它们模仿原始研究者的解释风格。评估由盲审和非盲审的现象学与临床心理学专家进行，评估指标包括语义一致性、Jaccard系数以及多维度有效性评分（可信度、连贯性、实质性、数据基础）。", "result": "模型与人类分析的重叠率差异显著：GPT为0%，Claude为42%，Gemini为58%。Jaccard系数较低（0.21-0.28）。然而，模型能够发现人类分析中遗漏的主题。Gemini的输出与人类分析最为接近，其有效性评分显著高于GPT和Claude（p < 0.0001），并被盲审专家判断为人类生成。所有评分与主题文本量和词数呈强相关（R > 0.78）。", "conclusion": "AI增强的主题分析，特别是通过Gemini等高性能模型，具有支持定性分析和减轻人类解释偏差的潜力。尽管不同大型语言模型的表现存在差异，但AI辅助方法在处理复杂的第一人称经验数据方面展现出显著的可能性。"}}
{"id": "2508.18872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18872", "abs": "https://arxiv.org/abs/2508.18872", "authors": ["Laurie Gale", "Sebastian Mateos Nicolajsen"], "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis", "comment": "7 pages, 2 figures", "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.", "AI": {"tldr": "本文提出了一种名为LLM辅助内容分析（LACA）的方法，旨在帮助计算教育研究（CER）领域的研究人员，在不增加负担的情况下，对大量定性文本数据进行严谨且可推广的分析。", "motivation": "计算教育研究人员常因缺乏同事、资源或能力，难以进行具有普遍性和严谨性的研究，尤其是在处理大量定性数据时。这限制了他们改进教学实践和推进学科发展的能力。因此，需要一种能够处理大量定性数据、同时不增加研究者负担的研究方法。", "method": "本文提出了一种LLM辅助内容分析（LACA）方法，它是内容分析与大型语言模型（LLM）结合的变体。作者使用一个计算教育数据集，演示了LACA如何以可重现和严谨的方式应用。", "result": "LACA方法能够帮助研究人员进行他们原本无法完成的大规模研究。通过计算教育数据集的案例，LACA被证明可以以可重现和严谨的方式应用，从而有望在CER中产生更具普遍性的发现。", "conclusion": "LACA方法在计算教育研究中具有巨大潜力，能够促进更广泛研究产生更具普遍性的发现。结合类似方法的发展，可以共同提升CER学科的实践水平和研究质量。"}}
{"id": "2508.18785", "categories": ["eess.SP", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18785", "abs": "https://arxiv.org/abs/2508.18785", "authors": ["Luqing Luo", "Wenjin Gui", "Yunfei Liu", "Ziyue Zhang", "Yunxi Zhang", "Fengxiang Wang", "Zonghao Guo", "Zizhi Ma", "Xinzhu Liu", "Hanxiang He", "Jinhai Li", "Xin Qiu", "Wupeng Xie", "Yangang Sun"], "title": "EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding", "comment": null, "summary": "Deep understanding of electromagnetic signals is fundamental to dynamic\nspectrum management, intelligent transportation, autonomous driving and\nunmanned vehicle perception. The field faces challenges because electromagnetic\nsignals differ greatly from text and images, showing high heterogeneity, strong\nbackground noise and complex joint time frequency structure, which prevents\nexisting general models from direct use. Electromagnetic communication and\nsensing tasks are diverse, current methods lack cross task generalization and\ntransfer efficiency, and the scarcity of large high quality datasets blocks the\ncreation of a truly general multitask learning framework. To overcome these\nissue, we introduce EMind, an electromagnetic signals foundation model that\nbridges large scale pretraining and the unique nature of this modality. We\nbuild the first unified and largest standardized electromagnetic signal dataset\ncovering multiple signal types and tasks. By exploiting the physical properties\nof electromagnetic signals, we devise a length adaptive multi-signal packing\nmethod and a hardware-aware training strategy that enable efficient use and\nrepresentation learning from heterogeneous multi-source signals. Experiments\nshow that EMind achieves strong performance and broad generalization across\nmany downstream tasks, moving decisively from task specific models to a unified\nframework for electromagnetic intelligence. The code is available at:\nhttps://github.com/GabrielleTse/EMind.", "AI": {"tldr": "本文提出了EMind，一个电磁信号基础模型，通过构建统一的大规模数据集和创新的训练策略，解决了电磁信号理解的挑战，实现了跨任务的强大性能和广泛泛化能力。", "motivation": "电磁信号的深入理解对动态频谱管理、智能交通、自动驾驶等至关重要。然而，电磁信号与文本图像差异大，具有高异构性、强背景噪声和复杂时频结构，导致现有通用模型无法直接使用。此外，当前方法缺乏跨任务泛化和迁移效率，且高质量大规模数据集稀缺，阻碍了通用多任务学习框架的建立。", "method": "本文提出了EMind电磁信号基础模型。具体方法包括：1) 构建了首个统一且最大的标准化电磁信号数据集，涵盖多种信号类型和任务；2) 利用电磁信号的物理特性，设计了长度自适应多信号打包方法；3) 采用了硬件感知训练策略，以高效利用异构多源信号进行表示学习。", "result": "实验结果表明，EMind在许多下游任务中都取得了强大的性能和广泛的泛化能力。", "conclusion": "EMind成功地将电磁信号分析从任务特定模型转向统一框架，为电磁智能领域的发展迈出了决定性一步。"}}
{"id": "2508.19014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19014", "abs": "https://arxiv.org/abs/2508.19014", "authors": ["Surajit Das", "Gourav Roy", "Aleksei Eliseev", "Ram Kumar Rajendran"], "title": "MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP", "comment": null, "summary": "The evolution of technology and education is driving the emergence of\nIntelligent & Autonomous Tutoring Systems (IATS), where objective and\ndomain-agnostic methods for determining question difficulty are essential.\nTraditional human labeling is subjective, and existing NLP-based approaches\nfail in symbolic domains like algebra. This study introduces the Approach of\nPassive Measures among Educands (APME), a reinforcement learning-based\nMulti-Armed Bandit (MAB) framework that estimates difficulty solely from solver\nperformance data -- marks obtained and time taken -- without requiring\nlinguistic features or expert labels. By leveraging the inverse coefficient of\nvariation as a risk-adjusted metric, the model provides an explainable and\nscalable mechanism for adaptive assessment. Empirical validation was conducted\non three heterogeneous datasets. Across these diverse contexts, the model\nachieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its\nrobustness, accuracy, and adaptability to different educational levels and\nassessment formats. Compared with baseline approaches-such as regression-based,\nNLP-driven, and IRT models-the proposed framework consistently outperformed\nalternatives, particularly in purely symbolic domains. The findings highlight\nthat (i) item heterogeneity strongly influences perceived difficulty, and (ii)\nvariance in solver outcomes is as critical as mean performance for adaptive\nallocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal\nDevelopment by identifying tasks that balance challenge and attainability,\nsupporting motivation while minimizing disengagement. This domain-agnostic,\nself-supervised approach advances difficulty tagging in IATS and can be\nextended beyond algebra wherever solver interaction data is available", "AI": {"tldr": "本研究提出了一种名为APME的强化学习多臂老虎机（MAB）框架，该框架仅通过学习者表现数据（得分和时间）来估计问题难度，无需语言特征或专家标签。它在不同数据集上表现出高精度和鲁棒性，特别是在符号领域优于现有基线方法，并支持自适应评估。", "motivation": "随着智能与自主辅导系统（IATS）的发展，需要客观、领域无关的问题难度评估方法。传统人工标注主观性强，现有基于NLP的方法在代数等符号领域无效。", "method": "本研究引入了“学习者被动测量方法”（APME），这是一个基于强化学习的多臂老虎机（MAB）框架。它仅从学习者的表现数据（获得的成绩和花费的时间）中估计难度，无需语言特征或专家标签。模型利用变异系数的倒数作为风险调整指标，提供可解释且可扩展的自适应评估机制。", "result": "该模型在三个异构数据集上进行了实证验证，平均R2达到0.9213，平均RMSE为0.0584，证实了其在不同教育水平和评估形式下的鲁棒性、准确性和适应性。与回归、NLP驱动和IRT模型等基线方法相比，该框架始终表现更优，尤其是在纯符号领域。研究发现：(i) 项目异质性强烈影响感知难度；(ii) 求解结果的方差与平均表现对于自适应分配同样关键。", "conclusion": "该模型与维果茨基的最近发展区理论相符，通过平衡挑战性和可实现性来识别任务，从而支持学习动机并最大程度地减少脱离感。这种领域无关、自监督的方法推动了IATS中的难度标注技术，并可扩展到任何具有学习者交互数据的领域，超越了代数范畴。"}}
{"id": "2508.18916", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18916", "abs": "https://arxiv.org/abs/2508.18916", "authors": ["Bojan Evkoski", "Igor Mozetič", "Nikola Ljubešić", "Petra Kralj Novak"], "title": "Affective Polarization across European Parliaments", "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments.", "AI": {"tldr": "本研究通过自动化自然语言处理分析欧洲六个国家议会的演讲，发现议员之间存在普遍的情感极化现象，即对对立群体的负面情绪增加，且互惠是其形成机制之一。", "motivation": "情感极化（对对立群体增加负面和敌意）已成为全球政治话语的显著特征，本研究旨在以全自动化方式，在选定的欧洲议会中检验这种极化的存在。", "method": "利用来自欧洲六个国家议会的综合议会演讲语料库，采用自然语言处理技术估算议员的情绪。通过比较提及对立群体和自身群体时所传达的负面情绪水平，来发现情感极化的互动模式。", "result": "研究发现所有六个欧洲议会中都存在一致的情感极化现象。虽然活跃度与负面情绪相关，但在不活跃和更活跃的议员之间，情感极化没有观察到差异。最后，研究表明互惠是所有六个议会中议员间情感极化的一个促成机制。", "conclusion": "欧洲六个议会中普遍存在一致的情感极化，表现为对对立群体的负面情绪增加。这种极化与议员活跃度相关，但活跃度本身并不影响极化程度。互惠是议员间情感极化的重要驱动机制。"}}
{"id": "2508.18787", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18787", "abs": "https://arxiv.org/abs/2508.18787", "authors": ["Constantino Álvarez Casado", "Sasan Sharifipour", "Manuel Lage Cañellas", "Nhi Nguyen", "Le Nguyen", "Miguel Bordallo López"], "title": "Design, Implementation and Evaluation of a Real-Time Remote Photoplethysmography (rPPG) Acquisition System for Non-Invasive Vital Sign Monitoring", "comment": "23 pages, 2 figures, 10 formulas, 3 tables", "summary": "The growing integration of smart environments and low-power computing\ndevices, coupled with mass-market sensor technologies, is driving advancements\nin remote and non-contact physiological monitoring. However, deploying these\nsystems in real-time on resource-constrained platforms introduces significant\nchallenges related to scalability, interoperability, and performance. This\npaper presents a real-time remote photoplethysmography (rPPG) system optimized\nfor low-power devices, designed to extract physiological signals, such as heart\nrate (HR), respiratory rate (RR), and oxygen saturation (SpO2), from facial\nvideo streams. The system is built on the Face2PPG pipeline, which processes\nvideo frames sequentially for rPPG signal extraction and analysis, while\nleveraging a multithreaded architecture to manage video capture, real-time\nprocessing, network communication, and graphical user interface (GUI) updates\nconcurrently. This design ensures continuous, reliable operation at 30 frames\nper second (fps), with adaptive feedback through a collaborative user interface\nto guide optimal signal capture conditions. The network interface includes both\nan HTTP server for continuous video streaming and a RESTful API for on-demand\nvital sign retrieval. To ensure accurate performance despite the limitations of\nlow-power devices, we use a hybrid programming model combining Functional\nReactive Programming (FRP) and the Actor Model, allowing event-driven\nprocessing and efficient task parallelization. The system is evaluated under\nreal-time constraints, demonstrating robustness while minimizing computational\noverhead. Our work addresses key challenges in real-time biosignal monitoring,\noffering practical solutions for optimizing performance in modern healthcare\nand human-computer interaction applications.", "AI": {"tldr": "本文提出一个针对低功耗设备的实时远程光电容积描记法（rPPG）系统，通过面部视频提取生理信号，并采用多线程架构及FRP/Actor混合编程模型，实现了高效、鲁棒的实时监测。", "motivation": "智能环境与低功耗计算设备的日益集成以及大众市场传感器技术的发展，推动了远程非接触式生理监测。然而，在资源受限平台上实时部署这些系统面临可扩展性、互操作性和性能方面的挑战。", "method": "该系统基于Face2PPG流水线，顺序处理视频帧以提取和分析rPPG信号。它采用多线程架构并发管理视频捕获、实时处理、网络通信和GUI更新，确保30 fps的连续可靠操作。网络接口包括HTTP服务器（用于视频流）和RESTful API（用于按需生命体征检索）。为保证在低功耗设备上的准确性，系统结合了函数响应式编程（FRP）和Actor模型，以实现事件驱动处理和高效任务并行化。", "result": "系统在实时约束下进行了评估，证明了其鲁棒性，同时最大限度地减少了计算开销，并能以30帧/秒的速度连续可靠运行。它为现代医疗保健和人机交互应用中的性能优化提供了实用解决方案。", "conclusion": "该工作解决了实时生物信号监测中的关键挑战，提供了一个针对低功耗设备优化的实时rPPG系统，适用于医疗保健和人机交互应用，具有实用且高性能的特点。"}}
{"id": "2508.19035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19035", "abs": "https://arxiv.org/abs/2508.19035", "authors": ["Congchi Yin", "Tianyi Wu", "Yankai Shu", "Alex Gu", "Yunhan Wang", "Jun Shao", "Xun Jiang", "Piji Li"], "title": "Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction", "comment": null, "summary": "Existing tasks fall short in evaluating reasoning ability of Large Language\nModels (LLMs) in an interactive, unknown environment. This deficiency leads to\nthe isolated assessment of deductive, inductive, and abductive reasoning,\nneglecting the integrated reasoning process that is indispensable for humans\ndiscovery of real world. We introduce a novel evaluation paradigm,\n\\textit{black-box interaction}, to tackle this challenge. A black-box is\ndefined by a hidden function that maps a specific set of inputs to outputs.\nLLMs are required to unravel the hidden function behind the black-box by\ninteracting with it in given exploration turns, and reasoning over observed\ninput-output pairs. Leveraging this idea, we build the \\textsc{Oracle}\nbenchmark which comprises 6 types of black-box task and 96 black-boxes. 19\nmodern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over\n70\\% accuracy on most easy black-boxes. But it still struggles with some hard\nblack-box tasks, where its average performance drops below 40\\%. Further\nanalysis indicates a universal difficulty among LLMs: They lack the high-level\nplanning capability to develop efficient and adaptive exploration strategies\nfor hypothesis refinement.", "AI": {"tldr": "本文提出了一种名为“黑盒交互”的新评估范式和Oracle基准，以评估大型语言模型（LLMs）在未知、交互式环境中的综合推理能力，发现LLMs在规划高效探索策略方面存在普遍不足。", "motivation": "现有任务无法在交互式、未知环境中有效评估LLMs的综合推理能力，导致对演绎、归纳和溯因推理的评估是孤立的，忽视了人类发现真实世界所需的整合推理过程。", "method": "引入了“黑盒交互”的评估范式，其中LLMs需要通过在给定探索回合内与黑盒（由隐藏函数定义）交互并基于观察到的输入-输出对进行推理来揭示隐藏函数。基于此，构建了包含6种黑盒任务和96个黑盒的Oracle基准，并对19个现代LLMs进行了基准测试。", "result": "在6种任务中，o3在其中5种任务中排名第一，在大多数简单黑盒上准确率超过70%。然而，在一些困难黑盒任务上，其平均性能下降到40%以下。进一步分析表明，LLMs普遍存在一个困难：它们缺乏开发高效自适应探索策略以完善假设的高级规划能力。", "conclusion": "“黑盒交互”范式和Oracle基准提供了一种评估LLMs综合推理能力的新方法。尽管领先模型在简单任务上表现良好，但LLMs在面对复杂、交互式推理任务时，普遍缺乏有效的高级规划能力来指导探索和假设完善，这限制了它们在未知环境中的推理表现。"}}
{"id": "2508.18929", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18929", "abs": "https://arxiv.org/abs/2508.18929", "authors": ["Ilias Driouich", "Hongliu Cao", "Eoin Thomas"], "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework", "comment": "ECAI 2025 TRUST AI workshop", "summary": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.", "AI": {"tldr": "本文提出了一种新颖的多智能体框架，用于生成合成的检索增强生成（RAG）系统评估问答数据集，特别关注语义多样性和隐私保护，以解决现有评估数据集中多样性不足和隐私考量缺失的问题。", "motivation": "RAG系统评估的有效性和可信度高度依赖于评估过程，特别是是否捕捉了隐私保护等真实世界约束。然而，当前的评估工作主要集中于性能指标，对底层评估数据集的设计和质量关注不足，导致评估缺乏意义和可靠性。", "method": "该方法引入了一个多智能体框架来生成合成问答数据集，包括：(1) 一个多样性智能体，利用聚类技术最大化主题覆盖和语义变异性；(2) 一个隐私智能体，检测并遮蔽跨多个领域的敏感信息；(3) 一个问答策划智能体，综合私密且多样化的问答对作为RAG评估的真实标签。", "result": "实验结果表明，该方法生成的评估数据集在多样性方面优于基线方法，并在特定领域数据集上实现了鲁棒的隐私遮蔽。", "conclusion": "这项工作为RAG系统提供了一条实用且符合伦理的、更安全、更全面的评估途径，为未来符合不断演进的AI法规和合规标准的增强奠定了基础。"}}
{"id": "2508.18790", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18790", "abs": "https://arxiv.org/abs/2508.18790", "authors": ["Yuhui Tao", "Yizhe Zhang", "Qiang Chen"], "title": "A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework", "comment": null, "summary": "The development of artificial intelligence models for macular edema (ME)\nanaly-sis always relies on expert-annotated pixel-level image datasets which\nare expen-sive to collect prospectively. While anomaly-detection-based\nweakly-supervised methods have shown promise in edema area (EA) segmentation\ntask, their per-formance still lags behind fully-supervised approaches. In this\npaper, we leverage the strong correlation between EA and retinal layers in\nspectral-domain optical coherence tomography (SD-OCT) images, along with the\nupdate characteristics of weakly-supervised learning, to enhance an\noff-the-shelf adversarial framework for EA segmentation with a novel\nlayer-structure-guided post-processing step and a test-time-adaptation (TTA)\nstrategy. By incorporating additional retinal lay-er information, our framework\nreframes the dense EA prediction task as one of confirming intersection points\nbetween the EA contour and retinal layers, result-ing in predictions that\nbetter align with the shape prior of EA. Besides, the TTA framework further\nhelps address discrepancies in the manifestations and presen-tations of EA\nbetween training and test sets. Extensive experiments on two pub-licly\navailable datasets demonstrate that these two proposed ingredients can im-prove\nthe accuracy and robustness of EA segmentation, bridging the gap between\nweakly-supervised and fully-supervised models.", "AI": {"tldr": "本文提出了一种结合视网膜层结构引导的后处理和测试时适应（TTA）策略的方法，以提升弱监督黄斑水肿（ME）分割模型的性能，缩小其与全监督模型之间的差距。", "motivation": "分析黄斑水肿的AI模型通常依赖昂贵的像素级专家标注数据集。虽然基于异常检测的弱监督方法在水肿区域（EA）分割任务中显示出潜力，但其性能仍落后于全监督方法。", "method": "利用SD-OCT图像中EA与视网膜层之间的强相关性，并结合弱监督学习的更新特性，作者增强了一个对抗性框架。具体方法包括：1) 引入一种新颖的层结构引导的后处理步骤，将密集的EA预测重构为确认EA轮廓与视网膜层之间的交点；2) 采用测试时适应（TTA）策略，以解决训练集和测试集之间EA表现形式的差异。", "result": "在两个公开数据集上进行的广泛实验表明，所提出的两种方法（层结构引导后处理和TTA）能够提高EA分割的准确性和鲁棒性，从而缩小了弱监督模型与全监督模型之间的性能差距。", "conclusion": "结合视网膜层结构信息和测试时适应策略，可以显著提升弱监督黄斑水肿分割模型的性能和鲁棒性，使其更接近全监督模型的水平。"}}
{"id": "2508.19042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19042", "abs": "https://arxiv.org/abs/2508.19042", "authors": ["Norihiro Maruyama", "Takahide Yoshida", "Hiroki Sato", "Atsushi Masumori", "Johnsmith", "Takashi Ikegami"], "title": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents", "comment": null, "summary": "We introduce the Concurrent Modular Agent (CMA), a framework that\norchestrates multiple Large-Language-Model (LLM)-based modules that operate\nfully asynchronously yet maintain a coherent and fault-tolerant behavioral\nloop. This framework addresses long-standing difficulties in agent\narchitectures by letting intention emerge from language-mediated interactions\namong autonomous processes. This approach enables flexible, adaptive, and\ncontext-dependent behavior through the combination of concurrently executed\nmodules that offload reasoning to an LLM, inter-module communication, and a\nsingle shared global state.We consider this approach to be a practical\nrealization of Minsky's Society of Mind theory. We demonstrate the viability of\nour system through two practical use-case studies. The emergent properties\nobserved in our system suggest that complex cognitive phenomena like\nself-awareness may indeed arise from the organized interaction of simpler\nprocesses, supporting Minsky-Society of Mind concept and opening new avenues\nfor artificial intelligence research. The source code for our work is available\nat: https://github.com/AlternativeMachine/concurrent-modular-agent.", "AI": {"tldr": "本文提出了并发模块化智能体（CMA）框架，它协调多个基于LLM的模块异步运行，同时保持连贯和容错的行为循环，实现了意图的涌现，并支持明斯基的“心智社会”理论。", "motivation": "解决现有智能体架构中长期存在的难题，通过自主进程间基于语言的交互，实现意图的涌现，并使智能体能够展现灵活、适应性强且依赖于上下文的行为。", "method": "CMA框架通过并发执行的LLM模块、模块间通信和单一共享全局状态相结合，实现异步操作、保持连贯性和容错性。该方法将推理任务卸载给LLM，被视为明斯基“心智社会”理论的实际实现。", "result": "通过两个实际用例研究验证了系统的可行性。观察到系统中涌现出复杂的认知现象（如自我意识），这表明复杂认知可能源于更简单过程的有序交互。", "conclusion": "CMA框架支持明斯基的“心智社会”概念，通过展示复杂认知现象可从简单过程的有序交互中涌现，为人工智能研究开辟了新途径。"}}
{"id": "2508.18988", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18988", "abs": "https://arxiv.org/abs/2508.18988", "authors": ["Hung Ming Liu"], "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models", "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html", "summary": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.", "AI": {"tldr": "本文提出了一种“AI母语”框架，使神经网络模型能够开发出一种原生的符号语言，同时支持直观推理、组合式符号链和固有的可解释性。", "motivation": "现有的事后解释方法无法将推理直接嵌入模型表示中，缺乏透明且灵活的推理机制。研究旨在开发一种能将推理直接融入模型表示，并提供固有可解释性的方法。", "method": "该框架通过以下方式实现：1) 开发“AI母语”——一种原生符号语言；2) 将推理直接嵌入模型表示中，通过符号捕捉语义模式，通过链条追踪决策路径，并通过门控归纳机制引导选择性关注；3) 引入互补的训练目标以增强符号纯度和决策稀疏性；4) 采用顺序专业化策略，先建立广泛的符号能力，再细化直观判断。", "result": "在AI任务上的实验表明，该方法在保持竞争性准确率的同时，能够提供可验证的推理轨迹。", "conclusion": "“AI母语”可以作为神经网络模型中可解释性、直觉和符号推理的统一机制。"}}
{"id": "2508.18799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18799", "abs": "https://arxiv.org/abs/2508.18799", "authors": ["Hassan Abid", "Khan Muhammad", "Muhammad Haris Khan"], "title": "Robust and Label-Efficient Deep Waste Detection", "comment": "Accepted to BMVC 2025", "summary": "Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection.", "AI": {"tldr": "本研究通过建立强大的基线和引入基于集成学习的半监督学习框架，显著提升了AI驱动的垃圾检测能力，特别是在零样本学习和利用未标注数据方面。", "motivation": "有效的垃圾分类对于可持续回收至关重要，但由于数据集有限和对传统目标检测器的依赖，该领域的AI研究落后于商业系统。", "method": "首先，在ZeroWaste数据集上基准测试了最先进的开放词汇目标检测（OVOD）模型，并评估了LLM优化提示的效果。其次，微调了现代基于Transformer的检测器。最后，提出了一种软伪标签策略，通过空间和共识感知的加权融合集成预测，实现了鲁棒的半监督训练。", "result": "LLM优化提示显著提高了OVOD的零样本准确性。通过微调Transformer检测器，达到了51.6 mAP的新基线。应用于未标注的ZeroWaste-s子集时，其伪标注实现了超越完全监督训练的性能提升，突显了可扩展标注管道的有效性。", "conclusion": "本工作为研究社区建立了严格的基线，引入了鲁棒的基于集成学习的伪标签管道，为未标注的ZeroWaste-s子集生成了高质量的标注，并系统评估了真实世界垃圾分类条件下的OVOD模型。"}}
{"id": "2508.19069", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19069", "abs": "https://arxiv.org/abs/2508.19069", "authors": ["Zhichao Yang", "Zhaoxin Fan", "Gen Li", "Yuanze Hu", "Xinyu Wang", "Ye Qiu", "Xin Wang", "Yifan Sun", "Wenjun Wu"], "title": "Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty", "comment": "9 pages", "summary": "Structured, procedural reasoning is essential for Large Language Models\n(LLMs), especially in mathematics. While post-training methods have improved\nLLM performance, they still fall short in capturing deep procedural logic on\ncomplex tasks. To tackle the issue, in this paper, we first investigate this\nlimitation and uncover a novel finding: a Scaling Law by Difficulty, which\nreveals that model performance follows a U-shaped curve with respect to\ntraining data complexity -- excessive low-difficulty data impedes abstraction,\nwhile high-difficulty data significantly enhances reasoning ability. Motivated\nby this, we propose the Structured Solution Template (SST) framework, which\nuses solution templates and a curriculum of varied difficulty to explicitly\nteach procedural reasoning. Specifically, SST comprises (1) fine-tuning with\nstructured solution-template chains and dynamically weighted loss to prioritize\nprocedural logic, (2) prompt-time injection of solution templates as cognitive\nscaffolds to guide inference, and (3) integrated curriculum fine-tuning that\nexplicitly teaches the model to self-plan - execute - self-correct. Experiments\non GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly\nimproves both accuracy and efficiency, especially on harder problems.", "AI": {"tldr": "本文提出了一种结构化解决方案模板（SST）框架，通过利用难度标度定律和课程学习，显著提升了大型语言模型在数学等复杂任务中的结构化程序推理能力。", "motivation": "大型语言模型（LLMs）在复杂任务中缺乏深层程序逻辑，尤其是在数学领域。研究发现存在一个“难度标度定律”，即模型性能与训练数据复杂度呈U型曲线关系，过多的低难度数据会阻碍抽象能力，而高难度数据能显著增强推理能力。这促使研究者寻求一种明确教授程序推理的方法。", "method": "本文提出了结构化解决方案模板（SST）框架，包含三个核心部分：1) 使用结构化解决方案模板链和动态加权损失进行微调，以优先处理程序逻辑；2) 在提示时注入解决方案模板作为认知支架来指导推理；3) 集成课程微调，明确教导模型进行“自我规划-执行-自我纠正”。", "result": "在GSM8K、AIME24和新的Dynamic En基准测试上的实验表明，SST显著提高了模型的准确性和效率，尤其是在解决较难问题方面。", "conclusion": "SST框架通过明确教授结构化程序推理，有效解决了LLMs在复杂任务中深层程序逻辑不足的问题，并显著提升了模型在数学等领域的推理能力和效率。"}}
{"id": "2508.18992", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18992", "abs": "https://arxiv.org/abs/2508.18992", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "Automatic Prompt Optimization with Prompt Distillation", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.", "AI": {"tldr": "本文提出了一种名为DistillPrompt的新型自动提示（autoprompting）方法，它通过多阶段整合任务特定信息并利用蒸馏、压缩和聚合操作，显著提高了语言模型在文本分类和生成任务上的表现。", "motivation": "随着大型语言模型（LLMs）的快速发展和提示工程（prompt engineering）的广泛研究，自动选择优化提示（autoprompting）变得越来越流行，促使研究人员探索更有效的自动提示方法。", "method": "DistillPrompt是一种基于大型语言模型的自动提示方法。它采用多阶段集成训练数据中的任务特定信息到提示中，并利用蒸馏（distillation）、压缩（compression）和聚合（aggregation）操作来更彻底地探索提示空间。这是一种非梯度（non-gradient）方法。", "result": "该方法在文本分类和生成任务的不同数据集上，使用t-lite-instruct-0.1语言模型进行了测试。结果显示，与现有方法（如Grips）相比，关键指标有显著的平均改进（例如，在整个数据集上平均提高了20.12%）。", "conclusion": "DistillPrompt被证明是自动提示领域中最有效的非梯度方法之一，在性能上超越了现有方法。"}}
{"id": "2508.18825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18825", "abs": "https://arxiv.org/abs/2508.18825", "authors": ["Yugo Kubota", "Seiichi Uchida"], "title": "Embedding Font Impression Word Tags Based on Co-occurrence", "comment": null, "summary": "Different font styles (i.e., font shapes) convey distinct impressions,\nindicating a close relationship between font shapes and word tags describing\nthose impressions. This paper proposes a novel embedding method for impression\ntags that leverages these shape-impression relationships. For instance, our\nmethod assigns similar vectors to impression tags that frequently co-occur in\norder to represent impressions of fonts, whereas standard word embedding\nmethods (e.g., BERT and CLIP) yield very different vectors. This property is\nparticularly useful for impression-based font generation and font retrieval.\nTechnically, we construct a graph whose nodes represent impression tags and\nwhose edges encode co-occurrence relationships. Then, we apply spectral\nembedding to obtain the impression vectors for each tag. We compare our method\nwith BERT and CLIP in qualitative and quantitative evaluations, demonstrating\nthat our approach performs better in impression-guided font generation.", "AI": {"tldr": "本文提出了一种新颖的印象标签嵌入方法，该方法利用字体形状与印象之间的关系，通过构建共现图并应用谱嵌入来生成印象向量，在印象引导的字体生成中优于BERT和CLIP。", "motivation": "不同的字体风格传达不同的印象，表明字体形状与描述这些印象的词语（印象标签）之间存在密切关系。然而，标准的词嵌入方法（如BERT和CLIP）在表示字体印象时，对经常共现的印象标签会产生非常不同的向量，这不利于基于印象的字体生成和检索。", "method": "该方法构建了一个图，其中节点代表印象标签，边编码了标签之间的共现关系。随后，对该图应用谱嵌入（spectral embedding）来获取每个标签的印象向量。", "result": "与BERT和CLIP不同，本方法能为频繁共现的印象标签分配相似的向量。定性和定量评估表明，该方法在印象引导的字体生成方面表现更好。", "conclusion": "所提出的基于印象标签共现关系和谱嵌入的印象向量生成方法，能够有效捕捉字体印象，并在印象引导的字体生成任务中优于现有标准方法，对印象引导的字体生成和检索具有实用价值。"}}
{"id": "2508.19096", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19096", "abs": "https://arxiv.org/abs/2508.19096", "authors": ["Yongwoo Song", "Minbyul Jeong", "Mujeen Sung"], "title": "Trustworthy Agents for Electronic Health Records through Confidence Estimation", "comment": null, "summary": "Large language models (LLMs) show promise for extracting information from\nElectronic Health Records (EHR) and supporting clinical decisions. However,\ndeployment in clinical settings faces challenges due to hallucination risks. We\npropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric\nquantifying the accuracy-reliability trade-off at varying confidence\nthresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating\nstepwise confidence estimation for clinical question answering. Experiments on\nMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under\nstrict reliability constraints, achieving improvements of 44.23%p and 25.34%p\nat HCAcc@70% while baseline methods fail at these thresholds. These results\nhighlight limitations of traditional accuracy metrics in evaluating healthcare\nAI agents. Our work contributes to developing trustworthy clinical agents that\ndeliver accurate information or transparently express uncertainty when\nconfidence is low.", "AI": {"tldr": "针对LLM在电子健康记录(EHR)信息提取中的幻觉风险，本文提出了HCAcc@k%指标和TrustEHRAgent，显著提升了临床问答的可靠性。", "motivation": "大型语言模型(LLMs)在从电子健康记录(EHR)中提取信息和支持临床决策方面展现出潜力，但其幻觉风险是临床部署面临的主要挑战。", "method": "本文提出了一种新颖的度量标准“Hallucination Controlled Accuracy at k% (HCAcc@k%)”，用于量化在不同置信度阈值下的准确性-可靠性权衡。同时，引入了TrustEHRAgent，一个置信度感知代理，它结合了逐步置信度估计来进行临床问答。", "result": "在MIMIC-III和eICU数据集上的实验表明，TrustEHRAgent在严格的可靠性约束下优于基线方法，在HCAcc@70%时分别实现了44.23%p和25.34%p的改进，而基线方法在这些阈值下均告失败。这些结果也突显了传统准确性指标在评估医疗AI代理方面的局限性。", "conclusion": "本研究致力于开发可信赖的临床代理，这些代理能够提供准确的信息，或在置信度较低时透明地表达不确定性，从而促进医疗领域AI的应用。"}}
{"id": "2508.19026", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19026", "abs": "https://arxiv.org/abs/2508.19026", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "title": "MovieCORE: COgnitive REasoning in Movies", "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "本文介绍了MovieCORE，一个用于电影内容深度认知理解的新型视频问答（VQA）数据集，该数据集通过多LLM智能体头脑风暴生成，并提出了一套认知测试来评估数据集质量，同时引入了Agentic Choice Enhancement（ACE）模块以提升现有视频语言模型的推理能力。", "motivation": "现有VQA数据集侧重于表面理解，缺乏对电影内容更深层次的认知理解（系统2思维）的探究。", "method": "本文提出了MovieCORE数据集，采用创新的智能体头脑风暴方法，利用多个大型语言模型（LLM）作为思维智能体来生成和完善高质量的问答对。为评估数据集质量，开发了一套评估深度、启发性和句法复杂度的认知测试。同时，提出了一套评估VQA模型在深度认知任务上表现的综合评估方案，并引入了Agentic Choice Enhancement (ACE) 智能体增强模块来提升现有视频语言模型的推理能力。", "result": "MovieCORE数据集强调与视频内容相关的、需要系统2思维的深度认知问题。通过智能体头脑风暴方法生成了高质量的问答对。所提出的ACE模块能够将模型训练后的推理能力提升高达25%。", "conclusion": "这项工作推动了AI系统在电影理解方面的发展，并为当前VQA模型在面对更具挑战性和细致入微的电影内容问题时的能力和局限性提供了宝贵的见解。"}}
{"id": "2508.18829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18829", "abs": "https://arxiv.org/abs/2508.18829", "authors": ["Takayuki Ishikawa", "Carmelo Bonannella", "Bas J. W. Lerink", "Marc Rußwurm"], "title": "Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory", "comment": null, "summary": "National Forest Inventory (NFI)s serve as the primary source of forest\ninformation, providing crucial tree species distribution data. However,\nmaintaining these inventories requires labor-intensive on-site campaigns.\nRemote sensing approaches, particularly when combined with machine learning,\noffer opportunities to update NFIs more frequently and at larger scales. While\nthe use of Satellite Image Time Series has proven effective for distinguishing\ntree species through seasonal canopy reflectance patterns, current approaches\nrely primarily on Random Forest classifiers with hand-designed features and\nphenology-based metrics. Using deep features from an available pre-trained\nremote sensing foundation models offers a complementary strategy. These\npre-trained models leverage unannotated global data and are meant to used for\ngeneral-purpose applications and can then be efficiently fine-tuned with\nsmaller labeled datasets for specific classification tasks. This work\nsystematically investigates how deep features improve tree species\nclassification accuracy in the Netherlands with few annotated data. Data-wise,\nwe extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites\ndata and SRTM data using Google Earth Engine. Our results demonstrate that\nfine-tuning a publicly available remote sensing time series foundation model\noutperforms the current state-of-the-art in NFI classification in the\nNetherlands by a large margin of up to 10% across all datasets. This\ndemonstrates that classic hand-defined harmonic features are too simple for\nthis task and highlights the potential of using deep AI features for\ndata-limited application like NFI classification. By leveraging openly\navailable satellite data and pre-trained models, this approach significantly\nimproves classification accuracy compared to traditional methods and can\neffectively complement existing forest inventory processes.", "AI": {"tldr": "本研究利用预训练的遥感基础模型中的深度特征，显著提高了荷兰国家森林清查（NFI）中树种分类的准确性，超越了传统方法。", "motivation": "国家森林清查（NFI）是获取森林信息的主要来源，但其维护工作耗时耗力。遥感结合机器学习为更频繁、更大规模地更新NFI提供了机会。尽管卫星图像时间序列在区分树种方面有效，但现有方法主要依赖于随机森林分类器和手工设计的特征。本研究旨在探索利用预训练遥感基础模型中的深度特征来改进树种分类，尤其是在标注数据有限的情况下。", "method": "研究系统地调查了深度特征如何用少量标注数据提高荷兰的树种分类精度。数据方面，利用Google Earth Engine提取了Sentinel-1、Sentinel-2、ERA5卫星数据以及SRTM数据的时间序列。通过对一个公开可用的遥感时间序列基础模型进行微调，并将其性能与当前NFI分类的最新技术进行了比较。", "result": "结果表明，对公开可用的遥感时间序列基础模型进行微调，在所有数据集上比荷兰当前NFI分类的最新技术高出高达10%的准确率。这表明经典的、手工定义的谐波特征对于此任务过于简单，突显了深度AI特征在NFI分类等数据受限应用中的巨大潜力。", "conclusion": "利用开放可用的卫星数据和预训练模型，本方法显著提高了分类准确性，优于传统方法，并能有效补充现有的森林清查流程。深度AI特征对于数据有限的应用（如NFI分类）具有巨大潜力。"}}
{"id": "2508.19097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19097", "abs": "https://arxiv.org/abs/2508.19097", "authors": ["Armin Berger", "Sarthak Khanna", "David Berghaus", "Rafet Sifa"], "title": "Reasoning LLMs in the Medical Domain: A Literature Survey", "comment": null, "summary": "The emergence of advanced reasoning capabilities in Large Language Models\n(LLMs) marks a transformative development in healthcare applications. Beyond\nmerely expanding functional capabilities, these reasoning mechanisms enhance\ndecision transparency and explainability-critical requirements in medical\ncontexts. This survey examines the transformation of medical LLMs from basic\ninformation retrieval tools to sophisticated clinical reasoning systems capable\nof supporting complex healthcare decisions. We provide a thorough analysis of\nthe enabling technological foundations, with a particular focus on specialized\nprompting techniques like Chain-of-Thought and recent breakthroughs in\nReinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates\npurpose-built medical frameworks while also examining emerging paradigms such\nas multi-agent collaborative systems and innovative prompting architectures.\nThe survey critically assesses current evaluation methodologies for medical\nvalidation and addresses persistent challenges in field interpretation\nlimitations, bias mitigation strategies, patient safety frameworks, and\nintegration of multimodal clinical data. Through this survey, we seek to\nestablish a roadmap for developing reliable LLMs that can serve as effective\npartners in clinical practice and medical research.", "AI": {"tldr": "本综述探讨了大型语言模型（LLMs）在医疗领域从信息检索到高级临床推理的变革性发展，分析了其技术基础、评估方法和挑战，并提出了开发可靠医疗LLMs的路线图。", "motivation": "LLMs先进的推理能力对医疗应用具有变革性意义，不仅扩展了功能，还增强了在医疗环境中至关重要的决策透明度和可解释性。", "method": "本综述审视了医疗LLMs的演变，深入分析了其技术基础（如Chain-of-Thought提示技术和强化学习突破如DeepSeek-R1），评估了专用医疗框架，考察了多智能体协作系统等新兴范式，并批判性评估了当前医疗验证的评估方法，同时探讨了领域解释限制、偏见缓解、患者安全和多模态数据整合等挑战。", "result": "本综述提供了对医疗LLMs技术基础、新兴范式和评估方法的全面分析，并识别了该领域面临的挑战，如解释限制、偏见缓解、患者安全框架和多模态临床数据整合。", "conclusion": "本综述旨在为开发可靠的LLMs提供一个路线图，使其能有效成为临床实践和医学研究中的合作伙伴。"}}
{"id": "2508.19076", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19076", "abs": "https://arxiv.org/abs/2508.19076", "authors": ["Ziyue Li", "Yuan Chang", "Gaihong Yu", "Xiaoqiu Le"], "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance", "comment": null, "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.", "AI": {"tldr": "HiPlan是一个分层规划框架，通过提供自适应的全局-局部指导，显著提升了大型语言模型（LLM）代理在复杂、长周期规划任务中的决策能力。", "motivation": "LLM代理在复杂、长周期规划任务中表现不佳，原因在于缺乏宏观指导导致迷失方向，以及执行过程中缺乏持续监督导致对环境变化反应迟钝和容易偏离。", "method": "HiPlan框架采用分层规划策略：1. 将复杂任务分解为里程碑行动指南（提供总体方向）和逐步提示（提供详细行动）。2. 在离线阶段，从专家演示中构建里程碑库，通过检索语义相似的任务和里程碑实现结构化经验复用。3. 在执行阶段，动态调整过去里程碑的轨迹片段，生成逐步提示，使当前观察与里程碑目标对齐，从而弥补差距和纠正偏差。", "result": "在两个具有挑战性的基准测试中，HiPlan显著优于强大的基线模型。消融研究也验证了其分层组件的互补优势。", "conclusion": "HiPlan通过提供自适应的全局-局部指导，成功解决了LLM代理在复杂、长周期规划任务中面临的挑战，有效提升了其决策能力和鲁棒性。"}}
{"id": "2508.18831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18831", "abs": "https://arxiv.org/abs/2508.18831", "authors": ["Yosuke Yamagishi", "Shouhei Hanaoka"], "title": "Automated Classification of Normal and Atypical Mitotic Figures Using ConvNeXt V2: MIDOG 2025 Track 2", "comment": "MIDOG 2025 solution", "summary": "This paper presents our solution for the MIDOG 2025 Challenge Track 2, which\nfocuses on binary classification of normal mitotic figures (NMFs) versus\natypical mitotic figures (AMFs) in histopathological images. Our approach\nleverages a ConvNeXt V2 base model with center cropping preprocessing and\n5-fold cross-validation ensemble strategy. The method addresses key challenges\nincluding severe class imbalance, high morphological variability, and domain\nheterogeneity across different tumor types, species, and scanners. Through\nstrategic preprocessing with 60% center cropping and mixed precision training,\nour model achieved robust performance on the diverse MIDOG 2025 dataset. The\nsolution demonstrates the effectiveness of modern convolutional architectures\nfor mitotic figure subtyping while maintaining computational efficiency through\ncareful architectural choices and training optimizations.", "AI": {"tldr": "本文提出了一种针对MIDOG 2025挑战赛Track 2的解决方案，利用ConvNeXt V2模型、中心裁剪预处理和5折交叉验证集成策略，实现了组织病理图像中正常与非典型有丝分裂像的二分类，并在解决类别不平衡、形态变异性和域异质性等挑战的同时，取得了稳健的性能。", "motivation": "该研究旨在解决MIDOG 2025挑战赛Track 2中，组织病理图像内正常有丝分裂像（NMFs）与非典型有丝分裂像（AMFs）的二分类问题，同时应对严重的类别不平衡、高形态变异性以及跨肿瘤类型、物种和扫描仪的域异质性等关键挑战。", "method": "本方法采用ConvNeXt V2作为基础模型，结合60%中心裁剪的图像预处理，并利用5折交叉验证集成策略。此外，还采用了混合精度训练以提高计算效率和模型性能，旨在有效处理类别不平衡、形态多样性和领域异质性问题。", "result": "该模型在多样化的MIDOG 2025数据集上取得了稳健的性能，成功地对有丝分裂像进行了亚型分类。", "conclusion": "该解决方案证明了现代卷积架构在有丝分裂像亚型分类方面的有效性，并通过精心选择的架构和训练优化，同时保持了计算效率。"}}
{"id": "2508.19113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19113", "abs": "https://arxiv.org/abs/2508.19113", "authors": ["Dayoon Ko", "Jihyuk Kim", "Haeju Park", "Sohyeon Kim", "Dahyun Lee", "Yongrae Jo", "Gunhee Kim", "Moontae Lee", "Kyungjae Lee"], "title": "Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) have demonstrated strong performance in\ncomplex, multi-step reasoning tasks. Existing methods enhance LRMs by\nsequentially integrating external knowledge retrieval; models iteratively\ngenerate queries, retrieve external information, and progressively reason over\nthis information. However, purely sequential querying increases inference\nlatency and context length, diminishing coherence and potentially reducing\naccuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search\nQA), a synthetic dataset automatically generated from Natural Questions,\nexplicitly designed to train LRMs to distinguish parallelizable from sequential\nqueries. HDS-QA comprises hybrid-hop questions that combine parallelizable\nindependent subqueries (executable simultaneously) and sequentially dependent\nsubqueries (requiring step-by-step resolution), along with synthetic\nreasoning-querying-retrieval paths involving parallel queries. We fine-tune an\nLRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms\nstate-of-the-art baselines across multiple benchmarks, notably achieving +15.9\nand +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both\nrequiring comprehensive and exhaustive search. Experimental results highlight\ntwo key advantages: HybridDeepSearcher reaches comparable accuracy with fewer\nsearch turns, significantly reducing inference latency, and it effectively\nscales as more turns are permitted. These results demonstrate the efficiency,\nscalability, and effectiveness of explicitly training LRMs to leverage hybrid\nparallel and sequential querying.", "AI": {"tldr": "本研究引入HDS-QA数据集和HybridDeepSearcher模型，旨在训练大型推理模型（LRMs）识别并利用混合（并行与序列）查询策略，以降低推理延迟并提高效率和准确性。", "motivation": "现有方法通过顺序集成外部知识检索来增强LRMs，但这导致推理延迟增加、上下文长度过长，并可能降低连贯性和准确性。", "method": "研究者创建了HDS-QA，一个从Natural Questions自动生成的合成数据集，专门用于训练LRMs区分可并行和序列依赖的子查询。HDS-QA包含混合跳数问题和涉及并行查询的合成推理-查询-检索路径。在此数据集上微调了一个LRM，命名为HybridDeepSearcher。", "result": "HybridDeepSearcher在多个基准测试中超越了现有最先进的基线，尤其在FanOutQA和BrowseComp子集上分别实现了+15.9和+11.5 F1的提升。实验结果表明，HybridDeepSearcher在更少的搜索轮次下达到相当的准确性，显著减少了推理延迟，并且在允许更多轮次时能有效扩展。", "conclusion": "这些结果证明，明确训练LRMs利用混合并行和序列查询策略，可以显著提高模型的效率、可扩展性和有效性。"}}
{"id": "2508.19077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19077", "abs": "https://arxiv.org/abs/2508.19077", "authors": ["Tom Röhr", "Soumyadeep Roy", "Fares Al Mohamad", "Jens-Michalis Papaioannou", "Wolfgang Nejdl", "Felix Gers", "Alexander Löser"], "title": "\"Where does it hurt?\" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues", "comment": "Accepted at ECAI 2025", "summary": "In a doctor-patient dialogue, the primary objective of physicians is to\ndiagnose patients and propose a treatment plan. Medical doctors guide these\nconversations through targeted questioning to efficiently gather the\ninformation required to provide the best possible outcomes for patients. To the\nbest of our knowledge, this is the first work that studies physician intent\ntrajectories in doctor-patient dialogues. We use the `Ambient Clinical\nIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with\nmedical professionals to develop a fine-grained taxonomy of physician intents\nbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). We\nthen conduct a large-scale annotation effort to label over 5000 doctor-patient\nturns with the help of a large number of medical experts recruited using\nProlific, a popular crowd-sourcing platform. This large labeled dataset is an\nimportant resource contribution that we use for benchmarking the\nstate-of-the-art generative and encoder models for medical intent\nclassification tasks. Our findings show that our models understand the general\nstructure of medical dialogues with high accuracy, but often fail to identify\ntransitions between SOAP categories. We also report for the first time common\ntrajectories in medical dialogue structures that provide valuable insights for\ndesigning `differential diagnosis' systems. Finally, we extensively study the\nimpact of intent filtering for medical dialogue summarization and observe a\nsignificant boost in performance. We make the codes and data, including\nannotation guidelines, publicly available at\nhttps://github.com/DATEXIS/medical-intent-classification.", "AI": {"tldr": "本研究首次探讨了医患对话中医生的意图轨迹，基于SOAP框架开发了细粒度分类法，并进行了大规模标注。通过对现有模型进行基准测试，揭示了对话结构洞察，并显著提升了医疗对话摘要的性能。", "motivation": "医生在医患对话中通过有针对性的提问来高效收集信息，以实现诊断和治疗目标。然而，据作者所知，此前没有研究系统地分析医生在对话中的意图轨迹。理解这些意图对于开发更有效的医疗AI系统（如鉴别诊断系统）至关重要。", "method": "研究使用了Aci-bench数据集，并与医学专业人士合作，基于SOAP框架（主观、客观、评估、计划）开发了细粒度的医生意图分类法。通过众包平台Prolific招募大量医学专家，对超过5000个医患对话轮次进行了大规模标注。随后，利用这个标注数据集对最先进的生成式和编码器模型在医疗意图分类任务上进行了基准测试。此外，还首次报告了医疗对话中的常见轨迹，并广泛研究了意图过滤对医疗对话摘要性能的影响。所有代码和数据（包括标注指南）均已公开。", "result": "研究发现，模型能够高精度地理解医疗对话的整体结构，但在识别SOAP类别之间的转换时常出现问题。首次报告了医疗对话结构中的常见轨迹，为设计“鉴别诊断”系统提供了有价值的见解。同时，观察到意图过滤显著提升了医疗对话摘要的性能。", "conclusion": "本研究首次对医患对话中的医生意图轨迹进行了探索，创建了一个基于SOAP框架的细粒度意图分类法和大规模标注数据集。研究不仅对现有模型进行了基准测试，还揭示了医疗对话结构的关键洞察，并证明了意图过滤在对话摘要中的有效性，为未来设计更智能的医疗AI系统奠定了基础。"}}
{"id": "2508.18834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18834", "abs": "https://arxiv.org/abs/2508.18834", "authors": ["Zizheng Guo", "Bochao Zou", "Yinuo Jia", "Xiangyu Li", "Huimin Ma"], "title": "Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression", "comment": null, "summary": "Micro-expressions (MEs) are involuntary, low-intensity, and short-duration\nfacial expressions that often reveal an individual's genuine thoughts and\nemotions. Most existing ME analysis methods rely on window-level classification\nwith fixed window sizes and hard decisions, which limits their ability to\ncapture the complex temporal dynamics of MEs. Although recent approaches have\nadopted video-level regression frameworks to address some of these challenges,\ninterval decoding still depends on manually predefined, window-based methods,\nleaving the issue only partially mitigated. In this paper, we propose a\nprior-guided video-level regression method for ME analysis. We introduce a\nscalable interval selection strategy that comprehensively considers the\ntemporal evolution, duration, and class distribution characteristics of MEs,\nenabling precise spotting of the onset, apex, and offset phases. In addition,\nwe introduce a synergistic optimization framework, in which the spotting and\nrecognition tasks share parameters except for the classification heads. This\nfully exploits complementary information, makes more efficient use of limited\ndata, and enhances the model's capability. Extensive experiments on multiple\nbenchmark datasets demonstrate the state-of-the-art performance of our method,\nwith an STRS of 0.0562 on CAS(ME)$^3$ and 0.2000 on SAMMLV. The code is\navailable at https://github.com/zizheng-guo/BoostingVRME.", "AI": {"tldr": "本文提出了一种先验引导的视频级回归方法，用于微表情分析。该方法通过可伸缩区间选择策略和协同优化框架，实现了微表情的精确识别和定位，并在多个基准数据集上取得了最先进的性能。", "motivation": "现有微表情分析方法依赖于固定窗口大小和硬决策的窗口级分类，难以捕捉复杂的时序动态。尽管视频级回归框架有所改进，但区间解码仍依赖手动预定义的窗口方法，问题未能完全解决。", "method": "本文提出了一种先验引导的视频级回归方法。核心方法包括：1) 可伸缩区间选择策略，综合考虑微表情的时序演变、持续时间和类别分布，以精确识别起始、顶点和结束阶段；2) 协同优化框架，其中微表情定位和识别任务共享参数（分类头除外），以充分利用互补信息并提高模型能力。", "result": "在多个基准数据集上的广泛实验表明，本文方法达到了最先进的性能，例如在CAS(ME)³数据集上的STRS为0.0562，在SAMMLV数据集上为0.2000。", "conclusion": "本文提出的先验引导视频级回归方法，结合可伸缩区间选择和协同优化框架，有效解决了微表情分析中时序动态捕捉和区间解码的挑战，显著提升了微表情的定位和识别性能。"}}
{"id": "2508.19149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19149", "abs": "https://arxiv.org/abs/2508.19149", "authors": ["Claudio Battiloro", "Pietro Greiner", "Bret Nestor", "Oumaima Amezgar", "Francesca Dominici"], "title": "Algorithmic Collective Action with Multiple Collectives", "comment": "12 pages", "summary": "As learning systems increasingly influence everyday decisions, user-side\nsteering via Algorithmic Collective Action (ACA)-coordinated changes to shared\ndata-offers a complement to regulator-side policy and firm-side model design.\nAlthough real-world actions have been traditionally decentralized and\nfragmented into multiple collectives despite sharing overarching\nobjectives-with each collective differing in size, strategy, and actionable\ngoals, most of the ACA literature focused on single collective settings. In\nthis work, we present the first theoretical framework for ACA with multiple\ncollectives acting on the same system. In particular, we focus on collective\naction in classification, studying how multiple collectives can plant signals,\ni.e., bias a classifier to learn an association between an altered version of\nthe features and a chosen, possibly overlapping, set of target classes. We\nprovide quantitative results about the role and the interplay of collectives'\nsizes and their alignment of goals. Our framework, by also complementing\nprevious empirical results, opens a path for a holistic treatment of ACA with\nmultiple collectives.", "AI": {"tldr": "本文提出了首个针对多集体算法集体行动（ACA）的理论框架，研究了在分类系统中，不同规模和目标一致性的集体如何通过植入信号来影响分类器，并量化了其作用和相互作用。", "motivation": "现有学习系统日益影响日常决策，用户侧的算法集体行动（ACA）是对监管侧政策和企业侧模型设计的补充。然而，尽管现实世界中的行动通常是去中心化且分散到多个集体中，但大多数ACA文献都集中在单一集体设置，缺乏对多集体情境的理论分析。", "method": "本文提出了一个针对多集体算法集体行动的理论框架，特别关注分类系统中的集体行动。研究了多个集体如何通过“植入信号”（即偏置分类器以学习特征的改变版本与目标类别之间的关联）来影响分类器。量化分析了集体规模及其目标一致性在其中的作用和相互作用。", "result": "提供了关于集体规模及其目标一致性在多集体ACA中作用和相互作用的定量结果。该框架补充了以往的实证结果。", "conclusion": "本工作通过提供首个多集体ACA的理论框架，为全面处理多集体算法集体行动开辟了道路，并揭示了不同集体规模和目标一致性在影响分类器中的关键作用和相互作用。"}}
{"id": "2508.19089", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19089", "abs": "https://arxiv.org/abs/2508.19089", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs", "comment": "Accepted by EMNLP 2025", "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.", "AI": {"tldr": "本文首次全面分析了大型语言模型（LLMs）如何通过上下文学习（ICL）或参数高效微调（PEFT）来支持极低资源语言，发现零样本ICL结合语言对齐对极低资源语言非常有效，而PEFT在语言及其文字极度未被LLM代表时表现受限。", "motivation": "大型语言模型（LLMs）对极低资源语言，特别是那些使用稀有文字的语言，支持不足，主要原因是训练数据匮乏。", "method": "研究对20种代表性不足的语言，在三个最先进的多语言LLMs上进行了系统评估。方法包括：纯粹的上下文学习（ICL），带或不带辅助对齐信号的ICL，以及参数高效微调（PEFT）。", "result": "研究发现，当语言及其文字在LLM中极度未被代表时，PEFT表现受限。相比之下，结合语言对齐的零样本ICL对极低资源语言非常有效。对于LLM中相对较好代表的语言，少量样本ICL或PEFT更为有益。", "conclusion": "为处理极低资源语言的LLM实践者提供了指导方针，例如，应避免对多语言模型在未见过的文字语言上进行微调。零样本ICL结合语言对齐是处理极低资源语言的有效策略。"}}
{"id": "2508.18836", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18836", "abs": "https://arxiv.org/abs/2508.18836", "authors": ["Luyin Hu", "Soheil Gholami", "George Dindelegan", "Torstein R. Meling", "Aude Billard"], "title": "Quantitative Outcome-Oriented Assessment of Microsurgical Anastomosis", "comment": "7 pages, 7 figures, accepted at EMBC2025", "summary": "Microsurgical anastomosis demands exceptional dexterity and visuospatial\nskills, underscoring the importance of comprehensive training and precise\noutcome assessment. Currently, methods such as the outcome-oriented anastomosis\nlapse index are used to evaluate this procedure. However, they often rely on\nsubjective judgment, which can introduce biases that affect the reliability and\nefficiency of the assessment of competence. Leveraging three datasets from\nhospitals with participants at various levels, we introduce a quantitative\nframework that uses image-processing techniques for objective assessment of\nmicrosurgical anastomoses. The approach uses geometric modeling of errors along\nwith a detection and scoring mechanism, enhancing the efficiency and\nreliability of microsurgical proficiency assessment and advancing training\nprotocols. The results show that the geometric metrics effectively replicate\nexpert raters' scoring for the errors considered in this work.", "AI": {"tldr": "本文提出了一种基于图像处理的定量框架，用于客观评估显微外科吻合术，通过几何建模和评分机制，提高评估效率和可靠性，并有效复现专家评分。", "motivation": "显微外科吻合术的评估方法（如吻合术失误指数）目前依赖主观判断，这引入了偏见，影响了能力评估的可靠性和效率，因此需要更客观的评估方法。", "method": "研究利用来自三家医院、包含不同水平参与者的数据集，引入了一个基于图像处理技术的定量框架。该方法采用错误几何建模以及检测和评分机制，以实现显微外科吻合术的客观评估。", "result": "结果表明，所提出的几何指标能够有效复现专家评分员对本研究中考虑的错误的评分。", "conclusion": "该定量框架提高了显微外科熟练度评估的效率和可靠性，并有助于改进培训方案。"}}
{"id": "2508.19152", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.19152", "abs": "https://arxiv.org/abs/2508.19152", "authors": ["Chiu-Chou Lin"], "title": "Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games", "comment": "PhD Dissertation, National Yang Ming Chiao Tung University, 2025.\n  This is the public version without Chinese abstract or postscript", "summary": "Contemporary artificial intelligence (AI) development largely centers on\nrational decision-making, valued for its measurability and suitability for\nobjective evaluation. Yet in real-world contexts, an intelligent agent's\ndecisions are shaped not only by logic but also by deeper influences such as\nbeliefs, values, and preferences. The diversity of human decision-making styles\nemerges from these differences, highlighting that \"style\" is an essential but\noften overlooked dimension of intelligence.\n  This dissertation introduces playstyle as an alternative lens for observing\nand analyzing the decision-making behavior of intelligent agents, and examines\nits foundational meaning and historical context from a philosophical\nperspective. By analyzing how beliefs and values drive intentions and actions,\nwe construct a two-tier framework for style formation: the external interaction\nloop with the environment and the internal cognitive loop of deliberation. On\nthis basis, we formalize style-related characteristics and propose measurable\nindicators such as style capacity, style popularity, and evolutionary dynamics.\n  The study focuses on three core research directions: (1) Defining and\nmeasuring playstyle, proposing a general playstyle metric based on discretized\nstate spaces, and extending it to quantify strategic diversity and competitive\nbalance; (2) Expressing and generating playstyle, exploring how reinforcement\nlearning and imitation learning can be used to train agents exhibiting specific\nstylistic tendencies, and introducing a novel approach for human-like style\nlearning and modeling; and (3) Practical applications, analyzing the potential\nof these techniques in domains such as game design and interactive\nentertainment.\n  Finally, the dissertation outlines future extensions, including the role of\nstyle as a core element in building artificial general intelligence (AGI).", "AI": {"tldr": "本论文引入“游戏风格”（playstyle）作为观察和分析智能体决策行为的新视角，并从哲学层面探讨其意义。论文构建了一个风格形成的两层框架，提出了可测量的风格指标，并围绕定义与测量、表达与生成以及实际应用三个核心方向展开研究，最终展望了风格在通用人工智能（AGI）中的核心作用。", "motivation": "当代人工智能（AI）主要关注可衡量和客观评估的理性决策。然而，在现实世界中，智能体的决策不仅受逻辑影响，还受信念、价值观和偏好等深层因素塑造，这些因素导致了人类决策风格的多样性。因此，“风格”是智能中一个重要但常被忽视的维度。", "method": "本研究从哲学角度探讨了“游戏风格”的含义和历史背景。通过分析信念和价值观如何驱动意图和行动，构建了一个包含外部交互循环和内部认知循环的两层风格形成框架。在此基础上，论文形式化了与风格相关的特征，并提出了风格能力、风格流行度和演化动力学等可测量指标。具体研究方向包括：1) 基于离散状态空间定义和测量通用游戏风格指标，并扩展量化战略多样性和竞争平衡；2) 探索如何利用强化学习和模仿学习训练具有特定风格倾向的智能体，并引入一种新颖的类人风格学习和建模方法；3) 分析这些技术在游戏设计和互动娱乐等领域的潜在应用。", "result": "论文提出了基于离散状态空间的通用游戏风格度量标准，并将其扩展用于量化战略多样性和竞争平衡。研究探索了如何利用强化学习和模仿学习来训练展现特定风格倾向的智能体，并引入了一种新颖的类人风格学习和建模方法。此外，论文还分析了这些技术在游戏设计和互动娱乐等领域的实际应用潜力。", "conclusion": "本论文强调了“风格”是智能中一个重要但常被忽视的维度，通过引入“游戏风格”概念、构建风格形成框架和提出可测量指标，深入研究了其定义、测量、表达、生成及应用。最终，论文指出风格将是构建通用人工智能（AGI）的核心要素。"}}
{"id": "2508.19093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19093", "abs": "https://arxiv.org/abs/2508.19093", "authors": ["Mathew Henrickson"], "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index", "comment": null, "summary": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research.", "AI": {"tldr": "本研究提出一个用于艺术品出处研究的检索增强生成（RAG）框架，利用自然语言和多语言搜索功能，有效处理碎片化的档案数据。", "motivation": "艺术品出处研究对于验证真伪、支持索赔和理解历史背景至关重要，但现有搜索工具因数据碎片化、多语言性和对精确元数据的依赖而效率低下。", "method": "开发了一个RAG框架，通过语义检索和上下文摘要实现自然语言和多语言搜索。该方法在Getty Provenance Index - German Sales的10,000条拍卖记录样本上进行了评估。", "result": "该方法成功检索并总结了拍卖记录，证明了其在处理艺术市场档案方面的可扩展性。", "conclusion": "RAG框架为历史学家和文化遗产专业人士进行敏感的出处研究提供了一个实用且可扩展的工具。"}}
{"id": "2508.18859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18859", "abs": "https://arxiv.org/abs/2508.18859", "authors": ["Muhammad Kashif Ali", "Eun Woo Im", "Dongjin Kim", "Tae Hyun Kim", "Vivek Gupta", "Haonan Luo", "Tianrui Li"], "title": "Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization", "comment": null, "summary": "Video stabilization remains a fundamental problem in computer vision,\nparticularly pixel-level synthesis solutions for video stabilization, which\nsynthesize full-frame outputs, add to the complexity of this task. These\nmethods aim to enhance stability while synthesizing full-frame videos, but the\ninherent diversity in motion profiles and visual content present in each video\nsequence makes robust generalization with fixed parameters difficult. To\naddress this, we present a novel method that improves pixel-level synthesis\nvideo stabilization methods by rapidly adapting models to each input video at\ntest time. The proposed approach takes advantage of low-level visual cues\navailable during inference to improve both the stability and visual quality of\nthe output. Notably, the proposed rapid adaptation achieves significant\nperformance gains even with a single adaptation pass. We further propose a jerk\nlocalization module and a targeted adaptation strategy, which focuses the\nadaptation on high-jerk segments for maximizing stability with fewer adaptation\nsteps. The proposed methodology enables modern stabilizers to overcome the\nlongstanding SOTA approaches while maintaining the full frame nature of the\nmodern methods, while offering users with control mechanisms akin to classical\napproaches. Extensive experiments on diverse real-world datasets demonstrate\nthe versatility of the proposed method. Our approach consistently improves the\nperformance of various full-frame synthesis models in both qualitative and\nquantitative terms, including results on downstream applications.", "AI": {"tldr": "本文提出了一种新颖的方法，通过在测试时快速适应每个输入视频，显著改进了像素级合成视频稳定方法，利用低级视觉线索并结合抖动定位模块进行有针对性的适应，从而提升了稳定性与视觉质量。", "motivation": "像素级合成视频稳定方法由于视频序列中多样的运动模式和视觉内容，难以通过固定参数实现鲁棒的泛化。", "method": "提出了一种在测试时快速适应模型到每个输入视频的新方法，利用推理过程中可用的低级视觉线索。此外，还提出了一个抖动定位模块和有针对性的适应策略，将适应集中在高抖动片段上，以在更少的适应步骤中最大化稳定性。", "result": "即使通过单次适应，也能实现显著的性能提升。该方法使现代稳定器能够超越长期以来的SOTA方法，同时保持全帧特性，并提供类似于经典方法的用户控制机制。在多样化的真实世界数据集上的广泛实验表明，该方法在定性和定量方面持续改进了各种全帧合成模型的性能，包括在下游应用中的表现。", "conclusion": "所提出的快速适应方法，特别是结合抖动定位和有针对性适应策略，有效解决了像素级合成视频稳定方法的泛化难题，显著提升了视频的稳定性和视觉质量，并为用户提供了更好的控制，使现代稳定器能够达到新的技术水平。"}}
{"id": "2508.19163", "categories": ["cs.AI", "cs.HC", "cs.MA", "68T50, 68T42, 92C50, 68Q60", "I.2.0; J.3"], "pdf": "https://arxiv.org/pdf/2508.19163", "abs": "https://arxiv.org/abs/2508.19163", "authors": ["Ernest Lim", "Yajie Vera He", "Jared Joselowitz", "Kate Preston", "Mohita Chowdhury", "Louis Williams", "Aisling Higham", "Katrina Mason", "Mariane Melo", "Tom Lawton", "Yan Jia", "Ibrahim Habli"], "title": "MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation", "comment": "36 pages, 16 figures", "summary": "Despite the growing use of large language models (LLMs) in clinical dialogue\nsystems, existing evaluations focus on task completion or fluency, offering\nlittle insight into the behavioral and risk management requirements essential\nfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion\nfRamework for safe Interactions and conteXtual clinical conversational\nevaluation), a structured, extensible framework for safety-oriented evaluation\nof clinical dialogue agents.\n  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical\nscenarios, expected system behaviors and failure modes derived through\nstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluator\nfor detecting safety-relevant dialogue failures, validated against expert\nclinician annotations; and (3) PatBot, a simulated patient agent capable of\nproducing diverse, scenario-conditioned responses, evaluated for realism and\nbehavioral fidelity with human factors expertise, and a patient-preference\nstudy.\n  Across three experiments, we show that MATRIX enables systematic, scalable\nsafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard\ndetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded\nassessment of 240 dialogues. We also conducted one of the first realism\nanalyses of LLM-based patient simulation, showing that PatBot reliably\nsimulates realistic patient behavior in quantitative and qualitative\nevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking\nfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios\nand 10 clinical domains.\n  MATRIX is the first framework to unify structured safety engineering with\nscalable, validated conversational AI evaluation, enabling regulator-aligned\nsafety auditing. We release all evaluation tools, prompts, structured\nscenarios, and datasets.", "AI": {"tldr": "本文提出了MATRIX，一个用于临床对话系统安全评估的结构化、可扩展的多智能体模拟框架，旨在解决现有评估在安全行为和风险管理方面的不足。", "motivation": "尽管大型语言模型（LLMs）在临床对话系统中应用日益广泛，但现有评估主要关注任务完成或流畅性，未能深入洞察安全关键系统所需的行为和风险管理要求。", "method": "MATRIX框架整合了三个核心组件：1) 基于安全工程方法构建的临床场景、预期系统行为和故障模式的安全对齐分类法；2) BehvJudge，一个基于LLM的评估器，用于检测安全相关的对话故障，并经过临床专家标注验证；3) PatBot，一个模拟患者智能体，能够生成多样化、情境化的响应，其真实性和行为忠实度经过人类因素专家评估和患者偏好研究。", "result": "MATRIX实现了系统化、可扩展的安全评估。BehvJudge结合Gemini 2.5-Pro在危害检测方面达到了专家级水平（F1 0.96，敏感性0.999），在240个对话的盲测中表现优于临床医生。PatBot在定量和定性评估中可靠地模拟了真实的患者行为。MATRIX成功用于基准测试五个LLM智能体，涵盖14个危害场景和10个临床领域的2,100个模拟对话。", "conclusion": "MATRIX是首个将结构化安全工程与可扩展、经过验证的对话式AI评估相结合的框架，实现了与监管机构对齐的安全审计。该框架的所有评估工具、提示、结构化场景和数据集均已发布。"}}
{"id": "2508.19099", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19099", "abs": "https://arxiv.org/abs/2508.19099", "authors": ["Thomas Compton"], "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic", "comment": "5 pages conference paper, 4 tables", "summary": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub.", "AI": {"tldr": "本文提出了一种混合、透明的定量话语分析（QDA）框架，结合词汇和语义方法，利用Python工具实现可复现、可解释的分析，强调代码透明度和研究者能动性。", "motivation": "现有QDA软件（如MAXQDA、NVivo）的“黑箱”特性损害了方法论透明度，且难以与研究目标对齐，促使研究者寻求更透明、可控的分析方法。", "method": "研究采用自定义Python管道（NLTK、spaCy、Sentence Transformers）进行预处理、词形还原和嵌入生成。通过迭代的BERTopic建模过程（包含UMAP降维、HDBSCAN聚类和c-TF-IDF关键词提取），并进行参数调优和多次运行以优化主题连贯性和覆盖率。该方法将精确的词汇搜索与上下文感知的语义聚类相结合，实现了多层次分析。", "result": "该框架通过结合词汇和语义方法，实现了对数据处理的精细控制，增强了分析的三角验证、可复现性和可解释性。它弥补了单一方法在孤立使用时的局限性，并在一项历史政治话语案例研究中得到了验证。", "conclusion": "研究强调了在计算话语研究中，代码层面的透明度、研究者能动性以及方法论三角验证的重要性，为QDA提供了一个更开放、可控的解决方案。"}}
{"id": "2508.18886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18886", "abs": "https://arxiv.org/abs/2508.18886", "authors": ["Yuexuan Xia", "Benteng Ma", "Jiang He", "Zhiyong Wang", "Qi Dou", "Yong Xia"], "title": "Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models", "comment": null, "summary": "Ensuring fairness across demographic groups in medical diagnosis is essential\nfor equitable healthcare, particularly under distribution shifts caused by\nvariations in imaging equipment and clinical practice. Vision-language models\n(VLMs) exhibit strong generalization, and text prompts encode identity\nattributes, enabling explicit identification and removal of sensitive\ndirections. However, existing debiasing approaches typically address vision and\ntext modalities independently, leaving residual cross-modal misalignment and\nfairness gaps. To address this challenge, we propose DualFairVL, a multimodal\nprompt-learning framework that jointly debiases and aligns cross-modal\nrepresentations. DualFairVL employs a parallel dual-branch architecture that\nseparates sensitive and target attributes, enabling disentangled yet aligned\nrepresentations across modalities. Approximately orthogonal text anchors are\nconstructed via linear projections, guiding cross-attention mechanisms to\nproduce fused features. A hypernetwork further disentangles attribute-related\ninformation and generates instance-aware visual prompts, which encode\ndual-modal cues for fairness and robustness. Prototype-based regularization is\napplied in the visual branch to enforce separation of sensitive features and\nstrengthen alignment with textual anchors. Extensive experiments on eight\nmedical imaging datasets across four modalities show that DualFairVL achieves\nstate-of-the-art fairness and accuracy under both in- and out-of-distribution\nsettings, outperforming full fine-tuning and parameter-efficient baselines with\nonly 3.6M trainable parameters. Code will be released upon publication.", "AI": {"tldr": "DualFairVL是一个多模态提示学习框架，通过联合去偏和对齐跨模态表示，显著提高了医学诊断在分布偏移下的公平性和准确性，且参数高效。", "motivation": "在医学诊断中，确保跨人群的公平性至关重要，尤其是在成像设备和临床实践变化导致的分布偏移下。现有去偏方法通常独立处理视觉和文本模态，导致残余的跨模态未对齐和公平性差距，需要一种能联合处理并对齐多模态表示的方法。", "method": "DualFairVL采用并行双分支架构来分离敏感和目标属性，实现跨模态的解耦但对齐的表示。它通过线性投影构建近似正交的文本锚点以引导交叉注意力，并使用超网络生成编码双模态线索的实例感知视觉提示。此外，视觉分支应用基于原型的正则化，以强制分离敏感特征并加强与文本锚点的对齐。", "result": "在八个医学影像数据集（涵盖四种模态）上进行的大量实验表明，DualFairVL在分布内和分布外设置下均实现了最先进的公平性和准确性。它优于全微调和参数高效基线，且仅需3.6M可训练参数。", "conclusion": "DualFairVL通过其创新的多模态提示学习框架，成功解决了医学诊断中公平性与跨模态对齐的挑战，显著提升了在不同分布设置下的公平性和准确性，为公平医疗保健提供了有效且参数高效的解决方案。"}}
{"id": "2508.19200", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19200", "abs": "https://arxiv.org/abs/2508.19200", "authors": ["Xinran Zhao", "Boyuan Zheng", "Chenglei Si", "Haofei Yu", "Ken Liu", "Runlong Zhou", "Ruochen Li", "Tong Chen", "Xiang Li", "Yiming Zhang", "Tongshuang Wu"], "title": "The Ramon Llull's Thinking Machine for Automated Ideation", "comment": "21 pages, 3 figures", "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.", "AI": {"tldr": "本文重新审视了中世纪的拉蒙·卢尔的“组合艺术”，并以此为概念基础，构建了一个现代的、基于大型语言模型（LLM）的“卢尔思维机器”，用于研究构思。", "motivation": "研究的动机是利用卢尔的组合知识生成框架，结合现代AI技术，为科学研究提供一种增强创造力、生成新想法的工具，并探索人机协作构思的潜力。", "method": "该方法定义了三个组合轴：主题（如效率、适应性）、领域（如问答、机器翻译）和方法（如对抗训练、线性注意力）。这些元素从人类专家或会议论文中提取。通过使用精心策划的组合提示大型语言模型，来生成研究想法。", "result": "结果表明，通过这种方法，大型语言模型生成的研发想法具有多样性、相关性，并以现有文献为基础。所构建的现代思维机器是一个轻量级、可解释的工具。", "conclusion": "该研究提供了一个增强科学创造力的工具，并为人类与AI之间的协作构思指明了一条道路。"}}
{"id": "2508.19111", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19111", "abs": "https://arxiv.org/abs/2508.19111", "authors": ["Zhikai Ding", "Shiyu Ni", "Keping Bi"], "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs", "comment": "EMNLP2025 Findings", "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.", "AI": {"tldr": "本文研究了大型视觉语言模型（LVLMs）对其知识边界的感知能力，评估了三种置信度信号，发现LVLMs具有一定感知能力但仍有提升空间。概率和一致性置信度更可靠，并提出了有效的校准方法。LVLMs相较于LLMs在感知方面表现更优。", "motivation": "大型视觉语言模型（LVLMs）在视觉问答（VQA）方面表现出色，但存在幻觉问题。一个可靠的模型应该能够感知其知识边界，即知道自己知道什么和不知道什么。因此，研究LVLMs对其知识边界的感知能力至关重要。", "method": "本文通过评估三种类型的置信度信号来研究LVLMs的知识边界感知：概率置信度、基于答案一致性的置信度以及口头表达的置信度。实验在三个LVLMs和三个VQA数据集上进行。此外，本文还从大型语言模型（LLMs）中借鉴并调整了几种已有的置信度校准方法，并提出了三种新的有效方法。最后，将LVLMs与其LLM对应模型进行了比较。", "result": "实验结果表明，尽管LVLMs具有合理的感知水平，但仍有很大的改进空间。在三种置信度信号中，概率置信度和基于一致性的信号是更可靠的指标，而口头表达的置信度通常会导致过度自信。本文提出的校准方法是有效的。与LLMs相比，LVLMs联合处理视觉和文本输入虽然降低了问答性能，但却降低了置信度，从而提高了感知水平。", "conclusion": "LVLMs对其知识边界具有一定感知能力，但存在显著的改进空间。概率和一致性置信度信号是更可靠的感知指标。通过适应和提出置信度校准方法，可以有效增强LVLMs的感知能力。此外，LVLMs在感知水平上优于其LLM对应模型，这表明多模态处理在提高模型自知能力方面具有优势。"}}
{"id": "2508.18896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18896", "abs": "https://arxiv.org/abs/2508.18896", "authors": ["Zhehao Li", "Chong Wang", "Yi Chen", "Yinghao Lu", "Jiangbo Qian", "Jiong Wang", "Jiafei Wu"], "title": "DQEN: Dual Query Enhancement Network for DETR-based HOI Detection", "comment": null, "summary": "Human-Object Interaction (HOI) detection focuses on localizing human-object\npairs and recognizing their interactions. Recently, the DETR-based framework\nhas been widely adopted in HOI detection. In DETR-based HOI models, queries\nwith clear meaning are crucial for accurately detecting HOIs. However, prior\nworks have typically relied on randomly initialized queries, leading to vague\nrepresentations that limit the model's effectiveness. Meanwhile, humans in the\nHOI categories are fixed, while objects and their interactions are variable.\nTherefore, we propose a Dual Query Enhancement Network (DQEN) to enhance object\nand interaction queries. Specifically, object queries are enhanced with\nobject-aware encoder features, enabling the model to focus more effectively on\nhumans interacting with objects in an object-aware way. On the other hand, we\ndesign a novel Interaction Semantic Fusion module to exploit the HOI candidates\nthat are promoted by the CLIP model. Semantic features are extracted to enhance\nthe initialization of interaction queries, thereby improving the model's\nability to understand interactions. Furthermore, we introduce an Auxiliary\nPrediction Unit aimed at improving the representation of interaction features.\nOur proposed method achieves competitive performance on both the HICO-Det and\nthe V-COCO datasets. The source code is available at\nhttps://github.com/lzzhhh1019/DQEN.", "AI": {"tldr": "本文提出了一种双重查询增强网络（DQEN），通过利用目标感知编码器特征和CLIP模型促进的交互语义融合来增强目标和交互查询，从而改进基于DETR的人-物交互（HOI）检测。", "motivation": "现有基于DETR的HOI模型通常依赖随机初始化的查询，导致表示模糊，限制了模型有效性。此外，HOI中人是固定的，而物体和交互是可变的，需要更精准地处理目标和交互查询。", "method": "本文提出了双重查询增强网络（DQEN）。具体来说，目标查询通过目标感知编码器特征进行增强，使其更有效地关注与物体交互的人。同时，设计了一个新颖的交互语义融合模块，利用CLIP模型提升的HOI候选，提取语义特征以增强交互查询的初始化。此外，引入了一个辅助预测单元来改进交互特征的表示。", "result": "所提出的方法在HICO-Det和V-COCO数据集上均取得了有竞争力的性能。", "conclusion": "DQEN通过对目标和交互查询进行有意义的增强，显著提升了基于DETR的HOI检测模型的有效性，解决了传统随机初始化查询带来的表示模糊问题。"}}
{"id": "2508.19218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19218", "abs": "https://arxiv.org/abs/2508.19218", "authors": ["Yufei Wu", "Manuel R. Torres", "Parisa Zehtabi", "Alberto Pozanco Lancho", "Michael Cashmore", "Daniel Borrajo", "Manuela Veloso"], "title": "The Subset Sum Matching Problem", "comment": "Paper accepted at ECAI 2025. This is an extended version that\n  includes Supplementary Material", "summary": "This paper presents a new combinatorial optimisation task, the Subset Sum\nMatching Problem (SSMP), which is an abstraction of common financial\napplications such as trades reconciliation. We present three algorithms, two\nsuboptimal and one optimal, to solve this problem. We also generate a benchmark\nto cover different instances of SSMP varying in complexity, and carry out an\nexperimental evaluation to assess the performance of the approaches.", "AI": {"tldr": "本论文提出了子集和匹配问题（SSMP），一种抽象自金融应用的组合优化任务，并提供了三种算法（两种次优、一种最优）来解决它，同时通过基准测试进行了性能评估。", "motivation": "解决金融应用（如交易对账）中常见的组合优化任务，通过抽象出子集和匹配问题（SSMP）来应对。", "method": "提出了三种算法来解决SSMP，包括两种次优算法和一种最优算法。同时，生成了一个涵盖不同复杂度的SSMP实例的基准，并进行了实验评估以衡量这些方法的性能。", "result": "通过实验评估，衡量了所提出的三种算法在不同复杂度的SSMP实例上的性能。", "conclusion": "论文成功定义了子集和匹配问题，并提供了多种算法解决方案，并通过严格的实验评估分析了这些方法的性能表现。"}}
{"id": "2508.19202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19202", "abs": "https://arxiv.org/abs/2508.19202", "authors": ["Alan Li", "Yixin Liu", "Arpan Sarkar", "Doug Downey", "Arman Cohan"], "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning", "comment": "28 pages, 16 figures", "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.", "AI": {"tldr": "本文引入了科学推理基准SciReas/SciReas-Pro和KRUX探究框架，深入分析了大型语言模型（LLM）在科学推理中知识与推理的独特作用，发现知识检索是关键瓶颈，外部知识和言语化推理对性能有显著提升，并发布了新的科学推理基线模型。", "motivation": "科学问题解决对LLM提出了独特挑战，需要深层领域知识和复杂推理能力。目前缺乏全面评估科学推理的基准，且很少有方法系统地解耦知识与推理在这些任务中的不同作用。", "method": "引入了多样化的科学推理基准套件SciReas及其更复杂的子集SciReas-Pro。提出了KRUX探究框架，用于研究知识和推理在科学任务中的独立作用。结合两者进行了深入分析，并与长CoT SFT的并发工作进行了比较。发布了8B科学推理基线模型SciLit01。", "result": "1) 从模型参数中检索任务相关知识是LLM科学推理的关键瓶颈；2) 推理模型始终受益于上下文内添加的外部知识，超越了推理本身的增强；3) 增强言语化推理能力可提高LLM显现任务相关知识的能力。", "conclusion": "通过全面的评估和探究框架，揭示了LLM在科学推理中知识检索的瓶颈，强调了外部知识和言语化推理对性能提升的重要性，为未来的科学推理研究提供了有价值的见解、基准和强有力的基线模型。"}}
{"id": "2508.18904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18904", "abs": "https://arxiv.org/abs/2508.18904", "authors": ["Thien-Phuc Tran", "Minh-Quang Nguyen", "Minh-Triet Tran", "Tam V. Nguyen", "Trong-Le Do", "Duy-Nam Ly", "Viet-Tham Huynh", "Khanh-Duy Le", "Mai-Khiem Tran", "Trung-Nghia Le"], "title": "Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025", "comment": "ACM Multimedia 2025", "summary": "The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM\nMultimedia 2025, introduces the first large-scale benchmark for event-level\nmultimodal understanding. Traditional captioning and retrieval tasks largely\nfocus on surface-level recognition of people, objects, and scenes, often\noverlooking the contextual and semantic dimensions that define real-world\nevents. EVENTA addresses this gap by integrating contextual, temporal, and\nsemantic information to capture the who, when, where, what, and why behind an\nimage. Built upon the OpenEvents V1 dataset, the challenge features two tracks:\nEvent-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval.\nA total of 45 teams from six countries participated, with evaluation conducted\nthrough Public and Private Test phases to ensure fairness and reproducibility.\nThe top three teams were invited to present their solutions at ACM Multimedia\n2025. EVENTA establishes a foundation for context-aware, narrative-driven\nmultimedia AI, with applications in journalism, media analysis, cultural\narchiving, and accessibility. Further details about the challenge are available\nat the official homepage: https://ltnghia.github.io/eventa/eventa-2025.", "AI": {"tldr": "EVENTA大挑战是ACM Multimedia 2025上推出的首个大规模事件级多模态理解基准，旨在通过整合上下文、时间、语义信息来超越传统图像分析的局限。", "motivation": "传统的图像标注和检索任务主要关注人物、物体和场景的表层识别，往往忽略了定义真实世界事件的上下文和语义维度。EVENTA旨在弥补这一空白，捕捉图像背后的“何人、何时、何地、何事、何因”。", "method": "EVENTA挑战基于OpenEvents V1数据集构建，设有事件增强图像检索与标注、以及基于事件的图像检索两个赛道。共有来自六个国家的45支团队参与，通过公开和私有测试阶段进行评估，以确保公平性和可复现性。", "result": "共有45支团队参与了挑战，前三名团队受邀在ACM Multimedia 2025上展示了他们的解决方案。EVENTA为上下文感知、叙事驱动的多媒体AI奠定了基础。", "conclusion": "EVENTA挑战为事件级多模态理解建立了新的基准，推动了上下文感知、叙事驱动的多媒体AI发展，并在新闻、媒体分析、文化存档和无障碍等领域具有广泛应用前景。"}}
{"id": "2508.19229", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19229", "abs": "https://arxiv.org/abs/2508.19229", "authors": ["Wei Xiong", "Wenting Zhao", "Weizhe Yuan", "Olga Golovneva", "Tong Zhang", "Jason Weston", "Sainbayar Sukhbaatar"], "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning", "comment": null, "summary": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.", "AI": {"tldr": "本文提出StepWiser，一个生成式判断模型，通过元推理（输出思考过程）为多步推理模型提供分步反馈。该模型通过强化学习训练，在中间步骤判断准确性、策略模型训练和推理时搜索方面均优于现有方法。", "motivation": "随着模型越来越多地利用多步推理策略解决复杂问题，监督这些中间步骤的逻辑有效性成为关键挑战。现有过程奖励模型通常是分类器，缺乏解释性，且依赖静态数据集进行监督微调，泛化能力受限。", "method": "本文将分步奖励建模从分类任务重新定义为推理任务。提出一个生成式判断模型StepWiser，它对策略模型的推理步骤进行“元推理”，在给出最终判断前输出思考过程（思考令牌）。StepWiser通过强化学习，利用rollout的相对结果进行训练。", "result": "研究结果表明，StepWiser (i) 在中间步骤的判断准确性优于现有方法；(ii) 可以在训练时用于改进策略模型；(iii) 改进了推理时的搜索能力。", "conclusion": "StepWiser通过将分步奖励建模重构为推理任务，并采用强化学习训练一个生成式判断模型，有效解决了现有过程奖励模型缺乏解释性和泛化能力差的问题，显著提升了对多步推理模型中间步骤的监督和改进能力。"}}
{"id": "2508.19205", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19205", "abs": "https://arxiv.org/abs/2508.19205", "authors": ["Zhiliang Peng", "Jianwei Yu", "Wenhui Wang", "Yaoyao Chang", "Yutao Sun", "Li Dong", "Yi Zhu", "Weijiang Xu", "Hangbo Bao", "Zehua Wang", "Shaohan Huang", "Yan Xia", "Furu Wei"], "title": "VibeVoice Technical Report", "comment": null, "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.", "AI": {"tldr": "VibeVoice是一种新型模型，通过结合下一token扩散（一种自回归生成潜在向量的统一方法）和一种高效的连续语音分词器，能够合成长达90分钟的多说话人语音，同时保持高保真度和计算效率。", "motivation": "研究动机是需要合成具有真实对话“氛围”的长篇多说话人语音，以超越现有开源和专有对话模型的局限性，尤其是在处理长序列和多说话人场景时。", "method": "该研究引入了VibeVoice模型，它采用下一token扩散（一种通过扩散自回归生成潜在向量来建模连续数据的统一方法）。此外，还提出了一种新颖的连续语音分词器，该分词器与Encodec模型相比，将数据压缩率提高了80倍，同时保持了可比的性能和音频保真度，并显著提升了处理长序列的计算效率。", "result": "VibeVoice模型能够合成长达90分钟（在64K上下文窗口长度下）的语音，最多支持4位说话人。其核心分词器在保持音频保真度的同时，数据压缩率比Encodec提高了80倍，显著提升了计算效率。VibeVoice能够捕捉真实的对话“氛围”，并超越了现有的开源和专有对话模型。", "conclusion": "VibeVoice通过其创新的下一token扩散方法和高效的连续语音分词器，在长篇多说话人语音合成方面取得了重大进展，实现了高保真、高效且能捕捉真实对话感的语音生成，显著优于现有技术。"}}
{"id": "2508.18939", "categories": ["cs.CV", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.18939", "abs": "https://arxiv.org/abs/2508.18939", "authors": ["Amartaivan Sanjjamts", "Morita Hiroshi"], "title": "Preliminary Study on Space Utilization and Emergent Behaviors of Group vs. Single Pedestrians in Real-World Trajectories", "comment": null, "summary": "This study presents an initial framework for distinguishing group and single\npedestrians based on real-world trajectory data, with the aim of analyzing\ntheir differences in space utilization and emergent behavioral patterns. By\nsegmenting pedestrian trajectories into fixed time bins and applying a\nTransformer-based pair classification model, we identify cohesive groups and\nisolate single pedestrians over a structured sequence-based filtering process.\nTo prepare for deeper analysis, we establish a comprehensive metric framework\nincorporating both spatial and behavioral dimensions. Spatial utilization\nmetrics include convex hull area, smallest enclosing circle radius, and\nheatmap-based spatial densities to characterize how different pedestrian types\noccupy and interact with space. Behavioral metrics such as velocity change,\nmotion angle deviation, clearance radius, and trajectory straightness are\ndesigned to capture local adaptations and responses during interactions.\nFurthermore, we introduce a typology of encounter types-single-to-single,\nsingle-to-group, and group-to-group to categorize and later quantify different\ninteraction scenarios. Although this version focuses primarily on the\nclassification pipeline and dataset structuring, it establishes the groundwork\nfor scalable analysis across different sequence lengths 60, 100, and 200\nframes. Future versions will incorporate complete quantitative analysis of the\nproposed metrics and their implications for pedestrian simulation and space\ndesign validation in crowd dynamics research.", "AI": {"tldr": "本研究提出了一个区分群体和个体行人的初步框架，旨在分析他们在空间利用和行为模式上的差异，并为后续的定量分析和应用奠定基础。", "motivation": "理解群体和个体行人在空间利用和紧急行为模式上的差异对于行人模拟和空间设计至关重要。", "method": "通过将行人轨迹分段，并应用基于Transformer的对分类模型来识别群体和个体行人。研究建立了一个综合度量框架，包含空间利用指标（如凸包面积、最小外接圆半径、热力图空间密度）和行为指标（如速度变化、运动角度偏差、净空半径、轨迹直线度）。此外，还引入了不同相遇类型（个体对个体、个体对群体、群体对群体）的分类。", "result": "本研究版本主要关注分类流程和数据集的构建，成功为在不同序列长度（60、100、200帧）下进行可扩展分析奠定了基础。", "conclusion": "本研究成功构建了一个区分群体和个体行人的初步框架，并建立了用于深入分析其空间利用和行为模式差异的度量体系和数据结构，为未来在人群动力学研究中的定量分析和应用奠定了基础。"}}
{"id": "2508.19239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19239", "abs": "https://arxiv.org/abs/2508.19239", "authors": ["Gaurab Chhetri", "Shriyank Somvanshi", "Md Monzurul Islam", "Shamyo Brotee", "Mahmuda Sultana Mimi", "Dipti Koirala", "Biplov Pandey", "Subasish Das"], "title": "Model Context Protocols in Adaptive Transport Systems: A Survey", "comment": null, "summary": "The rapid expansion of interconnected devices, autonomous systems, and AI\napplications has created severe fragmentation in adaptive transport systems,\nwhere diverse protocols and context sources remain isolated. This survey\nprovides the first systematic investigation of the Model Context Protocol (MCP)\nas a unifying paradigm, highlighting its ability to bridge protocol-level\nadaptation with context-aware decision making. Analyzing established\nliterature, we show that existing efforts have implicitly converged toward\nMCP-like architectures, signaling a natural evolution from fragmented solutions\nto standardized integration frameworks. We propose a five-category taxonomy\ncovering adaptive mechanisms, context-aware frameworks, unification models,\nintegration strategies, and MCP-enabled architectures. Our findings reveal\nthree key insights: traditional transport protocols have reached the limits of\nisolated adaptation, MCP's client-server and JSON-RPC structure enables\nsemantic interoperability, and AI-driven transport demands integration\nparadigms uniquely suited to MCP. Finally, we present a research roadmap\npositioning MCP as a foundation for next-generation adaptive, context-aware,\nand intelligent transport infrastructures.", "AI": {"tldr": "本调查系统性地研究了模型上下文协议（MCP）作为统一范式，以解决自适应传输系统中协议和上下文源的碎片化问题，并将其定位为下一代智能传输基础设施的基础。", "motivation": "互联设备、自主系统和AI应用的快速发展导致自适应传输系统严重碎片化，各种协议和上下文源相互隔离，缺乏统一的适应性传输系统。", "method": "本研究通过对现有文献的系统性调查和分析，展示了现有工作如何隐性地趋向于类MCP架构。研究提出了一种包含五类（自适应机制、上下文感知框架、统一模型、集成策略和MCP使能架构）的分类法。", "result": "研究发现：传统传输协议在孤立适应方面已达极限；MCP的客户端-服务器和JSON-RPC结构实现了语义互操作性；AI驱动的传输对集成范式提出了独特需求，而MCP恰好能满足这些需求。现有努力已趋向于类MCP架构。", "conclusion": "MCP能够将协议级适应与上下文感知决策相结合，是解决自适应传输系统碎片化问题的统一范式。本研究提出了一个研究路线图，将MCP定位为下一代自适应、上下文感知和智能传输基础设施的基础。"}}
{"id": "2508.19221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19221", "abs": "https://arxiv.org/abs/2508.19221", "authors": ["Isabel Cachola", "Daniel Khashabi", "Mark Dredze"], "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?", "comment": null, "summary": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data.", "AI": {"tldr": "本文调查了简明语言摘要（PLS）文献，发现传统可读性指标（如FKGL）与人类判断相关性差，并提出语言模型（LMs）能更好地评估PLS的可读性，特别是在深层可读性方面。", "motivation": "简明语言摘要（PLS）旨在为非专业读者简化复杂文档。目前评估PLS可读性的标准做法是使用传统可读性指标（如Flesch-Kincaid Grade Level, FKGL），但这些指标在PLS领域尚未与人类可读性判断进行比较，其有效性存疑。", "method": "作者对PLS文献进行了全面调查，评估了8种传统可读性指标与人类判断的相关性。随后，他们利用语言模型（LMs）来判断可读性，并将其结果与人类判断进行比较。最后，将分析扩展到PLS数据集，探究LMs在捕捉深层可读性方面的表现。", "result": "研究发现，大多数传统可读性指标（包括最流行的FKGL）与人类判断的相关性很差。语言模型（LMs）在可读性判断上表现更好，最佳模型的皮尔逊相关系数达到0.56。在PLS数据集上，LMs能更好地捕捉深层可读性（如所需背景知识），并得出与传统指标不同的结论。", "conclusion": "传统可读性指标不适用于评估简明语言摘要。语言模型是更有效的可读性判断工具，尤其是在评估深层可读性方面。基于这些发现，论文为简明语言摘要的评估提供了最佳实践建议。"}}
{"id": "2508.18958", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18958", "abs": "https://arxiv.org/abs/2508.18958", "authors": ["Matteo Contini", "Victor Illien", "Sylvain Poulain", "Serge Bernard", "Julien Barde", "Sylvain Bonhommeau", "Alexis Joly"], "title": "The point is the mask: scaling coral reef segmentation with weak supervision", "comment": null, "summary": "Monitoring coral reefs at large spatial scales remains an open challenge,\nessential for assessing ecosystem health and informing conservation efforts.\nWhile drone-based aerial imagery offers broad spatial coverage, its limited\nresolution makes it difficult to reliably distinguish fine-scale classes, such\nas coral morphotypes. At the same time, obtaining pixel-level annotations over\nlarge spatial extents is costly and labor-intensive, limiting the scalability\nof deep learning-based segmentation methods for aerial imagery. We present a\nmulti-scale weakly supervised semantic segmentation framework that addresses\nthis challenge by transferring fine-scale ecological information from\nunderwater imagery to aerial data. Our method enables large-scale coral reef\nmapping from drone imagery with minimal manual annotation, combining\nclassification-based supervision, spatial interpolation and self-distillation\ntechniques. We demonstrate the efficacy of the approach, enabling large-area\nsegmentation of coral morphotypes and demonstrating flexibility for integrating\nnew classes. This study presents a scalable, cost-effective methodology for\nhigh-resolution reef monitoring, combining low-cost data collection, weakly\nsupervised deep learning and multi-scale remote sensing.", "AI": {"tldr": "本文提出了一种多尺度弱监督语义分割框架，通过将水下图像的精细尺度生态信息转移到无人机图像中，实现了大规模珊瑚礁形态类型的低成本、高分辨率监测。", "motivation": "大规模珊瑚礁监测面临挑战：无人机图像空间覆盖广但分辨率不足以区分精细尺度类别（如珊瑚形态类型）；同时，对大范围航空图像进行像素级标注成本高昂且耗时，限制了深度学习分割方法的可扩展性。", "method": "本研究提出了一种多尺度弱监督语义分割框架。该方法通过结合基于分类的监督、空间插值和自蒸馏技术，将水下图像中的精细尺度生态信息转移到航空数据中，从而实现对无人机图像的大规模珊瑚礁测绘，同时最大限度地减少手动标注。", "result": "该方法有效实现了珊瑚形态类型的大面积分割，并展示了整合新类别的灵活性。", "conclusion": "本研究提出了一种可扩展、经济高效的高分辨率珊瑚礁监测方法，该方法结合了低成本数据收集、弱监督深度学习和多尺度遥感技术。"}}
{"id": "2508.19227", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19227", "abs": "https://arxiv.org/abs/2508.19227", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "title": "Generative Interfaces for Language Models", "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.", "AI": {"tldr": "本文提出了一种“语言模型生成界面”范式，LLM通过主动生成用户界面（UI）来响应查询，以提升多轮、信息密集型和探索性任务中的交互效率和用户体验，并在评估中表现优于传统对话式交互。", "motivation": "当前大型语言模型（LLMs）的交互方式（线性请求-响应）在处理多轮、信息密集型和探索性任务时效率低下，限制了其作为助手、副驾驶和顾问的潜力。", "method": "研究提出了一种“语言模型生成界面”范式，LLM通过生成任务特定的用户界面（UI）来响应用户查询。该框架利用结构化的界面特定表示和迭代优化过程。为了系统评估，引入了一个多维度评估框架，从功能、交互和情感方面比较了生成界面与传统聊天界面的用户体验。", "result": "评估结果显示，生成界面持续优于对话式界面，人类用户在超过70%的情况下更偏爱生成界面。这些发现阐明了用户何时以及为何偏爱生成界面。", "conclusion": "生成界面显著改善了人机交互体验，为未来人机交互技术的发展指明了方向，特别是在复杂任务场景下。"}}
{"id": "2508.18959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18959", "abs": "https://arxiv.org/abs/2508.18959", "authors": ["Claudio Affolter", "Sidi Wu", "Yizi Chen", "Lorenz Hurni"], "title": "Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers", "comment": null, "summary": "Traditional map-making relies heavily on Geographic Information Systems\n(GIS), requiring domain expertise and being time-consuming, especially for\nrepetitive tasks. Recent advances in generative AI (GenAI), particularly image\ndiffusion models, offer new opportunities for automating and democratizing the\nmap-making process. However, these models struggle with accurate map creation\ndue to limited control over spatial composition and semantic layout. To address\nthis, we integrate vector data to guide map generation in different styles,\nspecified by the textual prompts. Our model is the first to generate accurate\nmaps in controlled styles, and we have integrated it into a web application to\nimprove its usability and accessibility. We conducted a user study with\nprofessional cartographers to assess the fidelity of generated maps, the\nusability of the web application, and the implications of ever-emerging GenAI\nin map-making. The findings have suggested the potential of our developed\napplication and, more generally, the GenAI models in helping both non-expert\nusers and professionals in creating maps more efficiently. We have also\noutlined further technical improvements and emphasized the new role of\ncartographers to advance the paradigm of AI-assisted map-making.", "AI": {"tldr": "该研究提出了一种结合矢量数据和生成式AI（特别是图像扩散模型）的新方法，通过文本提示生成准确且风格可控的地图，并将其集成到Web应用中，旨在提高地图制作的效率和可访问性。", "motivation": "传统的地图制作依赖GIS，耗时且需要专业知识，尤其在重复任务中效率低下。现有的生成式AI模型在地图生成中难以精确控制空间构成和语义布局，因此需要一种能克服这些限制的方法。", "method": "该研究通过整合矢量数据来指导图像扩散模型生成地图，并允许用户通过文本提示指定地图风格。同时，开发了一个Web应用程序以提高模型可用性和可访问性。此外，还对专业制图师进行了用户研究，评估生成地图的保真度和应用可用性。", "result": "该模型首次实现了在受控风格下生成准确地图。用户研究结果表明，所开发的应用和生成式AI模型在帮助非专业用户和专业人士更高效地创建地图方面具有巨大潜力。", "conclusion": "研究结论指出，所开发的应用程序和生成式AI模型在提升地图制作效率方面具有广阔前景。同时，强调了制图师在推进AI辅助地图制作范式中的新角色，并提出了进一步的技术改进方向。"}}
{"id": "2508.18960", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18960", "abs": "https://arxiv.org/abs/2508.18960", "authors": ["Simpenzwe Honore Leandre", "Natenaile Asmamaw Shiferaw", "Dillip Rout"], "title": "Enhancing compact convolutional transformers with super attention", "comment": "9 pages, 4 figures", "summary": "In this paper, we propose a vision model that adopts token mixing,\nsequence-pooling, and convolutional tokenizers to achieve state-of-the-art\nperformance and efficient inference in fixed context-length tasks. In the\nCIFAR100 benchmark, our model significantly improves the baseline of the top 1%\nand top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%,\nwhile being more efficient than the Scaled Dot Product Attention (SDPA)\ntransformers when the context length is less than the embedding dimension and\nonly 60% the size. In addition, the architecture demonstrates high training\nstability and does not rely on techniques such as data augmentation like mixup,\npositional embeddings, or learning rate scheduling. We make our code available\non Github.", "AI": {"tldr": "本文提出了一种新的视觉模型，结合了token混合、序列池化和卷积tokenizer，在固定上下文长度任务中实现了最先进的性能和高效推理，特别是在CIFAR100基准上表现出色，且无需数据增强等复杂技术。", "motivation": "在固定上下文长度任务中，提升视觉模型的性能和推理效率，并减少对数据增强、位置嵌入或学习率调度等复杂训练技术的依赖。", "method": "该模型采用了以下核心组件：token混合（token mixing）、序列池化（sequence-pooling）和卷积tokenizer（convolutional tokenizers）。", "result": "在CIFAR100基准测试中，模型显著提高了验证准确率：Top 1%从36.50%提升到46.29%，Top 5%从66.33%提升到76.31%。当上下文长度小于嵌入维度时，该模型比Scaled Dot Product Attention (SDPA) 变换器更高效，且模型大小仅为其60%。此外，该架构训练稳定性高，不依赖mixup等数据增强、位置嵌入或学习率调度等技术。", "conclusion": "所提出的视觉模型在固定上下文长度任务中实现了卓越的性能和效率，并通过简化训练过程（无需多种常用技术）展示了其优越性。"}}
{"id": "2508.18966", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18966", "abs": "https://arxiv.org/abs/2508.18966", "authors": ["Shaojin Wu", "Mengqi Huang", "Yufeng Cheng", "Wenxu Wu", "Jiahe Tian", "Yiming Luo", "Fei Ding", "Qian He"], "title": "USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning", "comment": "Project page: https://bytedance.github.io/USO/ Code and model:\n  https://github.com/bytedance/USO", "summary": "Existing literature typically treats style-driven and subject-driven\ngeneration as two disjoint tasks: the former prioritizes stylistic similarity,\nwhereas the latter insists on subject consistency, resulting in an apparent\nantagonism. We argue that both objectives can be unified under a single\nframework because they ultimately concern the disentanglement and\nre-composition of content and style, a long-standing theme in style-driven\nresearch. To this end, we present USO, a Unified Style-Subject Optimized\ncustomization model. First, we construct a large-scale triplet dataset\nconsisting of content images, style images, and their corresponding stylized\ncontent images. Second, we introduce a disentangled learning scheme that\nsimultaneously aligns style features and disentangles content from style\nthrough two complementary objectives, style-alignment training and\ncontent-style disentanglement training. Third, we incorporate a style\nreward-learning paradigm denoted as SRL to further enhance the model's\nperformance. Finally, we release USO-Bench, the first benchmark that jointly\nevaluates style similarity and subject fidelity across multiple metrics.\nExtensive experiments demonstrate that USO achieves state-of-the-art\nperformance among open-source models along both dimensions of subject\nconsistency and style similarity. Code and model:\nhttps://github.com/bytedance/USO", "AI": {"tldr": "本文提出了USO模型，一个统一的风格-主体优化定制模型，旨在将风格驱动和主体驱动的生成任务整合到一个框架中，通过内容与风格的解耦实现卓越的风格相似性和主体一致性。", "motivation": "现有文献将风格驱动和主体驱动的生成视为两个独立的任务，前者侧重风格相似性，后者侧重主体一致性，导致两者之间存在明显冲突。本文认为这两个目标可以通过内容与风格的解耦和重组统一在一个框架下。", "method": "1. 构建了一个大规模的三元组数据集（内容图、风格图、风格化内容图）。2. 引入了一种解耦学习方案，通过风格对齐训练和内容-风格解耦训练两个互补目标，同时对齐风格特征并解耦内容与风格。3. 结合了风格奖励学习（SRL）范式以进一步提升模型性能。4. 发布了USO-Bench，首个联合评估风格相似性和主体保真度的基准测试。", "result": "广泛的实验表明，USO在主体一致性和风格相似性两个维度上，在开源模型中均达到了最先进的性能。", "conclusion": "USO模型成功地将风格驱动和主体驱动的生成任务统一在一个框架下，通过创新的解耦学习和奖励机制，实现了风格相似性和主体一致性的卓越平衡，并提供了首个联合评估基准。"}}
{"id": "2508.19003", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19003", "abs": "https://arxiv.org/abs/2508.19003", "authors": ["Siyuan You", "Guozheng Xu", "Pengwei Zhou", "Qiwen Jin", "Jian Yao", "Li Li"], "title": "RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation", "comment": "38 pages, 10 figures, 9 tables", "summary": "Roof plane segmentation is one of the key procedures for reconstructing\nthree-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from\nairborne light detection and ranging (LiDAR) point clouds. The majority of\ncurrent approaches for roof plane segmentation rely on the manually designed or\nlearned features followed by some specifically designed geometric clustering\nstrategies. Because the learned features are more powerful than the manually\ndesigned features, the deep learning-based approaches usually perform better\nthan the traditional approaches. However, the current deep learning-based\napproaches have three unsolved problems. The first is that most of them are not\ntruly end-to-end, the plane segmentation results may be not optimal. The second\nis that the point feature discriminability near the edges is relatively low,\nleading to inaccurate planar edges. The third is that the planar geometric\ncharacteristics are not sufficiently considered to constrain the network\ntraining. To solve these issues, a novel edge-aware transformer-based network,\nnamed RoofSeg, is developed for segmenting roof planes from LiDAR point clouds\nin a truly end-to-end manner. In the RoofSeg, we leverage a transformer\nencoder-decoder-based framework to hierarchically predict the plane instance\nmasks with the use of a set of learnable plane queries. To further improve the\nsegmentation accuracy of edge regions, we also design an Edge-Aware Mask Module\n(EAMM) that sufficiently incorporates planar geometric prior of edges to\nenhance its discriminability for plane instance mask refinement. In addition,\nwe propose an adaptive weighting strategy in the mask loss to reduce the\ninfluence of misclassified points, and also propose a new plane geometric loss\nto constrain the network training.", "AI": {"tldr": "本研究提出了一种名为RoofSeg的边缘感知Transformer网络，用于从LiDAR点云中端到端地分割屋顶平面，旨在解决现有深度学习方法在非端到端、边缘特征辨别力低和几何特征考虑不足方面的问题。", "motivation": "屋顶平面分割是三维建筑模型（LoD 2和3）重建的关键步骤。然而，当前的深度学习方法存在三个未解决的问题：1) 大多数不是真正的端到端，可能导致平面分割结果不是最优；2) 边缘附近的点特征辨别力相对较低，导致平面边缘不准确；3) 未充分考虑平面几何特性来约束网络训练。", "method": "本研究开发了一种名为RoofSeg的边缘感知Transformer网络。该网络利用基于Transformer编码器-解码器的框架，通过一组可学习的平面查询分层预测平面实例掩模。为提高边缘区域的分割精度，设计了边缘感知掩模模块（EAMM），充分融入边缘的平面几何先验来增强其辨别力，以细化平面实例掩模。此外，提出了掩模损失中的自适应加权策略以减少错误分类点的影响，并提出了一种新的平面几何损失来约束网络训练。", "result": "通过开发端到端的RoofSeg网络，结合边缘感知模块和几何损失函数，本方法旨在有效解决现有屋顶平面分割方法中非端到端、边缘区域分割不准确以及几何特征约束不足的问题，从而提高从LiDAR点云中分割屋顶平面的精度和鲁棒性。", "conclusion": "本研究提出了一种新颖的端到端边缘感知Transformer网络RoofSeg，通过其独特的架构设计、边缘感知模块和几何损失函数，为从LiDAR点云中进行屋顶平面分割提供了一个全面的解决方案，有效克服了现有深度学习方法的局限性。"}}
{"id": "2508.18971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18971", "abs": "https://arxiv.org/abs/2508.18971", "authors": ["Maxime Pietrantoni", "Martin Humenberger", "Torsten Sattler", "Gabriela Csurka"], "title": "Can we make NeRF-based visual localization privacy-preserving?", "comment": null, "summary": "Visual localization (VL) is the task of estimating the camera pose in a known\nscene. VL methods, a.o., can be distinguished based on how they represent the\nscene, e.g., explicitly through a (sparse) point cloud or a collection of\nimages or implicitly through the weights of a neural network. Recently,\nNeRF-based methods have become popular for VL. While NeRFs offer high-quality\nnovel view synthesis, they inadvertently encode fine scene details, raising\nprivacy concerns when deployed in cloud-based localization services as\nsensitive information could be recovered. In this paper, we tackle this\nchallenge on two ends. We first propose a new protocol to assess\nprivacy-preservation of NeRF-based representations. We show that NeRFs trained\nwith photometric losses store fine-grained details in their geometry\nrepresentations, making them vulnerable to privacy attacks, even if the head\nthat predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving\nNeural Segmentation Field), a NeRF variant trained with segmentation\nsupervision instead of RGB images. These segmentation labels are learned in a\nself-supervised manner, ensuring they are coarse enough to obscure identifiable\nscene details while remaining discriminativeness in 3D. The segmentation space\nof ppNeSF can be used for accurate visual localization, yielding\nstate-of-the-art results.", "AI": {"tldr": "本文提出ppNeSF，一种基于自监督分割而非RGB图像训练的NeRF变体，旨在解决NeRF在视觉定位中存在的隐私泄露问题，并在保持定位精度的同时保护场景细节。", "motivation": "NeRF在视觉定位中表现出色，但其会无意中编码场景的精细细节，这在云端定位服务中会引发隐私担忧，因为敏感信息可能被恢复。", "method": "1. 提出了一种评估基于NeRF表示的隐私保护能力的新协议。2. 提出了ppNeSF（Privacy-Preserving Neural Segmentation Field），一种NeRF变体，使用自监督学习的分割标签进行训练，而不是RGB图像，以确保标签足够粗糙以模糊可识别的场景细节，同时在3D中保持区分度。", "result": "1. 发现使用光度损失训练的NeRF即使移除了颜色预测头，其几何表示仍存储精细细节，容易受到隐私攻击。2. ppNeSF的分割空间可用于精确的视觉定位，并取得了最先进的结果。", "conclusion": "NeRF在视觉定位中存在隐私泄露风险。ppNeSF通过利用自监督分割监督，成功地在保护场景隐私的同时，实现了高精度的视觉定位，为解决NeRF在云端服务中的隐私问题提供了有效方案。"}}
{"id": "2508.19060", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19060", "abs": "https://arxiv.org/abs/2508.19060", "authors": ["Blaž Rolih", "Matic Fučka", "Danijel Skočaj"], "title": "No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes", "comment": "Accepted by The Journal of Intelligent Manufacturing", "summary": "Surface defect detection is a critical task across numerous industries, aimed\nat efficiently identifying and localising imperfections or irregularities on\nmanufactured components. While numerous methods have been proposed, many fail\nto meet industrial demands for high performance, efficiency, and adaptability.\nExisting approaches are often constrained to specific supervision scenarios and\nstruggle to adapt to the diverse data annotations encountered in real-world\nmanufacturing processes, such as unsupervised, weakly supervised, mixed\nsupervision, and fully supervised settings. To address these challenges, we\npropose SuperSimpleNet, a highly efficient and adaptable discriminative model\nbuilt on the foundation of SimpleNet. SuperSimpleNet incorporates a novel\nsynthetic anomaly generation process, an enhanced classification head, and an\nimproved learning procedure, enabling efficient training in all four\nsupervision scenarios, making it the first model capable of fully leveraging\nall available data annotations. SuperSimpleNet sets a new standard for\nperformance across all scenarios, as demonstrated by its results on four\nchallenging benchmark datasets. Beyond accuracy, it is very fast, achieving an\ninference time below 10 ms. With its ability to unify diverse supervision\nparadigms while maintaining outstanding speed and reliability, SuperSimpleNet\nrepresents a promising step forward in addressing real-world manufacturing\nchallenges and bridging the gap between academic research and industrial\napplications. Code: https://github.com/blaz-r/SuperSimpleNet", "AI": {"tldr": "SuperSimpleNet是一种高效且适应性强的判别模型，用于表面缺陷检测。它能处理无监督、弱监督、混合监督和全监督四种监督场景，并在性能和速度上树立了新标准，显著提升了工业应用的潜力。", "motivation": "现有表面缺陷检测方法难以满足工业界对高性能、高效率和高适应性的需求，并且通常受限于特定的监督场景，难以适应真实制造过程中多样化的数据标注（如无监督、弱监督、混合监督和全监督）。", "method": "SuperSimpleNet以SimpleNet为基础，并进行了改进。它引入了新颖的合成异常生成过程、增强的分类头部和改进的学习程序，使其能够在所有四种监督场景下进行高效训练，从而充分利用所有可用的数据标注。", "result": "SuperSimpleNet在所有监督场景下都设定了新的性能标准，并在四个具有挑战性的基准数据集上得到了验证。此外，它还具有极高的推理速度，推理时间低于10毫秒。", "conclusion": "SuperSimpleNet通过统一多样化的监督范式，同时保持卓越的速度和可靠性，代表着解决真实世界制造挑战和弥合学术研究与工业应用之间差距的重大进步。"}}
{"id": "2508.18975", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18975", "abs": "https://arxiv.org/abs/2508.18975", "authors": ["Jan Nikolas Morshuis", "Matthias Hein", "Christian F. Baumgartner"], "title": "Understanding Benefits and Pitfalls of Current Methods for the Segmentation of Undersampled MRI Data", "comment": null, "summary": "MR imaging is a valuable diagnostic tool allowing to non-invasively visualize\npatient anatomy and pathology with high soft-tissue contrast. However, MRI\nacquisition is typically time-consuming, leading to patient discomfort and\nincreased costs to the healthcare system. Recent years have seen substantial\nresearch effort into the development of methods that allow for accelerated MRI\nacquisition while still obtaining a reconstruction that appears similar to the\nfully-sampled MR image. However, for many applications a perfectly\nreconstructed MR image may not be necessary, particularly, when the primary\ngoal is a downstream task such as segmentation. This has led to growing\ninterest in methods that aim to perform segmentation directly on accelerated\nMRI data. Despite recent advances, existing methods have largely been developed\nin isolation, without direct comparison to one another, often using separate or\nprivate datasets, and lacking unified evaluation standards. To date, no\nhigh-quality, comprehensive comparison of these methods exists, and the optimal\nstrategy for segmenting accelerated MR data remains unknown. This paper\nprovides the first unified benchmark for the segmentation of undersampled MRI\ndata comparing 7 approaches. A particular focus is placed on comparing\n\\textit{one-stage approaches}, that combine reconstruction and segmentation\ninto a unified model, with \\textit{two-stage approaches}, that utilize\nestablished MRI reconstruction methods followed by a segmentation network. We\ntest these methods on two MRI datasets that include multi-coil k-space data as\nwell as a human-annotated segmentation ground-truth. We find that simple\ntwo-stage methods that consider data-consistency lead to the best segmentation\nscores, surpassing complex specialized methods that are developed specifically\nfor this task.", "AI": {"tldr": "本文首次对欠采样MRI数据进行分割方法的统一基准测试，比较了七种方法，发现简单的考虑数据一致性的两阶段方法表现最佳。", "motivation": "MRI采集耗时，导致患者不适和成本增加。虽然加速MRI会影响图像质量，但对于分割等下游任务，完美的重建可能并非必需。现有直接在加速MRI数据上进行分割的方法缺乏统一比较、使用不同数据集且缺乏统一评估标准，导致最佳策略未知。", "method": "本文对7种分割欠采样MRI数据的方法进行了统一基准测试，重点比较了将重建和分割结合的“一阶段方法”与先进行MRI重建再进行分割的“两阶段方法”。研究使用了两个包含多线圈k空间数据和人工标注分割真值的MRI数据集。", "result": "研究发现，考虑数据一致性的简单两阶段方法取得了最佳分割分数，甚至超越了专门为此任务开发的复杂专业方法。", "conclusion": "对于加速MRI数据的分割任务，简单且考虑数据一致性的两阶段方法是目前最优策略。"}}
{"id": "2508.19154", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19154", "abs": "https://arxiv.org/abs/2508.19154", "authors": ["Yan Chen", "Yi Wen", "Wei Li", "Junchao Liu", "Yong Guo", "Jie Hu", "Xinghao Chen"], "title": "RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration", "comment": null, "summary": "We present the RAW domain diffusion model (RDDM), an end-to-end diffusion\nmodel that restores photo-realistic images directly from the sensor RAW data.\nWhile recent sRGB-domain diffusion methods achieve impressive results, they are\ncaught in a dilemma between high fidelity and realistic generation. As these\nmodels process lossy sRGB inputs and neglect the accessibility of the sensor\nRAW images in many scenarios, e.g., in image and video capturing in edge\ndevices, resulting in sub-optimal performance. RDDM bypasses this limitation by\ndirectly restoring images in the RAW domain, replacing the conventional\ntwo-stage image signal processing (ISP) + IR pipeline. However, a simple\nadaptation of pre-trained diffusion models to the RAW domain confronts the\nout-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE\n(RVAE) learning optimal latent representations, (2) a differentiable Post Tone\nProcessing (PTP) module enabling joint RAW and sRGB space optimization. To\ncompensate for the deficiency in the dataset, we develop a scalable degradation\npipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for\nlarge-scale training. Furthermore, we devise a configurable multi-bayer (CMB)\nLoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive\nexperiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion\nmethods, yielding higher fidelity results with fewer artifacts.", "AI": {"tldr": "本文提出了RAW域扩散模型（RDDM），一个端到端模型，可以直接从传感器RAW数据恢复照片级真实图像，优于现有sRGB域方法。", "motivation": "现有的sRGB域扩散模型在处理有损sRGB输入时，面临高保真度与真实感生成之间的困境，且忽略了许多场景（如边缘设备图像/视频捕获）中传感器RAW数据的可访问性，导致性能不佳。", "method": "RDDM通过直接在RAW域恢复图像，取代传统的两阶段ISP+IR管线。为解决RAW域适应中的分布外（OOD）问题，本文提出：(1) 一个RAW域VAE（RVAE）学习最优潜在表示；(2) 一个可微分的后处理模块（PTP），实现RAW和sRGB空间的联合优化。此外，为弥补数据集不足，开发了可扩展的降级管线，从现有sRGB数据集合成RAW LQ-HQ对进行大规模训练，并设计了可配置的多拜耳（CMB）LoRA模块以处理多样化的RAW模式。", "result": "广泛的实验证明，RDDM优于最先进的sRGB扩散方法，能产生更高保真度、更少伪影的结果。", "conclusion": "RDDM成功地直接从RAW数据恢复图像，克服了sRGB模型的局限性，实现了卓越的图像质量和真实感，为图像恢复设定了新标准。"}}
{"id": "2508.18984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18984", "abs": "https://arxiv.org/abs/2508.18984", "authors": ["Eric López", "Artemis Llabrés", "Ernest Valveny"], "title": "Enhancing Document VQA Models via Retrieval-Augmented Generation", "comment": "Accepted at Workshop on Machine Learning in Document Analysis and\n  Recognition (ICDAR WML 2025), Wuhan, China", "summary": "Document Visual Question Answering (Document VQA) must cope with documents\nthat span dozens of pages, yet leading systems still concatenate every page or\nrely on very large vision-language models, both of which are memory-hungry.\nRetrieval-Augmented Generation (RAG) offers an attractive alternative, first\nretrieving a concise set of relevant segments before generating answers from\nthis selected evidence. In this paper, we systematically evaluate the impact of\nincorporating RAG into Document VQA through different retrieval variants -\ntext-based retrieval using OCR tokens and purely visual retrieval without OCR -\nacross multiple models and benchmarks. Evaluated on the multi-page datasets\nMP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the\n\"concatenate-all-pages\" baseline by up to +22.5 ANLS, while the visual variant\nachieves +5.0 ANLS improvement without requiring any text extraction. An\nablation confirms that retrieval and reranking components drive most of the\ngain, whereas the layout-guided chunking strategy - proposed in several recent\nworks to leverage page structure - fails to help on these datasets. Our\nexperiments demonstrate that careful evidence selection consistently boosts\naccuracy across multiple model sizes and multi-page benchmarks, underscoring\nits practical value for real-world Document VQA.", "AI": {"tldr": "本文系统评估了将检索增强生成（RAG）整合到多页文档视觉问答（Document VQA）中的效果，通过文本和视觉检索变体，证明了精心选择证据能显著提高准确性并解决内存消耗问题。", "motivation": "现有文档VQA系统在处理多页文档时，要么拼接所有页面，要么依赖大型视觉-语言模型，这两种方法都非常占用内存。RAG作为一种先检索相关片段再生成答案的替代方案，具有吸引力。", "method": "研究通过不同检索变体（基于OCR文本的检索和纯视觉检索）在多个模型和基准（MP-DocVQA、DUDE、InfographicVQA）上，系统评估了RAG对Document VQA的影响。还进行了消融实验，分析检索、重排和布局引导分块策略的贡献。", "result": "文本中心检索变体将“拼接所有页面”的基线性能提高了高达+22.5 ANLS，而视觉检索变体在无需文本提取的情况下实现了+5.0 ANLS的提升。消融实验证实，检索和重排组件是性能提升的主要驱动因素，而布局引导的分块策略在这些数据集中未能提供帮助。实验表明，仔细的证据选择能够持续提高多模型尺寸和多页基准的准确性。", "conclusion": "RAG，特别是通过精心选择证据，对于实际应用中的Document VQA具有实用价值，能显著提高多页文档的准确性，并有效解决内存消耗问题。"}}
{"id": "2508.19162", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19162", "abs": "https://arxiv.org/abs/2508.19162", "authors": ["Rafael Sterzinger", "Tingyu Lin", "Robert Sablatnig"], "title": "Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents", "comment": "15 pages, accepted at ACPR2025", "summary": "A foundational task for the digital analysis of documents is text line\nsegmentation. However, automating this process with deep learning models is\nchallenging because it requires large, annotated datasets that are often\nunavailable for historical documents. Additionally, the annotation process is a\nlabor- and cost-intensive task that requires expert knowledge, which makes\nfew-shot learning a promising direction for reducing data requirements. In this\nwork, we demonstrate that small and simple architectures, coupled with a\ntopology-aware loss function, are more accurate and data-efficient than more\ncomplex alternatives. We pair a lightweight UNet++ with a connectivity-aware\nloss, initially developed for neuron morphology, which explicitly penalizes\nstructural errors like line fragmentation and unintended line merges. To\nincrease our limited data, we train on small patches extracted from a mere\nthree annotated pages per manuscript. Our methodology significantly improves\nupon the current state-of-the-art on the U-DIADS-TL dataset, with a 200%\nincrease in Recognition Accuracy and a 75% increase in Line Intersection over\nUnion. Our method also achieves an F-Measure score on par with or even\nexceeding that of the competition winner of the DIVA-HisDB baseline detection\ntask, all while requiring only three annotated pages, exemplifying the efficacy\nof our approach. Our implementation is publicly available at:\nhttps://github.com/RafaelSterzinger/acpr_few_shot_hist.", "AI": {"tldr": "本文提出了一种针对历史文档的少样本文本行分割方法，通过结合轻量级网络和拓扑感知损失函数，显著提高了数据效率和分割精度。", "motivation": "历史文档的文本行分割面临两大挑战：一是缺乏大型标注数据集，二是标注过程耗时耗力且需要专业知识。因此，少样本学习成为减少数据需求的一个有前景的方向。", "method": "研究者采用了一个轻量级的UNet++网络，并结合了一种连通性感知（拓扑感知）损失函数（最初用于神经元形态学），该函数明确惩罚线条碎片化和意外合并等结构错误。为应对数据限制，模型仅使用每个手稿三页标注页面的小补丁进行训练。", "result": "该方法在U-DIADS-TL数据集上取得了显著提升，识别准确率提高了200%，线条交并比提高了75%。此外，其F-Measure分数与DIVA-HisDB基线检测任务的竞赛冠军持平或超越，且仅需三页标注页面。", "conclusion": "小型简单的网络架构结合拓扑感知损失函数，在少样本学习场景下，对于历史文档的文本行分割任务，比复杂模型更准确、数据效率更高。"}}
{"id": "2508.18989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18989", "abs": "https://arxiv.org/abs/2508.18989", "authors": ["Shaivi Malik", "Hasnat Md Abdullah", "Sriparna Saha", "Amit Sheth"], "title": "Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone", "comment": null, "summary": "As Vision Language Models (VLMs) become integral to real-world applications,\nunderstanding their demographic biases is critical. We introduce GRAS, a\nbenchmark for uncovering demographic biases in VLMs across gender, race, age,\nand skin tone, offering the most diverse coverage to date. We further propose\nthe GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark\nfive state-of-the-art VLMs and reveal concerning bias levels, with the least\nbiased model attaining a GRAS Bias Score of only 2 out of 100. Our findings\nalso reveal a methodological insight: evaluating bias in VLMs with visual\nquestion answering (VQA) requires considering multiple formulations of a\nquestion. Our code, data, and evaluation results are publicly available.", "AI": {"tldr": "该研究引入了GRAS基准和GRAS偏见分数，用于揭示和量化视觉语言模型（VLMs）在性别、种族、年龄和肤色方面的偏见，并发现现有VLMs存在显著偏见。", "motivation": "视觉语言模型（VLMs）已成为现实世界应用不可或缺的一部分，因此理解其人口统计学偏见至关重要。", "method": "引入了GRAS基准，涵盖性别、种族、年龄和肤色，以揭示VLMs的人口统计学偏见。提出了GRAS偏见分数，一个可解释的量化偏见的指标。对五种最先进的VLMs进行了基准测试。发现使用视觉问答（VQA）评估VLMs偏见时，需要考虑问题的多种表述。", "result": "揭示了令人担忧的偏见水平，即使偏见最小的模型也仅获得2/100的GRAS偏见分数。研究还发现，在VQA中评估偏见需要考虑问题的多种表述。", "conclusion": "当前最先进的视觉语言模型存在显著的人口统计学偏见。GRAS基准和GRAS偏见分数提供了量化这些偏见的有效工具，并提出了在VQA偏见评估中考虑多种问题表述的方法学见解。"}}
{"id": "2508.19204", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.19204", "abs": "https://arxiv.org/abs/2508.19204", "authors": ["Julian Ost", "Andrea Ramazzina", "Amogh Joshi", "Maximilian Bömer", "Mario Bijelic", "Felix Heide"], "title": "LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding", "comment": "Project webpage: https://light.princeton.edu/LSD-3D", "summary": "Large-scale scene data is essential for training and testing in robot\nlearning. Neural reconstruction methods have promised the capability of\nreconstructing large physically-grounded outdoor scenes from captured sensor\ndata. However, these methods have baked-in static environments and only allow\nfor limited scene control -- they are functionally constrained in scene and\ntrajectory diversity by the captures from which they are reconstructed. In\ncontrast, generating driving data with recent image or video diffusion models\noffers control, however, at the cost of geometry grounding and causality. In\nthis work, we aim to bridge this gap and present a method that directly\ngenerates large-scale 3D driving scenes with accurate geometry, allowing for\ncausal novel view synthesis with object permanence and explicit 3D geometry\nestimation. The proposed method combines the generation of a proxy geometry and\nenvironment representation with score distillation from learned 2D image\npriors. We find that this approach allows for high controllability, enabling\nthe prompt-guided geometry and high-fidelity texture and structure that can be\nconditioned on map layouts -- producing realistic and geometrically consistent\n3D generations of complex driving scenes.", "AI": {"tldr": "本文提出了一种结合代理几何生成和2D图像先验分数蒸馏的方法，可以直接生成具有精确几何、对象持久性和因果关系的大规模3D驾驶场景，以实现可控的新颖视图合成。", "motivation": "现有的神经重建方法生成的是静态环境，受限于捕获数据，场景和轨迹多样性有限。而图像/视频扩散模型虽然提供了控制能力，但缺乏几何基础和因果关系。本研究旨在弥合这一鸿沟，生成具有准确几何和因果关系的大规模3D驾驶数据。", "method": "所提出的方法结合了代理几何和环境表示的生成，并利用从学习到的2D图像先验中进行分数蒸馏。这使得能够通过提示词引导几何，并生成可基于地图布局的高保真纹理和结构。", "result": "该方法实现了高度可控性，能够通过提示词引导几何生成，并产生可基于地图布局的高保真纹理和结构。最终生成了逼真且几何一致的复杂驾驶场景3D模型。", "conclusion": "该方法成功弥合了现有技术的不足，实现了对大规模3D驾驶场景的直接生成，具有精确的几何、对象持久性和因果关系，并支持高度可控的生成过程。"}}
{"id": "2508.19021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19021", "abs": "https://arxiv.org/abs/2508.19021", "authors": ["Riju Marwah", "Riya Arora", "Navneet Yadav", "Himank Arora"], "title": "MicroDetect-Net (MDN): Leveraging Deep Learning to Detect Microplastics in Clam Blood, a Step Towards Human Blood Analysis", "comment": "10 pages, 5 figures. Accepted to ICICC 2025 (Innovative Computation\n  in Biomedical Imaging)", "summary": "With the prevalence of plastics exceeding 368 million tons yearly,\nmicroplastic pollution has grown to an extent where air, water, soil, and\nliving organisms have all tested positive for microplastic presence. These\nparticles, which are smaller than 5 millimeters in size, are no less harmful to\nhumans than to the environment. Toxicity research on microplastics has shown\nthat exposure may cause liver infection, intestinal injuries, and gut flora\nimbalance, leading to numerous potential health hazards. This paper presents a\nnew model, MicroDetect-Net (MDN), which applies fluorescence microscopy with\nNile Red dye staining and deep learning to scan blood samples for\nmicroplastics. Although clam blood has certain limitations in replicating real\nhuman blood, this study opens avenues for applying the approach to human\nsamples, which are more consistent for preliminary data collection. The MDN\nmodel integrates dataset preparation, fluorescence imaging, and segmentation\nusing a convolutional neural network to localize and count microplastic\nfragments. The combination of convolutional networks and Nile Red dye for\nsegmentation produced strong image detection and accuracy. MDN was evaluated on\na dataset of 276 Nile Red-stained fluorescent blood images and achieved an\naccuracy of ninety two percent. Robust performance was observed with an\nIntersection over Union of 87.4 percent, F1 score of 92.1 percent, Precision of\n90.6 percent, and Recall of 93.7 percent. These metrics demonstrate the\neffectiveness of MDN in the detection of microplastics.", "AI": {"tldr": "本文提出了一种名为 MicroDetect-Net (MDN) 的深度学习模型，结合尼罗红染色荧光显微镜技术，用于在血样中检测和量化微塑料，并在蛤蜊血样数据集上取得了高精度。", "motivation": "微塑料污染日益严重，每年塑料产量超过3.68亿吨，已广泛存在于空气、水、土壤和生物体中。微塑料对人类健康有害，可能导致肝脏感染、肠道损伤和肠道菌群失衡。因此，开发有效的微塑料检测方法至关重要。", "method": "研究采用尼罗红染料染色荧光显微镜技术获取血样图像。然后，利用一个名为 MicroDetect-Net (MDN) 的深度学习模型，该模型整合了数据集准备、荧光成像和使用卷积神经网络进行分割，以定位和计数微塑料碎片。该方法在276张尼罗红染色的荧光血样图像数据集上进行了评估。", "result": "MDN 模型在276张尼罗红染色的荧光血样图像数据集上实现了92%的准确率。其性能指标包括：交并比 (IoU) 为87.4%，F1 分数为92.1%，精确度为90.6%，召回率为93.7%。这些指标表明 MDN 在微塑料检测方面表现出色。", "conclusion": "MDN 模型结合卷积网络和尼罗红染料进行分割，在微塑料图像检测和精度方面表现出强大性能，有效证明了其在检测微塑料方面的有效性。尽管本研究使用了蛤蜊血样，但该方法为未来应用于人类样本检测微塑料开辟了途径。"}}
{"id": "2508.19024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19024", "abs": "https://arxiv.org/abs/2508.19024", "authors": ["Yi Pan", "Yujia Zhang", "Michael Kampffmeyer", "Xiaoguang Zhao"], "title": "ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task\nthat involves retrieving videos based on queries relevant to only specific\nsegments. While existing works follow the paradigm of developing models to\nprocess unimodal features, powerful pretrained vision-language models like CLIP\nremain underexplored in this field. To bridge this gap, we propose ProPy, a\nmodel with systematic architectural adaption of CLIP specifically designed for\nPRVR. Drawing insights from the semantic relevance of multi-granularity events,\nProPy introduces two key innovations: (1) A Prompt Pyramid structure that\norganizes event prompts to capture semantics at multiple granularity levels,\nand (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that\nenables dynamic semantic interaction among events. With these designs, ProPy\nachieves SOTA performance on three public datasets, outperforming previous\nmodels by significant margins. Code is available at\nhttps://github.com/BUAAPY/ProPy.", "AI": {"tldr": "本文提出ProPy模型，通过系统性地调整CLIP架构以适应部分相关视频检索（PRVR）任务，利用多粒度事件提示金字塔和祖先-后代交互机制，实现了最先进的性能。", "motivation": "部分相关视频检索（PRVR）是一个实用但具有挑战性的任务，现有方法主要处理单模态特征。强大的预训练视觉-语言模型（如CLIP）在该领域尚未得到充分探索，因此需要弥合这一差距。", "method": "本文提出了ProPy模型，该模型对CLIP进行了系统性架构调整以专门用于PRVR。它引入了两项关键创新：1) 一个Prompt Pyramid结构，用于组织事件提示以捕获多粒度语义；2) 一个基于金字塔构建的祖先-后代交互机制，实现事件间的动态语义交互。", "result": "ProPy在三个公共数据集上取得了最先进的性能，显著超越了现有模型。", "conclusion": "ProPy通过对CLIP进行创新的架构适应，特别是引入多粒度提示金字塔和动态交互机制，有效解决了PRVR任务中的语义捕获问题，并取得了卓越的性能，证明了其在PRVR领域的有效性。"}}
{"id": "2508.19030", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19030", "abs": "https://arxiv.org/abs/2508.19030", "authors": ["Julian Suk", "Jolanda J. Wentzel", "Patryk Rygiel", "Joost Daemen", "Daniel Rueckert", "Jelmer M. Wolterink"], "title": "GReAT: leveraging geometric artery data to improve wall shear stress assessment", "comment": "(MICCAI 2025) Workshop on Shape in Medical Imaging (ShapeMI)", "summary": "Leveraging big data for patient care is promising in many medical fields such\nas cardiovascular health. For example, hemodynamic biomarkers like wall shear\nstress could be assessed from patient-specific medical images via machine\nlearning algorithms, bypassing the need for time-intensive computational fluid\nsimulation. However, it is extremely challenging to amass large-enough datasets\nto effectively train such models. We could address this data scarcity by means\nof self-supervised pre-training and foundations models given large datasets of\ngeometric artery models. In the context of coronary arteries, leveraging\nlearned representations to improve hemodynamic biomarker assessment has not yet\nbeen well studied. In this work, we address this gap by investigating whether a\nlarge dataset (8449 shapes) consisting of geometric models of 3D blood vessels\ncan benefit wall shear stress assessment in coronary artery models from a\nsmall-scale clinical trial (49 patients). We create a self-supervised target\nfor the 3D blood vessels by computing the heat kernel signature, a quantity\nobtained via Laplacian eigenvectors, which captures the very essence of the\nshapes. We show how geometric representations learned from this datasets can\nboost segmentation of coronary arteries into regions of low, mid and high\n(time-averaged) wall shear stress even when trained on limited data.", "AI": {"tldr": "该研究利用大规模三维血管几何模型进行自监督预训练，以解决数据稀缺问题，从而提高冠状动脉壁面剪应力（WSS）评估的准确性，即使在有限的临床数据下也能有效分割WSS区域。", "motivation": "在心血管健康等领域，利用大数据通过机器学习算法从医学图像评估血流动力学生物标志物（如壁面剪应力）具有巨大潜力，可避免耗时的计算流体模拟。然而，训练此类模型面临数据量不足的挑战。此外，在冠状动脉背景下，利用学习到的表征来改善血流动力学生物标志物评估尚未得到充分研究。", "method": "研究利用一个包含8449个三维血管几何模型的大规模数据集进行自监督预训练。通过计算热核签名（Heat Kernel Signature，一种通过拉普拉斯特征向量获得并捕捉形状本质的量）作为自监督目标。然后，将从该数据集学习到的几何表征应用于一个来自小型临床试验（49名患者）的冠状动脉模型，用于将冠状动脉分割为低、中、高（时间平均）壁面剪应力区域。", "result": "研究结果表明，即使在有限的训练数据下，从大规模几何数据集中学习到的几何表征也能显著提升冠状动脉壁面剪应力区域的分割性能。", "conclusion": "大规模三维血管几何模型的自监督预训练能够有效解决数据稀缺问题，并通过学习到的几何表征，显著提高冠状动脉壁面剪应力评估的准确性，尤其是在临床数据有限的情况下。"}}
{"id": "2508.19068", "categories": ["cs.CV", "cs.LG", "math.OC", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19068", "abs": "https://arxiv.org/abs/2508.19068", "authors": ["Serban C. Tudosie", "Alexander Denker", "Zeljko Kereta", "Simon Arridge"], "title": "Learning Binary Sampling Patterns for Single-Pixel Imaging using Bilevel Optimisation", "comment": null, "summary": "Single-Pixel Imaging enables reconstructing objects using a single detector\nthrough sequential illuminations with structured light patterns. We propose a\nbilevel optimisation method for learning task-specific, binary illumination\npatterns, optimised for applications like single-pixel fluorescence microscopy.\nWe address the non-differentiable nature of binary pattern optimisation using\nthe Straight-Through Estimator and leveraging a Total Deep Variation\nregulariser in the bilevel formulation. We demonstrate our method on the\nCytoImageNet microscopy dataset and show that learned patterns achieve superior\nreconstruction performance compared to baseline methods, especially in highly\nundersampled regimes.", "AI": {"tldr": "该研究提出一种双层优化方法，用于学习单像素成像中任务特定的二值照明模式，通过处理二值模式的不可微性并引入正则化器，在高度欠采样情况下实现了优于基线方法的重建性能，特别适用于荧光显微镜等应用。", "motivation": "单像素成像通过结构光模式序列照明实现物体重建，但需要优化照明模式以提高性能。研究旨在开发一种方法，能够为特定任务（如单像素荧光显微镜）学习到高效、二值的照明模式。", "method": "提出了一种双层优化方法来学习任务特定的二值照明模式。为解决二值模式优化中的不可微性问题，采用了Straight-Through Estimator。同时，在双层公式中引入了Total Deep Variation正则化器。", "result": "在CytoImageNet显微镜数据集上验证了所提出的方法。结果表明，学习到的模式在重建性能上优于基线方法，尤其是在高度欠采样的情况下表现出显著优势。", "conclusion": "该方法成功地学习了任务特定的二值照明模式，显著提升了单像素成像的重建性能，特别是在数据采集受限（高度欠采样）的应用场景中展现出卓越的潜力。"}}
{"id": "2508.19165", "categories": ["cs.CV", "Information systems~Multimedia and multimodal retrieval"], "pdf": "https://arxiv.org/pdf/2508.19165", "abs": "https://arxiv.org/abs/2508.19165", "authors": ["Yuzhen Li", "Min Liu", "Yuan Bian", "Xueping Wang", "Zhaoyang Li", "Gen Li", "Yaonan Wang"], "title": "Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding", "comment": "10 pages", "summary": "Monocular 3D visual grounding is a novel task that aims to locate 3D objects\nin RGB images using text descriptions with explicit geometry information.\nDespite the inclusion of geometry details in the text, we observe that the text\nembeddings are sensitive to the magnitude of numerical values but largely\nignore the associated measurement units. For example, simply equidistant\nmapping the length with unit \"meter\" to \"decimeters\" or \"centimeters\" leads to\nsevere performance degradation, even though the physical length remains\nequivalent. This observation signifies the weak 3D comprehension of pre-trained\nlanguage model, which generates misguiding text features to hinder 3D\nperception. Therefore, we propose to enhance the 3D perception of model on text\nembeddings and geometry features with two simple and effective methods.\nFirstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE),\nwhich enhances the comprehension of mapping relationships between different\nunits by augmenting the diversity of distance descriptors in text queries.\nNext, we propose a Text-Guided Geometry Enhancement (TGE) module to further\nenhance the 3D-text information by projecting the basic text features into\ngeometrically consistent space. These 3D-enhanced text features are then\nleveraged to precisely guide the attention of geometry features. We evaluate\nthe proposed method through extensive comparisons and ablation studies on the\nMono3DRefer dataset. Experimental results demonstrate substantial improvements\nover previous methods, achieving new state-of-the-art results with a notable\naccuracy gain of 11.94\\% in the \"Far\" scenario. Our code will be made publicly\navailable.", "AI": {"tldr": "该论文提出了一种单目3D视觉定位方法，通过增强文本嵌入和几何特征的3D感知能力，解决了预训练语言模型对文本中几何信息（特别是度量单位）理解不足的问题，实现了显著的性能提升。", "motivation": "研究发现，在单目3D视觉定位任务中，文本嵌入对数值大小敏感但忽略了相关的度量单位（如米、分米、厘米），导致物理长度等效时性能显著下降。这表明预训练语言模型在3D理解方面存在弱点，产生了误导性的文本特征，阻碍了3D感知。", "method": "提出了两种简单有效的方法：1. **3D-text Enhancement (3DTE)**：一种预处理方法，通过增加文本查询中距离描述符的多样性，增强模型对不同单位之间映射关系的理解。2. **Text-Guided Geometry Enhancement (TGE)**：一个模块，通过将基本文本特征投影到几何一致的空间中，进一步增强3D-文本信息，并利用这些增强后的3D文本特征精确引导几何特征的注意力。", "result": "在Mono3DRefer数据集上进行了广泛的比较和消融研究。实验结果表明，该方法比现有方法有显著改进，取得了新的最先进成果，尤其在“远距离”场景中精度提升了11.94%。", "conclusion": "所提出的3D-text Enhancement (3DTE)和Text-Guided Geometry Enhancement (TGE)方法能够有效增强模型对文本嵌入和几何特征的3D感知能力，解决了预训练语言模型在处理带有几何信息的文本时对度量单位理解不足的问题，从而显著提升了单目3D视觉定位的性能。"}}
{"id": "2508.19167", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19167", "abs": "https://arxiv.org/abs/2508.19167", "authors": ["Zhihang Xin", "Xitong Hu", "Rui Wang"], "title": "Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions", "comment": null, "summary": "Vision Transformers have demonstrated remarkable success in computer vision\ntasks, yet their reliance on learnable one-dimensional positional embeddings\nfundamentally disrupts the inherent two-dimensional spatial structure of images\nthrough patch flattening procedures. Traditional positional encoding approaches\nlack geometric constraints and fail to establish monotonic correspondence\nbetween Euclidean spatial distances and sequential index distances, thereby\nlimiting the model's capacity to leverage spatial proximity priors effectively.\nWe propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a\nmathematically principled approach that directly addresses two-dimensional\ncoordinates through natural complex domain representation, where the doubly\nperiodic properties of elliptic functions align remarkably with translational\ninvariance patterns commonly observed in visual data. Our method exploits the\nnon-linear geometric nature of elliptic functions to encode spatial distance\nrelationships naturally, while the algebraic addition formula enables direct\nderivation of relative positional information between arbitrary patch pairs\nfrom their absolute encodings. Comprehensive experiments demonstrate that\nWEF-PE achieves superior performance across diverse scenarios, including\n63.78\\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,\n93.28\\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on\nVTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay\nproperty through rigorous mathematical proof, while attention visualization\nreveals enhanced geometric inductive bias and more coherent semantic focus\ncompared to conventional approaches.The source code implementing the methods\ndescribed in this paper is publicly available on GitHub.", "AI": {"tldr": "本文提出了一种基于魏尔斯特拉斯椭圆函数的位置编码（WEF-PE），用于解决Vision Transformers中一维位置编码破坏二维空间结构的问题。WEF-PE利用复数域和椭圆函数的双周期性，有效编码二维空间距离关系，并在多项任务中取得了卓越性能提升。", "motivation": "Vision Transformers中可学习的一维位置嵌入通过图像块展平过程，从根本上破坏了图像固有的二维空间结构。传统的位置编码缺乏几何约束，未能建立欧几里得空间距离与序列索引距离之间的单调对应关系，从而限制了模型有效利用空间邻近先验的能力。", "method": "本文提出了魏尔斯特拉斯椭圆函数位置编码（WEF-PE）。该方法通过自然的复数域表示直接处理二维坐标，其中椭圆函数的双周期性与视觉数据中常见的平移不变性模式高度吻合。WEF-PE利用椭圆函数的非线性几何特性自然地编码空间距离关系，其代数加法公式能够从绝对编码中直接推导出任意图像块对之间的相对位置信息。", "result": "WEF-PE在多种场景下均取得了卓越性能：在ViT-Tiny架构下CIFAR-100从头训练达到63.78%的准确率，在ViT-Base架构下CIFAR-100微调达到93.28%的准确率，并在VTAB-1k基准任务中持续改进。理论分析通过严格的数学证明证实了距离衰减特性，而注意力可视化揭示了相比传统方法，WEF-PE增强了几何归纳偏置和更连贯的语义焦点。", "conclusion": "魏尔斯特拉斯椭圆函数位置编码（WEF-PE）是一种数学上合理的方法，它通过直接处理二维坐标和利用椭圆函数的特性，有效解决了Vision Transformers中位置编码对空间结构破坏的问题，显著提升了模型性能、几何归纳偏置和语义聚焦能力。"}}
{"id": "2508.19182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19182", "abs": "https://arxiv.org/abs/2508.19182", "authors": ["Silvio Giancola", "Anthony Cioppa", "Marc Gutiérrez-Pérez", "Jan Held", "Carlos Hinojosa", "Victor Joos", "Arnaud Leduc", "Floriane Magera", "Karen Sanchez", "Vladimir Somers", "Artur Xarles", "Antonio Agudo", "Alexandre Alahi", "Olivier Barnich", "Albert Clapés", "Christophe De Vleeschouwer", "Sergio Escalera", "Bernard Ghanem", "Thomas B. Moeslund", "Marc Van Droogenbroeck", "Tomoki Abe", "Saad Alotaibi", "Faisal Altawijri", "Steven Araujo", "Xiang Bai", "Xiaoyang Bi", "Jiawang Cao", "Vanyi Chao", "Kamil Czarnogórski", "Fabian Deuser", "Mingyang Du", "Tianrui Feng", "Patrick Frenzel", "Mirco Fuchs", "Jorge García", "Konrad Habel", "Takaya Hashiguchi", "Sadao Hirose", "Xinting Hu", "Yewon Hwang", "Ririko Inoue", "Riku Itsuji", "Kazuto Iwai", "Hongwei Ji", "Yangguang Ji", "Licheng Jiao", "Yuto Kageyama", "Yuta Kamikawa", "Yuuki Kanasugi", "Hyungjung Kim", "Jinwook Kim", "Takuya Kurihara", "Bozheng Li", "Lingling Li", "Xian Li", "Youxing Lian", "Dingkang Liang", "Hongkai Lin", "Jiadong Lin", "Jian Liu", "Liang Liu", "Shuaikun Liu", "Zhaohong Liu", "Yi Lu", "Federico Méndez", "Huadong Ma", "Wenping Ma", "Jacek Maksymiuk", "Henry Mantilla", "Ismail Mathkour", "Daniel Matthes", "Ayaha Motomochi", "Amrulloh Robbani Muhammad", "Haruto Nakayama", "Joohyung Oh", "Yin May Oo", "Marcelo Ortega", "Norbert Oswald", "Rintaro Otsubo", "Fabian Perez", "Mengshi Qi", "Cristian Rey", "Abel Reyes-Angulo", "Oliver Rose", "Hoover Rueda-Chacón", "Hideo Saito", "Jose Sarmiento", "Kanta Sawafuji", "Atom Scott", "Xi Shen", "Pragyan Shrestha", "Jae-Young Sim", "Long Sun", "Yuyang Sun", "Tomohiro Suzuki", "Licheng Tang", "Masato Tonouchi", "Ikuma Uchida", "Henry O. Velesaca", "Tiancheng Wang", "Rio Watanabe", "Jay Wu", "Yongliang Wu", "Shunzo Yamagishi", "Di Yang", "Xu Yang", "Yuxin Yang", "Hao Ye", "Xinyu Ye", "Calvin Yeung", "Xuanlong Yu", "Chao Zhang", "Dingyuan Zhang", "Kexing Zhang", "Zhe Zhao", "Xin Zhou", "Wenbo Zhu", "Julian Ziegler"], "title": "SoccerNet 2025 Challenges Results", "comment": null, "summary": "The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNet\nopen benchmarking effort, dedicated to advancing computer vision research in\nfootball video understanding. This year's challenges span four vision-based\ntasks: (1) Team Ball Action Spotting, focused on detecting ball-related actions\nin football broadcasts and assigning actions to teams; (2) Monocular Depth\nEstimation, targeting the recovery of scene geometry from single-camera\nbroadcast clips through relative depth estimation for each pixel; (3)\nMulti-View Foul Recognition, requiring the analysis of multiple synchronized\ncamera views to classify fouls and their severity; and (4) Game State\nReconstruction, aimed at localizing and identifying all players from a\nbroadcast video to reconstruct the game state on a 2D top-view of the field.\nAcross all tasks, participants were provided with large-scale annotated\ndatasets, unified evaluation protocols, and strong baselines as starting\npoints. This report presents the results of each challenge, highlights the\ntop-performing solutions, and provides insights into the progress made by the\ncommunity. The SoccerNet Challenges continue to serve as a driving force for\nreproducible, open research at the intersection of computer vision, artificial\nintelligence, and sports. Detailed information about the tasks, challenges, and\nleaderboards can be found at https://www.soccer-net.org, with baselines and\ndevelopment kits available at https://github.com/SoccerNet.", "AI": {"tldr": "SoccerNet 2025挑战赛是其第五届年度开放基准测试，专注于足球视频理解中的计算机视觉研究。挑战赛涵盖四个任务：团队控球动作识别、单目深度估计、多视角犯规识别和比赛状态重建。本报告总结了挑战赛结果、顶尖解决方案和社区进展。", "motivation": "该研究旨在推动足球视频理解领域的计算机视觉研究发展，通过提供开放基准测试和大规模标注数据集，解决足球广播视频中复杂的视觉分析任务。", "method": "设立了四个主要的视觉任务：(1) 团队控球动作识别，用于检测球相关动作并分配给团队；(2) 单目深度估计，从单摄像头片段恢复场景几何；(3) 多视角犯规识别，分析多视角以分类犯规及其严重性；(4) 比赛状态重建，定位和识别所有球员以在2D俯视图上重建比赛状态。为参与者提供了大规模标注数据集、统一评估协议和强大的基线。", "result": "本报告展示了各项挑战赛的结果，重点介绍了表现最佳的解决方案，并提供了社区在足球视频理解领域取得进展的见解。", "conclusion": "SoccerNet挑战赛持续作为推动计算机视觉、人工智能和体育交叉领域可复现、开放研究的重要力量。"}}
{"id": "2508.19188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19188", "abs": "https://arxiv.org/abs/2508.19188", "authors": ["Jeonghwan Kim", "Yushi Lan", "Armando Fortes", "Yongwei Chen", "Xingang Pan"], "title": "FastMesh:Efficient Artistic Mesh Generation via Component Decoupling", "comment": null, "summary": "Recent mesh generation approaches typically tokenize triangle meshes into\nsequences of tokens and train autoregressive models to generate these tokens\nsequentially. Despite substantial progress, such token sequences inevitably\nreuse vertices multiple times to fully represent manifold meshes, as each\nvertex is shared by multiple faces. This redundancy leads to excessively long\ntoken sequences and inefficient generation processes. In this paper, we propose\nan efficient framework that generates artistic meshes by treating vertices and\nfaces separately, significantly reducing redundancy. We employ an\nautoregressive model solely for vertex generation, decreasing the token count\nto approximately 23\\% of that required by the most compact existing tokenizer.\nNext, we leverage a bidirectional transformer to complete the mesh in a single\nstep by capturing inter-vertex relationships and constructing the adjacency\nmatrix that defines the mesh faces. To further improve the generation quality,\nwe introduce a fidelity enhancer to refine vertex positioning into more natural\narrangements and propose a post-processing framework to remove undesirable edge\nconnections. Experimental results show that our method achieves more than\n8$\\times$ faster speed on mesh generation compared to state-of-the-art\napproaches, while producing higher mesh quality.", "AI": {"tldr": "该论文提出了一种高效的艺术网格生成框架，通过分离顶点和面来减少冗余，显著缩短了token序列长度，并利用自回归模型、双向Transformer和精细化步骤，实现了比现有方法快8倍以上的生成速度和更高的网格质量。", "motivation": "现有网格生成方法将网格标记化为序列，但由于顶点在多个面中共享，导致token序列冗余且过长，从而降低了生成效率。", "method": "该方法将顶点和面分开处理。首先，使用自回归模型仅生成顶点，将token数量减少至现有最紧凑分词器的23%。其次，利用双向Transformer一步完成网格生成，通过捕捉顶点间关系构建定义网格面的邻接矩阵。此外，引入保真度增强器以优化顶点位置，并提出后处理框架以去除不良边缘连接。", "result": "实验结果表明，该方法在网格生成速度上比最先进的方法快8倍以上，同时能产生更高质量的网格。", "conclusion": "通过分离顶点和面、减少冗余以及采用多阶段生成和优化策略，该框架显著提高了艺术网格生成的速度和质量。"}}
{"id": "2508.19195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19195", "abs": "https://arxiv.org/abs/2508.19195", "authors": ["Weixin Ye", "Hongguang Zhu", "Wei Wang", "Yahui Liu", "Mengyu Wang"], "title": "All-in-One Slider for Attribute Manipulation in Diffusion Models", "comment": null, "summary": "Text-to-image (T2I) diffusion models have made significant strides in\ngenerating high-quality images. However, progressively manipulating certain\nattributes of generated images to meet the desired user expectations remains\nchallenging, particularly for content with rich details, such as human faces.\nSome studies have attempted to address this by training slider modules.\nHowever, they follow a One-for-One manner, where an independent slider is\ntrained for each attribute, requiring additional training whenever a new\nattribute is introduced. This not only results in parameter redundancy\naccumulated by sliders but also restricts the flexibility of practical\napplications and the scalability of attribute manipulation. To address this\nissue, we introduce the All-in-One Slider, a lightweight module that decomposes\nthe text embedding space into sparse, semantically meaningful attribute\ndirections. Once trained, it functions as a general-purpose slider, enabling\ninterpretable and fine-grained continuous control over various attributes.\nMoreover, by recombining the learned directions, the All-in-One Slider supports\nzero-shot manipulation of unseen attributes (e.g., races and celebrities) and\nthe composition of multiple attributes. Extensive experiments demonstrate that\nour method enables accurate and scalable attribute manipulation, achieving\nnotable improvements compared to previous methods. Furthermore, our method can\nbe extended to integrate with the inversion framework to perform attribute\nmanipulation on real images, broadening its applicability to various real-world\nscenarios. The code and trained model will be released at:\nhttps://github.com/ywxsuperstar/KSAE-FaceSteer.", "AI": {"tldr": "本文提出“一体化滑块”（All-in-One Slider），一个轻量级模块，通过分解文本嵌入空间，实现对文生图模型中图像属性的精确、可扩展且零样本控制，解决了现有方法效率低、参数冗余的问题。", "motivation": "文生图模型在生成高质量图像方面表现出色，但对生成图像的特定属性进行渐进式操控（尤其是细节丰富的图像如人脸）仍然具有挑战。现有方法通常为每个属性训练一个独立的滑块（One-for-One），导致参数冗余，限制了实际应用的灵活性和属性操控的可扩展性，且每次引入新属性都需要额外训练。", "method": "本文引入了“一体化滑块”（All-in-One Slider），这是一个轻量级模块，它将文本嵌入空间分解为稀疏且具有语义意义的属性方向。一旦训练完成，它就能作为一个通用滑块，对各种属性进行可解释、细粒度的连续控制。此外，通过重新组合学习到的方向，该滑块支持对未见属性（如种族和名人）的零样本操控，以及多个属性的组合。", "result": "广泛的实验表明，该方法能够实现准确且可扩展的属性操控，与现有方法相比有显著改进。此外，该方法还可以扩展与反演框架集成，对真实图像执行属性操控，从而拓宽了其在各种现实场景中的适用性。", "conclusion": "“一体化滑块”提供了一种高效、灵活且可扩展的解决方案，用于文生图模型中的属性操控，解决了现有方法的参数冗余和可扩展性问题。它不仅能实现对已知属性的精细控制，还支持零样本操控和多属性组合，并可应用于真实图像，具有广泛的应用前景。"}}
{"id": "2508.19209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19209", "abs": "https://arxiv.org/abs/2508.19209", "authors": ["Jianwen Jiang", "Weihong Zeng", "Zerong Zheng", "Jiaqi Yang", "Chao Liang", "Wang Liao", "Han Liang", "Yuan Zhang", "Mingyuan Gao"], "title": "OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation", "comment": "Homepage: https://omnihuman-lab.github.io/v1_5/", "summary": "Existing video avatar models can produce fluid human animations, yet they\nstruggle to move beyond mere physical likeness to capture a character's\nauthentic essence. Their motions typically synchronize with low-level cues like\naudio rhythm, lacking a deeper semantic understanding of emotion, intent, or\ncontext. To bridge this gap, \\textbf{we propose a framework designed to\ngenerate character animations that are not only physically plausible but also\nsemantically coherent and expressive.} Our model, \\textbf{OmniHuman-1.5}, is\nbuilt upon two key technical contributions. First, we leverage Multimodal Large\nLanguage Models to synthesize a structured textual representation of conditions\nthat provides high-level semantic guidance. This guidance steers our motion\ngenerator beyond simplistic rhythmic synchronization, enabling the production\nof actions that are contextually and emotionally resonant. Second, to ensure\nthe effective fusion of these multimodal inputs and mitigate inter-modality\nconflicts, we introduce a specialized Multimodal DiT architecture with a novel\nPseudo Last Frame design. The synergy of these components allows our model to\naccurately interpret the joint semantics of audio, images, and text, thereby\ngenerating motions that are deeply coherent with the character, scene, and\nlinguistic content. Extensive experiments demonstrate that our model achieves\nleading performance across a comprehensive set of metrics, including lip-sync\naccuracy, video quality, motion naturalness and semantic consistency with\ntextual prompts. Furthermore, our approach shows remarkable extensibility to\ncomplex scenarios, such as those involving multi-person and non-human subjects.\nHomepage: \\href{https://omnihuman-lab.github.io/v1_5/}", "AI": {"tldr": "本文提出OmniHuman-1.5框架，通过结合多模态大语言模型（MLLMs）提供高级语义指导和专用的多模态DiT架构，生成不仅物理真实而且语义连贯、富有表现力的角色动画。", "motivation": "现有视频虚拟形象模型虽能生成流畅动画，但仅限于物理相似性，无法捕捉角色真实精髓。它们的动作通常与音频节奏等低级线索同步，缺乏对情感、意图或上下文的深层语义理解。", "method": "本研究基于OmniHuman-1.5模型，包含两项关键技术贡献：1) 利用多模态大语言模型（MLLMs）合成结构化的文本条件，提供高级语义指导，使动作生成超越简单的节奏同步，实现上下文和情感共鸣；2) 引入带有新颖“伪最后一帧”（Pseudo Last Frame）设计的专用多模态DiT架构，以有效融合多模态输入并减轻模态间冲突。", "result": "实验证明，该模型在唇形同步准确性、视频质量、动作自然度和与文本提示的语义一致性等全面指标上均达到领先性能。此外，该方法在多人物和非人类主体等复杂场景中也展现出卓越的扩展性。", "conclusion": "该模型通过准确解释音频、图像和文本的联合语义，生成与角色、场景和语言内容深度一致的动作，有效弥补了现有视频虚拟形象模型在语义理解和表达方面的不足。"}}
{"id": "2508.19232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19232", "abs": "https://arxiv.org/abs/2508.19232", "authors": ["Kaveh Safavigerdini", "Ramakrishna Surya", "Jaired Collins", "Prasad Calyam", "Filiz Bunyak", "Matthew R. Maschmann", "Kannappan Palaniappan"], "title": "Automated Feature Tracking for Real-Time Kinematic Analysis and Shape Estimation of Carbon Nanotube Growth", "comment": "Accepted at IEEE/CVF ICCV 2025, CV4MS Workshop (Computer Vision for\n  Materials Science), Code available at: https://github.com/kavehsfv/VFTrack", "summary": "Carbon nanotubes (CNTs) are critical building blocks in nanotechnology, yet\nthe characterization of their dynamic growth is limited by the experimental\nchallenges in nanoscale motion measurement using scanning electron microscopy\n(SEM) imaging. Existing ex situ methods offer only static analysis, while in\nsitu techniques often require manual initialization and lack continuous\nper-particle trajectory decomposition. We present Visual Feature Tracking\n(VFTrack) an in-situ real-time particle tracking framework that automatically\ndetects and tracks individual CNT particles in SEM image sequences. VFTrack\nintegrates handcrafted or deep feature detectors and matchers within a particle\ntracking framework to enable kinematic analysis of CNT micropillar growth. A\nsystematic using 13,540 manually annotated trajectories identifies the ALIKED\ndetector with LightGlue matcher as an optimal combination (F1-score of 0.78,\n$\\alpha$-score of 0.89). VFTrack motion vectors decomposed into axial growth,\nlateral drift, and oscillations, facilitate the calculation of heterogeneous\nregional growth rates and the reconstruction of evolving CNT pillar\nmorphologies. This work enables advancement in automated nano-material\ncharacterization, bridging the gap between physics-based models and\nexperimental observation to enable real-time optimization of CNT synthesis.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2508.19242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19242", "abs": "https://arxiv.org/abs/2508.19242", "authors": ["Miran Heo", "Sukjun Hwang", "Min-Hung Chen", "Yu-Chiang Frank Wang", "Albert Gu", "Seon Joo Kim", "Ryo Hachiuma"], "title": "Autoregressive Universal Video Segmentation Model", "comment": null, "summary": "Recent video foundation models such as SAM2 excel at prompted video\nsegmentation by treating masks as a general-purpose primitive. However, many\nreal-world settings require unprompted segmentation that aims to detect and\ntrack all objects in a video without external cues, leaving today's landscape\nfragmented across task-specific models and pipelines. We recast streaming video\nsegmentation as sequential mask prediction, analogous to language modeling, and\nintroduce the Autoregressive Universal Segmentation Model (AUSM), a single\narchitecture that unifies both prompted and unprompted video segmentation.\nBuilt on recent state-space models, AUSM maintains a fixed-size spatial state\nand scales to video streams of arbitrary length. Furthermore, all components of\nAUSM are designed for parallel training across frames, yielding substantial\nspeedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS\n2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior\nuniversal streaming video segmentation methods and achieves up to 2.5x faster\ntraining on 16-frame sequences.", "AI": {"tldr": "AUSM是一个统一的自回归通用分割模型，它将流式视频分割重新定义为序列掩码预测，并利用状态空间模型同时处理有提示和无提示的视频分割任务，实现了更快的训练速度和更优的性能。", "motivation": "现有的视频基础模型（如SAM2）在有提示的视频分割方面表现出色，但许多实际场景需要无提示的分割（即在没有外部提示的情况下检测和跟踪视频中的所有对象），而目前该领域被任务特定的模型和管道所碎片化。研究者旨在解决这一问题，创建一个能统一处理有提示和无提示视频分割的通用模型。", "method": "该研究将流式视频分割重新定义为序列掩码预测（类似于语言建模），并引入了自回归通用分割模型（AUSM）。AUSM是一个单一架构，基于最新的状态空间模型构建，维护固定大小的空间状态，可扩展到任意长度的视频流。此外，AUSM的所有组件都设计用于跨帧并行训练，从而显著加快了训练速度。", "result": "在DAVIS17、YouTube-VOS 2018 & 2019、MOSE、YouTube-VIS 2019 & 2021以及OVIS等标准基准测试中，AUSM的性能超越了之前通用的流式视频分割方法，并且在16帧序列上实现了高达2.5倍的训练加速。", "conclusion": "AUSM成功地将有提示和无提示的视频分割任务统一到一个单一的架构中，通过创新的序列掩码预测和状态空间模型设计，不仅在性能上超越了现有方法，还在训练效率上取得了显著提升，为流式视频分割提供了一个通用且高效的解决方案。"}}
{"id": "2508.19243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19243", "abs": "https://arxiv.org/abs/2508.19243", "authors": ["Beiqi Chen", "Shuai Shao", "Haitang Feng", "Jianhuang Lai", "Jianlou Si", "Guangcong Wang"], "title": "Style4D-Bench: A Benchmark Suite for 4D Stylization", "comment": "Project page: https://becky-catherine.github.io/Style4D . Code:\n  https://github.com/Becky-catherine/Style4D-Bench", "summary": "We introduce Style4D-Bench, the first benchmark suite specifically designed\nfor 4D stylization, with the goal of standardizing evaluation and facilitating\nprogress in this emerging area. Style4D-Bench comprises: 1) a comprehensive\nevaluation protocol measuring spatial fidelity, temporal coherence, and\nmulti-view consistency through both perceptual and quantitative metrics, 2) a\nstrong baseline that make an initial attempt for 4D stylization, and 3) a\ncurated collection of high-resolution dynamic 4D scenes with diverse motions\nand complex backgrounds. To establish a strong baseline, we present Style4D, a\nnovel framework built upon 4D Gaussian Splatting. It consists of three key\ncomponents: a basic 4DGS scene representation to capture reliable geometry, a\nStyle Gaussian Representation that leverages lightweight per-Gaussian MLPs for\ntemporally and spatially aware appearance control, and a Holistic\nGeometry-Preserved Style Transfer module designed to enhance spatio-temporal\nconsistency via contrastive coherence learning and structural content\npreservation. Extensive experiments on Style4D-Bench demonstrate that Style4D\nachieves state-of-the-art performance in 4D stylization, producing fine-grained\nstylistic details with stable temporal dynamics and consistent multi-view\nrendering. We expect Style4D-Bench to become a valuable resource for\nbenchmarking and advancing research in stylized rendering of dynamic 3D scenes.\nProject page: https://becky-catherine.github.io/Style4D . Code:\nhttps://github.com/Becky-catherine/Style4D-Bench .", "AI": {"tldr": "本文介绍了Style4D-Bench，首个针对4D风格化设计的基准测试套件，以及Style4D，一个基于4D高斯泼溅的新型4D风格化框架，并在Style4D-Bench上取得了最先进的性能。", "motivation": "目前4D风格化领域缺乏标准化的评估方法和进展，需要一个综合性的基准来推动该领域的发展。", "method": "本文提出了Style4D-Bench基准套件，包含：1) 衡量空间保真度、时间连贯性和多视角一致性的评估协议；2) 一个强大的基线方法；3) 精心策划的高分辨率动态4D场景集合。作为基线，本文提出了Style4D框架，其基于4D高斯泼溅，包含三个关键组件：基础4DGS场景表示、利用轻量级每高斯MLP进行时空感知外观控制的Style Gaussian Representation，以及通过对比连贯性学习和结构内容保持来增强时空一致性的Holistic Geometry-Preserved Style Transfer模块。", "result": "在Style4D-Bench上的大量实验表明，Style4D在4D风格化方面取得了最先进的性能，能够生成具有稳定时间动态和一致多视角渲染的精细风格细节。", "conclusion": "Style4D-Bench有望成为动态3D场景风格化渲染基准测试和推进研究的宝贵资源。"}}
{"id": "2508.19244", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19244", "abs": "https://arxiv.org/abs/2508.19244", "authors": ["Oishi Deb", "Anjun Hu", "Ashkan Khakzar", "Philip Torr", "Christian Rupprecht"], "title": "Articulate3D: Zero-Shot Text-Driven 3D Object Posing", "comment": "Project page:https://odeb1.github.io/articulate3d_page_deb/", "summary": "We propose a training-free method, Articulate3D, to pose a 3D asset through\nlanguage control. Despite advances in vision and language models, this task\nremains surprisingly challenging. To achieve this goal, we decompose the\nproblem into two steps. We modify a powerful image-generator to create target\nimages conditioned on the input image and a text instruction. We then align the\nmesh to the target images through a multi-view pose optimisation step. In\ndetail, we introduce a self-attention rewiring mechanism (RSActrl) that\ndecouples the source structure from pose within an image generative model,\nallowing it to maintain a consistent structure across varying poses. We\nobserved that differentiable rendering is an unreliable signal for articulation\noptimisation; instead, we use keypoints to establish correspondences between\ninput and target images. The effectiveness of Articulate3D is demonstrated\nacross a diverse range of 3D objects and free-form text prompts, successfully\nmanipulating poses while maintaining the original identity of the mesh.\nQuantitative evaluations and a comparative user study, in which our method was\npreferred over 85\\% of the time, confirm its superiority over existing\napproaches. Project page:https://odeb1.github.io/articulate3d_page_deb/", "AI": {"tldr": "Articulate3D是一种无需训练的方法，通过语言控制来调整3D资产的姿态。它通过修改图像生成器生成目标图像，然后利用多视图姿态优化和关键点对齐网格，成功实现姿态操作同时保持物体身份。", "motivation": "尽管视觉和语言模型取得了进展，但通过语言控制来调整3D资产姿态的任务仍然具有挑战性。", "method": "该方法将问题分解为两步：1. 修改一个强大的图像生成器（引入RSActrl自注意力重布线机制，将源结构与姿态解耦），根据输入图像和文本指令创建目标图像。2. 通过多视图姿态优化步骤将网格与目标图像对齐。值得注意的是，该方法使用关键点建立输入和目标图像之间的对应关系，而不是依赖不可靠的可微分渲染信号。", "result": "Articulate3D在各种3D对象和自由形式文本提示下都表现出有效性，成功地操纵了姿态同时保持了网格的原始身份。定量评估和用户研究（用户偏好度超过85%）证实了其优于现有方法。", "conclusion": "Articulate3D提供了一种卓越的、无需训练的语言控制3D资产姿态调整方法，能够有效地操纵姿态同时保持物体身份，解决了现有方法的挑战。"}}
{"id": "2508.19247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19247", "abs": "https://arxiv.org/abs/2508.19247", "authors": ["Lin Li", "Zehuan Huang", "Haoran Feng", "Gengxiong Zhuang", "Rui Chen", "Chunchao Guo", "Lu Sheng"], "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space", "comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/", "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.", "AI": {"tldr": "VoxHammer 是一种无需训练的 3D 潜在空间局部编辑方法，通过保留上下文特征，实现了对指定区域的精确、连贯编辑，并能有效保持未编辑区域的一致性。", "motivation": "3D 局部编辑在游戏和机器人交互中至关重要，但现有方法通常通过编辑多视角图像再重建 3D 模型，难以精确保留未编辑区域并维持整体连贯性。", "method": "VoxHammer 受到结构化 3D 生成模型的启发，首先预测 3D 模型的反演轨迹，获取每个时间步的反演潜在特征和键值令牌。在去噪和编辑阶段，该方法用对应的反演潜在特征和缓存的键值令牌替换保留区域的去噪特征，从而保留上下文特征。此外，研究构建了 Edit3D-Bench 数据集来评估保留区域的一致性。", "result": "实验表明，VoxHammer 在保留区域的 3D 一致性和整体质量方面显著优于现有方法。该方法有望合成高质量的编辑配对数据，为上下文 3D 生成奠定数据基础。", "conclusion": "VoxHammer 提供了一种精确且连贯的 3D 局部编辑解决方案，有效解决了现有方法在保持未编辑区域一致性方面的挑战，并为未来的 3D 生成研究提供了数据基础。"}}
