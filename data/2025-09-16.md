<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.CV](#cs.CV) [Total: 144]
- [cs.CL](#cs.CL) [Total: 88]
- [cs.RO](#cs.RO) [Total: 54]
- [eess.SY](#eess.SY) [Total: 33]
- [eess.IV](#eess.IV) [Total: 15]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Situation Model of the Transport, Transport Emissions and Meteorological Conditions](https://arxiv.org/abs/2509.10541)
*V. Benes,M. Svitek,A. Michalikova,M. Melicherik*

Main category: cs.AI

TL;DR: 本文提出一个基于模糊推理系统(FIS)的模型，用于预测城市交通排放量如何受气象条件影响，旨在为城市规划者提供环保的交通管理洞察。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染及其减少是当今社会面临的重要挑战。研究旨在理解气象条件如何影响交通排放量及其扩散，以帮助城市规划者和政策制定者更有效地管理城市交通并保护环境。

Method: 采用系统方法分析交通排放与气象条件的关系，利用模糊推理系统（FIS）开发了一个预测模型。该模型基于在捷克布拉格测量的交通、气象和排放数据。

Result: 开发了一个基于模糊推理系统（FIS）的预测模型，能够根据不同的交通和气象条件预测排放量的变化。该模型旨在为城市规划者和政策制定者提供关于如何更有效规划和管理城市交通以保护环境的洞察。

Conclusion: 研究通过建立一个预测模型，揭示了气象条件对城市交通排放的影响，为城市规划者和政策制定者提供了在考虑环境保护的前提下，更有效规划和管理城市交通的实用见解。

Abstract: Air pollution in cities and the possibilities of reducing this pollution
represents one of the most important factors that today's society has to deal
with. This paper focuses on a systemic approach to traffic emissions with their
relation to meteorological conditions, analyzing the effect of weather on the
quantity and dispersion of traffic emissions in a city. Using fuzzy inference
systems (FIS) the model for prediction of changes in emissions depending on
various conditions is developed. The proposed model is based on traffic,
meteorology and emission data measured in Prague, Czech Republic. The main
objective of the work is to provide insight into how urban planners and
policymakers can plan and manage urban transport more effectively with
environmental protection in mind.

</details>


### [2] [ZapGPT: Free-form Language Prompting for Simulated Cellular Control](https://arxiv.org/abs/2509.10660)
*Nam H. Le,Patrick Erickson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 该研究首次展示了通过自由形式的自然语言提示，无需人工设计的奖励或特定任务监督，即可引导简单智能体（模拟细胞）的集体行为，并能泛化到新提示。


<details>
  <summary>Details</summary>
Motivation: 现有的人工智能和生物系统难以有效解释和响应自然语言，依赖于预设奖励、任务特定监督或固定指令集，限制了其泛化能力。研究旨在弥合这一鸿沟，实现对复杂、去中心化系统的自然语言控制。

Method: 该方法使用两个AI模型：一个AI模型将命令式提示转换为对模拟细胞的干预措施；另一个AI模型评估提示与细胞动态的匹配程度。前一个AI模型通过进化来提高后一个AI模型生成的评分。该方法不需要人工设计的适应度函数或领域特定的提示设计。

Result: 该系统成功地通过自由形式的语言提示引导了集体行为。与以往工作不同，它无需重新训练即可泛化到未见的提示。这表明自然语言可以作为控制层。

Conclusion: 这项工作为AI与生物学的合作愿景迈出了具体一步，即自然语言可以取代数学目标函数、固定规则和领域特定编程，指导计算、机器人或生物系统实现期望的行为。

Abstract: Human language is one of the most expressive tools for conveying intent, yet
most artificial or biological systems lack mechanisms to interpret or respond
meaningfully to it. Bridging this gap could enable more natural forms of
control over complex, decentralized systems. In AI and artificial life, recent
work explores how language can specify high-level goals, but most systems still
depend on engineered rewards, task-specific supervision, or rigid command sets,
limiting generalization to novel instructions. Similar constraints apply in
synthetic biology and bioengineering, where the locus of control is often
genomic rather than environmental perturbation.
  A key open question is whether artificial or biological collectives can be
guided by free-form natural language alone, without task-specific tuning or
carefully designed evaluation metrics. We provide one possible answer here by
showing, for the first time, that simple agents' collective behavior can be
guided by free-form language prompts: one AI model transforms an imperative
prompt into an intervention that is applied to simulated cells; a second AI
model scores how well the prompt describes the resulting cellular dynamics; and
the former AI model is evolved to improve the scores generated by the latter.
  Unlike previous work, our method does not require engineered fitness
functions or domain-specific prompt design. We show that the evolved system
generalizes to unseen prompts without retraining. By treating natural language
as a control layer, the system suggests a future in which spoken or written
prompts could direct computational, robotic, or biological systems to desired
behaviors. This work provides a concrete step toward this vision of AI-biology
partnerships, in which language replaces mathematical objective functions,
fixed rules, and domain-specific programming.

</details>


### [3] [Maestro: Self-Improving Text-to-Image Generation via Agent Orchestration](https://arxiv.org/abs/2509.10704)
*Xingchen Wan,Han Zhou,Ruoxi Sun,Hootan Nakhost,Ke Jiang,Rajarishi Sinha,Sercan Ö. Arık*

Main category: cs.AI

TL;DR: Maestro是一个新颖的自进化图像生成系统，它利用多模态大语言模型（MLLM）进行自我批判和自我进化，仅通过初始提示词就能自主迭代改进文本到图像（T2I）模型的生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）模型高度依赖人工干预，需要对初始提示词进行耗时且重复的手动工程，这带来了显著的可用性挑战。

Method: Maestro系统包含两项关键创新：1) 自我批判：专门的MLLM代理充当“评论家”，识别生成图像的弱点、纠正规范不足之处，并提供可解释的编辑信号，然后由“验证器”代理整合这些信号，同时保留用户意图。2) 自我进化：利用MLLM作为“评判者”对迭代生成的图像进行两两比较，排除有问题图像，并进化出符合用户意图的创意提示词候选。

Result: 在复杂的T2I任务上，使用黑盒模型进行的广泛实验表明，Maestro显著提高了图像质量，优于初始提示词和现有最先进的自动化方法，并且其有效性随着MLLM组件的先进性而增强。

Conclusion: 这项工作为实现T2I生成过程的自我改进提供了一条鲁棒、可解释且有效的途径。

Abstract: Text-to-image (T2I) models, while offering immense creative potential, are
highly reliant on human intervention, posing significant usability challenges
that often necessitate manual, iterative prompt engineering over often
underspecified prompts. This paper introduces Maestro, a novel self-evolving
image generation system that enables T2I models to autonomously self-improve
generated images through iterative evolution of prompts, using only an initial
prompt. Maestro incorporates two key innovations: 1) self-critique, where
specialized multimodal LLM (MLLM) agents act as 'critics' to identify
weaknesses in generated images, correct for under-specification, and provide
interpretable edit signals, which are then integrated by a 'verifier' agent
while preserving user intent; and 2) self-evolution, utilizing MLLM-as-a-judge
for head-to-head comparisons between iteratively generated images, eschewing
problematic images, and evolving creative prompt candidates that align with
user intents. Extensive experiments on complex T2I tasks using black-box models
demonstrate that Maestro significantly improves image quality over initial
prompts and state-of-the-art automated methods, with effectiveness scaling with
more advanced MLLM components. This work presents a robust, interpretable, and
effective pathway towards self-improving T2I generation.

</details>


### [4] [Understanding AI Evaluation Patterns: How Different GPT Models Assess Vision-Language Descriptions](https://arxiv.org/abs/2509.10707)
*Sajjad Abdoli,Rudi Cilibrasi,Rima Al-Shikh*

Main category: cs.AI

TL;DR: 本研究分析了不同GPT模型在评估AI生成内容时的“评估个性”及其偏见，发现评估能力不与通用能力线性相关，且存在家族特有的负面评估偏好。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地评估其他AI的输出，理解其评估行为对于防止级联偏见至关重要。

Method: 研究使用NVIDIA的Describe Anything Model生成的视觉-语言描述作为评估对象，并由GPT-4o、GPT-4o-mini和GPT-5进行评估。通过Gemini 2.5 Pro作为独立的提问生成器进行受控实验以验证评估个性的固有性。通过语义相似性分析评估模型生成的提问，进行跨家族分析。

Result: GPT-4o-mini表现出系统性一致性和最小方差；GPT-4o擅长错误检测；GPT-5则显示出极度保守和高变异性。这些“评估个性”是模型的固有属性。GPT模型在评估策略上高度相似并聚类，而Gemini则展现出显著不同的评估策略。所有GPT模型均表现出2:1的负面评估偏好（倾向于负面评估而非正面确认），但这似乎是家族特有的模式。

Conclusion: 评估能力并不与通用能力同步提升，稳健的AI评估需要多样化的架构视角。

Abstract: As AI systems increasingly evaluate other AI outputs, understanding their
assessment behavior becomes crucial for preventing cascading biases. This study
analyzes vision-language descriptions generated by NVIDIA's Describe Anything
Model and evaluated by three GPT variants (GPT-4o, GPT-4o-mini, GPT-5) to
uncover distinct "evaluation personalities" the underlying assessment
strategies and biases each model demonstrates. GPT-4o-mini exhibits systematic
consistency with minimal variance, GPT-4o excels at error detection, while
GPT-5 shows extreme conservatism with high variability. Controlled experiments
using Gemini 2.5 Pro as an independent question generator validate that these
personalities are inherent model properties rather than artifacts. Cross-family
analysis through semantic similarity of generated questions reveals significant
divergence: GPT models cluster together with high similarity while Gemini
exhibits markedly different evaluation strategies. All GPT models demonstrate a
consistent 2:1 bias favoring negative assessment over positive confirmation,
though this pattern appears family-specific rather than universal across AI
architectures. These findings suggest that evaluation competence does not scale
with general capability and that robust AI assessment requires diverse
architectural perspectives.

</details>


### [5] [AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework](https://arxiv.org/abs/2509.10762)
*Arlen Kumar,Leanid Palkhouski*

Main category: cs.AI

TL;DR: 本研究引入了GEO-16审计框架来评估AI答案引擎引用的网页质量。通过分析三个引擎的1702条引用，发现网页整体质量是引用强预测因子，并提供了发布商的实用指南。


<details>
  <summary>Details</summary>
Motivation: AI答案引擎在获取领域知识方面日益重要，通过生成回复和引用网页来源。本研究旨在理解这些引擎引用来源的质量，并识别影响引用的网页质量信号。

Method: 研究引入了GEO-16框架，将页面质量信号转换为带状支柱分数和归一化的GEO分数G（0到1）。使用70个产品意图提示，收集了来自Brave Summary、Google AI Overviews和Perplexity三个引擎的1702条引用，并审计了1100个独立URL。通过逻辑模型分析了页面质量与引用的关联。

Result: 研究发现，不同引擎引用的页面GEO质量存在差异。元数据和新鲜度、语义HTML以及结构化数据等支柱与引用表现出最强的关联。整体页面质量是引用的强预测因子，高G分数（例如G至少0.70）结合至少12个支柱命中，与数据中更高的引用率一致。

Conclusion: AI答案引擎引用的网页质量是影响其引用的重要因素，特别是元数据、语义HTML和结构化数据等信号。研究结果为发布商提供了提高其内容被AI引擎引用概率的实用指南。

Abstract: AI answer engines increasingly mediate access to domain knowledge by
generating responses and citing web sources. We introduce GEO-16, a 16 pillar
auditing framework that converts on page quality signals into banded pillar
scores and a normalized GEO score G that ranges from 0 to 1. Using 70 product
intent prompts, we collected 1,702 citations across three engines (Brave
Summary, Google AI Overviews, and Perplexity) and audited 1,100 unique URLs. In
our corpus, the engines differed in the GEO quality of the pages they cited,
and pillars related to Metadata and Freshness, Semantic HTML, and Structured
Data showed the strongest associations with citation. Logistic models with
domain clustered standard errors indicate that overall page quality is a strong
predictor of citation, and simple operating points (for example, G at least
0.70 combined with at least 12 pillar hits) align with substantially higher
citation rates in our data. We report per engine contrasts, vertical effects,
threshold analysis, and diagnostics, then translate findings into a practical
playbook for publishers. The study is observational and focuses on English
language B2B SaaS pages; we discuss limitations, threats to validity, and
reproducibility considerations.

</details>


### [6] [AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise](https://arxiv.org/abs/2509.10769)
*Tara Bogavelli,Roshnee Sharma,Hari Subramani*

Main category: cs.AI

TL;DR: 本研究通过企业级基准测试，评估了多智能体系统中不同设计维度（编排策略、提示实现、内存架构、思维工具）的18种智能体配置，发现模型特定的架构偏好，并揭示了企业任务中智能体性能的显著弱点。


<details>
  <summary>Details</summary>
Motivation: 尽管智能体架构的各个组件已被单独研究，但对于复杂多智能体系统中不同设计维度如何相互作用的实证理解仍然有限。此外，缺乏针对企业特定任务的全面基准评估。

Method: 本研究构建了一个全面的企业级特定基准测试，评估了18种不同的智能体配置，这些配置基于最先进的大型语言模型。考察了四个关键的智能体系统维度：编排策略、智能体提示实现（ReAct与函数调用）、内存架构和思维工具集成。

Result: 基准测试结果揭示了显著的模型特定架构偏好，这挑战了智能体AI系统中普遍存在的“一刀切”范式。同时，研究发现智能体在企业任务上的整体性能存在显著弱点，即使得分最高的模型在更复杂的任务上成功率也仅为35.3%，在更简单的任务上为70.8%。

Conclusion: 这些发现为未来智能体系统的设计提供了信息，有助于在架构组件和模型选择方面做出更多基于实证的决策。

Abstract: While individual components of agentic architectures have been studied in
isolation, there remains limited empirical understanding of how different
design dimensions interact within complex multi-agent systems. This study aims
to address these gaps by providing a comprehensive enterprise-specific
benchmark evaluating 18 distinct agentic configurations across state-of-the-art
large language models. We examine four critical agentic system dimensions:
orchestration strategy, agent prompt implementation (ReAct versus function
calling), memory architecture, and thinking tool integration. Our benchmark
reveals significant model-specific architectural preferences that challenge the
prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals
significant weaknesses in overall agentic performance on enterprise tasks with
the highest scoring models achieving a maximum of only 35.3\% success on the
more complex task and 70.8\% on the simpler task. We hope these findings inform
the design of future agentic systems by enabling more empirically backed
decisions regarding architectural components and model selection.

</details>


### [7] [LLM Enhancement with Domain Expert Mental Model to Reduce LLM Hallucination with Causal Prompt Engineering](https://arxiv.org/abs/2509.10818)
*Boris Kovalerchuk,Brent D. Fegley*

Main category: cs.AI

TL;DR: 本文提出一种基于优化人机对话和单调布尔/k值函数的技术，以发现可计算的专家心理模型（EMM），从而提高大型语言模型（LLM）在决策制定中的效率，克服其幻觉和知识缺失问题。


<details>
  <summary>Details</summary>
Motivation: 决策制定问题普遍存在且复杂。尽管LLM在决策支持方面引起兴趣，但其训练数据缺失导致幻觉，且RAG等增强方法仍不足以解决所有信息缺失问题。LLM难以捕捉领域专家的复杂心理模型，尤其在关键信息缺失时表现不佳，这促使研究如何更有效地利用LLM进行决策。

Method: 本文提出一种基于优化人机对话和单调布尔以及k值函数的技术，用于发现可计算的个人专家心理模型（EMM）。其LLM提示工程的EMM算法包含四个步骤：(1) 因子识别，(2) 因子的分层结构化，(3) 生成泛化的专家心理模型规范，以及 (4) 从该规范生成详细的泛化专家心理模型。

Result: 本文探索了LLM如何通过所提出的技术和四步算法，利用专家心理模型使决策制定更加高效。该方法旨在解决LLM在处理复杂决策和信息缺失方面的局限性，提供一种结构化的方法来构建和利用领域知识。

Conclusion: 通过优化人机对话和基于单调布尔/k值函数的算法来构建专家心理模型，可以有效提升LLM在决策制定中的效率和准确性，克服其固有的信息缺失和幻觉问题，从而更好地辅助复杂决策过程。

Abstract: Difficult decision-making problems abound in various disciplines and domains.
The proliferation of generative techniques, especially large language models
(LLMs), has excited interest in using them for decision support. However, LLMs
cannot yet resolve missingness in their training data, leading to
hallucinations. Retrieval-Augmented Generation (RAG) enhances LLMs by
incorporating external information retrieval, reducing hallucinations and
improving accuracy. Yet, RAG and related methods are only partial solutions, as
they may lack access to all necessary sources or key missing information. Even
everyday issues often challenge LLMs' abilities. Submitting longer prompts with
context and examples is one approach to address knowledge gaps, but designing
effective prompts is non-trivial and may not capture complex mental models of
domain experts. For tasks with missing critical information, LLMs are
insufficient, as are many existing systems poorly represented in available
documents. This paper explores how LLMs can make decision-making more
efficient, using a running example of evaluating whether to respond to a call
for proposals. We propose a technology based on optimized human-machine
dialogue and monotone Boolean and k-valued functions to discover a
computationally tractable personal expert mental model (EMM) of
decision-making. Our EMM algorithm for LLM prompt engineering has four steps:
(1) factor identification, (2) hierarchical structuring of factors, (3)
generating a generalized expert mental model specification, and (4) generating
a detailed generalized expert mental model from that specification.

</details>


### [8] [From Grounding to Skolemization: A Logic-Constrained Vector Symbolic Architecture for Complex Query Answering](https://arxiv.org/abs/2509.10837)
*Yuyin Lu,Hegang Chen,Yanghui Rao*

Main category: cs.AI

TL;DR: 在不完整知识图谱上的复杂查询回答（CQA）中，存在逻辑完备性和计算效率的权衡。本文提出了逻辑约束向量符号架构（LVSA），一个神经符号框架，通过可微分Skolem化模块、神经否定器和逻辑约束优化协议，解决了现有方法的局限性，实现了EFO1查询的普适性，并显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在不完整知识图谱上进行复杂查询回答（CQA，形式化为EFO1推理）时，逻辑完备性与计算效率之间存在根本性权衡。基于Grounding的方法固有地遭受组合爆炸问题，而大多数基于Skolem化的方法则忽略明确建模Skolem函数，从而损害逻辑一致性。

Method: 本文提出了逻辑约束向量符号架构（LVSA），一个神经符号框架。它统一了可微分的Skolem化模块和神经否定器，并引入了一个逻辑约束驱动的优化协议来协调几何和逻辑要求。

Result: 理论上，LVSA保证了对所有EFO1查询的普适性。在经验上，它优于最先进的基于Skolem化的方法，并且与基于Grounding的基线相比，将推理成本降低了几个数量级。

Conclusion: LVSA通过提供一个统一的神经符号框架，成功解决了不完整知识图谱上复杂查询回答中逻辑完备性和计算效率的权衡问题，实现了理论上的普适性，并在实践中显著提升了性能和效率。

Abstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs),
typically formalized as reasoning with Existential First-Order predicate logic
with one free variable (EFO$_1$), faces a fundamental trade-off between logical
soundness and computational efficiency. This work establishes the
Grounding-Skolemization dichotomy for systematically analyzing CQA methods
through the lens of formal logic. While Grounding-based methods inherently
suffer from combinatorial explosion, most Skolemization-based methods neglect
to explicitly model Skolem functions and compromise logical consistency. To
address these limitations, we propose the Logic-constrained Vector Symbolic
Architecture (LVSA), a neuro-symbolic framework that unifies a differentiable
Skolemization module and a neural negator, as well as a logical
constraint-driven optimization protocol to harmonize geometric and logical
requirements. Theoretically, LVSA guarantees universality for all EFO$_1$
queries. Empirically, it outperforms state-of-the-art Skolemization-based
methods and reduces inference costs by orders of magnitude compared to
Grounding-based baselines.

</details>


### [9] [Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?](https://arxiv.org/abs/2509.10875)
*Jesse Gardner,Vladimir A. Baulin*

Main category: cs.AI

TL;DR: 本文批判性地重新评估了人工智能中以“智能体”为中心范式的必要性和最优性，指出其概念模糊性和人类中心偏见可能限制了AI发展，并提议转向基于系统级动态和物质智能的非智能体框架。


<details>
  <summary>Details</summary>
Motivation: “智能体”概念深刻影响了AI研究，但其持续的概念模糊性及固有人类中心偏见可能构成了限制性框架，促使研究者重新审视其在AI发展中的必要性和最优性。

Method: 本文通过对相关文献的系统性回顾，解构了各种AI框架下的智能体范式，区分了智能体系统、能动系统和非智能体系统，并强调了定义和衡量自主性、目标导向性等属性的挑战。

Result: 研究发现，许多AI系统（特别是大型语言模型）的“智能体”框架虽然在启发式上有用，但可能具有误导性，并掩盖了底层的计算机制。论文提出应将重点转向基于系统级动态、世界建模和物质智能的框架。

Conclusion: 为了发展出鲁棒、可扩展且可能非拟人化的通用智能，有必要研究受复杂系统、生物学和非常规计算启发的非智能体和系统性框架。这不仅需要新的架构，还需要超越智能体隐喻，对智能本身进行根本性的重新思考。

Abstract: The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI)
research, guiding development from foundational theories to contemporary
applications like Large Language Model (LLM)-based systems. This paper
critically re-evaluates the necessity and optimality of this agent-centric
paradigm. We argue that its persistent conceptual ambiguities and inherent
anthropocentric biases may represent a limiting framework. We distinguish
between agentic systems (AI inspired by agency, often semi-autonomous, e.g.,
LLM-based agents), agential systems (fully autonomous, self-producing systems,
currently only biological), and non-agentic systems (tools without the
impression of agency). Our analysis, based on a systematic review of relevant
literature, deconstructs the agent paradigm across various AI frameworks,
highlighting challenges in defining and measuring properties like autonomy and
goal-directedness. We argue that the 'agentic' framing of many AI systems,
while heuristically useful, can be misleading and may obscure the underlying
computational mechanisms, particularly in Large Language Models (LLMs). As an
alternative, we propose a shift in focus towards frameworks grounded in
system-level dynamics, world modeling, and material intelligence. We conclude
that investigating non-agentic and systemic frameworks, inspired by complex
systems, biology, and unconventional computing, is essential for advancing
towards robust, scalable, and potentially non-anthropomorphic forms of general
intelligence. This requires not only new architectures but also a fundamental
reconsideration of our understanding of intelligence itself, moving beyond the
agent metaphor.

</details>


### [10] [Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding](https://arxiv.org/abs/2509.10931)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 本文提出了HaPLa，一种黑盒越狱技术，通过“溯因框架”和“符号编码”绕过大型语言模型（LLMs）的安全防护，实现了高攻击成功率，并揭示了LLM安全调优与实用性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: LLMs在各种任务中表现出色，但其被滥用于有害目的的潜力是一个重大隐患。研究通用越狱攻击有助于发现LLMs的内在弱点，从而加强防御。

Method: 本文提出了“有害提示洗白”（HaPLa）技术，仅需黑盒访问目标模型。该方法包含两种策略：1) “溯因框架”（abductive framing），指示LLMs推断有害活动的中间步骤，而非直接响应有害查询；2) “符号编码”（symbolic encoding），一种轻量灵活的方法，用于混淆有害内容，因当前LLMs主要对显式有害关键词敏感。

Result: 实验结果显示，HaPLa在GPT系列模型上实现了超过95%的攻击成功率，在所有目标模型上平均达到70%的成功率。对不同符号编码规则的进一步分析表明，在不显著降低LLMs对良性查询的帮助性前提下，安全地调优LLMs仍然是一个根本性挑战。

Conclusion: HaPLa是一种有效且广泛适用的越狱技术，能够利用LLMs的内在弱点。研究结果凸显了在增强LLM安全性与保持其在良性应用中的有用性之间进行权衡的根本性挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, but their potential misuse for harmful purposes remains a
significant concern. To strengthen defenses against such vulnerabilities, it is
essential to investigate universal jailbreak attacks that exploit intrinsic
weaknesses in the architecture and learning paradigms of LLMs. In response, we
propose \textbf{H}armful \textbf{P}rompt \textbf{La}undering (HaPLa), a novel
and broadly applicable jailbreaking technique that requires only black-box
access to target models. HaPLa incorporates two primary strategies: 1)
\textit{abductive framing}, which instructs LLMs to infer plausible
intermediate steps toward harmful activities, rather than directly responding
to explicit harmful queries; and 2) \textit{symbolic encoding}, a lightweight
and flexible approach designed to obfuscate harmful content, given that current
LLMs remain sensitive primarily to explicit harmful keywords. Experimental
results show that HaPLa achieves over 95% attack success rate on GPT-series
models and 70% across all targets. Further analysis with diverse symbolic
encoding rules also reveals a fundamental challenge: it remains difficult to
safely tune LLMs without significantly diminishing their helpfulness in
responding to benign queries.

</details>


### [11] [Public Data Assisted Differentially Private In-Context Learning](https://arxiv.org/abs/2509.10932)
*Seongho Joo,Hyukhun Koh,Kyomin Jung*

Main category: cs.AI

TL;DR: 本文提出一种结合公共数据和差分隐私的上下文学习（ICL）方法，旨在解决ICL中的隐私泄露问题，同时克服传统差分隐私导致的模型效用显著下降的挑战，有效平衡隐私保护和模型效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的上下文学习（ICL）存在通过提示词泄露私有数据的风险，尤其是在恶意攻击下。虽然差分隐私（DP）能提供强隐私保障，但其通常会大幅降低ICL的效用。

Method: 在保持差分隐私（DP）保障的前提下，将与任务相关的公共数据整合到ICL框架中。基于此，提出了一种私有上下文学习算法。

Result: 实验证明，该方法在公共数据辅助下显著提高了私有ICL的效用。此外，该方法对成员推断攻击具有鲁棒性，展示了经验性的隐私保护。

Conclusion: 所提出的方法通过利用公共数据，有效地平衡了私有上下文学习中的隐私保护和模型效用，并在实验中验证了其在提高效用和抵御攻击方面的能力。

Abstract: In-context learning (ICL) in Large Language Models (LLMs) has shown
remarkable performance across various tasks without requiring fine-tuning.
However, recent studies have highlighted the risk of private data leakage
through the prompt in ICL, especially when LLMs are exposed to malicious
attacks. While differential privacy (DP) provides strong privacy guarantees, it
often significantly reduces the utility of in-context learning (ICL). To
address this challenge, we incorporate task-related public data into the ICL
framework while maintaining the DP guarantee. Based on this approach, we
propose a private in-context learning algorithm that effectively balances
privacy protection and model utility. Through experiments, we demonstrate that
our approach significantly improves the utility of private ICL with the
assistance of public data. Additionally, we show that our method is robust
against membership inference attacks, demonstrating empirical privacy
protection.

</details>


### [12] [Enhancing Computational Cognitive Architectures with LLMs: A Case Study](https://arxiv.org/abs/2509.10972)
*Ron Sun*

Main category: cs.AI

TL;DR: 本文探讨了将大型语言模型（LLMs）整合到认知架构中以结合计算能力和心理真实性的方法，并以Clarion架构为例进行了案例研究。


<details>
  <summary>Details</summary>
Motivation: 计算认知架构在心理学上是合理的，但其计算能力有限。大型语言模型（LLMs）已被证明具有卓越的计算能力。因此，为了同时处理现实世界的复杂性和心理真实性，将LLMs整合到认知架构中变得至关重要。

Method: 以Clarion认知架构为例，提出了一种与LLMs协同结合的方法。该方法利用了Clarion架构中固有的内隐-外显二分法，以实现Clarion和LLMs的无缝集成。

Result: 通过这种整合，LLMs的计算能力与Clarion的心理学精妙性相结合。

Conclusion: 将LLMs整合到认知架构（如Clarion，通过利用其内隐-外显二分法）中，可以有效提升模型的计算能力和心理学真实性，从而更好地应对现实世界的复杂性。

Abstract: Computational cognitive architectures are broadly scoped models of the human
mind that combine different psychological functionalities (as well as often
different computational methods for these different functionalities) into one
unified framework. They structure them in a psychologically plausible and
validated way. However, such models thus far have only limited computational
capabilities, mostly limited by the computational tools and techniques that
were adopted. More recently, LLMs have proved to be more capable
computationally than any other tools. Thus, in order to deal with both
real-world complexity and psychological realism at the same time, incorporating
LLMs into cognitive architectures naturally becomes an important task. In the
present article, a synergistic combination of the Clarion cognitive
architecture and LLMs is discussed as a case study. The implicit-explicit
dichotomy that is fundamental to Clarion is leveraged for a seamless
integration of Clarion and LLMs. As a result, computational power of LLMs is
combined with psychological nicety of Clarion.

</details>


### [13] [Rethinking Human Preference Evaluation of LLM Rationales](https://arxiv.org/abs/2509.11026)
*Ziang Li,Manasi Ganti,Zixian Ma,Helena Vasconcelos,Qijia He,Ranjay Krishna*

Main category: cs.AI

TL;DR: 本研究通过引入细粒度的属性评估来重新思考LLM生成理由的偏好评估，以克服传统二元比较的局限性，从而更好地理解理由质量并指导未来的评估实践。


<details>
  <summary>Details</summary>
Motivation: LLM生成的自然语言理由有助于提高复杂推理任务的性能和增强可解释性。然而，现有的理由评估（如人类或LLM评判的二元偏好）通常不透明且粗糙，无法深入了解一个理由优于另一个理由的原因。

Method: 1. 从现有文献中识别了一组关键的理由属性。2. 使用自动化指标、LLM判断和人类标注来评估这些属性。3. 使用SHAP分析了MT Bench和Chatbot Arena这两个标准人类偏好数据集，以确定哪些属性最能解释人类偏好结果。4. 使用属性特定的ELO分数重新评估了模型生成的理由。

Result: 研究发现，细粒度的属性评估能够提供更细致的模型比较和洞察。这些评估揭示了传统二元比较无法捕捉到的理由质量差异。

Conclusion: 研究结果表明，细粒度的属性评估能更好地表征理由质量，并能指导未来的研究走向更可解释和可靠的评估实践。

Abstract: Large language models (LLMs) often generate natural language rationales --
free-form explanations that help improve performance on complex reasoning tasks
and enhance interpretability for human users. However, evaluating these
rationales remains challenging. While recent work has relied on binary
preference judgments from humans or LLM judges, such evaluations are often
opaque and coarse-grained, offering limited insight into what makes one
rationale better than another. In this work, we rethink preference evaluation
for LLM-generated rationales by asking: (1) What attributes define good
rationales? (2) Can human preferences be explained by these attributes? (3) Can
attribute-based evaluation overcome the limitations of binary comparisons? We
identify a set of key rationale attributes from prior literature and assess
them using automatic metrics, LLM judgments, and human annotations. We then
analyze two standard human preference datasets MT Bench and Chatbot Arena using
SHAP to identify which attributes best explain human preference outcomes.
Finally, we re-evaluate model-generated rationales using attribute-specific ELO
scores, revealing more nuanced model comparisons and insights. Our findings
suggest that fine-grained attribute evaluations can better characterize
rationale quality and guide future research toward more interpretable and
reliable evaluation practices.

</details>


### [14] [Free-MAD: Consensus-Free Multi-Agent Debate](https://arxiv.org/abs/2509.11035)
*Yu Cui,Hang Fu,Haibin Zhang,Licheng Wang,Cong Zuo*

Main category: cs.AI

TL;DR: 本文提出了一种名为\textsc{Free-MAD}的新型多智能体辩论（MAD）框架，通过消除智能体间的共识需求、引入基于分数的决策机制和反从众机制，显著提高了LLM的推理性能，同时降低了成本并增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MAD方法存在局限性：多轮交互导致高token开销和可扩展性受限；LLM固有的从众性导致错误传播；多数投票决策引入随机性和不公平性，可能降低推理性能。

Method: \textsc{Free-MAD}通过以下方式解决问题：1) 消除智能体间的共识需求。2) 引入新颖的基于分数的决策机制，评估整个辩论轨迹而非仅最后一轮，以实现更准确和公平的结果。3) 通过引入反从众机制重建辩论阶段，使智能体能减轻多数的过度影响。4) 仅需单轮辩论。

Result: 在八个基准数据集上的实验表明，\textsc{Free-MAD}显著提高了推理性能，同时由于只需单轮辩论而降低了token成本。与现有MAD方法相比，\textsc{Free-MAD}在真实世界攻击场景中也表现出更高的鲁棒性。

Conclusion: \textsc{Free-MAD}通过其无共识、基于分数的决策和反从众机制，成功克服了传统MAD方法的缺点，为提高LLM推理能力提供了一个更高效、准确和鲁棒的框架。

Abstract: Multi-agent debate (MAD) is an emerging approach to improving the reasoning
capabilities of large language models (LLMs). Existing MAD methods rely on
multiple rounds of interaction among agents to reach consensus, and the final
output is selected by majority voting in the last round. However, this
consensus-based design faces several limitations. First, multiple rounds of
communication increases token overhead and limits scalability. Second, due to
the inherent conformity of LLMs, agents that initially produce correct
responses may be influenced by incorrect ones during the debate process,
causing error propagation. Third, majority voting introduces randomness and
unfairness in the decision-making phase, and can degrade the reasoning
performance.
  To address these issues, we propose \textsc{Free-MAD}, a novel MAD framework
that eliminates the need for consensus among agents. \textsc{Free-MAD}
introduces a novel score-based decision mechanism that evaluates the entire
debate trajectory rather than relying on the last round only. This mechanism
tracks how each agent's reasoning evolves, enabling more accurate and fair
outcomes. In addition, \textsc{Free-MAD} reconstructs the debate phase by
introducing anti-conformity, a mechanism that enables agents to mitigate
excessive influence from the majority. Experiments on eight benchmark datasets
demonstrate that \textsc{Free-MAD} significantly improves reasoning performance
while requiring only a single-round debate and thus reducing token costs. We
also show that compared to existing MAD approaches, \textsc{Free-MAD} exhibits
improved robustness in real-world attack scenarios.

</details>


### [15] [Agentic Lybic: Multi-Agent Execution System with Tiered Reasoning and Orchestration](https://arxiv.org/abs/2509.11067)
*Liangxuan Guo,Bin Zhu,Qingqian Tao,Kangning Liu,Xun Zhao,Xianzhe Qin,Jin Gao,Guangfu Hao*

Main category: cs.AI

TL;DR: Agentic Lybic是一种基于有限状态机（FSM）的新型多智能体系统，通过动态编排和持续质量控制，显著提升了桌面自动化在复杂多步骤任务中的成功率，并在OSWorld基准测试中达到了最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 现有的桌面自动化自主智能体在处理复杂多步骤任务时，由于协调性差和质量控制不足，表现不佳。

Method: 引入Agentic Lybic系统，其核心架构是一个有限状态机（FSM），实现了动态编排。该系统包含控制器、管理器、三个工人智能体（用于代码操作的Technician、用于GUI交互的Operator、用于决策支持的Analyst）以及一个评估器。FSM基于的组件间路由是关键机制，它通过动态选择每个子任务的最佳执行策略，提供了灵活性和泛化能力。结合鲁棒的质量门控，实现了自适应重规划和错误恢复。

Result: 在OSWorld基准测试中，Agentic Lybic在50步内达到了57.07%的成功率，显著优于现有方法，并取得了最先进的性能。

Conclusion: 有原则的多智能体编排与持续质量控制相结合，为复杂计算环境中的通用桌面自动化提供了卓越的可靠性。

Abstract: Autonomous agents for desktop automation struggle with complex multi-step
tasks due to poor coordination and inadequate quality control. We introduce
\textsc{Agentic Lybic}, a novel multi-agent system where the entire
architecture operates as a finite-state machine (FSM). This core innovation
enables dynamic orchestration. Our system comprises four components: a
Controller, a Manager, three Workers (Technician for code-based operations,
Operator for GUI interactions, and Analyst for decision support), and an
Evaluator. The critical mechanism is the FSM-based routing between these
components, which provides flexibility and generalization by dynamically
selecting the optimal execution strategy for each subtask. This principled
orchestration, combined with robust quality gating, enables adaptive replanning
and error recovery. Evaluated officially on the OSWorld benchmark,
\textsc{Agentic Lybic} achieves a state-of-the-art 57.07\% success rate in 50
steps, substantially outperforming existing methods. Results demonstrate that
principled multi-agent orchestration with continuous quality control provides
superior reliability for generalized desktop automation in complex computing
environments.

</details>


### [16] [Tractable Asymmetric Verification for Large Language Models via Deterministic Replicability](https://arxiv.org/abs/2509.11068)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.AI

TL;DR: 本文提出了一种验证框架，用于在多智能体LLM系统中建立计算信任，确保LLM输出的真实性，其验证成本远低于生成成本，并能有效分配验证工作量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）向动态、多智能体系统快速发展，如何建立计算信任成为一个根本性挑战。具体而言，一个智能体如何验证另一个智能体的输出确实由声称的LLM生成，而不是伪造或由更便宜、更劣质的模型生成。

Method: 该方法基于自回归模型的确定性可复制性原理，要求所有智能体在相同的硬件和软件堆栈上操作，形成一个计算同质环境。在此框架下，多个验证者可以概率性地审计LLM输出的小随机片段，并有效分配验证工作量，实现可处理的非对称工作量（验证成本远低于执行成本）。

Result: 模拟结果表明，目标验证比完全重新生成快12倍以上，并且具有可调节的参数来调整检测概率。

Conclusion: 通过为可审计的LLM系统建立一个可行的机制，这项工作为负责任的AI奠定了基础，并为未来更复杂、异构的多智能体系统研究提供了基石。

Abstract: The landscape of Large Language Models (LLMs) shifts rapidly towards dynamic,
multi-agent systems. This introduces a fundamental challenge in establishing
computational trust, specifically how one agent can verify that another's
output was genuinely produced by a claimed LLM, and not falsified or generated
by a cheaper or inferior model. To address this challenge, this paper proposes
a verification framework that achieves tractable asymmetric effort, where the
cost to verify a computation is substantially lower than the cost to perform
it. Our approach is built upon the principle of deterministic replicability, a
property inherent to autoregressive models that strictly necessitates a
computationally homogeneous environment where all agents operate on identical
hardware and software stacks. Within this defined context, our framework
enables multiple validators to probabilistically audit small, random segments
of an LLM's output and it distributes the verification workload effectively.
The simulations demonstrated that targeted verification can be over 12 times
faster than full regeneration, with tunable parameters to adjust the detection
probability. By establishing a tractable mechanism for auditable LLM systems,
our work offers a foundational layer for responsible AI and serves as a
cornerstone for future research into the more complex, heterogeneous
multi-agent systems.

</details>


### [17] [Patient-Zero: A Unified Framework for Real-Record-Free Patient Agent Generation](https://arxiv.org/abs/2509.11078)
*Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 该研究提出了Patient-Zero框架，利用大型语言模型生成逼真且可交互的合成患者数据，无需真实医疗记录，解决了现有方法在隐私、准确性、多样性和交互性方面的局限性，并能显著提升现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有利用大型语言模型生成合成医疗数据的方法主要通过改写或补全现有记录，但仍存在数据隐私、准确性和多样性方面的限制，并且缺乏像真实患者一样进行交互的能力。该研究旨在解决这些问题。

Method: Patient-Zero框架无需真实医疗记录。它首先引入了一种医学对齐的多步生成架构，通过分层医学知识注入构建全面的患者记录。其次，设计了一个动态更新机制，以优化虚拟患者与人类的交互能力，提高对话的一致性和表现。该框架还支持自适应对话策略和实时临床合理性验证。

Result: Patient-Zero能够生成上下文多样且医学连贯的患者记录，并在准确性、多样性和一致性方面表现良好。使用Patient-Zero生成的虚拟患者进行训练后，现有模型在MedQA数据集上显示出显著的性能提升。

Conclusion: Patient-Zero框架通过生成逼真、可交互的合成患者数据，有效克服了现有方法的局限性，为医疗领域的数据收集和模型训练提供了一个无需真实记录的创新解决方案，并能显著提高现有医疗模型的性能。

Abstract: Synthetic data generation using large language models (LLMs) has emerged as a
promising solution across various domains, particularly in medical field, to
mitigate data collection challenges. However, existing studies mainly utilize
LLMs to rewrite and complete existing medical records, where the limitations in
data privacy, accuracy, and diversity sill exist, and additionally lack the
ability to interact like real patients. To address these issues, we propose a
realistic patient generation framework, Patient-Zero, which requires no real
medical records. Patient-Zero first introduces a medically-aligned multi-step
generation architecture, which builds comprehensive patient records through
hierarchical medical knowledge injection without real medical records. Then, to
optimize the virtual patient's interaction abilities with humans, Patient-Zero
designs a dynamic updating mechanism to improve the consistency and
conversational performance. Our framework enables the generation of
contextually diverse patient records while maintaining strict medical
coherence, supported by adaptive dialogue strategies and real-time clinical
plausibility verification. Experimental results demonstrate that our model
achieves good performance in accuracy, diversity, and consistency. After
training with our generated virtual patients, existing models show significant
improvements on the MedQA dataset.

</details>


### [18] [Difficulty-Aware Agent Orchestration in LLM-Powered Workflows](https://arxiv.org/abs/2509.11079)
*Jinwei Su,Yinghui Xia,Qizhen Lan,Xinyuan Song,Yang Jingsong,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: DAAO是一个动态框架，通过根据输入查询的难度自适应地调整工作流深度、操作符选择和LLM分配，以优化LLM代理系统的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多代理框架通常依赖静态或任务级工作流，这导致对简单查询过度处理或对复杂查询表现不佳，并且忽略了异构LLM之间的效率-性能权衡。

Method: DAAO框架包含三个相互依赖的模块：用于难度估计的变分自编码器（VAE）、模块化操作符分配器以及成本和性能感知的LLM路由器。它通过利用异构LLM并动态调整工作流来实现细粒度、查询特定的推理策略。

Result: DAAO在六个基准测试中，其准确性和推理效率均优于之前的多代理系统。

Conclusion: DAAO通过利用异构LLM和动态定制工作流，实现了细粒度、查询特定的推理策略，有效解决了现有框架的局限性。

Abstract: Large Language Model (LLM)-based agentic systems have shown strong
capabilities across various tasks. However, existing multi-agent frameworks
often rely on static or task-level workflows, which either over-process simple
queries or underperform on complex ones, while also neglecting the
efficiency-performance trade-offs across heterogeneous LLMs. To address these
limitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a
dynamic framework that adapts workflow depth, operator selection, and LLM
assignment based on the difficulty of each input query. DAAO comprises three
interdependent modules: a variational autoencoder (VAE) for difficulty
estimation, a modular operator allocator, and a cost- and performance-aware LLM
router. By leveraging heterogeneous LLMs and dynamically tailoring workflows,
DAAO enables fine-grained, query-specific reasoning strategies. DAAO
outperforms prior multi-agent systems in both accuracy and inference efficiency
across six benchmarks. We will release our code and implementation details upon
publication.

</details>


### [19] [Neural cellular automata: applications to biology and beyond classical AI](https://arxiv.org/abs/2509.11131)
*Benedikt Hartl,Michael Levin,Léo Pio-Lopez*

Main category: cs.AI

TL;DR: 神经元元胞自动机（NCA）是一个强大的框架，通过可训练规则模拟生物自组织，适用于生物学、生物工程、机器人控制乃至现代生成式AI，展现出鲁棒、自适应和去中心化控制的能力，有望成为连接多尺度生物学与生成式AI的统一范式。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的系统无法捕捉生物体的自适应、自调节动态。NCA通过引入可训练、可微分（或可进化）的更新规则，将人工神经网络嵌入作为局部决策中心，从而能更好地模拟生物自组织过程，并在近期发展中取得了巨大成功。本文旨在综述NCA在生物学和生物工程应用中的最新文献，并强调其在生物学以外（如机器人控制、AI推理）的潜力，以及其作为统一计算范式的价值。

Method: NCA的核心方法是将人工神经网络（ANNs）作为局部决策中心，定义局部智能体之间的交互规则。这些更新规则是可训练的、可微分的或可进化的，从而允许系统在没有中心控制的情况下，通过局部交互实现复杂的集体行为和自组织。

Result: NCA能够模拟分子、细胞、组织和系统层面的生物过程，重现生物启发的目标模式，并推广到新条件，表现出对扰动的鲁棒性以及开放式适应和推理能力。它们在没有集中控制的情况下展现出鲁棒且泛化的目标导向动态（例如在机器人形态控制或AI推理任务ARC-AGI-1中）。此外，NCA的迭代状态细化原理与现代生成式AI（如概率扩散模型）有异曲同工之妙。

Conclusion: NCA构成了一个统一的、计算精简的范式，不仅能够连接多尺度生物学的基本洞察与现代生成式AI，而且有潜力设计出真正受生物启发的、能够进行分层推理和控制的集体智能。

Abstract: Neural Cellular Automata (NCA) represent a powerful framework for modeling
biological self-organization, extending classical rule-based systems with
trainable, differentiable (or evolvable) update rules that capture the adaptive
self-regulatory dynamics of living matter. By embedding Artificial Neural
Networks (ANNs) as local decision-making centers and interaction rules between
localized agents, NCA can simulate processes across molecular, cellular,
tissue, and system-level scales, offering a multiscale competency architecture
perspective on evolution, development, regeneration, aging, morphogenesis, and
robotic control. These models not only reproduce biologically inspired target
patterns but also generalize to novel conditions, demonstrating robustness to
perturbations and the capacity for open-ended adaptation and reasoning. Given
their immense success in recent developments, we here review current literature
of NCAs that are relevant primarily for biological or bioengineering
applications. Moreover, we emphasize that beyond biology, NCAs display robust
and generalizing goal-directed dynamics without centralized control, e.g., in
controlling or regenerating composite robotic morphologies or even on
cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same
principles of iterative state-refinement is reminiscent to modern generative
Artificial Intelligence (AI), such as probabilistic diffusion models. Their
governing self-regulatory behavior is constraint to fully localized
interactions, yet their collective behavior scales into coordinated
system-level outcomes. We thus argue that NCAs constitute a unifying
computationally lean paradigm that not only bridges fundamental insights from
multiscale biology with modern generative AI, but have the potential to design
truly bio-inspired collective intelligence capable of hierarchical reasoning
and control.

</details>


### [20] [AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment](https://arxiv.org/abs/2509.11135)
*Jing Xiao,Chang You,Zhiyu Chen*

Main category: cs.AI

TL;DR: AlignKT提出了一种前端到后端的架构，通过与基于教学理论的理想知识状态对齐，显式建模稳定的学习者知识状态，从而提高了知识追踪的解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识追踪（KT）模型主要关注拟合学习者交互序列，但往往忽视知识状态本身，这导致解释性降低，并限制了智能辅导系统（ITS）提供足够的教学支持。

Method: AlignKT采用前端到后端的架构来显式建模稳定的知识状态。它将初步知识状态与一个额外的标准对齐，具体来说，将基于教学理论定义的理想知识状态作为对齐标准。该方法利用五个编码器实现，并结合对比学习模块以增强对齐过程的鲁棒性。

Result: 通过广泛实验，AlignKT在三个真实世界数据集中表现出卓越的性能，超越了七个知识追踪基线模型。它在其中两个数据集上达到了最先进的结果，并在第三个数据集上展现出具有竞争力的表现。

Conclusion: AlignKT通过显式建模稳定且可解释的知识状态，有效解决了现有知识追踪模型的局限性，显著提升了知识追踪的性能和对教学支持的潜力。

Abstract: Knowledge Tracing (KT) serves as a fundamental component of Intelligent
Tutoring Systems (ITS), enabling these systems to monitor and understand
learners' progress by modeling their knowledge state. However, many existing KT
models primarily focus on fitting the sequences of learners' interactions, and
often overlook the knowledge state itself. This limitation leads to reduced
interpretability and insufficient instructional support from the ITS. To
address this challenge, we propose AlignKT, which employs a frontend-to-backend
architecture to explicitly model a stable knowledge state. In this approach,
the preliminary knowledge state is aligned with an additional criterion.
Specifically, we define an ideal knowledge state based on pedagogical theories
as the alignment criterion, providing a foundation for interpretability. We
utilize five encoders to implement this set-up, and incorporate a contrastive
learning module to enhance the robustness of the alignment process. Through
extensive experiments, AlignKT demonstrates superior performance, outperforming
seven KT baselines on three real-world datasets. It achieves state-of-the-art
results on two of these datasets and exhibits competitive performance on the
third. The code of this work is available at
https://github.com/SCNU203/AlignKT.

</details>


### [21] [AI-Generated Content in Cross-Domain Applications: Research Trends, Challenges and Propositions](https://arxiv.org/abs/2509.11151)
*Jianxin Li,Liang Qu,Taotao Cai,Zhixue Zhao,Nur Al Hasan Haldar,Aneesh Krishna,Xiangjie Kong,Flavio Romero Macau,Tanmoy Chakraborty,Aniket Deroy,Binshan Lin,Karen Blackmore,Nasimul Noman,Jingxian Cheng,Ningning Cui,Jianliang Xu*

Main category: cs.AI

TL;DR: 这篇论文汇集了多学科的16位学者，从跨领域视角全面探讨了AIGC的最新进展、发展趋势、社会影响及面临的技术挑战，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管AIGC已广泛应用于多个领域并取得了显著成果，但目前很少有研究能从跨领域视角探讨其最新进展和新兴挑战，本研究旨在弥补这一空白。

Method: 本研究汇集了来自多个学科的16位学者，通过提供以下内容来达到目的：1) 概述生成式AI的训练技术、检测方法以及AI生成内容在数字平台上的传播和使用。2) 介绍AIGC在不同领域的社会影响，并回顾现有方法。3) 讨论关键技术挑战并提出研究命题以指导未来工作。这是一篇愿景论文。

Result: 论文提供了AIGC的广泛概述，涵盖了生成式AI的训练技术、检测方法及其在数字平台上的传播和使用；阐述了AIGC在不同领域的社会影响并回顾了现有方法；讨论了关键技术挑战并提出了指导未来工作的研究命题。

Conclusion: 这篇愿景论文通过提供跨领域的视角，深入探讨了AIGC的当前研究趋势、持续挑战和未来发展方向，为读者提供了全面的见解。

Abstract: Artificial Intelligence Generated Content (AIGC) has rapidly emerged with the
capability to generate different forms of content, including text, images,
videos, and other modalities, which can achieve a quality similar to content
created by humans. As a result, AIGC is now widely applied across various
domains such as digital marketing, education, and public health, and has shown
promising results by enhancing content creation efficiency and improving
information delivery. However, there are few studies that explore the latest
progress and emerging challenges of AIGC across different domains. To bridge
this gap, this paper brings together 16 scholars from multiple disciplines to
provide a cross-domain perspective on the trends and challenges of AIGC.
Specifically, the contributions of this paper are threefold: (1) It first
provides a broader overview of AIGC, spanning the training techniques of
Generative AI, detection methods, and both the spread and use of AI-generated
content across digital platforms. (2) It then introduces the societal impacts
of AIGC across diverse domains, along with a review of existing methods
employed in these contexts. (3) Finally, it discusses the key technical
challenges and presents research propositions to guide future work. Through
these contributions, this vision paper seeks to offer readers a cross-domain
perspective on AIGC, providing insights into its current research trends,
ongoing challenges, and future directions.

</details>


### [22] [VideoAgent: Personalized Synthesis of Scientific Videos](https://arxiv.org/abs/2509.11253)
*Xiao Liang,Bangxin Li,Zixuan Chen,Hanyue Zheng,Zhi Ma,Di Wang,Cong Tian,Quan Wang*

Main category: cs.AI

TL;DR: VideoAgent是一个多智能体框架，通过对话界面自动化生成个性化的科学视频，并提出了SciVidEval评估套件，其性能显著优于现有商业服务。


<details>
  <summary>Details</summary>
Motivation: 现有的文档自动化主要关注海报和幻灯片等静态媒体，缺乏个性化动态编排和多模态内容同步的机制，以有效传播科学知识。

Method: 引入了VideoAgent，一个新颖的多智能体框架，通过对话界面合成个性化科学视频。它将源论文解析为细粒度资产库，并根据用户需求编排叙事流程，合成静态幻灯片和动态动画。为进行严格评估，还提出了SciVidEval，一个综合评估套件，结合了多模态内容质量和同步的自动化指标，以及基于视频测验的人工评估来衡量知识转移。

Result: 实验证明，VideoAgent显著优于现有的商业科学视频生成服务，并在科学传播方面接近人类水平的质量。

Conclusion: VideoAgent成功解决了科学视频生成中的挑战，通过个性化、动态编排和多模态内容同步，实现了高效的知识传播，并达到了接近人类水平的沟通质量。

Abstract: Automating the generation of scientific videos is a crucial yet challenging
task for effective knowledge dissemination. However, existing works on document
automation primarily focus on static media such as posters and slides, lacking
mechanisms for personalized dynamic orchestration and multimodal content
synchronization. To address these challenges, we introduce VideoAgent, a novel
multi-agent framework that synthesizes personalized scientific videos through a
conversational interface. VideoAgent parses a source paper into a fine-grained
asset library and, guided by user requirements, orchestrates a narrative flow
that synthesizes both static slides and dynamic animations to explain complex
concepts. To enable rigorous evaluation, we also propose SciVidEval, the first
comprehensive suite for this task, which combines automated metrics for
multimodal content quality and synchronization with a Video-Quiz-based human
evaluation to measure knowledge transfer. Extensive experiments demonstrate
that our method significantly outperforms existing commercial scientific video
generation services and approaches human-level quality in scientific
communication.

</details>


### [23] [Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble](https://arxiv.org/abs/2509.11311)
*Bingchen Wang,Zi-Yu Khoo,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的对齐框架，将大型语言模型（LLMs）视为人类调查受访者的代理，以经济高效且可控的方式解决社会科学中调查成本上升和人口结构失衡的问题。该方法通过构建多样化的代理角色并选择代表性子集来模拟人口响应模式，并在真实世界数据上展示了高保真度。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究面临调查部署成本不断上升以及调查响应数据中人口结构日益失衡的挑战。LLMs在模拟人类响应方面展现出巨大潜力，为解决这些问题提供了新的思路。

Method: 该研究提出了一种将LLMs作为人类调查受访者代理的对齐框架。受“显示偏好理论”启发，对齐被公式化为两阶段问题：首先，构建模拟可行受访者画像的多样化代理角色（称为“禀赋”）；其次，基于观测数据选择一个代表性子集以近似真实人口。为实现此范式，研究引入了P2P系统，该系统利用结构化提示工程、基于熵的采样和基于回归的选择来引导LLM代理生成代表性行为模式。该方法与人口统计学无关，仅依赖于聚合调查结果。

Result: 在真实世界的意见调查数据集上，该方法展现了其有效性。对齐后的代理群体能够以高保真度重现聚合响应模式，并且即使在没有人口统计学条件限制的情况下，也表现出显著的响应多样性。

Conclusion: 该框架为社会科学研究提供了一种成本效益高且可控的解决方案，改善了数据效率，并为研究多元对齐的操作化提供了试验平台。其“与人口统计学无关”和仅依赖聚合结果的特性，提供了更好的泛化能力和简洁性。

Abstract: Large language models (LLMs) have demonstrated promise in emulating
human-like responses across a wide range of tasks. In this paper, we propose a
novel alignment framework that treats LLMs as agent proxies for human survey
respondents, affording a cost-effective and steerable solution to two pressing
challenges in the social sciences: the rising cost of survey deployment and the
growing demographic imbalance in survey response data. Drawing inspiration from
the theory of revealed preference, we formulate alignment as a two-stage
problem: constructing diverse agent personas called endowments that simulate
plausible respondent profiles, and selecting a representative subset to
approximate a ground-truth population based on observed data. To implement the
paradigm, we introduce P2P, a system that steers LLM agents toward
representative behavioral patterns using structured prompt engineering,
entropy-based sampling, and regression-based selection. Unlike
personalization-heavy approaches, our alignment approach is
demographic-agnostic and relies only on aggregate survey results, offering
better generalizability and parsimony. Beyond improving data efficiency in
social science research, our framework offers a testbed for studying the
operationalization of pluralistic alignment. We demonstrate the efficacy of our
approach on real-world opinion survey datasets, showing that our aligned agent
populations can reproduce aggregate response patterns with high fidelity and
exhibit substantial response diversity, even without demographic conditioning.

</details>


### [24] [Decoding Plastic Toxicity: An Intelligent Framework for Conflict-Aware Relational Metapath Extraction from Scientific Abstracts](https://arxiv.org/abs/2509.11330)
*Sudeshna Jana,Manjira Sinha,Tirthankar Dasgupta*

Main category: cs.AI

TL;DR: 该研究提出了一种利用大型语言模型从科学摘要中提取关于塑料污染物源头到健康影响的关联元路径的新框架，并构建了毒性轨迹图，旨在解决复杂因果关系知识的提取和冲突证据的协调问题。


<details>
  <summary>Details</summary>
Motivation: 塑料的广泛使用及其在环境中的持久性导致微塑料和纳米塑料在空气、水和土壤中积累，对呼吸、胃肠和神经系统造成严重的健康风险。传统方法难以从大量科学文本中有效识别和追踪污染物传播及其复杂的健康影响。

Method: 研究提出一个新颖的框架，利用大型语言模型从科学摘要中提取关联元路径（连接污染物源头到健康影响的多跳语义链）。该系统识别并连接不同上下文中的实体，构建结构化的关联元路径，并将其聚合为毒性轨迹图，以追踪污染物通过暴露途径和生物系统的传播。此外，为了确保一致性和可靠性，研究纳入了一个动态证据协调模块，以解决因不断演变或矛盾的研究发现而产生的语义冲突。

Result: 该方法在从嘈杂的科学文本中提取可靠、高价值的关联知识方面表现出强大的性能。

Conclusion: 该研究为在特定领域语料库中挖掘复杂的因果结构提供了一个可扩展的解决方案。

Abstract: The widespread use of plastics and their persistence in the environment have
led to the accumulation of micro- and nano-plastics across air, water, and
soil, posing serious health risks including respiratory, gastrointestinal, and
neurological disorders. We propose a novel framework that leverages large
language models to extract relational metapaths, multi-hop semantic chains
linking pollutant sources to health impacts, from scientific abstracts. Our
system identifies and connects entities across diverse contexts to construct
structured relational metapaths, which are aggregated into a Toxicity
Trajectory Graph that traces pollutant propagation through exposure routes and
biological systems. Moreover, to ensure consistency and reliability, we
incorporate a dynamic evidence reconciliation module that resolves semantic
conflicts arising from evolving or contradictory research findings. Our
approach demonstrates strong performance in extracting reliable, high-utility
relational knowledge from noisy scientific text and offers a scalable solution
for mining complex cause-effect structures in domain-specific corpora.

</details>


### [25] [The power of dynamic causality in observer-based design for soft sensor applications](https://arxiv.org/abs/2509.11336)
*William Farlessyost,Sebastian Oberst,Shweta Singh*

Main category: cs.AI

TL;DR: 本文提出了一种基于动态因果分析的软传感器优化框架，利用液体时间常数（LTC）网络识别并修剪对状态估计因果影响最小的传感器输入，以提高预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的传感器选择方法（如线性化可观测性指标或统计相关性）无法捕捉复杂系统的时间演化和动态因果关系，导致传感器选择不理想。

Method: 该方法使用液体时间常数（LTC）网络作为连续时间神经架构的观测器。其核心是一个迭代工作流：首先，在候选输入上训练LTC观测器；其次，通过受控扰动分析量化每个输入的因果影响；然后，移除因果影响可忽略的输入；最后，重新训练，直到性能下降。

Result: 研究结果表明，这种因果导向的修剪方法能够持续识别出与底层物理原理一致的最小传感器集合，同时提高预测精度。该框架能自动区分必要的物理测量与噪声，并判断派生交互项何时提供互补信息或冗余信息。

Conclusion: 该方法不仅提高了计算效率，还通过将传感器选择决策建立在动态因果关系而非静态相关性上，增强了可解释性。这为过程工程、生态监测和农业等领域的软传感应用提供了显著优势。

Abstract: This paper introduces a novel framework for optimizing observer-based soft
sensors through dynamic causality analysis. Traditional approaches to sensor
selection often rely on linearized observability indices or statistical
correlations that fail to capture the temporal evolution of complex systems. We
address this gap by leveraging liquid-time constant (LTC) networks,
continuous-time neural architectures with input-dependent time constants, to
systematically identify and prune sensor inputs with minimal causal influence
on state estimation. Our methodology implements an iterative workflow: training
an LTC observer on candidate inputs, quantifying each input's causal impact
through controlled perturbation analysis, removing inputs with negligible
effect, and retraining until performance degradation occurs. We demonstrate
this approach on three mechanistic testbeds representing distinct physical
domains: a harmonically forced spring-mass-damper system, a nonlinear
continuous stirred-tank reactor, and a predator-prey model following the
structure of the Lotka-Volterra model, but with seasonal forcing and added
complexity. Results show that our causality-guided pruning consistently
identifies minimal sensor sets that align with underlying physics while
improving prediction accuracy. The framework automatically distinguishes
essential physical measurements from noise and determines when derived
interaction terms provide complementary versus redundant information. Beyond
computational efficiency, this approach enhances interpretability by grounding
sensor selection decisions in dynamic causal relationships rather than static
correlations, offering significant benefits for soft sensing applications
across process engineering, ecological monitoring, and agricultural domains.

</details>


### [26] [MAPGD: Multi-Agent Prompt Gradient Descent for Collaborative Prompt Optimization](https://arxiv.org/abs/2509.11361)
*Yichen Han,Bojun Liu,Zhengpeng zhou,Guanyu Liu,Zeng Zhang,Yang Yang,Wenli Wang,Isaac N Shi,Yunyan,Lewei He,Tianyu Shi*

Main category: cs.AI

TL;DR: 本文提出了MAPGD（多智能体提示梯度下降）框架，通过多智能体协作和梯度优化，解决了现有提示工程方法效率低、适应性差的问题，并在多种任务上实现了更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的提示工程方法通常依赖单一优化路径，导致适应性差、效率低、视角狭隘、存在梯度冲突且计算成本高昂。

Method: 本文提出了MAPGD框架，它将多智能体协作与基于梯度的优化相结合。MAPGD具有以下特点：1) 针对任务清晰度、示例选择、格式设计和风格优化等方面的专业智能体；2) 语义梯度协调机制以解决冲突；3) 基于多臂老虎机的候选选择，实现高效的探索与利用；4) 理论收敛保证。

Result: 在分类、生成和推理任务上的实验表明，MAPGD在准确性和效率方面均优于单智能体和随机基线方法。消融实验也证实了梯度融合、智能体专业化和冲突解决带来的益处。

Conclusion: MAPGD提供了一种统一的、受梯度启发的多智能体方法，用于实现鲁棒且可解释的提示优化，并验证了梯度融合、智能体专业化和冲突解决的有效性。

Abstract: Prompt engineering is crucial for leveraging large language models (LLMs),
but existing methods often rely on a single optimization trajectory, limiting
adaptability and efficiency while suffering from narrow perspectives, gradient
conflicts, and high computational cost. We propose MAPGD (Multi-Agent Prompt
Gradient Descent), a framework integrating multi-agent collaboration with
gradient-based optimization. MAPGD features specialized agents for task
clarity, example selection, format design, and stylistic refinement; semantic
gradient coordination to resolve conflicts; bandit-based candidate selection
for efficient exploration-exploitation; and theoretical convergence guarantees.
Experiments on classification, generation, and reasoning tasks show MAPGD
outperforms single-agent and random baselines in accuracy and efficiency.
Ablations confirm the benefits of gradient fusion, agent specialization, and
conflict resolution, providing a unified, gradient-inspired multi-agent
approach to robust and interpretable prompt optimization.

</details>


### [27] [Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications](https://arxiv.org/abs/2509.11431)
*Aadil Gani Ganie*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）有数据静态和需微调的局限性。AI代理通过外部工具克服部分局限，但在工业应用中面临提示注入等安全威胁。本文提出将基于角色的访问控制（RBAC）集成到AI代理中，以提供强大的安全防护，支持其在本地部署的有效和可扩展性。


<details>
  <summary>Details</summary>
Motivation: LLMs受限于静态训练数据和泛化性，需微调才能执行特定任务。AI代理虽能利用外部工具和实时数据，但在工业应用中仍易受提示注入等安全威胁，这严重影响其完整性和可靠性。因此，需要一个强大的安全防护来解决这些挑战。

Method: 本文提出一个将基于角色的访问控制（RBAC）集成到AI代理中的框架。

Result: 该框架旨在为AI代理提供强大的安全防护，支持其有效和可扩展的部署，尤其关注本地部署的实现。

Conclusion: 通过将RBAC集成到AI代理中，可以显著增强其安全性，从而支持AI代理在工业环境中（特别是本地部署）的有效和可扩展应用。

Abstract: The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

</details>


### [28] [Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction](https://arxiv.org/abs/2509.11459)
*Chen Jiang,Kofi Osei,Sai Deepthi Yeddula,Dongji Feng,Wei-Shinn Ku*

Main category: cs.AI

TL;DR: 该研究提出了一种自适应专家混合（MoE）模型，用于整合异构多源数据进行精确降水预测，并通过动态路由和可视化工具显著提高了预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确的降水预测对农业、灾害管理和可持续发展至关重要，但由于气候系统的复杂性和雷达、卫星、地面测量等异构多源观测数据的整合挑战（数据在空间、时间分辨率和领域特征上差异大），传统深度学习模型难以有效处理。

Method: 提出了一种专门用于降水率预测的自适应专家混合（MoE）模型。模型中的每个专家专注于特定的模态或时空模式。此外，还引入了一个动态路由器，学习将输入分配给最相关的专家。同时，开发了一个交互式网络可视化工具，用于直观探索历史天气模式。

Result: 模块化设计（自适应MoE）提高了预测精度和可解释性。在捕获2022年飓风伊恩期间真实条件的多模态气候数据集上进行评估，结果显示自适应MoE模型显著优于所有基线。可视化工具支持气候敏感部门的决策。

Conclusion: 自适应MoE模型通过有效整合异构多源数据，显著提升了降水预测的准确性和可解释性，并辅以交互式可视化工具，为气候敏感领域的决策提供了有力支持。

Abstract: Accurate precipitation forecasting is indispensable in agriculture, disaster
management, and sustainable strategies. However, predicting rainfall has been
challenging due to the complexity of climate systems and the heterogeneous
nature of multi-source observational data, including radar, satellite imagery,
and surface-level measurements. The multi-source data vary in spatial and
temporal resolution, and they carry domain-specific features, making it
challenging for effective integration in conventional deep learning models.
Previous research has explored various machine learning techniques for weather
prediction; however, most struggle with the integration of data with
heterogeneous modalities. To address these limitations, we propose an Adaptive
Mixture of Experts (MoE) model tailored for precipitation rate prediction. Each
expert within the model specializes in a specific modality or spatio-temporal
pattern. We also incorporated a dynamic router that learns to assign inputs to
the most relevant experts. Our results show that this modular design enhances
predictive accuracy and interpretability. In addition to the modeling
framework, we introduced an interactive web-based visualization tool that
enables users to intuitively explore historical weather patterns over time and
space. The tool was designed to support decision-making for stakeholders in
climate-sensitive sectors. We evaluated our approach using a curated multimodal
climate dataset capturing real-world conditions during Hurricane Ian in 2022.
The benchmark results show that the Adaptive MoE significantly outperformed all
the baselines.

</details>


### [29] [Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs](https://arxiv.org/abs/2509.11480)
*Amir Taherin,Juyi Lin,Arash Akbari,Arman Akbari,Pu Zhao,Weiwei Chen,David Kaeli,Yanzhi Wang*

Main category: cs.AI

TL;DR: 本文评估了五种视觉-语言-动作（VLA）模型在边缘和数据中心GPU平台上的性能，包括准确性、延迟、吞吐量和内存使用，并分析了架构选择和功耗限制对其性能的影响。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型已成为机器人控制的强大通用策略，但其在不同模型架构、硬件平台上的性能扩展及其相关功耗预算仍未被充分理解。

Method: 研究评估了五种代表性的VLA模型（包括最先进的基线和两种新提出的架构），目标平台为边缘和数据中心GPU。使用LIBERO基准测试，在不同的边缘功耗限制和高性能数据中心GPU配置下，测量了准确性以及系统级指标（如延迟、吞吐量和峰值内存使用）。

Result: 研究发现：(1) 架构选择（如动作分词和模型主干大小）强烈影响吞吐量和内存占用；(2) 功耗受限的边缘设备表现出非线性性能下降，但某些配置可以匹配或超越老旧的数据中心GPU；(3) 可以在不显著牺牲准确性的情况下实现高吞吐量变体。

Conclusion: 这些发现为在各种部署限制下选择和优化VLA模型提供了可操作的见解，并挑战了当前关于数据中心硬件在机器人推理方面具有绝对优势的假设。

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist
policies for robotic control, yet their performance scaling across model
architectures and hardware platforms, as well as their associated power
budgets, remain poorly understood. This work presents an evaluation of five
representative VLA models -- spanning state-of-the-art baselines and two newly
proposed architectures -- targeting edge and datacenter GPU platforms. Using
the LIBERO benchmark, we measure accuracy alongside system-level metrics,
including latency, throughput, and peak memory usage, under varying edge power
constraints and high-performance datacenter GPU configurations. Our results
identify distinct scaling trends: (1) architectural choices, such as action
tokenization and model backbone size, strongly influence throughput and memory
footprint; (2) power-constrained edge devices exhibit non-linear performance
degradation, with some configurations matching or exceeding older datacenter
GPUs; and (3) high-throughput variants can be achieved without significant
accuracy loss. These findings provide actionable insights when selecting and
optimizing VLAs across a range of deployment constraints. Our work challenges
current assumptions about the superiority of datacenter hardware for robotic
inference.

</details>


### [30] [MedicalOS: An LLM Agent based Operating System for Digital Healthcare](https://arxiv.org/abs/2509.11507)
*Jared Zhu,Junde Wu*

Main category: cs.AI

TL;DR: MedicalOS是一个统一的基于代理的操作系统，作为医疗保健领域的抽象层，将自然语言指令转化为可执行的数字医疗命令，旨在简化临床工作流程并提高效率。


<details>
  <summary>Details</summary>
Motivation: 尽管数字健康技术有所进步，但现有系统仍难以学习和使用，导致临床医生需要管理多个工具、重复手动操作、导航复杂界面，并花费大量时间在行政工作而非患者护理上。大型语言模型（LLM）代理在编码和计算机操作方面的潜力表明，可以通过自然语言指令与系统交互，但这在医疗保健领域需要一个遵循临床指南和标准的领域特定抽象层，以确保安全、透明和合规性。

Method: 本文提出了MedicalOS，一个统一的基于代理的操作系统，作为医疗保健领域的特定抽象层。它将人类指令转换为预定义的数字医疗命令，如患者查询、病史检索、检查管理、报告生成、转诊、治疗计划等。这些命令被封装成现成的工具，使用机器语言（如Python、API、MCP、Linux）实现。

Result: MedicalOS在22个专科的214个患者病例中进行了实证验证，结果表明其具有高诊断准确性和置信度、临床上合理的检查请求、以及一致的结构化报告和药物推荐生成能力。

Conclusion: MedicalOS为临床实践中的工作流程自动化提供了一个值得信赖且可扩展的基础，有望提高医疗保健系统的效率和安全性。

Abstract: Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.

</details>


### [31] [Task Decoding based on Eye Movements using Synthetic Data Augmentation](https://arxiv.org/abs/2509.11547)
*Shanmuka Sadhu,Arca Baran,Preeti Pandey,Ayush Kumar*

Main category: cs.AI

TL;DR: 本研究通过使用CTGAN、CopulaGAN和Gretel AI等生成器生成合成眼动数据，并将其与真实数据结合，显著提高了从眼动数据中解码任务类别的准确性，支持了Yarbus的假设。


<details>
  <summary>Details</summary>
Motivation: 眼动研究中，传统机器学习算法在从眼动数据解码任务方面表现不一，对于Yarbus关于可以通过眼动解码观察者任务的说法，结果褒贬不一。本研究旨在通过提高解码准确性来支持Yarbus的假设。

Method: 使用来自真实用户研究的眼动数据，并利用CTGAN、CopulaGAN和Gretel AI等合成数据生成器生成合成数据样本。将生成的合成数据与真实数据结合进行数据增强，然后使用传统机器学习算法（如随机森林）和深度学习模型（如Inception Time）进行任务分类。通过多种算法和真实/合成数据组合验证了数据增强对解码准确性的影响。

Result: 结果表明，通过增加合成眼动数据样本进行数据增强，即使使用传统机器学习算法也能提高分类准确性。当在320个真实眼动数据集样本的基础上增加五倍数据（合成数据）时，任务解码准确率从随机森林的28.1%显著提高到Inception Time的82%。本研究提出的框架，由于使用了额外的合成数据集，优于现有所有在该数据集上的研究。

Conclusion: 通过将合成生成的眼动数据与真实数据结合进行增强，可以显著提高任务解码的准确性，从而支持了Yarbus的假设，并且该方法在现有研究中表现最佳。解码准确率随生成数据对真实数据的增强比例的增加而提高。

Abstract: Machine learning has been extensively used in various applications related to
eye-tracking research. Understanding eye movement is one of the most
significant subsets of eye-tracking research that reveals the scanning pattern
of an individual. Researchers have thoroughly analyzed eye movement data to
understand various eye-tracking applications, such as attention mechanisms,
navigational behavior, task understanding, etc. The outcome of traditional
machine learning algorithms used for decoding tasks based on eye movement data
has received a mixed reaction to Yarbus' claim that it is possible to decode
the observer's task from their eye movements. In this paper, to support the
hypothesis by Yarbus, we are decoding tasks categories while generating
synthetic data samples using well-known Synthetic Data Generators CTGAN and its
variations such as CopulaGAN and Gretel AI Synthetic Data generators on
available data from an in-person user study. Our results show that augmenting
more eye movement data combined with additional synthetically generated
improves classification accuracy even with traditional machine learning
algorithms. We see a significant improvement in task decoding accuracy from
28.1% using Random Forest to 82% using Inception Time when five times more data
is added in addition to the 320 real eye movement dataset sample. Our proposed
framework outperforms all the available studies on this dataset because of the
use of additional synthetic datasets. We validated our claim with various
algorithms and combinations of real and synthetic data to show how decoding
accuracy increases with the increase in the augmentation of generated data to
real data.

</details>


### [32] [Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain](https://arxiv.org/abs/2509.11572)
*Tuan Bui,An Nguyen,Phat Thai,Minh Hua,Ngan Pham L. N.,Ngan Pham T. B.,Dung Le,Long Nguyen,Thanh-Tung Tran,Thang Bui,Tho Quan*

Main category: cs.AI

TL;DR: 本文提出MCFR，一个神经符号框架，将大型语言模型与模型检测相结合，用于封闭领域问答系统中的属性验证，以解决LLM推理不忠实和现有符号引擎在动态推理方面的局限性。MCFR将自然语言转化为形式化规范并在转换模型上进行验证，并在新引入的EduMC-QA基准上展示了其在推理忠实性和可解释性方面的改进。


<details>
  <summary>Details</summary>
Motivation: 封闭领域问答系统对程序正确性和策略合规性要求极高。尽管大型语言模型（LLM）在推理任务上表现出色，但其推理过程常不忠实，更像是合理化而非因果推导。LLM与符号引擎（如Prover9, Z3）结合虽然提高了可靠性，但仍局限于静态逻辑，难以处理多步骤进展和条件转换等动态、基于状态的推理。

Method: 本文提出了MCFR（Model Checking for Formal Reasoning），一个神经符号框架，它将LLM与模型检测技术相结合以支持属性验证。MCFR将自然语言输入转化为形式化规范，并在转换模型上进行验证。为评估该框架，作者引入了EduMC-QA，一个基于真实学术流程的基准数据集。

Result: 实验结果表明，MCFR显著提高了推理的忠实性和可解释性，为高风险封闭领域应用中的可验证问答提供了一条可行途径。此外，MCFR的性能与ChatGPT、DeepSeek和Claude等最先进的LLM进行了比较，以体现其有效性。

Conclusion: MCFR作为一个神经符号框架，通过整合LLM和模型检测，有效解决了LLM推理不忠实和现有符号方法在动态推理方面的不足。它为高风险封闭领域应用提供了实现可验证问答的有效路径，增强了推理的忠实性和可解释性。

Abstract: Reasoning is essential for closed-domain QA systems in which procedural
correctness and policy compliance are critical. While large language models
(LLMs) have shown strong performance on many reasoning tasks, recent work
reveals that their reasoning traces are often unfaithful - serving more as
plausible justifications than as causally grounded derivations. Efforts to
combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved
reliability but remain limited to static forms of logic, struggling with
dynamic, state-based reasoning such as multi-step progressions and conditional
transitions.
  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a
neuro-symbolic framework that integrates LLMs with model checking to support
property verification. MCFR translates natural language into formal
specifications and verifies them over transition models. To support evaluation,
we introduce EduMC-QA, a benchmark dataset grounded in real academic
procedures. Our results show that MCFR improves reasoning faithfulness and
interpretability, offering a viable path toward verifiable QA in high-stakes
closed-domain applications. In addition to evaluating MCFR, we compare its
performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to
contextualize its effectiveness.

</details>


### [33] [A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models](https://arxiv.org/abs/2509.11575)
*Ching Chang,Yidan Shi,Defu Cao,Wei Yang,Jeehyun Hwang,Haixin Wang,Jiacheng Pang,Wei Wang,Yan Liu,Wen-Chih Peng,Tien-Fu Chen*

Main category: cs.AI

TL;DR: 这篇综述定义了时间序列推理问题，根据推理拓扑（直接、线性链、分支结构）和主要目标（传统分析、解释、因果推理、生成）组织了现有文献，回顾了方法和系统，并提供了评估实践和未来研究的指导。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在定义时间序列推理问题，系统地组织现有文献，识别不同推理拓扑的优势和局限性，并为未来在该领域开发更可靠、可解释和可操作的系统提供指导。

Method: 该综述首先定义了时间序列推理，然后根据三种推理拓扑（直接、线性链、分支结构）对文献进行分类。它将这些拓扑与该领域的主要目标（如传统分析、解释、因果推理、生成）相结合，并使用紧凑的标签集跨越这些轴。此外，它还回顾了不同领域的方法和系统，并讨论了评估实践、数据集、基准和资源。

Result: 综述回顾了各种方法和系统，展示了每种推理拓扑的能力和局限性（在忠实性或鲁棒性方面），并提供了精选的数据集、基准和资源。它强调了保持证据可见且时间对齐的评估实践，并就如何根据不确定性匹配拓扑、与可观测工件进行接地、规划偏移和流式处理以及将成本和延迟视为设计预算提供了指导。

Conclusion: 推理结构必须在接地和自我纠正能力与计算成本和可复现性之间取得平衡。未来的进展将依赖于将推理质量与实用性联系起来的基准，以及在感知偏移、流式和长周期设置下权衡成本和风险的闭环测试平台。这些方向标志着从狭窄的准确性转向大规模可靠性，从而实现能够分析、理解、解释并基于动态世界采取行动的系统，并提供可追溯的证据和可信的结果。

Abstract: Time series reasoning treats time as a first-class axis and incorporates
intermediate evidence directly into the answer. This survey defines the problem
and organizes the literature by reasoning topology with three families: direct
reasoning in one step, linear chain reasoning with explicit intermediates, and
branch-structured reasoning that explores, revises, and aggregates. The
topology is crossed with the main objectives of the field, including
traditional time series analysis, explanation and understanding, causal
inference and decision making, and time series generation, while a compact tag
set spans these axes and captures decomposition and verification, ensembling,
tool use, knowledge access, multimodality, agent loops, and LLM alignment
regimes. Methods and systems are reviewed across domains, showing what each
topology enables and where it breaks down in faithfulness or robustness, along
with curated datasets, benchmarks, and resources that support study and
deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey).
Evaluation practices that keep evidence visible and temporally aligned are
highlighted, and guidance is distilled on matching topology to uncertainty,
grounding with observable artifacts, planning for shift and streaming, and
treating cost and latency as design budgets. We emphasize that reasoning
structures must balance capacity for grounding and self-correction against
computational cost and reproducibility, while future progress will likely
depend on benchmarks that tie reasoning quality to utility and on closed-loop
testbeds that trade off cost and risk under shift-aware, streaming, and
long-horizon settings. Taken together, these directions mark a shift from
narrow accuracy toward reliability at scale, enabling systems that not only
analyze but also understand, explain, and act on dynamic worlds with traceable
evidence and credible outcomes.

</details>


### [34] [AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions](https://arxiv.org/abs/2509.11595)
*Sabin Huda,Ernest Foo,Zahra Jadidi,MA Hakim Newton,Abdul Sattar*

Main category: cs.AI

TL;DR: AMLNet是一个知识驱动的多智能体框架，包含一个生成符合监管要求的合成交易数据的生成器和一个检测洗钱活动的集成检测管道，旨在解决反洗钱研究中缺乏公开数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 反洗钱（AML）研究受到缺乏可公开共享、符合监管要求的交易数据集的限制。

Method: 本文提出了AMLNet，一个知识驱动的多智能体框架，由两个协调单元组成：一个感知监管的交易生成器和一个集成检测管道。生成器生成涵盖核心洗钱阶段和高级洗钱模式的合成交易。

Result: 生成器产生了1,090,173笔合成交易（约0.16%为洗钱活动），基于AUSTRAC规则，监管一致性达到75%，综合技术保真度得分为0.75。检测集成在AMLNet内部测试分区上实现了0.90的F1分数（精确度0.84，召回率0.97），并能适应外部SynthAML数据集，表明其架构具有通用性。研究提供了多维度评估并发布了数据集。

Conclusion: AMLNet提供了一个可复现且符合监管意识的数据集和框架，以推动反洗钱实验，解决数据稀缺问题并促进相关研究进展。

Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly
shareable, regulation-aligned transaction datasets. We present AMLNet, a
knowledge-based multi-agent framework with two coordinated units: a
regulation-aware transaction generator and an ensemble detection pipeline. The
generator produces 1,090,173 synthetic transactions (approximately 0.16\%
laundering-positive) spanning core laundering phases (placement, layering,
integration) and advanced typologies (e.g., structuring, adaptive threshold
behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage
(Section 4.2), while a composite technical fidelity score of 0.75 summarizes
temporal, structural, and behavioral realism components (Section 4.4). The
detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the
internal test partitions of AMLNet and adapts to the external SynthAML dataset,
indicating architectural generalizability across different synthetic generation
paradigms. We provide multi-dimensional evaluation (regulatory, temporal,
network, behavioral) and release the dataset (Version 1.0,
https://doi.org/10.5281/zenodo.16736515), to advance reproducible and
regulation-conscious AML experimentation.

</details>


### [35] [Adapting and Evaluating Multimodal Large Language Models for Adolescent Idiopathic Scoliosis Self-Management: A Divide and Conquer Framework](https://arxiv.org/abs/2509.11645)
*Zhaolong Wu,Pu Luo,Jason Pui Yin Cheung,Teng Zhang*

Main category: cs.AI

TL;DR: 本研究首次全面评估了多模态大语言模型（MLLMs）在青少年特发性脊柱侧凸（AIS）自我管理方面的应用。结果显示MLLMs在解读脊柱X光片和理解AIS护理知识方面存在局限性，尽管通过脊柱关键点提示和检索增强生成（RAG）有所改进，但仍远不能实现个性化AIS护理，尤其在脊柱畸形检测方面准确率较低。


<details>
  <summary>Details</summary>
Motivation: 由于MLLMs的潜力，本研究旨在首次全面评估其在青少年特发性脊柱侧凸（AIS）自我管理中的能力。

Method: 研究构建了一个包含约3,000张前后位X光片和诊断文本的数据库，并通过“分而治之”框架评估了五种MLLMs，该框架包括视觉问答任务、领域知识评估任务和患者教育咨询评估任务。为解决MLLMs的局限性，研究分别引入了脊柱关键点提示和用于检索增强生成（RAG）的AIS知识库。

Result: 研究发现MLLMs在解读复杂脊柱X光片和理解AIS护理知识方面存在局限性。视觉提示在不同架构间效果不一，而RAG显著提升了模型在知识评估任务上的表现。然而，目前的MLLMs远不能实现AIS护理中的个性化助理，最大的挑战在于它们准确检测脊柱畸形位置（最佳准确率：0.55）和方向（最佳准确率：0.13）的能力。

Conclusion: 目前的MLLMs在实现AIS护理中的个性化助理方面能力不足，尤其在准确检测脊柱畸形位置和方向上存在显著挑战，尽管RAG和视觉提示带来了一些改进，但其解读复杂脊柱X光片和理解AIS护理知识的能力仍有待提高。

Abstract: This study presents the first comprehensive evaluation of Multimodal Large
Language Models (MLLMs) for Adolescent Idiopathic Scoliosis (AIS)
self-management. We constructed a database of approximately 3,000
anteroposterior X-rays with diagnostic texts and evaluated five MLLMs through a
`Divide and Conquer' framework consisting of a visual question-answering task,
a domain knowledge assessment task, and a patient education counseling
assessment task. Our investigation revealed limitations of MLLMs' ability in
interpreting complex spinal radiographs and comprehending AIS care knowledge.
To address these, we pioneered enhancing MLLMs with spinal keypoint prompting
and compiled an AIS knowledge base for retrieval augmented generation (RAG),
respectively. Results showed varying effectiveness of visual prompting across
different architectures, while RAG substantially improved models' performances
on the knowledge assessment task. Our findings indicate current MLLMs are far
from capable in realizing personalized assistant in AIS care. The greatest
challenge lies in their abilities to obtain accurate detections of spinal
deformity locations (best accuracy: 0.55) and directions (best accuracy: 0.13).

</details>


### [36] [HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction](https://arxiv.org/abs/2509.11719)
*Bingqing Wei,Lianmin Chen,Zhongyu Xia,Yongtao Wang*

Main category: cs.AI

TL;DR: HeLoFusion是一种高效且可扩展的编码器，通过构建局部多尺度图和聚合-分解消息传递方案，有效建模异构和多尺度智能体交互，在Waymo数据集上实现了最先进的多智能体轨迹预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以充分捕捉自动驾驶中复杂的多智能体社会动态，特别是多尺度交互的共存以及异构智能体多样化行为的建模。

Method: 本文提出了HeLoFusion，一个高效且可扩展的编码器。它不依赖全局上下文，而是构建以每个智能体为中心的局部多尺度图，以建模直接的成对依赖和复杂的群体交互。此外，HeLoFusion通过聚合-分解消息传递方案和特定类型特征网络来处理智能体异构性，从而学习细致的、依赖于类型的交互模式。这种以局部性为中心的方法能够对多层次社会背景进行原则性表示。

Result: HeLoFusion在具有挑战性的Waymo开放运动数据集上取得了最先进的性能，为包括Soft mAP和minADE在内的关键指标设定了新的基准。

Conclusion: 明确建模多尺度和异构交互的局部性架构，是推动运动预测领域发展的非常有效的策略。

Abstract: Multi-agent trajectory prediction in autonomous driving requires a
comprehensive understanding of complex social dynamics. Existing methods,
however, often struggle to capture the full richness of these dynamics,
particularly the co-existence of multi-scale interactions and the diverse
behaviors of heterogeneous agents. To address these challenges, this paper
introduces HeLoFusion, an efficient and scalable encoder for modeling
heterogeneous and multi-scale agent interactions. Instead of relying on global
context, HeLoFusion constructs local, multi-scale graphs centered on each
agent, allowing it to effectively model both direct pairwise dependencies and
complex group-wise interactions (\textit{e.g.}, platooning vehicles or
pedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of
agent heterogeneity through an aggregation-decomposition message-passing scheme
and type-specific feature networks, enabling it to learn nuanced,
type-dependent interaction patterns. This locality-focused approach enables a
principled representation of multi-level social context, yielding powerful and
expressive agent embeddings. On the challenging Waymo Open Motion Dataset,
HeLoFusion achieves state-of-the-art performance, setting new benchmarks for
key metrics including Soft mAP and minADE. Our work demonstrates that a
locality-grounded architecture, which explicitly models multi-scale and
heterogeneous interactions, is a highly effective strategy for advancing motion
forecasting.

</details>


### [37] [Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning](https://arxiv.org/abs/2509.11880)
*Carlos Celemin,Joseph Brennan,Pierluigi Vito Amadori,Tim Bradley*

Main category: cs.AI

TL;DR: 本文将监督对比学习（SupCon）应用于模仿学习（IL），旨在学习更有效的状态表示，以更好地捕捉视频游戏环境中与动作相关的因素。


<details>
  <summary>Details</summary>
Motivation: 目前的模仿学习方法在学习观察的潜在表示方面可能不足，未能有效捕捉与动作相关的因素，从而难以准确建模观察与演示者动作之间的因果关系。

Method: 提出了一种将监督对比学习（SupCon）损失与连续输出空间相结合的方法，使SupCon能够不受环境动作类型限制地运行，从而学习更有效的状态表示。

Result: 在3D游戏（Astro Bot, Returnal）和2D Atari游戏上的实验表明，与仅使用监督动作预测损失训练的基线模型相比，该方法提高了表示质量、加快了学习收敛速度并增强了泛化能力。

Conclusion: 通过将监督对比学习集成到模仿学习中，可以学习到更有效的状态表示，从而显著提升智能体在视频游戏环境中的学习效率和泛化性能。

Abstract: This paper introduces a novel application of Supervised Contrastive Learning
(SupCon) to Imitation Learning (IL), with a focus on learning more effective
state representations for agents in video game environments. The goal is to
obtain latent representations of the observations that capture better the
action-relevant factors, thereby modeling better the cause-effect relationship
from the observations that are mapped to the actions performed by the
demonstrator, for example, the player jumps whenever an obstacle appears ahead.
We propose an approach to integrate the SupCon loss with continuous output
spaces, enabling SupCon to operate without constraints regarding the type of
actions of the environment. Experiments on the 3D games Astro Bot and Returnal,
and multiple 2D Atari games show improved representation quality, faster
learning convergence, and better generalization compared to baseline models
trained only with supervised action prediction loss functions.

</details>


### [38] [EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models](https://arxiv.org/abs/2509.11914)
*Yiqun Yao,Naitong Yu,Xiang Li,Xin Jiang,Xuezhi Fang,Wenjia Ma,Xuying Meng,Jing Li,Aixin Sun,Yequan Wang*

Main category: cs.AI

TL;DR: EgoMem是首个为全双工、实时全模态流设计的终身记忆智能体，它能从原始音视频流中识别用户，提供个性化响应，并维护用户的长期知识。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型记忆智能体不适用于实时、具身场景，且不完全依赖原始音视频流。研究动机是开发一个能直接处理原始音视频流，实现用户识别、个性化交互和长期知识维护的终身记忆系统。

Method: EgoMem包含三个异步进程：(i) 检索进程，通过面部和声音动态识别用户并从长期记忆中获取相关上下文；(ii) 全模态对话进程，基于检索到的上下文生成个性化音频响应；(iii) 记忆管理进程，自动检测对话边界并提取必要信息以更新长期记忆。该系统完全依赖原始音视频流。

Result: EgoMem的检索和记忆管理模块在测试集上达到了95%以上的准确率。当与经过微调的RoboEgo全模态聊天机器人集成时，系统在实时个性化对话中实现了超过87%的事实一致性得分。

Conclusion: EgoMem为未来的研究建立了一个强大的基线，证明了其在处理原始音视频流、实现终身、实时和具身场景下的用户识别、个性化响应和长期知识维护方面的有效性。

Abstract: We introduce EgoMem, the first lifelong memory agent tailored for full-duplex
models that process real-time omnimodal streams. EgoMem enables real-time
models to recognize multiple users directly from raw audiovisual streams, to
provide personalized response, and to maintain long-term knowledge of users'
facts, preferences, and social relationships extracted from audiovisual
history. EgoMem operates with three asynchronous processes: (i) a retrieval
process that dynamically identifies user via face and voice, and gathers
relevant context from a long-term memory; (ii) an omnimodal dialog process that
generates personalized audio responses based on the retrieved context; and
(iii) a memory management process that automatically detects dialog boundaries
from omnimodal streams, and extracts necessary information to update the
long-term memory. Unlike existing memory agents for LLMs, EgoMem relies
entirely on raw audiovisual streams, making it especially suitable for
lifelong, real-time, and embodied scenarios. Experimental results demonstrate
that EgoMem's retrieval and memory management modules achieve over 95% accuracy
on the test set. When integrated with a fine-tuned RoboEgo omnimodal chatbot,
the system achieves fact-consistency scores above 87% in real-time personalized
dialogs, establishing a strong baseline for future research.

</details>


### [39] [BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning](https://arxiv.org/abs/2509.11922)
*Xilei Dai,Ruotian Chen,Songze Guan,Wen-Tai Li,Chau Yuen*

Main category: cs.AI

TL;DR: BuildingGym是一个开源的强化学习框架，旨在为建筑能源管理中的各种控制问题提供灵活、易于使用的解决方案，并已在冷却负荷管理任务中展示出良好性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在基于AI的建筑能源管理中表现出色，但目前缺乏一个灵活的框架来将其应用于各种控制问题。现有工具无法满足系统级和房间级控制的需求，也无法处理外部信号输入，限制了其在智能电网等复杂环境中的应用。

Method: 本文提出了BuildingGym，一个开源工具，集成了EnergyPlus作为核心模拟器，支持系统级和房间级控制。它能够接受外部信号作为控制输入，使其适用于智能电网和电动汽车社区等更灵活的环境。BuildingGym提供了多种内置的强化学习算法，并允许用户轻松配置和替换算法、模拟器和控制环境，从而连接了建筑经理和AI专家。

Result: 通过BuildingGym，研究人员高效地设置了恒定和动态冷却负荷管理训练任务。内置算法在这两项任务中均表现出强大的性能，证明了BuildingGym在优化冷却策略方面的有效性。

Conclusion: BuildingGym通过提供一个灵活、易于配置的平台，弥补了建筑经理和AI专家之间的鸿沟。它不仅能有效优化冷却策略，还因其处理外部信号的能力而适用于智能电网和电动汽车社区等更广泛、更灵活的能源管理场景。

Abstract: Reinforcement learning (RL) has proven effective for AI-based building energy
management. However, there is a lack of flexible framework to implement RL
across various control problems in building energy management. To address this
gap, we propose BuildingGym, an open-source tool designed as a
research-friendly and flexible framework for training RL control strategies for
common challenges in building energy management. BuildingGym integrates
EnergyPlus as its core simulator, making it suitable for both system-level and
room-level control. Additionally, BuildingGym is able to accept external
signals as control inputs instead of taking the building as a stand-alone
entity. This feature makes BuildingGym applicable for more flexible
environments, e.g. smart grid and EVs community. The tool provides several
built-in RL algorithms for control strategy training, simplifying the process
for building managers to obtain optimal control strategies. Users can achieve
this by following a few straightforward steps to configure BuildingGym for
optimization control for common problems in the building energy management
field. Moreover, AI specialists can easily implement and test state-of-the-art
control algorithms within the platform. BuildingGym bridges the gap between
building managers and AI specialists by allowing for the easy configuration and
replacement of RL algorithms, simulators, and control environments or problems.
With BuildingGym, we efficiently set up training tasks for cooling load
management, targeting both constant and dynamic cooling load management. The
built-in algorithms demonstrated strong performance across both tasks,
highlighting the effectiveness of BuildingGym in optimizing cooling strategies.

</details>


### [40] [Neuromorphic Intelligence](https://arxiv.org/abs/2509.11940)
*Marcel van Gerven*

Main category: cs.AI

TL;DR: 本文提出将动力系统理论作为神经拟态计算的统一理论框架，以整合多学科知识，利用噪声和微分遗传编程实现高效、自适应的智能系统，促进新兴神经拟态智能的发展。


<details>
  <summary>Details</summary>
Motivation: 传统数字计算方法对计算和能源资源消耗巨大，而神经拟态系统旨在模仿人脑的效率、灵活性和适应性，以实现更高的能效。然而，目前缺乏一个能够连接人工智能、神经科学、物理学、化学和材料科学等多个学科的统一理论框架。

Method: 本文主张将动力系统理论作为神经拟态计算的统一基础。在此框架下，利用微分演算为推理、学习和控制提供建模语言，将噪声作为学习的资源加以利用，并通过微分遗传编程发现能够实现自适应行为的动力系统。

Result: 通过采用动力系统理论，可以为自然和人工基质中的推理、学习和控制提供一个原则性的建模语言。噪声可以被利用作为学习的资源，微分遗传编程能够发现实现自适应行为的动力系统。这为新兴的神经拟态智能铺平了道路，使得智能行为可以从物理基质的动力学中涌现。

Conclusion: 动力系统理论为神经拟态计算提供了一个强大的统一框架，能够桥接多学科知识，从而实现可持续、透明且易于访问的智能系统。通过利用物理基质的动力学，该方法将推动人工智能的科学发展和可持续性。

Abstract: Neuromorphic computing seeks to replicate the remarkable efficiency,
flexibility, and adaptability of the human brain in artificial systems. Unlike
conventional digital approaches, which depend on massive computational and
energy resources, neuromorphic systems exploit brain-inspired principles of
computation to achieve orders of magnitude greater energy efficiency. By
drawing on insights from artificial intelligence, neuroscience, physics,
chemistry, and materials science, neuromorphic computing promises to deliver
intelligent systems that are sustainable, transparent, and widely accessible. A
central challenge, however, is to identify a unifying theoretical framework
capable of bridging these diverse disciplines. We argue that dynamical systems
theory provides such a foundation. Rooted in differential calculus, it offers a
principled language for modeling inference, learning, and control in both
natural and artificial substrates. Within this framework, noise can be
harnessed as a resource for learning, while differential genetic programming
enables the discovery of dynamical systems that implement adaptive behaviors.
Embracing this perspective paves the way toward emergent neuromorphic
intelligence, where intelligent behavior arises from the dynamics of physical
substrates, advancing both the science and sustainability of AI.

</details>


### [41] [How to Evaluate Medical AI](https://arxiv.org/abs/2509.11941)
*Ilia Kopanichuk,Petr Anokhin,Vladimir Shaposhnikov,Vladimir Makharev,Ekaterina Tsapieva,Iaroslav Bespalov,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 该研究引入了RPAD和RRAD两种新的AI诊断评估指标，通过与多个专家意见比较并考虑专家间变异性，提供更稳定真实的评估。结果显示顶级AI模型表现可与甚至超越专家共识，且专家判断本身也存在显著变异。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估指标（如精确度、召回率）未能充分考虑专家判断的固有变异性，导致评估不一致。而Kappa统计量虽可靠但缺乏可解释性。因此，需要更稳定、更能反映实际情况的AI诊断质量评估方法。

Method: 该研究引入了两种新的评估指标：相对精确度 (RPAD) 和相对召回率 (RRAD)。这些指标将AI输出与多个专家意见而非单一参考进行比较，并通过专家间分歧对性能进行归一化。此外，研究还开发了一种自动化方法来建立自由形式临床诊断的同一性。研究通过分析360个医学对话，将多个大型语言模型 (LLMs) 与一组医生进行了比较。

Result: 顶级模型（如DeepSeek-V3）在诊断一致性方面达到或超越了专家共识。专家判断本身表现出显著变异性，通常大于AI与人类之间的变异性。此外，用于建立自由形式诊断同一性的自动化方法达到了98%的准确率。

Conclusion: 医学AI评估中任何绝对指标都存在局限性，因此需要采用相对指标。RPAD和RRAD提供了更稳定和真实的AI诊断质量衡量标准。研究结果表明，AI在某些方面可以达到甚至超越专家水平，并且强调了专家判断固有的变异性。

Abstract: The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

</details>


### [42] [Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics](https://arxiv.org/abs/2509.11943)
*Antonin Sulc,Thorsten Hellert*

Main category: cs.AI

TL;DR: 本文提出了一种神经符号多智能体架构，利用克里普克模型和模态逻辑来形式化智能体信念状态，并通过逻辑约束引导大型语言模型（LMs）的假设生成，防止其得出不合理的结论。该系统在模拟粒子加速器环境中成功诊断了复杂的级联故障。


<details>
  <summary>Details</summary>
Motivation: 智能体（尤其是基于语言模型的智能体）在需要智能和自主决策的复杂环境中扮演关键角色。然而，当前AI研究在提升智能体在这些环境中的推理结构、保真度和逻辑一致性方面仍有不足，且LMs可能得出物理或逻辑上站不住脚的结论。

Method: 研究采用了一种神经符号多智能体架构。其中，单个智能体的信念状态被正式表示为克里普克模型，并使用模态逻辑来推理“可能性”和“必然性”等概念。通过将不可变、领域特定的知识编码为逻辑约束，这些约束主动指导LMs的假设生成，防止其得出不合理的结论。该方法结合了LMs强大的语义直觉与模态逻辑的严谨验证和事实世界模型。

Result: 在高度逼真的模拟粒子加速器环境中，该系统成功诊断了复杂的级联故障。它有效地阻止了LMs得出物理或逻辑上站不住脚的结论，展示了将LMs的语义直觉与模态逻辑的严谨验证相结合的有效性。

Conclusion: 这项工作展示了一条通往更鲁棒、可靠和可验证的自主智能体的可行路径，通过结合大型语言模型的强大语义直觉与模态逻辑的严格验证和事实世界模型，实现了对复杂环境的有效诊断和决策。

Abstract: The development of intelligent agents, particularly those powered by language
models (LMs), has shown the critical role in various environments that require
intelligent and autonomous decision. Environments are not passive testing
grounds and they represent the data required for agents to learn and exhibit
very challenging conditions that require adaptive, complex and autonomous
capacity to make decisions. While the paradigm of scaling models and datasets
has led to remarkable emergent capabilities, we argue that scaling the
structure, fidelity, and logical consistency of agent reasoning within these
environments is a crucial, yet underexplored, dimension of AI research. This
paper introduces a neuro-symbolic multi-agent architecture where the belief
states of individual agents are formally represented as Kripke models. This
foundational choice enables them to reason about known concepts of
\emph{possibility} and \emph{necessity} using the formal language of modal
logic. In this work, we use of immutable, domain-specific knowledge to make
infere information, which is encoded as logical constraints essential for
proper diagnosis. In the proposed model, we show constraints that actively
guide the hypothesis generation of LMs, effectively preventing them from
reaching physically or logically untenable conclusions. In a high-fidelity
simulated particle accelerator environment, our system successfully diagnoses
complex, cascading failures by combining the powerful semantic intuition of LMs
with the rigorous, verifiable validation of modal logic and a factual world
model and showcasing a viable path toward more robust, reliable, and verifiable
autonomous agents.

</details>


### [43] [Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare](https://arxiv.org/abs/2509.11944)
*Susanta Mitra*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的基于时间图的多智能体推理框架，用于多模态医疗诊断，旨在通过处理动态变化和整合多时间点数据来提高诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的多模态推理模型已应用于科学领域，但它们在医疗保健领域的应用有限，且在正确诊断方面存在不足，无法有效处理和推理多模态医疗数据以进行准确诊断。

Method: 研究提出了一种基于有向图的新型时间图推理过程，该过程能够通过回溯、细化和动态修改（创建/删除）原因来适应推理中的动态变化。它还考虑了不同时间点的多模态数据来跟踪患者健康和疾病进展。此外，该多智能体时间推理框架包含任务分配和交叉验证机制以提高推理输出的准确性。

Result: 初步的实验和分析结果证明了所提出方法的创新性和实用性。

Conclusion: 所提出的基于时间图的多智能体推理框架为多模态医疗推理和诊断提供了一种新颖且实用的方法，有望辅助医疗专业人员进行更准确的诊断。

Abstract: Healthcare and medicine are multimodal disciplines that deal with multimodal
data for reasoning and diagnosing multiple diseases. Although some multimodal
reasoning models have emerged for reasoning complex tasks in scientific
domains, their applications in the healthcare domain remain limited and fall
short in correct reasoning for diagnosis. To address the challenges of
multimodal medical reasoning for correct diagnosis and assist the healthcare
professionals, a novel temporal graph-based reasoning process modelled through
a directed graph has been proposed in the current work. It helps in
accommodating dynamic changes in reasons through backtracking, refining the
reasoning content, and creating new or deleting existing reasons to reach the
best recommendation or answer. Again, consideration of multimodal data at
different time points can enable tracking and analysis of patient health and
disease progression. Moreover, the proposed multi-agent temporal reasoning
framework provides task distributions and a cross-validation mechanism to
further enhance the accuracy of reasoning outputs. A few basic experiments and
analysis results justify the novelty and practical utility of the proposed
preliminary approach.

</details>


### [44] [MusicSwarm: Biologically Inspired Intelligence for Music Composition](https://arxiv.org/abs/2509.11973)
*Markus J. Buehler*

Main category: cs.AI

TL;DR: 该研究展示了一个由去中心化、固定的基础模型组成的群集，通过信息素式（stigmergic）的对等信号协调，无需权重更新，即可生成连贯的长篇音乐作品。与集中式系统相比，该群集在质量、多样性和创造力方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索一种无需权重更新、通过去中心化协调机制实现连贯、长篇创意作品生成的方法，以克服传统集中式模型的局限性，并提高计算和数据效率。

Method: 研究对比了两种系统：一种是带有全局评论器的集中式多智能体系统，另一种是完全去中心化的群集系统。在群集系统中，以小节为单位的智能体通过感知和释放和声、节奏和结构线索，适应短期记忆并达成共识。通过符号、音频和图论分析来评估这两种系统。

Result: 群集系统在质量上优于集中式系统，并提供了更大的多样性和结构变异性，在创造力指标上表现领先。群集动态收敛于互补角色的稳定配置，自相似性网络揭示了具有高效长距离连接和小世界架构的专业桥接模式，解释了局部创新如何整合为全局音乐形式。

Conclusion: MusicSwarm通过将专业化从参数更新转移到交互规则、共享内存和动态共识，提供了一种计算和数据高效的实现长周期创意结构的方法。这种方法不仅适用于音乐创作，也立即可以推广到协作写作、设计和科学发现等领域。

Abstract: We show that coherent, long-form musical composition can emerge from a
decentralized swarm of identical, frozen foundation models that coordinate via
stigmergic, peer-to-peer signals, without any weight updates. We compare a
centralized multi-agent system with a global critic to a fully decentralized
swarm in which bar-wise agents sense and deposit harmonic, rhythmic, and
structural cues, adapt short-term memory, and reach consensus. Across symbolic,
audio, and graph-theoretic analyses, the swarm yields superior quality while
delivering greater diversity and structural variety and leads across creativity
metrics. The dynamics contract toward a stable configuration of complementary
roles, and self-similarity networks reveal a small-world architecture with
efficient long-range connectivity and specialized bridging motifs, clarifying
how local novelties consolidate into global musical form. By shifting
specialization from parameter updates to interaction rules, shared memory, and
dynamic consensus, MusicSwarm provides a compute- and data-efficient route to
long-horizon creative structure that is immediately transferable beyond music
to collaborative writing, design, and scientific discovery.

</details>


### [45] [Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review](https://arxiv.org/abs/2509.12034)
*Emmanuel Adjei Domfeh,Christopher L. Dancy*

Main category: cs.AI

TL;DR: 本文系统综述了在灾害管理中支持决策的人机协作模式，识别了四大类和子模式，强调了AI的优势和局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在高风险灾害情景中，及时和知情的决策至关重要，但常受不确定性、动态环境和资源限制的挑战。

Method: 对51篇同行评审研究进行了系统综述，识别并分析了支持灾害管理各阶段决策的人机协作模式，包括人机决策支持系统、任务和资源协调、信任与透明度、模拟与训练等四大类及其子模式。

Result: 研究识别出人机协作的四大主要类别及其子模式（如认知增强智能、多智能体协调、可解释AI、虚拟训练环境）。结果表明AI系统能增强态势感知、提高响应效率并支持复杂决策，但同时存在可扩展性、可解释性和系统互操作性方面的局限性。

Conclusion: 文章总结了关键挑战和未来研究方向，强调需要开发自适应、值得信赖和情境感知的人机系统，以提高灾害韧性和公平的恢复成果。

Abstract: In high-stakes disaster scenarios, timely and informed decision-making is
critical yet often challenged by uncertainty, dynamic environments, and limited
resources. This paper presents a systematic review of Human-AI collaboration
patterns that support decision-making across all disaster management phases.
Drawing from 51 peer-reviewed studies, we identify four major categories:
Human-AI Decision Support Systems, Task and Resource Coordination, Trust and
Transparency, and Simulation and Training. Within these, we analyze
sub-patterns such as cognitive-augmented intelligence, multi-agent
coordination, explainable AI, and virtual training environments. Our review
highlights how AI systems may enhance situational awareness, improves response
efficiency, and support complex decision-making, while also surfacing critical
limitations in scalability, interpretability, and system interoperability. We
conclude by outlining key challenges and future research directions,
emphasizing the need for adaptive, trustworthy, and context-aware Human-AI
systems to improve disaster resilience and equitable recovery outcomes.

</details>


### [46] [When Safe Unimodal Inputs Collide: Optimizing Reasoning Chains for Cross-Modal Safety in Multimodal Large Language Models](https://arxiv.org/abs/2509.12060)
*Wei Cai,Shujuan Liu,Jian Zhao,Ziyan Shi,Yusheng Zhao,Yuchen Yuan,Tianle Zhang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出多模态大语言模型（MLLMs）存在隐式推理风险，即无害的单模态输入组合可能导致有害输出。为解决此问题，作者引入了首个具有可解释推理路径的SSUI数据集，并设计了SRPO训练框架以使MLLM的内部推理过程与人类安全价值观对齐。实验表明，SRPO训练的模型在安全基准测试中达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）容易受到隐式推理风险的影响，即无害的单模态输入通过协同作用可能形成有风险的多模态数据，从而产生有害输出。研究者认为这种漏洞源于MLLMs难以在长链推理中保持安全对齐。

Method: 本文引入了Safe-Semantics-but-Unsafe-Interpretation (SSUI) 数据集，这是首个针对跨模态挑战、具有可解释推理路径的数据集。基于SSUI数据集，设计了一种新颖的训练框架——Safety-aware Reasoning Path Optimization (SRPO)，旨在使MLLM的内部推理过程与人类安全价值观对齐。

Result: 实验结果表明，经过SRPO训练的模型在关键安全基准测试（包括提出的推理路径基准RSBench）上取得了最先进的成果，显著优于开源和顶级的商业MLLM。

Conclusion: 通过引入SSUI数据集和SRPO训练框架，可以有效解决多模态大语言模型的隐式推理风险，提升其在长链推理中的安全对齐能力，并显著提高模型在安全基准上的表现。

Abstract: Multimodal Large Language Models (MLLMs) are susceptible to the implicit
reasoning risk, wherein innocuous unimodal inputs synergistically assemble into
risky multimodal data that produce harmful outputs. We attribute this
vulnerability to the difficulty of MLLMs maintaining safety alignment through
long-chain reasoning. To address this issue, we introduce
Safe-Semantics-but-Unsafe-Interpretation (SSUI), the first dataset featuring
interpretable reasoning paths tailored for such a cross-modal challenge. A
novel training framework, Safety-aware Reasoning Path Optimization (SRPO), is
also designed based on the SSUI dataset to align the MLLM's internal reasoning
process with human safety values. Experimental results show that our
SRPO-trained models achieve state-of-the-art results on key safety benchmarks,
including the proposed Reasoning Path Benchmark (RSBench), significantly
outperforming both open-source and top-tier commercial MLLMs.

</details>


### [47] [Bridging Engineering and AI Planning through Model-Based Knowledge Transformation for the Validation of Automated Production System Variants](https://arxiv.org/abs/2509.12091)
*Hamied Nabizada,Lasse Beers,Alain Chahine,Felix Gehlhoff,Oliver Niggemann,Alexander Fay*

Main category: cs.AI

TL;DR: 本文提出了一种模型驱动方法，用于在基于SysML的工程模型中集成符号规划语义，并自动生成PDDL规划工件，从而支持通过AI规划验证系统变体。


<details>
  <summary>Details</summary>
Motivation: MBSE工程模型虽然包含详细的系统结构和行为信息，但缺乏符号规划语义（如前置条件、效果、资源和时间约束），这限制了其评估系统变体执行特定任务的能力和效率。

Method: 该方法是模型驱动的，通过引入一个专用的SysML配置文件，包含可重用的核心规划构造刻板印象。这些构造被整合到现有模型结构中，并通过一个算法处理，自动生成有效的PDDL领域文件和问题文件。与以往依赖手动转换或外部能力模型的方法不同，本文方法支持原生集成并保持工程和规划工件之间的一致性。

Result: 该方法成功实现了在SysML工程模型中丰富规划语义，并自动生成了一致的PDDL规划工件。通过一个飞机装配的案例研究，验证了该方法的适用性，展示了如何将现有工程模型与规划语义结合，并应用该工作流生成规划工件。

Conclusion: 所生成的一致规划工件能够通过AI规划对系统变体进行验证，从而弥补了MBSE模型在符号规划语义方面的不足。

Abstract: Engineering models created in Model-Based Systems Engineering (MBSE)
environments contain detailed information about system structure and behavior.
However, they typically lack symbolic planning semantics such as preconditions,
effects, and constraints related to resource availability and timing. This
limits their ability to evaluate whether a given system variant can fulfill
specific tasks and how efficiently it performs compared to alternatives.
  To address this gap, this paper presents a model-driven method that enables
the specification and automated generation of symbolic planning artifacts
within SysML-based engineering models. A dedicated SysML profile introduces
reusable stereotypes for core planning constructs. These are integrated into
existing model structures and processed by an algorithm that generates a valid
domain file and a corresponding problem file in Planning Domain Definition
Language (PDDL). In contrast to previous approaches that rely on manual
transformations or external capability models, the method supports native
integration and maintains consistency between engineering and planning
artifacts.
  The applicability of the method is demonstrated through a case study from
aircraft assembly. The example illustrates how existing engineering models are
enriched with planning semantics and how the proposed workflow is applied to
generate consistent planning artifacts from these models. The generated
planning artifacts enable the validation of system variants through AI
planning.

</details>


### [48] [JustEva: A Toolkit to Evaluate LLM Fairness in Legal Knowledge Inference](https://arxiv.org/abs/2509.12104)
*Zongyue Xue,Siyuan Zheng,Shaochun Wang,Yiran Hu,Shenran Wang,Yuxin Yao,Haitao Li,Qingyao Ai,Yiqun Liu,Yun Liu,Weixing Shen*

Main category: cs.AI

TL;DR: 本研究引入了JustEva，一个全面的开源评估工具包，旨在衡量大型语言模型（LLM）在法律任务中的公平性，并发现当前LLM存在显著的公平性缺陷。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在法律实践中的整合引发了对司法公平性的紧迫担忧，特别是由于其“黑箱”处理过程。

Method: JustEva工具包包含：(1) 涵盖65个法外因素的结构化标签系统；(2) 三个核心公平性指标：不一致性、偏差和不平衡不准确性；(3) 稳健的统计推断方法；(4) 信息丰富的可视化。该工具支持两种实验类型：从LLM生成结构化输出，以及通过回归和其他统计方法对LLM输出进行统计分析和推断。

Result: JustEva的实证应用揭示了当前LLM在公平性方面存在显著缺陷，突出表明目前缺乏公平且值得信赖的LLM法律工具。

Conclusion: JustEva为评估和改进法律领域的算法公平性提供了一个便捷的工具和方法论基础。

Abstract: The integration of Large Language Models (LLMs) into legal practice raises
pressing concerns about judicial fairness, particularly due to the nature of
their "black-box" processes. This study introduces JustEva, a comprehensive,
open-source evaluation toolkit designed to measure LLM fairness in legal tasks.
JustEva features several advantages: (1) a structured label system covering 65
extra-legal factors; (2) three core fairness metrics - inconsistency, bias, and
imbalanced inaccuracy; (3) robust statistical inference methods; and (4)
informative visualizations. The toolkit supports two types of experiments,
enabling a complete evaluation workflow: (1) generating structured outputs from
LLMs using a provided dataset, and (2) conducting statistical analysis and
inference on LLMs' outputs through regression and other statistical methods.
Empirical application of JustEva reveals significant fairness deficiencies in
current LLMs, highlighting the lack of fair and trustworthy LLM legal tools.
JustEva offers a convenient tool and methodological foundation for evaluating
and improving algorithmic fairness in the legal domain.

</details>


### [49] [Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation](https://arxiv.org/abs/2509.12179)
*Yubo Li,Weiyi Song*

Main category: cs.AI

TL;DR: 本文提出双向认知对齐（BiCA）范式，使人类和AI相互适应，而非单向对齐，实验证明其在协作任务中显著提升了性能、适应性、协议收敛性和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前的AI对齐方法（如RLHF）遵循单向范式，即AI适应人类偏好，而将人类认知视为固定不变。研究旨在转向人类和AI相互适应的协同对齐。

Method: 提出双向认知对齐（BiCA）方法，通过可学习协议、表示映射和KL-预算约束实现受控的共同演化。

Result: 在协作导航任务中，BiCA的成功率为85.5%（基线为70.3%），相互适应性提高230%，协议收敛性提高332%。涌现协议比手工协议表现好84%，双向适应意外地提高了安全性（分布外鲁棒性提高23%），协同效应提高46%。

Conclusion: 研究验证了从单向对齐到协同对齐范式的转变，表明人类和AI能力的交集而非并集是实现最佳协作的关键。

Abstract: Current AI alignment through RLHF follows a single directional paradigm that
AI conforms to human preferences while treating human cognition as fixed. We
propose a shift to co-alignment through Bidirectional Cognitive Alignment
(BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols,
representation mapping, and KL-budget constraints for controlled co-evolution.
In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline,
with 230% better mutual adaptation and 332% better protocol convergence.
Emergent protocols outperformed handcrafted ones by 84%, while bidirectional
adaptation unexpectedly improved safety (+23% out-of-distribution robustness).
The 46% synergy improvement demonstrates optimal collaboration exists at the
intersection, not union, of human and AI capabilities, validating the shift
from single-directional to co-alignment paradigms.

</details>


### [50] [Advancing Medical Artificial Intelligence Using a Century of Cases](https://arxiv.org/abs/2509.12194)
*Thomas A. Buckley,Riccardo Conci,Peter G. Brodeur,Jason Gusdorf,Sourik Beltrán,Bita Behrouzi,Byron Crowe,Jacob Dockterman,Muzzammil Muhammad,Sarah Ohnigian,Andrew Sanchez,James A. Diao,Aashna P. Shah,Daniel Restrepo,Eric S. Rosenberg,Andrew S. Lea,Marinka Zitnik,Scott H. Podolsky,Zahir Kanjee,Raja-Elie E. Abdulnour,Jacob M. Koshy,Adam Rodman,Arjun K. Manrai*

Main category: cs.AI

TL;DR: 本研究创建了CPC-Bench基准和Dr. CaBot AI讨论者，发现大型语言模型（LLMs）在基于文本的鉴别诊断和医学演示方面超越了医生，但在图像解读和文献检索方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 以往对AI在《新英格兰医学杂志》临床病理会议（CPCs）中的评估仅关注最终诊断，未能解决专家讨论者所需的全面推理和演示技能。

Method: 研究利用7102份CPCs和1021份图像挑战，通过医生标注和自动化处理创建了CPC-Bench，这是一个涵盖10个文本和多模态任务的医生验证基准。随后，评估了领先的LLMs。研究还开发了“Dr. CaBot”，一个能仅凭病例陈述生成书面和幻灯片视频演示的AI讨论者，以模拟人类专家角色。

Result: 在377个当代CPCs中，o3（OpenAI）在60%的病例中将最终诊断排在首位，在84%的病例中排在前十名，优于20位医生的基线表现；下一步测试选择准确率达到98%。AI在文献检索和图像任务上的表现较低，o3和Gemini 2.5 Pro（Google）在图像挑战中达到67%的准确率。在CaBot与人类专家生成的文本的盲法比较中，医生在62次试验中有46次（74%）错误地识别了鉴别诊断的来源，并认为CaBot在质量维度上表现更优。CPC-Bench和CaBot已发布以促进研究。

Conclusion: LLMs在复杂的基于文本的鉴别诊断方面超越了医生的表现，并能令人信服地模仿专家医学演示，但图像解读和文献检索仍是其弱点。CPC-Bench和CaBot有望实现医学AI进展的透明和持续跟踪。

Abstract: BACKGROUND: For over a century, the New England Journal of Medicine
Clinicopathological Conferences (CPCs) have tested the reasoning of expert
physicians and, recently, artificial intelligence (AI). However, prior AI
evaluations have focused on final diagnoses without addressing the multifaceted
reasoning and presentation skills required of expert discussants.
  METHODS: Using 7102 CPCs (1923-2025) and 1021 Image Challenges (2006-2025),
we conducted extensive physician annotation and automated processing to create
CPC-Bench, a physician-validated benchmark spanning 10 text-based and
multimodal tasks, against which we evaluated leading large language models
(LLMs). Then, we developed "Dr. CaBot," an AI discussant designed to produce
written and slide-based video presentations using only the case presentation,
modeling the role of the human expert in these cases.
  RESULTS: When challenged with 377 contemporary CPCs, o3 (OpenAI) ranked the
final diagnosis first in 60% of cases and within the top ten in 84% of cases,
outperforming a 20-physician baseline; next-test selection accuracy reached
98%. Event-level physician annotations quantified AI diagnostic accuracy per
unit of information. Performance was lower on literature search and image
tasks; o3 and Gemini 2.5 Pro (Google) achieved 67% accuracy on image
challenges. In blinded comparisons of CaBot vs. human expert-generated text,
physicians misclassified the source of the differential in 46 of 62 (74%) of
trials, and scored CaBot more favorably across quality dimensions. To promote
research, we are releasing CaBot and CPC-Bench.
  CONCLUSIONS: LLMs exceed physician performance on complex text-based
differential diagnosis and convincingly emulate expert medical presentations,
but image interpretation and literature retrieval remain weaker. CPC-Bench and
CaBot may enable transparent and continued tracking of progress in medical AI.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [51] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

TL;DR: 本文提出一个实时、基于图像修复的减弱现实（DR）系统，旨在混合现实（MR）会议中实现隐私控制，允许用户移除环境中的敏感物品，使其对其他参与者不可见。


<details>
  <summary>Details</summary>
Motivation: 在共享空间的混合现实（MR）会议中，用户需要控制个人隐私，防止敏感或私人物品被其他参与者看到。

Method: 该系统通过语义分割和精确物体选择实现物体移除，并利用移动ZED 2i深度摄像头从第二观察者的视角进行实时图像修复。它采用YOLOv11进行物体检测，并使用改进的解耦时空Transformer（DSTT）模型进行高质量视频修复。系统设计为便携和鲁棒，无需固定第二视角或预先进行3D环境扫描。

Result: 在720p分辨率下，该系统能够维持超过20帧/秒的帧率，证明了实时减弱现实在实际隐私保护MR应用中的可行性。

Conclusion: 该研究表明，所开发的实时减弱现实系统为隐私保护型混合现实应用提供了可行的解决方案，能够有效移除共享MR空间中的敏感物品。

Abstract: Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [52] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

TL;DR: 本文提出了SurgLaVi，一个迄今为止最大、最多样化且具有分层结构的医学手术视觉-语言数据集，并引入了SurgCLIP模型。实验证明，SurgLaVi显著提升了手术工作流理解和任务识别的性能，为手术基础模型的开发奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 手术视觉-语言预训练（VLP）在通过对齐语言与手术视频来理解工作流和跨任务迁移方面具有独特优势，无需依赖专家标注的数据集。然而，现有手术VLP数据集在规模、程序多样性、语义质量和层次结构方面均存在限制，阻碍了该领域的进一步发展。

Method: 本文提出了SurgLaVi数据集，包含近24万个剪辑-字幕对，涵盖200多种手术程序，并具有阶段、步骤和任务三个层次。SurgLaVi的核心是一个全自动管道，用于系统地生成手术视频的细粒度转录并将其分割成连贯的程序单元。为确保高质量标注，该管道应用了双模态过滤来移除不相关和噪声样本，并丰富了字幕的上下文细节。此外，作者还发布了SurgLaVi-beta，一个包含11.3万个剪辑-字幕对的开源衍生版本。为验证SurgLaVi的价值，本文引入了SurgCLIP，一个带有双编码器的CLIP风格视频-文本对比框架作为代表性基础模型。

Result: SurgCLIP在手术阶段、步骤、动作和工具识别方面取得了持续改进，显著超越了现有最先进的方法。这些结果证明，大规模、语义丰富且具有层次结构的数据集可以直接转化为更强大、更具泛化能力的表示。

Conclusion: SurgLaVi数据集因其规模、语义丰富性和层次结构，能够直接转化为更强、更具泛化能力的模型表示，从而成为开发手术基础模型的关键资源。

Abstract: Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [53] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本文提出并发布了一个基于SimCLR的通用高分辨率3D脑部结构MRI自监督学习基础模型，该模型在11个公开数据集上进行了预训练，并在多个下游任务中显著优于其他模型，即使在有限的标记数据下也表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在3D MRI分析中通常针对特定任务定制，泛化能力差，且依赖大量标记数据。尽管自监督学习（SSL）在2D医学影像中取得了成功，但少数已有的3D脑部MRI基础模型在分辨率、范围或可访问性方面仍存在局限性，无法充分利用多样化的无标记数据。

Method: 研究者开发了一个基于SimCLR的通用高分辨率自监督学习基础模型，用于3D脑部结构MRI。该模型在包含18,759名患者（44,958次扫描）的11个公开数据集上进行了预训练，这些数据集涵盖了多种神经系统疾病。他们将该模型与Masked Autoencoders (MAE) 以及两个监督基线模型在四项多样化的下游预测任务（包括同分布和异分布设置）中进行了比较。

Result: 经过微调的SimCLR模型在所有任务中均优于所有其他模型。值得注意的是，即使仅使用20%的标记训练样本进行阿尔茨海默病预测的微调，该模型仍能取得卓越的性能。

Conclusion: 本工作提出了一个广泛适用且易于访问的高分辨率3D脑部结构MRI自监督学习基础模型，并通过公开代码和训练模型（https://github.com/emilykaczmarek/3D-Neuro-SimCLR）为临床脑部MRI分析做出了贡献。

Abstract: 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [54] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

TL;DR: 该论文提出USCTNet，一个基于物理原理的深度展开网络，用于从单张RGB图像重建高光谱图像（HSI）。它通过显式估计相机光谱灵敏度（CSS）和场景光照来确保色彩一致性，并引入数据自适应低秩子空间奇异值阈值（SVT）算子，在重建精度上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 从单张RGB图像重建高光谱图像是一个病态问题，当相机光谱灵敏度（CSS）和场景光照参数不准确时，重建结果可能在物理上不一致。

Method: 该研究将RGB到HSI重建公式化为一个由可学习变换域中的核范数正则化的基于物理的逆问题。它显式估计CSS和光照以定义前向算子，确保色彩一致性。为避免全奇异值分解（SVD）的成本和不稳定性，引入了数据自适应低秩子空间SVT算子。在此基础上，开发了USCTNet，一个深度展开求解器，将参数估计模块与可学习的近端更新相结合。

Result: 在标准基准上的大量实验表明，USCTNet在重建精度方面比现有最先进的基于RGB的方法有持续的改进。

Conclusion: USCTNet通过其物理建模、显式参数估计和高效的低秩正则化方法，有效解决了RGB到HSI重建的病态和不一致问题，实现了卓越的重建性能。

Abstract: Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [55] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

TL;DR: 本研究发现，在胶质瘤分类和分割等医学图像任务中，传统卷积神经网络（CNNs）的表现优于大型语言模型（LLMs），LLMs在当前形式下不适用于图像处理任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在基于文本的医疗保健任务中表现出色，但它们在基于图像的应用中的效用尚未被探索。本研究旨在调查LLMs在医学影像任务中的有效性。

Method: 研究使用了BraTS 2020多模态脑部MRI数据集，评估了一个通用视觉-语言LLM（LLaMA 3.2 Instruct），包括微调前后，并将其性能与定制的3D CNNs进行比较。任务包括胶质瘤分类（低级别 vs. 高级别）和分割（中心点、边界框、多边形提取三种方法）。

Result: 在胶质瘤分类中，CNNs达到了80%的准确率，并平衡了精确度和召回率。通用LLM达到了76%的准确率，但特异性仅为18%，经常错误分类低级别肿瘤。微调将特异性提高到55%，但总体性能下降（准确率降至72%）。在分割任务中，CNNs能准确地定位胶质瘤。相比之下，LLMs的预测始终集中在图像中心附近，无法区分胶质瘤的大小、位置或放置。微调改善了输出格式，但未能显著提高空间准确性。总体而言，CNNs在两项任务中均优于LLMs。

Conclusion: 在当前形式下，LLMs在医学图像任务中表现出有限的空间理解能力和微小的微调改进，表明它们不适用于图像处理任务。LLMs需要更严格的微调或替代训练策略，才能在医学领域实现更好的性能、鲁棒性和实用性。

Abstract: Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [56] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

TL;DR: SP4D是一个从单目输入生成配对RGB和运动学部件视频的框架，它使用双分支扩散模型和空间颜色编码，生成跨视图和时间一致的运动学部件。


<details>
  <summary>Details</summary>
Motivation: 传统的部件分割依赖于外观语义线索，而SP4D旨在学习生成与物体关节对齐、跨视图和时间一致的运动学部件，以更好地支持下游动画和运动相关任务。

Method: SP4D采用双分支扩散模型，共同合成RGB帧和对应的部件分割图。引入空间颜色编码方案，将部件掩码映射为连续的类RGB图像，使分割分支能共享RGB分支的潜在VAE。通过双向扩散融合（BiDiFuse）模块增强跨分支一致性，并辅以对比部件一致性损失以促进空间和时间对齐。为训练和评估，构建了KinematicParts20K数据集。

Result: SP4D能够生成运动学感知的输出，并对包括真实世界视频、新生成的物体和罕见关节姿态在内的多样化场景表现出强大的泛化能力。生成的2D部件图可以提升到3D，以少量手动调整即可推导出骨骼结构和谐波蒙皮权重。

Conclusion: SP4D是一个有效的框架，能够从单目输入生成运动学感知的RGB和部件视频，其输出适用于下游动画和运动相关任务，并展现出强大的泛化能力。

Abstract: We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [57] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

TL;DR: SegSLR是一种新的孤立手语识别（ISLR）系统，它通过可提示的零样本视频分割技术，有效地结合了RGB和姿态信息，以保留关键的手部形状和方向细节，从而在复杂数据集上超越了现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的孤立手语识别（ISLR）方法主要依赖RGB数据或姿态信息，但在结合这些模态时，由于边界框等不精确的表示，常常导致手部形状和方向等关键细节的丢失。

Method: SegSLR系统通过可提示的零样本视频分割来结合RGB和姿态信息。它利用姿态信息中手部和身体的粗略定位，在视频中分割出这些相关部分，以保留所有相关的形状信息。随后，这些分割结果将RGB数据的处理集中在ISLR最相关的身体部位上。

Result: SegSLR在复杂的ChaLearn249 IsoGD数据集上的评估显示，其性能优于现有先进方法。此外，消融研究表明，SegSLR通过专注于手语者的身体和手部获得了显著益处，证明了其设计选择的合理性。

Conclusion: SegSLR通过创新的零样本视频分割方法，有效地结合了RGB和姿态信息，解决了传统方法中细节丢失的问题，显著提升了孤立手语识别的性能，并通过实验验证了其设计理念的有效性。

Abstract: Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [58] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

TL;DR: 本文提出了一种语音引导的协同感知（SCOPE）框架，结合大型语言模型（LLM）和视觉基础模型（VFM），实现术中视频中手术器械和解剖结构的即时、开放集分割、标注和跟踪，支持人机协作。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景分割和跟踪方案依赖于领域特定、监督模型和标注数据，难以适应新的手术场景和预定义标签之外的类别。尽管基于提示的视觉基础模型实现了开放集、零样本分割，但其对人工视觉或文本提示的依赖限制了在术中环境的部署，因此需要一种更自然、自适应的解决方案。

Method: SCOPE框架将LLM的推理能力与开放集VFM的感知能力相结合。核心是一个协同感知代理，它生成VFM分割的候选结果，并整合临床医生直观的语音反馈来指导手术器械的分割。随后，器械本身作为交互式指针来标注手术场景中的其他元素。

Result: 该框架在公开的Cataract1k数据集子集和内部离体颅底数据集上进行了评估，证明了其生成手术场景即时分割和跟踪的潜力。此外，通过一次离体模拟实验展示了其动态能力。

Conclusion: 这种人机协作范式展示了开发适应性强、免手操作、以外科医生为中心的工具在动态手术室环境中的巨大潜力。

Abstract: Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [59] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为4D高斯光线追踪（4D-GRT）的新方法，通过结合4D高斯泼溅和物理光线追踪，以快速且高质量地生成包含可控、物理准确相机效应的训练数据，从而解决现有计算机视觉系统在真实世界相机效应下表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉系统通常假设理想针孔相机模型，但在面对鱼眼畸变和卷帘快门等真实世界相机效应时会失效，主要是因为缺乏带有这些效应的训练数据。现有数据生成方法存在成本高、模拟与现实差距大或无法准确模拟相机效应的问题，因此需要一种新的解决方案。

Method: 本文提出4D高斯光线追踪（4D-GRT），这是一个两阶段管道。首先，它利用多视角视频重建动态场景（结合4D高斯泼溅）。然后，它应用物理光线追踪来生成带有可控、物理准确相机效应的视频。

Result: 4D-GRT在实现最快渲染速度的同时，与现有基线相比，渲染质量达到更好或相当的水平。此外，研究人员构建了一个包含八个室内动态场景和四种相机效应的合成基准，用于评估生成的带有相机效应的视频。

Conclusion: 4D-GRT通过提供一种高效、高质量且物理准确的相机效应模拟方法，有效解决了生成真实世界相机效应训练数据的瓶颈问题。它不仅提升了渲染速度和质量，还为相关研究提供了一个新的评估基准。

Abstract: Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [60] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

TL;DR: 该论文提出了一种将视频编辑自动化为顺序决策过程的多智能体方法，包含一个编辑智能体和一个评论智能体，并使用LLM作为评判指标进行评估，结果显示其在覆盖率、时间约束满足度和人类偏好方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频编辑工具主要集中在检索或用户界面上，实际的编辑工作仍需用户完成。本研究旨在自动化视频编辑的核心任务，将其表述为顺序决策过程。

Method: 该方法将视频编辑建模为顺序决策过程，并采用多智能体架构：一个编辑智能体（Editor）接收视频片段和自然语言指令，利用常用编辑工具生成编辑序列；一个评论智能体（Critic）根据生成的序列提供自然语言反馈，或在满意时渲染。文章引入了一种基于学习的方法来实现专业智能体之间有效的沟通，并探索了以大型语言模型（LLM）作为评判指标来评估视频编辑系统质量，并与人类偏好进行比较。

Result: 通过定性和定量用户研究，该系统在覆盖率、时间约束满足度和人类偏好方面显著优于现有方法。

Conclusion: 该研究成功地将视频编辑的核心任务自动化为多智能体顺序决策过程，并通过有效的智能体间通信和LLM评估，显著提升了视频编辑的自动化水平和质量，超越了现有方法。

Abstract: Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [61] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

TL;DR: 该研究提出一个稳定性感知框架，利用机器学习从非对比增强胶质瘤MRI预测对比增强，旨在减少对基于钆的造影剂的依赖，并提高多中心模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基于钆的对比剂（GBCAs）在胶质瘤成像中存在安全性、成本和可及性问题。从非对比增强MRI预测对比增强提供了一种更安全的替代方案，但扫描仪和队列变异性阻碍了稳健的模型选择。

Method: 分析了来自四个TCIA数据集的1,446例胶质瘤病例。以非对比增强T1WI作为输入，通过配对的对比增强T1WI获取增强信息。使用PyRadiomics（符合IBSI标准）提取108个特征，并结合48种降维方法和25种分类器，生成了1,200条机器学习流程。采用轮换验证法（在三个数据集上训练，在一个数据集上测试），并提出了一个稳定性感知框架来识别可重现的机器学习流程。

Result: 交叉验证预测准确率介于0.91至0.96之间。外部测试准确率分别为0.87 (UCSF-PDGM)、0.98 (UPENN-GB)和0.95 (BRATS-Africa)，平均为0.93。F1、精确度和召回率稳定在0.87至0.96之间，而ROC-AUC变化较大（0.50至0.82），反映了队列异质性。MI与ETr结合的流程在准确性和稳定性方面表现最佳。

Conclusion: 该稳定性感知框架能够可靠地从非对比增强胶质瘤MRI预测对比增强，减少了对GBCAs的依赖，并提高了跨中心的泛化能力。它为神经肿瘤学及其他领域的可重现机器学习提供了一个可扩展的模板。

Abstract: Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [62] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种与检测器无关的后处理框架，通过将重叠引起的冗余转换为群组证据，以提高无人机图像中密集小目标检测的召回率，尤其适用于召回敏感型应用。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的密集小目标由于远距离视角、遮挡和杂乱等因素，经常被遗漏。

Method: 该框架包括以下步骤：1) 重叠平铺以恢复低置信度候选；2) 空间门（基于边界框中心点的DBSCAN）和语义门（基于ResNet-18嵌入的DBSCAN）来验证群组证据；3) 对经过验证的群组进行受控的置信度重加权；4) 类别感知NMS融合。该方法无需重新训练，可与现代检测器集成。

Result: 在VisDrone数据集上的实验显示，召回率从0.685提高到0.778（+0.093），精度从0.801调整到0.595，F1分数达到0.669。后处理平均延迟为每张图像0.095秒。消融实验证实了平铺暴露遗漏目标、空间聚类稳定几何、语义聚类增强外观一致性以及重加权提供校准集成效果。

Conclusion: 该框架表现出召回优先、精度权衡的特性，对远场计数和监控等召回敏感型应用有益。它无需重新训练且能与现有检测器集成。未来的工作将侧重于降低语义门控成本并扩展到利用时间线索。

Abstract: Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [63] [MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs](https://arxiv.org/abs/2509.11662)
*Feilong Chen,Yijiang Liu,Yi Huang,Hao Wang,Miren Tian,Ya-Qi Yu,Minghui Liao,Jihao Wu*

Main category: cs.CV

TL;DR: 本文提出了MindVL，一个在昇腾NPU上训练的多模态大语言模型。它采用原生分辨率Vision Transformers处理可变分辨率图像，并使用专为昇腾NPU设计的Mindspeed-MLLM训练框架。MindVL通过三阶段训练和多种优化技术，仅用Qwen2.5-VL约1/10的数据，就在多模态理解、文档/表格理解和OCR方面取得了与Qwen2.5-VL相当甚至领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理高分辨率、视觉密集型内容（如复杂图表和图示）时，固定分辨率平铺会导致信息降级。研究旨在开发一种能有效处理原生分辨率图像，同时能在昇腾NPU上高效训练的多模态大语言模型，以保留细粒度细节和全局布局。

Method: MindVL模型采用原生分辨率Vision Transformers处理原始可变分辨率图像。为在昇腾NPU上高效训练，开发了Mindspeed-MLLM分布式多模态训练框架，并对特定算子进行等效替换以保持精度。训练过程分为三阶段：热身、多任务训练和监督指令微调。同时，采用了多模态数据打包和混合并行技术以提升训练速度，并通过测试时分辨率搜索和模型权重平均进一步提升性能。

Result: MindVL在通用多模态理解、文档/表格理解评估中达到了与Qwen2.5-VL相当的性能。值得注意的是，它仅使用了Qwen2.5-VL所需训练数据量的约1/10。此外，MindVL在OCR评估中也表现出领先性能。

Conclusion: MindVL证明了在昇腾NPU上，通过原生分辨率处理、定制化训练框架和优化策略，可以高效训练出高性能的多模态大语言模型。它能在显著减少训练数据的情况下，在多模态理解和文档/表格处理等任务上取得与现有顶尖模型相当的成果，并在OCR方面表现出色。

Abstract: We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

</details>


### [64] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文介绍了InternScenes，一个大规模、可模拟的室内3D场景数据集，旨在解决现有具身AI数据集在规模、多样性和真实性方面的不足，并推动复杂场景下的模型训练和应用。


<details>
  <summary>Details</summary>
Motivation: 具身AI的发展受限于缺乏大规模、可模拟、多样化且布局真实的3D场景数据集。现有数据集通常存在规模或多样性不足、布局过于“干净”（缺少小物件）以及物体碰撞严重等问题。

Method: 作者提出了InternScenes数据集，通过整合三种不同来源（真实世界扫描、程序生成场景和设计师创建场景）来构建。该数据集特别保留了大量小物件以实现真实复杂的布局，并通过数据处理管线确保可模拟性（为真实扫描创建真-模拟副本）、增强交互性（加入交互对象）和解决物体碰撞（通过物理模拟）。

Result: InternScenes包含约40,000个多样化场景，1.96M个3D对象，覆盖15种常见场景类型和288个对象类别，平均每个区域有41.5个对象，布局真实且复杂。在场景布局生成和点目标导航这两个基准应用中，InternScenes展示了其复杂和真实布局带来的新挑战，并为这些任务的模型训练规模化提供了可能。

Conclusion: InternScenes通过提供一个大规模、多样化且可模拟的真实室内场景数据集，解决了现有具身AI数据集的局限性。它为具身AI领域带来了新的挑战和机遇，尤其是在复杂场景生成和导航方面，并将通过开源数据、模型和基准来回馈社区。

Abstract: The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [65] [Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos](https://arxiv.org/abs/2509.11948)
*Mahmoud Z. A. Wahba,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 本文提出Sphere-GAN，一个基于球形卷积的生成对抗网络，用于360度视频的显著性检测，其性能优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 沉浸式应用的成功推动了360度图像和视频处理及传输优化方法的研究。显著性估计是识别视觉相关区域的强大工具，但针对360度内容的显著性估计算法非常少。

Method: 本文引入了Sphere-GAN，一个利用球形卷积的生成对抗网络（GAN）来检测360度视频的显著性。

Result: 在公开的360度视频显著性数据集上进行的广泛实验表明，Sphere-GAN在准确预测显著性图方面优于现有最先进的模型。

Conclusion: Sphere-GAN为360度视频显著性检测提供了一种有效且性能卓越的新方法，利用球形卷积改进了显著性预测。

Abstract: The recent success of immersive applications is pushing the research
community to define new approaches to process 360{\deg} images and videos and
optimize their transmission. Among these, saliency estimation provides a
powerful tool that can be used to identify visually relevant areas and,
consequently, adapt processing algorithms. Although saliency estimation has
been widely investigated for 2D content, very few algorithms have been proposed
for 360{\deg} saliency estimation. Towards this goal, we introduce Sphere-GAN,
a saliency detection model for 360{\deg} videos that leverages a Generative
Adversarial Network with spherical convolutions. Extensive experiments were
conducted using a public 360{\deg} video saliency dataset, and the results
demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately
predicting saliency maps.

</details>


### [66] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

TL;DR: 本文探讨了在数学手写识别中，使用不同多项式基（如勒让德、切比雪夫及其索伯列夫变体）和多项式次数进行曲线表示的权衡，旨在实现准确建模和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 先前的研究已使用参数化平面曲线多项式表示数学手写，并初步探索了勒让德和切比雪夫基。本文的动机是深入探究不同基选择和多项式次数之间的权衡，以在保持准确性的同时降低计算成本。

Method: 研究方法包括考虑在不同基中进行多项式评估的条件数，并界定各种内积如何为符号间的变化提供范数。

Result: 通过对多项式评估条件数和符号间变化范数的分析，探索了不同基选择（勒让德、切比雪夫及其索伯列夫变体）和多项式次数在实现准确建模与低计算成本之间的权衡。

Conclusion: 本文旨在提供关于在数学手写表示中，如何根据基选择和多项式次数，在建模准确性和计算效率之间取得最佳平衡的见解。

Abstract: Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [67] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

TL;DR: 本文提出了一种多任务扩散框架，用于胶质瘤时间无关、像素级进展预测，通过数据增强和放射治疗加权损失解决了稀疏纵向MRI数据的挑战，并生成了灵活的未来概率图。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤是一种侵袭性脑恶性肿瘤，进展迅速，预后差，准确预测其演变具有挑战性。临床实践中稀疏、不规则获取的纵向MRI数据，以及不完整的随访序列导致数据不平衡，使得可靠建模变得困难。

Method: 该研究提出了一个多任务扩散框架，用于时间无关、像素级的胶质瘤进展预测。该模型同时生成未来FLAIR序列并估计基于符号距离场（SDFs）的空间概率肿瘤演变图，实现不确定性量化。为捕捉任意时间间隔的肿瘤演变时间动态，集成了预训练的形变模块。针对数据稀缺问题，实施了目标性增强管道，合成完整的随访扫描序列并填补缺失的MRI模态。此外，引入了放射治疗加权焦点损失项，利用辐射剂量图突出临床重要区域。

Result: 该框架仅基于早期时间点的两次随访扫描，即可生成灵活的、依赖时间的概率图，使临床医生能够查询任何未来时间点的肿瘤进展风险。该方法在公共数据集上进行训练，并在内部私有数据集上进行评估，均取得了有前景的结果。

Conclusion: 该研究提出的多任务扩散框架为胶质瘤进展预测提供了一个稳健的解决方案，有效应对了临床实践中数据稀疏的挑战，并能为临床医生提供灵活、可解释的未来肿瘤进展风险图。

Abstract: Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [68] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

TL;DR: 本文提出了一种名为3PNet的方法，通过点平面投影从2D表示中学习特征，并引入了几何感知的数据增强技术，以提高激光雷达点云语义分割在数据稀缺场景中的性能，同时保持计算效率和仅依赖激光雷达数据。


<details>
  <summary>Details</summary>
Motivation: 现有的激光雷达点云语义分割方法通常计算复杂度高，且需要大量的训练数据，这限制了它们在数据稀缺场景中的泛化能力。

Method: 该方法通过点平面投影有效地从2D表示中学习特征，从而提取互补信息，且仅依赖激光雷达数据。此外，它引入了一种几何感知的数据增强技术，该技术与激光雷达传感器特性对齐并缓解了类别不平衡问题。具体实现是将点平面投影应用于点云的多个信息丰富的2D表示。

Result: 实验证明，该方法在有限数据场景中取得了显著的性能提升，并在SemanticKITTI和PandaSet这两个公开标准数据集上取得了具有竞争力的结果。

Conclusion: 通过利用点平面投影从2D表示中学习特征并结合几何感知的数据增强，所提出的方法能够有效提升激光雷达点云语义分割在数据稀缺场景中的性能，并展现出良好的泛化能力和竞争力。

Abstract: LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [69] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

TL;DR: OpenUrban3D是首个针对大规模城市场景的3D开放词汇语义分割框架，无需对齐的多视图图像、预训练点云分割网络或手动标注，通过多视图渲染和视觉-语言特征提取实现零样本分割，显著提升了分割精度和跨场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割对城市点云应用（如数字孪生、智慧城市管理）至关重要，但在此领域尚待探索。主要障碍包括大规模城市点云数据集中缺乏高质量、对齐的多视图图像，以及现有3D分割管道在几何、尺度和外观差异显著的城市环境中泛化能力差。

Method: OpenUrban3D通过多视图、多粒度渲染，掩码级视觉-语言特征提取和样本均衡融合，直接从原始点云生成鲁棒的语义特征，然后将其蒸馏到3D骨干模型中。这种设计使得模型能够对任意文本查询进行零样本分割，并捕获语义丰富性和几何先验。

Result: 在SensatUrban和SUM等大规模城市基准测试中，OpenUrban3D在分割精度和跨场景泛化方面均优于现有方法，取得了显著改进。

Conclusion: OpenUrban3D展示了其作为3D城市场景理解的灵活且可扩展解决方案的潜力，能够处理任意自然语言描述的对象识别和分割。

Abstract: Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [70] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

TL;DR: 本文提出AutoOEP，一个多模态自动化在线考试监考框架，利用双摄像头、计算机视觉和机器学习检测作弊行为，实现了高准确率和资源效率。


<details>
  <summary>Details</summary>
Motivation: 随着在线教育的兴起，远程考试的学术诚信保障面临巨大挑战。传统人工监考难以大规模实施，现有自动化方案则可能过于侵入性或无法检测多种作弊行为，因此急需一种鲁棒且可扩展的解决方案。

Method: AutoOEP采用双摄像头设置（正面和侧面视图）以减少盲点。它整合了并行分析模块：面部模块使用ArcFace进行身份验证、头部姿态估计、凝视追踪和嘴部运动分析；手部模块利用YOLOv11检测违禁物品（如手机、笔记）并追踪手与这些物品的距离。这些模块的特征被聚合并输入到长短期记忆（LSTM）网络中，分析时间模式以计算实时作弊概率得分。

Result: AutoOEP在模拟多种考试条件的自定义数据集中进行了评估，在分类可疑活动方面达到了90.7%的准确率。对象检测组件对违禁物品的平均精度（mAP@.5）为0.57。整个框架在没有GPU的情况下，能以大约每秒2.4帧的速度处理视频流。

Conclusion: AutoOEP是一个有效且资源高效的自动化监考解决方案，能够显著减少人工干预，提高在线评估的诚信度。

Abstract: The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [71] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

TL;DR: 本文提出了一种双相机编码孔径快照光谱成像（CASSI）重建框架，通过整合全变分（TV）次梯度理论和动态正则化策略，有效解决了传统CASSI在平衡分辨率和重建质量上的挑战，并提供了可解释的数学基础。


<details>
  <summary>Details</summary>
Motivation: 光谱成像在平衡光谱、空间和时间分辨率方面面临根本性挑战。基于压缩感知的CASSI虽然通过光学编码缓解了这一问题，但高压缩比导致重建病态。传统模型方法因依赖手工先验而性能有限，而深度学习方法因其黑盒性质而缺乏物理可解释性。

Method: 提出了一种双相机CASSI重建框架，整合了全变分（TV）次梯度理论。建立了端到端的SD-CASSI数学模型，降低了逆问题的计算复杂度。引入动态正则化策略，利用RGB/全色参考图像的归一化梯度约束，构建了具有严格凸优化保证的TV次梯度相似函数。设计了自适应参考生成和更新机制，利用辅助相机的空间先验提供次梯度指导。

Result: 实验结果表明，所提出的方法有效保持了空间-光谱结构的一致性。该理论框架为计算光谱成像建立了可解释的数学基础，并在各种重建场景中表现出鲁棒的性能。

Conclusion: 本文提出的基于TV次梯度理论的双相机CASSI重建框架，通过动态正则化和自适应参考机制，不仅解决了CASSI的重建难题，提高了性能，而且为计算光谱成像提供了一个具有可解释性的数学基础，展现了在不同重建场景下的强大鲁棒性。

Abstract: Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [72] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

TL;DR: 本文提出了一种紧凑的、元数据感知的混合专家掩码自编码器（MoE-MAE），参数量仅为2.5M，旨在解决地球观测（EO）基础模型计算成本高的问题，并在各种任务中展现出与大型模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的地球观测（EO）基础模型计算成本高昂，限制了它们的可访问性和下游任务的重用性。

Method: 本文提出了一种元数据感知的混合专家掩码自编码器（MoE-MAE），其参数量仅为2.5M。该模型结合了稀疏专家路由和地理-时间条件化，将图像数据与经纬度、季节/日循环编码等元数据相结合。模型在BigEarthNet-Landsat数据集上进行预训练，并使用线性探针评估其冻结编码器的嵌入效果。

Result: 尽管模型规模很小，但其性能与大得多的架构相当，表明元数据感知的预训练能够提高迁移和标签效率。在缺乏显式元数据的EuroSAT-Landsat数据集上评估时，模型仍能与拥有数亿参数的模型表现出竞争性性能。

Conclusion: 这些结果表明，紧凑的、元数据感知的MoE-MAE是未来地球观测基础模型发展中高效且可扩展的一步。

Abstract: Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [73] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

TL;DR: 本文优化了一种基于正弦图的方法来模拟HR-pQCT图像中的运动伪影，并提出了一个名为ESWGAN-GP的深度学习模型，利用边缘增强跳跃连接和自注意力机制进行运动校正，在模拟和真实数据集上均显示出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 高分辨率外周定量CT（HR-pQCT）中的刚性运动伪影（如皮质骨条纹和骨小梁涂抹）阻碍了骨微结构的体内评估。尽管存在多种运动分级技术，但由于缺乏标准化的退化模型，目前尚无有效的运动校正方法。

Method: 研究者优化了一种传统的基于正弦图的方法来模拟HR-pQCT图像中的运动伪影，创建了配对的运动损坏图像及其真实值数据集。在此基础上，他们提出了一个边缘增强自注意力Wasserstein生成对抗网络与梯度惩罚（ESWGAN-GP）模型，用于解决模拟和真实世界数据集中的运动伪影。该模型结合了边缘增强跳跃连接以保留骨小梁边缘，以及自注意力机制以捕获长程依赖关系。此外，还采用了基于VGG的感知损失来重建精细的微结构特征。

Result: ESWGAN-GP模型在模拟（源）数据集上实现了平均信噪比（SNR）26.78、结构相似性指数（SSIM）0.81和视觉信息保真度（VIF）0.76。在真实世界（目标）数据集上，性能进一步提高，SNR达到29.31，SSIM为0.87，VIF为0.81。

Conclusion: 尽管所提出的方法处理的是真实世界运动的简化表示，可能未能完全捕捉体内运动伪影的复杂性，但它们代表了在HR-pQCT中实现基于深度学习的运动校正的重要初始步骤。这对于解决运动伪影这一阻碍HR-pQCT更广泛应用的主要挑战至关重要。

Abstract: Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>


### [74] [Gaze Authentication: Factors Influencing Authentication Performance](https://arxiv.org/abs/2509.10969)
*Dillon Lohr,Michael J Proulx,Mehedi Hasan Raju,Oleg V Komogortsev*

Main category: cs.CV

TL;DR: 本研究探讨了影响最先进注视点认证性能的关键因素，发现眼动信号质量、校准方法和原始注视点数据融合对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别并量化影响最先进注视点认证性能的关键因素，以期优化其准确性和鲁棒性。

Method: 研究在一个包含8,849名受试者的大规模内部数据集上进行实验，该数据集使用与Meta Quest Pro等效的硬件和72Hz视频眼动图驱动的注视点估计管道收集。采用最先进的神经网络架构，分析了眼动信号质量、眼动校准的各个方面以及对估计的原始注视点进行简单滤波对认证性能的影响。

Result: 研究发现，使用相同的校准目标深度进行眼动校准、融合校准和非校准注视点数据，以及提高眼动信号质量，都能提升认证性能。然而，简单的三样本移动平均滤波器通常会略微降低认证性能。尽管这些发现大部分成立，但也存在一些例外情况。

Conclusion: 眼动信号质量、校准方法的选择以及注视点数据的处理方式是影响注视点认证性能的关键因素。通过优化这些方面，可以显著提高认证系统的有效性。

Abstract: This paper examines the key factors that influence the performance of
state-of-the-art gaze-based authentication. Experiments were conducted on a
large-scale, in-house dataset comprising 8,849 subjects collected with Meta
Quest Pro equivalent hardware running a video oculography-driven gaze
estimation pipeline at 72Hz. The state-of-the-art neural network architecture
was employed to study the influence of the following factors on authentication
performance: eye tracking signal quality, various aspects of eye tracking
calibration, and simple filtering on estimated raw gaze. We found that using
the same calibration target depth for eye tracking calibration, fusing
calibrated and non-calibrated gaze, and improving eye tracking signal quality
all enhance authentication performance. We also found that a simple
three-sample moving average filter slightly reduces authentication performance
in general. While these findings hold true for the most part, some exceptions
were noted.

</details>


### [75] [TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation](https://arxiv.org/abs/2509.10980)
*Haoming Lu*

Main category: cs.CV

TL;DR: 该研究引入了TrueSkin数据集（7299张图片，6个肤色类别），旨在解决现有大型多模态模型（LMMs）和图像生成模型在肤色识别和生成方面的不足与偏见，并证明了TrueSkin能显著提高这些任务的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 肤色识别和生成在模型公平性、医疗保健和生成式AI中扮演重要角色，但由于缺乏全面的数据集和强大的方法，现有的大型多模态模型和图像生成模型在准确识别和合成肤色方面表现不佳，存在显著偏见。

Method: 研究引入了TrueSkin数据集，包含7299张在多样化光照、摄像角度和捕获设置下系统分类为6个肤色类别的图像。利用TrueSkin，研究基准测试了现有的识别和生成方法，并在此基础上训练了一个识别模型，同时对图像生成模型进行了微调。

Result: 基准测试揭示了显著偏见：LMMs倾向于将中间肤色错误分类为较浅肤色；生成模型在受到提示中无关属性（如发型、环境）影响时，难以准确生成指定肤色。在TrueSkin上训练的识别模型将分类准确率提高了20%以上；用TrueSkin进行微调显著提高了图像生成模型中肤色的保真度。

Conclusion: 研究结果强调了像TrueSkin这样全面数据集的必要性，它不仅可以作为评估现有模型的基准，也为提高肤色识别和生成任务的公平性和准确性提供了宝贵的训练资源。

Abstract: Skin tone recognition and generation play important roles in model fairness,
healthcare, and generative AI, yet they remain challenging due to the lack of
comprehensive datasets and robust methodologies. Compared to other human image
analysis tasks, state-of-the-art large multimodal models (LMMs) and image
generation models struggle to recognize and synthesize skin tones accurately.
To address this, we introduce TrueSkin, a dataset with 7299 images
systematically categorized into 6 classes, collected under diverse lighting
conditions, camera angles, and capture settings. Using TrueSkin, we benchmark
existing recognition and generation approaches, revealing substantial biases:
LMMs tend to misclassify intermediate skin tones as lighter ones, whereas
generative models struggle to accurately produce specified skin tones when
influenced by inherent biases from unrelated attributes in the prompts, such as
hairstyle or environmental context. We further demonstrate that training a
recognition model on TrueSkin improves classification accuracy by more than
20\% compared to LMMs and conventional approaches, and fine-tuning with
TrueSkin significantly improves skin tone fidelity in image generation models.
Our findings highlight the need for comprehensive datasets like TrueSkin, which
not only serves as a benchmark for evaluating existing models but also provides
a valuable training resource to enhance fairness and accuracy in skin tone
recognition and generation tasks.

</details>


### [76] [Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring](https://arxiv.org/abs/2509.10995)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习（RL）的迁移学习框架，利用UCB算法自动选择最适合动物检测任务的预训练模型，以解决UAV动物监测中标记数据稀缺和模型选择困难的问题。


<details>
  <summary>Details</summary>
Motivation: 野生动物保护和畜牧管理日益依赖自动化检测和跟踪系统（如UAV结合计算机视觉）。然而，深度学习模型面临标记训练数据有限的挑战。迁移学习虽有潜力，但预训练神经网络架构众多，为研究人员选择最佳模型带来困难。

Method: 本文提出了一种基于强化学习（RL）的迁移学习框架。该框架采用上置信界（UCB）算法，通过系统评估和排名候选模型，自动选择最适合动物检测任务的预训练模型，从而简化模型选择过程。

Result: 实验结果表明，与传统方法相比，该框架在显著减少计算时间的同时，实现了更高的检测率。

Conclusion: 所提出的基于RL的迁移学习框架能够有效简化动物检测任务中的模型选择过程，提高了检测性能和计算效率，为资源受限场景下的动物监测提供了有效解决方案。

Abstract: Animal health monitoring and population management are critical aspects of
wildlife conservation and livestock management that increasingly rely on
automated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)
based systems combined with computer vision offer promising solutions for
non-invasive animal monitoring across challenging terrains, limited
availability of labeled training data remains an obstacle in developing
effective deep learning (DL) models for these applications. Transfer learning
has emerged as a potential solution, allowing models trained on large datasets
to be adapted for resource-limited scenarios such as those with limited data.
However, the vast landscape of pre-trained neural network architectures makes
it challenging to select optimal models, particularly for researchers new to
the field. In this paper, we propose a reinforcement learning (RL)-based
transfer learning framework that employs an upper confidence bound (UCB)
algorithm to automatically select the most suitable pre-trained model for
animal detection tasks. Our approach systematically evaluates and ranks
candidate models based on their performance, streamlining the model selection
process. Experimental results demonstrate that our framework achieves a higher
detection rate while requiring significantly less computational time compared
to traditional methods.

</details>


### [77] [Improving Fungi Prototype Representations for Few-Shot Classification](https://arxiv.org/abs/2509.11020)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: FungiCLEF 2025竞赛旨在解决基于实地观测数据进行真菌物种自动识别的挑战，尤其针对数据不平衡和稀有物种样本不足的问题。本文提出了一种基于原型网络的深度学习方法，显著提升了少样本真菌分类的性能。


<details>
  <summary>Details</summary>
Motivation: 真菌物种的准确识别工具对菌类学家和公民科学家都至关重要，能极大促进大规模生物多样性监测。现有识别系统需要处理高度不平衡的类别分布，并为许多物种（特别是稀有和记录不足的类群）提供可靠的性能，即使训练样本极少。

Method: 提出了一种基于原型网络的鲁棒深度学习方法，该方法通过增强原型表示来改进少样本真菌分类。

Result: 在公共和私人排行榜上，该原型网络方法在Recall@5指标上比竞赛基线提高了30多个百分点。

Conclusion: 该方法在准确识别常见和稀有真菌物种方面展现出强大潜力，支持了FungiCLEF 2025的主要目标。

Abstract: The FungiCLEF 2025 competition addresses the challenge of automatic fungal
species recognition using realistic, field-collected observational data.
Accurate identification tools support both mycologists and citizen scientists,
greatly enhancing large-scale biodiversity monitoring. Effective recognition
systems in this context must handle highly imbalanced class distributions and
provide reliable performance even when very few training samples are available
for many species, especially rare and under-documented taxa that are often
missing from standard training sets. According to competition organizers, about
20\% of all verified fungi observations, representing nearly 20,000 instances,
are associated with these rarely recorded species. To tackle this challenge, we
propose a robust deep learning method based on prototypical networks, which
enhances prototype representations for few-shot fungal classification. Our
prototypical network approach exceeds the competition baseline by more than 30
percentage points in Recall@5 on both the public (PB) and private (PR)
leaderboards. This demonstrates strong potential for accurately identifying
both common and rare fungal species, supporting the main objectives of
FungiCLEF 2025.

</details>


### [78] [Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images](https://arxiv.org/abs/2509.11034)
*Yuedi Zhang,Zhixiang Xia,Guosheng Yin,Bin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为csMIL的新型多实例学习（MIL）框架，通过整合全局-局部实例聚类、簇内注意力机制和簇级别稀疏归纳，解决了传统MIL在处理弱标注复杂数据集时实例冗余和非信息性实例丢弃的挑战，显著提升了模型的鲁棒性、可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的MIL方法（包括早期统计方法和近期基于注意力机制的框架）在处理计算病理学中的全玻片图像（WSI）等复杂、弱标注数据集时，面临实例冗余问题，并且缺乏明确机制来丢弃非信息性实例，这限制了它们的鲁棒性和可解释性。

Method: 本文提出了簇级别稀疏MIL（csMIL）框架。该方法首先对所有包进行全局聚类以建立K个簇中心，然后对每个包内进行局部聚类以分配簇标签。接着，在每个簇内计算注意力分数，并对簇权重应用稀疏正则化，从而选择性地保留诊断相关簇并丢弃不相关簇。

Result: csMIL增强了对噪声实例的鲁棒性，通过识别关键区域提高了可解释性，并降低了计算复杂性。理论分析表明，csMIL需要O(s log K)个包来恢复s个相关簇。在两个公共组织病理学基准（CAMELYON16, TCGA-NSCLC）上，csMIL取得了最先进的性能。

Conclusion: csMIL通过创新的聚类、注意力机制和簇级别稀疏性结合，有效地解决了MIL在处理复杂弱标注数据时面临的挑战，特别是在病理图像分析中，它能够更有效地识别和利用诊断相关信息，从而提升了模型的整体表现和洞察力。

Abstract: Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly
labeled datasets, such as whole-slide images (WSIs) in computational pathology,
where bags comprise unordered collections of instances with sparse diagnostic
relevance. Traditional MIL approaches, including early statistical methods and
recent attention-based frameworks, struggle with instance redundancy and lack
explicit mechanisms for discarding non-informative instances, limiting their
robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a
novel framework that integrates global-local instance clustering,
within-cluster attention, and cluster-level sparsity induction to address these
challenges. Our csMIL first performs global clustering across all bags to
establish $K$ cluster centers, followed by local clustering within each bag to
assign cluster labels. Attention scores are computed within each cluster, and
sparse regularization is applied to cluster weights, enabling the selective
retention of diagnostically relevant clusters while discarding irrelevant ones.
This approach enhances robustness to noisy instances, improves interpretability
by identifying critical regions, and reduces computational complexity.
Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to
recover $s$ relevant clusters, aligning with compressed sensing principles.
Empirically, csMIL achieves state-of-the-art performance on two public
histopathology benchmarks (CAMELYON16, TCGA-NSCLC).

</details>


### [79] [Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection](https://arxiv.org/abs/2509.11058)
*Canhui Tang,Sanping Zhou,Haoyue Shi,Le Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的零样本视频异常检测（ZS-VAD）框架，通过语言引导的语义典型性建模和测试时上下文独特性分析，充分利用骨架数据，无需目标域训练数据即可实现最先进的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 零样本视频异常检测（ZS-VAD）因数据隐私和新部署等实际需求至关重要。骨架方法在背景和人物外观方面消除了域差异，具有固有的泛化优势。然而，现有方法仅学习低级骨架表示并依赖于领域受限的正常性边界，导致其无法很好地泛化到具有不同正常和异常行为模式的新场景。

Method: 本文提出了一种通过动作典型性和独特性学习来解锁骨架数据潜力的方法。具体包括：1) 语言引导的语义典型性建模模块：将骨架片段投射到动作语义空间，并在训练期间提取大型语言模型（LLM）关于典型正常和异常行为的知识。2) 测试时上下文独特性分析模块：精细分析骨架片段之间的时空差异，从而推导出场景自适应的边界。

Result: 在不使用任何目标域训练样本的情况下，该方法在四个大型VAD数据集（ShanghaiTech、UBnormal、NWPU和UCF-Crime）上，针对100多个未见过的监控场景，超越了现有的骨架基方法，取得了最先进的成果。

Conclusion: 该论文成功提出了一种新的零样本视频异常检测框架，通过学习动作的典型性和独特性，有效克服了现有骨架方法的局限性，在无需目标域训练数据的情况下，实现了卓越的泛化能力和最先进的检测性能。

Abstract: Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing
anomalies without target domain training data, which is a crucial task due to
various practical concerns, e.g., data privacy or new surveillance deployments.
Skeleton-based approach has inherent generalizable advantages in achieving
ZS-VAD as it eliminates domain disparities both in background and human
appearance. However, existing methods only learn low-level skeleton
representation and rely on the domain-limited normality boundary, which cannot
generalize well to new scenes with different normal and abnormal behavior
patterns. In this paper, we propose a novel zero-shot video anomaly detection
framework, unlocking the potential of skeleton data via action typicality and
uniqueness learning. Firstly, we introduce a language-guided semantic
typicality modeling module that projects skeleton snippets into action semantic
space and distills LLM's knowledge of typical normal and abnormal behaviors
during training. Secondly, we propose a test-time context uniqueness analysis
module to finely analyze the spatio-temporal differences between skeleton
snippets and then derive scene-adaptive boundaries. Without using any training
samples from the target domain, our method achieves state-of-the-art results
against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,
UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.

</details>


### [80] [Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos](https://arxiv.org/abs/2509.11063)
*Xiaoyu Huang,Lauren M Maxson,Trang Nguyen,Cheng Jack Song,Yuankai Huo*

Main category: cs.CV

TL;DR: 该论文介绍了一个优化的肾脏类器官平台，用于多囊肾病（PKD）的筛选，并开发了Organoid Tracker，一个基于SAM2的图形用户界面（GUI）平台，用于自动化分析类器官显微镜视频，提取详细的定量指标，以克服手动分析的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 肾脏类器官模型在研究人类肾脏疾病机制和药物发现方面取得了显著进展，但对这些系统产生的空间-时间显微镜视频数据进行手动分析存在局限性，通常仅限于粗略分类，并遗漏了宝贵的像素级和纵向信息，这成为了研究瓶颈。

Method: 研究人员开发了Organoid Tracker，一个具有模块化插件架构的图形用户界面（GUI）平台。该平台基于前沿的视觉基础模型Segment Anything Model 2 (SAM2)，实现了空间-时间显微镜视频的零样本分割和自动化分析，无需编程专业知识。

Result: Organoid Tracker能够量化关键指标，如囊肿形成率、生长速度和形态变化，并生成全面的报告。它作为一个可扩展的开源框架，提供了改进和加速肾脏发育、PKD建模和治疗发现研究的强大解决方案。该平台已作为开源软件公开可用。

Conclusion: Organoid Tracker通过提供一个自动化、易用的平台来提取详细的定量指标，克服了肾脏类器官显微镜视频分析的瓶颈，从而显著改善和加速了肾脏发育、多囊肾病建模和治疗发现领域的研究。

Abstract: Recent advances in organoid models have revolutionized the study of human
kidney disease mechanisms and drug discovery by enabling scalable,
cost-effective research without the need for animal sacrifice. Here, we present
a kidney organoid platform optimized for efficient screening in polycystic
kidney disease (PKD). While these systems generate rich spatial-temporal
microscopy video datasets, current manual approaches to analysis remain limited
to coarse classifications (e.g., hit vs. non-hit), often missing valuable
pixel-level and longitudinal information. To help overcome this bottleneck, we
developed Organoid Tracker, a graphical user interface (GUI) platform designed
with a modular plugin architecture, which empowers researchers to extract
detailed, quantitative metrics without programming expertise. Built on the
cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid
Tracker enables zero-shot segmentation and automated analysis of
spatial-temporal microscopy videos. It quantifies key metrics such as cyst
formation rate, growth velocity, and morphological changes, while generating
comprehensive reports. By providing an extensible, open-source framework,
Organoid Tracker offers a powerful solution for improving and accelerating
research in kidney development, PKD modeling, and therapeutic discovery. The
platform is publicly available as open-source software at
https://github.com/hrlblab/OrganoidTracker.

</details>


### [81] [The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge](https://arxiv.org/abs/2509.11071)
*Jinghan Peng,Jingwen Wang,Xing Yu,Dehui Du*

Main category: cs.CV

TL;DR: 该报告介绍了一种基于视觉语言模型的方法，用于CVPR 2024自动驾驶大挑战的“语言驾驶”赛道。该方法利用DriveLM-nuScenes数据集，通过LoRA和DoRA微调增强LLaVA模型，并整合深度信息和采用思维链推理，最终在验证集排行榜上获得第一名。


<details>
  <summary>Details</summary>
Motivation: 参与CVPR 2024自动驾驶大挑战的“语言驾驶”（Driving with Language）赛道，旨在探索和展示视觉语言模型在自动驾驶场景中的应用能力。

Method: 研究方法包括：1) 使用DriveLM-nuScenes数据集进行模型训练；2) 以LLaVA模型为基础，采用LoRA和DoRA方法进行微调；3) 整合开源深度估计算法提供的深度信息，以丰富训练和推理过程；4) 在推理阶段，特别是针对多项选择和是非题，采用思维链（Chain-of-Thought）推理方法来提高准确性。

Result: 该综合方法在验证集排行榜上取得了0.7799的最高分数，位列第一。

Conclusion: 该报告所概述的综合方法成功地在CVPR 2024自动驾驶大挑战的“语言驾驶”赛道中取得了领先地位，证明了视觉语言模型结合深度信息和思维链推理的有效性。

Abstract: This report outlines our approach using vision language model systems for the
Driving with Language track of the CVPR 2024 Autonomous Grand Challenge. We
have exclusively utilized the DriveLM-nuScenes dataset for training our models.
Our systems are built on the LLaVA models, which we enhanced through
fine-tuning with the LoRA and DoRA methods. Additionally, we have integrated
depth information from open-source depth estimation models to enrich the
training and inference processes. For inference, particularly with
multiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning
approach to improve the accuracy of the results. This comprehensive methodology
enabled us to achieve a top score of 0.7799 on the validation set leaderboard,
ranking 1st on the leaderboard.

</details>


### [82] [Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation](https://arxiv.org/abs/2509.11082)
*Zongwu Xie,Kaijie Yun,Yang Liu,Yiming Ji,Han Li*

Main category: cs.CV

TL;DR: 该论文提出了一种鲁棒的多模态框架，用于预测行星探测车的通行成本图。该模型融合了相机和激光雷达数据，并使用IMU派生的标签进行自监督训练，在输入变化下表现出高度鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为行星探测车提供可靠的通行成本图预测能力，以实现自主导航。该研究旨在改进现有方法，并强调其在模拟环境、自监督标签和多模态模型方面的具体贡献。

Method: 该框架融合了相机和激光雷达数据，生成鸟瞰图（BEV）地形成本图。训练采用IMU派生标签进行自监督学习。关键更新包括基于DINOv3的图像编码器、基于FiLM的传感器融合以及结合Huber和平滑项的优化损失。通过移除图像颜色、遮挡输入和添加噪声等消融实验评估模型鲁棒性。

Result: 实验结果表明，即使在移除图像颜色、遮挡输入或添加噪声等条件下，MAE/MSE也仅有轻微变化（例如，激光雷达稀疏化时MAE从约0.0775增加到0.0915），这表明地形几何在学习到的成本中占主导地位，且模型具有高度鲁棒性。性能差异小归因于IMU标签主要反映地形几何而非语义，以及数据多样性有限。论文强调了其贡献：高保真、可复现的模拟环境；基于IMU的自监督标签管道；以及强大的多模态BEV成本图预测模型。

Conclusion: 该多模态模型在预测行星探测车通行成本图方面表现出高度鲁棒性，其中地形几何是学习成本的主要决定因素。未来的工作将侧重于领域泛化和数据集扩展。

Abstract: We present a robust multi-modal framework for predicting traversability
costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce
a bird's-eye-view (BEV) terrain costmap, trained self-supervised using
IMU-derived labels. Key updates include a DINOv3-based image encoder,
FiLM-based sensor fusion, and an optimization loss combining Huber and
smoothness terms. Experimental ablations (removing image color, occluding
inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases
from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry
dominates the learned cost and the model is highly robust. We attribute the
small performance differences to the IMU labeling primarily reflecting terrain
geometry rather than semantics and to limited data diversity. Unlike prior work
claiming large gains, we emphasize our contributions: (1) a high-fidelity,
reproducible simulation environment; (2) a self-supervised IMU-based labeling
pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss
limitations and future work such as domain generalization and dataset
expansion.

</details>


### [83] [End-to-End Visual Autonomous Parking via Control-Aided Attention](https://arxiv.org/abs/2509.11090)
*Chao Chen,Shunyu Yao,Yuanwu He,Tao Feng,Ruojing Song,Yuliang Guo,Xinyu Huang,Chenxu Wu,Ren Liu,Chen Feng*

Main category: cs.CV

TL;DR: 本文提出CAA-Policy，一个用于精确泊车的端到端模仿学习系统。它通过新颖的控制辅助注意力（CAA）机制，利用控制信号指导视觉注意力的学习，并采用自监督方式训练注意力模块，从而实现更稳定、鲁棒和可解释的策略。


<details>
  <summary>Details</summary>
Motivation: 精确泊车需要感知系统能自适应地提供精细控制所需的细节，但现有端到端学习方法在感知与控制之间缺乏有效协同。单独使用基于Transformer的自注意力机制会产生不稳定且时间不一致的空间注意力，从而影响下游策略决策的可靠性。

Method: 本文提出CAA-Policy，一个端到端模仿学习系统。核心是控制辅助注意力（CAA）机制，它允许控制信号指导视觉注意力的学习。该注意力模块以自监督方式训练，利用来自控制输出的反向传播梯度而非训练损失来引导注意力聚焦于对动作输出方差贡献大的视觉特征。为增强稳定性，CAA-Policy还整合了短时程路径点预测作为辅助任务，并引入独立的运动预测模块来鲁棒地跟踪目标泊车位。

Result: 在CARLA模拟器中的大量实验表明，CAA-Policy持续优于现有的端到端学习基线以及模块化的BEV分割+混合A*管道，在准确性、鲁棒性和可解释性方面均表现出色。

Conclusion: CAA-Policy通过控制辅助注意力机制和自监督注意力训练，有效地将感知与控制相结合，为精确泊车提供了一个更鲁棒、更具泛化能力的策略，并显著提高了性能和可解释性。

Abstract: Precise parking requires an end-to-end system where perception adaptively
provides policy-relevant details-especially in critical areas where fine
control decisions are essential. End-to-end learning offers a unified framework
by directly mapping sensor inputs to control actions, but existing approaches
lack effective synergy between perception and control. We find that
transformer-based self-attention, when used alone, tends to produce unstable
and temporally inconsistent spatial attention, which undermines the reliability
of downstream policy decisions over time. Instead, we propose CAA-Policy, an
end-to-end imitation learning system that allows control signal to guide the
learning of visual attention via a novel Control-Aided Attention (CAA)
mechanism. For the first time, we train such an attention module in a
self-supervised manner, using backpropagated gradients from the control outputs
instead of from the training loss. This strategy encourages the attention to
focus on visual features that induce high variance in action outputs, rather
than merely minimizing the training loss-a shift we demonstrate leads to a more
robust and generalizable policy. To further enhance stability, CAA-Policy
integrates short-horizon waypoint prediction as an auxiliary task, and
introduces a separately trained motion prediction module to robustly track the
target spot over time. Extensive experiments in the CARLA simulator show that
\titlevariable~consistently surpasses both the end-to-end learning baseline and
the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,
robustness, and interpretability. Code is released at
https://github.com/Joechencc/CAAPolicy.

</details>


### [84] [PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation](https://arxiv.org/abs/2509.11092)
*Zeyu Dong,Yuyang Yin,Yuqi Li,Eric Li,Hao-Xiang Guo,Yikai Wang*

Main category: cs.CV

TL;DR: 本文提出通过低秩适应（LoRA）方法，将全景视频生成视为从透视视图到全景视图的适应性问题，从而高效地生成高质量的360度全景视频。


<details>
  <summary>Details</summary>
Motivation: 生成高质量的360度全景视频是一个重大挑战，因为全景投影与传统透视投影存在根本差异。现有解决方案常引入复杂架构或大规模训练，导致效率低下和次优结果。

Method: 受LoRA在风格迁移任务中成功的启发，本文将全景视频生成视为一个从透视视图进行的适应性问题。通过理论分析，证明当LoRA的秩超过任务的自由度时，能有效建模这些投影间的转换。该方法使用LoRA高效地微调预训练的视频扩散模型。

Result: 该方法仅用约1000个视频就实现了高质量的全景生成。实验结果表明，该方法保持了正确的投影几何，并在视觉质量、左右一致性和运动多样性方面超越了以往的SOTA方法。

Conclusion: LoRA是一种有效且高效的策略，可以将预训练的透视视频模型适应于高质量的全景视频生成，克服了传统方法面临的挑战。

Abstract: Generating high-quality 360{\deg} panoramic videos remains a significant
challenge due to the fundamental differences between panoramic and traditional
perspective-view projections. While perspective videos rely on a single
viewpoint with a limited field of view, panoramic content requires rendering
the full surrounding environment, making it difficult for standard video
generation models to adapt. Existing solutions often introduce complex
architectures or large-scale training, leading to inefficiency and suboptimal
results. Motivated by the success of Low-Rank Adaptation (LoRA) in style
transfer tasks, we propose treating panoramic video generation as an adaptation
problem from perspective views. Through theoretical analysis, we demonstrate
that LoRA can effectively model the transformation between these projections
when its rank exceeds the degrees of freedom in the task. Our approach
efficiently fine-tunes a pretrained video diffusion model using only
approximately 1,000 videos while achieving high-quality panoramic generation.
Experimental results demonstrate that our method maintains proper projection
geometry and surpasses previous state-of-the-art approaches in visual quality,
left-right consistency, and motion diversity.

</details>


### [85] [SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing](https://arxiv.org/abs/2509.11093)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出了一种名为SMILE的超分辨率引导多任务学习方法用于高光谱解混，通过理论分析验证了任务亲和性和解混收敛性，并设计了新的框架以提高解混性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱解混的性能受限于低空间分辨率。直接将超分辨率与解混结合面临两大挑战：任务亲和性未经证实，以及解混收敛性无法保证。

Method: 本文提出了SMILE（超分辨率引导多任务学习高光谱解混方法）。该方法通过理论分析（关系定理和存在定理）证明了超分辨率的积极引导作用，从而验证了多任务学习的可行性和任务亲和性。此外，通过可达性定理证明了解混的最优解，从而保证了收敛性。该框架通过学习共享和特定表示，将超分辨率的积极信息泛化到解混中。

Result: 在合成数据集和真实数据集上的实验结果均证实了SMILE方法的有效性。

Conclusion: SMILE的主要贡献在于提供了渐进的理论支持，并设计了一个在超分辨率指导下进行解混的新框架，有效解决了低空间分辨率和直接集成超分辨率与解混所面临的问题。

Abstract: The performance of hyperspectral unmixing may be constrained by low spatial
resolution, which can be enhanced using super-resolution in a multitask
learning way. However, integrating super-resolution and unmixing directly may
suffer two challenges: Task affinity is not verified, and the convergence of
unmixing is not guaranteed. To address the above issues, in this paper, we
provide theoretical analysis and propose super-resolution guided multi-task
learning method for hyperspectral unmixing (SMILE). The provided theoretical
analysis validates feasibility of multitask learning way and verifies task
affinity, which consists of relationship and existence theorems by proving the
positive guidance of super-resolution. The proposed framework generalizes
positive information from super-resolution to unmixing by learning both shared
and specific representations. Moreover, to guarantee the convergence, we
provide the accessibility theorem by proving the optimal solution of unmixing.
The major contributions of SMILE include providing progressive theoretical
support, and designing a new framework for unmixing under the guidance of
super-resolution. Our experiments on both synthetic and real datasets have
substantiate the usefulness of our work.

</details>


### [86] [A Copula-Guided Temporal Dependency Method for Multitemporal Hyperspectral Images Unmixing](https://arxiv.org/abs/2509.11096)
*Ruiying Li,Bin Pan,Qiaoying Qu,Xia Xu,Zhenwei Shi*

Main category: cs.CV

TL;DR: 本文提出了一种基于Copula理论的多时相高光谱解混（MTHU）方法（Cog-TD），以显式建模和利用时间依赖性，从而更好地捕捉动态物质演变。


<details>
  <summary>Details</summary>
Motivation: 现有的多时相高光谱解混方法在建模时间依赖性方面存在局限，未能有效捕捉动态物质演变。Copula理论在显式建模依赖结构方面具有优势，因此被引入解决此问题。

Method: 本文提出Cog-TD方法，重新定义了MTHU问题，通过引入Copula理论描述时间依赖结构。构建了一个Copula引导框架来估计具有时间依赖性的动态端元和丰度，并开发了两个关键模块：Copula函数估计和时间依赖性引导。此外，还提供了理论支持，证明了估计的Copula函数的有效性以及高光谱图像中存在的时间依赖性。

Result: 在合成数据集和真实世界数据集上的实验结果表明，所提出的方法具有实用性。

Conclusion: 本文通过重新定义MTHU问题、提出Copula引导框架、开发关键模块并提供理论支持，成功地解决了MTHU中时间依赖性建模的挑战，实验证明了其有效性。

Abstract: Multitemporal hyperspectral unmixing (MTHU) aims to model variable endmembers
and dynamical abundances, which emphasizes the critical temporal information.
However, existing methods have limitations in modeling temporal dependency,
thus fail to capture the dynamical material evolution. Motivated by the ability
of copula theory in modeling dependency structure explicitly, in this paper, we
propose a copula-guided temporal dependency method (Cog-TD) for multitemporal
hyperspectral unmixing. Cog-TD defines new mathematical model, constructs
copula-guided framework and provides two key modules with theoretical support.
The mathematical model provides explicit formulations for MTHU problem
definition, which describes temporal dependency structure by incorporating
copula theory. The copula-guided framework is constructed for utilizing copula
function, which estimates dynamical endmembers and abundances with temporal
dependency. The key modules consist of copula function estimation and temporal
dependency guidance, which computes and employs temporal information to guide
unmixing process. Moreover, the theoretical support demonstrates that estimated
copula function is valid and the represented temporal dependency exists in
hyperspectral images. The major contributions of this paper include redefining
MTHU problem with temporal dependency, proposing a copula-guided framework,
developing two key modules and providing theoretical support. Our experimental
results on both synthetic and real-world datasets demonstrate the utility of
the proposed method.

</details>


### [87] [3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment](https://arxiv.org/abs/2509.11097)
*Nhut Le,Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 该研究提出了3DAeroRelief，首个专门用于灾后评估的三维基准数据集，旨在解决现有二维和三维数据在灾害场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的自然灾害分析主要依赖二维图像，缺乏深度、易受遮挡且空间上下文有限。尽管三维语义分割提供更丰富的信息，但现有三维基准数据集主要关注城市或室内场景，很少涉及受灾区域，导致灾后评估缺乏专门的三维数据支持。

Method: 研究团队使用低成本无人机（UAV）在飓风受损区域收集数据，通过运动结构（SfM）和多视图立体（MVS）技术重建密集的3D点云。语义标注通过手动进行2D标记并投影到3D空间来生成。最后，他们评估了多个最先进的3D分割模型在该数据集上的性能，以展示其实用性。

Result: 3DAeroRelief是首个专门为灾后评估设计的3D基准数据集，它捕获了具有细粒度结构损伤的真实世界灾害背景下的大规模室外3D环境。无人机的使用使得在危险区域进行经济、灵活和安全的数据收集成为可能。对现有3D分割模型的评估突出了灾害响应中3D场景理解的挑战和机遇。

Conclusion: 3DAeroRelief数据集是一个宝贵的资源，有助于推动在真实世界灾后场景中鲁棒3D视觉系统的发展。

Abstract: Timely assessment of structural damage is critical for disaster response and
recovery. However, most prior work in natural disaster analysis relies on 2D
imagery, which lacks depth, suffers from occlusions, and provides limited
spatial context. 3D semantic segmentation offers a richer alternative, but
existing 3D benchmarks focus mainly on urban or indoor scenes, with little
attention to disaster-affected areas. To address this gap, we present
3DAeroRelief--the first 3D benchmark dataset specifically designed for
post-disaster assessment. Collected using low-cost unmanned aerial vehicles
(UAVs) over hurricane-damaged regions, the dataset features dense 3D point
clouds reconstructed via Structure-from-Motion and Multi-View Stereo
techniques. Semantic annotations were produced through manual 2D labeling and
projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D
large-scale outdoor environments with fine-grained structural damage in
real-world disaster contexts. UAVs enable affordable, flexible, and safe data
collection in hazardous areas, making them particularly well-suited for
emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate
several state-of-the-art 3D segmentation models on the dataset to highlight
both the challenges and opportunities of 3D scene understanding in disaster
response. Our dataset serves as a valuable resource for advancing robust 3D
vision systems in real-world applications for post-disaster scenarios.

</details>


### [88] [Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.11102)
*Nhi Kieu,Kien Nguyen,Arnold Wiliem,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 针对遥感语义分割中多模态数据缺失的问题，本文提出了一种新型的生成增强多模态学习网络GEMMNet，通过混合特征提取、多尺度融合和互补损失机制，有效克服了现有生成模型的局限性，显著提升了模型在数据缺失场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在各种领域表现优异，但在真实世界中，由于传感器故障或恶劣天气，多模态信号容易缺失，严重影响模型性能。尽管自编码器（AE）和生成对抗网络（GAN）等生成模型可用于重建缺失模态，但它们在遥感语义分割中的有效性尚未充分探索。现有方法在处理遥感数据异构性、捕获复杂场景语义上下文以及避免主导模态偏差方面存在局限。

Method: 本文提出了一种生成增强多模态学习网络（GEMMNet），包含三个关键组件：(1) 混合特征提取器（HyFEx），用于有效学习模态特定表示；(2) 具有多尺度感知的混合融合（HyFMA），用于捕获跨尺度的模态协同语义上下文；(3) 互补损失（CoLoss）方案，通过鼓励跨模态和跨任务的一致性来减轻固有的偏差。

Result: GEMMNet在两个具有挑战性的遥感语义分割数据集（Vaihingen和Potsdam）上，性能优于生成基线（AE、cGAN）以及最先进的非生成方法（mmformer和shaspec）。

Conclusion: GEMMNet通过其新颖的组件设计，成功解决了现有生成方法在处理遥感多模态数据异构性、语义上下文捕获和模态偏差方面的局限性，在缺失模态条件下，为遥感语义分割提供了更鲁棒和高性能的解决方案。

Abstract: Multimodal learning has shown significant performance boost compared to
ordinary unimodal models across various domains. However, in real-world
scenarios, multimodal signals are susceptible to missing because of sensor
failures and adverse weather conditions, which drastically deteriorates models'
operation and performance. Generative models such as AutoEncoder (AE) and
Generative Adversarial Network (GAN) are intuitive solutions aiming to
reconstruct missing modality from available ones. Yet, their efficacy in remote
sensing semantic segmentation remains underexplored. In this paper, we first
examine the limitations of existing generative approaches in handling the
heterogeneity of multimodal remote sensing data. They inadequately capture
semantic context in complex scenes with large intra-class and small inter-class
variation. In addition, traditional generative models are susceptible to heavy
dependence on the dominant modality, introducing bias that affects model
robustness under missing modality conditions. To tackle these limitations, we
propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with
three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn
modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness
(HyFMA) to capture modality-synergistic semantic context across scales and (3)
Complementary Loss (CoLoss) scheme to alleviate the inherent bias by
encouraging consistency across modalities and tasks. Our method, GEMMNet,
outperforms both generative baselines AE, cGAN (conditional GAN), and
state-of-the-art non-generative approaches - mmformer and shaspec - on two
challenging semantic segmentation remote sensing datasets (Vaihingen and
Potsdam). Source code is made available.

</details>


### [89] [WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild](https://arxiv.org/abs/2509.11114)
*Yuqiu Liu,Jialin Song,Manolis Savva,Wuyang Chen*

Main category: cs.CV

TL;DR: 该论文提出一个从单视角野外视频中提取和重建动态3D烟雾资产的流水线，并支持交互式烟雾设计和编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管3D视觉在流体动力学重建和渲染方面取得了进展，但当前流体重建仍严重依赖受控的实验室环境。野外捕获的真实世界视频中的烟雾重建仍未得到充分探索。

Method: 设计了一个解决三个关键挑战的流水线：1) 通过背景去除进行烟雾提取；2) 烟雾粒子和相机姿态的初始化；3) 推断多视角视频。这些技术旨在克服真实世界视频中的重建难题。

Result: 该方法在野外视频上的烟雾重建质量优于现有方法（平均PSNR提高2.22），并且通过模拟烟雾资产，实现了多样化和逼真的流体动力学编辑。

Conclusion: 该研究提供了一种从单视角野外视频中高效重建动态3D烟雾并进行交互式设计和编辑的强大方法，显著提升了真实世界流体重建的能力。

Abstract: We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from
a single in-the-wild video, and further integrate interactive simulation for
smoke design and editing. Recent developments in 3D vision have significantly
improved reconstructing and rendering fluid dynamics, supporting realistic and
temporally consistent view synthesis. However, current fluid reconstructions
rely heavily on carefully controlled clean lab environments, whereas real-world
videos captured in the wild are largely underexplored. We pinpoint three key
challenges of reconstructing smoke in real-world videos and design targeted
techniques, including smoke extraction with background removal, initialization
of smoke particles and camera poses, and inferring multi-view videos. Our
method not only outperforms previous reconstruction and generation methods with
high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but
also enables diverse and realistic editing of fluid dynamics by simulating our
smoke assets. We provide our models, data, and 4D smoke assets at
[https://autumnyq.github.io/WildSmoke](https://autumnyq.github.io/WildSmoke).

</details>


### [90] [SVR-GS: Spatially Variant Regularization for Probabilistic Masks in 3D Gaussian Splatting](https://arxiv.org/abs/2509.11116)
*Ashkan Taghipour,Vahid Naghshin,Benjamin Southwell,Farid Boussaid,Hamid Laga,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 本文提出SVR-GS，一种空间变异正则化器，用于优化3D Gaussian Splatting (3DGS) 模型中的高斯数量。它通过在每个高斯对射线的有效贡献上应用稀疏性压力，显著减少了高斯数量，同时保持了高图像质量，从而实现了更小、更快、更节省内存的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS剪枝方法（如MaskGS）通过正则化掩码的全局平均值来优化高斯数量，但这与决定图像质量的局部逐像素（逐射线）重建损失不一致。这种错位导致了对低重要性高斯施加稀疏性压力的效率低下，未能有效减少高斯数量。

Method: 本文引入了SVR-GS（Spatially Variant Regularizer for Gaussian Splatting）。它通过渲染一个逐像素空间掩码，该掩码基于每个高斯沿射线的有效贡献，从而在关键的低重要性高斯上施加稀疏性压力。研究探索了三种空间掩码聚合策略，并在CUDA中实现，同时进行了梯度分析以指导最终设计。

Result: 在Tanks&Temples、Deep Blending和Mip-NeRF360数据集上的广泛实验表明，SVR-GS平均而言将高斯数量比MaskGS减少了1.79倍，比3DGS减少了5.63倍。同时，PSNR仅分别下降了0.50 dB和0.40 dB。这些改进带来了显著更小、更快、更节省内存的模型。

Conclusion: SVR-GS通过其空间变异正则化器，有效地解决了现有3DGS剪枝方法的局限性，实现了高斯数量的大幅削减，同时保持了高视觉质量。这些成果使得模型更适合机器人、AR/VR和移动感知等实时应用。

Abstract: 3D Gaussian Splatting (3DGS) enables fast, high-quality novel view synthesis
but typically relies on densification followed by pruning to optimize the
number of Gaussians. Existing mask-based pruning, such as MaskGS, regularizes
the global mean of the mask, which is misaligned with the local per-pixel
(per-ray) reconstruction loss that determines image quality along individual
camera rays. This paper introduces SVR-GS, a spatially variant regularizer that
renders a per-pixel spatial mask from each Gaussian's effective contribution
along the ray, thereby applying sparsity pressure where it matters: on
low-importance Gaussians. We explore three spatial-mask aggregation strategies,
implement them in CUDA, and conduct a gradient analysis to motivate our final
design. Extensive experiments on Tanks\&Temples, Deep Blending, and Mip-NeRF360
datasets demonstrate that, on average across the three datasets, the proposed
SVR-GS reduces the number of Gaussians by 1.79\(\times\) compared to MaskGS and
5.63\(\times\) compared to 3DGS, while incurring only 0.50 dB and 0.40 dB PSNR
drops, respectively. These gains translate into significantly smaller, faster,
and more memory-efficient models, making them well-suited for real-time
applications such as robotics, AR/VR, and mobile perception.

</details>


### [91] [No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images](https://arxiv.org/abs/2509.11164)
*Diego Eustachio Farchione,Ramzi Idoughi,Peter Wonka*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖、轻量级且可扩展的学习框架，能够从多视角2D RGB图像预测珊瑚状物体的3D体积和表面积，并提供置信度估计，以解决珊瑚形态复杂性带来的测量挑战。


<details>
  <summary>Details</summary>
Motivation: 有效的珊瑚礁监测需要通过准确估计珊瑚的体积和表面积来量化其生长，但由于珊瑚形态复杂，这是一项艰巨的任务。

Method: 该方法利用预训练模块（VGGT）从每个视角提取密集点图，将这些点图合并为统一的点云并丰富每视角的置信度分数。然后，将生成的点云输入到两个并行的DGCNN解码器头，共同输出珊瑚的体积、表面积及其相应的置信度估计。为提高预测稳定性和提供不确定性估计，引入了基于高斯负对数似然的复合损失函数（在实数域和对数域）。

Result: 该方法取得了具有竞争力的准确性，并能很好地泛化到未见过的珊瑚形态。

Conclusion: 该框架为直接从稀疏图像集高效、可扩展地估计珊瑚几何形状铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在应用。

Abstract: Effective reef monitoring requires the quantification of coral growth via
accurate volumetric and surface area estimates, which is a challenging task due
to the complex morphology of corals. We propose a novel, lightweight, and
scalable learning framework that addresses this challenge by predicting the 3D
volume and surface area of coral-like objects from 2D multi-view RGB images.
Our approach utilizes a pre-trained module (VGGT) to extract dense point maps
from each view; these maps are merged into a unified point cloud and enriched
with per-view confidence scores. The resulting cloud is fed to two parallel
DGCNN decoder heads, which jointly output the volume and the surface area of
the coral, as well as their corresponding confidence estimate. To enhance
prediction stability and provide uncertainty estimates, we introduce a
composite loss function based on Gaussian negative log-likelihood in both real
and log domains. Our method achieves competitive accuracy and generalizes well
to unseen morphologies. This framework paves the way for efficient and scalable
coral geometry estimation directly from a sparse set of images, with potential
applications in coral growth analysis and reef monitoring.

</details>


### [92] [Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic](https://arxiv.org/abs/2509.11165)
*Waikit Xiu,Qiang Lu,Xiying Li,Chen Hu,Shengbo Sun*

Main category: cs.CV

TL;DR: 本文提出Traffic-MLLM，一个专为细粒度交通分析设计的多模态大语言模型，通过轻量级微调和知识提示模块，显著提升了交通视频理解中的时空因果建模和领域知识整合能力，并在基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的交通视频理解方法在准确建模时空因果关系和整合领域特定知识方面面临挑战，限制了它们在复杂场景中的有效性，因此需要一种更强大的模型来解决这些局限性。

Method: 研究者提出了Traffic-MLLM模型，它基于Qwen2.5-VL骨干网络，利用高质量的交通专用多模态数据集，并通过低秩适应（LoRA）进行轻量级微调，以增强其对视频序列中连续时空特征的建模能力。此外，还引入了一个创新的知识提示模块，融合了思维链（CoT）推理和检索增强生成（RAG），以精确地将详细的交通法规和领域知识注入推理过程，从而提升模型的逻辑推理和知识适应能力。

Result: 实验结果表明，Traffic-MLLM在TrafficQA和DriveQA基准测试中均取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。该模型还展现出显著的零样本推理和跨场景泛化能力。

Conclusion: Traffic-MLLM通过其定制的多模态大语言模型架构、高效的微调策略和创新的知识提示模块，成功解决了交通视频理解中的关键挑战，实现了在复杂交通场景中优越的时空因果分析和领域知识整合能力，并达到了行业领先的性能和泛化能力。

Abstract: As intelligent transportation systems advance, traffic video understanding
plays an increasingly pivotal role in comprehensive scene perception and causal
analysis. Yet, existing approaches face notable challenges in accurately
modeling spatiotemporal causality and integrating domain-specific knowledge,
limiting their effectiveness in complex scenarios. To address these
limitations, we propose Traffic-MLLM, a multimodal large language model
tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,
our model leverages high-quality traffic-specific multimodal datasets and uses
Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing
its capacity to model continuous spatiotemporal features in video sequences.
Furthermore, we introduce an innovative knowledge prompting module fusing
Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),
enabling precise injection of detailed traffic regulations and domain knowledge
into the inference process. This design markedly boosts the model's logical
reasoning and knowledge adaptation capabilities. Experimental results on
TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art
performance, validating its superior ability to process multimodal traffic
data. It also exhibits remarkable zero-shot reasoning and cross-scenario
generalization capabilities.

</details>


### [93] [Multispectral-NeRF:a multispectral modeling approach based on neural radiance fields](https://arxiv.org/abs/2509.11169)
*Hong Zhang,Fei Guo,Zihan Xie,Dizhao Yao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Multispectral-NeRF的增强型NeRF模型，旨在有效整合多光谱信息，解决现有方法在多光谱3D重建中面临的精度和特征问题，并成功处理多波段光谱特征。


<details>
  <summary>Details</summary>
Motivation: 传统的基于2D图像的3D重建技术主要依赖RGB信息，而随着传感器技术发展，更多光谱波段被纳入。现有整合扩展光谱数据的方法存在成本高昂、精度低和几何特征差等问题。虽然NeRF能有效解决当前多光谱3D重建的问题，但NeRF及其改进模型（如NeRFacto）目前仅针对三波段数据进行训练，无法充分利用多波段信息。

Method: 本文提出了Multispectral-NeRF模型，它是NeRF的一种增强型神经架构，旨在有效整合多光谱信息。技术贡献包括三方面修改：1. 扩展隐藏层维度以适应6波段光谱输入；2. 重新设计残差函数以优化重建图像与参考图像之间的光谱差异计算；3. 调整数据压缩模块以满足多光谱图像增加的位深要求。

Result: 实验结果证实，Multispectral-NeRF成功处理了多波段光谱特征，同时准确保留了原始场景的光谱特性。

Conclusion: Multispectral-NeRF模型能够有效整合多光谱信息，克服了现有NeRF模型无法处理多波段数据的局限性，实现了高精度和高质量的多光谱3D重建。

Abstract: 3D reconstruction technology generates three-dimensional representations of
real-world objects, scenes, or environments using sensor data such as 2D
images, with extensive applications in robotics, autonomous vehicles, and
virtual reality systems. Traditional 3D reconstruction techniques based on 2D
images typically relies on RGB spectral information. With advances in sensor
technology, additional spectral bands beyond RGB have been increasingly
incorporated into 3D reconstruction workflows. Existing methods that integrate
these expanded spectral data often suffer from expensive scheme prices, low
accuracy and poor geometric features. Three - dimensional reconstruction based
on NeRF can effectively address the various issues in current multispectral 3D
reconstruction methods, producing high - precision and high - quality
reconstruction results. However, currently, NeRF and some improved models such
as NeRFacto are trained on three - band data and cannot take into account the
multi - band information. To address this problem, we propose
Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can
effectively integrates multispectral information. Our technical contributions
comprise threefold modifications: Expanding hidden layer dimensionality to
accommodate 6-band spectral inputs; Redesigning residual functions to optimize
spectral discrepancy calculations between reconstructed and reference images;
Adapting data compression modules to address the increased bit-depth
requirements of multispectral imagery. Experimental results confirm that
Multispectral-NeRF successfully processes multi-band spectral features while
accurately preserving the original scenes' spectral characteristics.

</details>


### [94] [SPHERE: Semantic-PHysical Engaged REpresentation for 3D Semantic Scene Completion](https://arxiv.org/abs/2509.11171)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出SPHERE方法，将体素和高斯表示融合，用于基于摄像头的3D语义场景补全（SSC），旨在同时捕捉语义信息和物理细节，提高大规模场景的感知精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素和平面的SSC方法难以捕捉真实的几何细节，而NeRF和3DGS等神经重建方法虽然物理感知能力强，但在处理大规模自动驾驶场景时计算成本高、收敛慢，导致语义精度不足。

Method: SPHERE方法整合了体素和高斯表示。首先，语义引导高斯初始化（SGI）模块利用双分支3D场景表示定位焦点体素作为锚点，指导高效的高斯初始化。其次，物理感知谐波增强（PHE）模块引入语义球谐函数来建模物理感知的上下文细节，并通过焦点分布对齐促进语义-几何一致性。

Result: SPHERE方法生成的SSC结果具有逼真的细节。在流行的SemanticKITTI和SSCBench-KITTI-360基准测试上进行了广泛的实验和分析，验证了SPHERE的有效性。

Conclusion: SPHERE通过联合利用语义和物理信息，有效解决了现有SSC方法在几何细节和计算效率方面的不足，显著提升了基于摄像头的3D语义场景补全任务的性能和真实感。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task in
autonomous driving systems, assessing voxel-level geometry and semantics for
holistic scene perception. While existing voxel-based and plane-based SSC
methods have achieved considerable progress, they struggle to capture physical
regularities for realistic geometric details. On the other hand, neural
reconstruction methods like NeRF and 3DGS demonstrate superior physical
awareness, but suffer from high computational cost and slow convergence when
handling large-scale, complex autonomous driving scenes, leading to inferior
semantic accuracy. To address these issues, we propose the Semantic-PHysical
Engaged REpresentation (SPHERE) for camera-based SSC, which integrates voxel
and Gaussian representations for joint exploitation of semantic and physical
information. First, the Semantic-guided Gaussian Initialization (SGI) module
leverages dual-branch 3D scene representations to locate focal voxels as
anchors to guide efficient Gaussian initialization. Then, the Physical-aware
Harmonics Enhancement (PHE) module incorporates semantic spherical harmonics to
model physical-aware contextual details and promote semantic-geometry
consistency through focal distribution alignment, generating SSC results with
realistic details. Extensive experiments and analyses on the popular
SemanticKITTI and SSCBench-KITTI-360 benchmarks validate the effectiveness of
SPHERE. The code is available at
https://github.com/PKU-ICST-MIPL/SPHERE_ACMMM2025.

</details>


### [95] [StegOT: Trade-offs in Steganography via Optimal Transport](https://arxiv.org/abs/2509.11178)
*Chengde Lin,Xuezhu Gong,Shuxue Ding,Mingzhe Yang,Xijun Lu,Chengjun Mo*

Main category: cs.CV

TL;DR: 本文提出StegOT，一种基于自编码器和最优传输理论的隐写模型，通过多通道最优传输（MCOT）模块解决现有模型中的模式崩溃问题，实现封面图像与秘密图像之间的信息权衡，并提升隐写图像和恢复图像的质量。


<details>
  <summary>Details</summary>
Motivation: 许多基于GAN和VAE的隐写模型存在模式崩溃问题，导致隐写图像中封面图像和秘密图像之间的信息不平衡，进而影响后续提取。

Method: 提出StegOT模型，一个基于自编码器的隐写模型，融合了最优传输理论。设计了多通道最优传输（MCOT）模块，将多峰值的特征分布转换为单峰值，以实现信息权衡。

Result: 实验证明，该模型不仅实现了封面图像和秘密图像之间的信息权衡，还提升了隐写图像和恢复图像的质量。

Conclusion: 通过引入最优传输理论（特别是MCOT模块），StegOT模型成功解决了隐写术中的模式崩溃挑战，平衡了信息，并显著改善了隐写图像和恢复图像的质量。

Abstract: Image hiding is often referred to as steganography, which aims to hide a
secret image in a cover image of the same resolution. Many steganography models
are based on genera-tive adversarial networks (GANs) and variational
autoencoders (VAEs). However, most existing models suffer from mode collapse.
Mode collapse will lead to an information imbalance between the cover and
secret images in the stego image and further affect the subsequent extraction.
To address these challenges, this paper proposes StegOT, an autoencoder-based
steganography model incorporating optimal transport theory. We designed the
multiple channel optimal transport (MCOT) module to transform the feature
distribution, which exhibits multiple peaks, into a single peak to achieve the
trade-off of information. Experiments demonstrate that we not only achieve a
trade-off between the cover and secret images but also enhance the quality of
both the stego and recovery images. The source code will be released on
https://github.com/Rss1124/StegOT.

</details>


### [96] [The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models](https://arxiv.org/abs/2509.11184)
*Partha Shah,Durva Sankhe,Maariyah Rashid,Zakaa Khaled,Esther Puyol-Antón,Tiarna Lee,Maram Alqarni,Sweta Rai,Andrew P. King*

Main category: cs.CV

TL;DR: 本文研究了 Fitzpatrick 皮肤类型 (FST) 量表的粒度对 AI 皮肤病变分类模型性能和偏差的影响，并建议转向更具代表性的替代量表。


<details>
  <summary>Details</summary>
Motivation: AI 皮肤病变分类模型表现出皮肤色调偏见。常用的 FST 量表因其在浅肤色类别中具有更高粒度而受到批评，这促使研究其粒度对模型性能和偏差的影响。

Method: 通过使用不同粒度的 FST 特定数据训练多个 AI 模型来分类良性与恶性病变，并比较它们的性能和偏差。

Result: (i) 当使用基于三个组（FST 1/2、3/4 和 5/6）的 FST 特定数据训练模型时，与使用 FST 平衡数据训练的通用模型相比，FST 特定数据的模型性能通常更好；(ii) 降低 FST 量表信息的粒度（从 1/2 和 3/4 降低到 1/2/3/4）可能会对性能产生不利影响。

Conclusion: FST 分组的粒度对于病变分类模型的训练至关重要。鉴于 FST 量表类别选择中可能存在人为偏见，本文提供证据支持公平 AI 研究应放弃 FST 量表，转而采用更能代表人类皮肤色调多样性的替代量表。

Abstract: Artificial intelligence (AI) models to automatically classify skin lesions
from dermatology images have shown promising performance but also
susceptibility to bias by skin tone. The most common way of representing skin
tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has
been criticised for having greater granularity in its skin tone categories for
lighter-skinned subjects. This paper conducts an investigation of the impact
(on performance and bias) on AI classification models of granularity in the FST
scale. By training multiple AI models to classify benign vs. malignant lesions
using FST-specific data of differing granularity, we show that: (i) when
training models using FST-specific data based on three groups (FST 1/2, 3/4 and
5/6), performance is generally better for models trained on FST-specific data
compared to a general model trained on FST-balanced data; (ii) reducing the
granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a
detrimental effect on performance. Our results highlight the importance of the
granularity of FST groups when training lesion classification models. Given the
question marks over possible human biases in the choice of categories in the
FST scale, this paper provides evidence for a move away from the FST scale in
fair AI research and a transition to an alternative scale that better
represents the diversity of human skin tones.

</details>


### [97] [Scaling Up Forest Vision with Synthetic Data](https://arxiv.org/abs/2509.11201)
*Yihang She,Andrew Blake,David Coomes,Srinivasan Keshav*

Main category: cs.CV

TL;DR: 本文提出了一种利用合成数据进行预训练，然后用少量真实数据微调的方法，以解决3D森林激光扫描中树木分割数据集不足的问题，并取得了与全量真实数据训练模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 准确的树木分割是理解碳循环等生态系统功能的重要一步，但现有的公共3D森林数据集规模不足以构建鲁棒的树木分割系统。鉴于真实数据采集和标注成本高昂，且合成数据在自动驾驶等领域已取得成功，研究者探索了合成数据在树木分割中的应用潜力。

Method: 研究开发了一个新的合成数据生成管道，该管道结合了游戏引擎的最新进展和基于物理的LiDAR仿真。利用此管道，生成了一个前所未有规模的、全面且多样化的标注3D森林数据集。然后，使用一个最先进的树木分割算法，先在合成数据上进行预训练，再用极少量的真实森林样地标注数据进行微调。

Result: 实验结果表明，合成数据可以显著减少对标记真实数据的需求。经过不到0.1公顷的单个真实森林样地微调后，预训练模型实现了与在完整规模真实数据上训练的模型具有竞争力的分割效果。研究还确定了成功使用合成数据的关键因素：物理准确性、多样性和规模。

Conclusion: 合成数据预训练结合少量真实数据微调是构建鲁棒3D森林视觉系统的一种有效策略，能够大幅降低对昂贵真实标注数据的依赖，为未来的森林视觉系统铺平了道路。研究还提供了数据生成管道和数据集供公开使用。

Abstract: Accurate tree segmentation is a key step in extracting individual tree
metrics from forest laser scans, and is essential to understanding ecosystem
functions in carbon cycling and beyond. Over the past decade, tree segmentation
algorithms have advanced rapidly due to developments in AI. However existing,
public, 3D forest datasets are not large enough to build robust tree
segmentation systems. Motivated by the success of synthetic data in other
domains such as self-driving, we investigate whether similar approaches can
help with tree segmentation. In place of expensive field data collection and
annotation, we use synthetic data during pretraining, and then require only
minimal, real forest plot annotation for fine-tuning.
  We have developed a new synthetic data generation pipeline to do this for
forest vision tasks, integrating advances in game-engines with physics-based
LiDAR simulation. As a result, we have produced a comprehensive, diverse,
annotated 3D forest dataset on an unprecedented scale. Extensive experiments
with a state-of-the-art tree segmentation algorithm and a popular real dataset
show that our synthetic data can substantially reduce the need for labelled
real data. After fine-tuning on just a single, real, forest plot of less than
0.1 hectare, the pretrained model achieves segmentations that are competitive
with a model trained on the full scale real data. We have also identified
critical factors for successful use of synthetic data: physics, diversity, and
scale, paving the way for more robust 3D forest vision systems in the future.
Our data generation pipeline and the resulting dataset are available at
https://github.com/yihshe/CAMP3D.git.

</details>


### [98] [Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation](https://arxiv.org/abs/2509.11213)
*Yufei Tang,Daiheng Gao,Pingyu Wu,Wenbo Zhou,Bang Zhang,Weiming Zhang*

Main category: cs.CV

TL;DR: 本文提出“Beyond Sliders”框架，结合GAN和扩散模型，通过细粒度的文本和视觉对抗性指导，显著提升了图像（尤其是非AIGC和真实世界图像）的质量和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法（如概念滑块）在真实世界或非AIGC图像上表现不佳，难以满足对真实感和定制化的需求。

Method: 引入“Beyond Sliders”框架，该框架整合了生成对抗网络（GANs）和扩散模型。它通过对抗性的方式，利用细粒度的文本和视觉指导来优化图像，从而提升图像质量。

Result: 实验验证表明，“Beyond Sliders”在图像质量和真实感方面取得了显著提升，并且在多种应用中展现出鲁棒性和多功能性。

Conclusion: “Beyond Sliders”是一个创新的框架，能够实现跨多样图像类别的复杂图像操作，尤其擅长处理真实世界图像，并显著提高图像质量和真实感。

Abstract: In the realm of image generation, the quest for realism and customization has
never been more pressing. While existing methods like concept sliders have made
strides, they often falter when it comes to no-AIGC images, particularly images
captured in real world settings. To bridge this gap, we introduce Beyond
Sliders, an innovative framework that integrates GANs and diffusion models to
facilitate sophisticated image manipulation across diverse image categories.
Improved upon concept sliders, our method refines the image through fine
grained guidance both textual and visual in an adversarial manner, leading to a
marked enhancement in image quality and realism. Extensive experimental
validation confirms the robustness and versatility of Beyond Sliders across a
spectrum of applications.

</details>


### [99] [Geometrically Constrained and Token-Based Probabilistic Spatial Transformers](https://arxiv.org/abs/2509.11218)
*Johann Schmidt,Sebastian Stober*

Main category: cs.CV

TL;DR: 本文提出了一种概率的、分量分解的STN（空间变换网络）扩展，通过将仿射变换分解并建模不确定性，以提高细粒度视觉分类中对几何变化的鲁棒性，并在蛾类分类基准上取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类（FGVC）对几何变化（如任意方向、尺度和透视畸变）高度敏感。现有等变架构计算成本高且限制了假设空间。STN作为一种规范化工具虽然灵活，但其鲁棒性仍有待提高。

Method: 本文重新审视STN作为基于Transformer的视觉流水线的规范化工具，并提出了一种概率的、分量分解的扩展方法。具体而言，将仿射变换分解为旋转、缩放和剪切，并使用共享的定位编码器在几何约束下回归每个分量。通过高斯变分后验建模每个分量以捕获不确定性，并在推理时进行基于采样的规范化。此外，引入了一种新的分量对齐损失，利用增强参数来指导空间对齐。

Result: 在具有挑战性的蛾类分类基准上进行的实验表明，与现有其他STN方法相比，本文提出的方法能持续提高鲁棒性。

Conclusion: 所提出的概率的、分量分解的STN扩展有效地解决了FGVC中几何变异性问题，提供了更高的鲁棒性，且避免了等变架构的缺点。

Abstract: Fine-grained visual classification (FGVC) remains highly sensitive to
geometric variability, where objects appear under arbitrary orientations,
scales, and perspective distortions. While equivariant architectures address
this issue, they typically require substantial computational resources and
restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs)
as a canonicalization tool for transformer-based vision pipelines, emphasizing
their flexibility, backbone-agnostic nature, and lack of architectural
constraints. We propose a probabilistic, component-wise extension that improves
robustness. Specifically, we decompose affine transformations into rotation,
scaling, and shearing, and regress each component under geometric constraints
using a shared localization encoder. To capture uncertainty, we model each
component with a Gaussian variational posterior and perform sampling-based
canonicalization during inference.A novel component-wise alignment loss
leverages augmentation parameters to guide spatial alignment. Experiments on
challenging moth classification benchmarks demonstrate that our method
consistently improves robustness compared to other STNs.

</details>


### [100] [CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning](https://arxiv.org/abs/2509.11219)
*Rabin Dulal,Lihong Zheng,Ashad Kabir*

Main category: cs.CV

TL;DR: 本文提出了一种基于协作模型不可知元学习（CCoMAML）和多头注意力特征融合（MHAFF）的少样本学习框架，用于实时牛只鼻纹识别，克服了传统深度学习模型在数据受限和频繁重训练方面的挑战，并取得了卓越的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前的RFID牛只识别系统易受丢失、损坏、篡改和攻击。虽然基于牛只鼻纹的生物识别是一种有前景的替代方案，但深度学习模型面临数据有限、数据收集中断以及动态牛群需要频繁重训练等挑战。

Method: 提出了一种新颖的少样本学习框架，用于实时牛只识别。该框架结合了协作模型不可知元学习（CCoMAML）和多头注意力特征融合（MHAFF）作为特征提取器。该模型通过从少量数据样本中高效学习，实现对新数据的良好适应性，无需重新训练。

Result: 通过与现有最先进的少样本学习技术进行严格评估，所提出的CCoMAML结合MHAFF在牛只识别方面表现出卓越的性能，F1分数分别达到98.46%和97.91%。

Conclusion: 所提出的CCoMAML结合MHAFF模型在牛只识别方面表现出优越的性能，能够有效地从少量数据样本中学习并适应新数据，解决了深度学习模型在数据限制和重训练方面的挑战。

Abstract: Cattle identification is critical for efficient livestock farming management,
currently reliant on radio-frequency identification (RFID) ear tags. However,
RFID-based systems are prone to failure due to loss, damage, tampering, and
vulnerability to external attacks. As a robust alternative, biometric
identification using cattle muzzle patterns similar to human fingerprints has
emerged as a promising solution. Deep learning techniques have demonstrated
success in leveraging these unique patterns for accurate identification. But
deep learning models face significant challenges, including limited data
availability, disruptions during data collection, and dynamic herd compositions
that require frequent model retraining. To address these limitations, this
paper proposes a novel few-shot learning framework for real-time cattle
identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with
Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This
model offers great model adaptability to new data through efficient learning
from few data samples without retraining. The proposed approach has been
rigorously evaluated against current state-of-the-art few-shot learning
techniques applied in cattle identification. Comprehensive experimental results
demonstrate that our proposed CCoMAML with MHAFF has superior cattle
identification performance with 98.46% and 97.91% F1 scores.

</details>


### [101] [ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification](https://arxiv.org/abs/2509.11220)
*Gao Yu Lee,Tanmoy Dam,Md Meftahul Ferdaus,Daniel Puiu Poenar,Vu N. Duong*

Main category: cs.CV

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Few-Shot Learning (FSL), which involves learning to generalize using only a
few data samples, has demonstrated promising and superior performances to
ordinary CNN methods. While Bayesian based estimation approaches using
Kullback-Leibler (KL) divergence have shown improvements, they remain
vulnerable to adversarial attacks and natural noises. We introduce
ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation
Network that significantly advances the state-of-the-art in FSL robustness and
performance. Our approach implements an adversarially and naturally robust
Hellinger distance-based feature class aggregation scheme, demonstrating
resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian
noise up to $\sigma=0.30$. The network achieves substantial improvements across
benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot
scenarios on miniImageNet respectively. We introduce a novel Hellinger
Similarity contrastive loss function that generalizes cosine similarity
contrastive loss for variational few-shot inference scenarios. Our approach
also achieves superior image reconstruction quality with a FID score of 2.75,
outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive
experiments conducted on four few-shot benchmarked datasets verify that
ANROT-HELANet's combination of Hellinger distance-based feature aggregation,
attention mechanisms, and our novel loss function establishes new
state-of-the-art performance while maintaining robustness against both
adversarial and natural perturbations. Our code repository will be available at
https://github.com/GreedYLearner1146/ANROT-HELANet/tree/main.

</details>


### [102] [MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction](https://arxiv.org/abs/2509.11232)
*Seongwan Park,Jieun Woo,Siheon Yang*

Main category: cs.CV

TL;DR: 本文提出MIS-LSTM混合框架，结合CNN编码器和LSTM序列模型，利用多模态生活日志数据进行日常睡眠质量和压力预测。该框架通过不确定性感知集成UALRE进一步提升鲁棒性，并在ETRI Lifelog挑战赛数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 从多模态生活日志数据中准确预测日常睡眠质量和压力是一个复杂而重要的任务，需要有效处理连续传感器流和稀疏离散事件，并捕捉长期时间依赖性。

Method: MIS-LSTM框架首先将连续传感器流分割成N小时块并渲染为多通道图像，同时使用专用1D-CNN编码稀疏离散事件。一个卷积块注意力模块融合两种模态生成精炼的块嵌入，然后LSTM聚合这些嵌入以捕获长期时间依赖性。为增强鲁棒性，引入了UALRE（不确定性感知集成），通过高置信度个体预测覆盖低置信度多数投票。

Result: MIS-LSTM在2025 ETRI Lifelog挑战赛数据集上取得了0.615的Macro-F1分数；结合UALRE集成后，分数提高到0.647，优于LSTM、1D-CNN和CNN等基线模型。消融实验证实了多通道成像优于堆叠垂直成像、4小时块粒度的益处以及模态特定离散编码的有效性。

Conclusion: MIS-LSTM是一个有效的混合框架，结合UALRE不确定性感知集成，能够从多模态生活日志数据中准确预测睡眠质量和压力。其关键组件，如多通道成像、4小时块粒度和模态特定离散编码，对模型性能提升至关重要。

Abstract: This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with
an LSTM sequence model for sleep quality and stress prediction at the day level
from multimodal lifelog data. Continuous sensor streams are first partitioned
into N-hour blocks and rendered as multi-channel images, while sparse discrete
events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention
Module fuses the two modalities into refined block embeddings, which an LSTM
then aggregates to capture long-range temporal dependencies. To further boost
robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides
lowconfidence majority votes with high-confidence individual predictions.
Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base
MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to
0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm
(i) the superiority of multi-channel over stacked-vertical imaging, (ii) the
benefit of a 4-hour block granularity, and (iii) the efficacy of
modality-specific discrete encoding.

</details>


### [103] [Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States](https://arxiv.org/abs/2509.11247)
*Robert Long,Rongxin Jiang,Mingrui Yan*

Main category: cs.CV

TL;DR: 针对人员重识别（ReID）在服装变化（CCReID）和持续学习（LReID）中的挑战，本文提出了LReID-Hybrid任务和基于CLIP的CMLReID框架。该框架通过上下文感知语义提示（CASP）和自适应知识融合与投影（AKFP）模块，解决了表示不匹配和遗忘问题，实现了在同衣物（SC）和换衣（CC）设置下的持续学习，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有ReID方法主要针对同衣物（SC）场景，或将换衣ReID（CCReID）视为独立子问题，缺乏在持续学习（LReID）设置下同时处理SC和CC的能力。在任务间存在表示不匹配和遗忘问题，限制了模型在复杂真实监控系统中的应用。

Method: 本文提出了LReID-Hybrid任务，旨在持续学习设置下同时实现SC和CC ReID。为此，开发了CMLReID框架，该框架基于CLIP，包含两个核心模块：1) 上下文感知语义提示（CASP），用于生成自适应提示并融合上下文，以对齐多粒度视觉线索与语义文本空间；2) 自适应知识融合与投影（AKFP），通过双路径学习器和服装状态感知投影损失（Clothing-State-Aware Projection Loss）来生成鲁棒的SC/CC原型并对齐特征。

Result: CMLReID在广泛的数据集上进行了实验，结果表明它在性能上超越了所有最先进的方法。尽管存在服装变化和复杂的顺序学习过程，CMLReID仍展现出强大的鲁棒性和泛化能力。

Conclusion: CMLReID成功解决了ReID在服装变化和持续学习中的关键挑战。通过其创新的CASP和AKFP机制，该模型在LReID-Hybrid任务上实现了卓越的性能，并在处理复杂现实世界监控场景方面表现出强大的潜力和有效性。

Abstract: Person Re-Identification (ReID) has several challenges in real-world
surveillance systems due to clothing changes (CCReID) and the need for
maintaining continual learning (LReID). Previous existing methods either
develop models specifically for one application, which is mostly a same-cloth
(SC) setting or treat CCReID as its own separate sub-problem. In this work, we
will introduce the LReID-Hybrid task with the goal of developing a model to
achieve both SC and CC while learning in a continual setting. Mismatched
representations and forgetting from one task to the next are significant
issues, we address this with CMLReID, a CLIP-based framework composed of two
novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive
prompts, and also incorporates context to align richly multi-grained visual
cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection
(AKFP) which produces robust SC/CC prototypes through the use of a dual-path
learner that aligns features with our Clothing-State-Aware Projection Loss.
Experiments performed on a wide range of datasets and illustrate that CMLReID
outperforms all state-of-the-art methods with strong robustness and
generalization despite clothing variations and a sophisticated process of
sequential learning.

</details>


### [104] [Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking](https://arxiv.org/abs/2509.11453)
*BaiChen Fan,Sifan Zhou,Jian Li,Shibo Zhao,Muqing Cao,Qin Wang*

Main category: cs.CV

TL;DR: TrajTrack是一种新型的基于轨迹的3D单目标跟踪（3D SOT）范式，它通过隐式学习历史边界框轨迹的运动连续性，显著提升了双帧跟踪器的性能和效率，解决了现有方法在效率和鲁棒性之间的困境。


<details>
  <summary>Details</summary>
Motivation: 现有的3D单目标跟踪方法面临两难：双帧方法效率高但缺乏长期上下文，在稀疏或遮挡场景中表现不佳；而基于序列的方法虽然鲁棒性强，但计算成本高昂。研究动机在于寻找一种既能保持效率又能提升鲁棒性的解决方案。

Method: 本文提出TrajTrack框架，它通过隐式学习历史边界框轨迹的运动连续性来增强基础的双帧跟踪器，且无需额外的点云输入。具体而言，它首先生成一个快速、显式的运动提议，然后使用一个隐式运动建模模块预测未来轨迹，进而对初始提议进行修正和校正。

Result: 在NuScenes基准测试中，TrajTrack取得了最先进的性能，相比强大的基线方法，跟踪精度显著提升了4.48%，同时以56 FPS的速度运行。此外，TrajTrack在不同的基础跟踪器上表现出强大的泛化能力。

Conclusion: TrajTrack通过引入基于轨迹的范式，有效地解决了3D单目标跟踪中效率和鲁棒性之间的矛盾。它利用历史轨迹隐式学习运动连续性，在不增加额外点云输入的情况下，实现了高精度和高效率的跟踪，并具有良好的泛化性。

Abstract: LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics
and autonomous systems. Existing methods typically follow frame-wise motion
estimation or a sequence-based paradigm. However, the two-frame methods are
efficient but lack long-term temporal context, making them vulnerable in sparse
or occluded scenes, while sequence-based methods that process multiple point
clouds gain robustness at a significant computational cost. To resolve this
dilemma, we propose a novel trajectory-based paradigm and its instantiation,
TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame
tracker by implicitly learning motion continuity from historical bounding box
trajectories alone-without requiring additional, costly point cloud inputs. It
first generates a fast, explicit motion proposal and then uses an implicit
motion modeling module to predict the future trajectory, which in turn refines
and corrects the initial proposal. Extensive experiments on the large-scale
NuScenes benchmark show that TrajTrack achieves new state-of-the-art
performance, dramatically improving tracking precision by 4.48% over a strong
baseline while running at 56 FPS. Besides, we also demonstrate the strong
generalizability of TrajTrack across different base trackers. Video is
available at https://www.bilibili.com/video/BV1ahYgzmEWP.

</details>


### [105] [Cross-Domain Attribute Alignment with CLIP: A Rehearsal-Free Approach for Class-Incremental Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.11264)
*Kerun Mi,Guoliang Kang,Guangyu Li,Lin Zhao,Tao Zhou,Chen Gong*

Main category: cs.CV

TL;DR: 本文提出了一种无排练的类增量无监督域适应（CI-UDA）方法，通过利用CLIP挖掘并对齐跨域的领域不变、类别无关的“属性”，有效减轻灾难性遗忘并缓解域偏移。


<details>
  <summary>Details</summary>
Motivation: 现有CI-UDA方法通常依赖存储和重放目标域样本以避免灾难性遗忘，导致内存持续增长；同时，它们仅在共享类别间进行不对称对齐，可能导致知识遗忘。这些限制促使研究者寻找一种无排练、更有效的解决方案。

Method: 本文提出挖掘和保留领域不变、类别无关的知识来解决CI-UDA任务。具体方法是：1) 使用CLIP提取类别无关的“属性”。2) 将属性表示为“键值”对，其中键是视觉原型，值是文本提示。3) 维护两个属性字典（分别对应源域和目标域）。4) 通过鼓励视觉注意力一致性和预测一致性，在不同域之间进行属性对齐，以缓解域偏移。该方法无需样本重放。

Result: 在三个CI-UDA基准测试上的实验表明，本文方法优于现有最先进的方法，并能有效缓解灾难性遗忘。

Conclusion: 通过属性建模和跨域对齐，本文提出的无排练方法在有效缓解域偏移的同时，显著减少了灾难性知识遗忘，为CI-UDA任务提供了一种新颖且高效的解决方案。

Abstract: Class-Incremental Unsupervised Domain Adaptation (CI-UDA) aims to adapt a
model from a labeled source domain to an unlabeled target domain, where the
sets of potential target classes appearing at different time steps are disjoint
and are subsets of the source classes. The key to solving this problem lies in
avoiding catastrophic forgetting of knowledge about previous target classes
during continuously mitigating the domain shift. Most previous works
cumbersomely combine two technical components. On one hand, they need to store
and utilize rehearsal target sample from previous time steps to avoid
catastrophic forgetting; on the other hand, they perform alignment only between
classes shared across domains at each time step. Consequently, the memory will
continuously increase and the asymmetric alignment may inevitably result in
knowledge forgetting. In this paper, we propose to mine and preserve
domain-invariant and class-agnostic knowledge to facilitate the CI-UDA task.
Specifically, via using CLIP, we extract the class-agnostic properties which we
name as "attribute". In our framework, we learn a "key-value" pair to represent
an attribute, where the key corresponds to the visual prototype and the value
is the textual prompt. We maintain two attribute dictionaries, each
corresponding to a different domain. Then we perform attribute alignment across
domains to mitigate the domain shift, via encouraging visual attention
consistency and prediction consistency. Through attribute modeling and
cross-domain alignment, we effectively reduce catastrophic knowledge forgetting
while mitigating the domain shift, in a rehearsal-free way. Experiments on
three CI-UDA benchmarks demonstrate that our method outperforms previous
state-of-the-art methods and effectively alleviates catastrophic forgetting.
Code is available at https://github.com/RyunMi/VisTA.

</details>


### [106] [Synthetic Dataset Evaluation Based on Generalized Cross Validation](https://arxiv.org/abs/2509.11273)
*Zhihang Song,Dingyi Yao,Ruibo Ming,Lihui Peng,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的评估框架，结合广义交叉验证和域迁移学习，用于可泛化且可比较地评估合成数据集的质量，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据集生成技术的快速发展，评估合成数据质量成为关键研究焦点。然而，当前的评估研究有限，缺乏普遍接受的标准框架，这阻碍了数据生成方法的创新和合成资源利用的优化。

Method: 该框架整合了广义交叉验证实验和域迁移学习原理。具体方法包括：使用合成数据集和多个真实世界基准（如KITTI、BDD100K）训练特定任务模型（如YOLOv5s），形成交叉性能矩阵。通过归一化构建广义交叉验证（GCV）矩阵以量化域可迁移性。框架引入了两个关键指标：一个衡量合成数据与真实世界数据集的相似性（模拟质量），另一个评估合成数据在各种真实世界场景中的多样性和覆盖范围（迁移质量）。

Result: 在Virtual KITTI上的实验验证表明，所提出的框架和指标在评估合成数据保真度方面是有效的。

Conclusion: 该框架提供了一个可扩展、可量化且有原则的评估解决方案，克服了传统限制，为人工智能研究中合成数据集的优化提供了指导。

Abstract: With the rapid advancement of synthetic dataset generation techniques,
evaluating the quality of synthetic data has become a critical research focus.
Robust evaluation not only drives innovations in data generation methods but
also guides researchers in optimizing the utilization of these synthetic
resources. However, current evaluation studies for synthetic datasets remain
limited, lacking a universally accepted standard framework. To address this,
this paper proposes a novel evaluation framework integrating generalized
cross-validation experiments and domain transfer learning principles, enabling
generalizable and comparable assessments of synthetic dataset quality. The
framework involves training task-specific models (e.g., YOLOv5s) on both
synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),
forming a cross-performance matrix. Following normalization, a Generalized
Cross-Validation (GCV) Matrix is constructed to quantify domain
transferability. The framework introduces two key metrics. One measures the
simulation quality by quantifying the similarity between synthetic data and
real-world datasets, while another evaluates the transfer quality by assessing
the diversity and coverage of synthetic data across various real-world
scenarios. Experimental validation on Virtual KITTI demonstrates the
effectiveness of our proposed framework and metrics in assessing synthetic data
fidelity. This scalable and quantifiable evaluation solution overcomes
traditional limitations, providing a principled approach to guide synthetic
dataset optimization in artificial intelligence research.

</details>


### [107] [Learning to Generate 4D LiDAR Sequences](https://arxiv.org/abs/2509.11959)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: LiDARCrafter是一个统一框架，能将自由语言指令转换为可编辑的4D LiDAR序列，实现了高保真度、可控性和时间一致性，为LiDAR仿真和数据增强奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式世界模型在视频和基于占据的数据合成方面取得了进展，但LiDAR生成仍未被充分探索，而其对于精确3D感知至关重要。将生成扩展到4D LiDAR数据带来了可控性、时间稳定性和评估方面的挑战。

Method: LiDARCrafter框架将指令解析为以自我为中心的场景图，然后由一个三分支扩散模型将其转换为物体布局、轨迹和形状。一个距离图像扩散模型生成初始扫描，一个自回归模块将其扩展为时间连贯的序列。显式布局设计还支持物体级别的编辑（如插入或重新定位）。为公平评估，论文提供了EvalSuite基准，涵盖场景、物体和序列级别的指标。

Result: 在nuScenes数据集上，LiDARCrafter在保真度、可控性和时间一致性方面达到了最先进的水平。

Conclusion: LiDARCrafter为LiDAR-based仿真和数据增强提供了基础，解决了4D LiDAR数据生成中的关键挑战，并实现了语言到可编辑LiDAR序列的转换。

Abstract: While generative world models have advanced video and occupancy-based data
synthesis, LiDAR generation remains underexplored despite its importance for
accurate 3D perception. Extending generation to 4D LiDAR data introduces
challenges in controllability, temporal stability, and evaluation. We present
LiDARCrafter, a unified framework that converts free-form language into
editable LiDAR sequences. Instructions are parsed into ego-centric scene
graphs, which a tri-branch diffusion model transforms into object layouts,
trajectories, and shapes. A range-image diffusion model generates the initial
scan, and an autoregressive module extends it into a temporally coherent
sequence. The explicit layout design further supports object-level editing,
such as insertion or relocation. To enable fair assessment, we provide
EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On
nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and
temporal consistency, offering a foundation for LiDAR-based simulation and data
augmentation.

</details>


### [108] [ROSGS: Relightable Outdoor Scenes With Gaussian Splatting](https://arxiv.org/abs/2509.11275)
*Lianjun Liao,Chunhui Zhang,Tong Wu,Henglei Lv,Bailin Deng,Lin Gao*

Main category: cs.CV

TL;DR: ROSGS是一个两阶段流水线，利用2D高斯溅射和混合光照模型，高效且准确地重建可重打光室外场景，解决了现有方法计算开销大和重打光精度低的问题。


<details>
  <summary>Details</summary>
Motivation: 室外图像数据因无界场景和多变光照条件，难以分解为几何、反射和光照。现有基于NeRF或3DGS的方法存在两大局限：NeRF的神经网络计算开销高，以及3DGS使用低频光照表示，导致渲染效率低下和重打光精度不佳。

Method: ROSGS采用两阶段流水线：
1. 利用单目法线先验，通过紧凑的2D高斯溅射（2DGS）表示高效重建场景几何基础。
2. 在重建几何之上，通过混合光照模型分解场景纹理和光照：使用球形高斯函数捕获太阳光的定向高频分量，并通过球谐系数学习辐射传输函数以全面建模剩余的低频天光。

Result: 定量指标和定性比较均表明，ROSGS在室外场景重打光方面达到了最先进的性能，并展现了卓越的重打光精度和渲染效率。

Conclusion: ROSGS提供了一种高效且准确的解决方案，用于可重打光的室外场景重建，显著提升了重打光效果和渲染效率。

Abstract: Image data captured outdoors often exhibit unbounded scenes and
unconstrained, varying lighting conditions, making it challenging to decompose
them into geometry, reflectance, and illumination. Recent works have focused on
achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D
Gaussian Splatting (3DGS) representation but remain hindered by two key
limitations: the high computational overhead associated with neural networks of
NeRF and the use of low-frequency lighting representations, which often result
in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS,
a two-stage pipeline designed to efficiently reconstruct relightable outdoor
scenes using the Gaussian Splatting representation. By leveraging monocular
normal priors, ROSGS first reconstructs the scene's geometry with the compact
2D Gaussian Splatting (2DGS) representation, providing an efficient and
accurate geometric foundation. Building upon this reconstructed geometry, ROSGS
then decomposes the scene's texture and lighting through a hybrid lighting
model. This model effectively represents typical outdoor lighting by employing
a spherical Gaussian function to capture the directional, high-frequency
components of sunlight, while learning a radiance transfer function via
Spherical Harmonic coefficients to model the remaining low-frequency skylight
comprehensively. Both quantitative metrics and qualitative comparisons
demonstrate that ROSGS achieves state-of-the-art performance in relighting
outdoor scenes and highlight its ability to deliver superior relighting
accuracy and rendering efficiency.

</details>


### [109] [Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations](https://arxiv.org/abs/2509.11287)
*Yifan Lu,Ziqi Zhang,Chunfeng Yuan,Jun Gao,Congxuan Zhang,Xiaojuan Qi,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: APASI是一种无需外部依赖的自主偏好对齐方法，通过模型自注入幻觉来生成偏好数据，有效缓解了大型视觉-语言模型（LVLMs）的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）存在严重的幻觉问题，即生成响应与视觉输入不一致。现有缓解方法主要依赖偏好对齐，但需要外部人工标注或辅助模型收集偏好数据，这增加了成本并限制了可持续改进。

Method: 本文提出了自主偏好对齐自注入（APASI）方法。APASI利用目标LVLM自身向生成响应中注入幻觉，创建具有不同偏好水平的响应对。自注入过程基于对幻觉的三个关键观察，确保模拟真实的幻觉模式，从而提供准确的学习信号。此外，APASI结合了迭代对齐训练策略和课程学习，周期性更新挑战性渐增的偏好数据，实现LVLM的稳定持续增强。

Result: 在六个基准测试中进行的广泛实验表明，APASI不仅有效缓解了三个基线模型的幻觉问题，而且达到了与依赖外部数据的对齐方法相当甚至更优的性能，证明了其有效性和泛化能力。

Conclusion: APASI提供了一种新颖且通用的方法，通过自主偏好对齐和自注入机制，无需外部依赖即可有效缓解大型视觉-语言模型的幻觉问题，并取得了与传统方法竞争或超越的性能。

Abstract: Large Vision-Language Models (LVLMs) suffer from serious hallucination
problems, where the model-generated responses are inconsistent with the visual
inputs. Existing hallucination mitigation methods are mainly based on
preference alignment and require external human annotations or auxiliary models
for preference data collection, which increase costs and limit sustainable
improvement. To tackle these challenges, we propose Autonomous Preference
Alignment via Self-Injection (APASI), a novel and generalizable method that
mitigates hallucinations without external dependencies. APASI leverages the
target LVLM to self-inject hallucinations into a generated response, creating a
pair of responses with varying preference levels. During the self-injection
process, the dis-preferred response is generated based on three key
observations of hallucinations, ensuring it simulates real hallucination
patterns. This fidelity offers an accurate learning signal for hallucination
mitigation. Moreover, APASI incorporates an iterative alignment training
strategy combined with curriculum learning to periodically update the
preference data with increasing challenge, enabling stable and continuous
enhancement of the LVLM. Extensive experiments across six benchmarks show that
APASI not only effectively mitigates hallucinations for three baseline models
but also achieves comparable or even superior performance to alignment-based
methods with external dependency, thereby demonstrating its effectiveness and
generalization capability. The code is available at
https://github.com/davidluciolu/APASI.

</details>


### [110] [Leveraging Geometric Priors for Unaligned Scene Change Detection](https://arxiv.org/abs/2509.11292)
*Ziling Liu,Ziwei Chen,Mingqi Gao,Jinyu Yang,Feng Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的框架，首次利用几何基础模型的几何先验来解决未对齐场景变化检测的核心挑战，包括可靠识别视觉重叠、建立鲁棒对应关系和显式检测遮挡，从而在视点未对齐的情况下实现可靠的变化检测。


<details>
  <summary>Details</summary>
Motivation: 当前未对齐场景变化检测（Unaligned SCD）方法仅依赖2D视觉线索建立跨图像对应关系，但在大视点变化下，外观匹配容易失效。此外，小规模数据集上有限的2D变化掩码监督限制了泛化多视图知识的学习，难以可靠识别视觉重叠和处理遮挡。这种缺乏显式几何推理是当前方法的一个关键且被忽视的局限性。

Method: 本文首次利用几何基础模型的几何先验来解决Unaligned SCD的核心挑战。在此基础上，提出一个无需训练的框架，将这些几何先验与视觉基础模型的强大表示相结合，以实现视点未对齐下的可靠变化检测。

Result: 在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，本文提出的方法取得了卓越且鲁棒的性能。

Conclusion: 通过利用几何基础模型的几何先验，本文提出的训练无关框架能够有效应对未对齐场景变化检测中的核心挑战，包括视觉重叠识别、鲁棒对应和遮挡检测，从而在视点未对齐的复杂条件下实现更可靠的变化检测。

Abstract: Unaligned Scene Change Detection aims to detect scene changes between image
pairs captured at different times without assuming viewpoint alignment. To
handle viewpoint variations, current methods rely solely on 2D visual cues to
establish cross-image correspondence to assist change detection. However, large
viewpoint changes can alter visual observations, causing appearance-based
matching to drift or fail. Additionally, supervision limited to 2D change masks
from small-scale SCD datasets restricts the learning of generalizable
multi-view knowledge, making it difficult to reliably identify visual overlaps
and handle occlusions. This lack of explicit geometric reasoning represents a
critical yet overlooked limitation. In this work, we are the first to leverage
geometric priors from a Geometric Foundation Model to address the core
challenges of unaligned SCD, including reliable identification of visual
overlaps, robust correspondence establishment, and explicit occlusion
detection. Building on these priors, we propose a training-free framework that
integrates them with the powerful representations of a visual foundation model
to enable reliable change detection under viewpoint misalignment. Through
extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we
demonstrate that our approach achieves superior and robust performance. Our
code will be released at https://github.com/ZilingLiu/GeoSCD.

</details>


### [111] [Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding](https://arxiv.org/abs/2509.11323)
*Jian Song,Wei Mei,Yunfeng Xu,Qiang Fu,Renke Kou,Lina Bu,Yucheng Long*

Main category: cs.CV

TL;DR: 本文提出了一种名为SIKNet的新型学习辅助滤波器，用于多目标跟踪（MOT）中的运动估计，通过语义独立编码器提升了非线性运动和参数不匹配情况下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 多目标跟踪中的运动估计是关键环节，但传统的卡尔曼滤波器（KF）在参数不匹配或物体非平稳运动时表现不佳，导致跟踪失败和身份切换。因此，需要更鲁棒、准确的运动估计方法。

Method: 作者提出了一种名为Semantic-Independent KalmanNet (SIKNet) 的学习辅助滤波器。该方法使用一个语义独立编码器（SIE）来编码状态向量。SIE分两步进行：首先，使用1D卷积（核大小为1）沿不同状态向量中同质语义元素的维度进行卷积，以编码独立的语义信息；其次，采用全连接层和非线性激活层来编码异质语义元素之间的非线性和交叉依赖信息。为了独立评估运动估计模块的性能，作者构建了一个大规模的半模拟数据集。

Result: 实验结果表明，所提出的SIKNet优于传统的卡尔曼滤波器，并且比现有的学习辅助滤波器具有更卓越的鲁棒性和准确性。

Conclusion: SIKNet通过其新颖的语义独立编码器，有效解决了传统卡尔曼滤波器在非平稳运动和参数不匹配时的局限性，为多目标跟踪提供了一种更鲁棒、更准确的运动估计解决方案。

Abstract: Motion estimation is a crucial component in multi-object tracking (MOT).
  It predicts the trajectory of objects by analyzing the changes in their
positions in consecutive frames of images, reducing tracking failures and
identity switches.
  The Kalman filter (KF) based on the linear constant-velocity model is one of
the most commonly used methods in MOT.
  However, it may yield unsatisfactory results when KF's parameters are
mismatched and objects move in non-stationary.
  In this work, we utilize the learning-aided filter to handle the motion
estimation of MOT.
  In particular, we propose a novel method named Semantic-Independent KalmanNet
(SIKNet), which encodes the state vector (the input feature) using a
Semantic-Independent Encoder (SIE) by two steps.
  First, the SIE uses a 1D convolution with a kernel size of 1, which convolves
along the dimension of homogeneous-semantic elements across different state
vectors to encode independent semantic information.
  Then it employs a fully-connected layer and a nonlinear activation layer to
encode nonlinear and cross-dependency information between
heterogeneous-semantic elements.
  To independently evaluate the performance of the motion estimation module in
MOT, we constructed a large-scale semi-simulated dataset from several
open-source MOT datasets.
  Experimental results demonstrate that the proposed SIKNet outperforms the
traditional KF and achieves superior robustness and accuracy than existing
learning-aided filters.
  The code is available at (https://github.com/SongJgit/filternet and
https://github.com/SongJgit/TBDTracker).

</details>


### [112] [UnLoc: Leveraging Depth Uncertainties for Floorplan Localization](https://arxiv.org/abs/2509.11301)
*Matthias Wüest,Francis Engelmann,Ondrej Miksik,Marc Pollefeys,Daniel Barath*

Main category: cs.CV

TL;DR: UnLoc是一种高效的数据驱动解决方案，用于基于平面图的相机序列定位。它通过引入不确定性建模和利用现成的预训练深度模型，显著提高了定位的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在关键局限性：深度预测缺乏不确定性建模，并且需要为每个环境训练定制的深度网络。而平面图数据易于获取、长期稳定且对视觉外观变化具有鲁棒性，是解决这些问题的潜在途径。

Method: 本文提出了UnLoc，一个新颖的概率模型，将深度预测建模为明确的概率分布，从而纳入了不确定性估计。通过利用现成的预训练单目深度模型，UnLoc消除了对每个环境训练深度网络的依赖，增强了对未知空间的泛化能力。

Result: UnLoc在大型合成和真实世界数据集上，在准确性和鲁棒性方面均显著优于现有方法。特别是在具有挑战性的LaMAR HGE数据集上，长序列（100帧）的定位召回率比现有技术高出2.7倍，短序列（15帧）高出16.7倍。

Conclusion: UnLoc提供了一种高效、准确且鲁棒的相机序列定位解决方案，克服了现有方法在深度预测不确定性建模和环境泛化方面的局限性，特别适用于利用平面图数据进行定位的场景。

Abstract: We propose UnLoc, an efficient data-driven solution for sequential camera
localization within floorplans. Floorplan data is readily available, long-term
persistent, and robust to changes in visual appearance. We address key
limitations of recent methods, such as the lack of uncertainty modeling in
depth predictions and the necessity for custom depth networks trained for each
environment. We introduce a novel probabilistic model that incorporates
uncertainty estimation, modeling depth predictions as explicit probability
distributions. By leveraging off-the-shelf pre-trained monocular depth models,
we eliminate the need to rely on per-environment-trained depth networks,
enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale
synthetic and real-world datasets, demonstrating significant improvements over
existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$
times higher localization recall on long sequences (100 frames) and $16.7$
times higher on short ones (15 frames) than the state of the art on the
challenging LaMAR HGE dataset.

</details>


### [113] [Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness](https://arxiv.org/abs/2509.11355)
*Robin Narsingh Ranabhat,Longwei Wang,Amit Kumar Patel,KC santosh*

Main category: cs.CV

TL;DR: 为解决CNN对图像损坏的脆弱性（因依赖局部纹理而非全局形状），本文提出了两种互补的正则化策略：一是辅助损失，强制原始输入和低频滤波输入之间的特征一致性；二是监督对比学习，以构建形状相关的特征空间。两种方法均在不降低干净数据准确率的前提下，提高了对损坏图像的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNNs）在图像分类方面表现出色，但对人类容易处理的常见图像损坏却很脆弱。这种脆弱性的一个关键原因是CNNs依赖局部纹理线索而非全局物体形状，这与人类感知形成鲜明对比。

Method: 本文提出了两种互补的正则化策略以鼓励形状偏向的表示并增强鲁棒性：1. 引入一个辅助损失，强制原始输入和低频滤波输入之间的特征一致性，从而抑制对高频纹理的依赖。2. 整合监督对比学习，以围绕类别一致、形状相关的表示来构建特征空间。

Result: 在CIFAR-10-C基准测试中，这两种方法都提高了对图像损坏的鲁棒性，同时没有降低在干净数据上的准确率。

Conclusion: 研究结果表明，损失层面的正则化可以有效地引导CNNs生成更具形状感知能力和更具弹性的表示。

Abstract: Convolutional Neural Networks (CNNs) excel at image classification but remain
vulnerable to common corruptions that humans handle with ease. A key reason for
this fragility is their reliance on local texture cues rather than global
object shapes -- a stark contrast to human perception. To address this, we
propose two complementary regularization strategies designed to encourage
shape-biased representations and enhance robustness. The first introduces an
auxiliary loss that enforces feature consistency between original and
low-frequency filtered inputs, discouraging dependence on high-frequency
textures. The second incorporates supervised contrastive learning to structure
the feature space around class-consistent, shape-relevant representations.
Evaluated on the CIFAR-10-C benchmark, both methods improve corruption
robustness without degrading clean accuracy. Our results suggest that
loss-level regularization can effectively steer CNNs toward more shape-aware,
resilient representations.

</details>


### [114] [Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency](https://arxiv.org/abs/2509.11328)
*Mingyuan Meng*

Main category: cs.CV

TL;DR: 本研究旨在通过探索有效的长距离视觉依赖建模来改进医学图像计算（MIC），发现基于MLP的模型在处理高分辨率医学图像中的细粒度长距离依赖方面优于Transformer和CNN。


<details>
  <summary>Details</summary>
Motivation: 医学图像计算需要模型同时捕捉全局上下文和局部细微特征，这要求细粒度的长距离视觉依赖建模。CNN受限于局部性，Transformer虽擅长长距离建模但计算成本高，难以处理高分辨率特征中的细微细节。MLP在建模长距离依赖方面计算/内存效率高，但在MIC领域尚未得到广泛研究。

Method: 本研究首先创新性地将Transformer应用于像素级和图像级的医学视觉任务。随后，开创性地开发了基于MLP的视觉模型，以捕捉医学图像中细粒度的长距离视觉依赖。

Result: 广泛的实验证实了长距离依赖建模在MIC中的关键作用。主要发现是：MLP在建模包含丰富解剖/病理细节的更高分辨率医学特征之间的更细粒度长距离依赖方面具有可行性。MLP在各种医学视觉任务中持续提升了性能。

Conclusion: 这一发现确立了MLP作为优于Transformer/CNN的范式，为下一代医学视觉骨干网络铺平了道路，因为它能有效建模高分辨率医学图像中的细粒度长距离依赖。

Abstract: Medical Image Computing (MIC) is a broad research topic covering both
pixel-wise (e.g., segmentation, registration) and image-wise (e.g.,
classification, regression) vision tasks. Effective analysis demands models
that capture both global long-range context and local subtle visual
characteristics, necessitating fine-grained long-range visual dependency
modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by
intrinsic locality, transformers excel at long-range modeling; however, due to
the high computational loads of self-attention, transformers typically cannot
process high-resolution features (e.g., full-scale image features before
downsampling or patch embedding) and thus face difficulties in modeling
fine-grained dependency among subtle medical image details. Concurrently,
Multi-layer Perceptron (MLP)-based visual models are recognized as
computation/memory-efficient alternatives in modeling long-range visual
dependency but have yet to be widely investigated in the MIC community. This
doctoral research advances deep learning-based MIC by investigating effective
long-range visual dependency modeling. It first presents innovative use of
transformers for both pixel- and image-wise medical vision tasks. The focus
then shifts to MLPs, pioneeringly developing MLP-based visual models to capture
fine-grained long-range visual dependency in medical images. Extensive
experiments confirm the critical role of long-range dependency modeling in MIC
and reveal a key finding: MLPs provide feasibility in modeling finer-grained
long-range dependency among higher-resolution medical features containing
enriched anatomical/pathological details. This finding establishes MLPs as a
superior paradigm over transformers/CNNs, consistently enhancing performance
across various medical vision tasks and paving the way for next-generation
medical vision backbones.

</details>


### [115] [Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2509.11587)
*Haonan Shi,Yubin Wang,De Cheng,Lingfeng He,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 该论文提出了分层身份学习（HIL）框架，通过二次聚类生成多中心记忆并结合多中心对比学习（MCCL）来解决无监督可见光-红外行人重识别（USVI-ReID）中忽略细粒度差异的问题，并通过双向反向选择传输（BRST）机制提高跨模态匹配质量。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督可见光-红外行人重识别（USVI-ReID）方法通常使用基于聚类的对比学习，将一个人表示为一个单一的聚类中心，但它们主要关注每个聚类内部图像的共性，而忽略了它们之间更细粒度的差异。

Method: 该论文提出了分层身份学习（HIL）框架。它通过二次聚类为每个粗粒度聚类生成多个记忆（即子聚类），以反映图像间的细粒度变化。此外，还提出了多中心对比学习（MCCL）来优化表示，增强模态内聚类并最小化跨模态差异。为进一步提高跨模态匹配质量，设计了双向反向选择传输（BRST）机制，通过伪标签的双向匹配建立可靠的跨模态对应关系。

Result: 在SYSU-MM01和RegDB数据集上进行的广泛实验表明，所提出的方法优于现有方法。

Conclusion: HIL框架通过处理细粒度差异和增强跨模态匹配，有效提升了无监督可见光-红外行人重识别的性能。

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to
learn modality-invariant image features from unlabeled cross-modal person
datasets by reducing the modality gap while minimizing reliance on costly
manual annotations. Existing methods typically address USVI-ReID using
cluster-based contrastive learning, which represents a person by a single
cluster center. However, they primarily focus on the commonality of images
within each cluster while neglecting the finer-grained differences among them.
To address the limitation, we propose a Hierarchical Identity Learning (HIL)
framework. Since each cluster may contain several smaller sub-clusters that
reflect fine-grained variations among images, we generate multiple memories for
each existing coarse-grained cluster via a secondary clustering. Additionally,
we propose Multi-Center Contrastive Learning (MCCL) to refine representations
for enhancing intra-modal clustering and minimizing cross-modal discrepancies.
To further improve cross-modal matching quality, we design a Bidirectional
Reverse Selection Transmission (BRST) mechanism, which establishes reliable
cross-modal correspondences by performing bidirectional matching of
pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB
datasets demonstrate that the proposed method outperforms existing approaches.
The source code is available at: https://github.com/haonanshi0125/HIL.

</details>


### [116] [Dual Band Video Thermography Near Ambient Conditions](https://arxiv.org/abs/2509.11334)
*Sriram Narayanan,Mani Ramanagopal,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 本文提出了一种使用双波段热像仪分离近环境条件下热辐射中反射和发射分量的方法，以估计物体属性。


<details>
  <summary>Details</summary>
Motivation: 热像仪捕获的红外辐射包含反射和发射两个分量。在计算机视觉应用中最相关的近环境条件下，这两个分量通常大小相当且随时间变化，而以往的研究常假设其中一个占主导或恒定。分离它们对于理解物体发射率、温度、反射率和形状至关重要。

Method: 该方法利用两个具有不同光谱灵敏度的热像仪捕获的视频。它推导了一个双波段热图像形成模型，并开发了算法来估计表面的发射率和随时间变化的温度，同时隔离动态背景。

Result: 研究通过对一系列材料的精确校准发射率进行了定量评估，并展示了在复杂日常场景（如装有热液体的玻璃杯和背景中移动的人）上的定性结果。

Conclusion: 本文首次提出了一种在双波段热像仪视频中分离反射和发射光分量的方法，能够估计表面发射率和随时间变化的温度，并处理动态背景，这在近环境计算机视觉应用中具有重要意义。

Abstract: Long-wave infrared radiation captured by a thermal camera consists of two
components: (a) light from the environment reflected or transmitted by a
surface, and (b) light emitted by the surface after undergoing heat transport
through the object and exchanging heat with the surrounding environment.
Separating these components is essential for understanding object properties
such as emissivity, temperature, reflectance and shape. Previous thermography
studies often assume that only one component is dominant (e.g., in welding) or
that the second component is constant and can be subtracted. However, in
near-ambient conditions, which are most relevant to computer vision
applications, both components are typically comparable in magnitude and vary
over time. We introduce the first method that separates reflected and emitted
components of light in videos captured by two thermal cameras with different
spectral sensitivities. We derive a dual-band thermal image formation model and
develop algorithms to estimate the surface's emissivity and its time-varying
temperature while isolating a dynamic background. We quantitatively evaluate
our approach using carefully calibrated emissivities for a range of materials
and show qualitative results on complex everyday scenes, such as a glass filled
with hot liquid and people moving in the background.

</details>


### [117] [DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition](https://arxiv.org/abs/2509.11661)
*Lifei Hao,Yue Cheng,Baoqi Huang,Bing Jia,Xuandong Zhao*

Main category: cs.CV

TL;DR: DTGen是一种基于生成扩散模型的少样本数据增强方案，专为细粒度脏餐具识别设计，通过高效域特化、多样化生成和质量过滤，显著提升分类器性能，并支持轻量化部署，验证了生成式AI在少样本工业视觉中的价值。


<details>
  <summary>Details</summary>
Motivation: 智能餐具清洗是食品安全和智能家居中的关键应用，但现有方法受限于粗粒度分类和少样本数据稀缺，难以满足工业化需求。

Method: 提出DTGen，一种基于生成扩散模型的少样本数据增强方案。具体方法包括：通过LoRA实现高效域特化，利用结构化提示生成多样化的脏餐具图像，并通过基于CLIP的跨模态过滤确保数据质量。

Result: 在极度有限的真实少样本条件下，DTGen能合成几乎无限的高质量样本，显著提高了分类器性能，支持细粒度脏餐具识别。此外，还提出了轻量级部署策略，可将DTGen集成到嵌入式洗碗机中，智能调节能耗和洗涤剂用量。

Conclusion: DTGen不仅验证了生成式AI在少样本工业视觉中的价值，还为自动化餐具清洗和食品安全监测提供了可行的部署路径。

Abstract: Intelligent tableware cleaning is a critical application in food safety and
smart homes, but existing methods are limited by coarse-grained classification
and scarcity of few-shot data, making it difficult to meet industrialization
requirements. We propose DTGen, a few-shot data augmentation scheme based on
generative diffusion models, specifically designed for fine-grained dirty
tableware recognition. DTGen achieves efficient domain specialization through
LoRA, generates diverse dirty images via structured prompts, and ensures data
quality through CLIP-based cross-modal filtering. Under extremely limited real
few-shot conditions, DTGen can synthesize virtually unlimited high-quality
samples, significantly improving classifier performance and supporting
fine-grained dirty tableware recognition. We further elaborate on lightweight
deployment strategies, promising to transfer DTGen's benefits to embedded
dishwashers and integrate with cleaning programs to intelligently regulate
energy consumption and detergent usage. Research results demonstrate that DTGen
not only validates the value of generative AI in few-shot industrial vision but
also provides a feasible deployment path for automated tableware cleaning and
food safety monitoring.

</details>


### [118] [Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning](https://arxiv.org/abs/2509.11344)
*Huaiyuan Qin,Muli Yang,Siyuan Hu,Peng Hu,Yu Zhang,Chen Gong,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 传统自监督学习（SSL）的实例一致性假设在非标志性数据上失效。本研究发现，即使缺乏严格的实例一致性，SSL仍能学习有意义的表示，并且在一定范围内增加视图多样性（通过EMD衡量）可以提高下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习（SSL）依赖实例一致性假设，即同一图像的不同视图是正样本对。然而，对于非标志性数据，不同视图可能包含不同对象或语义信息，导致此假设失效。本研究旨在探讨在实例一致性无法保证的情况下，SSL的有效性。

Method: 通过广泛的消融研究，探讨在实例一致性不严格的情况下SSL的有效性。通过强制零重叠或使用较小的裁剪尺度来增加视图多样性。采用地球移动距离（EMD）作为估计器来衡量视图间的互信息，以量化视图多样性。

Result: SSL即使在正样本对缺乏严格实例一致性时，也能学习到有意义的表示。增加视图多样性（通过强制零重叠或使用较小的裁剪尺度）可以增强分类和密集预测任务的下游性能。然而，过度的多样性会降低效果，表明存在一个视图多样性的最佳范围。适度的EMD值与改进的SSL学习相关。

Conclusion: SSL在缺乏严格实例一致性的情况下仍然有效。视图多样性是关键因素，存在一个最佳范围，并且可以通过EMD进行量化，为未来的SSL框架设计提供了指导。这些发现具有鲁棒性和广泛适用性。

Abstract: Self-supervised learning (SSL) conventionally relies on the instance
consistency paradigm, assuming that different views of the same image can be
treated as positive pairs. However, this assumption breaks down for non-iconic
data, where different views may contain distinct objects or semantic
information. In this paper, we investigate the effectiveness of SSL when
instance consistency is not guaranteed. Through extensive ablation studies, we
demonstrate that SSL can still learn meaningful representations even when
positive pairs lack strict instance consistency. Furthermore, our analysis
further reveals that increasing view diversity, by enforcing zero overlapping
or using smaller crop scales, can enhance downstream performance on
classification and dense prediction tasks. However, excessive diversity is
found to reduce effectiveness, suggesting an optimal range for view diversity.
To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to
measure mutual information between views, finding that moderate EMD values
correlate with improved SSL learning, providing insights for future SSL
framework design. We validate our findings across a range of settings,
highlighting their robustness and applicability on diverse data sources.

</details>


### [119] [GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration](https://arxiv.org/abs/2509.11360)
*Wan Xu,Feng Zhu,Yihan Zeng,Yuanfan Guo,Ming Liu,Hang Xu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 本文提出GLaVE-Cap框架，通过整合视觉专家和全局-局部对齐机制，解决了现有视频详细字幕生成方法中字幕细节不足和上下文不一致的问题。同时，构建了新的基准和训练数据集，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频详细字幕生成方法多采用“局部到全局”范式，即先生成视频片段的局部字幕再汇总成全局字幕。然而，这种范式导致字幕细节不足和上下文不一致，原因在于缺乏确保细粒度字幕的机制以及局部与全局字幕之间交互薄弱。

Method: 本文提出了GLaVE-Cap框架，包含两个核心模块：1) TrackFusion，通过利用视觉专家获取跨帧视觉提示并结合双流结构，实现全面的局部字幕生成；2) CaptionBridge，通过全局上下文指导局部字幕生成，并自适应地将局部字幕汇总为连贯的全局字幕，从而建立局部与全局的交互。此外，本文还构建了GLaVE-Bench基准和GLaVE-1.2M训练数据集，用于更可靠的评估和训练。

Result: GLaVE-Cap在四个基准测试中均取得了最先进的性能。消融研究和学生模型分析进一步验证了所提出模块的有效性以及GLaVE-1.2M数据集对视频理解社区的贡献。

Conclusion: GLaVE-Cap框架通过创新的全局-局部对齐和视觉专家集成，有效解决了视频详细字幕生成中存在的细节不足和上下文不一致问题，实现了显著的性能提升。同时，新构建的基准和数据集为该领域的研究提供了宝贵的资源。

Abstract: Video detailed captioning aims to generate comprehensive video descriptions
to facilitate video understanding. Recently, most efforts in the video detailed
captioning community have been made towards a local-to-global paradigm, which
first generates local captions from video clips and then summarizes them into a
global caption. However, we find this paradigm leads to less detailed and
contextual-inconsistent captions, which can be attributed to (1) no mechanism
to ensure fine-grained captions, and (2) weak interaction between local and
global captions. To remedy the above two issues, we propose GLaVE-Cap, a
Global-Local aligned framework with Vision Expert integration for Captioning,
which consists of two core modules: TrackFusion enables comprehensive local
caption generation, by leveraging vision experts to acquire cross-frame visual
prompts, coupled with a dual-stream structure; while CaptionBridge establishes
a local-global interaction, by using global context to guide local captioning,
and adaptively summarizing local captions into a coherent global caption.
Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark
featuring 5X more queries per video than existing benchmarks, covering diverse
visual dimensions to facilitate reliable evaluation. We further provide a
training dataset GLaVE-1.2M containing 16K high-quality fine-grained video
captions and 1.2M related question-answer pairs. Extensive experiments on four
benchmarks show that our GLaVE-Cap achieves state-of-the-art performance.
Besides, the ablation studies and student model analyses further validate the
effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the
video understanding community. The source code, model weights, benchmark, and
dataset will be open-sourced.

</details>


### [120] [Microsurgical Instrument Segmentation for Robot-Assisted Surgery](https://arxiv.org/abs/2509.11727)
*Tae Kyeong Jeong,Garam Kim,Juyoun Park*

Main category: cs.CV

TL;DR: 本文提出了MISRA框架，通过结合亮度通道、跳跃注意力机制和迭代反馈模块，解决了显微手术场景中薄结构分割的挑战，并引入了新的数据集，显著提升了分割性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 显微手术场景中薄结构的精确分割对于场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡，这一任务仍具挑战性。

Method: 本文提出了MISRA（Microsurgery Instrument Segmentation for Robotic Assistance）分割框架，该框架：1) 使用亮度通道增强RGB输入；2) 集成跳跃注意力机制以保留细长特征；3) 采用迭代反馈模块（IFM）在多次传递中恢复连续性。此外，还引入了一个包含精细标注的显微手术数据集（MISAW-Seg）作为评估基准。

Result: 实验表明，MISRA实现了具有竞争力的性能，与现有方法相比，平均类别IoU提高了5.37%，并在器械接触和重叠处提供了更稳定的预测。

Conclusion: MISRA是实现计算机辅助和机器人显微手术中可靠场景解析的有希望的一步。

Abstract: Accurate segmentation of thin structures is critical for microsurgical scene
understanding but remains challenging due to resolution loss, low contrast, and
class imbalance. We propose Microsurgery Instrument Segmentation for Robotic
Assistance(MISRA), a segmentation framework that augments RGB input with
luminance channels, integrates skip attention to preserve elongated features,
and employs an Iterative Feedback Module(IFM) for continuity restoration across
multiple passes. In addition, we introduce a dedicated microsurgical dataset
with fine-grained annotations of surgical instruments including thin objects,
providing a benchmark for robust evaluation Dataset available at
https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstrate
that MISRA achieves competitive performance, improving the mean class IoU by
5.37% over competing methods, while delivering more stable predictions at
instrument contacts and overlaps. These results position MISRA as a promising
step toward reliable scene parsing for computer-assisted and robotic
microsurgery.

</details>


### [121] [In-Vivo Skin 3-D Surface Reconstruction and Wrinkle Depth Estimation using Handheld High Resolution Tactile Sensing](https://arxiv.org/abs/2509.11385)
*Akhil Padmanabha,Arpit Agarwal,Catherine Li,Austin Williams,Dinesh K. Patel,Sankalp Chopkar,Achu Wilson,Ahmet Ozkan,Wenzhen Yuan,Sonal Choudhary,Arash Mostaghimi,Zackory Erickson,Carmel Majidi*

Main category: cs.CV

TL;DR: 本文提出一种基于GelSight触觉成像的紧凑型3D皮肤重建探头，实现了微米级皱纹高度估计，并首次验证了其在不同身体部位的皱纹深度测量能力，可用于皮肤分析和护肤品功效评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏便携、高分辨率且经过验证的3D皮肤表面重建设备，无法对不同身体部位的皮肤进行客观和定量的皮肤学评估。

Method: 研究人员开发了一种紧凑型3D皮肤重建探头，该探头基于GelSight触觉成像技术，结合定制弹性凝胶和学习型重建算法，用于微米级皱纹高度估计。该探头集成了力传感功能，以确保接触一致性。

Result: 该探头在模拟皱纹的测试对象上实现了12.55微米的平均绝对误差。在对15名无皮肤疾病参与者的研究中，首次提供了经过验证的跨多个身体区域的皱纹深度指标。此外，研究还发现使用非处方保湿霜后，三个部位的皱纹高度有统计学上的显著降低。

Conclusion: 该工作提供了一种经验证的工具，可用于临床和美容皮肤分析，在诊断、治疗监测和护肤功效评估方面具有潜在应用。

Abstract: Three-dimensional (3-D) skin surface reconstruction offers promise for
objective and quantitative dermatological assessment, but no portable,
high-resolution device exists that has been validated and used for depth
reconstruction across various body locations. We present a compact 3-D skin
reconstruction probe based on GelSight tactile imaging with a custom elastic
gel and a learning-based reconstruction algorithm for micron-level wrinkle
height estimation. Our probe, integrated into a handheld probe with force
sensing for consistent contact, achieves a mean absolute error of 12.55 micron
on wrinkle-like test objects. In a study with 15 participants without skin
disorders, we provide the first validated wrinkle depth metrics across multiple
body regions. We further demonstrate statistically significant reductions in
wrinkle height at three locations following over-the-counter moisturizer
application. Our work offers a validated tool for clinical and cosmetic skin
analysis, with potential applications in diagnosis, treatment monitoring, and
skincare efficacy evaluation.

</details>


### [122] [Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference](https://arxiv.org/abs/2509.11731)
*Yudong Shen,Wenyu Wu,Jiali Mao,Yixiao Tong,Guoping Liu,Chaoya Wang*

Main category: cs.CV

TL;DR: DGMap是一个双解码框架，利用全局上下文感知来解决轨迹数据地图推断中轨迹密度不均导致的路段碎片化和冗余问题，显著提升了地图推断性能。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据在自动化地图推断中具有低成本、广覆盖和持续可用性等优势，但轨迹密度不均会导致稀疏区域路段碎片化和密集区域路段冗余，给现有方法带来挑战。

Method: 本文提出了DGMap，一个具有全局上下文感知的双解码框架。它包含多尺度网格编码（Multi-scale Grid Encoding）、掩码增强关键点提取（Mask-enhanced Keypoint Extraction）和全局上下文感知关系预测（Global Context-aware Relation Prediction）模块。DGMap通过整合全局语义上下文与局部几何特征来提高关键点检测精度，并利用全局上下文感知关系预测模块建模长距离轨迹模式以抑制密集区域的错误连接。

Result: 在三个真实世界数据集上的实验结果表明，DGMap在APLS指标上比现有最先进方法高出5%，在滴滴出行平台轨迹数据上的性能提升尤为显著。

Conclusion: DGMap通过有效处理轨迹密度不均问题，显著提高了从轨迹数据进行地图推断的准确性，尤其在关键点检测和关系预测方面表现出色。

Abstract: Trajectory data has become a key resource for automated map in-ference due to
its low cost, broad coverage, and continuous availability. However, uneven
trajectory density often leads to frag-mented roads in sparse areas and
redundant segments in dense regions, posing significant challenges for existing
methods. To address these issues, we propose DGMap, a dual-decoding framework
with global context awareness, featuring Multi-scale Grid Encoding,
Mask-enhanced Keypoint Extraction, and Global Context-aware Relation
Prediction. By integrating global semantic context with local geometric
features, DGMap improves keypoint detection accuracy to reduce road
fragmentation in sparse-trajectory areas. Additionally, the Global
Context-aware Relation Prediction module suppresses false connections in
dense-trajectory regions by modeling long-range trajectory patterns.
Experimental results on three real-world datasets show that DGMap outperforms
state-of-the-art methods by 5% in APLS, with notable performance gains on
trajectory data from the Didi Chuxing platform

</details>


### [123] [MixANT: Observation-dependent Memory Propagation for Stochastic Dense Action Anticipation](https://arxiv.org/abs/2509.11394)
*Syed Talal Wasim,Hamid Suleman,Olga Zatsarynna,Muzammal Naseer,Juergen Gall*

Main category: cs.CV

TL;DR: MixANT是一种用于长期人类活动预测的新型架构，通过引入专家混合方法动态选择遗忘门（A矩阵），克服了现有状态空间模型（SSM）的局限性，并在多个数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有的状态空间模型（如Mamba）在处理输入依赖选择性方面表现出潜力，但其关键的遗忘门（A矩阵）是静态的，限制了其时间记忆控制和表示能力，尤其是在随机的长期人类活动预测方面。

Method: MixANT提出了一种专家混合方法，根据输入特征动态选择与上下文相关的A矩阵，从而使遗忘门机制具有输入依赖性，在不牺牲计算效率的情况下增强了表示能力。

Result: 在50Salads、Breakfast和Assembly101数据集上进行的广泛实验表明，MixANT在所有评估设置中均持续优于现有最先进的方法。

Conclusion: 研究结果强调了输入依赖的遗忘门机制对于在多样化真实世界场景中可靠预测人类行为的重要性。

Abstract: We present MixANT, a novel architecture for stochastic long-term dense
anticipation of human activities. While recent State Space Models (SSMs) like
Mamba have shown promise through input-dependent selectivity on three key
parameters, the critical forget-gate ($\textbf{A}$ matrix) controlling temporal
memory remains static. We address this limitation by introducing a mixture of
experts approach that dynamically selects contextually relevant $\textbf{A}$
matrices based on input features, enhancing representational capacity without
sacrificing computational efficiency. Extensive experiments on the 50Salads,
Breakfast, and Assembly101 datasets demonstrate that MixANT consistently
outperforms state-of-the-art methods across all evaluation settings. Our
results highlight the importance of input-dependent forget-gate mechanisms for
reliable prediction of human behavior in diverse real-world scenarios.

</details>


### [124] [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.11815)
*Haiduo Huang,Fuwei Yang,Zhenhua Liu,Xuanwu Yin,Dong Li,Pengju Ren,Emad Barsoum*

Main category: cs.CV

TL;DR: SpecVLM是一种针对视觉-语言模型（VLM）的推测解码系统，通过弹性视觉压缩和在线logit蒸馏，在保持无损解码的同时，实现了2.5-2.9倍的端到端推理加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码在大型语言模型（LLM）中表现出色，但直接应用于VLM面临独特系统限制：预填充阶段由视觉token主导，其数量随图像分辨率和视频长度增加，导致计算和内存（尤其是KV缓存）膨胀。

Method: 1. 建立了名为EagleVLM的强大EAGLE-2风格基线。2. 引入弹性视觉压缩器，自适应选择剪枝、池化、卷积和重采样器等原语，以平衡FLOPs/参数和每输入精度。3. 提出在线logit蒸馏协议，利用即时教师logit和倒数第二层特征，结合交叉熵和Smooth L1目标训练草稿模型，避免了昂贵的离线蒸馏语料库。4. 观察到训练时间缩放效应：更长的在线训练单调增加草稿模型的平均接受长度，提高推测效率。

Result: 1. EagleVLM基线比完全自回归推理实现了1.5-2.3倍的端到端加速。2. SpecVLM进一步加速，在LLaVA和MMMU数据集上，经过5个epochs的训练，实现了2.5-2.9倍的端到端加速。3. 加速效果在不同分辨率和任务难度下保持一致。4. 保留了目标模型的输出分布（无损解码）。

Conclusion: SpecVLM通过创新的弹性视觉压缩和高效的在线logit蒸馏协议，成功解决了VLM推测解码中的系统约束，实现了显著的推理加速，同时保持了模型的输出质量。

Abstract: Speculative decoding is a powerful way to accelerate autoregressive large
language models (LLMs), but directly porting it to vision-language models
(VLMs) faces unique systems constraints: the prefill stage is dominated by
visual tokens whose count scales with image resolution and video length,
inflating both compute and memory, especially the key-value (KV) cache. We
study speculative decoding for VLMs and introduce SpecVLM, a practical system
that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering
1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)
further accelerates VLM inference with an elastic visual compressor that
adaptively selects among pruning, pooling, convolution, and resampler
primitives to balance FLOPs/parameters and accuracy per input. To avoid costly
offline distillation corpora, we propose an online-logit distillation protocol
that trains the draft model with on-the-fly teacher logits and penultimate
features using a combined cross-entropy and Smooth L1 objective, eliminating
storage and preprocessing while remaining compute-efficient. This protocol
reveals a training-time scaling effect: longer online training monotonically
increases the draft model's average accepted length, improving speculative
efficiency. Empirically, SpecVLM achieves additional acceleration, culminating
in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,
consistently over resolutions and task difficulties, while preserving the
target model's output distribution (lossless decoding). Our code is available
at https://github.com/haiduo/SpecVLM.

</details>


### [125] [No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data](https://arxiv.org/abs/2509.11406)
*Christoph Fürböck,Paul Weiser,Branko Mitic,Philipp Seeböck,Thomas Helbich,Georg Langs*

Main category: cs.CV

TL;DR: 本文提出一种基于超网络的动态模型生成方法，以解决多模态医学图像数据中部分数据缺失的问题，该方法在处理不完整数据时表现出优越的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 在真实的临床环境中，深度学习模型在多模态医学图像数据上训练和应用时，常面临数据部分缺失的挑战。标准方法（如丢弃缺失样本、进行插补或重新利用dropout方案）限制了模型的鲁棒性和泛化能力。

Method: 本文提出一种基于超网络（hypernetwork）的方法。该超网络根据可用的模态集合，动态生成特定任务的分类模型参数。它学习预测适应可用模态的任务模型参数，从而允许在所有样本（无论数据完整性如何）上进行训练和推理。研究将此方法与仅在完整数据上训练的模型、最先进的通道dropout方法和基于插补的方法进行了比较。

Result: 结果表明，该方法具有卓越的适应性，在完整性为25%（75%的训练数据存在模态缺失）的数据集上训练时，其准确率比最先进的方法提高了高达8%。

Conclusion: 通过使一个单一模型能够泛化到所有模态配置，本文的方法为真实世界的多模态医学数据分析提供了一个高效的解决方案。

Abstract: In real world clinical environments, training and applying deep learning
models on multi-modal medical imaging data often struggles with partially
incomplete data. Standard approaches either discard missing samples, require
imputation or repurpose dropout learning schemes, limiting robustness and
generalizability. To address this, we propose a hypernetwork-based method that
dynamically generates task-specific classification models conditioned on the
set of available modalities. Instead of training a fixed model, a hypernetwork
learns to predict the parameters of a task model adapted to available
modalities, enabling training and inference on all samples, regardless of
completeness. We compare this approach with (1) models trained only on complete
data, (2) state of the art channel dropout methods, and (3) an imputation-based
method, using artificially incomplete datasets to systematically analyze
robustness to missing modalities. Results demonstrate superior adaptability of
our method, outperforming state of the art approaches with an absolute increase
in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of
training data with missing modalities). By enabling a single model to
generalize across all modality configurations, our approach provides an
efficient solution for real-world multi-modal medical data analysis.

</details>


### [126] [Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network](https://arxiv.org/abs/2509.11838)
*Navid Hashemi,Samuel Sasaki,Diego Manzanas Lopez,Ipek Oguz,Meiyi Ma,Taylor T. Johnson*

Main category: cs.CV

TL;DR: 本文提出了一种可扩展、与架构无关的概率验证框架，结合采样可达性分析和共形推断，为语义分割网络提供可靠且不那么保守的安全保障，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 语义分割网络在医疗成像、自动驾驶等安全关键领域至关重要，但在不确定性下需要可靠的模型行为。现有概率验证方法难以扩展到现代分割任务的复杂性和高维度，且提供的保障过于保守，不切实际。

Method: 该方法结合了基于采样的可达性分析与共形推断（CI），以提供可证明的保障，同时避免了现有方法的过度保守性。为克服CI在高维设置中的局限性，提出了新颖策略以在不牺牲严谨性的前提下减少保守性。该框架与架构无关。

Result: 在大规模分割模型（CamVid、OCTA-500、肺分割、Cityscapes）上的实证评估表明，该方法提供了可靠的安全保障，并相较于现有最先进技术显著收紧了边界。同时提供了一个实现该技术的工具箱。

Conclusion: 本研究引入了一个新颖的概率验证框架，能够为语义分割网络提供可扩展、与架构无关且更实际、更严格的安全保障，解决了现有方法在复杂性和保守性方面的局限性。

Abstract: Semantic segmentation networks (SSNs) play a critical role in domains such as
medical imaging, autonomous driving, and environmental monitoring, where safety
hinges on reliable model behavior under uncertainty. Yet, existing
probabilistic verification approaches struggle to scale with the complexity and
dimensionality of modern segmentation tasks, often yielding guarantees that are
too conservative to be practical. We introduce a probabilistic verification
framework that is both architecture-agnostic and scalable to high-dimensional
outputs. Our approach combines sampling-based reachability analysis with
conformal inference (CI) to deliver provable guarantees while avoiding the
excessive conservatism of prior methods. To counteract CI's limitations in
high-dimensional settings, we propose novel strategies that reduce conservatism
without compromising rigor. Empirical evaluation on large-scale segmentation
models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates
that our method provides reliable safety guarantees while substantially
tightening bounds compared to SOTA. We also provide a toolbox implementing this
technique, available on Github.

</details>


### [127] [On the Skinning of Gaussian Avatars](https://arxiv.org/abs/2509.11411)
*Nikolaos Zioulis,Nikolaos Kotarelas,Georgios Albanis,Spyridon Thermos,Anargyros Chatzitofis*

Main category: cs.CV

TL;DR: 本文提出了一种基于四元数平均的加权旋转混合方法，以解决高斯泼溅在人体化身重建中线性混合蒙皮无法正确处理非线性旋转属性的问题，从而实现更简单、高效且易于集成的顶点高斯动画。


<details>
  <summary>Details</summary>
Motivation: 神经辐射场在人体化身重建中渲染慢且逆向映射困难。高斯泼溅解决了这些问题，但其变形所需的线性混合蒙皮（LBS）无法正确处理高斯点的非线性旋转特性，导致伪影。现有解决方案（如使用网格属性或预测校正偏移）较为复杂。

Method: 作者提出了一种加权旋转混合方法，该方法利用四元数平均来处理高斯点的旋转。通过仅修改线性混合蒙皮技术，并结合任何高斯光栅化器，实现顶点高斯的动画。

Result: 该方法产生了更简单的顶点高斯，可以高效地进行动画制作，并且通过仅修改线性混合蒙皮技术，可以轻松集成到任何引擎中，同时使用任何高斯光栅化器。

Conclusion: 所提出的加权旋转混合方法通过利用四元数平均，有效地解决了高斯泼溅在人体化身动画中线性混合蒙皮造成的非线性旋转伪影问题，提供了一种更简单、高效且易于集成的解决方案。

Abstract: Radiance field-based methods have recently been used to reconstruct human
avatars, showing that we can significantly downscale the systems needed for
creating animated human avatars. Although this progress has been initiated by
neural radiance fields, their slow rendering and backward mapping from the
observation space to the canonical space have been the main challenges. With
Gaussian splatting overcoming both challenges, a new family of approaches has
emerged that are faster to train and render, while also straightforward to
implement using forward skinning from the canonical to the observation space.
However, the linear blend skinning required for the deformation of the
Gaussians does not provide valid results for their non-linear rotation
properties. To address such artifacts, recent works use mesh properties to
rotate the non-linear Gaussian properties or train models to predict corrective
offsets. Instead, we propose a weighted rotation blending approach that
leverages quaternion averaging. This leads to simpler vertex-based Gaussians
that can be efficiently animated and integrated in any engine by only modifying
the linear blend skinning technique, and using any Gaussian rasterizer.

</details>


### [128] [Bridging Vision Language Models and Symbolic Grounding for Video Question Answering](https://arxiv.org/abs/2509.11862)
*Haodi Ma,Vyom Pathak,Daisy Zhe Wang*

Main category: cs.CV

TL;DR: 本文提出SG-VLM框架，通过将符号场景图作为中间接地信号，与冻结的视觉语言模型（VLM）结合，以提高视频问答（VQA）中的因果和时间推理能力，并在多个基准测试中取得了改进。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型（VLM）在视频问答（VQA）中虽然表现出色，但往往依赖浅层关联，导致时间定位能力弱且可解释性有限。研究者认为符号场景图（SG）可以提供结构化的对象-关系表示，从而弥补VLM整体推理的不足。

Method: SG-VLM是一个模块化框架，它通过提示（prompting）和视觉定位（visual localization）将冻结的VLM与场景图接地（grounding）相结合。该方法在NExT-QA、iVQA和ActivityNet-QA三个基准测试以及QwenVL、InternVL等多个VLM上进行了评估。

Result: SG-VLM改进了因果和时间推理，并超越了先前的基线模型。然而，相对于强大的VLM，其性能提升有限。

Conclusion: 这些发现突出了符号接地在视频理解中的潜力和局限性，并为未来VLM与符号方法混合的视频理解研究提供了指导。

Abstract: Video Question Answering (VQA) requires models to reason over spatial,
temporal, and causal cues in videos. Recent vision language models (VLMs)
achieve strong results but often rely on shallow correlations, leading to weak
temporal grounding and limited interpretability. We study symbolic scene graphs
(SGs) as intermediate grounding signals for VQA. SGs provide structured
object-relation representations that complement VLMs holistic reasoning. We
introduce SG-VLM, a modular framework that integrates frozen VLMs with scene
graph grounding via prompting and visual localization. Across three benchmarks
(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM
improves causal and temporal reasoning and outperforms prior baselines, though
gains over strong VLMs are limited. These findings highlight both the promise
and current limitations of symbolic grounding, and offer guidance for future
hybrid VLM-symbolic approaches in video understanding.

</details>


### [129] [Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery](https://arxiv.org/abs/2509.11436)
*Jeanny Pan,Philipp Seeböck,Christoph Fürböck,Svitlana Pochepnia,Jennifer Straub,Lucian Beer,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 该研究提出了一种通过事后旋转数据潜在空间来主动学习域偏移的方法，以解耦医学图像中的生物学和技术因素。这使得能够发现稳定的、具有生物学意义的聚类，并显著提高了聚类一致性，最终增强了生存预测。


<details>
  <summary>Details</summary>
Motivation: 机器学习在医学影像数据中识别疾病相关模式的能力受到成像技术（供应商、扫描/重建参数）引起的域偏移的阻碍。这些偏移掩盖了生物学上有意义的聚类，使得数据表示学习和生物标志物发现变得困难。

Method: 该研究引入了一种方法，通过对数据潜在空间进行事后旋转，主动学习域偏移。这种方法旨在解耦数据中的生物学和技术因素，从而实现更清晰的表示。

Result: 在真实的异构临床数据上，学习到的解耦表示产生了跨不同采集设置的稳定组织类型聚类。与纠缠表示相比，聚类一致性显著提高（ARI +19.01%，NMI +16.85%，Dice +12.39%），并且优于四种最先进的协调方法。将这些聚类用于量化特发性肺纤维化患者的组织成分时，学习到的特征谱增强了Cox生存预测。

Conclusion: 所提出的无标签框架通过解耦生物学和技术因素，促进了多中心常规影像数据中的生物标志物发现。这表明该方法在处理医学图像异质性方面的有效性及其在临床应用中的潜力。

Abstract: Identifying new disease-related patterns in medical imaging data with the
help of machine learning enlarges the vocabulary of recognizable findings. This
supports diagnostic and prognostic assessment. However, image appearance varies
not only due to biological differences, but also due to imaging technology
linked to vendors, scanning- or re- construction parameters. The resulting
domain shifts impedes data representation learning strategies and the discovery
of biologically meaningful cluster appearances. To address these challenges, we
introduce an approach to actively learn the domain shift via post-hoc rotation
of the data latent space, enabling disentanglement of biological and technical
factors. Results on real-world heterogeneous clinical data showcase that the
learned disentangled representation leads to stable clusters representing
tissue-types across different acquisition settings. Cluster consistency is
improved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to the
entangled representation, outperforming four state-of-the-art harmonization
methods. When using the clusters to quantify tissue composition on idiopathic
pulmonary fibrosis patients, the learned profiles enhance Cox survival
prediction. This indicates that the proposed label-free framework facilitates
biomarker discovery in multi-center routine imaging data. Code is available on
GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.

</details>


### [130] [Integrating Prior Observations for Incremental 3D Scene Graph Prediction](https://arxiv.org/abs/2509.11895)
*Marian Renz,Felix Igelbrink,Martin Atzmueller*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的异构图模型，用于增量式3D语义场景图（3DSSG）预测，该模型将多模态信息（如先验观测和语义嵌入）直接整合到消息传递过程中，适用于复杂真实的增量环境。


<details>
  <summary>Details</summary>
Motivation: 现有3DSSG方法主要依赖传感器数据，未能充分整合语义丰富的环境信息，且大多需要完整的场景重建，限制了其在真实世界、增量式应用中的适用性。

Method: 引入了一种新颖的异构图模型，用于增量式3DSSG预测。该模型通过消息传递过程直接整合多模态信息（如先验观测和语义嵌入，例如CLIP）。它利用多层结构灵活地整合全局和局部场景表示，无需专门模块或完整的场景重建。

Result: 在3DSSG数据集上的评估表明，富含多模态信息（如语义嵌入和先验观测）的GNN为复杂、真实世界环境提供了可扩展且可泛化的解决方案。

Conclusion: 所提出的多模态、增量式异构图模型为在复杂真实世界环境中进行3DSSG预测提供了一种可扩展且可泛化的解决方案。

Abstract: 3D semantic scene graphs (3DSSG) provide compact structured representations
of environments by explicitly modeling objects, attributes, and relationships.
While 3DSSGs have shown promise in robotics and embodied AI, many existing
methods rely mainly on sensor data, not integrating further information from
semantically rich environments. Additionally, most methods assume access to
complete scene reconstructions, limiting their applicability in real-world,
incremental settings. This paper introduces a novel heterogeneous graph model
for incremental 3DSSG prediction that integrates additional, multi-modal
information, such as prior observations, directly into the message-passing
process. Utilizing multiple layers, the model flexibly incorporates global and
local scene representations without requiring specialized modules or full scene
reconstructions. We evaluate our approach on the 3DSSG dataset, showing that
GNNs enriched with multi-modal information such as semantic embeddings (e.g.,
CLIP) and prior observations offer a scalable and generalizable solution for
complex, real-world environments. The full source code of the presented
architecture will be made available at
https://github.com/m4renz/incremental-scene-graph-prediction.

</details>


### [131] [MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder](https://arxiv.org/abs/2509.11442)
*Ayhan Can Erdur,Christian Beischl,Daniel Scholz,Jiazhen Pan,Benedikt Wiestler,Daniel Rueckert,Jan C Peeken*

Main category: cs.CV

TL;DR: 该研究提出了一种基于MultiMAE的掩码自编码器（MAE）范式，用于3D脑部MRI的多模态、多任务学习，旨在解决医学影像中输入序列缺失的问题，并学习鲁棒的表示。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据中常见的输入序列缺失问题，对依赖完整输入数据的深度学习模型构成了挑战。

Method: 受MultiMAE启发，开发了一个用于3D脑部MRI的多模态、多任务学习的掩码自编码器（MAE）范式。该方法将每个MRI序列视为独立的输入模态，利用后期融合（late-fusion）风格的Transformer编码器整合多序列信息，并为每个模态使用独立的解码器流进行多任务重建。这种预训练策略通过跨序列推理使模型能够处理缺失输入。

Result: 该方法产生了一个灵活且泛化性强的脑部MRI编码器，能够从可用输入中推断缺失序列，并可适应各种下游应用。在下游分割和分类任务中，面对缺失输入序列时，相较于MAE-ViT基线，Dice分数绝对提升了10.1，MCC提升了0.46。

Conclusion: 该预训练策略表现出强大的性能和鲁棒性，能够有效处理缺失数据并学习丰富的模态表示，具有良好的泛化能力和适应性。

Abstract: Missing input sequences are common in medical imaging data, posing a
challenge for deep learning models reliant on complete input data. In this
work, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm
for multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our
method treats each MRI sequence as a separate input modality, leveraging a
late-fusion-style transformer encoder to integrate multi-sequence information
(multi-modal) and individual decoder streams for each modality for multi-task
reconstruction. This pretraining strategy guides the model to learn rich
representations per modality while also equipping it to handle missing inputs
through cross-sequence reasoning. The result is a flexible and generalizable
encoder for brain MRIs that infers missing sequences from available inputs and
can be adapted to various downstream applications. We demonstrate the
performance and robustness of our method against an MAE-ViT baseline in
downstream segmentation and classification tasks, showing absolute improvement
of $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing
input sequences. Our experiments demonstrate the strength of this pretraining
strategy. The implementation is made available.

</details>


### [132] [Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision](https://arxiv.org/abs/2509.11476)
*Tianyao Sun,Dawei Xiang,Tianqi Ding,Xiang Fang,Yijiashun Qi,Zunduo Zhao*

Main category: cs.CV

TL;DR: 本文提出了FusionNet，一个端到端的红外与可见光图像融合框架，通过模态感知注意力、像素级Alpha混合和目标感知损失，实现语义增强、高质量且可解释的融合图像。


<details>
  <summary>Details</summary>
Motivation: 红外与可见光图像融合（IVIF）是多模态感知中的一项基本任务，旨在整合来自不同光谱域的互补结构和纹理信息。当前挑战在于如何有效建模模态间交互、增强任务关键区域，并确保融合图像的语义一致性。

Method: FusionNet是一个端到端的融合框架，包含以下核心机制：1) 模态感知注意力机制，动态调整红外和可见光特征的贡献；2) 像素级Alpha混合模块，学习自适应且内容感知的空间可变融合权重；3) 目标感知损失函数，利用弱ROI监督来保持包含重要对象区域的语义一致性。

Result: 在公共M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留能力、高感知质量和清晰的可解释性。

Conclusion: FusionNet为语义感知的多模态图像融合提供了一个通用且可扩展的解决方案，对目标检测和场景理解等下游任务具有潜在益处。

Abstract: Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal
perception that aims to integrate complementary structural and textural cues
from different spectral domains. In this paper, we propose FusionNet, a novel
end-to-end fusion framework that explicitly models inter-modality interaction
and enhances task-critical regions. FusionNet introduces a modality-aware
attention mechanism that dynamically adjusts the contribution of infrared and
visible features based on their discriminative capacity. To achieve
fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha
blending module, which learns spatially-varying fusion weights in an adaptive
and content-aware manner. Moreover, we formulate a target-aware loss that
leverages weak ROI supervision to preserve semantic consistency in regions
containing important objects (e.g., pedestrians, vehicles). Experiments on the
public M3FD dataset demonstrate that FusionNet generates fused images with
enhanced semantic preservation, high perceptual quality, and clear
interpretability. Our framework provides a general and extensible solution for
semantic-aware multi-modal image fusion, with benefits for downstream tasks
such as object detection and scene understanding.

</details>


### [133] [Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing](https://arxiv.org/abs/2509.12040)
*Bingyu Li,Haocheng Dong,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 本文针对遥感开放词汇图像分割（OVRSIS）任务，提出了一个统一的评估基准OVRSISBench，并开发了新的框架RSKT-Seg，通过多方向成本图聚合、高效成本图融合和遥感知识迁移模块，显著提升了分割性能和推理速度。


<details>
  <summary>Details</summary>
Motivation: 遥感开放词汇图像分割（OVRSIS）作为一个新兴任务，由于缺乏统一的评估基准以及自然图像与遥感图像之间的领域差异，尚未得到充分探索。

Method: 1. 建立了基于常用遥感分割数据集的标准化OVRSIS评估基准（OVRSISBench）。2. 在该基准上全面评估了现有代表性OVS/OVRSIS模型，揭示其局限性。3. 提出了专门为遥感定制的开放词汇分割框架RSKT-Seg，包含三个关键组件：多方向成本图聚合（RS-CMA）模块以捕获旋转不变视觉线索；高效成本图融合（RS-Fusion）Transformer以建模空间和语义依赖；遥感知识迁移（RS-Transfer）模块以注入预训练知识并促进领域适应。

Result: 在OVRSISBench上，RSKT-Seg在mIoU上比现有强OVS基线高出3.8个百分点，在mACC上高出5.9个百分点，同时通过高效聚合实现了2倍更快的推理速度。

Conclusion: RSKT-Seg是一个有效且高效的遥感开放词汇分割解决方案，它通过新颖的模块设计，成功解决了现有模型在遥感场景中的局限性，并在性能和速度上均超越了基线方法。

Abstract: Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task
that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS)
domain, remains underexplored due to the absence of a unified evaluation
benchmark and the domain gap between natural and RS images. To bridge these
gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench})
based on widely-used RS segmentation datasets, enabling consistent evaluation
across methods. Using this benchmark, we comprehensively evaluate several
representative OVS/OVRSIS models and reveal their limitations when directly
applied to remote sensing scenarios. Building on these insights, we propose
\textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for
remote sensing. RSKT-Seg integrates three key components: (1) a
Multi-Directional Cost Map Aggregation (RS-CMA) module that captures
rotation-invariant visual cues by computing vision-language cosine similarities
across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion)
transformer, which jointly models spatial and semantic dependencies with a
lightweight dimensionality reduction strategy; and (3) a Remote Sensing
Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and
facilitates domain adaptation via enhanced upsampling. Extensive experiments on
the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines
by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through
efficient aggregation. Our code is
\href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.

</details>


### [134] [Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis](https://arxiv.org/abs/2509.11526)
*Wenhao Tang,Sheng Huang,Heng Fang,Fengtao Zhou,Bo Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MHIM-MIL的新型多实例学习（MIL）框架，用于计算病理学中的全玻片图像分析。该框架通过掩蔽显著实例并挖掘困难实例，解决了现有MIL方法偏向易分类实例的问题，从而提高了癌症诊断、亚型分类和生存分析的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学中的多实例学习（MIL）方法，由于注意力机制，通常侧重于识别显著实例，导致偏向于易分类的实例，而忽略了对准确建模判别边界至关重要的挑战性（困难）实例。

Method: MHIM-MIL框架采用Siamese结构和一致性约束来探索困难实例。它利用动量教师模型，根据类别感知的实例概率掩蔽显著实例，从而隐式地挖掘困难实例供学生模型训练。为获得多样化、非冗余的困难实例，该方法采用大规模随机掩蔽，并结合全局回收网络以降低丢失关键特征的风险。此外，学生模型通过指数移动平均（EMA）更新教师模型，以在后续训练迭代中识别新的困难实例并稳定优化。

Result: MHIM-MIL在癌症诊断、亚型分类、生存分析任务以及12个基准测试中，在性能和效率方面均优于最新的方法。

Conclusion: MHIM-MIL通过创新的掩蔽困难实例挖掘机制，有效解决了计算病理学中多实例学习对易分类实例的偏见问题，显著提升了全玻片图像分析的准确性和效率。

Abstract: Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has
opened new avenues for Computational Pathology (CPath). As positive tissue
comprises only a small fraction of gigapixel WSIs, existing Multiple Instance
Learning (MIL) methods typically focus on identifying salient instances via
attention mechanisms. However, this leads to a bias towards easy-to-classify
instances while neglecting challenging ones. Recent studies have shown that
hard examples are crucial for accurately modeling discriminative boundaries.
Applying such an idea at the instance level, we elaborate a novel MIL framework
with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure
with a consistency constraint to explore the hard instances. Using a
class-aware instance probability, MHIM-MIL employs a momentum teacher to mask
salient instances and implicitly mine hard instances for training the student
model. To obtain diverse, non-redundant hard instances, we adopt large-scale
random masking while utilizing a global recycle network to mitigate the risk of
losing key features. Furthermore, the student updates the teacher using an
exponential moving average, which identifies new hard instances for subsequent
training iterations and stabilizes optimization. Experimental results on cancer
diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate
that MHIM-MIL outperforms the latest methods in both performance and
efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.

</details>


### [135] [Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking](https://arxiv.org/abs/2509.12046)
*Zirui Zheng,Takashi Isobe,Tong Shen,Xu Jia,Jianbin Zhao,Xiaomin Li,Mengmeng Ge,Baolu Li,Qinghe Wang,Dong Li,Dong Zhou,Yunzhi Zhuge,Huchuan Lu,Emad Barsoum*

Main category: cs.CV

TL;DR: SMARLI是一个新颖的框架，通过结构化掩码和GRPO后训练，解决了自回归模型在布局到图像生成中面临的稀疏条件和特征纠缠问题，实现了卓越的布局感知控制和生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）模型在图像生成方面表现出色，但将其扩展到布局条件生成时，由于布局条件的稀疏性和特征纠缠的风险，仍然面临挑战。

Method: 提出了SMARLI框架，通过以下方法实现布局到图像生成：1) 应用专门设计的结构化掩码策略到注意力计算中，以管理全局提示、布局和图像token之间的交互，防止不同区域及其描述之间的错误关联，同时充分注入布局约束。2) 结合基于组相对策略优化（GRPO）的后训练方案，并为基于下一集合的AR模型设计了特殊的布局奖励函数，以进一步提升生成质量和布局准确性。

Result: 实验结果表明，SMARLI能够无缝地将布局token与文本和图像token集成，而不会影响生成质量。它在保持AR模型结构简单性和生成效率的同时，实现了卓越的布局感知控制。

Conclusion: SMARLI框架有效地将空间布局约束集成到基于AR的图像生成中，通过结构化掩码和GRPO后训练，解决了现有挑战，实现了高质量、布局精确且高效的图像生成。

Abstract: While autoregressive (AR) models have demonstrated remarkable success in
image generation, extending them to layout-conditioned generation remains
challenging due to the sparse nature of layout conditions and the risk of
feature entanglement. We present Structured Masking for AR-based
Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that
effectively integrates spatial layout constraints into AR-based image
generation. To equip AR model with layout control, a specially designed
structured masking strategy is applied to attention computation to govern the
interaction among the global prompt, layout, and image tokens. This design
prevents mis-association between different regions and their descriptions while
enabling sufficient injection of layout constraints into the generation
process. To further enhance generation quality and layout accuracy, we
incorporate Group Relative Policy Optimization (GRPO) based post-training
scheme with specially designed layout reward functions for next-set-based AR
models. Experimental results demonstrate that SMARLI is able to seamlessly
integrate layout tokens with text and image tokens without compromising
generation quality. It achieves superior layoutaware control while maintaining
the structural simplicity and generation efficiency of AR models.

</details>


### [136] [SFGNet: Semantic and Frequency Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2509.11539)
*Dezhen Wang,Haixiang Zhao,Xiang Shen,Sheng Miao*

Main category: cs.CV

TL;DR: 本文提出了一种语义与频率引导网络（SFGNet），通过结合语义提示和频域特征来提高伪装目标检测的性能，尤其在处理复杂背景和模糊边界方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的伪装目标检测方法忽略了不同目标文本提示之间的语义差异以及细粒度频率特征，导致对伪装目标的边界感知能力不足。

Method: 本文提出了语义与频率引导网络（SFGNet），该网络结合了语义提示和频域特征以捕获伪装目标并改善边界感知。具体方法包括：1) 设计多波段傅里叶模块（MBFM）以增强网络处理复杂背景和模糊边界的能力；2) 设计交互式结构增强模块（ISEB）以确保预测结果的结构完整性和边界细节。

Result: 在三个伪装目标检测基准数据集上进行的广泛实验表明，所提出的方法显著优于现有最先进的方法。

Conclusion: SFGNet通过有效结合语义提示和频域特征，显著提升了伪装目标检测的性能，尤其在解决复杂背景和边界模糊问题上取得了突破，确保了预测的结构完整性和边界细节。

Abstract: Camouflaged object detection (COD) aims to segment objects that blend into
their surroundings. However, most existing studies overlook the semantic
differences among textual prompts of different targets as well as fine-grained
frequency features. In this work, we propose a novel Semantic and Frequency
Guided Network (SFGNet), which incorporates semantic prompts and
frequency-domain features to capture camouflaged objects and improve boundary
perception. We further design Multi-Band Fourier Module(MBFM) to enhance the
ability of the network in handling complex backgrounds and blurred boundaries.
In addition, we design an Interactive Structure Enhancement Block (ISEB) to
ensure structural integrity and boundary details in the predictions. Extensive
experiments conducted on three COD benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches. The core code of
the model is available at the following link:
https://github.com/winter794444/SFGNetICASSP2026.

</details>


### [137] [A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset](https://arxiv.org/abs/2509.12047)
*Haiyu Yang,Enhong Liu,Jennifer Sun,Sumit Sharma,Meike van Leerdam,Sebastien Franceschini,Puchun Niu,Miel Hostens*

Main category: cs.CV

TL;DR: 该论文提出了一种模块化计算机视觉管道，用于自动化群体饲养环境中的动物行为分析，特别是在室内猪监测中，实现了高精度和鲁棒性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的动物行为观察方法耗时、主观且难以扩展，限制了在农业环境中对动物福利、健康状况和生产力进行有效评估。

Method: 该方法采用模块化管道，整合了最先进的计算机视觉技术，包括零样本目标检测、运动感知跟踪和分割，以及使用视觉Transformer进行高级特征提取，以实现鲁棒的行为识别。该管道旨在解决动物遮挡和群体饲养场景中的挑战。

Result: 在爱丁堡猪行为视频数据集上，时间模型实现了94.2%的总体准确率，比现有方法提高了21.2个百分点。管道展现了强大的跟踪能力，身份保持得分达到93.3%，目标检测精度为89.3%。

Conclusion: 该开源实现提供了一个可扩展的自动化、客观、连续的行为监测解决方案，有助于精准养猪和福利评估。其模块化设计具有适应其他场景的潜力，但需要进一步的跨物种验证。

Abstract: Animal behavior analysis plays a crucial role in understanding animal
welfare, health status, and productivity in agricultural settings. However,
traditional manual observation methods are time-consuming, subjective, and
limited in scalability. We present a modular pipeline that leverages
open-sourced state-of-the-art computer vision techniques to automate animal
behavior analysis in a group housing environment. Our approach combines
state-of-the-art models for zero-shot object detection, motion-aware tracking
and segmentation, and advanced feature extraction using vision transformers for
robust behavior recognition. The pipeline addresses challenges including animal
occlusions and group housing scenarios as demonstrated in indoor pig
monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset
for multiple behavioral tasks. Our temporal model achieved 94.2% overall
accuracy, representing a 21.2 percentage point improvement over existing
methods. The pipeline demonstrated robust tracking capabilities with 93.3%
identity preservation score and 89.3% object detection precision. The modular
design suggests potential for adaptation to other contexts, though further
validation across species would be required. The open-source implementation
provides a scalable solution for behavior monitoring, contributing to precision
pig farming and welfare assessment through automated, objective, and continuous
analysis.

</details>


### [138] [How Auxiliary Reasoning Unleashes GUI Grounding in VLMs](https://arxiv.org/abs/2509.11548)
*Weiming Li,Yan Shao,Jing Yang,Yujing Lu,Ling Zhong,Yuhan Wang,Manni Duan*

Main category: cs.CV

TL;DR: 本文提出了一种零样本辅助推理方法，通过在输入图像中添加明确的空间线索（如坐标轴、网格），帮助视觉语言模型（VLMs）更好地表达其隐式空间理解能力，从而显著提升了图形用户界面（GUI）定位任务的性能。


<details>
  <summary>Details</summary>
Motivation: GUI定位是构建GUI代理的关键任务。通用视觉语言模型（VLMs）在此任务上表现不佳，原因在于它们缺乏特定的优化。尽管VLMs具有显著的潜在定位能力（通过指向游戏衡量），但在输出明确坐标时表现不佳。此外，当前的微调方法存在高昂的数据和标注成本。

Method: 为了解决VLMs在输出明确坐标方面的不足并规避微调的高成本，本文提出了三种零样本辅助推理方法。这些方法通过在输入图像中提供明确的空间线索，例如坐标轴、网格和带标签的交点，使VLMs能够表达其隐式空间理解能力。

Result: 本文在四个GUI定位基准上，对七个开源和专有VLMs进行了评估。评估结果表明，所提出的方法显著提高了GUI定位的性能。

Conclusion: 通过提供明确的空间线索作为输入，零样本辅助推理方法能有效帮助视觉语言模型（VLMs）利用其隐式空间理解能力，从而大幅提升GUI定位任务的表现。

Abstract: Graphical user interface (GUI) grounding is a fundamental task for building
GUI agents. However, general vision-language models (VLMs) struggle with this
task due to a lack of specific optimization. We identify a key gap in this
paper: while VLMs exhibit significant latent grounding potential, as
demonstrated by their performance measured by Pointing Game, they underperform
when tasked with outputting explicit coordinates. To address this discrepancy,
and bypass the high data and annotation costs of current fine-tuning
approaches, we propose three zero-shot auxiliary reasoning methods. By
providing explicit spatial cues such as axes, grids and labeled intersections
as part of the input image, these methods enable VLMs to articulate their
implicit spatial understanding capabilities. We evaluate these methods on four
GUI grounding benchmarks across seven open-source and proprietary VLMs. The
evaluation results demonstrate that the proposed methods substantially improve
the performance of GUI grounding.

</details>


### [139] [U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT](https://arxiv.org/abs/2509.12069)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 本文提出U-Mamba2，一种结合Mamba2状态空间模型和U-Net的新型神经网络架构，用于牙科CBCT多解剖结构分割。它通过集成交互式提示、自监督预训练和领域知识，实现了高效且高性能的分割，并在ToothFairy3挑战赛中名列前三。


<details>
  <summary>Details</summary>
Motivation: 牙科CBCT图像中解剖结构的精确分割对于临床诊断和手术规划至关重要，但目前仍面临耗时且具有挑战性的难题。

Method: 本文提出了U-Mamba2架构，将Mamba2状态空间模型集成到U-Net中，以在不牺牲性能的情况下提高效率。此外，它还结合了带有交叉注意力块的交互式点击提示、通过自监督学习进行预训练，并融入牙科领域知识来解决CBCT牙科解剖分割的关键挑战。

Result: U-Mamba2在ToothFairy3挑战赛的两个任务中均获得前三名。在任务1中，使用保留测试数据获得了0.792的平均Dice系数和93.19的HD95；在任务2中，获得了0.852的平均Dice系数和7.39的HD95。实验证明其既有效又高效。

Conclusion: U-Mamba2是一种针对牙科CBCT多解剖结构分割的有效且高效的解决方案，其卓越的性能在ToothFairy3挑战赛中得到了验证，并已公开代码。

Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in
dentistry, providing volumetric information about the anatomical structures of
jaws and teeth. Accurate segmentation of these anatomies is critical for
clinical applications such as diagnosis and surgical planning, but remains
time-consuming and challenging. In this paper, we present U-Mamba2, a new
neural network architecture designed for multi-anatomy CBCT segmentation in the
context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state
space models into the U-Net architecture, enforcing stronger structural
constraints for higher efficiency without compromising performance. In
addition, we integrate interactive click prompts with cross-attention blocks,
pre-train U-Mamba2 using self-supervised learning, and incorporate dental
domain knowledge into the model design to address key challenges of dental
anatomy segmentation in CBCT. Extensive experiments, including independent
tests, demonstrate that U-Mamba2 is both effective and efficient, securing top
3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2
achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with
an average inference time of XX (TBC during the ODIN workshop). In Task 2,
U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out
test data. The code is publicly available at
https://github.com/zhiqin1998/UMamba2.

</details>


### [140] [Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps](https://arxiv.org/abs/2509.11574)
*Zhexi Peng,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: 本文提出了一种高斯-SDF混合表示方法，名为GPS-SLAM，用于实时三维重建。它通过结合SDF处理平滑几何和高斯捕获细节，显著提高了高斯SLAM的速度（超过150帧/秒），同时保持了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯（Gaussian）的SLAM方法虽然能实现逼真的重建，但计算性能是瓶颈（低于20帧/秒），远低于KinectFusion等几何中心方法。这主要是因为需要大量高斯和复杂的迭代优化，导致计算负担重。

Method: 提出了一种高斯-SDF混合表示。SDF（符号距离场）通过RGB-D融合（类似于几何中心方法）高效构建，用于表示平滑的几何和外观；3D高斯则用于捕获未充分表示的细节，并进行迭代优化。这种方法通过避免全场景高斯建模，实现了高斯数量的显著减少（50%），并通过有针对性的外观优化，减少了高斯优化迭代次数（75%）。在此基础上开发了GPS-SLAM系统。

Result: GPS-SLAM系统在真实世界的Azure Kinect序列上实现了超过150帧/秒的速度，比现有最先进技术快了一个数量级，同时保持了可比的重建质量。

Conclusion: 所提出的高斯-SDF混合表示和GPS-SLAM系统显著提升了基于高斯SLAM的速度，实现了实时三维重建，且不牺牲重建质量，为未来的研究提供了新的方向。

Abstract: While recent Gaussian-based SLAM methods achieve photorealistic
reconstruction from RGB-D data, their computational performance remains a
critical bottleneck. State-of-the-art techniques operate at less than 20 fps,
significantly lagging behind geometry-centric approaches like KinectFusion
(hundreds of fps). This limitation stems from the heavy computational burden:
modeling scenes requires numerous Gaussians and complex iterative optimization
to fit RGB-D data, where insufficient Gaussian counts or optimization
iterations cause severe quality degradation. To address this, we propose a
Gaussian-SDF hybrid representation, combining a colorized Signed Distance Field
(SDF) for smooth geometry and appearance with 3D Gaussians to capture
underrepresented details. The SDF is efficiently constructed via RGB-D fusion
(as in geometry-centric methods), while Gaussians undergo iterative
optimization. Our representation enables drastic Gaussian reduction (50% fewer)
by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization
(75% fewer iterations) through targeted appearance refinement. Building upon
this representation, we develop GPS-SLAM (Gaussian-Plus-SDF SLAM), a real-time
3D reconstruction system achieving over 150 fps on real-world Azure Kinect
sequences -- delivering an order-of-magnitude speedup over state-of-the-art
techniques while maintaining comparable reconstruction quality. We will release
the source code and data to facilitate future research.

</details>


### [141] [Optimizing Class Distributions for Bias-Aware Multi-Class Learning](https://arxiv.org/abs/2509.11588)
*Mirco Felske,Stefan Stiene*

Main category: cs.CV

TL;DR: 本文提出了BiCDO（偏差控制类分布优化器），一个迭代的、以数据为中心的框架，用于识别多类图像分类中的Pareto优化类分布，以实现特定类别的性能优先级，并提高模型性能和平衡性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中，需要对特定类别进行性能优先级排序（例如，优先考虑“人类”而非“狗”）。传统的均匀分布无法满足此需求，且需要一种方法来优化每个类别的图像数量，以提高可靠性并最小化目标函数中的偏差和方差。

Method: 本文提出了BiCDO框架，通过迭代地识别Pareto优化的类分布来确定每个类别的最优图像数量。该方法旨在增强可靠性并最小化目标函数中的偏差和方差。BiCDO可以轻松集成到现有训练管道中，并支持任何带标签的多类数据集。作者使用EfficientNet、ResNet和ConvNeXt在CIFAR-10和iNaturalist21数据集上进行了验证。

Result: BiCDO在CIFAR-10和iNaturalist21数据集上，通过优化的数据分布，展示了改进且平衡的模型性能。

Conclusion: BiCDO通过优化多类图像分类中的类分布，能够有效实现特定类别的性能优先级控制，从而提高模型的可靠性和平衡性。它是一个易于集成且普适性强的框架。

Abstract: We propose BiCDO (Bias-Controlled Class Distribution Optimizer), an
iterative, data-centric framework that identifies Pareto optimized class
distributions for multi-class image classification. BiCDO enables performance
prioritization for specific classes, which is useful in safety-critical
scenarios (e.g. prioritizing 'Human' over 'Dog'). Unlike uniform distributions,
BiCDO determines the optimal number of images per class to enhance reliability
and minimize bias and variance in the objective function. BiCDO can be
incorporated into existing training pipelines with minimal code changes and
supports any labelled multi-class dataset. We have validated BiCDO using
EfficientNet, ResNet and ConvNeXt on CIFAR-10 and iNaturalist21 datasets,
demonstrating improved, balanced model performance through optimized data
distribution.

</details>


### [142] [MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment](https://arxiv.org/abs/2509.11589)
*Yanyun Pu,Kehan Li,Zeyi Huang,Zhijie Zhong,Kaixiang Yang*

Main category: cs.CV

TL;DR: 本文介绍了一个名为MVQA-68K的多维度视频质量评估（VQA）数据集，包含68K+视频，涵盖七个质量维度和思维链推理，显著提升了多模态大语言模型（MLLMs）在VQA任务上的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着Sora等视频生成模型的快速发展，视频质量评估对于从大规模数据集中筛选高质量视频进行预训练变得至关重要。传统的VQA方法通常只产生单一数值分数，缺乏全面性和可解释性。

Method: 研究者构建了MVQA-68K数据集，包含超过68,000个经过精心标注的视频，涵盖整体美学、镜头运动、动态程度、纹理细节、构图、视觉质量和事实一致性七个关键质量维度。每个标注都包含详细的思维链推理。该数据集用于训练多模态大语言模型（MLLMs）。

Result: 实验证明，MVQA-68K显著提升了各种多模态大语言模型在VQA任务上的性能，在内部测试集以及LSVQ-test、LSVQ-1080p和LIVE-VQC等公共基准测试中均达到了最先进的水平。同时，在VQA训练中融入显式推理过程，大幅提升了模型的零样本泛化能力。

Conclusion: MVQA-68K是一个新颖的多维度VQA数据集，通过提供细致的多维度标注和思维链推理，有效解决了传统VQA方法的局限性，显著提升了MLLMs在视频质量评估任务上的性能、可解释性和零样本泛化能力，对于高质量视频生成模型的预训练数据选择具有重要意义。

Abstract: With the rapid advancement of video generation models such as Sora, video
quality assessment (VQA) is becoming increasingly crucial for selecting
high-quality videos from large-scale datasets used in pre-training. Traditional
VQA methods, typically producing single numerical scores, often lack
comprehensiveness and interpretability. To address these challenges, we
introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over
68,000 carefully annotated videos, covering seven essential quality dimensions:
overall aesthetics, camera movement, dynamic degree, texture detail,
composition, visual quality, and factual consistency. Each annotation includes
detailed chain-of-thought reasoning to facilitate interpretability and
comprehensive understanding. Extensive experiments demonstrate that MVQA-68K
significantly enhances the performance of various multimodal large language
models (MLLMs) on the VQA task, achieving state-of-the-art results not only on
our internal test set (Fig.1) but also on public benchmarks including
LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning
process during VQA training substantially boosts the zero-shot generalization.
Code and dataset will be available at github:
https://github.com/Controller01-ai/MVQA-68K

</details>


### [143] [3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data](https://arxiv.org/abs/2509.12143)
*Nojod M. Alotaibi,Areej M. Alhothali,Manar S. Ali*

Main category: cs.CV

TL;DR: 该研究开发了一种结合Vision Transformers (ViTs)和图神经网络 (GNN) 的统一流程，用于基于结构磁共振成像 (sMRI) 自动检测重度抑郁症 (MDD)，并发现基于脑图谱的方法优于基于立方体的方法。


<details>
  <summary>Details</summary>
Motivation: 重度抑郁症 (MDD) 是一种普遍存在的精神疾病，对个人福祉和全球公共卫生造成负面影响。现有的基于sMRI和深度学习的MDD自动检测方法多采用体素级特征或预定义脑图谱的手工区域表示，限制了其捕捉复杂大脑模式的能力，因此需要改进诊断准确性和实现早期干预。

Method: 本研究开发了一个统一的流程：使用Vision Transformers (ViTs) 从sMRI数据中提取3D区域嵌入，并利用图神经网络 (GNN) 进行分类。探索了两种区域定义策略：1) 基于预定义结构和功能脑图谱的方法；2) 基于ViTs直接从均匀提取的3D块中识别区域的立方体方法。此外，生成余弦相似度图来建模区域间的关系，并指导基于GNN的分类。在REST-meta-MDD数据集上进行了广泛实验，采用分层10折交叉验证进行评估。

Result: 最佳模型在分层10折交叉验证中获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。此外，基于脑图谱的模型始终优于基于立方体的方法。

Conclusion: 该研究提出的ViT-GNN模型能有效检测MDD。结果强调了使用领域特定的解剖学先验（即基于脑图谱的方法）对于MDD检测的重要性。

Abstract: Major depressive disorder (MDD) is a prevalent mental health condition that
negatively impacts both individual well-being and global public health.
Automated detection of MDD using structural magnetic resonance imaging (sMRI)
and deep learning (DL) methods holds increasing promise for improving
diagnostic accuracy and enabling early intervention. Most existing methods
employ either voxel-level features or handcrafted regional representations
built from predefined brain atlases, limiting their ability to capture complex
brain patterns. This paper develops a unified pipeline that utilizes Vision
Transformers (ViTs) for extracting 3D region embeddings from sMRI data and
Graph Neural Network (GNN) for classification. We explore two strategies for
defining regions: (1) an atlas-based approach using predefined structural and
functional brain atlases, and (2) an cube-based method by which ViTs are
trained directly to identify regions from uniformly extracted 3D patches.
Further, cosine similarity graphs are generated to model interregional
relationships, and guide GNN-based classification. Extensive experiments were
conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of
our model. With stratified 10-fold cross-validation, the best model obtained
78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and
78.98% F1-score. Further, atlas-based models consistently outperformed the
cube-based approach, highlighting the importance of using domain-specific
anatomical priors for MDD detection.

</details>


### [144] [Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework](https://arxiv.org/abs/2509.11598)
*Siming Fu,Sijun Dong,Xiaoliang Meng*

Main category: cs.CV

TL;DR: 自监督学习（SSL）的泛化能力受捷径学习（Shortcut Learning）阻碍，模型利用表面特征而非内在结构。本文验证了生成范式中的此问题，并提出HyGDL混合框架，通过显式内容-风格解耦和不变性预训练原则，从根本上解决捷径依赖。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）的泛化能力因捷径学习而受限，模型倾向于利用纹理等表面特征而非内在结构。这导致模型在未见域上失效，且现有方法未能从根本上改变导致捷径依赖的学习机制。

Method: 提出HyGDL（混合生成-判别学习框架），一个混合框架，实现显式的内容-风格解耦。其核心是“不变性预训练原则”：通过系统地改变输入偏差（如风格）同时保持监督信号不变，迫使模型学习不变的本质。HyGDL使用单个编码器，并通过向量投影将风格解析为表示中与风格不变内容正交的组件。

Result: 实验验证了生成范式（如MAE）中捷径学习的缺陷，并指出这是判别方法在未见域上失败的根本原因。HyGDL旨在从根本上解决这一问题，通过内容-风格解耦实现不变性学习，从而克服捷径依赖。

Conclusion: 捷径学习是自监督学习泛化能力的根本障碍。通过提出HyGDL框架及其不变性预训练原则，研究旨在通过显式的内容-风格解耦，从根本上改变学习机制，从而克服模型对表面特征的依赖，提升在未见域上的泛化能力。

Abstract: Despite the remarkable success of Self-Supervised Learning (SSL), its
generalization is fundamentally hindered by Shortcut Learning, where models
exploit superficial features like texture instead of intrinsic structure. We
experimentally verify this flaw within the generative paradigm (e.g., MAE) and
argue it is a systemic issue also affecting discriminative methods, identifying
it as the root cause of their failure on unseen domains. While existing methods
often tackle this at a surface level by aligning or separating domain-specific
features, they fail to alter the underlying learning mechanism that fosters
shortcut dependency. To address this at its core, we propose HyGDL (Hybrid
Generative-Discriminative Learning Framework), a hybrid framework that achieves
explicit content-style disentanglement. Our approach is guided by the
Invariance Pre-training Principle: forcing a model to learn an invariant
essence by systematically varying a bias (e.g., style) at the input while
keeping the supervision signal constant. HyGDL operates on a single encoder and
analytically defines style as the component of a representation that is
orthogonal to its style-invariant content, derived via vector projection.

</details>


### [145] [Multi Anatomy X-Ray Foundation Model](https://arxiv.org/abs/2509.12146)
*Nishank Singla,Krisztian Koos,Farzin Haddadpour,Amin Honarmandi Shandiz,Lovish Chum,Xiaojian Xu,Qing Jin,Erhan Bas*

Main category: cs.CV

TL;DR: XR-0是一个多解剖区域X射线基础模型，通过自监督学习在包含115万张图像的私有数据集上训练，并在20项下游任务中表现出最先进的性能，证明了多样性对通用医疗AI的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI基础模型在X射线影像领域大多局限于胸部解剖结构，难以泛化到更广泛的临床任务。

Method: 研究引入了XR-0，一个多解剖区域X射线基础模型。该模型采用自监督学习，在一个包含115万张图像的大型私有数据集上进行训练，这些图像涵盖了多样化的解剖区域。模型在12个数据集和20项下游任务上进行了评估，包括分类、检索、分割、定位、视觉接地和报告生成。

Result: XR-0在大多数多解剖区域任务上实现了最先进的性能，并在胸部特定基准测试中保持了竞争力。

Conclusion: 研究结果表明，解剖学多样性和监督对于构建强大、通用的医学视觉模型至关重要，为放射学领域可扩展和适应性强的AI系统铺平了道路。

Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation
models are limited to chest anatomy and fail to generalize across broader
clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray
foundation model using self-supervised learning on a large, private dataset of
1.15 million images spanning diverse anatomical regions and evaluated across 12
datasets and 20 downstream tasks, including classification, retrieval,
segmentation, localization, visual grounding, and report generation. XR-0
achieves state-of-the-art performance on most multi-anatomy tasks and remains
competitive on chest-specific benchmarks. Our results demonstrate that
anatomical diversity and supervision are critical for building robust,
general-purpose medical vision models, paving the way for scalable and
adaptable AI systems in radiology.

</details>


### [146] [DUAL-VAD: Dual Benchmarks and Anomaly-Focused Sampling for Video Anomaly Detection](https://arxiv.org/abs/2509.11605)
*Seoik Jung,Taekyung Song,Joshua Jordan Daniel,JinYoung Lee,SungJun Lee*

Main category: cs.CV

TL;DR: 该研究针对视频异常检测（VAD）现有基准的局限性，提出了一种基于softmax的帧分配策略和两个互补的基准，以实现更全面的模型泛化评估，并在实验中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测（VAD）对监控和公共安全至关重要。然而，现有的基准仅限于帧级或视频级任务，限制了对模型泛化能力的全面评估。

Method: 本研究首先引入了一种基于softmax的帧分配策略，该策略优先考虑异常密集的片段，同时保持视频的完整覆盖，从而实现跨时间尺度的平衡采样。在此基础上，构建了两个互补的基准：一个基于图像的基准用于评估帧级推理，另一个基于视频的基准扩展到时间局部化片段并包含异常评分任务。

Result: 在UCF-Crime数据集上的实验表明，模型在帧级和视频级都得到了改进。消融研究证实，异常聚焦采样相比均匀采样和随机采样基线具有明显的优势。

Conclusion: 通过引入新的帧分配策略和构建互补的基准，该研究为视频异常检测提供了更全面和有效的评估方法，并证明了其在提升模型性能和泛化能力方面的潜力。

Abstract: Video Anomaly Detection (VAD) is critical for surveillance and public safety.
However, existing benchmarks are limited to either frame-level or video-level
tasks, restricting a holistic view of model generalization. This work first
introduces a softmax-based frame allocation strategy that prioritizes
anomaly-dense segments while maintaining full-video coverage, enabling balanced
sampling across temporal scales. Building on this process, we construct two
complementary benchmarks. The image-based benchmark evaluates frame-level
reasoning with representative frames, while the video-based benchmark extends
to temporally localized segments and incorporates an abnormality scoring
task.Experiments on UCF-Crime demonstrate improvements at both the frame and
video levels, and ablation studies confirm clear advantages of anomaly-focused
sampling over uniform and random baselines.

</details>


### [147] [A Controllable 3D Deepfake Generation Framework with Gaussian Splatting](https://arxiv.org/abs/2509.11624)
*Wending Liu,Siyun Liang,Huy H. Nguyen,Isao Echizen*

Main category: cs.CV

TL;DR: 该研究提出了一种基于3D高斯辐射场的3D深度伪造框架，实现逼真、身份保持、多视角一致且可控的人脸替换和重演，并揭示了3D高斯辐射场在视觉伪造方面的潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 传统的2D深度伪造方法存在几何不一致性，且对新视角的泛化能力有限。本研究旨在克服这些局限性，提供一种在完全可控的3D空间中进行深度伪造的方法。

Method: 该方法结合了参数化头部模型和动态高斯表示，以支持多视角一致渲染、精确的表情控制和无缝的背景集成。为解决基于点表示的编辑挑战，它明确分离了头部和背景高斯，并利用预训练的2D引导来优化跨视角的面部区域。此外，还引入了一个修复模块，以增强极端姿态和表情下的视觉一致性。

Result: 实验表明，该方法在身份保持、姿态和表情一致性方面与最先进的2D方法性能相当，但在多视角渲染质量和3D一致性方面显著优于它们。

Conclusion: 该方法弥合了3D建模和深度伪造合成之间的鸿沟，为场景感知、可控和沉浸式视觉伪造开辟了新方向，并揭示了新兴的3D高斯辐射场技术可能被用于操纵攻击的威胁。

Abstract: We propose a novel 3D deepfake generation framework based on 3D Gaussian
Splatting that enables realistic, identity-preserving face swapping and
reenactment in a fully controllable 3D space. Compared to conventional 2D
deepfake approaches that suffer from geometric inconsistencies and limited
generalization to novel view, our method combines a parametric head model with
dynamic Gaussian representations to support multi-view consistent rendering,
precise expression control, and seamless background integration. To address
editing challenges in point-based representations, we explicitly separate the
head and background Gaussians and use pre-trained 2D guidance to optimize the
facial region across views. We further introduce a repair module to enhance
visual consistency under extreme poses and expressions. Experiments on
NeRSemble and additional evaluation videos demonstrate that our method achieves
comparable performance to state-of-the-art 2D approaches in identity
preservation, as well as pose and expression consistency, while significantly
outperforming them in multi-view rendering quality and 3D consistency. Our
approach bridges the gap between 3D modeling and deepfake synthesis, enabling
new directions for scene-aware, controllable, and immersive visual forgeries,
revealing the threat that emerging 3D Gaussian Splatting technique could be
used for manipulation attacks.

</details>


### [148] [IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed](https://arxiv.org/abs/2509.11638)
*Yongzhe Lyu,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 扩散模型在图像修复中存在随机初始种子导致语义不匹配的问题。IS-Diff提出一种免训练方法，利用来自未遮蔽区域的和谐初始种子，并通过动态选择性细化机制，显著提升了修复结果的一致性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型在自由形式图像修复中，采用随机初始种子（噪声），可能在遮蔽区域引入不匹配的语义信息，导致修复结果与未遮蔽区域的低一致性和低连贯性。

Method: 本文提出初始种子精炼扩散模型（IS-Diff），这是一种完全免训练的方法。它通过从未遮蔽区域采样初始种子来模拟遮蔽数据分布，从而为扩散过程设定一个有前景的方向。此外，还提出了一种动态选择性细化机制，用于检测中间潜在空间中严重不和谐的修复结果，并动态调整初始化先验的强度。

Result: 该方法在CelebA-HQ、ImageNet和Places2数据集上，针对标准和大幅度遮蔽的图像修复任务进行了验证。结果表明，与最先进的修复方法相比，IS-Diff在所有指标上都展现出其有效性。

Conclusion: IS-Diff通过引入分布和谐的初始种子和动态选择性细化机制，有效地解决了扩散模型中随机初始种子导致的图像修复不一致问题，实现了更好的修复结果，并超越了现有技术水平。

Abstract: Diffusion models have shown promising results in free-form inpainting. Recent
studies based on refined diffusion samplers or novel architectural designs led
to realistic results and high data consistency. However, random initialization
seed (noise) adopted in vanilla diffusion process may introduce mismatched
semantic information in masked regions, leading to biased inpainting results,
e.g., low consistency and low coherence with the other unmasked area. To
address this issue, we propose the Initial Seed refined Diffusion Model
(IS-Diff), a completely training-free approach incorporating distributional
harmonious seeds to produce harmonious results. Specifically, IS-Diff employs
initial seeds sampled from unmasked areas to imitate the masked data
distribution, thereby setting a promising direction for the diffusion
procedure. Moreover, a dynamic selective refinement mechanism is proposed to
detect severe unharmonious inpaintings in intermediate latent and adjust the
strength of our initialization prior dynamically. We validate our method on
both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet,
and Places2 datasets, demonstrating its effectiveness across all metrics
compared to state-of-the-art inpainting methods.

</details>


### [149] [WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration](https://arxiv.org/abs/2509.11642)
*Qiyuan Guan,Qianfeng Yang,Xiang Chen,Tianyu Song,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: 本文提出并发布了一个大规模真实世界一体化恶劣天气图像恢复基准数据集，以解决现有合成数据集的域差距问题和真实世界一体化数据集的缺失，从而推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的一体化图像恢复方法主要使用混合的单一天气合成数据集进行训练和评估，这些数据集在分辨率、风格和领域特性上存在显著差异，导致严重的域差距。此外，缺乏大规模、真实世界的一体化天气恢复数据集是该领域发展的关键瓶颈。

Method: 作者构建了一个真实世界一体化恶劣天气图像恢复基准数据集。该数据集包含在雨、雪、雾霾等多种天气条件下，以及多样户外场景和光照设置下捕获的图像对，并提供了精确对齐的退化图像和清晰图像。他们还利用该数据集对多种任务特定、任务通用和一体化恢复方法进行了基准测试。

Result: 该数据集为推进真实世界场景中鲁棒且实用的一体化图像恢复提供了宝贵的基础。通过对各种恢复方法进行全面实验和基准测试，验证了数据集的有效性。

Conclusion: 所提出的真实世界一体化恶劣天气图像恢复数据集成功解决了现有合成数据集的域差距问题和真实世界一体化数据集的缺失，为未来在该领域进行监督学习和严格评估提供了关键资源，有助于推动一体化图像恢复技术在实际应用中的发展。

Abstract: Existing all-in-one image restoration approaches, which aim to handle
multiple weather degradations within a single framework, are predominantly
trained and evaluated using mixed single-weather synthetic datasets. However,
these datasets often differ significantly in resolution, style, and domain
characteristics, leading to substantial domain gaps that hinder the development
and fair evaluation of unified models. Furthermore, the lack of a large-scale,
real-world all-in-one weather restoration dataset remains a critical bottleneck
in advancing this field. To address these limitations, we present a real-world
all-in-one adverse weather image restoration benchmark dataset, which contains
image pairs captured under various weather conditions, including rain, snow,
and haze, as well as diverse outdoor scenes and illumination settings. The
resulting dataset provides precisely aligned degraded and clean images,
enabling supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of task-specific, task-general, and
all-in-one restoration methods on our dataset. Our dataset offers a valuable
foundation for advancing robust and practical all-in-one image restoration in
real-world scenarios. The dataset has been publicly released and is available
at https://github.com/guanqiyuan/WeatherBench.

</details>


### [150] [HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments](https://arxiv.org/abs/2509.12187)
*Johanna Karras,Yingwei Li,Yasamin Jafarian,Ira Kemelmacher-Shlizerman*

Main category: cs.CV

TL;DR: HoloGarment提出了一种从少量图像或视频合成野外服装360度新视图的方法，通过弥合真实与合成数据之间的域差距，实现了对复杂现实世界衣物变形的鲁棒处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于合成的3D训练数据，这些数据多为无遮挡和静态物体，导致在处理真实世界服装时，因显著遮挡、复杂人体姿态和衣物变形而泛化能力差。

Method: HoloGarment采用一种新颖的隐式训练范式，结合大规模真实视频数据和小规模合成3D数据来优化一个共享的服装嵌入空间，从而弥合真实与合成数据之间的域差距。在推理时，通过对特定真实视频进行微调，构建一个服装“图集”表示，该图集捕获与身体姿态或运动无关的服装特定几何和纹理，从而实现动态的视频到360度新视图合成。

Result: HoloGarment在从图像和视频合成野外服装新视图方面取得了最先进的性能。它能鲁棒地处理褶皱、姿态变化和遮挡等具有挑战性的真实世界伪影，同时保持照片级真实感、视图一致性、精细纹理细节和准确的几何形状。

Conclusion: HoloGarment通过创新的训练范式和共享嵌入空间，成功解决了野外服装新视图合成的难题，在真实世界场景中展现出卓越的性能和鲁棒性，能够生成高质量、一致性强的服装360度视图。

Abstract: Novel view synthesis (NVS) of in-the-wild garments is a challenging task due
significant occlusions, complex human poses, and cloth deformations. Prior
methods rely on synthetic 3D training data consisting of mostly unoccluded and
static objects, leading to poor generalization on real-world clothing. In this
paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3
images or a continuous video of a person wearing a garment and generates
360{\deg} novel views of the garment in a canonical pose. Our key insight is to
bridge the domain gap between real and synthetic data with a novel implicit
training paradigm leveraging a combination of large-scale real video data and
small-scale synthetic 3D data to optimize a shared garment embedding space.
During inference, the shared embedding space further enables dynamic
video-to-360{\deg} NVS through the construction of a garment "atlas"
representation by finetuning a garment embedding on a specific real-world
video. The atlas captures garment-specific geometry and texture across all
viewpoints, independent of body pose or motion. Extensive experiments show that
HoloGarment achieves state-of-the-art performance on NVS of in-the-wild
garments from images and videos. Notably, our method robustly handles
challenging real-world artifacts -- such as wrinkling, pose variation, and
occlusion -- while maintaining photorealism, view consistency, fine texture
details, and accurate geometry. Visit our project page for additional results:
https://johannakarras.github.io/HoloGarment

</details>


### [151] [Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba](https://arxiv.org/abs/2509.11649)
*Chuang Liu,Nan Guo*

Main category: cs.CV

TL;DR: 本文提出Joint-OCTAMamba框架，结合Mamba状态空间模型，用于提高OCTA图像中视网膜血管和中心凹无血管区（FAZ）的分割精度，并解决多任务分割中的性能不平衡问题，实验证明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前的二维视网膜血管分割方法精度不足；现有OCTA数据联合分割模型在不同任务之间存在性能不平衡。

Method: 提出RVMamba，一个结合Mamba状态空间模型和多特征提取模块的新型架构，用于视网膜血管分割。引入FAZMamba以改善FAZ分割。最终提出统一的Joint-OCTAMamba框架，旨在同时提高FAZ分割并缓解性能不平衡。

Result: 在OCTA-500数据集上的实验结果表明，Joint-OCTAMamba在各项评估指标上均优于现有模型。

Conclusion: Joint-OCTAMamba框架通过整合Mamba模型，有效提升了OCTA图像中视网膜血管和FAZ的分割精度，并解决了多任务分割的性能不平衡问题，展现出卓越的性能。

Abstract: OCTA is a crucial non-invasive imaging technique for diagnosing and
monitoring retinal diseases like diabetic retinopathy, age-related macular
degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV)
segmentation offer insufficient accuracy. To address this, we propose RVMamba,
a novel architecture integrating multiple feature extraction modules with the
Mamba state-space model. Moreover, existing joint segmentation models for OCTA
data exhibit performance imbalance between different tasks. To simultaneously
improve the segmentation of the foveal avascular zone (FAZ) and mitigate this
imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework.
Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba
outperforms existing models across evaluation metrics.The code is available at
https://github.com/lc-sfis/Joint-OCTAMamba.

</details>


### [152] [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/abs/2509.11986)
*Wenyan Li,Raphael Tang,Chengzu Li,Caiqi Zhang,Ivan Vulić,Anders Søgaard*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型（VLMs）中视觉编码器投影到语言模型嵌入空间的连接器组件所导致的信息损失。研究发现，该投影步骤会显著扭曲视觉表示的局部几何结构，导致信息损失，并直接影响模型性能，尤其是在信息损失高的区域。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言模型（VLMs）中，视觉输入通过预训练的视觉编码器处理后，需要通过连接器组件投影到语言模型的嵌入空间。尽管这一投影步骤对模态融合至关重要，但其可能导致的信息损失及其对模型能力的直接影响尚未得到充分研究。

Method: 本文提出了两种互补的方法来检验和量化信息损失：
1. 通过分析投影前后图像表示的k-最近邻（k-NN）关系变化，评估语义信息保留情况。
2. 通过从投影表示中重建视觉嵌入，在图像块级别上直接测量信息损失。

Result: 实验结果表明：
1. 连接器显著扭曲了视觉表示的局部几何结构，投影后k-NN关系分歧高达40-60%，这与检索性能的下降相关。
2. 图像块级别的嵌入重建提供了对模型在视觉接地问答任务中行为的可解释性洞察，发现信息损失高的区域能够可靠地预测模型表现不佳的情况。

Conclusion: VLMs中的连接器投影步骤会导致视觉表示的显著信息损失和扭曲，这直接影响了模型的性能。通过量化和定位这些损失，可以更好地理解模型行为，并为未来的模型改进提供方向。

Abstract: Vision--language models (VLMs) often process visual inputs through a
pretrained vision encoder, followed by a projection into the language model's
embedding space via a connector component. While crucial for modality fusion,
the potential information loss induced by this projection step and its direct
impact on model capabilities remain understudied. We introduce two
complementary approaches to examine and quantify this loss by analyzing the
latent representation space. First, we evaluate semantic information
preservation by analyzing changes in k-nearest neighbor relationships between
image representations, before and after projection. Second, we directly measure
information loss by reconstructing visual embeddings from the projected
representation, localizing loss at an image patch level. Experiments reveal
that connectors substantially distort the local geometry of visual
representations, with k-nearest neighbors diverging by 40--60\%
post-projection, correlating with degradation in retrieval performance. The
patch-level embedding reconstruction provides interpretable insights for model
behavior on visually grounded question-answering tasks, finding that areas of
high information loss reliably predict instances where models struggle.

</details>


### [153] [RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps](https://arxiv.org/abs/2509.11674)
*Bjoern Kremser,Yusuke Matsui*

Main category: cs.CV

TL;DR: 该论文提出了一种从扫描纸质地图中提取可导航路径的流程，使其能用于GPS导航，解决了数字地图缺乏精选路径的问题。


<details>
  <summary>Details</summary>
Motivation: 纸质地图因包含数字导航应用（如Google Maps）中常缺失的精选路径和本地相关注释，仍在徒步和观光中广泛使用。本研究旨在将这些纸质地图的优势引入GPS导航。

Method: 该方法结合了地理配准、基于U-Net的二值分割、图构建，以及使用路由引擎的迭代细化过程。

Result: 评估显示，该端到端流程以及各个组件都能从不同地图风格中稳健地恢复路径网络，并生成适合实际使用的GPS路线。

Conclusion: 该方法能够有效地从扫描纸质地图中提取路径网络，并生成实用的GPS导航路线，从而将纸质地图的丰富信息整合到数字导航系统中。

Abstract: Paper maps remain widely used for hiking and sightseeing because they contain
curated trails and locally relevant annotations that are often missing from
digital navigation applications such as Google Maps. We propose a pipeline to
extract navigable trails from scanned maps, enabling their use in GPS-based
navigation. Our method combines georeferencing, U-Net-based binary
segmentation, graph construction, and an iterative refinement procedure using a
routing engine. We evaluate the full end-to-end pipeline as well as individual
components, showing that the approach can robustly recover trail networks from
diverse map styles and generate GPS routes suitable for practical use.

</details>


### [154] [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/abs/2509.12132)
*Pu Jian,Junhong Wu,Wei Sun,Chen Wang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 Reflection-V 的新型视觉推理模型 (VRM)，通过构建以视觉为中心的推理数据和基于视觉注意力的奖励模型来增强视觉反射能力，从而在多个视觉推理基准上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 文本领域的“慢思考”推理能力进步促使人们尝试将其迁移到视觉语言模型 (VLM) 中以训练视觉推理模型 (VRM)。然而，VRM 中的有效“慢思考”需要“视觉反射”能力，即根据视觉信息检查推理过程。现有 VRM 的视觉反射能力有限，其对视觉信息的注意力会随着生成响应的长度增加而迅速减弱。

Method: 本文提出了 Reflection-V 模型来解决视觉反射不足的问题。方法包括两方面：1) 构建以视觉为中心的推理数据，通过一个在 VLM 和推理 LLM 之间交互的代理来引导视觉反射模式的冷启动学习。2) 在强化学习 (RL) 过程中采用基于视觉注意力的奖励模型，以鼓励模型基于视觉信息进行推理。

Result: Reflection-V 在多个视觉推理基准上表现出显著的改进。此外，Reflection-V 在视觉推理过程中对视觉信息保持了更强、更一致的依赖，这表明其视觉反射能力得到了有效增强。

Conclusion: Reflection-V 通过创新的数据构建和奖励设计，成功增强了视觉推理模型的视觉反射能力，使其在复杂的视觉推理任务中能够更有效地利用视觉信息进行“慢思考”。

Abstract: Recent advances in text-only "slow-thinking" reasoning have prompted efforts
to transfer this capability to vision-language models (VLMs), for training
visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical
challenges: Effective "slow thinking" in VRMs requires \textbf{visual
reflection}, the ability to check the reasoning process based on visual
information. Through quantitative analysis, we observe that current VRMs
exhibit limited visual reflection, as their attention to visual information
diminishes rapidly with longer generated responses. To address this challenge,
we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection
based on reasoning data construction for cold-start and reward design for
reinforcement learning (RL). Firstly, we construct vision-centered reasoning
data by leveraging an agent that interacts between VLMs and reasoning LLMs,
enabling cold-start learning of visual reflection patterns. Secondly, a visual
attention based reward model is employed during RL to encourage reasoning based
on visual information. Therefore, \textbf{Reflection-V} demonstrates
significant improvements across multiple visual reasoning benchmarks.
Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent
reliance on visual information during visual reasoning, indicating effective
enhancement in visual reflection capabilities.

</details>


### [155] [IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects](https://arxiv.org/abs/2509.11680)
*Ruimin Ma,Sebastian Zudaire,Zhen Li,Chi Zhang*

Main category: cs.CV

TL;DR: 针对现有6D姿态估计基准在工业场景中泛化能力不足的问题，本文提出了一个新的工业金属数据集（IMD）和基准，以促进更适用于工业机器人场景的分割和姿态估计算法的发展。


<details>
  <summary>Details</summary>
Motivation: 6D物体姿态估计对机器人感知，尤其是在工业环境中至关重要。然而，现有基准主要使用纹理丰富、低反射率的日常物品，这限制了模型在工业场景（物体通常是金属、无纹理且高反射）中的泛化能力。

Method: 研究者提出了一个名为“工业金属数据集（IMD）”的新型数据集和基准，专门用于工业应用。该数据集包含45个真实比例的工业部件，使用RGB-D相机在自然室内光照和多种物体排列下捕获。该基准支持视频物体分割、6D姿态跟踪和单次6D姿态估计三项任务。研究者还评估了包括XMem、SAM2、BundleTrack和BundleSDF在内的现有最先进模型，以评估它们在工业环境中的性能。

Result: 评估结果表明，该工业数据集比现有家用物品数据集更具挑战性。

Conclusion: 该基准为开发和比较能更好泛化到工业机器人场景的分割和姿态估计算法提供了基线。

Abstract: Object 6DoF (6D) pose estimation is essential for robotic perception,
especially in industrial settings. It enables robots to interact with the
environment and manipulate objects. However, existing benchmarks on object 6D
pose estimation primarily use everyday objects with rich textures and
low-reflectivity, limiting model generalization to industrial scenarios where
objects are often metallic, texture-less, and highly reflective. To address
this gap, we propose a novel dataset and benchmark namely \textit{Industrial
Metallic Dataset (IMD)}, tailored for industrial applications. Our dataset
comprises 45 true-to-scale industrial components, captured with an RGB-D camera
under natural indoor lighting and varied object arrangements to replicate
real-world conditions. The benchmark supports three tasks, including video
object segmentation, 6D pose tracking, and one-shot 6D pose estimation. We
evaluate existing state-of-the-art models, including XMem and SAM2 for
segmentation, and BundleTrack and BundleSDF for pose estimation, to assess
model performance in industrial contexts. Evaluation results show that our
industrial dataset is more challenging than existing household object datasets.
This benchmark provides the baseline for developing and comparing segmentation
and pose estimation algorithms that better generalize to industrial robotics
scenarios.

</details>


### [156] [Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation](https://arxiv.org/abs/2509.11689)
*Jeremiah Fadugba,Petru Manescu,Bolanle Oladejo,Delmiro Fernandez-Reyes,Philipp Berens*

Main category: cs.CV

TL;DR: 本文提出集成蒸馏（Ensemble Distillation）方法，通过将多个集成模型的知识提炼到一个单一模型中，在显著降低计算复杂度的同时，实现了与深度集成相当的视网膜血管分割不确定性估计性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗图像分割中，不确定性估计对可靠性至关重要，尤其在视网膜血管分析中。深度集成模型虽能提高性能，但其训练和测试成本随集成数量增加而显著提高。

Method: 提出集成蒸馏（Ensemble Distillation）方法，将多个独立训练的集成模型的知识提炼到一个单一模型中，作为常用不确定性估计技术的替代方案。

Result: 在DRIVE和FIVES数据集上，集成蒸馏在校准和分割指标上实现了与深度集成相当的性能，同时显著降低了计算复杂度。

Conclusion: 集成蒸馏为视网膜血管分割中的不确定性估计提供了一种高效可靠的方法，在医疗影像应用中具有广阔前景。

Abstract: Uncertainty estimation is critical for reliable medical image segmentation,
particularly in retinal vessel analysis, where accurate predictions are
essential for diagnostic applications. Deep Ensembles, where multiple networks
are trained individually, are widely used to improve medical image segmentation
performance. However, training and testing costs increase with the number of
ensembles. In this work, we propose Ensemble Distillation as a robust
alternative to commonly used uncertainty estimation techniques by distilling
the knowledge of multiple ensemble models into a single model. Through
extensive experiments on the DRIVE and FIVES datasets, we demonstrate that
Ensemble Distillation achieves comparable performance via calibration and
segmentation metrics, while significantly reducing computational complexity.
These findings suggest that Ensemble distillation provides an efficient and
reliable approach for uncertainty estimation in the segmentation of the retinal
vessels, making it a promising tool for medical imaging applications.

</details>


### [157] [The Quest for Universal Master Key Filters in DS-CNNs](https://arxiv.org/abs/2509.11711)
*Zahra Babaiee,Peyman M. Kiassari,Daniela Rus,Radu Grosu*

Main category: cs.CV

TL;DR: 本研究发现深度可分离卷积网络 (DS-CNN) 固有的收敛于一组仅包含8个通用滤波器的“主密钥滤波器”，这些滤波器与经典图像处理和生物视觉系统相似，并且在网络初始化时能实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究提出了卷积神经网络滤波器的“主密钥滤波器假说”。本研究旨在通过发现DS-CNN中数千个训练滤波器主要是这组通用滤波器的线性偏移，从而将该假说范围收敛到一组更小的通用滤波器，并系统地揭示这些基本模式。

Method: 通过系统性的无监督搜索，从不同架构和数据集中提取了这些基本模式。随后，使用这8个独特的冻结滤波器初始化网络，并评估其在ImageNet和小型数据集上的性能。

Result: 研究发现了8个深度可分离卷积网络固有的通用滤波器。这些滤波器与高斯差分 (DoGs)、高斯函数及其导数密切匹配，与经典图像处理和哺乳动物视觉系统的感受野惊人地相似。用这些冻结滤波器初始化的网络在ImageNet上达到了80%以上的准确率，在较小数据集上甚至超越了拥有数千个可训练参数的模型。

Conclusion: 研究结果提供了令人信服的证据，表明深度卷积层无论任务或架构如何，都会自然地趋向于这组基本的空间算子。这项工作为通过这些主密钥滤波器理解泛化和迁移学习提供了新的见解。

Abstract: A recent study has proposed the "Master Key Filters Hypothesis" for
convolutional neural network filters. This paper extends this hypothesis by
radically constraining its scope to a single set of just 8 universal filters
that depthwise separable convolutional networks inherently converge to. While
conventional DS-CNNs employ thousands of distinct trained filters, our analysis
reveals these filters are predominantly linear shifts (ax+b) of our discovered
universal set. Through systematic unsupervised search, we extracted these
fundamental patterns across different architectures and datasets. Remarkably,
networks initialized with these 8 unique frozen filters achieve over 80%
ImageNet accuracy, and even outperform models with thousands of trainable
parameters when applied to smaller datasets. The identified master key filters
closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,
structures that are not only fundamental to classical image processing but also
strikingly similar to receptive fields in mammalian visual systems. Our
findings provide compelling evidence that depthwise convolutional layers
naturally gravitate toward this fundamental set of spatial operators regardless
of task or architecture. This work offers new insights for understanding
generalization and transfer learning through the universal language of these
master key filters.

</details>


### [158] [Advanced Layout Analysis Models for Docling](https://arxiv.org/abs/2509.11720)
*Nikolaos Livathinos,Christoph Auer,Ahmed Nassar,Rafael Teixeira de Lima,Maksym Lysak,Brown Ebouky,Cesar Berrospi,Michele Dolfi,Panagiotis Vagenas,Matteo Omenetti,Kasper Dinkla,Yusik Kim,Valery Weber,Lucas Morin,Ingmar Meijer,Viktor Kuropiatnyk,Tim Strohmeyer,A. Said Gurbuz,Peter W. J. Staar*

Main category: cs.CV

TL;DR: 本技术报告开发了五种新的文档布局分析模型，并将其集成到Docling文档转换流程中，相较于基线模型实现了显著的mAP提升和高效的运行时间，并发布了相关资源。


<details>
  <summary>Details</summary>
Motivation: 旨在改进Docling文档转换流程中的布局分析能力，以提高文档转换任务的适用性和效率。

Method: 研究人员在包含15万份文档的异构语料库上，训练了基于RT-DETR、RT-DETRv2和DFINE架构的先进目标检测器。对原始检测结果进行了后处理，以适应文档转换任务。通过不同方法在各种文档基准上评估了布局分析的有效性，并测量了在CPU、Nvidia和Apple GPU等不同环境下的运行时性能。

Result: 引入了五种新的文档布局模型，相较于Docling之前的基线模型，mAP提升了20.6% - 23.9%，且运行时性能相当或更优。其中最佳模型“heron-101”在单个NVIDIA A100 GPU上实现了78%的mAP，推理时间为28毫秒/图像。此外，研究还建立了训练、评估和部署文档布局检测器的最佳实践。

Conclusion: 通过开发新的布局分析模型，显著提升了Docling文档转换管道的性能和效率，为文档转换社区提供了可操作的指导。所有训练好的检查点、代码和文档均已在HuggingFace上以开放许可发布。

Abstract: This technical report documents the development of novel Layout Analysis
models integrated into the Docling document-conversion pipeline. We trained
several state-of-the-art object detectors based on the RT-DETR, RT-DETRv2 and
DFINE architectures on a heterogeneous corpus of 150,000 documents (both openly
available and proprietary). Post-processing steps were applied to the raw
detections to make them more applicable to the document conversion task. We
evaluated the effectiveness of the layout analysis on various document
benchmarks using different methodologies while also measuring the runtime
performance across different environments (CPU, Nvidia and Apple GPUs). We
introduce five new document layout models achieving 20.6% - 23.9% mAP
improvement over Docling's previous baseline, with comparable or better
runtime. Our best model, "heron-101", attains 78% mAP with 28 ms/image
inference time on a single NVIDIA A100 GPU. Extensive quantitative and
qualitative experiments establish best practices for training, evaluating, and
deploying document-layout detectors, providing actionable guidance for the
document conversion community. All trained checkpoints, code, and documentation
are released under a permissive license on HuggingFace.

</details>


### [159] [A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications](https://arxiv.org/abs/2509.11752)
*Hongyuan Zhang,Yuheng Wu,Mingyang Zhao,Zhiwei Chen,Rebecca Li,Fei Zhu,Haohan Zhao,Xiaohua Yuan,Meng Yang,Chunli Qiu,Xiang Cong,Haiyan Chen,Lina Luan,Randolph H. L. Wong,Huai Liao,Colin A Graham,Shi Chang,Guowei Tao,Dong Yi,Zhen Lei,Nassir Navab,Sebastien Ourselin,Jiebo Luo,Hongbin Liu,Gaofeng Meng*

Main category: cs.CV

TL;DR: 本研究推出了EchoCare，一个基于自监督学习的超声通用基础模型，通过在包含450万张多源图像的EchoCareData数据集上训练，并在10个超声基准测试中超越了现有先进模型，解决了超声AI中数据稀缺和模型泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 在实际临床环境中，大型标注数据集的稀缺性以及任务特定模型泛化能力有限，阻碍了超声AI通用临床模型的发展。

Method: 研究提出了EchoCare，一个超声基础模型，通过自监督学习在EchoCareData上进行训练。EchoCareData是一个包含来自23个国家、5大洲、多种成像设备的450万张超声图像的大规模数据集。与以往采用现成视觉基础模型架构不同，EchoCare引入了分层分类器，以联合学习像素级和表示级特征，捕获全局解剖背景和局部超声特征。

Result: EchoCare在仅需少量训练的情况下，在涵盖疾病诊断、病灶分割、器官检测、地标预测、定量回归、图像增强和报告生成等10个具有不同诊断难度的超声基准测试中，均优于现有先进的对比模型。模型代码和预训练模型已公开发布。

Conclusion: EchoCare提供了一个完全开放且可泛化的基础模型，有望推动AI技术在各种临床超声应用中的发展。

Abstract: Artificial intelligence (AI) that can effectively learn ultrasound
representations by integrating multi-source data holds significant promise for
advancing clinical care. However, the scarcity of large labeled datasets in
real-world clinical environments and the limited generalizability of
task-specific models have hindered the development of generalizable clinical AI
models for ultrasound applications. In this study, we present EchoCare, a novel
ultrasound foundation model for generalist clinical use, developed via
self-supervised learning on our curated, publicly available, large-scale
dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,
sourced from over 23 countries across 5 continents and acquired via a diverse
range of distinct imaging devices, thus encompassing global cohorts that are
multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt
off-the-shelf vision foundation model architectures, we introduce a
hierarchical classifier into EchoCare to enable joint learning of pixel-level
and representation-level features, capturing both global anatomical contexts
and local ultrasound characteristics. With minimal training, EchoCare
outperforms state-of-the-art comparison models across 10 representative
ultrasound benchmarks of varying diagnostic difficulties, spanning disease
diagnosis, lesion segmentation, organ detection, landmark prediction,
quantitative regression, imaging enhancement and report generation. The code
and pretrained model are publicly released, rendering EchoCare accessible for
fine-tuning and local adaptation, supporting extensibility to additional
applications. EchoCare provides a fully open and generalizable foundation model
to boost the development of AI technologies for diverse clinical ultrasound
applications.

</details>


### [160] [MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images](https://arxiv.org/abs/2509.11763)
*Danling Cao*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSMA的多尺度特征融合与多属性框架，用于从单张无约束图像重建3D人脸。该方法通过融合多尺度特征、关注多属性学习并引入大核注意力模块，提高了特征提取精度，实现了准确的3D人脸参数估计，并在挑战性条件下取得了与现有最先进方法相当甚至超越的性能。


<details>
  <summary>Details</summary>
Motivation: 从单张无约束图像重建3D人脸是一个挑战性问题，原因在于无约束环境下的多样化条件。尽管基于学习的方法表现出色，但它们通常需要大量难以获取且成本高昂的3D人脸数据。此外，现有方法在捕获不同面部属性和条件下的细节和多尺度特征时常遇到困难，导致重建不完整或不准确。

Method: 本文提出了一种名为Multi-Scale Feature Fusion with Multi-Attribute (MSMA) 的框架。该方法整合了多尺度特征融合，并侧重于多属性学习。此外，它利用一个大核注意力模块来增强跨尺度特征提取的精度，从而实现从单张2D图像准确估计3D人脸参数。

Result: 在MICC Florence、Facewarehouse和自定义数据集上进行的综合实验表明，本文提出的方法取得了与当前最先进方法相当的结果，并且在某些情况下，在挑战性条件下超越了SOTA性能。

Conclusion: MSMA框架通过集成多尺度特征融合、多属性学习和大核注意力模块，有效解决了从无约束图像进行3D人脸重建的难题，显著提高了特征提取精度和重建质量，达到了与现有SOTA方法相当甚至更优的性能。

Abstract: Reconstructing 3D face from a single unconstrained image remains a
challenging problem due to diverse conditions in unconstrained environments.
Recently, learning-based methods have achieved notable results by effectively
capturing complex facial structures and details across varying conditions.
Consequently, many existing approaches employ projection-based losses between
generated and input images to constrain model training. However, learning-based
methods for 3D face reconstruction typically require substantial amounts of 3D
facial data, which is difficult and costly to obtain. Consequently, to reduce
reliance on labeled 3D face datasets, many existing approaches employ
projection-based losses between generated and input images to constrain model
training. Nonetheless, despite these advancements, existing approaches
frequently struggle to capture detailed and multi-scale features under diverse
facial attributes and conditions, leading to incomplete or less accurate
reconstructions. In this paper, we propose a Multi-Scale Feature Fusion with
Multi-Attribute (MSMA) framework for 3D face reconstruction from unconstrained
images. Our method integrates multi-scale feature fusion with a focus on
multi-attribute learning and leverages a large-kernel attention module to
enhance the precision of feature extraction across scales, enabling accurate 3D
facial parameter estimation from a single 2D image. Comprehensive experiments
on the MICC Florence, Facewarehouse and custom-collect datasets demonstrate
that our approach achieves results on par with current state-of-the-art
methods, and in some instances, surpasses SOTA performance across challenging
conditions.

</details>


### [161] [Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization](https://arxiv.org/abs/2509.11772)
*Diogo Mendonça,Tiago Barros,Cristiano Premebida,Urbano J. Nunes*

Main category: cs.CV

TL;DR: Seg2Track-SAM2是一个无需微调、与检测器无关的框架，它将预训练目标检测器与SAM2和新型Seg2Track模块结合，解决了多目标跟踪与分割（MOTS）中的身份管理和内存效率问题，在KITTI基准上实现了最先进的性能和显著的内存优化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要强大的多目标跟踪（MOT）能力。尽管SAM2等基础模型在视频分割方面表现出强大的零样本泛化能力，但它们在多目标跟踪与分割（MOTS）中的直接应用受限于身份管理不足和内存效率低下。

Method: 本文提出了Seg2Track-SAM2框架，它将预训练的目标检测器与SAM2以及一个新颖的Seg2Track模块相结合。该方法无需微调，且与检测器无关，旨在解决跟踪初始化、跟踪管理和强化等问题。此外，还引入了一种滑动窗口内存策略，以提高内存效率。

Result: Seg2Track-SAM2在KITTI MOT和KITTI MOTS基准测试中取得了最先进（SOTA）的性能，在KITTI MOTS的汽车和行人类别中均排名第四，并在关联准确性（AssA）方面建立了新的基准。此外，滑动窗口内存策略将内存使用量减少了高达75%，而性能下降可以忽略不计。

Conclusion: Seg2Track-SAM2通过结合鲁棒的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS领域的发展。这些结果证实了其在资源受限部署下的潜力。

Abstract: Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to
operate reliably in dynamic environments. MOT ensures consistent object
identity assignment and precise spatial delineation. Recent advances in
foundation models, such as SAM2, have demonstrated strong zero-shot
generalization for video segmentation, but their direct application to MOTS
(MOT+Segmentation) remains limited by insufficient identity management and
memory efficiency. This work introduces Seg2Track-SAM2, a framework that
integrates pre-trained object detectors with SAM2 and a novel Seg2Track module
to address track initialization, track management, and reinforcement. The
proposed approach requires no fine-tuning and remains detector-agnostic.
Experimental results on KITTI MOT and KITTI MOTS benchmarks show that
Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth
overall in both car and pedestrian classes on KITTI MOTS, while establishing a
new benchmark in association accuracy (AssA). Furthermore, a sliding-window
memory strategy reduces memory usage by up to 75% with negligible performance
degradation, supporting deployment under resource constraints. These results
confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot
tracking, enhanced identity preservation, and efficient memory utilization. The
code is available at https://github.com/hcmr-lab/Seg2Track-SAM2

</details>


### [162] [SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.11774)
*Changlu Guo,Anders Nymark Christensen,Anders Bjorholm Dahl,Yugen Yi,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: SA-UNetv2是一种轻量级视网膜血管分割模型，通过在跳跃连接中注入跨尺度空间注意力并使用加权BCE+MCC损失来解决类别不平衡问题，在DRIVE和STARE数据集上实现了最先进的性能，同时具有极高的效率。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割对于糖尿病视网膜病变、高血压和神经退行性疾病的早期诊断至关重要。现有的SA-UNet模型虽然在瓶颈层引入了空间注意力，但未充分利用跳跃连接中的注意力，也未能有效解决前景-背景严重不平衡的问题。

Method: 本文提出了SA-UNetv2模型，它是一种轻量级架构。该模型在所有跳跃连接中注入跨尺度空间注意力，以增强多尺度特征融合。此外，它采用加权二元交叉熵（BCE）损失与马修斯相关系数（MCC）损失相结合的方法，以提高对类别不平衡的鲁棒性。

Result: SA-UNetv2在公共DRIVE和STARE数据集上取得了最先进的性能，且模型非常轻量，仅占用1.2MB内存和0.26M参数（不到SA-UNet的50%）。在592 x 592 x 3图像上，其CPU推理时间仅需1秒。

Conclusion: SA-UNetv2展示了在资源受限、仅限CPU环境下强大的效率和可部署性，为视网膜血管分割提供了一个高性能且轻量级的解决方案。

Abstract: Retinal vessel segmentation is essential for early diagnosis of diseases such
as diabetic retinopathy, hypertension, and neurodegenerative disorders.
Although SA-UNet introduces spatial attention in the bottleneck, it underuses
attention in skip connections and does not address the severe
foreground-background imbalance. We propose SA-UNetv2, a lightweight model that
injects cross-scale spatial attention into all skip connections to strengthen
multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE)
plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class
imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves
state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less
than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images,
demonstrating strong efficiency and deployability in resource-constrained,
CPU-only settings.

</details>


### [163] [FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning](https://arxiv.org/abs/2509.11796)
*Haodong Chen,Haojian Huang,XinXiang Yin,Dian Shao*

Main category: cs.CV

TL;DR: 本文提出了FineQuest，一个无需训练的框架，通过认知科学启发的双模态推理（反应式和审慎式）来解决基于LLM的体育视频问答（VideoQA）中的挑战。它引入了多模态体育知识场景图SSGraph以弥补知识鸿沟，并发布了两个新的体育VideoQA基准，在现有和新基准上均达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型（LLM）的视频问答在通用视频理解方面显示出潜力，但在应用于固有的复杂体育视频领域时面临重大挑战。主要问题在于通用模型与领域特定体育理解之间的知识鸿沟。

Method: 本文提出了FineQuest框架，这是第一个无需训练的框架，它利用了认知科学启发的双模态推理：i) 反应式推理用于直接的体育查询，ii) 审慎式推理用于更复杂的查询。为弥补知识鸿沟，FineQuest整合了SSGraph，这是一个涵盖九种体育的多模态体育知识场景图，编码了视觉实例和领域特定术语。此外，作者还引入了两个新的体育VideoQA基准，Gym-QA和Diving-QA，分别源自FineGym和FineDiving数据集。

Result: FineQuest在这些新基准以及现有的SPORTU数据集上均取得了最先进的性能，同时保持了强大的通用VideoQA能力。

Conclusion: FineQuest是一个有效的无需训练的框架，通过结合双模态推理和领域特定的多模态知识图谱（SSGraph），显著提升了LLM在复杂体育视频问答中的表现。新引入的基准也为未来研究提供了全面的评估工具，证明了该框架在体育领域理解上的优越性和通用性。

Abstract: Video Question Answering (VideoQA) based on Large Language Models (LLMs) has
shown potential in general video understanding but faces significant challenges
when applied to the inherently complex domain of sports videos. In this work,
we propose FineQuest, the first training-free framework that leverages
dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for
straightforward sports queries and ii) Deliberative Reasoning for more complex
ones. To bridge the knowledge gap between general-purpose models and
domain-specific sports understanding, FineQuest incorporates SSGraph, a
multimodal sports knowledge scene graph spanning nine sports, which encodes
both visual instances and domain-specific terminology to enhance reasoning
accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA
and Diving-QA, derived from the FineGym and FineDiving datasets, enabling
diverse and comprehensive evaluation. FineQuest achieves state-of-the-art
performance on these benchmarks as well as the existing SPORTU dataset, while
maintains strong general VideoQA capabilities.

</details>


### [164] [Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics](https://arxiv.org/abs/2509.11800)
*Ang Nan Gu,Michael Tsang,Hooman Vaseli,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: 本文提出一个新颖的框架，利用神经网络训练动态生成不确定性感知的伪标签，以解决医学图像诊断中标签过度简化的问题，从而提高模型的校准和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前的计算机辅助诊断系统在处理嘈杂、模糊或冲突的医学图像时，依赖过于简化的单热编码标签，这忽略了诊断不确定性和评估者之间的差异，导致模型过度自信的预测。

Method: 引入了一个新颖的框架，利用神经网络训练动态（NNTD）评估每个训练样本的固有难度。通过在训练过程中聚合和校准模型预测，生成反映学习过程中模糊性的不确定性感知伪标签。该标签增强方法与模型架构无关，可应用于任何监督学习流程。

Result: 在具有挑战性的超声心动图分类基准上验证了该方法，在校准、选择性分类和多视图融合方面，表现出优于专业基线的性能。

Conclusion: 该框架成功地将不确定性重新引入标签空间，显著提升了医学图像诊断中不确定性估计的准确性和模型的鲁棒性。

Abstract: Computer-aided diagnosis systems must make critical decisions from medical
images that are often noisy, ambiguous, or conflicting, yet today's models are
trained on overly simplistic labels that ignore diagnostic uncertainty. One-hot
labels erase inter-rater variability and force models to make overconfident
predictions, especially when faced with incomplete or artifact-laden inputs. We
address this gap by introducing a novel framework that brings uncertainty back
into the label space. Our method leverages neural network training dynamics
(NNTD) to assess the inherent difficulty of each training sample. By
aggregating and calibrating model predictions during training, we generate
uncertainty-aware pseudo-labels that reflect the ambiguity encountered during
learning. This label augmentation approach is architecture-agnostic and can be
applied to any supervised learning pipeline to enhance uncertainty estimation
and robustness. We validate our approach on a challenging echocardiography
classification benchmark, demonstrating superior performance over specialized
baselines in calibration, selective classification, and multi-view fusion.

</details>


### [165] [LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio](https://arxiv.org/abs/2509.11811)
*Mehwish Mehmood,Shahzaib Iqbal,Tariq Mahmood Khan,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: LFRA-Net是一种轻量级视网膜血管分割网络，通过引入焦点调制注意力和区域感知注意力，在保持低计算成本的同时，显著提高了分割精度，尤其适用于资源受限的临床环境。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割对早期诊断眼部和全身性疾病至关重要，但在资源有限的实际临床环境中，现有的深度学习模型在提取微小血管方面存在挑战，且计算成本高昂。

Method: 本研究提出了LFRA-Net，这是一个轻量级的编解码器网络。它在编解码器瓶颈处整合了焦点调制注意力（focal modulation attention），并在选择性跳跃连接中引入了区域感知注意力（region-aware attention），以增强特征表示和区域聚焦，从而实现精确有效的视网膜血管分割。

Result: LFRA-Net在保持轻量级特性的同时（仅0.17M参数、0.66 MB内存、10.50 GFLOPs），超越了许多现有最先进模型。在DRIVE、STARE和CHASE_DB三个公开数据集上，其Dice分数分别达到84.28%、88.44%和85.50%，Jaccard指数分别达到72.86%、79.31%和74.70%，表现优异。

Conclusion: LFRA-Net在分割精度和计算成本之间取得了理想的平衡，使其非常适合在资源有限的区域进行实时临床应用，从而促进视网膜血管分割的早期诊断。

Abstract: Retinal vessel segmentation is critical for the early diagnosis of
vision-threatening and systemic diseases, especially in real-world clinical
settings with limited computational resources. Although significant
improvements have been made in deep learning-based segmentation methods,
current models still face challenges in extracting tiny vessels and suffer from
high computational costs. In this study, we present LFRA-Net by incorporating
focal modulation attention at the encoder-decoder bottleneck and region-aware
attention in the selective skip connections. LFRA-Net is a lightweight network
optimized for precise and effective retinal vascular segmentation. It enhances
feature representation and regional focus by efficiently capturing local and
global dependencies. LFRA-Net outperformed many state-of-the-art models while
maintaining lightweight characteristics with only 0.17 million parameters, 0.66
MB memory size, and 10.50 GFLOPs. We validated it on three publicly available
datasets: DRIVE, STARE, and CHASE\_DB. It performed better in terms of Dice
score (84.28\%, 88.44\%, and 85.50\%) and Jaccard index (72.86\%, 79.31\%, and
74.70\%) on the DRIVE, STARE, and CHASE\_DB datasets, respectively. LFRA-Net
provides an ideal ratio between segmentation accuracy and computational cost
compared to existing deep learning methods, which makes it suitable for
real-time clinical applications in areas with limited resources. The code can
be found at https://github.com/Mehwish4593/LFRA-Net.

</details>


### [166] [MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation](https://arxiv.org/abs/2509.11817)
*Liying Wang,Xiaoli Zhang,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: 该论文提出了一种名为 MAFS 的统一网络，用于同时进行红外-可见光图像融合和语义分割。它通过像素级融合与跨模态特征融合感知任务之间的相互促进，结合异构特征融合策略和多阶段 Transformer 解码器，实现了良好的视觉质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语义驱动方法虽然考虑了语义信息注入，但未能从宏观任务层面探讨像素级图像融合与跨模态特征融合感知任务之间的相互促进潜力，以同时提升两者性能。

Method: MAFS 采用并行结构，包含一个融合子网络和一个分割子网络。它设计了异构特征融合策略来增强图像融合的语义感知能力；通过级联融合子网络和分割骨干网络，将分割相关知识转移以促进基于特征级融合的分割；引入了新颖的多阶段 Transformer 解码器来高效聚合细粒度多尺度融合特征；并基于最大-最小公平分配原则引入动态因子，为两个任务生成自适应权重以保证多任务训练的平稳性。

Result: 大量的实验结果表明，该方法与现有最先进的方法相比，取得了具有竞争力的结果。

Conclusion: 该研究成功地将图像融合和语义分割任务整合到一个统一的框架中，通过任务间的相互促进机制，有效提升了融合图像的视觉质量和下游语义分割任务的性能，验证了其方法的有效性。

Abstract: Infrared-visible image fusion methods aim at generating fused images with
good visual quality and also facilitate the performance of high-level tasks.
Indeed, existing semantic-driven methods have considered semantic information
injection for downstream applications. However, none of them investigates the
potential for reciprocal promotion between pixel-wise image fusion and
cross-modal feature fusion perception tasks from a macroscopic task-level
perspective. To address this limitation, we propose a unified network for image
fusion and semantic segmentation. MAFS is a parallel structure, containing a
fusion sub-network and a segmentation sub-network. On the one hand, We devise a
heterogeneous feature fusion strategy to enhance semantic-aware capabilities
for image fusion. On the other hand, by cascading the fusion sub-network and a
segmentation backbone, segmentation-related knowledge is transferred to promote
feature-level fusion-based segmentation. Within the framework, we design a
novel multi-stage Transformer decoder to aggregate fine-grained multi-scale
fused features efficiently. Additionally, a dynamic factor based on the max-min
fairness allocation principle is introduced to generate adaptive weights of two
tasks and guarantee smooth training in a multi-task manner. Extensive
experiments demonstrate that our approach achieves competitive results compared
with state-of-the-art methods. The code is available at
https://github.com/Abraham-Einstein/MAFS/.

</details>


### [167] [Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation](https://arxiv.org/abs/2509.11840)
*Tim Lebailly,Vijay Veerabadran,Satwik Kottur,Karl Ridgeway,Michael Louis Iuzzolino*

Main category: cs.CV

TL;DR: 本文通过使用生成式视觉-语言模型（VLMs）生成的合成描述，弥合了生成式VLMs的高级理解能力与密集对齐方法的空间对齐能力之间的鸿沟，从而在零样本开放词汇分割任务中取得了SOTA性能并提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉-语言模型（VLMs）在高级图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐。另一方面，致力于视觉-语言对齐的表示学习研究关注零样本密集任务（如分割），但可能缺乏VLMs提供的高级语义理解。本文旨在结合两者的优势。

Method: 本研究通过将图像与由VLMs生成的合成描述进行密集对齐来弥合上述差距。合成描述被视为一种廉价、可扩展且易于生成的高级语义理解来源，用于密集对齐方法。

Result: 经验证，该方法在标准的零样本开放词汇分割基准/数据集上优于现有工作，并且数据效率更高。

Conclusion: 通过将生成式VLMs的合成描述用于密集对齐，可以有效结合高级语义理解和空间对齐能力，从而显著提升零样本开放词汇分割的性能和数据效率。

Abstract: Generative vision-language models (VLMs) exhibit strong high-level image
understanding but lack spatially dense alignment between vision and language
modalities, as our findings indicate. Orthogonal to advancements in generative
VLMs, another line of research has focused on representation learning for
vision-language alignment, targeting zero-shot inference for dense tasks like
segmentation. In this work, we bridge these two directions by densely aligning
images with synthetic descriptions generated by VLMs. Synthetic captions are
inexpensive, scalable, and easy to generate, making them an excellent source of
high-level semantic understanding for dense alignment methods. Empirically, our
approach outperforms prior work on standard zero-shot open-vocabulary
segmentation benchmarks/datasets, while also being more data-efficient.

</details>


### [168] [Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2509.11853)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Måarten Sjöström*

Main category: cs.CV

TL;DR: SDI-GS是一种针对稀疏视角3D高斯泼溅(3DGS)的初始化方法，它利用区域分割来识别并保留结构上重要的区域，从而选择性地对点云进行降采样，显著减少高斯数量，同时保持渲染质量，降低内存消耗并加速训练。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角合成面临难以从有限观测中恢复准确几何和外观的挑战。现有的3DGS方法依赖SfM进行相机姿态估计，但在真正的稀疏视角下表现不佳。另一些SfM-free方法（基于MVS）通过将每个像素反投影到3D空间来生成大量3D高斯，导致高内存成本和低效率。

Method: 本文提出了Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS)。该方法通过利用基于区域的分割来识别并仅保留结构上重要的区域，从而缓解效率低下问题。这使得可以对密集点云进行选择性降采样，在显著减少高斯数量的同时保持场景保真度。

Result: 实验表明，SDI-GS在高斯数量上减少了高达50%，并在PSNR和SSIM方面实现了相当或更优的渲染质量，LPIPS仅有轻微下降。此外，它还实现了更快的训练速度和更低的内存占用。

Conclusion: SDI-GS通过解决稀疏视角下3DGS的效率问题，提升了其在受限视角场景中的实用性，降低了高斯数量和内存需求，同时保持了高质量的渲染效果。

Abstract: Sparse-view synthesis remains a challenging problem due to the difficulty of
recovering accurate geometry and appearance from limited observations. While
recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time
rendering with competitive quality, existing pipelines often rely on
Structure-from-Motion (SfM) for camera pose estimation, an approach that
struggles in genuinely sparse-view settings. Moreover, several SfM-free methods
replace SfM with multi-view stereo (MVS) models, but generate massive numbers
of 3D Gaussians by back-projecting every pixel into 3D space, leading to high
memory costs. We propose Segmentation-Driven Initialization for Gaussian
Splatting (SDI-GS), a method that mitigates inefficiency by leveraging
region-based segmentation to identify and retain only structurally significant
regions. This enables selective downsampling of the dense point cloud,
preserving scene fidelity while substantially reducing Gaussian count.
Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count
by up to 50% and achieves comparable or superior rendering quality in PSNR and
SSIM, with only marginal degradation in LPIPS. It further enables faster
training and lower memory footprint, advancing the practicality of 3DGS for
constrained-view scenarios.

</details>


### [169] [Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding](https://arxiv.org/abs/2509.11866)
*Meng Luo,Shengqiong Wu,Liqiang Jing,Tianjie Ju,Li Zheng,Jinxiang Lai,Tianlong Wu,Xinya Du,Jian Li,Siyuan Yan,Jiebo Luo,William Yang Wang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 大型视频模型（LVMs）存在幻觉问题，本研究提出了Dr.V分层框架，包含Dr.V-Bench数据集和Dr.V-Agent代理，通过细粒度时空定位诊断LVM幻觉，提高了诊断的有效性、可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视频模型（LVMs）在视频理解方面取得了显著进展，但它们仍然存在幻觉问题，即生成与输入视频不符的内容。

Method: 本研究提出了Dr.V框架，一个涵盖感知、时序和认知层面的分层框架，通过细粒度时空定位来诊断视频幻觉。Dr.V包含两个关键组件：
1. Dr.V-Bench基准数据集：包含10k个实例，源自4,974个视频，涵盖多种任务，并富含详细的时空标注。
2. Dr.V-Agent卫星视频代理：通过系统地在感知和时序层面应用细粒度时空定位，然后进行认知层面推理，逐步检测LVM中的幻觉。这一流程模仿了人类的视频理解方式。

Result: 广泛的实验证明，Dr.V-Agent在诊断幻觉方面是有效的，同时增强了可解释性和可靠性。

Conclusion: Dr.V为在真实场景中实现鲁棒的视频理解提供了一个实用的蓝图，有效解决了大型视频模型中的幻觉问题。

Abstract: Recent advancements in large video models (LVMs) have significantly enhance
video understanding. However, these models continue to suffer from
hallucinations, producing content that conflicts with input videos. To address
this issue, we propose Dr.V, a hierarchical framework covering perceptive,
temporal, and cognitive levels to diagnose video hallucination by fine-grained
spatial-temporal grounding. Dr.V comprises of two key components: a benchmark
dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes
10k instances drawn from 4,974 videos spanning diverse tasks, each enriched
with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in
LVMs by systematically applying fine-grained spatial-temporal grounding at the
perceptive and temporal levels, followed by cognitive level reasoning. This
step-by-step pipeline mirrors human-like video comprehension and effectively
identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is
effective in diagnosing hallucination while enhancing interpretability and
reliability, offering a practical blueprint for robust video understanding in
real-world scenarios. All our data and code are available at
https://github.com/Eurekaleo/Dr.V.

</details>


### [170] [Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods](https://arxiv.org/abs/2509.11873)
*Anne Marthe Sophie Ngo Bibinbe,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 本研究比较了传统多动物追踪（MAT）工具与先进多目标追踪（MOT）方法在猪只长期追踪中的表现，结果显示MOT方法整体优于MAT工具，有望提升精准畜牧业的追踪准确性。


<details>
  <summary>Details</summary>
Motivation: 精准畜牧业需要先进的监控工具，特别是用于连续行为监测的长期多动物追踪系统。现有MAT工具与最先进的MOT方法相比表现不佳，导致下游任务（如行为分析、健康评估）不准确。

Method: 研究评估了多种MAT工具（如DeepLabCut、idTracker）和基于MOT的方法（如ByteTrack、DeepSORT、跨输入一致性、Track-Anything、PromptTrack）。所有方法均在一个10分钟的猪只追踪数据集上进行评估。

Result: 总体而言，MOT方法在长期追踪场景下表现优于传统的MAT工具。

Conclusion: 这些发现突显了最新MOT技术在提高自动化畜牧追踪准确性和可靠性方面的潜力。

Abstract: Precision livestock farming requires advanced monitoring tools to meet the
increasing management needs of the industry. Computer vision systems capable of
long-term multi-animal tracking (MAT) are essential for continuous behavioral
monitoring in livestock production. MAT, a specialized subset of multi-object
tracking (MOT), shares many challenges with MOT, but also faces domain-specific
issues including frequent animal occlusion, highly similar appearances among
animals, erratic motion patterns, and a wide range of behavior types.
  While some existing MAT tools are user-friendly and widely adopted, they
often underperform compared to state-of-the-art MOT methods, which can result
in inaccurate downstream tasks such as behavior analysis, health state
estimation, and related applications. In this study, we benchmarked both MAT
and MOT approaches for long-term tracking of pigs. We compared tools such as
DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT,
cross-input consistency, and newer approaches like Track-Anything and
PromptTrack.
  All methods were evaluated on a 10-minute pig tracking dataset. Our results
demonstrate that, overall, MOT approaches outperform traditional MAT tools,
even for long-term tracking scenarios. These findings highlight the potential
of recent MOT techniques to enhance the accuracy and reliability of automated
livestock tracking.

</details>


### [171] [Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation](https://arxiv.org/abs/2509.11878)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,K J Joseph*

Main category: cs.CV

TL;DR: 本文提出了一种名为加权提示词操作（WPM）的新技术，用于在零样本设置下为诗歌生成并修改图像，允许用户根据个人理解调整图像。


<details>
  <summary>Details</summary>
Motivation: 诗歌具有多重解读性，读者常根据自身情感、经验和文化背景理解诗歌。研究旨在使观众能够根据自身需求修改为诗歌生成的图像。

Method: 引入了加权提示词操作（WPM）技术，系统地修改扩散模型中的注意力权重和文本嵌入，以动态调整特定词语的重要性。该方法结合了扩散模型、大型语言模型（如GPT）和现有诗歌数据集。

Result: 通过WPM，生成的图像在语义上更丰富，在语境上更准确。这是首次尝试将加权提示词操作集成到诗歌语言图像增强中。

Conclusion: 该研究提供了一种新颖的、全面的结构化方法，通过加权提示词操作改进了文学领域的图像生成，实现了更符合语境的视觉化效果，并允许用户个性化修改图像。

Abstract: Poetry is an expressive form of art that invites multiple interpretations, as
readers often bring their own emotions, experiences, and cultural backgrounds
into their understanding of a poem. Recognizing this, we aim to generate images
for poems and improve these images in a zero-shot setting, enabling audiences
to modify images as per their requirements. To achieve this, we introduce a
novel Weighted Prompt Manipulation (WPM) technique, which systematically
modifies attention weights and text embeddings within diffusion models. By
dynamically adjusting the importance of specific words, WPM enhances or
suppresses their influence in the final generated image, leading to
semantically richer and more contextually accurate visualizations. Our approach
exploits diffusion models and large language models (LLMs) such as GPT in
conjunction with existing poetry datasets, ensuring a comprehensive and
structured methodology for improved image generation in the literary domain. To
the best of our knowledge, this is the first attempt at integrating weighted
prompt manipulation for enhancing imagery in poetic language.

</details>


### [172] [SAM-TTT: Segment Anything Model via Reverse Parameter Configuration and Test-Time Training for Camouflaged Object Detection](https://arxiv.org/abs/2509.11884)
*Zhenni Yu,Li Zhao,Guobao Xiao,Xiaoqin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAM-TTT的新型Segment Anything Model (SAM) 变体，通过逆向参数配置和测试时训练来增强其在伪装目标检测（COD）任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM的COD模型主要关注提取有利特征和放大优势参数，但忽略了损害SAM在下游任务中语义理解的不利参数。

Method: SAM-TTT包含两个模块：1. 逆向SAM参数配置模块，通过配置SAM参数以无训练方式有效减轻不利参数的影响。2. T-Visioner模块，通过将最初为语言任务开发的测试时训练层（具有线性复杂度和富有表现力的隐藏状态）集成到视觉任务中，来强化有利参数。SAM-TTT结合这两个模块，同时抑制不利参数并增强有利参数。

Result: 实验结果表明，该方法在各种COD基准测试上实现了最先进的性能，并在该领域树立了新的标杆。

Conclusion: SAM-TTT通过同时抑制不利参数和强化有利参数，显著提高了SAM在COD任务中的语义理解能力，从而设定了新的性能基准。

Abstract: This paper introduces a new Segment Anything Model (SAM) that leverages
reverse parameter configuration and test-time training to enhance its
performance on Camouflaged Object Detection (COD), named SAM-TTT. While most
existing SAM-based COD models primarily focus on enhancing SAM by extracting
favorable features and amplifying its advantageous parameters, a crucial gap is
identified: insufficient attention to adverse parameters that impair SAM's
semantic understanding in downstream tasks. To tackle this issue, the Reverse
SAM Parameter Configuration Module is proposed to effectively mitigate the
influence of adverse parameters in a train-free manner by configuring SAM's
parameters. Building on this foundation, the T-Visioner Module is unveiled to
strengthen advantageous parameters by integrating Test-Time Training layers,
originally developed for language tasks, into vision tasks. Test-Time Training
layers represent a new class of sequence modeling layers characterized by
linear complexity and an expressive hidden state. By integrating two modules,
SAM-TTT simultaneously suppresses adverse parameters while reinforcing
advantageous ones, significantly improving SAM's semantic understanding in COD
task. Our experimental results on various COD benchmarks demonstrate that the
proposed approach achieves state-of-the-art performance, setting a new
benchmark in the field. The code will be available at
https://github.com/guobaoxiao/SAM-TTT.

</details>


### [173] [BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation](https://arxiv.org/abs/2509.11885)
*Francis Xiatian Zhang,Emile Mackute,Mohammadreza Kasaei,Kevin Dhaliwal,Robert Thomson,Mohsen Khadem*

Main category: cs.CV

TL;DR: 本文提出Brea-Depth框架，通过整合气道特有的几何先验知识（包括深度感知CycleGAN和气道结构感知损失），改进支气管镜检查中的单目深度估计，以提高导航精度和干预安全性，解决了现有模型缺乏解剖学意识的问题。


<details>
  <summary>Details</summary>
Motivation: 支气管镜检查中的单目深度估计对于复杂、分支气道内的实时导航精度和干预安全性至关重要。然而，现有的深度基础模型在支气管镜场景中往往缺乏解剖学意识，过度拟合局部纹理而非捕捉全局气道结构，尤其是在深度线索模糊和光照不佳的情况下。

Method: 本文提出了Brea-Depth框架，其方法包括：1) 将气道特有的几何先验知识整合到基础模型适应中；2) 引入一个深度感知CycleGAN，用于细化真实支气管镜图像与解剖数据中气道几何形状之间的转换，以弥合领域差距；3) 引入一个气道结构感知损失，以在气道腔内强制执行深度一致性，同时保持平滑过渡和结构完整性；4) 引入一个新的结构一致性评估指标——气道深度结构评估（Airway Depth Structure Evaluation）。

Result: Brea-Depth增强了模型的泛化能力，产生了更鲁棒、更准确的3D气道重建。在收集的离体人肺数据集和开放支气管镜数据集中，它在解剖深度保持方面优于现有方法。

Conclusion: 通过整合解剖学先验知识，Brea-Depth框架为支气管镜检查提供了更具解剖学真实性和鲁棒性的深度估计，从而提高了模型泛化能力、3D气道重建的准确性和可靠性，并有助于改善导航精度和手术安全性。

Abstract: Monocular depth estimation in bronchoscopy can significantly improve
real-time navigation accuracy and enhance the safety of interventions in
complex, branching airways. Recent advances in depth foundation models have
shown promise for endoscopic scenarios, yet these models often lack anatomical
awareness in bronchoscopy, overfitting to local textures rather than capturing
the global airway structure, particularly under ambiguous depth cues and poor
lighting. To address this, we propose Brea-Depth, a novel framework that
integrates airway-specific geometric priors into foundation model adaptation
for bronchoscopic depth estimation. Our method introduces a depth-aware
CycleGAN, refining the translation between real bronchoscopic images and airway
geometries from anatomical data, effectively bridging the domain gap. In
addition, we introduce an airway structure awareness loss to enforce depth
consistency within the airway lumen while preserving smooth transitions and
structural integrity. By incorporating anatomical priors, Brea-Depth enhances
model generalization and yields more robust, accurate 3D airway
reconstructions. To assess anatomical realism, we introduce Airway Depth
Structure Evaluation, a new metric for structural consistency. We validate
BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic
dataset, where it outperforms existing methods in anatomical depth
preservation.

</details>


### [174] [Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection](https://arxiv.org/abs/2509.11892)
*Akito Shinohara,Kohei Fukuda,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: 本文提出了一种在Logit空间进行线性插值混合的方法，以平滑类间Logit并提高模型对分布外数据的检测性能，尤其是在分布内数据附近。


<details>
  <summary>Details</summary>
Motivation: 现有的分布外检测方法（如Outlier Exposure和Mixture Outlier Exposure）在有效学习类间关系和清晰区分分布内/外数据方面仍面临挑战，尤其对于接近分布内数据的分布外数据。因此，研究人员关注Logit空间，因其类间分布特性更为明确。

Method: 本文提出在Logit空间对分布内和分布外数据进行线性插值混合，以平滑类间Logit并改善分布外检测性能。此外，还强制Logit空间混合所得Logit与输入空间混合生成Logit之间保持一致性。

Result: 实验表明，所提出的Logit空间混合技术减少了决策边界附近模型输出的剧烈波动，使得分布内和分布外数据之间分离更平滑、更可靠。该方法在细粒度分布外检测任务中也表现出有效性。

Conclusion: 通过在Logit空间进行线性插值混合，并结合输入空间混合的一致性，可以有效提高模型对分布外数据的检测能力，特别是对于与分布内数据相近的分布外数据，从而增强模型的鲁棒性和泛化性能。

Abstract: The ability to detect out-of-distribution data is essential not only for
ensuring robustness against unknown or unexpected input data but also for
improving the generalization performance of the model. Among various
out-of-distribution detection methods, Outlier Exposure and Mixture Outlier
Exposure are promising approaches that enhance out-of-distribution detection
performance by exposing the outlier data during training. However, even with
these sophisticated techniques, it remains challenging for models to learn the
relationships between classes effectively and to distinguish data sampling from
in-distribution and out-of-distribution clearly. Therefore, we focus on the
logit space, where the properties between class-wise distributions are
distinctly separated from those in the input or feature spaces. Specifically,
we propose a linear interpolation technique in the logit space that mixes
in-distribution and out-of-distribution data to facilitate smoothing logits
between classes and improve the out-of-distribution detection performance,
particularly for out-of-distribution data that lie close to the in-distribution
data. Additionally, we enforce consistency between the logits obtained through
mixing in the logit space and those generated via mixing in the input space.
Our experiments demonstrate that our logit-space mixing technique reduces the
abrupt fluctuations in the model outputs near the decision boundaries,
resulting in smoother and more reliable separation between in-distribution and
out-of-distribution data. Furthermore, we evaluate the effectiveness of the
proposed method on a fine-grained out-of-distribution detection task.

</details>


### [175] [NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition](https://arxiv.org/abs/2509.11916)
*Zilin Li,Weiwei Xu,Xuanqi Zhao,Yiran Zhu*

Main category: cs.CV

TL;DR: 该研究提出了NeuroGaze-Distill，一个跨模态蒸馏框架，将脑电图（EEG）信息（通过静态效价/唤醒原型和抑郁症启发几何先验）转移到仅基于图像的面部表情识别（FER）模型中，以提高模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的面部表情识别（FER）模型仅依赖像素进行训练，由于面部外观是底层情感的间接且有偏见的代理，导致模型在不同数据集上泛化能力差。

Method: 该方法名为NeuroGaze-Distill，采用跨模态蒸馏框架。首先，一个在DREAMER数据集（以MAHNOB-HCI为无标签支持）的EEG地形图上训练的教师模型生成一个固定的5x5效价/唤醒（V/A）原型网格。然后，学生模型（ResNet-18/50）在FERPlus数据集上进行训练，并结合两种轻量级正则化器：(i) Proto-KD（余弦相似度）使学生特征与静态原型对齐；(ii) D-Geo（抑郁症启发几何先验）根据情感研究发现（如高效价区域的快感缺失样收缩）软性地塑造嵌入几何结构。部署时无需EEG-面部配对或非视觉信号。

Result: 在域内（FERPlus验证）和跨数据集（AffectNet-mini；可选CK+）协议上进行评估。结果显示，原型和D-Geo正则化器带来了持续的性能提升，其中5x5的网格比更密集的网格更稳定。该方法在不增加架构复杂性的情况下，提高了模型的鲁棒性。

Conclusion: NeuroGaze-Distill是一个简单、可部署的方法，通过将脑电图信息蒸馏到基于图像的FER模型中，显著提高了模型的鲁棒性，而无需增加架构复杂性。

Abstract: Facial emotion recognition (FER) models trained only on pixels often fail to
generalize across datasets because facial appearance is an indirect and biased
proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal
distillation framework that transfers brain-informed priors into an image-only
FER student via static Valence/Arousal (V/A) prototypes and a
depression-inspired geometric prior (D-Geo). A teacher trained on EEG
topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a
consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face
pairing and no non-visual signals at deployment are required. The student
(ResNet-18/50) is trained on FERPlus with conventional CE/KD and two
lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the
static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with
affective findings often reported in depression research (e.g., anhedonia-like
contraction in high-valence regions). We evaluate both within-domain (FERPlus
validation) and cross-dataset protocols (AffectNet-mini; optional CK+),
reporting standard 8-way scores alongside present-only Macro-F1 and balanced
accuracy to fairly handle label-set mismatch. Ablations attribute consistent
gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability.
The method is simple, deployable, and improves robustness without architectural
complexity.

</details>


### [176] [Enriched text-guided variational multimodal knowledge distillation network (VMD) for automated diagnosis of plaque vulnerability in 3D carotid artery MRI](https://arxiv.org/abs/2509.11924)
*Bo Cao,Fan Yu,Mengmeng Feng,SenHao Zhang,Xin Meng,Yue Zhang,Zhen Qian,Jie Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为VMD（Variation inference and Multimodal knowledge Distillation）的策略，通过变分推断和多模态知识蒸馏，利用放射科医生的领域知识和有限的多模态数据（图像标注和放射学报告）中的交叉模态先验知识，自动诊断颈动脉斑块的易损性，显著提高了对未标注3D MRI图像的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 从颈动脉3D MRI图像直接诊断动脉粥样硬化斑块的易损性对放射科医生和传统3D视觉网络都极具挑战性。临床实践中，放射科医生会结合多种影像模态和领域专业知识进行评估，这启发了开发多模态诊断网络的思路。

Method: 开发了VMD（Variation inference and Multimodal knowledge Distillation）策略。该方法通过变分推断和多模态知识蒸馏，利用训练数据中有限的图像标注和放射学报告中的交叉模态先验知识，以及放射科医生的领域知识，来增强诊断网络对未标注3D MRI图像的准确性。

Result: VMD策略能够有效地利用交叉模态先验知识，从而提高诊断网络对未标注3D MRI图像的准确性。在内部收集的数据集上进行的深入实验验证了所提出VMD策略的有效性。

Conclusion: 所提出的VMD策略通过有效利用放射科医生的领域知识和多模态交叉知识，自动化了颈动脉斑块易损性的诊断，显著提高了诊断网络的准确性，特别是在处理未标注的3D MRI图像时表现出色。

Abstract: Multimodal learning has attracted much attention in recent years due to its
ability to effectively utilize data features from a variety of different
modalities. Diagnosing the vulnerability of atherosclerotic plaques directly
from carotid 3D MRI images is relatively challenging for both radiologists and
conventional 3D vision networks. In clinical practice, radiologists assess
patient conditions using a multimodal approach that incorporates various
imaging modalities and domain-specific expertise, paving the way for the
creation of multimodal diagnostic networks. In this paper, we have developed an
effective strategy to leverage radiologists' domain knowledge to automate the
diagnosis of carotid plaque vulnerability through Variation inference and
Multimodal knowledge Distillation (VMD). This method excels in harnessing
cross-modality prior knowledge from limited image annotations and radiology
reports within training data, thereby enhancing the diagnostic network's
accuracy for unannotated 3D MRI images. We conducted in-depth experiments on
the dataset collected in-house and verified the effectiveness of the VMD
strategy we proposed.

</details>


### [177] [Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization](https://arxiv.org/abs/2509.11926)
*Xue Zhang,Bingshuo Hu,Gene Cheung*

Main category: cs.CV

TL;DR: 该研究针对图像插值问题，通过将已知插值器映射到图滤波器来初始化图邻接矩阵，然后学习扰动矩阵并将其恢复效果通过Douglas-Rachford迭代展开为轻量级神经网络，实现了最先进的性能并显著减少了网络参数。


<details>
  <summary>Details</summary>
Motivation: 传统的深度神经网络（DNNs）随机初始化网络参数并通过随机梯度下降（SGD）进行优化，这导致了陷入性能不佳的局部最优解的巨大风险。

Method: 该方法利用一个定理，将（伪）线性插值器映射到有向图滤波器（该滤波器是经过图移变异（GSV）先验正则化的MAP问题的解）。首先，根据已知的插值器初始化有向图邻接矩阵A，建立基线性能。然后，从数据中学习扰动矩阵P和P(2)来增强A，其恢复效果通过Douglas-Rachford（DR）迭代实现，这些迭代被展开成一个轻量级、可解释的神经网络。

Result: 实验结果表明，该方法在图像插值方面取得了最先进的成果，同时大幅减少了网络参数。

Conclusion: 通过结合结构化初始化和数据驱动的扰动学习（通过展开的Douglas-Rachford迭代实现），该方法为图像插值提供了一个高效且高性能的解决方案，克服了传统DNNs的局限性。

Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random
and then optimize each one via stochastic gradient descent (SGD), resulting in
substantial risk of poor-performing local minima.Focusing on the image
interpolation problem and leveraging a recent theorem that maps a
(pseudo-)linear interpolator {\Theta} to a directed graph filter that is a
solution to a MAP problem regularized with a graph shift variation (GSV) prior,
we first initialize a directed graph adjacency matrix A based on a known
interpolator {\Theta}, establishing a baseline performance.Then, towards
further gain, we learn perturbation matrices P and P(2) from data to augment A,
whose restoration effects are implemented via Douglas-Rachford (DR) iterations,
which we unroll into a lightweight interpretable neural net.Experimental
results demonstrate state-of-the-art image interpolation results, while
drastically reducing network parameters.

</details>


### [178] [CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation](https://arxiv.org/abs/2509.11952)
*Debopom Sutradhar,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sheikh Izzal Azid,Sami Azam*

Main category: cs.CV

TL;DR: 该研究提出了一种名为CLAIRE的双编码器架构，结合跨模态注意力融合和RIFT混合损失函数，用于光学和SAR图像的土地覆盖分类，以解决复杂性、视觉相似性和类别不平衡问题，并在多个基准测试中取得了有竞争力的性能，同时通过小型语言模型增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 精确的土地覆盖分类对于环境监测和可持续资源管理至关重要，但由于自然景观的复杂性、类别间的视觉相似性以及数据集中显著的类别不平衡，这一任务仍然充满挑战。

Method: 该研究提出了一种双编码器架构，独立提取光学和合成孔径雷达（SAR）图像的模态特定特征。这些特征随后通过名为CLAIRE的跨模态注意力融合模块进行融合。为了解决类别不平衡问题并提高欠代表类别的分割性能，模型引入了一个名为RIFT（稀有实例焦点-特沃斯基）的混合损失函数，该函数结合了加权焦点损失和Tversky损失。此外，研究还引入了一个由小型语言模型（Phi-3）生成的度量驱动推理模块，为模型预测提供专家级、样本特定的解释，以增强透明度和可解释性。

Result: 该模型在多个基准测试中取得了有竞争力的性能：在WHU-OPT-SAR数据集上，平均交并比（mIoU）为56.02%，总体准确度（OA）为84.56%；在OpenEarthMap-SAR数据集上，mIoU为59.89%，OA为73.92%，展现出强大的泛化能力；在PIE-RGB-SAR数据集上，mIoU为86.86%，OA为94.58%，显示出在云遮挡条件下的卓越鲁棒性。此外，引入的推理模块增强了模型的透明度和可解释性。

Conclusion: 所提出的CLAIRE模型通过有效融合光学和SAR图像，并结合RIFT混合损失函数，成功应对了土地覆盖分类中的复杂性、视觉相似性和类别不平衡等挑战。该模型在不同数据集上表现出竞争性的性能、强大的泛化能力和鲁棒性，并且通过整合小型语言模型生成的推理模块，显著提升了模型预测的透明度和可解释性。

Abstract: Accurate land cover classification from satellite imagery is crucial in
environmental monitoring and sustainable resource management. However, it
remains challenging due to the complexity of natural landscapes, the visual
similarity between classes, and the significant class imbalance in the
available datasets. To address these issues, we propose a dual encoder
architecture that independently extracts modality-specific features from
optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using
a cross-modality attention-fusion module named Cross-modality Land cover
segmentation with Attention and Imbalance-aware Reasoning-Enhanced Explanations
(CLAIRE). This fusion mechanism highlights complementary spatial and textural
features, enabling the network to better capture detailed and diverse land
cover patterns. We incorporate a hybrid loss function that utilizes Weighted
Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address
class imbalance and improve segmentation performance across underrepresented
categories. Our model achieves competitive performance across multiple
benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall
Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with
a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and
remarkable robustness under cloud-obstructed conditions, achieving an mIoU of
86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce
a metric-driven reasoning module generated by a Small Language Model (Phi-3),
which generates expert-level, sample-specific justifications for model
predictions, thereby enhancing transparency and interpretability.

</details>


### [179] [Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness](https://arxiv.org/abs/2509.12024)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wen,Le Ku,Daheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: SCORE是一个新颖的框架，通过对抗性独立性问题解决了扩散模型中敏感概念的鲁棒擦除问题，并提供了理论保证和卓越的实证性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了巨大成功，但也带来了隐私、公平性和安全性方面的风险。因此，迫切需要从这些模型中擦除敏感或有害概念（例如，NSFW内容、私人个体、艺术风格），同时保留其整体生成能力。

Method: SCORE（安全和概念导向的鲁棒擦除）将概念擦除表述为一个对抗性独立性问题，理论上保证模型输出与被擦除概念统计独立。它通过最小化目标概念与生成输出之间的互信息来实现，并整合了对抗性优化、轨迹一致性和显著性驱动的微调。该方法还提供了收敛性证明和残余概念泄露的上限。

Result: SCORE在Stable Diffusion和FLUX上，针对物体擦除、NSFW移除、名人面部抑制和艺术风格遗忘这四个具有挑战性的基准测试中进行了评估。结果显示，SCORE始终优于现有最先进的方法（包括EraseAnything、ANT、MACE、ESD和UCE），擦除效率提高了高达12.5%，同时保持了相当或更优的图像质量。

Conclusion: 通过整合对抗性优化、轨迹一致性和显著性驱动的微调，SCORE为扩散模型中安全和鲁棒的概念擦除设定了新标准。

Abstract: Diffusion models have achieved unprecedented success in image generation but
pose increasing risks in terms of privacy, fairness, and security. A growing
demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW
content, private individuals, artistic styles) from these models while
preserving their overall generative capabilities. We introduce \textbf{SCORE}
(Secure and Concept-Oriented Robust Erasure), a novel framework for robust
concept removal in diffusion models. SCORE formulates concept erasure as an
\emph{adversarial independence} problem, theoretically guaranteeing that the
model's outputs become statistically independent of the erased concept. Unlike
prior heuristic methods, SCORE minimizes the mutual information between a
target concept and generated outputs, yielding provable erasure guarantees. We
provide formal proofs establishing convergence properties and derive upper
bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable
Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW
removal, celebrity face suppression, and artistic style unlearning. SCORE
consistently outperforms state-of-the-art methods including EraseAnything, ANT,
MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy
while maintaining comparable or superior image quality. By integrating
adversarial optimization, trajectory consistency, and saliency-driven
fine-tuning, SCORE sets a new standard for secure and robust concept erasure in
diffusion models.

</details>


### [180] [RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration](https://arxiv.org/abs/2509.12039)
*Zilong Zhang,Chujie Qin,Chunle Guo,Yong Zhang,Chao Xue,Ming-Ming Cheng,Chongyi Li*

Main category: cs.CV

TL;DR: RAM++ 是一种两阶段一体化图像修复框架，通过自适应掩码和语义理解，解决了现有方法在极端场景、性能不平衡和泛化能力弱等问题，实现了鲁棒且最先进的修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有面向降级的方法在极端场景（如降级与图像结构强耦合）中存在局限性，并且面临任务间性能不平衡、对已知降级过拟合以及对未知降级泛化能力弱等常见挑战。

Method: RAM++ 是一个两阶段框架，整合了高级语义理解和低级纹理生成。它包含三个关键设计：1) 自适应语义感知掩码 (AdaSAM)：一种预训练策略，在语义丰富和纹理区域应用像素级掩码，学习生成先验和图像内容先验。2) 掩码属性传导 (MAC)：一种选择性微调策略，调整贡献度较高的层，弥合掩码预训练与全图微调之间的完整性差距。3) 鲁棒特征正则化 (RFR)：利用 DINOv2 的语义一致且降级不变的表示，结合高效特征融合，实现忠实且语义连贯的修复。

Result: RAM++ 在已知、未知、极端和混合降级下，实现了鲁棒、均衡且最先进的性能。

Conclusion: RAM++ 通过其创新的两阶段框架和三个关键设计，有效解决了图像修复中的鲁棒性、性能均衡和泛化性挑战，在多种降级场景下表现出色，达到了最先进的水平。

Abstract: This work presents Robust Representation Learning via Adaptive Mask (RAM++),
a two-stage framework for all-in-one image restoration. RAM++ integrates
high-level semantic understanding with low-level texture generation to achieve
content-oriented robust restoration. It addresses the limitations of existing
degradation-oriented methods in extreme scenarios (e.g., degradations strongly
coupled with image structures). RAM++ also mitigates common challenges such as
unbalanced performance across tasks, overfitting to seen degradations, and weak
generalization to unseen ones through three key designs: 1) Adaptive
Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level
masks to semantically rich and textured regions. This design enables the
network to learn both generative priors and image content priors from various
degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning
strategy that adjusts the layers with higher contributions to bridge the
integrity gap between masked pretraining and full-image fine-tuning while
retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy
that leverages DINOv2's semantically consistent and degradation-invariant
representations, together with efficient feature fusion, to achieve faithful
and semantically coherent restoration. With these designs, RAM++ achieves
robust, well-balanced, and state-of-the-art performance across seen, unseen,
extreme, and mixed degradations. Our code and model will be released at
https://github.com/DragonisCV/RAM

</details>


### [181] [AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective](https://arxiv.org/abs/2509.12052)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Suiyang Zhang,Yi He,Yuxing Han*

Main category: cs.CV

TL;DR: AvatarSync是一个基于音素表示的自回归框架，通过两阶段生成策略，解决了现有GAN/扩散模型在说话人动画中存在的闪烁、身份漂移和推理慢等问题，实现了从单张参考图生成逼真、可控且高效的动画。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GAN或扩散模型的说话人动画方法存在帧间闪烁、身份漂移和推理速度慢等问题，这些固有的局限性限制了它们在实际应用中的适用性。

Method: 该研究引入了AvatarSync，一个基于音素表示的自回归框架。它采用“分而治之”的两阶段生成策略：1. 人脸关键帧生成（FKG），利用文本或音频到音素的多对一映射，构建音素到视觉映射并结合定制的文本-帧因果注意力掩码来生成关键帧；2. 帧间插值，引入基于选择性状态空间模型的时戳感知自适应策略，实现高效的双向上下文推理，以确保时间连贯性和视觉平滑度。此外，还优化了推理流程以降低延迟。

Result: 广泛的实验表明，AvatarSync在视觉保真度、时间一致性和计算效率方面均优于现有说话人动画方法。

Conclusion: AvatarSync提供了一种可扩展且可控的解决方案，能够生成逼真、可控的说话人动画，有效解决了现有方法的局限性。

Abstract: Existing talking-head animation approaches based on Generative Adversarial
Networks (GANs) or diffusion models often suffer from inter-frame flicker,
identity drift, and slow inference. These limitations inherent to their video
generation pipelines restrict their suitability for applications. To address
this, we introduce AvatarSync, an autoregressive framework on phoneme
representations that generates realistic and controllable talking-head
animations from a single reference image, driven directly text or audio input.
In addition, AvatarSync adopts a two-stage generation strategy, decoupling
semantic modeling from visual dynamics, which is a deliberate "Divide and
Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on
phoneme-level semantic representation by leveraging the many-to-one mapping
from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to
anchor abstract phonemes to character-level units. Combined with a customized
Text-Frame Causal Attention Mask, the keyframes are generated. The second
stage, inter-frame interpolation, emphasizes temporal coherence and visual
smoothness. We introduce a timestamp-aware adaptive strategy based on a
selective state space model, enabling efficient bidirectional context
reasoning. To support deployment, we optimize the inference pipeline to reduce
latency without compromising visual fidelity. Extensive experiments show that
AvatarSync outperforms existing talking-head animation methods in visual
fidelity, temporal consistency, and computational efficiency, providing a
scalable and controllable solution.

</details>


### [182] [Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation](https://arxiv.org/abs/2509.12062)
*Sebastian Diaz,Benjamin Billot,Neel Dey,Molin Zhang,Esra Abaci Turk,P. Ellen Grant,Polina Golland,Elfar Adalsteinsson*

Main category: cs.CV

TL;DR: 本文提出了一种跨人群数据增强框架，通过模拟早期胎龄（GA）独特的宫内环境和胎儿姿态，使姿态估计模型能够利用高胎龄标注数据泛化到低胎龄临床队列，从而更可靠地量化胎儿运动。


<details>
  <summary>Details</summary>
Motivation: 胎儿运动是神经发育和宫内健康的关键指标，但其量化具有挑战性，尤其是在早期胎龄。现有方法主要针对晚期胎龄，在训练分布内表现良好，但由于孕期母体和胎儿的显著解剖学变化以及早期胎龄EPI数据标注的困难，它们无法泛化到早期胎龄。

Method: 开发了一个跨人群数据增强框架，使姿态估计模型能够稳健地泛化到年轻胎龄临床队列，仅使用高胎龄队列的标注图像。具体来说，引入了一种胎儿特异性增强策略，模拟了早期胎龄独特的宫内环境和胎儿定位。

Result: 实验发现，跨人群增强在较高胎龄和具有挑战性的早期胎龄病例中均能降低变异性并显著改善性能。

Conclusion: 通过实现跨孕期更可靠的姿态估计，这项工作可能有助于在具有挑战性的4D胎儿成像环境中进行早期临床检测和干预。

Abstract: Fetal motion is a critical indicator of neurological development and
intrauterine health, yet its quantification remains challenging, particularly
at earlier gestational ages (GA). Current methods track fetal motion by
predicting the location of annotated landmarks on 3D echo planar imaging (EPI)
time-series, primarily in third-trimester fetuses. The predicted landmarks
enable simplification of the fetal body for downstream analysis. While these
methods perform well within their training age distribution, they consistently
fail to generalize to early GAs due to significant anatomical changes in both
mother and fetus across gestation, as well as the difficulty of obtaining
annotated early GA EPI data. In this work, we develop a cross-population data
augmentation framework that enables pose estimation models to robustly
generalize to younger GA clinical cohorts using only annotated images from
older GA cohorts. Specifically, we introduce a fetal-specific augmentation
strategy that simulates the distinct intrauterine environment and fetal
positioning of early GAs. Our experiments find that cross-population
augmentation yields reduced variability and significant improvements across
both older GA and challenging early GA cases. By enabling more reliable pose
estimation across gestation, our work potentially facilitates early clinical
detection and intervention in challenging 4D fetal imaging settings. Code is
available at https://github.com/sebodiaz/cross-population-pose.

</details>


### [183] [End-to-End Learning of Multi-Organ Implicit Surfaces from 3D Medical Imaging Data](https://arxiv.org/abs/2509.12068)
*Farahdiba Zarin,Nicolas Padoy,Jérémy Dana,Vinkle Srivastav*

Main category: cs.CV

TL;DR: ImplMORe是一种端到端深度学习方法，利用隐式表面表示从3D医学图像中进行多器官重建，提供超越输入分辨率的精细表面细节。


<details>
  <summary>Details</summary>
Motivation: 3D医学图像中器官的精细表面重建受限于分辨率，高分辨率需要更多内存和计算资源。虽然隐式表示在通用计算机视觉中能缓解此问题，但由于架构和数据差异，无法直接应用于医学图像。

Method: 本文提出了ImplMORe，一种端到端深度学习方法，用于从3D医学图像中进行多器官重建。它结合了使用3D CNN编码器提取局部特征，并利用多尺度插值通过占用函数在连续域中学习特征。

Result: ImplMORe在totalsegmentator数据集上进行单器官和多器官重建，其性能优于基于离散显式表示的表面重建方法。通过利用占用函数的连续性，该方法提供了比给定输入图像更高分辨率的器官精细表面细节。

Conclusion: ImplMORe通过引入适用于医学图像的隐式表面表示，成功实现了高分辨率、精细的多器官重建，克服了传统方法在分辨率和资源方面的限制，为诊断支持和手术规划提供了改进的工具。

Abstract: The fine-grained surface reconstruction of different organs from 3D medical
imaging can provide advanced diagnostic support and improved surgical planning.
However, the representation of the organs is often limited by the resolution,
with a detailed higher resolution requiring more memory and computing
footprint. Implicit representations of objects have been proposed to alleviate
this problem in general computer vision by providing compact and differentiable
functions to represent the 3D object shapes. However, architectural and
data-related differences prevent the direct application of these methods to
medical images. This work introduces ImplMORe, an end-to-end deep learning
method using implicit surface representations for multi-organ reconstruction
from 3D medical images. ImplMORe incorporates local features using a 3D CNN
encoder and performs multi-scale interpolation to learn the features in the
continuous domain using occupancy functions. We apply our method for single and
multiple organ reconstructions using the totalsegmentator dataset. By
leveraging the continuous nature of occupancy functions, our approach
outperforms the discrete explicit representation based surface reconstruction
approaches, providing fine-grained surface details of the organ at a resolution
higher than the given input image. The source code will be made publicly
available at: https://github.com/CAMMA-public/ImplMORe

</details>


### [184] [Progressive Flow-inspired Unfolding for Spectral Compressive Imaging](https://arxiv.org/abs/2509.12079)
*Xiaodong Wang,Ping Wang,Zijun He,Mengjie Qin,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出一种新型轨迹可控的深度展开框架，用于Coded Aperture Snapshot Spectral Imaging (CASSI) 高光谱图像重建，通过强制平滑优化路径，并结合高效空间-光谱Transformer和频域融合模块，实现了比现有技术更好的重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度展开网络（DUNs）在CASSI重建中表现出色，但存在重建轨迹不可控的问题，导致重建质量突然跳跃和非渐进式优化。

Method: 受扩散轨迹和流匹配的启发，本文提出了一种轨迹可控的展开框架，旨在实现从初始估计到高质量重建的平滑、连续优化路径。为提高计算效率，设计了一个专门用于高光谱重建的高效空间-光谱Transformer，并引入了一个频域融合模块以确保特征一致性。

Result: 在模拟和真实数据上的实验表明，该方法比现有最先进的方法取得了更好的重建质量和效率。

Conclusion: 所提出的轨迹可控展开框架，结合高效的空间-光谱Transformer和频域融合模块，成功解决了CASSI重建中深度展开网络轨迹不可控的问题，并显著提升了重建性能。

Abstract: Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral
image (HSI) from a single 2D compressed measurement, which is a highly
challenging reconstruction task. Recent deep unfolding networks (DUNs),
empowered by explicit data-fidelity updates and implicit deep denoisers, have
achieved the state of the art in CASSI reconstruction. However, existing
unfolding approaches suffer from uncontrollable reconstruction trajectories,
leading to abrupt quality jumps and non-gradual refinement across stages.
Inspired by diffusion trajectories and flow matching, we propose a novel
trajectory-controllable unfolding framework that enforces smooth, continuous
optimization paths from noisy initial estimates to high-quality
reconstructions. To achieve computational efficiency, we design an efficient
spatial-spectral Transformer tailored for hyperspectral reconstruction, along
with a frequency-domain fusion module to gurantee feature consistency.
Experiments on simulation and real data demonstrate that our method achieves
better reconstruction quality and efficiency than prior state-of-the-art
approaches.

</details>


### [185] [End-to-End 4D Heart Mesh Recovery Across Full-Stack and Sparse Cardiac MRI](https://arxiv.org/abs/2509.12090)
*Yihong Chen,Jiancheng Yang,Deniz Sayin Mercadier,Hieu Le,Juerg Schwitter,Pascal Fua*

Main category: cs.CV

TL;DR: TetHeart是一个端到端框架，利用深度可变形四面体，能够从完整的电影CMR序列和术中稀疏切片观测中重建4D多结构心脏网格运动，解决了现有方法对完整数据栈的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法在心脏运动重建时依赖完整的CMR数据栈，这限制了它们在术中场景中的应用，因为术中通常只能获得稀疏的观测数据。

Method: TetHeart框架采用显式-隐式混合表示的深度可变形四面体来捕捉心脏形状和运动。它通过预处理或离线获取的完整数据栈进行初始化，然后可根据任何可用切片（从完整数据栈到单个切片）进行更新。该方法还包含两项关键创新：(i) 一种注意力机制，用于切片自适应的2D-3D特征组合，动态整合来自任意数量和位置切片的信息，并结合从完整切片到稀疏切片设置的蒸馏策略，以确保在极端稀疏情况下的准确重建；(ii) 一种两阶段弱监督运动学习方案，仅需要关键帧（如ED和ES）标注。

Result: TetHeart在三个大型公共数据集上进行训练和验证，并在额外的私有介入和公共CMR数据集上进行零样本外部评估，在术前和术中设置中均实现了最先进的准确性和强大的泛化能力。

Conclusion: TetHeart是首个统一的端到端框架，能够从完整和稀疏的CMR数据中高精度地重建4D多结构心脏运动，显著扩展了心脏运动分析在术中场景中的应用潜力。

Abstract: Reconstructing cardiac motion from cine CMR sequences is critical for
diagnosis, prediction, and intervention. Existing methods rely on complete CMR
stacks to infer full heart motion, limiting their utility in intra-procedural
scenarios where only sparse observations are available. We present TetHeart,
the first end-to-end framework that unifies full 4D multi-structure heart mesh
recovery from both offline full-stack acquisitions and intra-procedural
sparse-slice observations. Our method leverages deep deformable tetrahedra, an
explicit-implicit hybrid representation, to capture shape and motion in a
coherent space shared across cardiac structures. It is initialized from
high-quality pre-procedural or offline-acquired full stacks to build detailed,
patient-specific heart meshes, which can then be updated using whatever slices
are available, from full stacks down to a single slice. We further incorporate
several key innovations: (i) an attentive mechanism for slice-adaptive 2D-3D
feature assembly that dynamically integrates information from arbitrary numbers
of slices at any position, combined with a distillation strategy from
full-slice to sparse-slice settings to ensure accurate reconstruction under
extreme sparsity; and (ii) a two-stage weakly supervised motion learning scheme
requiring only keyframe (e.g., ED and ES) annotations. Trained and validated on
three large public datasets and externally evaluated zero-shot on additional
private interventional and public CMR datasets, TetHeart achieves
state-of-the-art accuracy and strong generalization in both pre- and
intra-procedural settings.

</details>


### [186] [FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation](https://arxiv.org/abs/2509.12105)
*Bernardo Forni,Gabriele Lombardi,Federico Pozzi,Mirco Planamente*

Main category: cs.CV

TL;DR: 本文提出FS-SAM2，一种基于SAM2的少样本语义分割方法，通过重用SAM2的视频能力并结合LoRA进行参数高效适应，实现了卓越的性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本语义分割方法通常需要从头开始训练额外模块，并在大规模数据集上进行大量训练以达到最佳性能。SAM2是一个零样本图像和视频分割的基础模型，具有模块化设计，但其视频能力需要被重新利用和适应，以处理少样本任务中常见的不同于时间连接帧的图像多样性。

Method: 本文提出FS-SAM2方法，直接重用SAM2的视频能力进行少样本分割任务。此外，为了处理标准数据集中常见的异构图像（而非SAM2预训练中使用的时间连接帧），对SAM2的原始模块应用低秩适应（LoRA）。这种方法仅元训练少量参数，从而有效适应SAM2并利用其强大的分割性能。

Result: FS-SAM2在PASCAL-5$^i$、COCO-20$^i$和FSS-1000数据集上取得了显著结果，并展示了出色的推理计算效率。该方法支持任意K-shot配置。

Conclusion: FS-SAM2成功地将SAM2模型应用于少样本语义分割任务，通过参数高效的LoRA适应，不仅利用了SAM2的强大性能，还保持了优秀的计算效率。

Abstract: Few-shot semantic segmentation has recently attracted great attention. The
goal is to develop a model capable of segmenting unseen classes using only a
few annotated samples. Most existing approaches adapt a pre-trained model by
training from scratch an additional module. Achieving optimal performance with
these approaches requires extensive training on large-scale datasets. The
Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and
video segmentation with a modular design. In this paper, we propose a Few-Shot
segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities
are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank
Adaptation (LoRA) to the original modules in order to handle the diverse images
typically found in standard datasets, unlike the temporally connected frames
used in SAM2's pre-training. With this approach, only a small number of
parameters is meta-trained, which effectively adapts SAM2 while benefiting from
its impressive segmentation performance. Our method supports any K-shot
configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and
FSS-1000 datasets, achieving remarkable results and demonstrating excellent
computational efficiency during inference. Code is available at
https://github.com/fornib/FS-SAM2

</details>


### [187] [RailSafeNet: Visual Scene Understanding for Tram Safety](https://arxiv.org/abs/2509.12125)
*Ing. Ondrej Valach,Ing. Ivan Gruber*

Main category: cs.CV

TL;DR: 本文提出RailSafeNet，一个利用数字图像处理、深度学习和人工智能的实时框架，通过融合语义分割、目标检测和基于规则的距离评估器，使用单目视频提升有轨电车与行人、骑行者等互动时的安全性，并能在危险情况升级前向司机发出警告。


<details>
  <summary>Details</summary>
Motivation: 有轨电车经常在人口稠密区域运行，碰撞可能导致从轻微伤害到致命后果，因此改善有轨电车与人类互动的安全性是一个重要的挑战。

Method: RailSafeNet是一个实时框架，它融合了语义分割、目标检测和一个基于规则的距离评估器（Distance Assessor）来突出轨道入侵。该系统仅使用单目视频，识别轨道、定位附近物体并通过将投影距离与标准1435毫米轨距进行比较来分类其风险。使用了深度学习模型，如SegFormer B3（用于语义分割）和YOLOv8（用于目标检测）。

Result: 在多样化的RailSem19数据集上进行的实验表明，经过类别过滤的SegFormer B3模型实现了65%的交并比（IoU），而经过微调的YOLOv8在IoU阈值为0.50时达到了75.6%的平均精度（mAP）。RailSafeNet因此提供了准确、标注轻量级的场景理解，能够在危险情况升级前向司机发出警告。

Conclusion: RailSafeNet是一个有效的实时AI解决方案，能够准确理解有轨电车运行环境的场景，并通过早期预警显著提高有轨电车与行人等互动时的安全性。

Abstract: Tram-human interaction safety is an important challenge, given that trams
frequently operate in densely populated areas, where collisions can range from
minor injuries to fatal outcomes. This paper addresses the issue from the
perspective of designing a solution leveraging digital image processing, deep
learning, and artificial intelligence to improve the safety of pedestrians,
drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a
real-time framework that fuses semantic segmentation, object detection and a
rule-based Distance Assessor to highlight track intrusions. Using only
monocular video, the system identifies rails, localises nearby objects and
classifies their risk by comparing projected distances with the standard 1435mm
rail gauge. Experiments on the diverse RailSem19 dataset show that a
class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),
while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated
at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore
delivers accurate, annotation-light scene understanding that can warn drivers
before dangerous situations escalate. Code available at
https://github.com/oValach/RailSafeNet.

</details>


### [188] [Open-ended Hierarchical Streaming Video Understanding with Vision Language Models](https://arxiv.org/abs/2509.12145)
*Hyolim Kang,Yunsu Park,Youngbeom Yoo,Yeeun Choi,Seon Joo Kim*

Main category: cs.CV

TL;DR: 本文提出“分层流式视频理解”任务，结合在线时序动作定位和自由形式描述生成。利用LLM丰富现有数据集，并提出OpenHOUSE系统，显著提升了紧密相邻动作的边界检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏分层和细粒度的时序标注，且流式动作感知任务通常仅限于动作分类。研究旨在将流式动作感知扩展到更复杂的、包含自由形式描述的层级理解。

Method: 引入LLM将原子动作分组为更高级别的事件，以丰富现有数据集。提出OpenHOUSE（开放式分层在线事件理解系统），其中包含一个专门的流式模块，用于精确检测紧密相邻动作之间的边界。

Result: LLM能有效将原子动作分组为高级事件，丰富了数据集。OpenHOUSE在紧密相邻动作的边界检测方面，性能几乎是现有方法直接扩展的两倍。

Conclusion: 未来流式动作感知的方向是集成强大的生成模型，OpenHOUSE是实现这一愿景的关键一步。

Abstract: We introduce Hierarchical Streaming Video Understanding, a task that combines
online temporal action localization with free-form description generation.
Given the scarcity of datasets with hierarchical and fine-grained temporal
annotations, we demonstrate that LLMs can effectively group atomic actions into
higher-level events, enriching existing datasets. We then propose OpenHOUSE
(Open-ended Hierarchical Online Understanding System for Events), which extends
streaming action perception beyond action classification. OpenHOUSE features a
specialized streaming module that accurately detects boundaries between closely
adjacent actions, nearly doubling the performance of direct extensions of
existing methods. We envision the future of streaming action perception in the
integration of powerful generative models, with OpenHOUSE representing a key
step in that direction.

</details>


### [189] [LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury](https://arxiv.org/abs/2509.12155)
*M. Bolhassani,B. Veasey,E. Daugherty,S. Keltner,N. Kumar,N. Dunlap,A. Amini*

Main category: cs.CV

TL;DR: 本研究发现，LoRA能高效且有效地微调大型视觉模型（如DinoV2和SwinV2），用于从CT扫描中诊断放射性肺损伤（RILI），其性能与传统完全微调相当或更优，同时显著降低了计算成本和训练时间。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估低秩适应（LoRA）在微调大型视觉模型（DinoV2和SwinV2）以诊断立体定向放疗（SBRT）后X射线CT扫描中的放射性肺损伤（RILI）方面的有效性。核心动机是寻找一种既高效又鲁棒的方法来适应这些大型模型进行医学图像分析。

Method: 研究比较了LoRA、传统完全微调和仅推理（无微调）三种方法。数据方面，使用了以治疗等中心为中心的两种尺寸（50 mm³和75 mm³）的裁剪CT图像。此外，还探讨了将2D大型视觉模型适应3D数据以评估模型对空间上下文的敏感性的不同技术。

Result: 实验结果表明，LoRA在性能上与传统完全微调相当或更优。更重要的是，LoRA显著减少了计算成本和训练时间，因为它所需的训练参数更少。

Conclusion: LoRA是一种高效且有效的微调大型视觉模型的方法，适用于从CT扫描中诊断放射性肺损伤。它能够在保持或提升性能的同时，大幅降低计算资源和时间消耗，使其成为医学图像分析中适应大型模型的有力工具。

Abstract: This study investigates the efficacy of Low-Rank Adaptation (LoRA) for
fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose
Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic
Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of
this approach, we compare LoRA with traditional full fine-tuning and
inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3
and 75 mm3), centered at the treatment isocenter, in addition to different
adaptation techniques for adapting the 2D LVMs for 3D data were used to
determine the sensitivity of the models to spatial context. Experimental
results show that LoRA achieves comparable or superior performance to
traditional fine-tuning while significantly reducing computational costs and
training times by requiring fewer trainable parameters.

</details>


### [190] [Domain-Adaptive Pretraining Improves Primate Behavior Recognition](https://arxiv.org/abs/2509.12193)
*Felix B. Mueller,Timo Lueddecke,Richard Vogg,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 本研究利用自监督学习，特别是结合V-JEPA模型和域适应预训练（DAP），显著提升了灵长类动物行为识别的准确性，超越了现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉在动物行为研究中潜力巨大，但视频相机陷阱数据的高昂标注成本是瓶颈，阻碍了大规模数据集的创建，因此需要数据高效的学习方法。

Method: 研究采用自监督学习方法，具体是利用预训练的V-JEPA模型，并在此基础上进行域适应预训练（DAP），即使用领域内数据继续预训练过程。

Result: 在两个大猿行为数据集（PanAf和ChimpACT）上，该方法分别在准确率和mAP指标上超越了已发布的SOTA行为识别模型6.1个百分点和6.3个百分点。结果表明，大部分性能提升来源于DAP。

Conclusion: 该方法，尤其是无需标注样本的DAP，在改进动物行为识别方面展现出巨大潜力。

Abstract: Computer vision for animal behavior offers promising tools to aid research in
ecology, cognition, and to support conservation efforts. Video camera traps
allow for large-scale data collection, but high labeling costs remain a
bottleneck to creating large-scale datasets. We thus need data-efficient
learning approaches. In this work, we show that we can utilize self-supervised
learning to considerably improve action recognition on primate behavior. On two
datasets of great ape behavior (PanAf and ChimpACT), we outperform published
state-of-the-art action recognition models by 6.1 %pt. accuracy and 6.3 %pt.
mAP, respectively. We achieve this by utilizing a pretrained V-JEPA model and
applying domain-adaptive pretraining (DAP), i.e. continuing the pretraining
with in-domain data. We show that most of the performance gain stems from the
DAP. Our method promises great potential for improving the recognition of
animal behavior, as DAP does not require labeled samples. Code is available at
https://github.com/ecker-lab/dap-behavior

</details>


### [191] [3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review](https://arxiv.org/abs/2509.12197)
*Salma Galaaoui,Eduardo Valle,David Picard,Nermin Samet*

Main category: cs.CV

TL;DR: 本文对野外LiDAR点云中的3D人体姿态估计和人体网格恢复进行了全面综述，提出了分类法，比较了现有方法、数据集和评估指标，并建立了基准表和指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了系统地理解和组织LiDAR点云中3D人体姿态估计和网格恢复的现有方法，提供统一的比较标准（数据集、评估指标），并促进该领域的研究进展。

Method: 本文采用综合性综述方法，包括：提出结构化分类法对现有方法进行分类；分析每种方法的优缺点和设计选择；对三种常用数据集进行定量比较；统一评估指标的定义；建立基准表以实现公平比较；并概述开放挑战和未来研究方向。此外，还维护了一个配套网页。

Result: 提出了一个结构化的方法分类法；详细分析了各种方法的优势、局限性和设计选择；对常用数据集进行了定量比较；统一了所有评估指标的定义；为两项任务在这些数据集上建立了基准表；并明确了LiDAR-based 3D人体理解领域的开放挑战和关键研究方向。

Conclusion: 该综述为LiDAR点云中的3D人体姿态估计和网格恢复领域提供了一个全面的概览、标准化的比较基准和未来的研究指南，旨在促进该领域的公平比较和持续进步。

Abstract: In this paper, we present a comprehensive review of 3D human pose estimation
and human mesh recovery from in-the-wild LiDAR point clouds. We compare
existing approaches across several key dimensions, and propose a structured
taxonomy to classify these methods. Following this taxonomy, we analyze each
method's strengths, limitations, and design choices. In addition, (i) we
perform a quantitative comparison of the three most widely used datasets,
detailing their characteristics; (ii) we compile unified definitions of all
evaluation metrics; and (iii) we establish benchmark tables for both tasks on
these datasets to enable fair comparisons and promote progress in the field. We
also outline open challenges and research directions critical for advancing
LiDAR-based 3D human understanding. Moreover, we maintain an accompanying
webpage that organizes papers according to our taxonomy and continuously update
it with new studies:
https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR

</details>


### [192] [OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling](https://arxiv.org/abs/2509.12201)
*Yang Zhou,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Haoyu Guo,Zizun Li,Kaijing Ma,Xinyue Li,Yating Wang,Haoyi Zhu,Mingyu Liu,Dingning Liu,Jiange Yang,Zhoujie Fu,Junyi Chen,Chunhua Shen,Jiangmiao Pang,Kaipeng Zhang,Tong He*

Main category: cs.CV

TL;DR: 本文介绍了OmniWorld，一个大规模、多领域、多模态的4D世界建模数据集和基准，旨在解决现有数据在动态复杂性、多样性和标注方面的不足，并促进通用4D世界模型的发展。


<details>
  <summary>Details</summary>
Motivation: 当前4D世界建模领域的发展受到高质量数据稀缺的限制。现有数据集和基准缺乏动态复杂性、多领域多样性以及支持4D几何重建、未来预测和摄像机控制视频生成等关键任务所需的时空标注。

Method: 研究者引入了OmniWorld数据集，它包含新收集的OmniWorld-Game数据集和多个精选的公共数据集，涵盖了不同领域。OmniWorld-Game提供了更丰富的模态、更大的规模和更真实的动态交互。基于此数据集，他们建立了一个具有挑战性的基准。

Result: OmniWorld-Game相比现有合成数据集具有更丰富的模态覆盖、更大的规模和更真实的动态交互。建立的基准揭示了当前最先进（SOTA）方法在建模复杂4D环境方面的局限性。此外，在OmniWorld上微调现有SOTA方法，在4D重建和视频生成任务上取得了显著的性能提升。

Conclusion: OmniWorld是一个强大的资源，可以有效训练和评估4D世界模型，有望加速通用4D世界模型的发展，并最终提升机器对物理世界的整体理解。

Abstract: The field of 4D world modeling - aiming to jointly capture spatial geometry
and temporal dynamics - has witnessed remarkable progress in recent years,
driven by advances in large-scale generative models and multimodal learning.
However, the development of truly general 4D world models remains fundamentally
constrained by the availability of high-quality data. Existing datasets and
benchmarks often lack the dynamic complexity, multi-domain diversity, and
spatial-temporal annotations required to support key tasks such as 4D geometric
reconstruction, future prediction, and camera-control video generation. To
address this gap, we introduce OmniWorld, a large-scale, multi-domain,
multi-modal dataset specifically designed for 4D world modeling. OmniWorld
consists of a newly collected OmniWorld-Game dataset and several curated public
datasets spanning diverse domains. Compared with existing synthetic datasets,
OmniWorld-Game provides richer modality coverage, larger scale, and more
realistic dynamic interactions. Based on this dataset, we establish a
challenging benchmark that exposes the limitations of current state-of-the-art
(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning
existing SOTA methods on OmniWorld leads to significant performance gains
across 4D reconstruction and video generation tasks, strongly validating
OmniWorld as a powerful resource for training and evaluation. We envision
OmniWorld as a catalyst for accelerating the development of general-purpose 4D
world models, ultimately advancing machines' holistic understanding of the
physical world.

</details>


### [193] [LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence](https://arxiv.org/abs/2509.12203)
*Zixin Yin,Xili Dai,Duomin Wang,Xianfang Zeng,Lionel M. Ni,Gang Yu,Heung-Yeung Shum*

Main category: cs.CV

TL;DR: LazyDrag是首个用于多模态扩散Transformer的拖拽编辑方法，通过生成显式对应图，消除了对隐式点匹配的依赖，实现了稳定的全强度反演，无需测试时优化，从而提升了扩散模型的生成能力和编辑精度。


<details>
  <summary>Details</summary>
Motivation: 现有的拖拽编辑方法过度依赖通过注意力机制进行的隐式点匹配，这导致了反演强度减弱和高成本的测试时优化（TTO），严重限制了扩散模型的生成能力，影响了高保真修复和文本引导创作。

Method: LazyDrag通过用户拖拽输入生成一个显式对应图，作为可靠的参考来增强注意力控制。这种方法实现了稳定的全强度反演过程，消除了对测试时优化的需求，从而解锁了模型的生成能力。

Result: LazyDrag自然地将精确的几何控制与文本引导相结合，实现了以前难以实现复杂编辑，例如打开狗的嘴巴并修复内部、生成新物体或进行上下文感知的模糊拖拽修改。它还支持多轮工作流，包括同时移动和缩放操作。在DragBench上，LazyDrag在拖拽精度和感知质量方面优于基线方法，并通过VIEScore和人工评估验证。

Conclusion: LazyDrag不仅在拖拽编辑任务中建立了新的最先进性能，还为编辑范式开辟了新途径，通过消除隐式点匹配的依赖和实现稳定的全强度反演，显著提升了扩散模型的编辑能力和效率。

Abstract: The reliance on implicit point matching via attention has become a core
bottleneck in drag-based editing, resulting in a fundamental compromise on
weakened inversion strength and costly test-time optimization (TTO). This
compromise severely limits the generative capabilities of diffusion models,
suppressing high-fidelity inpainting and text-guided creation. In this paper,
we introduce LazyDrag, the first drag-based image editing method for
Multi-Modal Diffusion Transformers, which directly eliminates the reliance on
implicit point matching. In concrete terms, our method generates an explicit
correspondence map from user drag inputs as a reliable reference to boost the
attention control. This reliable reference opens the potential for a stable
full-strength inversion process, which is the first in the drag-based editing
task. It obviates the necessity for TTO and unlocks the generative capability
of models. Therefore, LazyDrag naturally unifies precise geometric control with
text guidance, enabling complex edits that were previously out of reach:
opening the mouth of a dog and inpainting its interior, generating new objects
like a ``tennis ball'', or for ambiguous drags, making context-aware changes
like moving a hand into a pocket. Additionally, LazyDrag supports multi-round
workflows with simultaneous move and scale operations. Evaluated on the
DragBench, our method outperforms baselines in drag accuracy and perceptual
quality, as validated by VIEScore and human evaluation. LazyDrag not only
establishes new state-of-the-art performance, but also paves a new way to
editing paradigms.

</details>


### [194] [Character-Centric Understanding of Animated Movies](https://arxiv.org/abs/2509.12204)
*Zhongrui Gui,Junyu Xie,Tengda Han,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了一种音视频融合的管道，用于自动鲁棒地识别动画角色，并将其应用于为视障和听障人士生成辅助内容，同时引入了一个新的动画电影数据集CMD-AM。


<details>
  <summary>Details</summary>
Motivation: 现有识别系统难以处理动画角色外观、动作和形变上的极端多样性，导致无法有效地进行角色识别和以角色为中心的动画电影理解。

Method: 该研究提出一个音视频管道，核心是自动从在线资源构建一个包含视觉样本和语音样本的音视频角色库，以实现多模态角色识别。在此基础上，探索了两个下游应用：为视障观众生成音频描述（AD）和为听障观众生成角色感知字幕。为支持研究，引入了包含75部动画电影及全面标注的新数据集CMD-AM。

Result: 与以往基于面部检测的方法相比，该以角色为中心的管道在动画内容的无障碍性和叙事理解方面表现出显著改进。

Conclusion: 所提出的音视频管道能够有效且鲁棒地识别动画角色，从而显著提升了动画内容的无障碍性及观众的叙事理解，并通过新数据集支持了该领域的研究。

Abstract: Animated movies are captivating for their unique character designs and
imaginative storytelling, yet they pose significant challenges for existing
recognition systems. Unlike the consistent visual patterns detected by
conventional face recognition methods, animated characters exhibit extreme
diversity in their appearance, motion, and deformation. In this work, we
propose an audio-visual pipeline to enable automatic and robust animated
character recognition, and thereby enhance character-centric understanding of
animated movies. Central to our approach is the automatic construction of an
audio-visual character bank from online sources. This bank contains both visual
exemplars and voice (audio) samples for each character, enabling subsequent
multi-modal character recognition despite long-tailed appearance distributions.
Building on accurate character recognition, we explore two downstream
applications: Audio Description (AD) generation for visually impaired
audiences, and character-aware subtitling for the hearing impaired. To support
research in this domain, we introduce CMD-AM, a new dataset of 75 animated
movies with comprehensive annotations. Our character-centric pipeline
demonstrates significant improvements in both accessibility and narrative
comprehension for animated content over prior face-detection-based approaches.
For the code and dataset, visit
https://www.robots.ox.ac.uk/~vgg/research/animated_ad/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [195] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

TL;DR: 本研究引入了“风险隐藏攻击”（RCA），这是一种多轮红队攻击框架，旨在揭示金融领域大型语言模型（LLMs）的监管风险漏洞。通过构建金融专用基准FIN-Bench，实验证明RCA能有效绕过主流LLMs，攻击成功率高达93.18%，揭示了当前对齐技术的不足。


<details>
  <summary>Details</summary>
Motivation: 现有红队研究主要关注有害内容，却忽视了金融应用中LLMs的监管风险。本研究旨在调查金融LLMs在监管风险方面的脆弱性。

Method: 引入了“风险隐藏攻击”（RCA），这是一种新颖的多轮框架，通过迭代隐藏监管风险来诱导LLMs产生看似合规但实则违规的响应。构建了FIN-Bench，一个用于评估金融背景下LLM安全性的领域特定基准。

Result: 在FIN-Bench上的大量实验表明，RCA有效绕过了九个主流LLMs，平均攻击成功率（ASR）为93.18%，其中包括在GPT-4.1上达到98.28%，在OpenAI o1上达到97.56%。

Conclusion: 这些发现揭示了当前对齐技术中的一个关键空白，并强调了金融领域对更强审查机制的迫切需求。本工作为推进稳健且领域感知的LLM对齐提供了实用见解。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [196] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）在生成答案之前就能预测其回答的正确性，且这种能力在模型内部计算中期形成。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究大型语言模型（LLMs）是否能在生成答案之前预判其回答的正确性，即模型是否具备自我评估的能力。

Method: 研究人员在LLM读取问题后、生成任何token之前提取模型激活，并训练线性探针来预测模型即将生成的答案是否正确。他们在三个参数量从70亿到700亿的开源模型家族上进行了实验，使用通用琐事问题进行训练，并在分布内和多样化的分布外知识数据集上进行测试，同时与黑盒基线和口头表达的置信度进行了比较。此外，还测试了对数学推理问题的泛化能力，并分析了模型回答“我不知道”与探针分数的相关性。

Result: 训练的“提前正确性方向”探针在分布内和分布外的知识数据集上能有效预测模型回答的成功率，且优于黑盒基线和口头表达的置信度。预测能力在模型中间层饱和，表明自我评估在计算中期出现。然而，这种泛化能力在需要数学推理的问题上表现不佳。此外，模型回答“我不知道”与探针分数高度相关，表明该方向也能捕捉模型的置信度。

Conclusion: LLMs在生成答案之前能够内部预测其回答的正确性，这种自我评估能力在计算中期形成，并且与模型的置信度密切相关。本研究通过探针方法为理解LLM内部机制及其行为（如真实性和自我评估）提供了重要发现。

Abstract: Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [197] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

TL;DR: 计算形态学在语言文档化中的应用受限，本文认为NLP研究与实践脱节，并主张通过用户中心设计（UCD）来弥合差距，以提升工具有效性和研究相关性。


<details>
  <summary>Details</summary>
Motivation: 计算形态学（如形态分割和IGT生成）在语言文档化中的实际应用有限。研究人员认为，这反映了自然语言处理（NLP）研究与实践之间更广泛的错位，若不系统整合用户中心设计，该领域可能脱离实际且效率低下。

Method: 本文是一篇立场论文，通过一个多语言IGT生成模型GlossLM的案例研究来论证用户中心设计原则。研究方法包括对三位文档语言学家进行小规模用户研究，以评估系统在真实文档化场景中的可用性。

Result: 尽管GlossLM模型在基于度量的性能上表现出色，但在实际文档化环境中未能满足核心可用性需求。这些发现引出了关于模型约束、标签标准化、分割和个性化等新的研究问题。

Conclusion: 以用户为中心不仅能开发出更有效的工具，还能发现更丰富、更相关的研究方向。NLP领域应系统整合用户中心设计，以避免研究脱离语境并失去效力。

Abstract: Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [198] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 当大型语言模型（LLMs）面临上下文与内部参数知识冲突时，其行为不一致。本文研究了“熵神经元”在解决此类冲突中的作用，发现它们负责抑制上下文复制行为，并且移除它们会显著改变生成过程。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理上下文信息与内部参数知识冲突时表现不一致，且缺乏普遍接受的解释。先前的研究识别出“熵神经元”，它们对模型输出熵有显著影响但对预测token排名影响适中。本文旨在初步探究这些神经元是否通过抑制上下文复制行为来解决信息冲突。

Method: 本文通过观察“熵神经元”在解决上下文与参数信息冲突中的作用，来调查它们抑制Transformer模型中上下文复制行为的初步主张。研究还通过消融这些神经元来观察其对生成过程的影响。

Result: 研究表明，“熵神经元”负责抑制一系列LLMs中的上下文复制行为。消融这些神经元会导致生成过程发生显著变化。

Conclusion: 这些结果加深了我们对LLMs在处理冲突信息时内部动态的理解。

Abstract: The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [199] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种名为 EthosAgents 的轻量级、通用型多元化对齐方法，旨在模拟多样化的视角和价值观，以应对大型语言模型在医疗等敏感领域中现有对齐方法不足以反映多元文化和个人因素的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地部署在医疗等敏感领域，确保其输出能反映不同人群的多元价值观和视角至关重要。然而，包括 Modular Pluralism 在内的现有对齐方法在医疗领域表现不足，因为个人、文化和情境因素塑造了该领域的多元性。

Method: 本文提出了一种名为 EthosAgents 的轻量级、通用型多元化对齐方法。该方法旨在模拟多样化的视角和价值观，以实现多元化对齐。

Result: 实验结果表明，EthosAgents 在七种不同规模的开放和封闭模型上，在所有三种模式下都显著提升了多元化对齐的效果。

Conclusion: 研究发现，与健康相关的多元化需求需要适应性强且具有规范意识的方法。这为如何在其他高风险领域中让模型更好地尊重多样性提供了见解。

Abstract: As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [200] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

TL;DR: 本文提出了Struct-Bench，一个用于评估包含自然语言的结构化差分隐私合成数据集的框架和基准，旨在解决现有评估方法在处理此类数据时的不足。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）合成数据生成对于利用私有数据集至关重要。然而，现有研究主要关注非结构化文本和图像数据，而企业环境中常见的结构化数据（如表格数据）往往包含自然语言字段。现有的合成数据评估技术（如FID）难以捕捉这类数据集的结构特性和相关性，因此需要一个更合适的评估框架。

Method: 本文提出了Struct-Bench框架和基准。用户需提供数据集结构作为上下文无关文法（CFG）。该基准包含5个真实世界和2个合成生成的数据集，每个都用CFG进行了标注。Struct-Bench还包括不同度量的参考实现和一个排行榜，以提供标准化的评估平台。

Result: 研究表明，即使对于最先进的DP合成数据生成方法，Struct-Bench中的数据集也构成了巨大挑战。此外，通过案例研究展示了如何使用Struct-Bench来改进Private Evolution (PE) 在结构化数据上的合成数据质量。

Conclusion: Struct-Bench为研究人员提供了一个标准化的评估平台，用于基准测试和研究隐私保护合成数据生成方法，特别是在处理包含自然语言的结构化数据方面。该基准和排行榜已公开发布，有助于推动该领域的发展。

Abstract: Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [201] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

TL;DR: 本综述探讨了检索与结构化（RAS）增强生成技术，该技术通过整合动态信息检索和结构化知识表示来解决大型语言模型（LLM）在实际应用中面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在文本生成和推理方面表现出色，但在实际部署中面临幻觉生成、知识过时和领域专业知识有限等关键挑战。

Method: 本研究审查了三种主要方法：1) 检索机制（稀疏、密集和混合方法）用于获取外部知识；2) 文本结构化技术（分类法构建、层次分类和信息提取）用于将非结构化文本转换为有组织表示；3) 结构化表示与LLM的集成方式（基于提示的方法、推理框架和知识嵌入技术）。

Result: 该综述识别了检索效率、结构质量和知识集成方面的技术挑战，并强调了多模态检索、跨语言结构和交互式系统等研究机会。

Conclusion: 本全面概述为研究人员和从业者提供了关于RAS方法、应用和未来发展方向的深刻见解。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [202] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

TL;DR: SearchInstruct是一种创新方法，通过扩展少量人工问题并动态检索领域资源来为大型语言模型（LLMs）构建高质量的指令微调（SFT）数据集，显著提升LLM在特定领域的性能并支持模型编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管监督微调（SFT）对于训练大型语言模型至关重要，能显著增强指令遵循和上下文学习能力，但由于独特的领域限制和数据稀缺性，为特定领域创建合适的训练数据集仍然充满挑战。

Method: 该方法首先利用少量领域特定的人工生成问题，然后使用大型语言模型系统地扩展这些问题。随后，动态检索领域相关资源，为每个扩展后的问题生成准确且符合上下文的答案。

Result: 实验评估表明，SearchInstruct增强了SFT数据集的多样性和质量，从而在专业领域中显著提升了LLM的性能。此外，该方法除了数据集生成外，还能有效促进模型编辑等任务，实现对现有模型的有效更新。

Conclusion: SearchInstruct成功解决了领域特定SFT数据集构建的挑战，通过其创新的问答生成机制提高了LLM在专业领域的表现，并展示了其在模型编辑方面的潜力，为社区提供了可复现的完整实现和资源。

Abstract: Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [203] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

TL;DR: 该研究系统比较了五种多语言Transformer模型在多语言虚假信息检测任务上的表现，并引入了一个新的多语言语料库，发现RemBERT在低资源语言中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 虚假信息跨语言传播迅速，但大多数AI模型仅以英语为基准进行测试。本研究旨在弥补多语言虚假信息检测的空白。

Method: 研究系统比较了mBERT、XLM、XLM-RoBERTa、RemBERT和mT5五种多语言Transformer模型在一个共同的真假分类任务上的表现。为此，引入了一个新的PolyTruth Disinfo Corpus语料库，包含60,486对声明（虚假声明与事实纠正），涵盖25种以上语言和多种主题。

Result: 实验结果显示模型性能存在差异。RemBERT实现了更好的整体准确性，尤其在低资源语言中表现出色；而mBERT和XLM在训练数据稀缺时表现出显著局限性。

Conclusion: 研究讨论了这些性能模式及其对实际部署的影响，并开源了数据集以鼓励进一步实验。研究结果揭示了AI系统在多语言虚假信息检测方面的潜力和当前局限性。

Abstract: Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [204] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

TL;DR: 本研究首次全面评估了大型语言模型（LLMs）在明确离散概率分布上的概率推理能力，发现大型模型表现更强但存在符号敏感性和上下文长度增加导致的性能下降等局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在语言理解和生成方面取得了巨大成功，但在需要概率推理的任务中，它们的行为往往不明确且不一致。此前缺乏对LLMs在明确离散概率分布上推理能力的全面研究。

Method: 研究通过提供概率分布的观测数据，评估LLMs在三种任务上的表现：众数识别（mode identification）、最大似然估计（maximum likelihood estimation）和样本生成（sample generation）。这些任务通过询问联合分布或条件分布来探究LLMs的频率分析、边缘化和生成行为等概率技能。

Result: 结果显示，小型模型和大型模型之间存在明显的性能差距，其中大型模型在推理和样本生成方面表现出更强的能力。然而，研究也揭示了显著的局限性，包括对用于表示概率结果的符号变化敏感，以及随着上下文长度增加，性能下降超过60%。

Conclusion: 本研究详细阐明了LLMs的概率推理能力，并指出了未来改进的关键方向。

Abstract: Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [205] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

TL;DR: 本文提出一个可扩展的模块化框架，用于从科学论文中自动生成多项选择题问答（MCQA）基准。通过评估小型语言模型，发现推理轨迹检索能显著提升模型性能，使其在特定领域超越GPT-4。


<details>
  <summary>Details</summary>
Motivation: 科学知识以前所未有的速度增长，现有的评估基准需要更新以反映新发现，并确保语言模型在当前、多样化的文献上得到测试。

Method: 开发了一个可扩展的模块化框架，直接从大量科学论文语料库中生成MCQA基准。该流程自动化了PDF解析、语义分块、问题生成和模型评估。以放射和癌症生物学领域的22,000篇开放获取文章为例，生成了超过16,000道MCQ。评估了一系列小型语言模型（1.1B-14B参数），比较了基线准确性、基于论文语义块的检索增强生成（RAG）以及从GPT-4提炼的推理轨迹RAG的性能。

Result: 推理轨迹检索持续提升了模型在合成和专家标注基准上的性能。这使得多个小型模型在2023年Astro放射和癌症生物学考试中超越了GPT-4。

Conclusion: 所提出的框架及其结合推理轨迹检索的方法，能够有效地从科学论文中生成高质量的MCQA基准，并显著提高小型语言模型在科学知识评估中的表现，使其在特定领域具备与大型模型（如GPT-4）竞争的能力。

Abstract: As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [206] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 该研究提出了RECAP框架，一个无需重新训练的推理时框架，通过分解同理心为结构化情感推理阶段，显著提升了医疗领域大型语言模型的情感智能和同理心沟通能力，并通过基准测试和临床医生评估得到验证。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的大型语言模型（LLMs）常忽略关键情感线索，提供医学上准确但情感上冷淡的建议。这在患者痛苦和脆弱的临床环境中尤其成问题，因为患者需要同理心沟通来支持安全、依从性和信任。

Method: RECAP（Reflect-Extract-Calibrate-Align-Produce）是一个推理时框架，它在不重新训练模型的情况下，通过将同理心分解为透明的评估理论阶段并暴露每个维度的李克特（Likert）信号，添加了结构化的情感推理，从而产生细致且可审计的响应。

Result: 在EmoBench、SECEU和EQ-Bench基准测试中，RECAP在8B模型上将情感推理能力提高了22-28%，在更大模型上提高了10-13%，优于零样本基线。临床医生评估进一步证实了其卓越的同理心沟通能力。

Conclusion: RECAP表明，模块化、基于理论的提示（prompting）可以系统性地增强医疗AI的情感智能，同时保持部署所需的问责制。

Abstract: Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [207] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出Judge Q，一种低成本的训练方法，通过引入软令牌列表并调整嵌入层，使模型在KV缓存逐出时能有效捕获全局信息，从而在保持解码质量的同时减少性能下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的KV缓存大小随序列长度线性增长，严重影响内存使用和解码效率。现有KV缓存逐出方法通常使用预填充阶段的最后窗口作为查询，计算逐出重要性分数，但这过于关注局部信息，可能忽略或遗漏关键的全局信息。

Method: 本文提出Judge Q，一种新颖的训练方法，通过引入一个软令牌列表来解决上述问题。该方法仅以低训练成本调整模型的嵌入层。通过将软令牌列表连接到输入序列的末尾，训练这些软令牌的注意力图，使其与实际解码令牌的注意力图对齐。这样，软令牌对应的查询能够有效捕获全局信息，更好地评估KV缓存中键值对的重要性，从而在KV缓存逐出时保持解码质量。

Result: 在相同的逐出预算下，Judge Q方法比现有逐出方法表现出更少的性能下降。在Llama-3.1-8B-Instruct和Mistral-7B-Instruct-v0.3模型上，使用LongBench、RULER和Needle-in-a-Haystack基准进行实验验证，结果显示LongBench上性能提升约1点，RULER上提升超过3点。

Conclusion: Judge Q方法能够无缝集成到现有的开源模型中，训练开销极小，显著提升了KV缓存逐出场景下的性能，通过捕获全局信息有效缓解了局部信息关注的局限性。

Abstract: Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [208] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一个名为“自动化错误发现”的框架，用于检测和定义对话式AI中的错误，并引入了基于编码器的SEEED方法实现该框架。SEEED通过改进损失函数和样本选择，显著提高了对未知错误的检测精度，并超越了GPT-4o和Phi-4等基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的对话代理具有流畅性和连贯性，但它们仍会产生不良行为（错误），且在部署过程中难以阻止这些错误触达用户。当前的LLM错误检测方法难以识别未明确指定（如由模型更新或用户行为变化引起）的错误。

Method: 引入了“自动化错误发现”框架，用于检测和定义对话式AI中的错误。提出了SEEED（Soft Clustering Extended Encoder-Based Error Detection）作为其基于编码器的实现方法。SEEED通过增强Soft Nearest Neighbor Loss中负样本的距离权重，并引入基于标签的样本排序来选择高对比度示例以改进表示学习。

Result: SEEED在多个带错误标注的对话数据集上优于包括GPT-4o和Phi-4在内的基线模型。它将未知错误检测的准确性提高了多达8个百分点，并对未知意图检测显示出强大的泛化能力。

Conclusion: SEEED框架及其基于编码器的实现方法，通过创新的损失函数和样本选择策略，有效解决了现有LLM在识别未指定错误方面的局限性，显著提升了对话式AI中未知错误检测的性能和泛化能力。

Abstract: Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [209] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
*Can Wang,Yiqun Chen*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在生物医学和临床循证问答中的表现。结果显示，LLMs在结构化指南上的准确率最高（90%），但在叙述性指南和系统性综述问题上较低（60-70%）。引用量与准确率呈正相关。检索增强提示（RAG）显著提高了事实准确性，强调了来源清晰度和目标检索的重要性，而非仅模型大小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生物医学和临床应用中取得了显著进展，因此需要严格评估它们回答细致入微、基于证据的问题的能力。

Method: 研究构建了一个多源基准，数据来源于Cochrane系统性综述、临床指南（如美国心脏协会的结构化建议）和保险公司的叙述性指导。使用GPT-4o-mini和GPT-5进行评估，分析了模型在不同来源和临床领域的一致性表现。同时，探究了准确率与底层系统性综述引用次数之间的相关性，并测试了模型在提供上下文信息时对证据质量的推理能力。此外，研究还通过检索增强提示（提供金标准摘要、PubMed前3条摘要或随机摘要）来评估其对准确率的影响。

Result: 模型在不同来源和临床领域表现出一致的模式：结构化指南建议的准确率最高（90%），而叙述性指南和系统性综述问题的准确率较低（60-70%）。准确率与底层系统性综述的引用次数呈强相关，引用次数每翻倍，正确答案的几率约增加30%。模型在提供上下文信息时，对证据质量表现出中等推理能力。检索增强提示显著提高了准确率：提供金标准摘要可将先前错误项的准确率提高到0.79；提供PubMed前3条摘要可将准确率提高到0.23；而随机摘要则会降低准确率（0.10）。这些效果在GPT-4o-mini中也得到了体现，表明来源清晰度和目标检索是驱动性能的关键因素。

Conclusion: 本研究突出了LLMs在循证临床问答方面的潜力和局限性。检索增强提示是一种有效的策略，可以提高事实准确性和与源证据的一致性。同时，按专业和问题类型进行分层评估对于理解当前的知识获取和情境化模型性能至关重要。

Abstract: Large Language Models (LLMs) have demonstrated substantial progress in
biomedical and clinical applications, motivating rigorous evaluation of their
ability to answer nuanced, evidence-based questions. We curate a multi-source
benchmark drawing from Cochrane systematic reviews and clinical guidelines,
including structured recommendations from the American Heart Association and
narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe
consistent performance patterns across sources and clinical domains: accuracy
is highest on structured guideline recommendations (90%) and lower on narrative
guideline and systematic review questions (60--70%). We also find a strong
correlation between accuracy and the citation count of the underlying
systematic reviews, where each doubling of citations is associated with roughly
a 30% increase in the odds of a correct answer. Models show moderate ability to
reason about evidence quality when contextual information is supplied. When we
incorporate retrieval-augmented prompting, providing the gold-source abstract
raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed
abstracts (ranked by semantic relevance) improves accuracy to 0.23, while
random abstracts reduce accuracy (0.10, within temperature variation). These
effects are mirrored in GPT-4o-mini, underscoring that source clarity and
targeted retrieval -- not just model size -- drive performance. Overall, our
results highlight both the promise and current limitations of LLMs for
evidence-based clinical question answering. Retrieval-augmented prompting
emerges as a useful strategy to improve factual accuracy and alignment with
source evidence, while stratified evaluation by specialty and question type
remains essential to understand current knowledge access and to contextualize
model performance.

</details>


### [210] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: GAPrune是一种新颖的剪枝框架，通过结合领域重要性和通用语言基础，对大型领域特定嵌入模型进行高效压缩和性能增强，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 领域特定嵌入模型（基于LLM）性能优越但参数量巨大，难以在资源受限环境中部署。现有剪枝方法未能区分通用语义和领域特定模式，导致次优的剪枝决策。

Method: 提出GAPrune框架，通过Fisher信息量化参数的领域重要性，并使用通用领域梯度对齐评估参数行为。这些信号通过领域对齐重要性（DAI）评分结合，以识别对领域任务不重要或与通用目标冲突的参数。

Result: 在FinMTEB和ChemTEB两个领域基准测试中，GAPrune在50%稀疏度的一次性剪枝下，性能保持在密集模型的2.5%以内，并优于所有基线。经过100步再训练后，GAPrune在FinMTEB上实现+4.51%的提升，在ChemTEB上实现+1.73%的提升，表明其剪枝策略不仅保留而且增强了领域特定能力。

Conclusion: 有原则的剪枝策略可以实现模型压缩和增强领域专业化，为研究社区提供了一种新的模型开发方法。

Abstract: Domain-specific embedding models have shown promise for applications that
require specialized semantic understanding, such as coding agents and financial
retrieval systems, often achieving higher performance gains than general
models. However, state-of-the-art embedding models are typically based on LLMs,
which contain billions of parameters, making deployment challenging in
resource-constrained environments. Model compression through pruning offers a
promising solution, but existing pruning methods treat all parameters
uniformly, failing to distinguish between general semantic representations and
domain-specific patterns, leading to suboptimal pruning decisions. Thus, we
propose GAPrune, a pruning framework that addresses this challenge by
considering both domain importance and preserving general linguistic
foundation. Our method uses Fisher Information to measure importance and
general-domain gradient alignment to assess parameter behavior, then combines
these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI
scores indicate that the parameter is either less important for the domain task
or creates conflicts between domain and general objectives. Experiments on two
domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance
within 2.5% of dense models in one-shot pruning at 50% sparsity, while
outperforming all baselines. With retraining in 100 steps, GAPrune achieves
+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our
pruning strategy not only preserves but enhances domain-specific capabilities.
Our findings demonstrate that principled pruning strategies can achieve model
compression and enhanced domain specialization, providing the research
community with a new approach for development.

</details>


### [211] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
*Liqian Feng,Lintao Wang,Kun Hu,Dehui Kong,Zhiyong Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为Text2Sign Diffusion（Text2SignDiff）的无手语词素（gloss-free）扩散模型，用于将口语文本生成为手语姿态序列，旨在解决现有手语生成方法对手语词素标注的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有的手语生成（SLP）方法通常依赖手语词素（gloss）作为中间步骤，但手语词素标注往往难以获取且具有语言特异性，限制了SLP的灵活性和泛化能力。因此，研究人员希望开发一种无需手语词素的SLP方法。

Method: 本文提出了一种无手语词素的潜在扩散模型（latent diffusion model），通过非自回归迭代去噪过程，从噪声潜在手语编码和口语文本联合生成手语序列。同时，设计了一个跨模态手语对齐器（cross-modal signing aligner），学习共享的潜在空间，以桥接手语和口语中的视觉和文本内容，从而支持条件扩散过程。

Result: 在常用的PHOENIX14T和How2Sign数据集上进行的广泛实验表明，该方法有效，并取得了最先进的性能。

Conclusion: Text2SignDiff通过其无手语词素的扩散生成方法和跨模态对齐器，实现了更准确、上下文更相关的手语生成，有效弥合了手语和口语之间的沟通鸿沟，并促进了数字包容性。

Abstract: Sign language production (SLP) aims to translate spoken language sentences
into a sequence of pose frames in a sign language, bridging the communication
gap and promoting digital inclusion for deaf and hard-of-hearing communities.
Existing methods typically rely on gloss, a symbolic representation of sign
language words or phrases that serves as an intermediate step in SLP. This
limits the flexibility and generalization of SLP, as gloss annotations are
often unavailable and language-specific. Therefore, we present a novel
diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for
gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed
to generate sign language sequences from noisy latent sign codes and spoken
text jointly, reducing the potential error accumulation through a
non-autoregressive iterative denoising process. We also design a cross-modal
signing aligner that learns a shared latent space to bridge visual and textual
content in sign and spoken languages. This alignment supports the conditioned
diffusion-based process, enabling more accurate and contextually relevant sign
language generation without gloss. Extensive experiments on the commonly used
PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,
achieving the state-of-the-art performance.

</details>


### [212] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 本研究使用脑电图（EEG）比较了人们对AI幽默和人类幽默的认知和情感反应。结果显示，AI幽默虽然被认为同样有趣，但引发了更低的认知努力（N400减小）和更高的惊喜/情感反应（LPP增大），这可能源于对AI能力较低的初始预期。随着时间推移，大脑对AI幽默表现出更高的处理效率和情感奖励，挑战了“算法厌恶”，并表明幽默能促进人机互动中的真实参与。


<details>
  <summary>Details</summary>
Motivation: 随着AI伴侣能够进行类人交流，包括讲笑话，了解人们对AI幽默的认知和情感反应变得越来越重要。

Method: 本研究采用了脑电图（EEG）技术，并结合行为分析，比较了人们处理来自AI和人类的幽默时的反应。

Result: 行为分析显示，参与者认为AI和人类幽默同样有趣。然而，神经生理学数据显示，AI幽默引发了更小的N400效应，表明在处理不协调性时认知努力减少；同时伴随着更大的晚期正电位（LPP），表明更高程度的惊喜和情感反应，这可能源于对AI幽默能力较低的初始预期。在时间动态上，人类幽默表现出习惯化效应（N400增加，LPP减少），而AI幽默则显示出处理效率和情感奖励的增加（N400减少，LPP增加），这表明大脑动态更新了其对AI能力的预测模型。此外，参与者对AI的社会态度也调节了这些神经反应，对AI信任度越高，情感参与度越强。

Conclusion: 研究结果表明，大脑对AI幽默的反应出乎意料地积极和强烈，凸显了幽默在促进人机社会互动中真实参与的潜力。这种累积强化过程挑战了“算法厌恶”，因为它展示了认知适应AI语言模式如何导致情感奖励的增强。

Abstract: As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


### [213] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)
*Sangyeop Kim,Yohan Lee,Sanghwa Kim,Hyunjong Kim,Sungzoon Cho*

Main category: cs.CL

TL;DR: PREMem是一种新方法，通过将复杂的推理过程从推理阶段转移到记忆构建阶段，为对话式AI提供有效的长期记忆，从而显著提高性能并降低对模型大小的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前对话式AI在跨会话的长期记忆方面存在问题，因为响应生成阶段承担了过多的推理负担，导致性能严重依赖于模型大小。

Method: PREMem在记忆预存储阶段进行推理，提取细粒度的记忆片段（事实、经验、主观信息），并建立跨会话记忆项之间的显式关系（如扩展、转换、含义），从而创建丰富的记忆表示，同时减少交互时的计算需求。

Result: 实验表明，PREMem在所有模型尺寸上都取得了显著的性能提升，小型模型实现了与大型基线模型相当的结果，并且即使在有限的token预算下也能保持有效性。

Conclusion: PREMem通过将推理提前到记忆构建阶段，有效解决了对话式AI长期记忆的挑战，提高了性能和效率，尤其对小型模型具有重要意义。

Abstract: Effective long-term memory in conversational AI requires synthesizing
information across multiple sessions. However, current systems place excessive
reasoning burden on response generation, making performance significantly
dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for
Episodic Memory), a novel approach that shifts complex reasoning processes from
inference to memory construction. PREMem extracts fine-grained memory fragments
categorized into factual, experiential, and subjective information; it then
establishes explicit relationships between memory items across sessions,
capturing evolution patterns like extensions, transformations, and
implications. By performing this reasoning during pre-storage rather than when
generating a response, PREMem creates enriched representations while reducing
computational demands during interactions. Experiments show significant
performance improvements across all model sizes, with smaller models achieving
results comparable to much larger baselines while maintaining effectiveness
even with constrained token budgets. Code and dataset are available at
https://github.com/sangyeop-kim/PREMem.

</details>


### [214] [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)
*Shaohua Fang,Yue Li,Yan Cong*

Main category: cs.CL

TL;DR: 本研究采用跨语言方法（英语和中文）分析了大型语言模型（LLMs）如何处理量词范围解释，并与人类表现进行了比较，发现LLMs在表面范围解释上与人类相似，但在逆向范围解释上存在差异，且模型架构和预训练数据语言背景是关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 量词句常导致解释歧义，且这种歧义因语言而异。研究旨在探讨LLMs如何处理跨语言的量词范围解释，并评估其模拟人类表现的程度。

Method: 采用跨语言方法（英语和中文）研究LLMs的量词范围解释，通过概率评估解释的可能性。使用人类相似性（HS）分数量化LLMs在不同语言组中模拟人类表现的程度。

Result: 大多数LLMs偏好表面范围解释，与人类倾向一致。部分LLMs能区分英语和中文在逆向范围解释上的偏好，反映出与人类相似的模式。HS分数显示LLMs在近似人类行为方面存在差异，但其与人类对齐的总体潜力显著。模型架构、规模，尤其是预训练数据语言背景，显著影响LLMs近似人类量词范围解释的程度。

Conclusion: LLMs在量词范围解释上表现出与人类对齐的潜力，尤其是在表面范围解释方面。然而，它们在处理跨语言的细微差别（如逆向范围解释）时表现不一，这受到模型设计和预训练数据语言背景的显著影响。

Abstract: Sentences with multiple quantifiers often lead to interpretive ambiguities,
which can vary across languages. This study adopts a cross-linguistic approach
to examine how large language models (LLMs) handle quantifier scope
interpretation in English and Chinese, using probabilities to assess
interpretive likelihood. Human similarity (HS) scores were used to quantify the
extent to which LLMs emulate human performance across language groups. Results
reveal that most LLMs prefer the surface scope interpretations, aligning with
human tendencies, while only some differentiate between English and Chinese in
the inverse scope preferences, reflecting human-similar patterns. HS scores
highlight variability in LLMs' approximation of human behavior, but their
overall potential to align with humans is notable. Differences in model
architecture, scale, and particularly models' pre-training data language
background, significantly influence how closely LLMs approximate human
quantifier scope interpretations.

</details>


### [215] [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)
*Yuping Wu,Viktor Schlegel,Warren Del-Pinto,Srinivasan Nandakumar,Iqra Zahid,Yidan Sun,Usama Farghaly Omar,Amirah Jasmine,Arun-Kumar Kaliya-Perumal,Chun Shen Tham,Gabriel Connors,Anil A Bharath,Goran Nenadic*

Main category: cs.CL

TL;DR: Term2Note是一种在强差分隐私(DP)约束下合成长篇临床笔记的方法，它通过分离内容和形式，生成具有高保真度和实用性的隐私保护合成笔记。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，隐私泄露担忧严重限制了真实训练数据的使用。差分隐私(DP)合成数据是一个有前景的解决方案，但在临床笔记合成中，由于其领域特异性和长文本生成的复杂性，平衡隐私保护和数据实用性仍然是一个挑战。

Method: 本文提出了Term2Note方法，用于在强DP约束下合成长篇临床笔记。它通过结构性地分离内容和形式，根据DP医学术语生成分节的笔记内容，每个部分都由独立的DP约束管理。此外，一个DP质量最大化器通过选择高质量的输出进一步增强了合成笔记。

Result: 实验结果表明，Term2Note生成的合成笔记在统计特性上与真实临床笔记高度一致，展现出强大的保真度。此外，在这些合成笔记上训练的多标签分类模型表现与在真实数据上训练的模型相当，证实了其高实用性。与现有DP文本生成基线相比，Term2Note在更少假设下实现了保真度和实用性的显著提升。

Conclusion: Term2Note在强差分隐私约束下，生成了与真实临床笔记高度一致且实用性强的合成笔记，证明了其作为敏感临床笔记隐私保护替代方案的潜力。

Abstract: Training data is fundamental to the success of modern machine learning
models, yet in high-stakes domains such as healthcare, the use of real-world
training data is severely constrained by concerns over privacy leakage. A
promising solution to this challenge is the use of differentially private (DP)
synthetic data, which offers formal privacy guarantees while maintaining data
utility. However, striking the right balance between privacy protection and
utility remains challenging in clinical note synthesis, given its domain
specificity and the complexity of long-form text generation. In this paper, we
present Term2Note, a methodology to synthesise long clinical notes under strong
DP constraints. By structurally separating content and form, Term2Note
generates section-wise note content conditioned on DP medical terms, with each
governed by separate DP constraints. A DP quality maximiser further enhances
synthetic notes by selecting high-quality outputs. Experimental results show
that Term2Note produces synthetic notes with statistical properties closely
aligned with real clinical notes, demonstrating strong fidelity. In addition,
multi-label classification models trained on these synthetic notes perform
comparably to those trained on real data, confirming their high utility.
Compared to existing DP text generation baselines, Term2Note achieves
substantial improvements in both fidelity and utility while operating under
fewer assumptions, suggesting its potential as a viable privacy-preserving
alternative to using sensitive clinical notes.

</details>


### [216] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)
*Xinyu Zhang,Pei Zhang,Shuang Luo,Jialong Tang,Yu Wan,Baosong Yang,Fei Huang*

Main category: cs.CL

TL;DR: 为解决现有文化基准的局限性，本文提出了CultureSynth框架，包含一个多语言文化分类法和基于RAG的数据合成方法，创建了一个大规模文化基准。评估结果显示LLM在文化能力上存在性能分层、参数阈值、架构偏见和地理差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在全球环境中越来越需要文化能力。然而，现有文化基准存在分类法碎片化、领域特异性以及严重依赖手动数据标注等问题。

Method: 本文引入了CultureSynth框架，包括：1) 一个全面的分层多语言文化分类法，涵盖12个主要和130个次要主题；2) 一种基于检索增强生成（RAG）的方法，利用事实知识合成文化相关的问答对。CultureSynth-7合成基准包含19,360个条目（其中4,149个经过手动验证），覆盖7种语言。该研究评估了14个不同规模的流行LLM。

Result: 评估显示LLM性能存在明显分层，其中ChatGPT-4o-Latest和Qwen2.5-72B-Instruct表现领先。结果表明，达到基本文化能力需要3B参数的阈值；模型在知识处理上表现出不同的架构偏见；以及模型之间存在显著的地理差异。

Conclusion: CultureSynth提供了一个可扩展的框架，用于开发具有文化意识的AI系统，同时减少了对手动标注的依赖。

Abstract: Cultural competence, defined as the ability to understand and adapt to
multicultural contexts, is increasingly vital for large language models (LLMs)
in global environments. While several cultural benchmarks exist to assess LLMs'
cultural competence, current evaluations suffer from fragmented taxonomies,
domain specificity, and heavy reliance on manual data annotation. To address
these limitations, we introduce CultureSynth, a novel framework comprising (1)
a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary
and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based
methodology leveraging factual knowledge to synthesize culturally relevant
question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360
entries and 4,149 manually verified entries across 7 languages. Evaluation of
14 prevalent LLMs of different sizes reveals clear performance stratification
led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that
a 3B-parameter threshold is necessary for achieving basic cultural competence,
models display varying architectural biases in knowledge processing, and
significant geographic disparities exist across models. We believe that
CultureSynth offers a scalable framework for developing culturally aware AI
systems while reducing reliance on manual annotation\footnote{Benchmark is
available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [217] [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)
*Tsuyoshi Iwata,Guillaume Comte,Melissa Flores,Ryoma Kondo,Ryohei Hisano*

Main category: cs.CL

TL;DR: 本文提出了一种半自动方法，利用轻量级本体论、模式建模和大型语言模型，将新闻中报告的环境、社会和治理（ESG）事件与联合国全球契约等规范性框架对齐，构建结构化知识图谱，以解释可持续性合规性。


<details>
  <summary>Details</summary>
Motivation: 环境、社会和治理（ESG）数据在监管和投资中的重要性日益增长，需要准确、可解释且国际对齐的非财务风险表示，尤其是在非结构化新闻来源中报告的风险。然而，将这些争议相关数据与联合国全球契约等基于原则的规范性框架对齐面临重大挑战，因为这些框架语言抽象、缺乏标准化分类，且与商业数据提供商的专有分类系统不同。

Method: 本文提出了一种半自动方法。该方法结合了轻量级本体论设计、形式模式建模和大型语言模型（LLMs），将规范性原则转换为以资源描述框架（RDF）表示的可重用模板。这些模板用于从新闻内容中提取相关信息，并填充一个结构化知识图谱，将报告的事件与特定的框架原则联系起来。

Result: 通过该方法，构建了一个可扩展且透明的框架，用于识别和解释对国际可持续性准则的不合规行为。它能够将报告的事件与具体的框架原则联系起来，形成结构化的知识表示。

Conclusion: 该研究提供了一个可扩展且透明的框架，通过将新闻中报告的ESG事件结构化并与国际可持续性准则的规范性原则对齐，有效识别和解释企业对这些准则的不合规行为。

Abstract: The growing importance of environmental, social, and governance data in
regulatory and investment contexts has increased the need for accurate,
interpretable, and internationally aligned representations of non-financial
risks, particularly those reported in unstructured news sources. However,
aligning such controversy-related data with principle-based normative
frameworks, such as the United Nations Global Compact or Sustainable
Development Goals, presents significant challenges. These frameworks are
typically expressed in abstract language, lack standardized taxonomies, and
differ from the proprietary classification systems used by commercial data
providers. In this paper, we present a semi-automatic method for constructing
structured knowledge representations of environmental, social, and governance
events reported in the news. Our approach uses lightweight ontology design,
formal pattern modeling, and large language models to convert normative
principles into reusable templates expressed in the Resource Description
Framework. These templates are used to extract relevant information from news
content and populate a structured knowledge graph that links reported incidents
to specific framework principles. The result is a scalable and transparent
framework for identifying and interpreting non-compliance with international
sustainability guidelines.

</details>


### [218] [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)
*Ankan Mullick,Sombit Bose,Rounak Saha,Ayan Kumar Bhowmick,Aditya Vempaty,Prasenjit Dey,Ravi Kokku,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 本文提出了一种名为Spotlight的新型信息提取范式，旨在通过突出文档中最引人注目的部分来生成简洁、引人入胜的叙述，以提高读者参与度。


<details>
  <summary>Details</summary>
Motivation: 传统的摘要侧重于全面覆盖，而Spotlight则通过选择性地强调有趣内容来促进读者对源材料的更深层次参与，从而解决传统摘要在读者参与度方面的不足。

Method: Spotlight方法包括两个阶段：首先，使用为该工作策展的新数据集对大型语言模型进行微调；其次，通过直接偏好优化（DPO）进行对齐。研究还正式区分了Spotlight与相关概念，并进行了详细的基准测试研究。

Result: 综合评估表明，所产生的模型不仅能精确识别关键元素，还能增强可读性，并显著提升原始文档的参与价值。

Conclusion: Spotlight范式及其两阶段生成方法能够有效生成高质量的、引人入胜的叙述，从而提高信息提取的效率和读者的参与度。

Abstract: In this paper, we introduce Spotlight, a novel paradigm for information
extraction that produces concise, engaging narratives by highlighting the most
compelling aspects of a document. Unlike traditional summaries, which
prioritize comprehensive coverage, spotlights selectively emphasize intriguing
content to foster deeper reader engagement with the source material. We
formally differentiate spotlights from related constructs and support our
analysis with a detailed benchmarking study using new datasets curated for this
work. To generate high-quality spotlights, we propose a two-stage approach:
fine-tuning a large language model on our benchmark data, followed by alignment
via Direct Preference Optimization (DPO). Our comprehensive evaluation
demonstrates that the resulting model not only identifies key elements with
precision but also enhances readability and boosts the engagement value of the
original document.

</details>


### [219] [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)
*Lihi Nofar,Tomer Portal,Aviv Elbaz,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 本文提出一个可解释的点击诱饵检测模型，不仅识别点击诱饵标题，还能归因其具体语言操纵策略。通过合成数据集和两阶段框架（BERT/LLM检测与BERT策略归因），旨在提升对抗操纵性媒体内容的AI系统透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题的泛滥严重挑战了信息可信度和用户对数字媒体的信任。尽管机器学习在检测操纵性内容方面有所进步，但缺乏可解释性限制了其实际应用。

Method: 研究引入了一个通过系统性增强真实新闻标题生成的合成数据集，该数据集基于预定义的点击诱饵策略目录。提出了一个两阶段自动点击诱饵分析框架：第一阶段使用微调BERT分类器与GPT-4.0和Gemini 2.4 Flash等大型语言模型（LLMs）进行零样本和少样本检测；第二阶段使用专门的BERT分类器预测标题中存在的具体点击诱饵策略。

Result: 本文提出了一个能够识别点击诱饵标题并将其归因于特定语言操纵策略的模型。构建了一个用于受控实验和模型行为详细分析的合成数据集。展示了一个包括检测和策略归因的两阶段自动点击诱饵分析框架。

Conclusion: 这项工作推动了透明和可信赖人工智能系统的发展，以有效打击操纵性媒体内容。

Abstract: The proliferation of clickbait headlines poses significant challenges to the
credibility of information and user trust in digital media. While recent
advances in machine learning have improved the detection of manipulative
content, the lack of explainability limits their practical adoption. This paper
presents a model for explainable clickbait detection that not only identifies
clickbait titles but also attributes them to specific linguistic manipulation
strategies. We introduce a synthetic dataset generated by systematically
augmenting real news headlines using a predefined catalogue of clickbait
strategies. This dataset enables controlled experimentation and detailed
analysis of model behaviour. We present a two-stage framework for automatic
clickbait analysis comprising detection and tactic attribution. In the first
stage, we compare a fine-tuned BERT classifier with large language models
(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot
prompting and few-shot prompting enriched with illustrative clickbait headlines
and their associated persuasive tactics. In the second stage, a dedicated
BERT-based classifier predicts the specific clickbait strategies present in
each headline. This work advances the development of transparent and
trustworthy AI systems for combating manipulative media content. We share the
dataset with the research community at
https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection

</details>


### [220] [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: 本文介绍了EmoBench-Reddit，一个用于评估多模态大语言模型（MLLMs）理解复杂主观人类情感的新型分层基准测试，旨在弥补现有评估基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs评估基准主要侧重于客观的视觉问答或图像描述，无法充分评估模型理解复杂和主观人类情感的能力。

Method: 研究团队构建了EmoBench-Reddit数据集，包含350个精心筛选自Reddit的样本，每个样本包括图像、用户提供的文本和经用户标签确认的情感类别（悲伤、幽默、讽刺、快乐）。设计了一个从基本感知到高级认知的分层任务框架，每个数据点包含六个选择题和一个开放式问题，难度递增。感知任务评估模型识别基本视觉元素的能力，认知任务则要求场景推理、意图理解和结合文本语境的深度共情。数据标注质量通过AI辅助（Claude 4）和人工验证相结合的方式确保。

Result: 创建了EmoBench-Reddit，这是一个新颖的、分层的多模态情感理解基准测试。该基准包含350个来自Reddit的样本，每个样本都具有图像、文本和情感类别，并设计了从感知到认知的多难度层级问题，能够更全面地评估MLLMs对复杂和主观人类情感的理解能力。

Conclusion: EmoBench-Reddit成功弥补了当前MLLMs评估基准在复杂主观情感理解方面的不足，为评估模型深层情感认知能力提供了一个重要的工具。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), they
have demonstrated exceptional capabilities across a variety of vision-language
tasks. However, current evaluation benchmarks predominantly focus on objective
visual question answering or captioning, inadequately assessing the models'
ability to understand complex and subjective human emotions. To bridge this
gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for
multimodal emotion understanding. The dataset comprises 350 meticulously
curated samples from the social media platform Reddit, each containing an
image, associated user-provided text, and an emotion category (sad, humor,
sarcasm, happy) confirmed by user flairs. We designed a hierarchical task
framework that progresses from basic perception to advanced cognition, with
each data point featuring six multiple-choice questions and one open-ended
question of increasing difficulty. Perception tasks evaluate the model's
ability to identify basic visual elements (e.g., colors, objects), while
cognition tasks require scene reasoning, intent understanding, and deep empathy
integrating textual context. We ensured annotation quality through a
combination of AI assistance (Claude 4) and manual verification.

</details>


### [221] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)
*Valentin Hofmann,David Heineman,Ian Magnusson,Kyle Lo,Jesse Dodge,Maarten Sap,Pang Wei Koh,Chun Wang,Hannaneh Hajishirzi,Noah A. Smith*

Main category: cs.CL

TL;DR: 本文提出了一种名为“动态基准测试”（Fluid Benchmarking）的新型评估方法，通过借鉴心理测量学，根据语言模型的能力水平动态选择评估项目，从而在效率、有效性、方差和饱和度等多个维度上显著提升语言模型基准测试的质量。


<details>
  <summary>Details</summary>
Motivation: 语言模型基准测试面临多重挑战：全面评估成本高昂、基准测试未能有效衡量预期能力、以及标签错误和基准饱和导致评估质量下降。现有策略通常孤立地解决这些问题，未能全面提升评估质量。

Method: 动态基准测试受心理测量学启发，认为基准项目的相对价值取决于语言模型的能力水平，因此评估应适应每个语言模型。该方法基于现有语言模型评估结果估计项目反应模型，并利用推断量动态选择评估项目，类似于教育领域的计算机自适应测试。它包含两个核心组件：项目反应理论（用于将性能映射到潜在能力空间，提高有效性）和动态项目选择（用于减少方差）。

Result: 实验结果显示，动态基准测试在效率、有效性、方差和饱和度四个维度上均优于随机项目抽样和更复杂的基线方法（包括基于项目反应理论的其他方法）。例如，在MMLU数据集上，使用少50倍的项目，动态基准测试仍能实现更高的有效性和更小的方差。分析表明，项目反应理论提高了有效性，而动态项目选择减少了方差。

Conclusion: 研究结果表明，通过超越静态评估，语言模型基准测试可以得到实质性改进，动态基准测试提供了一种有效的新范式。

Abstract: Language model (LM) benchmarking faces several challenges: comprehensive
evaluations are costly, benchmarks often fail to measure the intended
capabilities, and evaluation quality can degrade due to labeling errors and
benchmark saturation. Although various strategies have been proposed to
mitigate these issues, they tend to address individual aspects in isolation,
neglecting broader questions about overall evaluation quality. Here, we
introduce Fluid Benchmarking, a new evaluation approach that advances LM
benchmarking across multiple dimensions. Inspired by psychometrics, Fluid
Benchmarking is based on the insight that the relative value of benchmark items
depends on an LM's capability level, suggesting that evaluation should adapt to
each LM. Methodologically, Fluid Benchmarking estimates an item response model
based on existing LM evaluation results and uses the inferred quantities to
select evaluation items dynamically, similar to computerized adaptive testing
in education. In our experiments, we compare Fluid Benchmarking against the
common practice of random item sampling as well as more sophisticated
baselines, including alternative methods grounded in item response theory. We
examine four dimensions -- efficiency, validity, variance, and saturation --
and find that Fluid Benchmarking achieves superior performance in all of them
(e.g., higher validity and less variance on MMLU with fifty times fewer items).
Our analysis shows that the two components of Fluid Benchmarking have distinct
effects: item response theory, used to map performance into a latent ability
space, increases validity, while dynamic item selection reduces variance.
Overall, our results suggest that LM benchmarking can be substantially improved
by moving beyond static evaluation.

</details>


### [222] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)
*Priyanshu Priya,Saurav Dudhate,Desai Vishesh Yasheshbhai,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文提出了一个新任务——个性化论证式协商对话生成（PAN-DG），并构建了PACT数据集，该数据集包含由大型语言模型（LLMs）生成的、针对旅游行业的个性化论证式协商对话。研究表明，经过微调的LLMs能有效生成符合个性的理性协商回复，提升了协商对话系统的个性化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 将论证机制整合到协商对话系统中可以提升冲突解决能力，而融入个性属性则能通过匹配个体偏好和风格来增强系统适应性。为了推动协商对话系统在这些方面的能力，研究者旨在结合这两点。

Method: 1. 提出了一个新任务：个性化论证式协商对话生成（PAN-DG）。2. 构建了PACT数据集，该数据集是为旅游行业设计的、由大型语言模型（LLMs）生成的个性化论证式协商对话。数据集包含论证、偏好和购买风格三种不同的个性档案。3. 对PACT数据集进行了自动和人工评估，以验证对话质量。4. 对比了预训练和微调的LLMs在PAN-DG任务上的表现。

Result: 1. PACT数据集的自动和人工评估结果表明其对话质量高。2. 多维度评估显示，经过微调的LLMs能够有效生成在协商过程中具有个性化驱动的理性回复。

Conclusion: PACT数据集有效增强了协商对话系统的个性化和推理能力，为该领域的未来研究奠定了基础。

Abstract: Integrating argumentation mechanisms into negotiation dialogue systems
improves conflict resolution through exchanges of arguments and critiques.
Moreover, incorporating personality attributes enhances adaptability by
aligning interactions with individuals' preferences and styles. To advance
these capabilities in negotiation dialogue systems, we propose a novel
Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG)
task. To support this task, we introduce PACT, a dataset of Personality-driven
Argumentation-based negotiation Conversations for Tourism sector. This dataset,
generated using Large Language Models (LLMs), features three distinct
personality profiles, viz. Argumentation Profile, Preference Profile, and
Buying Style Profile to simulate a variety of negotiation scenarios involving
diverse personalities. Thorough automatic and manual evaluations indicate that
the dataset comprises high-quality dialogues. Further, we conduct comparative
experiments between pre-trained and fine-tuned LLMs for the PAN-DG task.
Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively
generate personality-driven rational responses during negotiations. This
underscores the effectiveness of PACT in enhancing personalization and
reasoning capabilities in negotiation dialogue systems, thereby establishing a
foundation for future research in this domain.

</details>


### [223] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)
*Hongxu Zhou,Hylke Westerdijk,Khondoker Ittehadul Islam*

Main category: cs.CL

TL;DR: 本研究发现，在谬误分类任务中，大型语言模型（LLM）加入语境和情感语气元数据，尤其是后者，往往会降低其性能，基本提示通常优于增强型提示。


<details>
  <summary>Details</summary>
Motivation: 研究语境和情感语气元数据如何影响大型语言模型（LLM）在谬误分类任务中的推理和性能，特别是在政治辩论背景下。

Method: 使用Qwen-3 (8B) 模型，通过基线提示和两种理论驱动的思维链（CoT）框架（语用辩证法和论证周期表），对美国总统辩论中的六种谬误类型进行分类。评估了三种输入设置：纯文本、带语境文本以及带语境和基于音频的情感语气元数据的文本。

Result: 理论提示可以提高可解释性，并在某些情况下提高准确性。然而，添加语境，特别是情感语气元数据，通常会导致性能下降。情感语气元数据会使模型偏向于将陈述标记为“诉诸情感”，从而恶化逻辑推理。总体而言，基本提示通常优于增强型提示，表明额外输入可能因注意力分散而降低谬误分类性能。

Conclusion: 在谬误分类任务中，为LLM提供额外的语境和情感语气元数据，特别是情感语气，往往会损害模型的性能，可能原因在于注意力分散和模型偏见。在这些任务中，简洁的基本提示可能比包含更多信息的提示更有效。

Abstract: This study investigates how context and emotional tone metadata influence
large language model (LLM) reasoning and performance in fallacy classification
tasks, particularly within political debate settings. Using data from U.S.
presidential debates, we classify six fallacy types through various prompting
strategies applied to the Qwen-3 (8B) model. We introduce two theoretically
grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table
of Arguments, and evaluate their effectiveness against a baseline prompt under
three input settings: text-only, text with context, and text with both context
and audio-based emotional tone metadata. Results suggest that while theoretical
prompting can improve interpretability and, in some cases, accuracy, the
addition of context and especially emotional tone metadata often leads to
lowered performance. Emotional tone metadata biases the model toward labeling
statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall,
basic prompts often outperformed enhanced ones, suggesting that attention
dilution from added inputs may worsen rather than improve fallacy
classification in LLMs.

</details>


### [224] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)
*Shiyao Cui,Xijia Feng,Yingkang Wang,Junxiao Yang,Zhexin Zhang,Biplab Sikdar,Hongning Wang,Han Qiu,Minlie Huang*

Main category: cs.CL

TL;DR: 本研究发现表情符号能显著增强大型语言模型(LLMs)的毒性内容生成，并通过绕过安全机制和预训练语料污染来解释这一现象。


<details>
  <summary>Details</summary>
Motivation: 研究观察到表情符号（通常与友好性相关）有时会触发LLMs生成有毒内容，因此旨在探究表情符号是否能增强LLM的毒性生成，并解释其原因。

Method: 研究通过自动化构建包含表情符号的提示语来表达毒性意图，并在5种主流语言的7个著名LLMs上进行实验，包括越狱任务。此外，还进行了模型层面的解释（语义认知、序列生成、分词）以及对预训练语料库的探测，以揭示潜在的关联。

Result: 实验表明，带有表情符号的提示语可以轻易诱导LLMs生成毒性内容。解释性分析指出，表情符号可以作为一种异构语义通道，绕过LLMs的安全机制。进一步的语料库分析揭示了表情符号相关数据污染与毒性生成行为之间可能存在关联。

Conclusion: 表情符号能够显著增强LLMs的毒性内容生成，这可能是因为它们作为一种异构语义通道绕过了安全机制，并且预训练语料中与表情符号相关的数据污染也可能促成了这一现象。

Abstract: Emojis are globally used non-verbal cues in digital communication, and
extensive research has examined how large language models (LLMs) understand and
utilize emojis across contexts. While usually associated with friendliness or
playfulness, it is observed that emojis may trigger toxic content generation in
LLMs. Motivated by such a observation, we aim to investigate: (1) whether
emojis can clearly enhance the toxicity generation in LLMs and (2) how to
interpret this phenomenon. We begin with a comprehensive exploration of
emoji-triggered LLM toxicity generation by automating the construction of
prompts with emojis to subtly express toxic intent. Experiments across 5
mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate
that prompts with emojis could easily induce toxicity generation. To understand
this phenomenon, we conduct model-level interpretations spanning semantic
cognition, sequence generation and tokenization, suggesting that emojis can act
as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue
deeper insights, we further probe the pre-training corpus and uncover potential
correlation between the emoji-related data polution with the toxicity
generation behaviors. Supplementary materials provide our implementation code
and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [225] [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)
*Felix Wang,Boyu Chen,Kerun Xu,Bo Tang,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: Text2Mem是一种统一的内存操作语言，旨在为大型语言模型（LLM）代理提供标准化、可靠且可移植的内存管理，解决现有框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理内存框架功能有限，仅提供少量基本操作，缺乏高阶操作（如合并、提升、降级等），且没有正式可执行的内存命令规范，导致行为不可预测。

Method: Text2Mem定义了一个紧凑而富有表达力的操作集，与编码、存储和检索对齐。每个指令表示为一个基于JSON的模式实例，包含必需字段和语义不变性。解析器将其转换为带标准化参数的类型化操作对象，验证器在执行前确保正确性。适配器将类型化对象映射到SQL原型后端或实际内存框架。需要时集成嵌入或摘要等模型服务，并通过统一的执行契约返回所有结果。

Result: Text2Mem的设计确保了异构后端之间的安全性、确定性和可移植性。它建立了代理内存控制的第一个标准化基础。此外，论文还提出了Text2Mem Bench，一个计划中的基准测试，用于系统评估。

Conclusion: Text2Mem通过提供统一的内存操作语言和标准化流程，解决了LLM代理内存管理的现有痛点，确保了内存操作的可靠性、安全性和跨平台可移植性，为代理的内存控制奠定了标准化基础。

Abstract: Large language model agents increasingly depend on memory to sustain long
horizon interaction, but existing frameworks remain limited. Most expose only a
few basic primitives such as encode, retrieve, and delete, while higher order
operations like merge, promote, demote, split, lock, and expire are missing or
inconsistently supported. Moreover, there is no formal and executable
specification for memory commands, leaving scope and lifecycle rules implicit
and causing unpredictable behavior across systems. We introduce Text2Mem, a
unified memory operation language that provides a standardized pathway from
natural language to reliable execution. Text2Mem defines a compact yet
expressive operation set aligned with encoding, storage, and retrieval. Each
instruction is represented as a JSON based schema instance with required fields
and semantic invariants, which a parser transforms into typed operation objects
with normalized parameters. A validator ensures correctness before execution,
while adapters map typed objects either to a SQL prototype backend or to real
memory frameworks. Model based services such as embeddings or summarization are
integrated when required. All results are returned through a unified execution
contract. This design ensures safety, determinism, and portability across
heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark
that separates schema generation from backend execution to enable systematic
evaluation. Together, these components establish the first standardized
foundation for memory control in agents.

</details>


### [226] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)
*Erion Çano,Ivan Habernal*

Main category: cs.CL

TL;DR: 研究发现，在差分隐私（DP）下微调的大语言模型（LLMs）生成的文本质量（长度、语法、词汇多样性）和下游任务效用会随着隐私强度的增加而显著下降。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）微调LLMs以合成数据保护用户隐私的方法日益流行，但其对生成文本的语言质量和实用性影响尚未得到充分研究。

Method: 本研究在四种隐私级别下，使用三个语料库对五个LLMs进行微调。评估了生成文本的长度、语法正确性和词汇多样性。同时，通过下游分类任务（如书籍类型识别和死因识别）探究了合成数据的实用性。

Result: 结果显示，在更强隐私约束下微调的LLMs生成的文本更短（至少减少77%）、语法正确性更低（至少下降9%），以及双词多样性更差（至少下降10%）。此外，这些文本在下游分类任务中的准确性也有所下降。

Conclusion: 研究得出结论，更强的隐私约束会损害DP微调LLMs生成文本的质量和实用性，这可能对合成数据的实际应用价值产生不利影响。

Abstract: Ensuring user privacy by synthesizing data from large language models (LLMs)
tuned under differential privacy (DP) has become popular recently. However, the
impact of DP fine-tuned LLMs on the quality of the language and the utility of
the texts they produce has not been investigated. In this work, we tune five
LLMs with three corpora under four levels of privacy and assess the length, the
grammatical correctness, and the lexical diversity of the text outputs they
produce. We also probe the utility of the synthetic outputs in downstream
classification tasks such as book genre recognition based on book descriptions
and cause of death recognition based on verbal autopsies. The results indicate
that LLMs tuned under stronger privacy constrains produce texts that are
shorter by at least 77 %, that are less grammatically correct by at least 9 %,
and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the
accuracy they reach in downstream classification tasks decreases, which might
be detrimental to the usefulness of the generated synthetic data.

</details>


### [227] [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)
*Hang Guo,Yawei Li,Luca Benini*

Main category: cs.CL

TL;DR: 该研究提出了一种名为OBR的通用无训练框架，通过误差补偿结合量化和剪枝来压缩大型语言模型（LLM），解决了两种技术对权重分布的冲突要求，实现了显著的加速和内存减少。


<details>
  <summary>Details</summary>
Motivation: 单一的LLM压缩方法（如量化和剪枝）已接近各自极限，难以进一步提升压缩率。虽然结合两种方法前景广阔，但它们对权重分布的要求存在内在冲突：量化偏好紧凑范围，而剪枝则受益于高方差。

Method: 本文提出了一种名为“最优大脑恢复”（Optimal Brain Restoration, OBR）的通用无训练框架。OBR通过误差补偿来协调剪枝和量化。它通过构建一个二阶Hessian目标来最小化下游任务的性能下降，然后通过替代近似将其重构为一个可处理的问题，并最终通过组误差补偿达到闭式解。

Result: 实验表明，OBR能够在现有LLM上实现激进的W4A4KV4量化和50%的稀疏性，与FP16-dense基线相比，实现了高达4.72倍的加速和6.4倍的内存减少。

Conclusion: OBR框架成功地结合了量化和稀疏性，解决了它们之间固有的冲突，为LLM的进一步压缩提供了一个有效的解决方案，并带来了显著的性能提升和资源节约。

Abstract: Recent advances in Large Language Model (LLM) compression, such as
quantization and pruning, have achieved notable success. However, as these
techniques gradually approach their respective limits, relying on a single
method for further compression has become increasingly challenging. In this
work, we explore an alternative solution by combining quantization and
sparsity. This joint approach, though promising, introduces new difficulties
due to the inherently conflicting requirements on weight distributions:
quantization favors compact ranges, while pruning benefits from high variance.
To attack this problem, we propose Optimal Brain Restoration (OBR), a general
and training-free framework that aligns pruning and quantization by error
compensation between both. OBR minimizes performance degradation on downstream
tasks by building on a second-order Hessian objective, which is then
reformulated into a tractable problem through surrogate approximation and
ultimately reaches a closed-form solution via group error compensation.
Experiments show that OBR enables aggressive W4A4KV4 quantization with 50%
sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory
reduction compared to the FP16-dense baseline.

</details>


### [228] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)
*Jian Chen,Shengyi Lv,Leilei Su*

Main category: cs.CL

TL;DR: 本文提出随机对抗训练（RAT）框架，将其应用于生物医学信息抽取（BioIE）任务。RAT在提升模型泛化性和鲁棒性的同时，显著降低了计算成本，有效解决了传统对抗训练计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的对抗训练虽然能显著提升预训练语言模型在BioIE任务上的性能，但会引入大量的计算开销。研究旨在找到一种既能保持性能提升又能提高计算效率的解决方案。

Method: 研究以PubMedBERT为基础架构，首先验证了传统对抗训练在BioIE任务上的有效性。在此基础上，提出了随机对抗训练（RAT）框架，该框架将随机采样机制与对抗训练原理相结合，以实现提升模型性能和降低计算成本的双重目标。

Result: 通过全面的评估，RAT在BioIE任务中表现出优于基线模型的性能。结果表明，RAT在增强模型泛化性和鲁棒性的同时，显著降低了计算成本。

Conclusion: RAT为生物医学自然语言处理提供了一个变革性的框架，它在模型性能和计算效率之间取得了平衡，具有巨大的应用潜力。

Abstract: We introduce random adversarial training (RAT), a novel framework
successfully applied to biomedical information extraction (BioIE) tasks.
Building on PubMedBERT as the foundational architecture, our study first
validates the effectiveness of conventional adversarial training in enhancing
pre-trained language models' performance on BioIE tasks. While adversarial
training yields significant improvements across various performance metrics, it
also introduces considerable computational overhead. To address this
limitation, we propose RAT as an efficiency solution for biomedical information
extraction. This framework strategically integrates random sampling mechanisms
with adversarial training principles, achieving dual objectives: enhanced model
generalization and robustness while significantly reducing computational costs.
Through comprehensive evaluations, RAT demonstrates superior performance
compared to baseline models in BioIE tasks. The results highlight RAT's
potential as a transformative framework for biomedical natural language
processing, offering a balanced solution to the model performance and
computational efficiency.

</details>


### [229] [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)
*Valentin Romanov,Steven A Niederer*

Main category: cs.CL

TL;DR: 本文旨在为生命科学领域研究人员提供可操作的提示工程核心技术指南，将58种技术提炼为6种核心方法，以提高LLM响应质量和研究效率，并减少使用摩擦。


<details>
  <summary>Details</summary>
Motivation: 开发有效的提示以从大型语言模型（LLM）获取可靠、高质量的响应需要大量的认知投入。尽管存在许多提示工程技术，但缺乏可操作的指导，导致研究人员在导航各种方法时遇到困难，从而阻碍了效率提升。

Method: 作者将2025年《提示报告》中概述的58种基于文本的提示工程技术提炼为6种核心技术：零样本、少样本、思维生成、集成、自我批评和分解。文章详细阐述了每种方法的意义及其在生命科学（如文献摘要、数据提取、编辑任务）中的用例，并提供了提示结构化的详细建议，解决了多轮对话退化、幻觉等常见陷阱。此外，还探讨了上下文窗口限制、代理工具（如Claude Code）以及OpenAI、Google、Anthropic和Perplexity平台上的深度研究工具的有效性及局限性。

Result: 本文展示了提示工程如何增强而非取代现有数据处理和文档编辑的个体实践，并提供了关于核心提示工程原则的可操作指导。通过这种方法，研究人员可以更有效地利用LLMs，避免常见陷阱，并将其从机会主义提示转变为高效、低摩擦的系统实践。

Conclusion: 通过提供核心提示工程原则的可操作指导，本文旨在促进从机会主义提示到有效、低摩擦的系统化实践的转变，从而有助于提升研究质量。提示工程能够显著增强生命科学领域的研究效率和成果。

Abstract: Developing effective prompts demands significant cognitive investment to
generate reliable, high-quality responses from Large Language Models (LLMs). By
deploying case-specific prompt engineering techniques that streamline
frequently performed life sciences workflows, researchers could achieve
substantial efficiency gains that far exceed the initial time investment
required to master these techniques. The Prompt Report published in 2025
outlined 58 different text-based prompt engineering techniques, highlighting
the numerous ways prompts could be constructed. To provide actionable
guidelines and reduce the friction of navigating these various approaches, we
distil this report to focus on 6 core techniques: zero-shot, few-shot
approaches, thought generation, ensembling, self-criticism, and decomposition.
We breakdown the significance of each approach and ground it in use cases
relevant to life sciences, from literature summarization and data extraction to
editorial tasks. We provide detailed recommendations for how prompts should and
shouldn't be structured, addressing common pitfalls including multi-turn
conversation degradation, hallucinations, and distinctions between reasoning
and non-reasoning models. We examine context window limitations, agentic tools
like Claude Code, while analyzing the effectiveness of Deep Research tools
across OpenAI, Google, Anthropic and Perplexity platforms, discussing current
limitations. We demonstrate how prompt engineering can augment rather than
replace existing established individual practices around data processing and
document editing. Our aim is to provide actionable guidance on core prompt
engineering principles, and to facilitate the transition from opportunistic
prompting to an effective, low-friction systematic practice that contributes to
higher quality research.

</details>


### [230] [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)
*Dasol Choi,Jungwhan Kim,Guijin Son*

Main category: cs.CL

TL;DR: 该论文介绍了Ko-PIQA，一个包含文化背景的韩语物理常识推理数据集，旨在解决现有数据集以英语为中心且缺乏文化多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的物理常识推理数据集（如PIQA）主要以英语为中心，缺乏文化多样性，无法充分评估语言模型在文化相关场景下的推理能力。

Method: 研究人员首先从301万个网络爬取问题中，通过三阶段过滤方法（使用三个语言模型）筛选出11,553个PIQA风格的问题。随后，利用GPT-4o进行精炼并经过人工验证，最终获得了441个高质量的问答对，其中19.7%的问题包含韩国特有的文化元素。

Result: Ko-PIQA数据集成功创建，其显著特点是包含丰富的文化背景问题。在Ko-PIQA上评估了七个语言模型，最佳模型准确率为83.22%，最差为59.86%，表明模型仍有很大的改进空间。模型在处理文化特定场景时表现尤为挣扎，凸显了文化多样性数据集的重要性。

Conclusion: Ko-PIQA数据集不仅可以作为韩语语言模型的基准测试，也为更具包容性的常识推理研究奠定了基础。该数据集和代码将公开发布。

Abstract: Physical commonsense reasoning datasets like PIQA are predominantly
English-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean
physical commonsense reasoning dataset that incorporates cultural context.
Starting from 3.01 million web-crawled questions, we employed a multi-stage
filtering approach using three language models to identify 11,553 PIQA-style
questions. Through GPT-4o refinement and human validation, we obtained 441
high-quality question-answer pairs. A key feature of Ko-PIQA is its cultural
grounding: 19.7\% of questions contain culturally specific elements like
traditional Korean foods (kimchi), clothing (hanbok), and specialized
appliances (kimchi refrigerators) that require culturally-aware reasoning
beyond direct translation. We evaluate seven language models on Ko-PIQA, with
the best model achieving 83.22\% accuracy while the weakest reaches only
59.86\%, demonstrating significant room for improvement. Models particularly
struggle with culturally specific scenarios, highlighting the importance of
culturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean
language models and a foundation for more inclusive commonsense reasoning
research. The dataset and code will be publicly available.

</details>


### [231] [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)
*Mohamed Tarek,Seif Ahmed,Mohamed Basem*

Main category: cs.CL

TL;DR: 本文介绍了作者在AraHealthQA-2025共享任务Track 2（通用阿拉伯语健康QA，MedArabiQ）中获得第二名的系统，该系统利用Gemini 2.5 Flash模型，结合少样本提示、数据预处理、集成和角色扮演等技术，解决了多项选择和开放式问题。


<details>
  <summary>Details</summary>
Motivation: 参与AraHealthQA-2025共享任务的Track 2 (General Arabic Health QA, MedArabiQ)，旨在解决阿拉伯语临床背景下的健康问答问题。

Method: 对于子任务1（多项选择），使用了Gemini 2.5 Flash模型，结合少样本提示、数据集预处理和三种提示配置的集成。对于子任务2（开放式问答），采用了相同的模型，使用统一提示，并融入了扮演阿拉伯医学专家、少样本示例和后处理技术。

Result: 在AraHealthQA-2025共享任务的Track 2中，其方法在子任务1（多项选择问答）和子任务2（开放式问答）中均获得了第二名。

Conclusion: 所提出的系统和方法，特别是利用Gemini 2.5 Flash模型结合少样本提示、集成和角色扮演等策略，在阿拉伯语临床健康问答任务中表现出色，取得了优异的成绩。

Abstract: We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of
the AraHealthQA-2025 shared task, where our methodology secured 2nd place in
both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended
question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage
the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and
an ensemble of three prompt configurations to improve classification accuracy
on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ
a unified prompt with the same model, incorporating role-playing as an Arabic
medical expert, few-shot examples, and post-processing to generate concise
responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased
variants.

</details>


### [232] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)
*Bowen Jing,Yang Cui,Tianpeng Huang*

Main category: cs.CL

TL;DR: 本文系统性比较了无Transformer和基于Transformer的深度监督学习方法在关系抽取任务上的性能，结果显示基于Transformer的模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 关系抽取在信息抽取中至关重要，能将非结构化文本转化为结构化数据。作者旨在系统地比较不同深度监督学习方法（有无Transformer）在该任务上的表现。

Method: 研究使用了无Transformer架构（如PA-LSTM、C-GCN、AGGCN）和基于Transformer架构（如BERT、RoBERTa、R-BERT）进行比较。评估指标包括Micro F1，并在不同句子长度和不同训练数据集比例的场景下进行。实验在TACRED、TACREV和RE-TACRED数据集上进行。

Result: 实验结果表明，基于Transformer的模型在Micro F1得分上（80-90%）显著优于无Transformer模型（64-67%）。此外，论文还简要回顾了监督关系分类的研究历程，并讨论了大型语言模型在关系抽取中的作用和现状。

Conclusion: 基于Transformer的模型在监督关系抽取任务中表现出卓越的性能，明显优于传统的非Transformer深度学习方法。

Abstract: In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

</details>


### [233] [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)
*Abraham Toluwase Owodunni,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出Layer-Selective LoRA (LayRA)，一种在不完全重训的情况下，通过选择性地在初始和最终层添加LoRA来持续向多语言模型中添加新语言的方法，有效平衡了对现有语言能力的保持和对新语言的学习。


<details>
  <summary>Details</summary>
Motivation: 多语言模型支持新语言需要从头重训，成本高昂且不可行，因为开发者通常不发布预训练数据。简单的持续预训练会导致灾难性遗忘，而经验回放等缓解策略因缺乏原始预训练数据而无法应用。

Method: 研究了在仅有目标语言预训练数据的情况下，持续向多语言模型添加新语言的问题。提出了Layer-Selective LoRA (LayRA) 方法，即在选定的初始层和最终层添加低秩适配器（LoRA），同时冻结模型的其余部分。LayRA基于两个洞察：1) LoRA能减少遗忘；2) 多语言模型在初始层编码源语言，在中间层进行英语推理，并在最终层翻译回源语言。

Result: 通过添加加利西亚语、斯瓦希里语和乌尔都语的组合进行实验，LayRA在保留模型对先前支持语言的能力方面提供了整体最佳的权衡，同时在新语言学习方面与现有方法（如LoRA）具有竞争力。此外，通过模型算术，适配后的模型无需目标语言的指令微调数据即可获得强大的指令遵循能力。

Conclusion: LayRA是一种有效且高效的解决方案，用于在资源有限的情况下，持续扩展多语言模型以支持新语言，同时保持现有能力并能通过模型算术获得指令遵循能力。

Abstract: Multilingual language models are trained on a fixed set of languages, and to
support new languages, the models need to be retrained from scratch. This is an
expensive endeavor and is often infeasible, as model developers tend not to
release their pre-training data. Naive approaches, such as continued
pretraining, suffer from catastrophic forgetting; however, mitigation
strategies like experience replay cannot be applied due to the lack of original
pretraining data. In this work, we investigate the problem of continually
adding new languages to a multilingual model, assuming access to pretraining
data in only the target languages. We explore multiple approaches to address
this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank
Adapters (LoRA) to selected initial and final layers while keeping the rest of
the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,
and (2) multilingual models encode inputs in the source language in the initial
layers, reason in English in intermediate layers, and translate back to the
source language in final layers. We experiment with adding multiple
combinations of Galician, Swahili, and Urdu to pretrained language models and
evaluate each method on diverse multilingual tasks. We find that LayRA provides
the overall best tradeoff between preserving models' capabilities in previously
supported languages, while being competitive with existing approaches such as
LoRA in learning new languages. We also demonstrate that using model
arithmetic, the adapted models can be equipped with strong instruction
following abilities without access to any instruction tuning data in the target
languages.

</details>


### [234] [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)
*Gaurab Chhetri,Darrell Anderson,Boniphace Kutela,Subasish Das*

Main category: cs.CL

TL;DR: 本研究首次对“15分钟城市”概念在Twitter、Reddit和新闻媒体上的公众舆论进行了多平台情感分析，使用压缩型Transformer模型和Llama-3-8B进行标注，发现压缩模型表现良好，并揭示了不同平台的数据特性和挑战。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在首次对“15分钟城市”概念进行多平台（Twitter、Reddit、新闻媒体）的情感分析，以理解公众舆论，并探索在异构文本领域中情感分类的有效方法。

Method: 研究采用压缩型Transformer模型（DistilRoBERTa, DistilBERT, MiniLM, ELECTRA, TinyBERT）和Llama-3-8B进行文本标注。构建了一个处理长短文本、支持一致标注和可复现评估的管道。通过分层5折交叉验证对五种模型进行基准测试，评估指标包括F1分数、AUC和训练时间。

Result: 结果显示，DistilRoBERTa取得了最高的F1分数（0.8292），TinyBERT效率最佳，而MiniLM则表现出最佳的跨平台一致性。新闻数据因类别不平衡导致性能虚高，Reddit数据存在总结损失，Twitter数据则提供了中等挑战。研究还发现，压缩型模型表现出竞争力，挑战了大型模型是必需的假设。

Conclusion: 研究揭示了平台特有的权衡，并为城市规划语境中可扩展、真实世界的情感分类提出了发展方向。压缩型模型在多平台情感分析中表现出色，表明并非总是需要更大的模型。

Abstract: This study presents the first multi-platform sentiment analysis of public
opinion on the 15-minute city concept across Twitter, Reddit, and news media.
Using compressed transformer models and Llama-3-8B for annotation, we classify
sentiment across heterogeneous text domains. Our pipeline handles long-form and
short-form text, supports consistent annotation, and enables reproducible
evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,
ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting
F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1
(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform
consistency. Results show News data yields inflated performance due to class
imbalance, Reddit suffers from summarization loss, and Twitter offers moderate
challenge. Compressed models perform competitively, challenging assumptions
that larger models are necessary. We identify platform-specific trade-offs and
propose directions for scalable, real-world sentiment classification in urban
planning discourse.

</details>


### [235] [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)
*Gaurab Chhetri,Anandi Dutta,Subasish Das*

Main category: cs.CL

TL;DR: CognitiveSky是一个开源、可扩展的框架，专门用于在去中心化社交媒体平台Bluesky上进行实时情感、情绪和叙事分析，它利用Transformer模型处理数据并提供可视化仪表板，同时实现了低成本和高可访问性。


<details>
  <summary>Details</summary>
Motivation: 去中心化社交媒体平台的出现为公共话语的实时分析带来了新的机遇和挑战，需要一个能够有效分析这些平台上海量用户生成内容的工具。

Method: CognitiveSky通过Bluesky的API获取数据，然后应用基于Transformer的模型对大规模用户生成内容进行情感、情绪和叙事标注，生成结构化、可分析的输出。这些输出驱动一个动态仪表板，用于可视化情绪、活动和对话主题的演变模式。整个系统完全基于免费层基础设施构建。

Result: CognitiveSky实现了低运营成本和高可访问性。它能够对Bluesky上的公共话语进行大规模、实时的情感、情绪和叙事分析，并能通过动态仪表板清晰地可视化数据。该框架已成功应用于监测心理健康话语。

Conclusion: CognitiveSky是一个透明、可扩展的计算社会科学工具，它通过连接大型语言模型和去中心化网络，为分析不断变化的数字生态系统提供了可能。其模块化设计使其除了用于心理健康话语监测外，还可应用于虚假信息检测、危机响应和公民情绪分析等多个领域。

Abstract: The emergence of decentralized social media platforms presents new
opportunities and challenges for real-time analysis of public discourse. This
study introduces CognitiveSky, an open-source and scalable framework designed
for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter
or X.com alternative. By ingesting data through Bluesky's Application
Programming Interface (API), CognitiveSky applies transformer-based models to
annotate large-scale user-generated content and produces structured and
analyzable outputs. These summaries drive a dynamic dashboard that visualizes
evolving patterns in emotion, activity, and conversation topics. Built entirely
on free-tier infrastructure, CognitiveSky achieves both low operational cost
and high accessibility. While demonstrated here for monitoring mental health
discourse, its modular design enables applications across domains such as
disinformation detection, crisis response, and civic sentiment analysis. By
bridging large language models with decentralized networks, CognitiveSky offers
a transparent, extensible tool for computational social science in an era of
shifting digital ecosystems.

</details>


### [236] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)
*Amirhossein Abaskohi,Raymond Li,Chuyuan Li,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: CEMTM是一种上下文增强多模态主题模型，利用微调的大型视觉语言模型和分布注意力机制，从包含文本和图像的文档中推断出连贯且可解释的主题结构。它能高效处理多张图像，并通过重建目标确保语义一致性，在多模态基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从包含文本和图像的短文档或长文档中推断出连贯且可解释的主题结构时面临挑战，尤其是在处理每个文档多张图像的效率和模型解释性方面。

Method: CEMTM构建于微调的大型视觉语言模型（LVLMs）之上，以获取上下文嵌入。它采用分布注意力机制来加权令牌级别对主题推断的贡献。一个重建目标用于将基于主题的表示与文档嵌入对齐，以鼓励跨模态的语义一致性。与现有方法不同，CEMTM无需重复编码即可处理每个文档多张图像，并通过显式词-主题和文档-主题分布保持可解释性。

Result: 在六个多模态基准测试中，CEMTM始终优于单模态和多模态基线，取得了2.61的显著平均LLM分数。进一步分析表明，它在下游少样本检索中有效，并能捕获科学文章等复杂领域中视觉基础的语义。

Conclusion: CEMTM是一种有效且可解释的上下文增强多模态主题模型，能够从包含文本和图像的文档中推断出高质量的主题结构。它在性能、多图像处理效率和语义理解方面均表现出色，尤其在复杂领域和下游任务中展示了其强大能力。

Abstract: We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

</details>


### [237] [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)
*Yujian Gan,Yuan Liang,Yanni Lin,Juntao Yu,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文研究了现有LLM在共指消解（CR）中的局限性，并提出了“反向训练与联合推理”和“迭代文档生成”两种新方法，显著提升了CR性能并消除了幻觉。


<details>
  <summary>Details</summary>
Motivation: 共指消解（CR）对许多NLP任务至关重要，但现有的大型语言模型（LLMs）在CR中存在幻觉和性能不佳的问题。

Method: 研究了基于LLM的CR方法（问答模板和文档模板），并提出了两种新技术：1. 反向训练与联合推理（Reversed Training with Joint Inference）；2. 迭代文档生成（Iterative Document Generation）。

Result: 实验表明，反向训练改进了问答模板方法，而迭代文档生成消除了生成源文本中的幻觉并提升了共指消解。整合这些方法和技术提供了一个有效且鲁棒的基于LLM的共指消解解决方案。

Conclusion: 通过整合反向训练和迭代文档生成，本文提出了一种有效且鲁棒的解决方案，成功解决了LLM在共指消解中面临的幻觉和性能问题。

Abstract: Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs
struggle with hallucination and under-performance. In this paper, we
investigate the limitations of existing LLM-based approaches to CR-specifically
the Question-Answering (QA) Template and Document Template methods and propose
two novel techniques: Reversed Training with Joint Inference and Iterative
Document Generation. Our experiments show that Reversed Training improves the
QA Template method, while Iterative Document Generation eliminates
hallucinations in the generated source text and boosts coreference resolution.
Integrating these methods and techniques offers an effective and robust
solution to LLM-based coreference resolution.

</details>


### [238] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)
*Anirban Saha Anik,Md Fahimul Kabir Chowdhury,Andrew Wyckoff,Sagnik Ray Choudhury*

Main category: cs.CL

TL;DR: 本文介绍了CLEF 2025 CheckThat! Lab任务3的系统，该系统利用LLMs（零样本提示和LoRA微调）结合证据选择策略来验证数值和时间声明。系统在验证集上表现良好，但在测试集上泛化能力下降，凸显了证据粒度和模型适应性的重要性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决CLEF 2025 CheckThat! Lab任务3中验证数值和时间声明的挑战，通过检索到的证据来支持或驳斥这些声明。

Method: 研究采用了两种互补方法：使用指令调优的大型语言模型（LLMs）进行零样本提示，以及使用参数高效的LoRA进行监督微调。为提高证据质量，探索了多种证据选择策略，包括全文档输入和使用BM25及MiniLM进行top-k句子过滤。最佳模型是使用LoRA微调的LLaMA。

Result: 使用LoRA微调的LLaMA模型在英文验证集上取得了优异性能。然而，在测试集上的表现显著下降，揭示了泛化能力方面的挑战。

Conclusion: 研究结果强调了证据粒度（granularity）和模型适应性对于实现鲁棒的数值事实验证的重要性。

Abstract: This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,
which focuses on verifying numerical and temporal claims using retrieved
evidence. We explore two complementary approaches: zero-shot prompting with
instruction-tuned large language models (LLMs) and supervised fine-tuning using
parameter-efficient LoRA. To enhance evidence quality, we investigate several
selection strategies, including full-document input and top-k sentence
filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned
with LoRA achieves strong performance on the English validation set. However, a
notable drop in the test set highlights a generalization challenge. These
findings underscore the importance of evidence granularity and model adaptation
for robust numerical fact verification.

</details>


### [239] [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)
*Fabrycio Leite Nakano Almada,Kauan Divino Pouso Mariano,Maykon Adriell Dutra,Victor Emanuel da Silva Monteiro,Juliana Resplande Sant'Anna Gomes,Arlindo Rodrigues Galvão Filho,Anderson da Silva Soares*

Main category: cs.CL

TL;DR: 该论文描述了其在CLEF-2025 CheckThat! Task 2中进行跨20种语言（包括有监督和零样本）声明规范化的提交，通过微调小型语言模型（SLM）和大型语言模型（LLM）提示策略，在15种语言中取得了前三名的成绩。


<details>
  <summary>Details</summary>
Motivation: 声明规范化（将非正式社交媒体帖子转化为简洁、独立的陈述）是自动化事实核查流程中的关键步骤。

Method: 对于有监督语言（高资源），采用微调的小型语言模型（SLM）；对于零样本语言（无训练数据），采用大型语言模型（LLM）提示策略。

Result: 该方法在20种语言中的15种取得了前三名的成绩，其中8种语言获得第二名，包括7种零样本语言中的5种。在葡萄牙语上，系统获得了0.5290的平均METEOR分数，排名第三。

Conclusion: 该研究证明了其SLM和LLM结合的方法在多语言声明规范化任务中的有效性，尤其是在零样本场景下基于LLM的策略表现出色。

Abstract: Claim normalization, the transformation of informal social media posts into
concise, self-contained statements, is a crucial step in automated
fact-checking pipelines. This paper details our submission to the CLEF-2025
CheckThat! Task~2, which challenges systems to perform claim normalization
across twenty languages, divided into thirteen supervised (high-resource) and
seven zero-shot (no training data) tracks.
  Our approach, leveraging fine-tuned Small Language Models (SLMs) for
supervised languages and Large Language Model (LLM) prompting for zero-shot
scenarios, achieved podium positions (top three) in fifteen of the twenty
languages. Notably, this included second-place rankings in eight languages,
five of which were among the seven designated zero-shot languages, underscoring
the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our
initial development language, our system achieved an average METEOR score of
0.5290, ranking third. All implementation artifacts, including inference,
training, evaluation scripts, and prompt configurations, are publicly available
at https://github.com/ju-resplande/checkthat2025_normalization.

</details>


### [240] [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)
*Zhuoxuan Ju,Jingni Wu,Abhishek Purushothama,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文介绍了乔治城大学参加DISRPT 2025话语关系分类共享任务的DeDisCo系统，该系统结合了mt5编码器和Qwen解码器，并利用数据增强和语言特征，最终宏观准确率达到71.28%。


<details>
  <summary>Details</summary>
Motivation: 参加DISRPT 2025话语关系分类共享任务。

Method: 采用了两种方法：一是基于mt5的编码器，二是基于公开Qwen模型的解码器。此外，还通过将英语数据自动翻译成低资源语言来增强训练数据集，并引入了借鉴往届共享任务的额外语言特征。

Result: 系统实现了71.28的宏观准确率。

Conclusion: 论文对结果进行了阐释并提供了错误分析。

Abstract: This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025
shared task on discourse relation classification. We test two approaches, using
an mt5-based encoder and a decoder based approach using the openly available
Qwen model. We also experiment on training with augmented dataset for
low-resource languages using matched data translated automatically from
English, as well as using some additional linguistic features inspired by
entries in previous editions of the Shared Task. Our system achieves a
macro-accuracy score of 71.28, and we provide some interpretation and error
analysis for our results.

</details>


### [241] [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)
*Zhongyang Hu,Naijie Gu,Xiangzhi Tao,Tianhui Gu,Yibing Zhou*

Main category: cs.CL

TL;DR: 本文提出两种方法（基于注意力权重和集成梯度）来改进词汇替换中的候选词排序，通过测量上下文对目标词的影响以及原始句与替换句的语义相似性，有效解决了双向影响建模和语义变化表征的挑战。


<details>
  <summary>Details</summary>
Motivation: 词汇替换中的候选词排序面临挑战，现有方法难以有效建模候选词替换对目标词及其上下文的双向影响，且往往只关注目标位置的语义变化或依赖多评价指标的参数调整，难以准确表征语义变异。

Method: 研究了两种方法：一种基于注意力权重，另一种利用更具解释性的集成梯度。这两种方法都旨在测量上下文词元对目标词元的影响，并通过结合原始句和替换句之间的语义相似性来对候选词进行排序。

Result: 在LS07和SWORDS数据集上的实验表明，这两种方法都提高了排序性能。

Conclusion: 所提出的基于注意力权重和集成梯度的方法能够有效提升词汇替换任务中的候选词排序表现，通过更好地建模语义影响和相似性，解决了现有方法的局限性。

Abstract: A key subtask in lexical substitution is ranking the given candidate words. A
common approach is to replace the target word with a candidate in the original
sentence and feed the modified sentence into a model to capture semantic
differences before and after substitution. However, effectively modeling the
bidirectional influence of candidate substitution on both the target word and
its context remains challenging. Existing methods often focus solely on
semantic changes at the target position or rely on parameter tuning over
multiple evaluation metrics, making it difficult to accurately characterize
semantic variation. To address this, we investigate two approaches: one based
on attention weights and another leveraging the more interpretable integrated
gradients method, both designed to measure the influence of context tokens on
the target token and to rank candidates by incorporating semantic similarity
between the original and substituted sentences. Experiments on the LS07 and
SWORDS datasets demonstrate that both approaches improve ranking performance.

</details>


### [242] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)
*Zhengxiang Wang,Weiling Li,Panagiotis Kaliosis,Owen Rambow,Susan E. Brennan*

Main category: cs.CL

TL;DR: 研究发现，当前大型视觉语言模型（LVLMs）在理解自发对话中重复使用的指代表达方面面临挑战，且无法通过重复监听同一参与者的对话来持续提高性能。


<details>
  <summary>Details</summary>
Motivation: 具身智能体需要理解指代表达，以便在现实世界中执行任务。这要求整合和理解语言、视觉和对话交互。本研究旨在评估LVLMs在此方面的能力。

Method: 研究选取了七个最先进的大型视觉语言模型（LVLMs），让它们作为旁听者，监听一个由人类参与者（两人一组）进行协作式物体匹配任务的自发对话语料库。

Result: 结果表明，当前LVLMs在理解此类任务时仍面临挑战。所有模型都未能随着监听同一对话参与者在多轮任务中重复进行的更多对话而表现出持续的性能提升。

Conclusion: 理解自发对话中的指代表达，特别是其重复使用，对现有LVLMs来说是一个难题，且它们未能有效利用重复的对话信息来提高性能。研究发布了语料库和代码以促进未来研究。

Abstract: During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

</details>


### [243] [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)
*Rodrigo M. Carrillo-Larco,Jesus Lovón Melgarejo,Manuel Castillo-Cara,Gusseppe Bravo-Rocca*

Main category: cs.CL

TL;DR: 本研究评估了医学大型语言模型在秘鲁西班牙语医学考试问题上的表现，并发现MedGemma-27B表现最佳，而经过微调的MedGemma-4B也显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管医学大型语言模型在医学考试中表现出色，但其在西班牙语和拉丁美洲国家的医学问题上的表现和可迁移性尚未被探索。了解这一点对于LLM在拉丁美洲的医疗应用至关重要。

Method: 研究构建了PeruMedQA数据集，包含8,380个秘鲁医学考试的多项选择题（2018-2025年）。选择了8个医学LLM（包括medgemma-4b-it和medgemma-27b-text-it），并使用零样本任务特定提示进行评估。使用参数高效微调（PEFT）和低秩适应（LoRA）技术，对medgemma-4b-it进行了微调，训练数据为2018-2024年的问题，2025年的问题作为测试集。

Result: medgemma-27b-text-it表现优于所有其他模型，在某些情况下正确答案比例超过90%。参数小于10亿的LLM正确答案比例低于60%，有些考试甚至低于50%。经过微调的medgemma-4b-it击败了所有参数小于10亿的LLM，并在多项考试中与参数为70亿的LLM相媲美。

Conclusion: 对于需要西班牙语国家知识库且流行病学特征与秘鲁相似的医学AI应用和研究，建议使用medgemma-27b-text-it或经过微调的medgemma-4b-it。

Abstract: BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

</details>


### [244] [On the Distinctive Co-occurrence Characteristics of Antonymy](https://arxiv.org/abs/2509.11534)
*Zhihan Cao,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: 本研究发现，与其他语义关系相比，反义词对在文本中具有独特的共现模式，表现为高强度、偏好线性顺序和短距离共现。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明反义词对在文本中频繁共现，但缺乏与其他语义关系的比较，因此不清楚这种共现模式是否是反义词所独有的。

Method: 本研究通过使用稳健的共现度量，将反义词与另外三种语义关系在不同词性上进行了比较。

Result: 研究发现反义词在三个方面具有独特性：反义词对共现强度高、存在偏好的线性顺序，并且在短距离内共现。所有结果均已在线提供。

Conclusion: 反义词对在文本中的共现模式是独特的，它以高强度、特定的线性顺序和短距离共现的方式区别于其他语义关系。

Abstract: Antonymy has long received particular attention in lexical semantics.
Previous studies have shown that antonym pairs frequently co-occur in text,
across genres and parts of speech, more often than would be expected by chance.
However, whether this co-occurrence pattern is distinctive of antonymy remains
unclear, due to a lack of comparison with other semantic relations. This work
fills the gap by comparing antonymy with three other relations across parts of
speech using robust co-occurrence metrics. We find that antonymy is distinctive
in three respects: antonym pairs co-occur with high strength, in a preferred
linear order, and within short spans. All results are available online.

</details>


### [245] [A Dynamic Fusion Model for Consistent Crisis Response](https://arxiv.org/abs/2509.01053)
*Xiaoying Song,Anirban Saha Anik,Eduardo Blanco,Vanessa Frias-Martinez,Lingzi Hong*

Main category: cs.CL

TL;DR: 针对危机沟通中语言模型自动回复风格一致性不足的问题，本文提出了一种新的评估指标和融合生成方法，显著提高了回复质量并降低了风格差异。


<details>
  <summary>Details</summary>
Motivation: 危机沟通急需有效的自动化回复，但语言模型生成的回复风格一致性常被忽视，这会影响受影响人群对回复者的信任。目前很少有研究探索维持生成回复风格一致性的方法。

Method: 本文提出了一种评估风格一致性的新指标，并引入了一种基于此指标的融合生成方法。该方法采用两阶段过程：首先评估候选回复的风格，然后通过融合过程在实例层面进行优化和整合，以生成高质量且风格差异显著减少的回复。

Result: 在多个数据集上的实验结果表明，本文提出的方法在回复质量和风格一致性方面均持续优于基线方法，并显著减少了实例间的风格差异。

Conclusion: 本文提出的融合生成方法能有效生成高质量且风格一致性强的危机沟通回复，填补了该领域的研究空白，有助于提升受影响人群的信任。

Abstract: In response to the urgent need for effective communication with
crisis-affected populations, automated responses driven by language models have
been proposed to assist in crisis communications. A critical yet often
overlooked factor is the consistency of response style, which could affect the
trust of affected individuals in responders. Despite its importance, few
studies have explored methods for maintaining stylistic consistency across
generated responses. To address this gap, we propose a novel metric for
evaluating style consistency and introduce a fusion-based generation approach
grounded in this metric. Our method employs a two-stage process: it first
assesses the style of candidate responses and then optimizes and integrates
them at the instance level through a fusion process. This enables the
generation of high-quality responses while significantly reducing stylistic
variation between instances. Experimental results across multiple datasets
demonstrate that our approach consistently outperforms baselines in both
response quality and stylistic uniformity.

</details>


### [246] [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)
*Junjie Hu,Gang Tu,ShengYu Cheng,Jinxin Li,Jinting Wang,Rui Chen,Zhilong Zhou,Dongbo Shan*

Main category: cs.CL

TL;DR: 本文提出HARP框架，通过将大型语言模型（LLM）的隐藏状态空间分解为语义和推理子空间，并利用推理子空间投影作为特征，实现鲁棒且高精度的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: LLMs中的幻觉是其在关键决策中可靠使用的主要障碍。现有幻觉检测方法在解耦语义和推理信息以及保持鲁棒性方面仍存在不足。

Method: HARP框架将LLM的隐藏状态空间分解为语义子空间（编码语言表达）和推理子空间（捕捉内部推理过程）。通过Unembedding层解耦这些子空间，并对Unembedding参数应用奇异值分解（SVD）获取子空间的基础向量。最终，HARP将隐藏状态投影到推理子空间的基础向量上，将这些投影作为幻觉检测的输入特征，从而降低特征维度（约5%）并过滤噪声。

Result: HARP显著降低了特征维度，过滤了大部分噪声，并增强了鲁棒性。在多个数据集上的实验表明，HARP实现了最先进的幻觉检测性能，特别是在TriviaQA数据集上，AUROC达到92.8%，比之前最佳方法高出7.5%。

Conclusion: HARP通过利用LLM内部推理子空间的投影，提供了一种有效且鲁棒的幻觉检测新范式，显著提升了检测性能，为LLM的可靠应用铺平道路。

Abstract: Hallucinations in Large Language Models (LLMs) pose a major barrier to their
reliable use in critical decision-making. Although existing hallucination
detection methods have improved accuracy, they still struggle with
disentangling semantic and reasoning information and maintaining robustness. To
address these challenges, we propose HARP (Hallucination detection via
reasoning subspace projection), a novel hallucination detection framework. HARP
establishes that the hidden state space of LLMs can be decomposed into a direct
sum of a semantic subspace and a reasoning subspace, where the former encodes
linguistic expression and the latter captures internal reasoning processes.
Moreover, we demonstrate that the Unembedding layer can disentangle these
subspaces, and by applying Singular Value Decomposition (SVD) to its
parameters, the basis vectors spanning the semantic and reasoning subspaces are
obtained. Finally, HARP projects hidden states onto the basis vectors of the
reasoning subspace, and the resulting projections are then used as input
features for hallucination detection in LLMs. By using these projections, HARP
reduces the dimension of the feature to approximately 5% of the original,
filters out most noise, and achieves enhanced robustness. Experiments across
multiple datasets show that HARP achieves state-of-the-art hallucination
detection performance; in particular, it achieves an AUROC of 92.8% on
TriviaQA, outperforming the previous best method by 7.5%.

</details>


### [247] [Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL](https://arxiv.org/abs/2509.01058)
*Xiaoying Song,Anirban Saha Anik,Dibakar Barua,Pengcheng Luo,Junhua Ding,Lingzi Hong*

Main category: cs.CL

TL;DR: 本研究提出了一种名为Controlled-Literacy的框架，结合检索增强生成（RAG）和强化学习（RL），根据受众的健康素养水平自动生成定制化的反驳信息，以提高公共卫生传播的公平性和影响力。


<details>
  <summary>Details</summary>
Motivation: 在线健康虚假信息对公众健康构成严重威胁。现有的反驳信息生成方法通常产生统一的回复，忽略了受众健康素养水平可能影响反驳信息的可访问性和有效性。

Method: 该研究提出了一个Controlled-Literacy框架，利用检索增强生成（RAG）和强化学习（RL）来生成适应不同健康素养水平的定制化反驳信息。具体来说，它检索与特定健康素养水平对齐的知识，以支持生成可访问且真实的信息。设计了一个奖励函数，结合了主观用户偏好和基于可读性的客观奖励，以优化反驳信息使其符合目标健康素养水平。

Result: 实验结果表明，Controlled-Literacy框架在生成更易于理解和用户偏好的反驳信息方面优于基线方法。

Conclusion: 这项研究通过提高健康虚假信息反驳信息的可访问性和理解度，为更公平和更具影响力的公共卫生传播做出了贡献。

Abstract: Health misinformation spreading online poses a significant threat to public
health. Researchers have explored methods for automatically generating
counterspeech to health misinformation as a mitigation strategy. Existing
approaches often produce uniform responses, ignoring that the health literacy
level of the audience could affect the accessibility and effectiveness of
counterspeech. We propose a Controlled-Literacy framework using
retrieval-augmented generation (RAG) with reinforcement learning (RL) to
generate tailored counterspeech adapted to different health literacy levels. In
particular, we retrieve knowledge aligned with specific health literacy levels,
enabling accessible and factual information to support generation. We design a
reward function incorporating subjective user preferences and objective
readability-based rewards to optimize counterspeech to the target health
literacy level. Experiment results show that Controlled-Literacy outperforms
baselines by generating more accessible and user-preferred counterspeech. This
research contributes to more equitable and impactful public health
communication by improving the accessibility and comprehension of counterspeech
to health misinformation

</details>


### [248] [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)
*Wensheng Lu,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.CL

TL;DR: 本文提出HiCBench，一个用于评估RAG系统中文档分块质量的基准测试，并引入HiChunk框架，一个基于微调LLM的多级文档结构化方法，以提升RAG性能。


<details>
  <summary>Details</summary>
Motivation: RAG系统中文档分块质量的评估工具不足，现有RAG评估基准因证据稀疏性而无法有效评估分块质量。

Method: 首先分析了现有RAG评估基准在评估文档分块质量方面的不足（证据稀疏性）。然后提出了HiCBench，包含手动标注的多级分块点、合成的证据密集问答对及其证据来源。此外，引入了HiChunk框架，一个基于微调LLM的多级文档结构化框架，并结合Auto-Merge检索算法来提高检索质量。

Result: 实验表明，HiCBench能有效评估不同分块方法对整个RAG流程的影响。HiChunk在合理的时间消耗内实现了更好的分块质量，从而提升了RAG系统的整体性能。

Conclusion: HiCBench解决了RAG中文档分块评估工具不足的问题，而HiChunk则通过改进分块质量有效提升了RAG系统的性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.

</details>


### [249] [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)
*Sabrina Patania,Luca Annese,Anna Lambiase,Anita Pellegrini,Tom Foulsham,Azzurra Ruggeri,Silvia Rossi,Silvia Serino,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 该研究将大型语言模型（LLMs）与具身视角采择相结合，通过PerspAct系统模拟塞尔曼理论的视角采择发展阶段。结果显示GPT能生成符合发展阶段的叙事，且交互中的语言交流能促进其视角采择能力提升，较高发展阶段通常能提高协作效率。


<details>
  <summary>Details</summary>
Motivation: 人类协作离不开语言和具身视角采择，但目前很少有计算模型能同时处理这两方面。

Method: 本研究利用PerspAct系统（整合ReAct范式与LLMs）模拟视角采择的发展阶段，并基于塞尔曼理论。通过扩展的“指令者任务”评估GPT生成符合特定发展阶段内部叙事的能力，并定性（行动选择）和定量（任务效率）评估这些叙事对协作表现的影响。

Result: GPT在任务执行前能可靠地生成符合发展阶段的叙事，但在交互过程中常转向更高级的阶段，表明语言交流有助于完善内部表征。较高的发展阶段通常能增强协作效率，而早期阶段在复杂情境下结果更具变异性。

Conclusion: 研究强调了在LLMs中整合具身视角采择和语言以更好地模拟发展动态的潜力，并强调了在结合语言和具身任务时评估内部言语的重要性。

Abstract: Language and embodied perspective taking are essential for human
collaboration, yet few computational models address both simultaneously. This
work investigates the PerspAct system [1], which integrates the ReAct (Reason
and Act) paradigm with Large Language Models (LLMs) to simulate developmental
stages of perspective taking, grounded in Selman's theory [2]. Using an
extended director task, we evaluate GPT's ability to generate internal
narratives aligned with specified developmental stages, and assess how these
influence collaborative performance both qualitatively (action selection) and
quantitatively (task efficiency). Results show that GPT reliably produces
developmentally-consistent narratives before task execution but often shifts
towards more advanced stages during interaction, suggesting that language
exchanges help refine internal representations. Higher developmental stages
generally enhance collaborative effectiveness, while earlier stages yield more
variable outcomes in complex contexts. These findings highlight the potential
of integrating embodied perspective taking and language in LLMs to better model
developmental dynamics and stress the importance of evaluating internal speech
during combined linguistic and embodied tasks.

</details>


### [250] [D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs](https://arxiv.org/abs/2509.11569)
*Yue Ding,Xiaofang Zhu,Tianze Xia,Junfei Wu,Xinlong Chen,Qiang Liu,Liang Wang*

Main category: cs.CL

TL;DR: 本文提出D$^2$HScore，一个无需训练和标签的幻觉检测框架，通过分析大型语言模型（LLMs）的多层结构和自回归解码过程中的语义离散度（Intra-Layer Dispersion）和语义漂移（Inter-Layer Drift）来捕捉幻觉信号。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然取得了显著成功，但其生成的非事实内容（即“幻觉”）阻碍了实际应用，尤其在金融、安全和医疗等高风险领域，确保LLMs输出的可靠性是一个关键挑战。

Method: 该研究从模型架构和生成动态的角度重新审视幻觉检测。它利用LLMs的多层结构和自回归解码过程，将幻觉信号分解为两个互补维度：每层内token表示的语义广度（层内离散度）和核心概念跨层演变的语义深度（层间漂移）。基于此，提出了D$^2$HScore框架，它联合测量：1) 层内离散度，量化每层内token表示的语义多样性；2) 层间漂移，跟踪关键token表示跨层的渐进式转换。通过注意力信号引导token选择，确保漂移反映有意义的语义演变。

Result: 在五种开源LLMs和五个广泛使用的基准测试上的大量实验表明，D$^2$HScore始终优于现有的无需训练的基线方法。

Conclusion: D$^2$HScore通过捕捉推理过程中表示的水平和垂直动态，提供了一种可解释且轻量级的幻觉检测代理方法。

Abstract: Although large Language Models (LLMs) have achieved remarkable success, their
practical application is often hindered by the generation of non-factual
content, which is called "hallucination". Ensuring the reliability of LLMs'
outputs is a critical challenge, particularly in high-stakes domains such as
finance, security, and healthcare. In this work, we revisit hallucination
detection from the perspective of model architecture and generation dynamics.
Leveraging the multi-layer structure and autoregressive decoding process of
LLMs, we decompose hallucination signals into two complementary dimensions: the
semantic breadth of token representations within each layer, and the semantic
depth of core concepts as they evolve across layers. Based on this insight, we
propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},
a training-free and label-free framework that jointly measures: (1)
\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of
token representations within each layer; and (2) \textbf{Inter-Layer Drift},
which tracks the progressive transformation of key token representations across
layers. To ensure drift reflects the evolution of meaningful semantics rather
than noisy or redundant tokens, we guide token selection using attention
signals. By capturing both the horizontal and vertical dynamics of
representation during inference, D$^2$HScore provides an interpretable and
lightweight proxy for hallucination detection. Extensive experiments across
five open-source LLMs and five widely used benchmarks demonstrate that
D$^2$HScore consistently outperforms existing training-free baselines.

</details>


### [251] [Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges](https://arxiv.org/abs/2509.11570)
*Sampoorna Poria,Xiaolei Huang*

Main category: cs.CL

TL;DR: 该综述全面审视了自2020年以来南亚语言NLP模型（特别是基于Transformer的模型）的现状和挑战，旨在提高社区对数据、模型和评估标准不足的认识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语NLP任务中取得了革命性进展，但低资源语言（尤其是南亚语言）的模型及其评估被忽视。南亚有650多种语言，但计算资源有限或未被现有模型覆盖。因此，需要评估当前阶段和挑战，以指导NLP社区并促进南亚语言模型的发展。

Method: 本研究通过检索自2020年以来的相关文献，对南亚语言NLP模型的现有工作和挑战进行了全面审查，重点关注BERT、T5、GPT等基于Transformer的模型。分析了数据、模型和任务三个关键方面，包括可用数据源、微调策略和领域应用。

Result: 研究发现存在诸多重大问题，包括关键领域（如健康）的数据缺失、语码混合（code-mixing）现象以及缺乏标准化的评估基准。

Conclusion: 该综述旨在提高NLP社区的认识，呼吁进行更有针对性的数据整理，统一适应南亚文化和语言细微差别的基准，并鼓励南亚语言的公平代表性。

Abstract: Rapid developments of large language models have revolutionized many NLP
tasks for English data. Unfortunately, the models and their evaluations for
low-resource languages are being overlooked, especially for languages in South
Asia. Although there are more than 650 languages in South Asia, many of them
either have very limited computational resources or are missing from existing
language models. Thus, a concrete question to be answered is: Can we assess the
current stage and challenges to inform our NLP community and facilitate model
developments for South Asian languages? In this survey, we have comprehensively
examined current efforts and challenges of NLP models for South Asian languages
by retrieving studies since 2020, with a focus on transformer-based models,
such as BERT, T5, & GPT. We present advances and gaps across 3 essential
aspects: data, models, & tasks, such as available data sources, fine-tuning
strategies, & domain applications. Our findings highlight substantial issues,
including missing data in critical domains (e.g., health), code-mixing, and
lack of standardized evaluation benchmarks. Our survey aims to raise awareness
within the NLP community for more targeted data curation, unify benchmarks
tailored to cultural and linguistic nuances of South Asia, and encourage an
equitable representation of South Asian languages. The complete list of
resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.

</details>


### [252] [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)
*Chu-Hsuan Lee,Chen-Chi Chang,Hung-Shin Lee,Yun-Hsiang Hsu,Ching-Yuan Chen*

Main category: cs.CL

TL;DR: 本研究通过对客家语AI聊天机器人TALKA的用户行为分析，结合布鲁姆认知分类和对话行为分类，发现生成式AI聊天机器人能有效支持语言学习、增强学习者自信并促进文化认同，为濒危语言保护提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 随着许多濒危语言面临消失的风险，保护工作日益依赖技术与文化教学策略的结合。本研究旨在探讨生成式AI聊天机器人在低资源语言学习中的作用，特别是其如何支持认知发展、语用协商和社会文化归属感。

Method: 本研究采用双层分析框架，结合布鲁姆认知过程分类和对话行为分类，分析了客家语AI聊天机器人TALKA中的7,077条用户话语。每条话语都根据六个认知水平和十一种对话行为类型（如信息询问、翻译请求、文化查询、创造性语言使用等）进行标注。语用分类进一步揭示了不同对话行为（如反馈、控制指令、社交问候）与特定认知意图的关联。

Result: 结果表明，生成式AI聊天机器人能够以有意义的方式支持语言学习，尤其是在设计时考虑了用户思维和沟通方式的情况下。它们还可以帮助学习者更自信地表达自己，并与自己的文化身份建立联系。TALKA案例提供了实证见解，说明AI介导的对话如何促进低资源语言学习者的认知发展、语用协商和社会文化归属感。

Conclusion: 本研究聚焦于AI辅助语言学习，为技术如何支持语言保护和教育实践提供了新的见解。生成式AI聊天机器人是濒危语言保护和学习的有效工具，其成功关键在于对用户认知和交流模式的深刻理解。

Abstract: With many endangered languages at risk of disappearing, efforts to preserve
them now rely more than ever on using technology alongside culturally informed
teaching strategies. This study examines user behaviors in TALKA, a generative
AI-powered chatbot designed for Hakka language engagement, by employing a
dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive
processes and dialogue act categorization. We analyzed 7,077 user utterances,
each carefully annotated according to six cognitive levels and eleven dialogue
act types. These included a variety of functions, such as asking for
information, requesting translations, making cultural inquiries, and using
language creatively. Pragmatic classifications further highlight how different
types of dialogue acts--such as feedback, control commands, and social
greetings--align with specific cognitive intentions. The results suggest that
generative AI chatbots can support language learning in meaningful
ways--especially when they are designed with an understanding of how users
think and communicate. They may also help learners express themselves more
confidently and connect with their cultural identity. The TALKA case provides
empirical insights into how AI-mediated dialogue facilitates cognitive
development in low-resource language learners, as well as pragmatic negotiation
and socio-cultural affiliation. By focusing on AI-assisted language learning,
this study offers new insights into how technology can support language
preservation and educational practice.

</details>


### [253] [Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification](https://arxiv.org/abs/2509.11604)
*Md. Mithun Hossain,Sanjara,Md. Shakil Hossain,Sudipto Chaki*

Main category: cs.CL

TL;DR: 本文提出SpanEIT框架，通过整合动态跨度交互和图感知记忆机制，解决了实体级情感分类中实体与情感表达复杂交互、跨句依赖和共指一致性等挑战，并在多个数据集上超越了现有SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 实体级情感分类面临多重挑战：实体与情感表达间的微妙复杂交互、跨句依赖、通过共指消解确保同一实体多次提及的情感预测一致性、以及否定、歧义和重叠观点等语言现象。这些复杂性在真实世界的噪声文本数据中尤为突出。

Method: SpanEIT框架通过以下方式实现：为实体和候选情感短语构建基于跨度的表示；利用双向注意力进行细粒度交互；使用图注意力网络捕获句法和共现关系；并通过一个共指感知记忆模块确保跨文档的实体级一致性。

Result: 在FSAD、BARU和IMDB数据集上的实验表明，SpanEIT在准确率和F1分数方面均优于最先进的Transformer和混合基线模型。消融实验和可解释性分析也验证了该方法的有效性。

Conclusion: SpanEIT框架通过其创新机制，有效解决了实体级情感分类的复杂问题。其在细粒度情感分析方面的潜力巨大，尤其适用于社交媒体监控和客户反馈分析等应用场景。

Abstract: Entity-level sentiment classification involves identifying the sentiment
polarity linked to specific entities within text. This task poses several
challenges: effectively modeling the subtle and complex interactions between
entities and their surrounding sentiment expressions; capturing dependencies
that may span across sentences; and ensuring consistent sentiment predictions
for multiple mentions of the same entity through coreference resolution.
Additionally, linguistic phenomena such as negation, ambiguity, and overlapping
opinions further complicate the analysis. These complexities make entity-level
sentiment classification a difficult problem, especially in real-world, noisy
textual data. To address these issues, we propose SpanEIT, a novel framework
integrating dynamic span interaction and graph-aware memory mechanisms for
enhanced entity-sentiment relational modeling. SpanEIT builds span-based
representations for entities and candidate sentiment phrases, employs
bidirectional attention for fine-grained interactions, and uses a graph
attention network to capture syntactic and co-occurrence relations. A
coreference-aware memory module ensures entity-level consistency across
documents. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT
outperforms state-of-the-art transformer and hybrid baselines in accuracy and
F1 scores. Ablation and interpretability analyses validate the effectiveness of
our approach, underscoring its potential for fine-grained sentiment analysis in
applications like social media monitoring and customer feedback analysis.

</details>


### [254] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)
*Spandan Anaokar,Shrey Ganatra,Harshvivek Kashid,Swapnil Bhattacharyya,Shruti Nair,Reshma Sekhar,Siddharth Manohar,Rahul Hemrajani,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本研究针对基于LLaMA 3.1 8B Instruct的消费者投诉聊天机器人，开发了HalluDetect幻觉检测系统（F1达69%），并发现AgentBot架构能有效将幻觉降至每回合0.4159，同时保持96.13%的令牌准确性，提供了一个可扩展的幻觉缓解框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在工业中广泛应用，但其易产生幻觉的问题限制了在关键应用中的可靠性。本研究旨在解决消费者投诉聊天机器人中的幻觉问题。

Method: 研究开发了一个基于LLM的幻觉检测系统HalluDetect。同时，对五种聊天机器人架构进行了基准测试，评估它们在幻觉减少和令牌准确性方面的表现。其中AgentBot被识别为一种有效的缓解策略。

Result: HalluDetect幻觉检测系统实现了69%的F1分数，比基线检测器高出25.44%。在测试的五种聊天机器人架构中，AgentBot将幻觉最小化至每回合0.4159，并保持了最高的令牌准确性（96.13%），成为最有效的缓解策略。

Conclusion: 本研究提供了一个可扩展的幻觉缓解框架，证明了优化的推理策略可以显著提高事实准确性。尽管应用于消费者法律领域，但该方法可推广至其他高风险领域，以增强对LLM驱动助手的信任。

Abstract: Large Language Models (LLMs) are widely used in industry but remain prone to
hallucinations, limiting their reliability in critical applications. This work
addresses hallucination reduction in consumer grievance chatbots built using
LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop
HalluDetect, an LLM-based hallucination detection system that achieves an F1
score of 69% outperforming baseline detectors by 25.44%. Benchmarking five
chatbot architectures, we find that out of them, AgentBot minimizes
hallucinations to 0.4159 per turn while maintaining the highest token accuracy
(96.13%), making it the most effective mitigation strategy. Our findings
provide a scalable framework for hallucination mitigation, demonstrating that
optimized inference strategies can significantly improve factual accuracy.
While applied to consumer law, our approach generalizes to other high-risk
domains, enhancing trust in LLM-driven assistants. We will release the code and
dataset

</details>


### [255] [AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment](https://arxiv.org/abs/2509.11620)
*Kun Li,Lai-Man Po,Hongzheng Yang,Xuyuan Xu,Kangcheng Liu,Yuzhi Zhao*

Main category: cs.CL

TL;DR: 本研究提出了AesBiasBench基准，用于评估多模态大语言模型（MLLMs）在个性化图像美学评估（PIAA）中的刻板印象偏见和与人类偏好的一致性。结果显示，较小的模型偏见更强，而身份信息常会加剧偏见。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在个性化图像美学评估（PIAA）中作为专家评估的可扩展替代方案日益普及，但其预测可能受到性别、年龄、教育等人口统计因素影响，存在微妙的偏见。因此，需要一个评估框架来量化和分析这些偏见。

Method: 研究提出了AesBiasBench基准，从两个维度评估MLLMs：1) 刻板印象偏见（通过衡量不同人口统计群体间美学评估的差异量化）；2) 模型输出与真实人类美学偏好的一致性。该基准涵盖三个子任务（美学感知、评估、同理心），并引入结构化指标（IFD、NRD、AAS）来评估偏见和一致性。研究评估了包括GPT-4o、Claude-3.5-Sonnet、InternVL-2.5、Qwen2.5-VL在内的19个MLLMs。

Result: 评估结果表明，较小的模型表现出更强的刻板印象偏见，而较大的模型与人类偏好的一致性更高。此外，整合身份信息往往会加剧偏见，尤其是在情感判断方面。

Conclusion: 这些发现强调了在主观视觉-语言任务中，身份感知评估框架的重要性。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly applied in
Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to
expert evaluations. However, their predictions may reflect subtle biases
influenced by demographic factors such as gender, age, and education. In this
work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two
complementary dimensions: (1) stereotype bias, quantified by measuring
variations in aesthetic evaluations across demographic groups; and (2)
alignment between model outputs and genuine human aesthetic preferences. Our
benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and
introduces structured metrics (IFD, NRD, AAS) to assess both bias and
alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,
Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).
Results indicate that smaller models exhibit stronger stereotype biases,
whereas larger models align more closely with human preferences. Incorporating
identity information often exacerbates bias, particularly in emotional
judgments. These findings underscore the importance of identity-aware
evaluation frameworks in subjective vision-language tasks.

</details>


### [256] [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648)
*Sai Kartheek Reddy Kasu*

Main category: cs.CL

TL;DR: 本研究引入了一个名为 EthicsMH 的试点数据集，旨在评估大型语言模型在心理健康领域中处理伦理困境的能力，以促进人工智能的负责任部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在心理健康等敏感领域的应用，引发了对伦理推理、公平性和责任对齐的紧急质疑。现有道德和临床决策基准未能充分捕捉心理健康实践中特有的伦理困境，如保密性、自主性、受益性和偏见等交叉问题。

Method: 研究引入了 EthicsMH 数据集，这是一个包含125个场景的试点数据集，用于评估AI系统在治疗和精神病学背景下如何处理伦理敏感情况。每个场景都包含结构化字段，如多个决策选项、专家对齐的推理、预期的模型行为、现实世界影响和多方利益相关者观点。这种结构不仅能评估决策准确性，还能评估解释质量和与专业规范的对齐程度。数据集通过模型辅助生成。

Result: 研究成果是 EthicsMH 数据集，一个虽然规模适中但建立了连接AI伦理与心理健康决策的任务框架。该数据集能够评估AI系统的决策准确性、解释质量以及与专业规范的对齐程度。

Conclusion: EthicsMH 数据集作为一项初步资源，旨在通过社区和专家贡献进行扩展，以促进开发能够负责任地处理社会中最微妙决策的AI系统。

Abstract: The deployment of large language models (LLMs) in mental health and other
sensitive domains raises urgent questions about ethical reasoning, fairness,
and responsible alignment. Yet, existing benchmarks for moral and clinical
decision-making do not adequately capture the unique ethical dilemmas
encountered in mental health practice, where confidentiality, autonomy,
beneficence, and bias frequently intersect. To address this gap, we introduce
Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios
designed to evaluate how AI systems navigate ethically charged situations in
therapeutic and psychiatric contexts. Each scenario is enriched with structured
fields, including multiple decision options, expert-aligned reasoning, expected
model behavior, real-world impact, and multi-stakeholder viewpoints. This
structure enables evaluation not only of decision accuracy but also of
explanation quality and alignment with professional norms. Although modest in
scale and developed with model-assisted generation, EthicsMH establishes a task
framework that bridges AI ethics and mental health decision-making. By
releasing this dataset, we aim to provide a seed resource that can be expanded
through community and expert contributions, fostering the development of AI
systems capable of responsibly handling some of society's most delicate
decisions.

</details>


### [257] [A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection](https://arxiv.org/abs/2509.11687)
*Di Jin,Jun Yang,Xiaobao Wang,Junwei Zhang,Shuqi Li,Dongxiao He*

Main category: cs.CL

TL;DR: 本文提出了一种名为DYNAMO的动态知识更新驱动模型，用于假新闻检测。该模型利用知识图谱实现新知识的持续更新，并结合大型语言模型进行新闻真实性检测和新知识正确性验证，解决了现有检索增强方法中知识可信度不足和噪声干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 随着互联网和社交媒体的快速发展，区分可信新闻变得极具挑战性。新闻事件的突发性和不稳定性可能导致新闻真实性标签随事件发展而变化，因此获取最新事件更新对假新闻检测至关重要。现有方法（如检索增强生成）在填补知识空白时存在检索内容可信度不足和噪声信息干扰的问题。

Method: DYNAMO模型通过以下步骤实现：1. 构建新闻领域特定的知识图谱。2. 使用蒙特卡洛树搜索（Monte Carlo Tree Search）分解复杂新闻并逐步验证。3. 从已验证的真实新闻文本和推理路径中提取并更新新知识。该模型将知识图谱与大型语言模型相结合，实现新闻真实性检测和新知识正确性验证的双重功能。

Result: 实验结果表明，DYNAMO模型在两个真实世界数据集上取得了最佳性能。

Conclusion: DYNAMO模型通过动态知识更新和新知识正确性验证，有效解决了假新闻检测中知识可信度和语义挖掘的挑战，并在实际应用中表现出色。

Abstract: As the Internet and social media evolve rapidly, distinguishing credible news
from a vast amount of complex information poses a significant challenge. Due to
the suddenness and instability of news events, the authenticity labels of news
can potentially shift as events develop, making it crucial for fake news
detection to obtain the latest event updates. Existing methods employ
retrieval-augmented generation to fill knowledge gaps, but they suffer from
issues such as insufficient credibility of retrieved content and interference
from noisy information. We propose a dynamic knowledge update-driven model for
fake news detection (DYNAMO), which leverages knowledge graphs to achieve
continuous updating of new knowledge and integrates with large language models
to fulfill dual functions: news authenticity detection and verification of new
knowledge correctness, solving the two key problems of ensuring the
authenticity of new knowledge and deeply mining news semantics. Specifically,
we first construct a news-domain-specific knowledge graph. Then, we use Monte
Carlo Tree Search to decompose complex news and verify them step by step.
Finally, we extract and update new knowledge from verified real news texts and
reasoning paths. Experimental results demonstrate that DYNAMO achieves the best
performance on two real-world datasets.

</details>


### [258] [CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model](https://arxiv.org/abs/2509.11698)
*Wei-Hsin Yeh,Yu-An Su,Chih-Ning Chen,Yi-Hsueh Lin,Calvin Ku,Wen-Hsin Chiu,Min-Chun Hu,Lun-Wei Ku*

Main category: cs.CL

TL;DR: CoachMe是一个基于参考的运动指导模型，通过分析学习者与参考动作在时间和物理方面的差异，生成精确且特定运动的指导，其在花样滑冰和拳击上的表现显著优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型在动作理解方面有所进步，但由于体育运动的高度领域特定性以及对信息丰富指导的需求，生成精确且特定运动的动作指导仍然具有挑战性。

Method: 本文提出了CoachMe模型，它通过分析学习者的动作与参考动作在时间和物理方面的差异来识别错误并提供改进反馈。该方法能够学习领域知识并模拟教练的思维过程。CoachMe通过学习通用动作并利用有限数据来适应花样滑冰和拳击等特定运动。

Result: 实验表明，CoachMe能够提供高质量的指导，而不仅仅是模仿教练的语气但缺乏关键信息。在G-Eval评估中，CoachMe在花样滑冰项目上比GPT-4o高出31.6%，在拳击项目上高出58.3%。分析进一步证实，CoachMe在生成的指导中详细阐述了错误及其对应的改进方法。

Conclusion: CoachMe是一个有效的参考基模型，能够为运动员提供精确且特定运动的动作指导，显著优于通用大型模型，并通过学习领域知识和提供可操作的反馈来帮助运动员改进技术。

Abstract: Motion instruction is a crucial task that helps athletes refine their
technique by analyzing movements and providing corrective guidance. Although
recent advances in multimodal models have improved motion understanding,
generating precise and sport-specific instruction remains challenging due to
the highly domain-specific nature of sports and the need for informative
guidance. We propose CoachMe, a reference-based model that analyzes the
differences between a learner's motion and a reference under temporal and
physical aspects. This approach enables both domain-knowledge learning and the
acquisition of a coach-like thinking process that identifies movement errors
effectively and provides feedback to explain how to improve. In this paper, we
illustrate how CoachMe adapts well to specific sports such as skating and
boxing by learning from general movements and then leveraging limited data.
Experiments show that CoachMe provides high-quality instructions instead of
directions merely in the tone of a coach but without critical information.
CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on
boxing. Analysis further confirms that it elaborates on errors and their
corresponding improvement methods in the generated instructions. You can find
CoachMe here: https://motionxperts.github.io/

</details>


### [259] [Room acoustics affect communicative success in hybrid meeting spaces: a pilot study](https://arxiv.org/abs/2509.11709)
*Robert Einig,Stefan Janscha,Jonas Schuster,Julian Koch,Martin Hagmueller,Barbara Schuppler*

Main category: cs.CL

TL;DR: 本研究调查了混合会议中房间声学干预对沟通效果的影响，发现声学改善能提高沟通成功率。


<details>
  <summary>Details</summary>
Motivation: 自2020年新冠疫情以来，混合会议日益普及，但会议空间的声学设计（特别是混响）常被忽视，导致沟通障碍、语音清晰度下降以及认知和声音疲劳。

Method: 一项初步研究，在格拉茨技术大学的一个研讨室中进行。研究人员记录了两组人员两次，分别在房间声学改善前和改善后进行。

Result: 尽管由于样本量小未能达到统计学显著性，但研究结果明确表明，空间声学干预改善了混合会议中的沟通成功率。

Conclusion: 房间声学干预（如减少混响）能够支持和改善混合会议中的沟通效果。

Abstract: Since the COVID-19 pandemic in 2020, universities and companies have
increasingly integrated hybrid features into their meeting spaces, or even
created dedicated rooms for this purpose. While the importance of a fast and
stable internet connection is often prioritized, the acoustic design of seminar
rooms is frequently overlooked. Poor acoustics, particularly excessive
reverberation, can lead to issues such as misunderstandings, reduced speech
intelligibility or cognitive and vocal fatigue. This pilot study investigates
whether room acoustic interventions in a seminar room at Graz University of
Technology support better communication in hybrid meetings. For this purpose,
we recorded two groups of persons twice, once before and once after improving
the acoustics of the room. Our findings -- despite not reaching statistical
significance due to the small sample size - indicate clearly that our spatial
interventions improve communicative success in hybrid meetings. To make the
paper accessible also for readers from the speech communication community, we
explain room acoustics background, relevant for the interpretation of our
results.

</details>


### [260] [An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents](https://arxiv.org/abs/2509.11773)
*Gaye Colakoglu,Gürkan Solmaz,Jonathan Fürst*

Main category: cs.CL

TL;DR: 针对欧盟建筑产品性能声明（DoP）文档的多样性，本文提出一个领域特定、有状态的代理系统，用于稳健地提取结构化数据。


<details>
  <summary>Details</summary>
Motivation: 欧盟规定的建筑产品性能声明（DoP）文档在布局、语言、模式和格式上差异巨大，导致现有的静态或仅基于LLM的信息抽取（IE）管道难以进行自动化的键值对（KVP）提取和问答（QA），且容易产生幻觉和无法适应这种结构多样性。

Method: 本文提出一个领域特定、有状态的代理系统，采用规划器-执行器-响应器（planner-executor-responder）架构。该系统能够推断用户意图、检测文档模态，并动态协调工具以实现稳健、可追溯的推理，同时避免工具误用或执行循环。

Result: 在一个精选的DoP数据集上进行评估，结果表明该系统在不同格式和语言下都展现出更高的鲁棒性。

Conclusion: 该系统为受监管工作流程中的结构化数据提取提供了一个可扩展的解决方案。

Abstract: Declaration of Performance (DoP) documents, mandated by EU regulation,
certify the performance of construction products. While some of their content
is standardized, DoPs vary widely in layout, language, schema, and format,
posing challenges for automated key-value pair extraction (KVP) and question
answering (QA). Existing static or LLM-only IE pipelines often hallucinate and
fail to adapt to this structural diversity. Our domain-specific, stateful
agentic system addresses these challenges through a planner-executor-responder
architecture. The system infers user intent, detects document modality, and
orchestrates tools dynamically for robust, traceable reasoning while avoiding
tool misuse or execution loops. Evaluation on a curated DoP dataset
demonstrates improved robustness across formats and languages, offering a
scalable solution for structured data extraction in regulated workflows.

</details>


### [261] [User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums](https://arxiv.org/abs/2509.11777)
*Mikhail Kulyabin,Jan Joosten,Choro Ulan uulu,Nuno Miguel Martins Pacheco,Fabian Ries,Filippos Petridis,Jan Bosch,Helena Holmström Olsson*

Main category: cs.CL

TL;DR: 本文介绍了用户体验感知洞察数据集（UXPID），一个包含7130条从工业论坛提取并经大型语言模型（LLM）分析和标注的用户反馈数据，旨在解决非结构化客户反馈的分析挑战。


<details>
  <summary>Details</summary>
Motivation: 工业论坛中的客户反馈是了解产品实际体验的丰富但未被充分利用的来源。然而，由于内容的非结构化和领域特定性，传统数据分析技术难以准确解释和量化这些反馈，从而限制了其对产品开发和支持策略的指导潜力。

Method: 本文构建了用户体验感知洞察数据集（UXPID），该数据集包含7130条从公共工业自动化论坛中提取的人工合成和匿名化用户反馈分支。每个JSON记录包含与特定硬件和软件产品相关的多帖子评论，并辅以元数据和上下文对话数据。利用大型语言模型（LLM），每个分支都系统地分析并标注了用户体验洞察、用户期望、严重性、情感评级和主题分类。

Result: 创建了UXPID数据集，这是一个包含7130条经过LLM系统分析和标注的用户反馈分支的集合。这些数据包含多帖子评论、元数据和上下文信息，并标注了用户体验洞察、期望、严重性、情感和主题分类。

Conclusion: UXPID数据集旨在促进用户需求、用户体验（UX）分析和AI驱动反馈处理领域的研究，特别是在隐私和许可限制限制真实世界数据访问的情况下。它支持训练和评估基于Transformer的模型，用于技术论坛中的问题检测、情感分析和需求提取等任务。

Abstract: Customer feedback in industrial forums reflect a rich but underexplored
source of insight into real-world product experience. These publicly shared
discussions offer an organic view of user expectations, frustrations, and
success stories shaped by the specific contexts of use. Yet, harnessing this
information for systematic analysis remains challenging due to the unstructured
and domain-specific nature of the content. The lack of structure and
specialized vocabulary makes it difficult for traditional data analysis
techniques to accurately interpret, categorize, and quantify the feedback,
thereby limiting its potential to inform product development and support
strategies. To address these challenges, this paper presents the User
eXperience Perception Insights Dataset (UXPID), a collection of 7130
artificially synthesized and anonymized user feedback branches extracted from a
public industrial automation forum. Each JavaScript object notation (JSON)
record contains multi-post comments related to specific hardware and software
products, enriched with metadata and contextual conversation data. Leveraging a
large language model (LLM), each branch is systematically analyzed and
annotated for UX insights, user expectations, severity and sentiment ratings,
and topic classifications. The UXPID dataset is designed to facilitate research
in user requirements, user experience (UX) analysis, and AI-driven feedback
processing, particularly where privacy and licensing restrictions limit access
to real-world data. UXPID supports the training and evaluation of
transformer-based models for tasks such as issue detection, sentiment analysis,
and requirements extraction in the context of technical forums.

</details>


### [262] [When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries](https://arxiv.org/abs/2509.11802)
*Dvora Goncharok,Arbel Shifman,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 该研究引入了一个新的药物相关问题标注数据集，并基准测试了传统机器学习和大型语言模型在检测在线医疗论坛中关键性患者问题方面的性能，以支持数字健康预警系统。


<details>
  <summary>Details</summary>
Motivation: 在线医疗论坛包含大量未充分利用的患者担忧信息，特别是关于药物使用。其中一些问题可能预示着混淆、误用或潜在的健康危机。及时检测这些可能导致严重不良事件或危及生命并发症的关键问题对于早期干预和提高患者安全至关重要。

Method: 研究构建了一个新的药物相关问题标注数据集，这些问题从在线论坛中提取，并根据临床风险因素进行人工标注。研究使用TF-IDF文本表示，基准测试了六种传统机器学习分类器以及三种利用深度上下文理解的先进大型语言模型（LLM）分类方法。

Result: 研究结果表明，无论是经典方法还是现代方法，都具有支持数字健康领域实时分诊和预警系统的潜力。此外，该策展数据集已公开可用。

Conclusion: 该研究通过提供一个公开的、标注过的药物相关问题数据集和性能基准，鼓励在患者生成数据、自然语言处理和关键健康事件早期预警系统交叉领域进行进一步研究，从而为数字健康领域的实时分诊和预警系统提供了支持。

Abstract: Online medical forums are a rich and underutilized source of insight into
patient concerns, especially regarding medication use. Some of the many
questions users pose may signal confusion, misuse, or even the early warning
signs of a developing health crisis. Detecting these critical questions that
may precede severe adverse events or life-threatening complications is vital
for timely intervention and improving patient safety. This study introduces a
novel annotated dataset of medication-related questions extracted from online
forums. Each entry is manually labelled for criticality based on clinical risk
factors. We benchmark the performance of six traditional machine learning
classifiers using TF-IDF textual representations, alongside three
state-of-the-art large language model (LLM)-based classification approaches
that leverage deep contextual understanding. Our results highlight the
potential of classical and modern methods to support real-time triage and alert
systems in digital health spaces. The curated dataset is made publicly
available to encourage further research at the intersection of
patient-generated data, natural language processing, and early warning systems
for critical health events. The dataset and benchmark are available at:
https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.

</details>


### [263] [From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives](https://arxiv.org/abs/2509.11803)
*Eden Mama,Liel Sheri,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本研究引入了一个名为NDB（Noisy Diagnostic Benchmark）的合成数据集，旨在模拟患者自述中常见的非正式、模糊和嘈杂的语言，以评估大型语言模型（LLMs）在真实医疗场景下的诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要依赖于清晰、结构化的临床文本，无法有效反映LLMs在处理患者生成的非正式、模糊和嘈杂叙述时的表现，这限制了LLMs在医疗领域实际应用中的洞察力。

Method: 研究者创建了一个新颖的合成数据集，模拟了包含不同程度语言噪声、模糊语言和外行术语的患者自述。该数据集包含临床一致的场景，并标注了真实诊断，涵盖了不同清晰度的沟通风格。利用此基准，研究者对BERT和T5等先进的LLMs进行了微调和评估，并发布了NDB数据集供社区使用。

Result: 研究成果是发布了Noisy Diagnostic Benchmark (NDB)，这是一个结构化的合成患者描述数据集，旨在对大型语言模型在真实语言条件下的诊断能力进行压力测试和比较。该基准可用于微调和评估现有最先进的LLMs。

Conclusion: NDB数据集为评估LLMs处理嘈杂患者数据时的诊断能力提供了一个现实的基准，有助于支持该领域的可复现性和未来研究。

Abstract: The widespread adoption of large language models (LLMs) in healthcare raises
critical questions about their ability to interpret patient-generated
narratives, which are often informal, ambiguous, and noisy. Existing benchmarks
typically rely on clean, structured clinical text, offering limited insight
into model performance under realistic conditions. In this work, we present a
novel synthetic dataset designed to simulate patient self-descriptions
characterized by varying levels of linguistic noise, fuzzy language, and
layperson terminology. Our dataset comprises clinically consistent scenarios
annotated with ground-truth diagnoses, spanning a spectrum of communication
clarity to reflect diverse real-world reporting styles. Using this benchmark,
we fine-tune and evaluate several state-of-the-art models (LLMs), including
BERT-based and encoder-decoder T5 models. To support reproducibility and future
research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset
of noisy, synthetic patient descriptions designed to stress-test and compare
the diagnostic capabilities of large language models (LLMs) under realistic
linguistic conditions. We made the benchmark available for the community:
https://github.com/lielsheri/PatientSignal

</details>


### [264] [PledgeTracker: A System for Monitoring the Fulfilment of Pledges](https://arxiv.org/abs/2509.11804)
*Yulong Chen,Michael Sejr Schlichtkrull,Zhenyun Deng,David Corney,Nasim Asl,Joshua Salisbury,Andrew Dudfield,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出PledgeTracker系统，通过构建结构化事件时间线来跟踪政治承诺的履行情况，解决了现有方法忽视动态、时间性和多文档性质的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将政治承诺履行跟踪简化为文档分类任务，忽视了其动态、时间性和跨多文档的特性，导致无法有效追踪承诺的渐进式履行证据。

Method: PledgeTracker系统将承诺验证重构为结构化事件时间线构建。它包含三个核心模块：1) 多步骤证据检索模块；2) 时间线构建模块；3) 履行过滤模块。这些模块协同工作，捕捉承诺履行的演变过程并生成可解释的结构化时间线。

Result: PledgeTracker在与专业事实核查员的实际工作流程中进行了评估，结果表明它能有效检索相关证据并显著减少人工验证工作量。

Conclusion: PledgeTracker提供了一个有效、可解释且能处理动态、多源信息的系统，用于跟踪政治承诺的履行，显著提升了事实核查的效率和准确性。

Abstract: Political pledges reflect candidates' policy commitments, but tracking their
fulfilment requires reasoning over incremental evidence distributed across
multiple, dynamically updated sources. Existing methods simplify this task into
a document classification task, overlooking its dynamic, temporal and
multi-document nature. To address this issue, we introduce
\textsc{PledgeTracker}, a system that reformulates pledge verification into
structured event timeline construction. PledgeTracker consists of three core
components: (1) a multi-step evidence retrieval module; (2) a timeline
construction module and; (3) a fulfilment filtering module, allowing the
capture of the evolving nature of pledge fulfilment and producing interpretable
and structured timelines. We evaluate PledgeTracker in collaboration with
professional fact-checkers in real-world workflows, demonstrating its
effectiveness in retrieving relevant evidence and reducing human verification
effort.

</details>


### [265] [SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection](https://arxiv.org/abs/2509.11818)
*Taichi Aida,Danushka Bollegala*

Main category: cs.CL

TL;DR: SCDTour是一种语义变化检测方法，通过排序和合并可解释轴来平衡可解释性和性能，并能通过精炼的轴提供有意义的语义变化解释。


<details>
  <summary>Details</summary>
Motivation: 在语义变化检测（SCD）中，获取既可解释又高性能的嵌入是一个常见问题，因为提高可解释性通常会导致SCD性能下降，反之亦然。

Method: SCDTour通过排序和合并可解释轴来缓解SCD性能下降。该方法同时考虑了嵌入空间中轴之间的语义相似性以及每个轴对语义变化的贡献程度。

Result: 实验结果表明，SCDTour在保持高可解释性的同时，能保持语义变化检测的性能。此外，聚合排序后的轴能产生更精炼的词义集，其在SCD任务中的表现与原始全维度嵌入相当或有所提升。

Conclusion: SCDTour有效地平衡了可解释性和SCD性能，通过少量精炼的轴实现了对语义变化的有意义解释。

Abstract: In Semantic Change Detection (SCD), it is a common problem to obtain
embeddings that are both interpretable and high-performing. However, improving
interpretability often leads to a loss in the SCD performance, and vice versa.
To address this problem, we propose SCDTour, a method that orders and merges
interpretable axes to alleviate the performance degradation of SCD. SCDTour
considers both (a) semantic similarity between axes in the embedding space, as
well as (b) the degree to which each axis contributes to semantic change.
Experimental results show that SCDTour preserves performance in semantic change
detection while maintaining high interpretability. Moreover, agglomerating the
sorted axes produces a more refined set of word senses, which achieves
comparable or improved performance against the original full-dimensional
embeddings in the SCD task. These findings demonstrate that SCDTour effectively
balances interpretability and SCD performance, enabling meaningful
interpretation of semantic shifts through a small number of refined axes.
Source code is available at https://github.com/LivNLP/svp-tour .

</details>


### [266] [Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles](https://arxiv.org/abs/2509.11991)
*Jesús Calleja,David Ponce,Thierry Etchegoyhen*

Main category: cs.CL

TL;DR: Vicomtech在CLEARS挑战赛中，通过对大型语言模型生成的适应文本进行迭代自动后编辑，在西班牙语Plain Language和Easy Read文本适应任务中分别获得第一和第二名。


<details>
  <summary>Details</summary>
Motivation: 参与CLEARS挑战赛，旨在解决西班牙语文本向Plain Language和Easy Read进行适应的需求。

Method: 采用自动后编辑方法，对不同类型的大型语言模型（LLM）初始适应结果进行处理。通过迭代生成和细化适应文本，直到可读性和相似性指标显示无法进一步有效改进。

Result: 在所有官方指标的平均值上，Vicomtech的提交分别在Plain Language适应和Easy Read适应任务中取得了第一名和第二名。

Conclusion: 迭代的LLM自动后编辑方法在西班牙语Plain Language和Easy Read文本适应任务中表现出色，取得了领先的成绩。

Abstract: We describe Vicomtech's participation in the CLEARS challenge on text
adaptation to Plain Language and Easy Read in Spanish. Our approach features
automatic post-editing of different types of initial Large Language Model
adaptations, where successive adaptations are generated iteratively until
readability and similarity metrics indicate that no further adaptation
refinement can be successfully performed. Taking the average of all official
metrics, our submissions achieved first and second place in Plain language and
Easy Read adaptation, respectively.

</details>


### [267] [MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues](https://arxiv.org/abs/2509.11860)
*Weishu Chen,Jinyi Tang,Zhouhui Hou,Shihao Han,Mingjie Zhan,Zhiyuan Huang,Delong Liu,Jiawei Guo,Zhicheng Zhao,Fei Su*

Main category: cs.CL

TL;DR: MOOM是一个双分支记忆插件，通过建模情节发展和角色刻画，并结合遗忘机制，解决了人机角色扮演中超长对话的记忆无限制增长问题。它在新的中文数据集上优于现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 在人机角色扮演场景中，维持超长对话的连贯性需要有效的记忆提取，但现有方法常面临记忆无限制增长的问题。

Method: 本文提出了MOOM，首个双分支记忆插件，它利用文学理论，将情节发展和角色刻画作为核心叙事元素进行建模。具体而言，一个分支总结跨多时间尺度的情节冲突，另一个分支提取用户角色画像。MOOM还融入了受“竞争-抑制”记忆理论启发的遗忘机制，以限制记忆容量并缓解无限制增长。此外，本文还发布了ZH-4O，一个专门为角色扮演设计的中文超长对话数据集，平均对话轮次达600，并包含手动标注的记忆信息。

Result: 实验结果表明，MOOM在性能上超越了所有最先进的记忆提取方法，同时需要更少的大型语言模型调用次数，并能保持可控的记忆容量。

Conclusion: MOOM通过其新颖的双分支结构和遗忘机制，成功解决了超长对话中记忆无限制增长的挑战，并在性能和效率上均表现出色，为未来人机角色扮演对话系统提供了有效的记忆管理方案。

Abstract: Memory extraction is crucial for maintaining coherent ultra-long dialogues in
human-robot role-playing scenarios. However, existing methods often exhibit
uncontrolled memory growth. To address this, we propose MOOM, the first
dual-branch memory plugin that leverages literary theory by modeling plot
development and character portrayal as core storytelling elements.
Specifically, one branch summarizes plot conflicts across multiple time scales,
while the other extracts the user's character profile. MOOM further integrates
a forgetting mechanism, inspired by the ``competition-inhibition'' memory
theory, to constrain memory capacity and mitigate uncontrolled growth.
Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset
specifically designed for role-playing, featuring dialogues that average 600
turns and include manually annotated memory information. Experimental results
demonstrate that MOOM outperforms all state-of-the-art memory extraction
methods, requiring fewer large language model invocations while maintaining a
controllable memory capacity.

</details>


### [268] [Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible](https://arxiv.org/abs/2509.11915)
*Aadil Gani Ganie*

Main category: cs.CL

TL;DR: 本文将量子不确定性原理与自然语言中作者身份检测的局限性进行类比，指出在区分人类和AI文本时存在一个根本的权衡：越试图精确检测，就越可能破坏文本的自然流畅性和真实性，最终认为完美检测在理论上是不可行的。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的进步，区分人类撰写和AI生成文本变得越来越困难，这促使研究者探讨作者身份检测的根本限制。

Method: 本文通过将量子不确定性原理与自然语言的作者身份检测进行概念性类比，分析了当前检测方法（如文体学、水印和神经网络分类器）所面临的固有局限性。研究探讨了提高检测准确性如何导致AI输出的变化，从而使其他特征变得不可靠。

Result: 研究发现，在检测AI文本时存在一个基本权衡：越自信地尝试识别文本来源，就越可能扰乱文本的自然流畅性和真实性。当AI生成的文本与人类写作高度相似时，完美的检测不仅在技术上困难，而且在理论上也是不可能的。检测行为本身会在文本的其他地方引入不确定性。

Conclusion: AI文本检测的挑战不仅仅是工具层面的问题，它反映了语言本质中一个更深层次、不可避免的张力。本文讨论了对作者身份、伦理和政策的广泛影响，并反驳了相关论点。

Abstract: As large language models (LLMs) become more advanced, it is increasingly
difficult to distinguish between human-written and AI-generated text. This
paper draws a conceptual parallel between quantum uncertainty and the limits of
authorship detection in natural language. We argue that there is a fundamental
trade-off: the more confidently one tries to identify whether a text was
written by a human or an AI, the more one risks disrupting the text's natural
flow and authenticity. This mirrors the tension between precision and
disturbance found in quantum systems. We explore how current detection
methods--such as stylometry, watermarking, and neural classifiers--face
inherent limitations. Enhancing detection accuracy often leads to changes in
the AI's output, making other features less reliable. In effect, the very act
of trying to detect AI authorship introduces uncertainty elsewhere in the text.
Our analysis shows that when AI-generated text closely mimics human writing,
perfect detection becomes not just technologically difficult but theoretically
impossible. We address counterarguments and discuss the broader implications
for authorship, ethics, and policy. Ultimately, we suggest that the challenge
of AI-text detection is not just a matter of better tools--it reflects a
deeper, unavoidable tension in the nature of language itself.

</details>


### [269] [Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation](https://arxiv.org/abs/2509.11921)
*Helene Tenzer,Oumnia Abidi,Stefan Feuerriegel*

Main category: cs.CL

TL;DR: 本研究分析了大型语言模型在职场邮件英日翻译中的文化敏感性，发现文化定制的提示策略能显著提升翻译的文化契合度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）能够生成接近完美的字面翻译，但在多语言、跨文化交流中，它们是否支持文化适宜的沟通尚不明确。

Method: 研究采用混合方法，分析了不同LLM设计在职场邮件英日翻译中的文化敏感性。具体通过以下三种提示策略进行对比：(1) 直接翻译提示，(2) 目标受众文化背景提示，(3) 明确指导日本沟通规范的提示。随后，通过分析文化特定的语言模式来评估翻译对文化规范的适应程度，并由母语使用者评估翻译语气的适宜性。

Result: 研究发现，文化定制的提示策略能够有效提升翻译的文化契合度。

Conclusion: 基于研究结果，论文为在多语言环境中设计具有文化包容性的LLM提供了建议。

Abstract: Large language models (LLMs) are increasingly used in everyday communication,
including multilingual interactions across different cultural contexts. While
LLMs can now generate near-perfect literal translations, it remains unclear
whether LLMs support culturally appropriate communication. In this paper, we
analyze the cultural sensitivity of different LLM designs when applied to
English-Japanese translations of workplace e-mails. Here, we vary the prompting
strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts
specifying the recipient's cultural background, and (3) instructional prompts
with explicit guidance on Japanese communication norms. Using a mixed-methods
study, we then analyze culture-specific language patterns to evaluate how well
translations adapt to cultural norms. Further, we examine the appropriateness
of the tone of the translations as perceived by native speakers. We find that
culturally-tailored prompting can improve cultural fit, based on which we offer
recommendations for designing culturally inclusive LLMs in multilingual
settings.

</details>


### [270] [Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding](https://arxiv.org/abs/2509.11961)
*Mingxiao Huo,Jiayi Zhang,Hewei Wang,Jinfeng Xu,Zheyu Chen,Huilin Tai,Yijun Chen*

Main category: cs.CL

TL;DR: Spec-LLaVA通过应用推测解码（speculative decoding）来加速视觉-语言模型（VLMs），使用轻量级草稿模型和大型目标模型，并结合动态树状验证算法，在不牺牲输出质量的情况下实现高达3.28倍的加速。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）虽然具有强大的多模态推理能力，但其自回归推理速度较慢，限制了它们在实时应用中的部署。

Method: Spec-LLaVA采用推测解码（speculative decoding）方法，将一个轻量级草稿VLM与一个大型目标模型配对。草稿模型预测未来的token序列，目标模型并行验证这些token，从而在每个步骤中生成多个token。为了最大化效率，论文设计了一种动态树状验证算法，该算法利用草稿模型的置信度自适应地扩展和修剪推测分支。

Result: 在MS COCO域外图像上，Spec-LLaVA在LLaVA-1.5 (7B, 13B) 模型上实现了高达3.28倍的解码加速，且生成质量没有损失。

Conclusion: 这项工作提出了一个使用动态树状推测解码的无损VLM加速框架，为实现实用的实时多模态助手开辟了道路。此外，轻量级草稿模型的设计使得该框架适用于资源受限或设备端部署环境。

Abstract: Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer
from slow autoregressive inference, limiting their deployment in real-time
applications. We introduce Spec-LLaVA, a system that applies speculative
decoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA
pairs a lightweight draft VLM with a large target model: the draft speculates
future tokens, which the target verifies in parallel, allowing multiple tokens
to be generated per step. To maximize efficiency, we design a dynamic
tree-based verification algorithm that adaptively expands and prunes
speculative branches using draft model confidence. On MS COCO out-of-domain
images, Spec-LLaVA achieves up to 3.28$\times$ faster decoding on LLaVA-1.5
(7B, 13B) with no loss in generation quality. This work presents a lossless
acceleration framework for VLMs using dynamic tree-structured speculative
decoding, opening a path toward practical real-time multimodal assistants.
Importantly, the lightweight draft model design makes the framework amenable to
resource-constrained or on-device deployment settings.

</details>


### [271] [ToolRM: Outcome Reward Models for Tool-Calling Large Language Models](https://arxiv.org/abs/2509.11963)
*Mayank Agarwal,Ibrahim Abdelaziz,Kinjal Basu,Merve Unuvar,Luis A. Lastras,Yara Rizk,Pavan Kapanipathi*

Main category: cs.CL

TL;DR: 该研究引入了FC-RewardBench基准，用于评估LLM工具使用场景下的奖励模型性能，并提出了一个基于开放权重LLM合成数据的训练框架，显著提升了工具使用奖励模型的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）与外部工具的交互日益增多，工具使用的奖励建模变得至关重要但研究不足。现有奖励模型主要基于自然语言输出训练，难以有效评估基于工具的推理和执行。

Method: 研究引入了FC-RewardBench，这是首个系统评估奖励模型在工具调用场景中表现的基准。为解决现有模型的不足，提出了一个基于结果的奖励模型训练框架，利用许可的、开放权重LLM合成数据进行训练。训练的模型参数范围从1.7B到14B，并在七个域外基准上进行评估。

Result: 分析表明，当前奖励模型常常忽略有效工具使用的关键信号。所提出的模型持续优于通用基线模型，在下游任务性能上平均提升高达25%，并通过奖励引导过滤实现了数据高效的微调。

Conclusion: 该研究量化了现有奖励模型在工具使用评估上的不足，并提出了一个有效的训练框架，证明了领域特定奖励模型在提升LLM工具使用性能和数据效率方面的潜力。

Abstract: As large language models (LLMs) increasingly interact with external tools,
reward modeling for tool use has become a critical yet underexplored area.
Existing reward models, trained primarily on natural language outputs, struggle
to evaluate tool-based reasoning and execution. To quantify this gap, we
introduce FC-RewardBench, the first benchmark designed to systematically assess
reward models' performance in tool-calling scenarios. Our analysis shows that
current reward models often miss key signals of effective tool use,
highlighting the need for domain-specific modeling. To address this, we propose
a training framework for outcome-based reward models using data synthesized
from permissively licensed, open-weight LLMs. We train models ranging from 1.7B
to 14B parameters and evaluate them across seven out-of-domain benchmarks.
These models consistently outperform general-purpose baselines, achieving up to
25\% average improvement in downstream task performance and enabling
data-efficient fine-tuning through reward-guided filtering.

</details>


### [272] [Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities](https://arxiv.org/abs/2509.12098)
*Payam Latifi*

Main category: cs.CL

TL;DR: 本研究对比了六种命名实体识别（NER）系统（三款非LLM NLP工具和三款通用LLM）在一个小型标注数据集上的性能。结果显示LLM在上下文敏感实体识别上表现更优，而传统工具在结构化标签上更一致。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在比较传统NLP工具和大型语言模型（LLM）在命名实体识别（NER）任务上的表现，尤其关注它们在不同实体类型上的优势和劣势，以指导模型选择。

Method: 研究构建了一个包含119个token和五种实体类型（PERSON, LOCATION, ORGANIZATION, DATE, TIME）的小规模、精心标注的基准数据集。评估了NLTK、spaCy、Stanza（非LLM）以及Gemini-1.5-flash、DeepSeek-V3、Qwen-3-4B（LLM）六个系统的性能，并使用F1分数进行评估。

Result: LLM在识别上下文敏感实体（如人名）方面通常优于传统工具，其中Gemini取得了最高的平均F1分数。然而，Stanza等传统系统在结构化标签（如LOCATION和DATE）上表现出更高的一致性。LLM之间也存在差异，尤其在处理时间表达式和多词组织方面。

Conclusion: 研究结果表明，尽管LLM提供了改进的上下文理解能力，但传统工具在特定任务中仍具有竞争力。这些发现有助于在NER任务中进行模型选择。

Abstract: This pilot study presents a small-scale but carefully annotated benchmark of
Named Entity Recognition (NER) performance across six systems: three non-LLM
NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models
(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119
tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).
We evaluated each system's output against the manually annotated gold standard
dataset using F1-score. The results show that LLMs generally outperform
conventional tools in recognizing context-sensitive entities like person names,
with Gemini achieving the highest average F1-score. However, traditional
systems like Stanza demonstrate greater consistency in structured tags such as
LOCATION and DATE. We also observed variability among LLMs, particularly in
handling temporal expressions and multi-word organizations. Our findings
highlight that while LLMs offer improved contextual understanding, traditional
tools remain competitive in specific tasks, informing model selection.

</details>


### [273] [Query-Focused Extractive Summarization for Sentiment Explanation](https://arxiv.org/abs/2509.11989)
*Ahmed Moubtahij,Sylvie Ratté,Yazid Attabi,Maxime Dumas*

Main category: cs.CL

TL;DR: 本文提出了一种多偏置框架，用于解决查询聚焦摘要（QFS）中查询与文档之间的语言差异问题，并通过情感偏置和查询扩展的专门方法，在情感解释任务上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 从大量文本中确定客户情绪的原因是一项耗时的工作。查询聚焦摘要（QFS）可以提高效率，但其模型常受查询与源文档之间语言差异的阻碍。

Method: 提出一个领域无关的多偏置框架来弥合查询与文档间的语言差异。针对情感解释问题，进一步制定了基于情感偏置和查询扩展的专门方法。

Result: 在真实世界的专有情感感知QFS数据集上，实验结果优于基线模型。

Conclusion: 所提出的多偏置框架以及结合情感偏置和查询扩展的专门方法，能有效提高QFS在情感解释任务上的性能，克服了语言差异的挑战。

Abstract: Constructive analysis of feedback from clients often requires determining the
cause of their sentiment from a substantial amount of text documents. To assist
and improve the productivity of such endeavors, we leverage the task of
Query-Focused Summarization (QFS). Models of this task are often impeded by the
linguistic dissonance between the query and the source documents. We propose
and substantiate a multi-bias framework to help bridge this gap at a
domain-agnostic, generic level; we then formulate specialized approaches for
the problem of sentiment explanation through sentiment-based biases and query
expansion. We achieve experimental results outperforming baseline models on a
real-world proprietary sentiment-aware QFS dataset.

</details>


### [274] [In-domain SSL pre-training and streaming ASR](https://arxiv.org/abs/2509.12101)
*Jarod Duret,Salima Mdhaffar,Gaëlle Laperrière,Ryan Whetten,Audrey Galametz,Catherine Kobus,Marion-Cécile Martin,Jo Oleiwan,Yannick Estève*

Main category: cs.CL

TL;DR: 本研究表明，针对空中交通管制（ATC）环境的领域特定自监督预训练，能显著提高离线和流式自动语音识别（ASR）的性能，特别是在低延迟场景下。


<details>
  <summary>Details</summary>
Motivation: 通用语音编码器在特定领域（如ATC）的性能可能不足，且ATC环境对准确性和实时性要求高，因此需要探索如何提高ATC ASR系统的准确性和效率。

Method: 研究人员在4.5k小时的未标注ATC数据上训练了BEST-RQ模型进行自监督预训练，随后在一个较小的标注ATC数据集上进行微调。为实现实时处理，他们提出了使用分块注意力（chunked attention）和动态卷积（dynamic convolutions）的方法。性能对比对象是w2v-BERT 2.0和HuBERT等最先进的通用语音编码器。

Result: 领域适应的预训练显著提高了ATC基准测试的性能，与使用通用语音语料库训练的模型相比，词错误率（WER）大幅降低。此外，所提出的流式处理方法在更严格的延迟限制下进一步改善了词错误率。

Conclusion: 研究结果强调，为ATC数据专门化自监督学习（SSL）表示是提高实际操作环境中ASR系统准确性和效率的有效途径，尤其适用于对安全性要求极高的航空应用。

Abstract: In this study, we investigate the benefits of domain-specific self-supervised
pre-training for both offline and streaming ASR in Air Traffic Control (ATC)
environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then
fine-tune on a smaller supervised ATC set. To enable real-time processing, we
propose using chunked attention and dynamic convolutions, ensuring low-latency
inference. We compare these in-domain SSL models against state-of-the-art,
general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show
that domain-adapted pre-training substantially improves performance on standard
ATC benchmarks, significantly reducing word error rates when compared to models
trained on broad speech corpora. Furthermore, the proposed streaming approach
further improves word error rate under tighter latency constraints, making it
particularly suitable for safety-critical aviation applications. These findings
highlight that specializing SSL representations for ATC data is a practical
path toward more accurate and efficient ASR systems in real-world operational
settings.

</details>


### [275] [Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect](https://arxiv.org/abs/2509.12065)
*Alina Klerings,Jannik Brinkmann,Daniel Ruffinelli,Simone Ponzetto*

Main category: cs.CL

TL;DR: 本研究探究大型语言模型（LLMs）如何编码动词时态和体态等多维语法知识，并使用概念引导技术实现对这些特征的因果控制，同时分析了有效控制的关键因素。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注LLMs对二元语法对比的编码，但对于多维分层语法的内部表示方式知之甚少。本研究旨在深入理解LLMs如何编码和控制动词时态和体态等复杂的语法特征。

Method: 研究方法包括：1) 使用线性判别分析（LDA）在残差空间中识别出时态和体态的独立、正交方向；2) 通过概念引导（concept steering）在三个生成任务中演示对这些语法特征的因果控制；3) 进行案例研究，调查影响多词元生成中有效引导的因素。

Result: 研究发现：1) LLMs在残差空间中以结构化方式编码时态和体态，存在独立的正交方向；2) 成功通过概念引导实现了对时态和体态的因果控制；3) 引导强度、位置和持续时间是减少主题漂移和退化等不良副作用的关键参数。

Conclusion: 研究表明LLMs以结构化、类人的方式编码时态和体态。然而，在生成过程中有效控制这些特征对多个因素敏感，需要手动调整或自动化优化。

Abstract: Large language models (LLMs) are able to generate grammatically well-formed
text, but how do they encode their syntactic knowledge internally? While prior
work has focused largely on binary grammatical contrasts, in this work, we
study the representation and control of two multidimensional hierarchical
grammar phenomena - verb tense and aspect - and for each, identify distinct,
orthogonal directions in residual space using linear discriminant analysis.
Next, we demonstrate causal control over both grammatical features through
concept steering across three generation tasks. Then, we use these identified
features in a case study to investigate factors influencing effective steering
in multi-token generation. We find that steering strength, location, and
duration are crucial parameters for reducing undesirable side effects such as
topic shift and degeneration. Our findings suggest that models encode tense and
aspect in structurally organized, human-like ways, but effective control of
such features during generation is sensitive to multiple factors and requires
manual tuning or automated optimization.

</details>


### [276] [SENSE models: an open source solution for multilingual and multimodal semantic-based tasks](https://arxiv.org/abs/2509.12093)
*Salima Mdhaffar,Haroun Elleuch,Chaimae Chellaf,Ha Nguyen,Yannick Estève*

Main category: cs.CL

TL;DR: 本文介绍了SENSE，一个开源的N语言语音和文本共享嵌入解决方案，它通过师生框架将自监督语音编码器与文本编码器对齐，并在多语言多模态语义任务上取得了极具竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是创建一个能够将语音和文本表示对齐的共享嵌入空间，灵感来源于SAMU-XLSR和Meta AI的SONAR模型，旨在通过改进师生框架中的模型选择来提升性能。

Method: SENSE采用师生框架，在话语层面将自监督语音编码器与语言无关的文本编码器连续表示对齐。方法上的更新包括选择更强的教师文本模型和更好的初始语音编码器。训练和使用SENSE模型的源代码已集成到SpeechBrain工具包中。

Result: 实验结果表明，SENSE模型在多语言和多模态语义任务上取得了高度竞争性的性能。此外，该研究还提供了关于此类语义对齐语音编码器如何捕获语义的新见解。

Conclusion: SENSE是一个高性能的开源解决方案，用于创建N语言语音和文本的共享嵌入，其性能表现优异，并为理解语义在对齐编码器中的捕获方式提供了新的视角。

Abstract: This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),
an open-source solution inspired by the SAMU-XLSR framework and conceptually
similar to Meta AI's SONAR models. These approaches rely on a teacher-student
framework to align a self-supervised speech encoder with the language-agnostic
continuous representations of a text encoder at the utterance level. We
describe how the original SAMU-XLSR method has been updated by selecting a
stronger teacher text model and a better initial speech encoder. The source
code for training and using SENSE models has been integrated into the
SpeechBrain toolkit, and the first SENSE model we trained has been publicly
released. We report experimental results on multilingual and multimodal
semantic tasks, where our SENSE model achieves highly competitive performance.
Finally, this study offers new insights into how semantics are captured in such
semantically aligned speech encoders.

</details>


### [277] [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)
*Min Zeng,Jinfei Sun,Xueyou Luo,Caiquan Liu,Shiqi Zhang,Li Xie,Xiaoxin Chen*

Main category: cs.CL

TL;DR: GTA框架结合SFT和RL的优势，通过“猜测-思考-回答”过程，实现自然语言处理任务中更快的收敛速度和更高的性能上限。


<details>
  <summary>Details</summary>
Motivation: 纯强化学习（RL）在自然语言处理任务中探索效率低、收敛慢；而监督微调（SFT）虽然训练高效，但性能上限有限且理论基础不如RL坚实。本研究旨在解决效率与能力之间的权衡问题。

Method: 提出Guess-Think-Answer (GTA) 框架。模型首先生成一个初步“猜测”（通过交叉熵损失优化），然后对该猜测进行“思考”，最后生成最终“回答”。RL奖励用于塑造最终输出和整个GTA结构的格式。为缓解两种训练信号之间的梯度冲突，采用损失掩码和梯度约束。

Result: GTA方法比纯RL收敛更快，比纯SFT性能上限更高。在四个文本分类基准测试中，GTA显著加速了收敛，并优于独立的SFT和RL基线。

Conclusion: GTA框架成功地将SFT的效率与RL的能力提升相结合，提供了一种统一的训练范式，解决了自然语言处理任务中效率与能力之间的权衡问题，取得了卓越的性能和收敛速度。

Abstract: In natural language processing tasks, pure reinforcement learning (RL)
fine-tuning methods often suffer from inefficient exploration and slow
convergence; while supervised fine-tuning (SFT) methods, although efficient in
training, have limited performance ceiling and less solid theoretical
foundation compared to RL. To address efficiency-capability trade-off, we
propose the Guess-Think-Answer (GTA) framework that combines the efficiency of
SFT with the capability gains of RL in a unified training paradigm. GTA works
by having the model first produce a provisional guess (optimized via
cross-entropy loss), then reflect on this guess before generating the final
answer, with RL rewards shaping both the final output and the format of the
entire GTA structure. This hybrid approach achieves both faster convergence
than pure RL and higher performance ceiling than pure SFT. To mitigate gradient
conflicts between the two training signals, we employ loss masking and gradient
constraints. Empirical results on four text classification benchmarks
demonstrate that GTA substantially accelerates convergence while outperforming
both standalone SFT and RL baselines.

</details>


### [278] [Pun Unintended: LLMs and the Illusion of Humor Understanding](https://arxiv.org/abs/2509.12158)
*Alessandro Zangari,Matteo Marcuzzo,Andrea Albarelli,Mohammad Taher Pilehvar,Jose Camacho-Collados*

Main category: cs.CL

TL;DR: 本文指出大型语言模型（LLMs）对双关语的理解仍停于表面，缺乏人类的细致洞察，并通过修改现有基准展示了模型在此方面的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在检测双关语方面表现出潜力，但作者认为它们对双关语的理解往往是肤浅的，未能达到人类解读的细致入微的程度，因此需要深入探究其理解的局限性。

Method: 研究方法包括系统分析和重新制定现有的双关语基准，引入细微变化来测试LLMs的鲁棒性。此外，还对最新的LLMs进行了人工评估。

Result: 研究结果表明，双关语中细微的变化足以误导LLMs，这揭示了这些模型在处理双关语时面临的鲁棒性挑战。本文贡献了全面且细致的双关语检测基准。

Conclusion: LLMs在双关语理解上仍存在深度和细致性不足的问题，缺乏人类般的理解能力，并且在处理双关语时面临显著的鲁棒性挑战。

Abstract: Puns are a form of humorous wordplay that exploits polysemy and phonetic
similarity. While LLMs have shown promise in detecting puns, we show in this
paper that their understanding often remains shallow, lacking the nuanced grasp
typical of human interpretation. By systematically analyzing and reformulating
existing pun benchmarks, we demonstrate how subtle changes in puns are
sufficient to mislead LLMs. Our contributions include comprehensive and nuanced
pun detection benchmarks, human evaluation across recent LLMs, and an analysis
of the robustness challenges these models face in processing puns.

</details>


### [279] [CBP-Tuning: Efficient Local Customization for Black-box Large Language Models](https://arxiv.org/abs/2509.12112)
*Jiaxuan Zhao,Naibin Gu,Yuchen Feng,Xiyu Liu,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 本文提出CBP-Tuning框架，通过两阶段方法实现大语言模型（LLM）的本地化高效定制，同时保护用户和提供者的双向隐私。


<details>
  <summary>Details</summary>
Motivation: 定制LLM成本高昂，导致其作为云服务时难以大规模支持个性化需求，且用户面临敏感数据隐私风险。

Method: 设计了一个两阶段框架：1) 服务器端训练一个提示生成器，捕获领域特定且任务无关的能力；2) 用户端进行无梯度优化，为单个任务定制软提示。用户无需访问模型权重或上传私有数据，每个任务仅需一个定制向量即可实现有效适应。

Result: CBP-Tuning在常识推理、医疗和金融领域表现出优于基线的性能，展示了其在任务无关处理和隐私保护方面的优势。

Conclusion: CBP-Tuning提供了一种高效、本地化且隐私友好的LLM定制方案，有效解决了现有云服务模式下的成本和隐私挑战。

Abstract: The high costs of customizing large language models (LLMs) fundamentally
limit their adaptability to user-specific needs. Consequently, LLMs are
increasingly offered as cloud-based services, a paradigm that introduces
critical limitations: providers struggle to support personalized customization
at scale, while users face privacy risks when exposing sensitive data. To
address this dual challenge, we propose Customized Black-box Prompt Tuning
(CBP-Tuning), a novel framework that facilitates efficient local customization
while preserving bidirectional privacy. Specifically, we design a two-stage
framework: (1) a prompt generator trained on the server-side to capture
domain-specific and task-agnostic capabilities, and (2) user-side gradient-free
optimization that tailors soft prompts for individual tasks. This approach
eliminates the need for users to access model weights or upload private data,
requiring only a single customized vector per task while achieving effective
adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense
reasoning, medical and financial domain settings demonstrates superior
performance compared to baselines, showcasing its advantages in task-agnostic
processing and privacy preservation.

</details>


### [280] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)
*Timothy Rupprecht,Enfu Nan,Arash Akbari,Arman Akbari,Lei Lu,Priyanka Maan,Sean Duffy,Pu Zhao,Yumei He,David Kaeli,Yanzhi Wang*

Main category: cs.CL

TL;DR: 针对高风险领域中角色扮演LLM在面对恶意用户时易“脱戏”的问题，本文提出了RAGs-to-Riches框架。该框架将角色扮演重构为文本检索问题，利用精心策划的参考示例，显著提高了LLM在面对恶意用户时的角色真实性和一致性。


<details>
  <summary>Details</summary>
Motivation: 角色扮演大型语言模型（LLMs）在高风险领域（如医疗、教育、治理）中应用日益广泛，但现有的少样本学习方法在面对恶意用户时，模型容易意外地“脱戏”，可能对用户信任和福祉造成负面影响。因此，需要一种经济高效且鲁棒的方法来确保LLM在角色扮演中的一致性。

Method: 本文将LLM角色扮演重新定义为文本检索问题，并提出了一个名为RAGs-to-Riches的新提示框架，该框架借鉴了检索增强生成（RAG）的思想，利用精心策划的参考演示来约束LLM的响应。评估方法包括LLM作为评判者的偏好投票，并引入了两个新颖的token级别ROUGE指标：输出交集（IOO）来量化LLM的即兴创作程度，以及参考交集（IOR）来衡量少样本演示的利用率。

Result: 在模拟与恶意用户的交互时，本文提出的提示策略平均从参考演示中引入了35%的额外token。在453次角色扮演交互中，采用该方法的模型被持续评判为更真实，并且比零样本和上下文学习（ICL）方法更频繁地保持角色一致性。

Conclusion: RAGs-to-Riches方法为构建健壮、与人类对齐的LLM角色扮演框架提供了一种可扩展的策略，尤其适用于高风险场景下与恶意用户的交互。

Abstract: Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

</details>


### [281] [XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models](https://arxiv.org/abs/2509.12130)
*Ariana Sahitaj,Jiaao Li,Pia Wenzel Neves,Fedor Splitt,Premtim Sahitaj,Charlott Jakob,Veronika Solopova,Vera Schmitt*

Main category: cs.CL

TL;DR: XplaiNLP团队在CheckThat! 2025多语言主观性检测任务中提交了两种方法：有监督微调Transformer编码器和零样本LLM提示。在意大利语和罗马尼亚语等多个子任务中取得了显著成果，但在某些低资源跨语言场景中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 参与CheckThat! 2025关于多语言主观性检测的共享任务，旨在探索和评估不同方法在该任务中的表现。

Method: 1. 有监督微调Transformer编码器（EuroBERT, XLM-RoBERTa, German-BERT），使用单语和机器翻译的训练数据。2. 零样本提示（zero-shot prompting），使用大型语言模型（LLMs），包括o3-mini用于Annotation（基于规则的标注）和gpt-4.1-mini用于DoubleDown（对比重写）及Perspective（比较推理）。

Result: Annotation方法在意大利语单语子任务中F1得分0.8104，排名第一，超越基线0.6941。微调后的XLM-RoBERTa模型在罗马尼亚语零样本设置中F1得分0.7917，排名第三，超越基线0.6461，并在多语言任务和希腊语中表现可靠。针对德语，使用翻译训练数据微调的German-BERT模型表现优于基线。然而，在乌克兰语和波兰语的零样本设置中，性能略低于基线。

Conclusion: 所提出的方法在多语言主观性检测任务的多个语言和设置中取得了成功，特别是在意大利语和罗马尼亚语。然而，在低资源的跨语言场景中（如乌克兰语和波兰语），泛化能力仍然是一个挑战。

Abstract: This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared
task on multilingual subjectivity detection. We evaluate two approaches: (1)
supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and
German-BERT, on monolingual and machine-translated training data; and (2)
zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based
labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and
Perspective (comparative reasoning). The Annotation Approach achieves 1st place
in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming
the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned
XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the
baseline of 0.6461. The same model also performs reliably in the multilingual
task and improves over the baseline in Greek. For German, a German-BERT model
fine-tuned on translated training data from typologically related languages
yields competitive performance over the baseline. In contrast, performance in
the Ukrainian and Polish zero-shot settings falls slightly below the respective
baselines, reflecting the challenge of generalization in low-resource
cross-lingual scenarios.

</details>


### [282] [Preservation of Language Understanding Capabilities in Speech-aware Large Language Models](https://arxiv.org/abs/2509.12171)
*Marek Kubis,Paweł Skórzewski,Iwona Christop,Mateusz Czyżnikiewicz,Jakub Kubiak,Łukasz Bondaruk,Marcin Lewandowski*

Main category: cs.CL

TL;DR: 本文提出了C3T基准测试，用于评估语音感知大型语言模型在语音输入下保留语言理解能力、公平性和鲁棒性的情况。


<details>
  <summary>Details</summary>
Motivation: 评估语音感知大型语言模型（speech-aware LLMs）的性能，特别是量化当通过语音输入访问模型时，其语言理解能力被保留的程度，并衡量模型对不同说话者类别的公平性及其在文本和语音模态间的鲁棒性。

Method: 引入C3T（跨模态能力保持测试）基准。该基准利用文本任务和语音克隆文本到语音（TTS）模型，来量化模型在语音输入下语言理解能力的保留程度。

Result: C3T能够量化模型在语音输入下语言理解能力的保留程度，并量化模型对不同说话者类别的公平性及其在文本和语音模态间的鲁棒性。

Conclusion: C3T提供了一个新的基准，用于评估语音感知大型语言模型在语音输入下的性能、公平性和跨模态鲁棒性。

Abstract: The paper presents C3T (Cross-modal Capabilities Conservation Test), a new
benchmark for assessing the performance of speech-aware large language models.
The benchmark utilizes textual tasks and a voice cloning text-to-speech model
to quantify the extent to which language understanding capabilities are
preserved when the model is accessed via speech input. C3T quantifies the
fairness of the model for different categories of speakers and its robustness
across text and speech modalities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [283] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: 这篇综述系统性地回顾了大型基础模型（特别是大型语言模型和多模态大型语言模型）在轨迹预测领域的最新进展，旨在解决传统深度学习方法的解释性差、数据依赖重和泛化能力弱等问题，通过融合语言和场景语义来提升预测的安全性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的轨迹预测对行车安全至关重要。传统的深度学习方法在准确性上有所提升，但仍受限于解释性不足、对大规模标注数据的高度依赖以及在长尾场景中泛化能力弱等固有缺陷。大型基础模型的兴起正在改变轨迹预测的研究范式。

Method: 这篇综述对大型基础模型（特别是大型语言模型和多模态大型语言模型）在轨迹预测方面的最新进展进行了系统性回顾。文章重点介绍了三种核心方法论：轨迹-语言映射、多模态融合和基于约束的推理。同时涵盖了车辆和行人的预测任务、评估指标以及数据集分析。

Result: 大型基础模型通过整合语言和场景语义，促进了可解释的上下文推理，显著增强了复杂环境中预测的安全性与泛化能力。

Conclusion: 大型基础模型正在改变轨迹预测的研究范式。文章讨论了计算延迟、数据稀缺和真实世界鲁棒性等关键挑战，并提出了未来的研究方向，包括低延迟推理、因果感知建模和运动基础模型。

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [284] [STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle](https://arxiv.org/abs/2509.10692)
*Giuseppe Silano,Amr Afifi,Martin Saska,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出一种基于信号时序逻辑（STL）的运动规划和风险分析新方法，旨在增强多旋翼无人机（MRAV）在人机协作中的安全性、效率和鲁棒性，特别关注人体工程学和不确定性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升多旋翼无人机在人机协作中的表现，解决安全、时间、人类偏好（如人体工程学和舒适度）等关键任务目标，并应对动态环境中的不确定性和突发事件。

Method: 该方法采用信号时序逻辑（STL）编码任务目标，包括安全性、时序和人类偏好。通过优化框架生成动态可行的轨迹，同时考虑MRAV的物理约束。针对问题的非线性和非凸性，使用平滑近似和基于梯度的技术。此外，引入了不确定性感知风险分析来评估任务偏差，并实施了事件触发的重规划策略以应对突发事件。通过MATLAB和Gazebo模拟验证了该方法，模拟场景为电力线维护中的物体交接任务。

Result: 模拟结果表明，该方法能够有效实现安全、高效且具有弹性的多旋翼无人机人机协作。

Conclusion: 该研究提出的方法在人机协作中，通过整合STL、优化框架和不确定性风险分析，显著提升了多旋翼无人机的安全、效率和应对不确定性的能力，为未来的人机协作系统提供了有价值的解决方案。

Abstract: This paper presents a novel approach to motion planning and risk analysis for
enhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).
The proposed method uses Signal Temporal Logic (STL) to encode key mission
objectives, such as safety, timing, and human preferences, with a strong focus
on ergonomics and comfort. An optimization framework generates dynamically
feasible trajectories while considering the MRAV's physical constraints. Given
the nonlinear and non-convex nature of the problem, smooth approximations and
gradient-based techniques assist in handling the problem's computational
complexity. Additionally, an uncertainty-aware risk analysis is incorporated to
assess potential deviations from the mission specifications, providing insights
into the likelihood of mission success under uncertain conditions. Further, an
event-triggered replanning strategy is implemented to respond to unforeseen
events and external disturbances. The approach is validated through MATLAB and
Gazebo simulations, using an object handover task in a mock-up environment
inspired by power line maintenance scenarios. The results highlight the
method's effectiveness in achieving safe, efficient, and resilient human-robot
collaboration.

</details>


### [285] [A Survey on LiDAR-based Autonomous Aerial Vehicles](https://arxiv.org/abs/2509.10730)
*Yunfan Ren,Yixi Cai,Haotian Li,Nan Chen,Fangcheng Zhu,Longji Yin,Fanze Kong,Rundong Li,Fu Zhang*

Main category: cs.RO

TL;DR: 这篇综述全面概述了基于激光雷达的自主无人机在设计、感知、规划和控制方面的最新进展，并讨论了其应用、现有挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 激光雷达技术已成为实现高速、敏捷、可靠的无人机导航的关键，尤其是在GPS受限环境中。其高精度、长距离深度测量和在各种光照条件下的稳健性能使其非常适合无人机应用，显著增强了无人机的自主性。

Method: 本综述首先回顾了激光雷达传感器的演变及其独特优势。接着，探讨了基于激光雷达的无人机所需的关键软件组件，包括用于状态估计和建图的感知技术，以及轨迹规划和控制方法。此外，还分析了其在工业运营、不同空中平台和无人机集群部署等方面的实际应用。

Result: 激光雷达与无人机的集成显著提升了无人机在复杂和挑战性环境中的自主执行复杂任务的能力。综述涵盖了从传感器发展到感知、规划、控制策略以及多种实际应用（如工业操作、空中平台支持和无人机集群）的全面进展。

Conclusion: 当前基于激光雷达的无人机系统仍面临挑战，未来研究方向应侧重于进一步提升其性能和促进多无人机协作，以推动该领域的发展。

Abstract: This survey offers a comprehensive overview of recent advancements in
LiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,
perception, planning, and control strategies. Over the past decade, LiDAR
technology has become a crucial enabler for high-speed, agile, and reliable UAV
navigation, especially in GPS-denied environments. The paper begins by
examining the evolution of LiDAR sensors, emphasizing their unique advantages
such as high accuracy, long-range depth measurements, and robust performance
under various lighting conditions, making them particularly well-suited for UAV
applications. The integration of LiDAR with UAVs has significantly enhanced
their autonomy, enabling complex missions in diverse and challenging
environments. Subsequently, we explore essential software components, including
perception technologies for state estimation and mapping, as well as trajectory
planning and control methodologies, and discuss their adoption in LiDAR-based
UAVs. Additionally, we analyze various practical applications of the
LiDAR-based UAVs, ranging from industrial operations to supporting different
aerial platforms and UAV swarm deployments. The survey concludes by discussing
existing challenges and proposing future research directions to advance
LiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent
developments, this paper aims to provide a valuable resource for researchers
and practitioners working to push the boundaries of LiDAR-based UAV systems.

</details>


### [286] [Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures](https://arxiv.org/abs/2509.10735)
*Mohammad Rafiee Javazm,Yash Kulkarni,Jiaqi Xue,Naruhiko Ikoma,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 本文提出并开发了一个直观、模块化、易于安装的机电框架，用于机器人辅助内窥镜控制，以解决手动操作的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管内窥镜设备在癌症筛查中广泛应用，但其手动控制对临床医生来说仍然具有挑战性，导致工作量增加、疲劳和分心等关键问题。

Method: 该框架包括：(i) 一种新颖的嵌套筒夹式夹持机构，可与现有内窥镜设备集成并控制其弯曲自由度；(ii) 一种进给机构，可控制结肠镜的插入/ rút 出自由度；(iii) 一个互补且直观的用户界面，可在手术过程中同时控制所有自由度。此外，还引入了数学建模方法和设计空间，用于优化夹持和进给机构的参数选择。

Result: 仿真和实验研究充分证明了所提出的数学建模和机器人框架的性能。

Conclusion: 所提出的机电框架及其数学建模方法有效地解决了内窥镜手动控制的挑战，并为机器人辅助内窥镜操作提供了高性能解决方案。

Abstract: Despite the widespread adoption of endoscopic devices for several cancer
screening procedures, manual control of these devices still remains challenging
for clinicians, leading to several critical issues such as increased workload,
fatigue, and distractions. To address these issues, in this paper, we introduce
the design and development of an intuitive, modular, and easily installable
mechatronic framework. This framework includes (i) a novel nested collet-chuck
gripping mechanism that can readily be integrated and assembled with the
existing endoscopic devices and control their bending degrees-of-freedom
(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF
of a colonoscope, and (iii) a complementary and intuitive user interface that
enables simultaneous control of all DoFs during the procedure. To analyze the
design of the proposed mechanisms, we also introduce a mathematical modeling
approach and a design space for optimal selection of the parameters involved in
the design of gripping and feeder mechanisms. Our simulation and experimental
studies thoroughly demonstrate the performance of the proposed mathematical
modeling and robotic framework.

</details>


### [287] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: 该研究利用GPU加速ORB-SLAM3的跟踪模块，显著提升了立体惯性SLAM的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 视觉惯性SLAM系统的跟踪模块需要及时处理每一帧数据以估计位姿，否则会导致定位不佳或跟踪丢失。因此，提高跟踪效率至关重要。

Method: 提出了一种新方法，利用GPU计算能力（CUDA）加速跟踪过程中耗时的组件，包括立体特征匹配和局部地图跟踪。该设计在ORB-SLAM3的跟踪过程中实现。

Result: 在立体惯性模式下，使用EuRoC和TUM-VI数据集进行评估，桌面和Jetson Xavier NX开发板上的跟踪性能整体提升高达2.8倍。

Conclusion: 通过利用GPU加速跟踪模块中的关键组件，可以显著提高视觉惯性SLAM系统的跟踪性能和效率。

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [288] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL-RL是一个开源强化学习库，专为机器人社区设计，特点是代码紧凑、易于修改、专注于机器人算法，并支持GPU加速训练，在仿真和实际机器人实验中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有通用强化学习框架未能充分满足机器人社区的特定需求，如需要紧凑、易于修改的代码库，以及针对机器人特定挑战的辅助技术。

Method: RSL-RL通过采用紧凑且易于修改的代码库设计，专注于机器人领域最广泛采用的算法及其辅助技术，并针对GPU训练进行优化，以实现大规模仿真环境中的高吞吐量性能。

Result: 该库在大型仿真环境和实际机器人实验中均得到了验证，展示了其作为开发基于学习的机器人控制器的轻量级、可扩展和实用框架的有效性，并实现了高吞吐量性能。

Conclusion: RSL-RL是一个实用、轻量级且可扩展的开源强化学习框架，非常适合机器人社区开发基于学习的机器人控制器。

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [289] [Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following](https://arxiv.org/abs/2509.10796)
*Hanjing Ye,Weixi Situ,Jianwei Peng,Yu Zhan,Bingyi Xia,Kuanqi Cai,Hong Zhang*

Main category: cs.RO

TL;DR: 本研究对机器人跟随人（RPF）进行了首次端到端分析，提出了一个名为Follow-Bench的统一基准，重新实现了六种主流RPF规划器，并评估了它们在安全性和舒适性方面的权衡，同时提供了真实世界部署的见解。


<details>
  <summary>Details</summary>
Motivation: 机器人跟随人（RPF）在个人助理、安保巡逻、老年护理和物流等领域具有新兴应用。然而，为确保有效性，机器人必须在跟随目标的同时，兼顾目标和周围人员的安全与舒适性。

Method: 本研究方法包括：(i) 调查代表性RPF场景、运动规划方法和评估指标，重点关注安全性和舒适性；(ii) 引入Follow-Bench，一个模拟多样化场景（包括不同目标轨迹、动态人群流和环境布局）的统一基准；(iii) 重新实现六种流行的RPF规划器，系统地考虑安全性和舒适性；(iv) 在差动驱动机器人上评估基准中表现最佳的两个规划器，以提供真实世界部署的见解。

Result: 广泛的模拟和真实世界实验提供了对现有规划器在安全-舒适权衡方面的定量见解。研究还揭示了当前存在的开放挑战和未来的研究方向。

Conclusion: 本研究首次对机器人跟随人进行了端到端分析，通过引入统一基准和系统评估，量化了现有规划器在安全性和舒适性方面的表现和权衡，并为未来的研究指明了方向。

Abstract: Robot person following (RPF) -- mobile robots that follow and assist a
specific person -- has emerging applications in personal assistance, security
patrols, eldercare, and logistics. To be effective, such robots must follow the
target while ensuring safety and comfort for both the target and surrounding
people. In this work, we present the first end-to-end study of RPF, which (i)
surveys representative scenarios, motion-planning methods, and evaluation
metrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a
unified benchmark simulating diverse scenarios, including various target
trajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)
re-implements six popular RPF planners, ensuring that both safety and comfort
are systematically considered. Moreover, we evaluate the two highest-performing
planners from our benchmark on a differential-drive robot to provide insights
into real-world deployment. Extensive simulation and real-world experiments
provide quantitative insights into the safety-comfort trade-offs of existing
planners, while revealing open challenges and future research directions.

</details>


### [290] [A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots](https://arxiv.org/abs/2509.10862)
*Temma Suzuki,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 本研究开发了一种通用线缆测试机，用于测量和调整线缆特性，以减少线缆驱动机构的建模误差，并成功应用于线缆驱动机器人的力控制，降低了末端执行器力误差。


<details>
  <summary>Details</summary>
Motivation: 线缆作为传动机制具有轻量化、低摩擦的优点，但由于其柔性特性导致建模误差大，限制了其在工业和研究机器人中的应用。

Method: 构建了一台通用线缆测试机。利用该机器进行了：1) 消除线缆初始拉伸；2) 测量八种不同直径被动滑轮的张力传输效率；3) 测量变长线缆的动态行为。最后，将获得的线缆特性数据应用于实际线缆驱动机器人的力控制。

Result: 通过该测试机实现了线缆特性的测量和调整。将测试数据应用于线缆驱动机器人后，成功降低了末端执行器的力误差。

Conclusion: 所开发的通用线缆测试机及其获取的数据能够有效改善线缆驱动机构的性能，特别是通过精确的力控制来解决线缆固有的柔性和建模误差问题。

Abstract: Compared with gears and linkages, wires constitute a lightweight,
low-friction transmission mechanism. However, because wires are flexible
materials, they tend to introduce large modeling errors, and their adoption in
industrial and research robots remains limited.In this study, we built a
Universal Wire Testing Machine that enables measurement and adjustment of wire
characteristics to improve the performance of wire-driven mechanisms. Using
this testing machine, we carried out removal of initial wire stretch,
measurement of tension transmission efficiency for eight different diameters of
passive pulleys, and measurement of the dynamic behavior of variable-length
wires. Finally, we applied the data obtained from this testing machine to the
force control of an actual wire-driven robot, reducing the end-effector force
error.

</details>


### [291] [Nav-R1: Reasoning and Navigation in Embodied Scenes](https://arxiv.org/abs/2509.10884)
*Qingxiang Liu,Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.RO

TL;DR: Nav-R1是一个具身基础模型，通过构建CoT数据集、基于GRPO的多奖励强化学习框架和“快中慢”推理范式，解决了具身导航中推理不稳定、泛化能力差和实时控制与语义推理平衡困难的问题，并在基准测试和真实世界部署中展现出优越性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航方法存在推理轨迹不连贯不稳定、泛化能力差以及难以平衡长程语义推理与低延迟控制的问题。

Method: 1. 提出了具身基础模型Nav-R1。2. 构建了包含110K步进式思维链（CoT）的大规模数据集Nav-CoT-110K，用于结构化推理的冷启动。3. 设计了一个基于GRPO的强化学习框架，包含格式、理解和导航三种互补奖励，以提高结构依从性、语义基础和路径保真度。4. 引入了“快中慢”推理范式，将深思熟虑的语义推理与低延迟的反应式控制解耦。

Result: Nav-R1在具身AI基准测试中持续优于强基线，推理和导航性能平均提高超过8%。在移动机器人上的真实世界部署进一步验证了其在有限板载资源下的鲁棒性。

Conclusion: Nav-R1通过统一具身环境中的推理，有效解决了现有具身导航方法的挑战，实现了更连贯、稳定和高效的导航，并在多种环境下表现出卓越的性能和实际应用价值。

Abstract: Embodied navigation requires agents to integrate perception, reasoning, and
action for robust interaction in complex 3D environments. Existing approaches
often suffer from incoherent and unstable reasoning traces that hinder
generalization across diverse environments, and difficulty balancing
long-horizon semantic reasoning with low-latency control for real-time
navigation. To address these challenges, we propose Nav-R1, an embodied
foundation model that unifies reasoning in embodied environments. We first
construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought
(CoT) for embodied tasks, which enables cold-start initialization with
structured reasoning. Building on this foundation, we design a GRPO-based
reinforcement learning framework with three complementary rewards: format,
understanding, and navigation, to improve structural adherence, semantic
grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow
reasoning paradigm, decoupling deliberate semantic reasoning from low-latency
reactive control for efficient yet coherent navigation. Extensive evaluations
on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms
strong baselines, with over 8% average improvement in reasoning and navigation
performance. Real-world deployment on a mobile robot further validates its
robustness under limited onboard resources. Code:
https://github.com/AIGeeksGroup/Nav-R1. Website:
https://aigeeksgroup.github.io/Nav-R1.

</details>


### [292] [Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics](https://arxiv.org/abs/2509.10888)
*Weijie Liu,Ziyi Qiu,Shihang Wang,Deqing Mei,Yancheng Wang*

Main category: cs.RO

TL;DR: 该研究提出了一种受CDMA启发的正交数字编码新架构，通过去中心化编码策略解决现有柔性触觉传感器在编码效率和布线复杂性方面的挑战，实现了大规模、高灵敏度、快速响应的类人触觉感知，大幅减少了布线并提高了数据吞吐量。


<details>
  <summary>Details</summary>
Motivation: 下一代智能机器人需要类人具身触觉感知能力。然而，现有柔性触觉传感器在编码效率和布线复杂性方面存在瓶颈，严重阻碍了大规模、全身软覆盖、高灵敏度、快速响应的实现，无法达到人皮肤级别的触觉感知所需的可扩展性和实时性能。

Method: 该研究提出了一种受码分多址（CDMA）启发的正交数字编码新架构。其去中心化编码策略通过分布式传感节点实现能量正交基码的并行叠加，将传统的串行信号传输转变为并行传输，从而大幅减少布线需求并提高数据吞吐量。

Result: 研究人员使用现成的16节点传感阵列验证了该策略，仅用一根传输线就重建了压力分布，实现了12.8毫秒的时间分辨率。更重要的是，该架构在节点数量跨越多个数量级（达到数千个节点）的情况下，仍能保持低于20毫秒的延迟。

Conclusion: 这项工作通过从根本上重新定义软电子学中的信号编码范式，为开发具有类人感知能力的可扩展具身智能系统开辟了新领域。

Abstract: Human-like embodied tactile perception is crucial for the next-generation
intelligent robotics. Achieving large-area, full-body soft coverage with high
sensitivity and rapid response, akin to human skin, remains a formidable
challenge due to critical bottlenecks in encoding efficiency and wiring
complexity in existing flexible tactile sensors, thus significantly hinder the
scalability and real-time performance required for human skin-level tactile
perception. Herein, we present a new architecture employing code division
multiple access-inspired orthogonal digital encoding to overcome these
challenges. Our decentralized encoding strategy transforms conventional serial
signal transmission by enabling parallel superposition of energy-orthogonal
base codes from distributed sensing nodes, drastically reducing wiring
requirements and increasing data throughput. We implemented and validated this
strategy with off-the-shelf 16-node sensing array to reconstruct the pressure
distribution, achieving a temporal resolution of 12.8 ms using only a single
transmission wire. Crucially, the architecture can maintain sub-20ms latency
across orders-of-magnitude variations in node number (to thousands of nodes).
By fundamentally redefining signal encoding paradigms in soft electronics, this
work opens new frontiers in developing scalable embodied intelligent systems
with human-like sensory capabilities.

</details>


### [293] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: 该论文提出ViSTR-GP框架，利用独立于控制器之外的视觉侧信道，通过将编码器报告的测量值与顶置摄像头的视觉估计值进行交叉检查，在线检测工业机器人制造过程中的数据完整性攻击。


<details>
  <summary>Details</summary>
Motivation: 智能制造中的工业机器人系统面临日益增长的网络安全风险，特别是数据完整性攻击。这类攻击通过操纵操作数据，难以仅凭现有入侵检测或基于模型的检测方法发现。

Method: 开发了ViSTR-GP在线检测框架。该框架利用顶置摄像头作为视觉侧信道。具体方法包括：使用一次性交互式分割初始化SAM-Track以生成每帧掩码；低秩张量回归代理将每个掩码映射到测量值；矩阵变量高斯过程建模标称残差，捕获时间结构和交叉关节相关性；从预测分布中导出逐帧测试统计量，提供具有可解释阈值的在线检测器。

Result: 在真实机器人测试台上进行了验证，结果表明所提出的框架能准确恢复关节角度，并且比所有基线方法更早、更频繁地检测到数据完整性攻击，尤其在最细微的攻击中表现出显著改进。

Conclusion: 研究表明，通过增加一个独立的物理通道（视觉），绕过控制器的权限，工厂无需复杂的仪器设备即可有效检测数据完整性攻击。

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [294] [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://arxiv.org/abs/2509.10952)
*Yangcen Liu,Woo Chul Shin,Yunhai Han,Zhenyang Chen,Harish Ravichandar,Danfei Xu*

Main category: cs.RO

TL;DR: ImMimic是一个跨实体协同训练框架，通过动态时间规整和轨迹插值，利用人类视频和少量机器人示教数据，有效弥合领域差距，提高机器人操作的成功率和流畅性。


<details>
  <summary>Details</summary>
Motivation: 机器人专用数据收集成本高昂，而直接模仿人类视频存在视觉、形态和物理方面的领域差距，阻碍了机器人从人类视频中学习操作。

Method: ImMimic框架采用动态时间规整（DTW），结合动作或视觉映射将重定向的人类手部姿态映射到机器人关节。随后，它在配对的人类和机器人轨迹之间进行MixUp插值。核心思想是：重定向的人类手部轨迹提供信息丰富的动作标签；对映射数据进行插值创建中间领域，促进协同训练期间的平滑领域适应。

Result: 在四种真实世界操作任务（抓取放置、推动、锤击、翻转）和四种机器人实体上进行的评估表明，ImMimic显著提高了任务成功率和执行流畅性。

Conclusion: ImMimic框架通过有效弥合领域差距，实现了从人类视频中进行稳健的机器人操作学习，并提高了任务性能。

Abstract: Learning robot manipulation from abundant human videos offers a scalable
alternative to costly robot-specific data collection. However, domain gaps
across visual, morphological, and physical aspects hinder direct imitation. To
effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic
co-training framework that leverages both human videos and a small amount of
teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with
either action- or visual-based mapping to map retargeted human hand poses to
robot joints, followed by MixUp interpolation between paired human and robot
trajectories. Our key insights are (1) retargeted human hand trajectories
provide informative action labels, and (2) interpolation over the mapped data
creates intermediate domains that facilitate smooth domain adaptation during
co-training. Evaluations on four real-world manipulation tasks (Pick and Place,
Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,
Ability) show that ImMimic improves task success rates and execution
smoothness, highlighting its efficacy to bridge the domain gap for robust robot
manipulation. The project website can be found at
https://sites.google.com/view/immimic.

</details>


### [295] [Pogosim -- a Simulator for Pogobot robots](https://arxiv.org/abs/2509.10968)
*Leo Cazenille,Loona Macabre,Nicolas Bredeche*

Main category: cs.RO

TL;DR: 本文介绍Pogosim，一个为Pogobots（一种开源群机器人）设计的快速可扩展模拟器，旨在降低群智能算法的开发成本，并允许模拟和真实机器人使用相同的代码。


<details>
  <summary>Details</summary>
Motivation: 在真实的Pogobots上测试分布式算法非常耗费人力和资源，特别是对于复杂问题或参数校准，会带来巨大的负担。

Method: 本文提出了Pogosim，一个为Pogobots设计的快速可扩展模拟器。它详细介绍了Pogosim的软件架构、配置文件的编写、用户程序的开发，以及模拟如何近似或异于实验。此外，还描述了如何并行启动大量模拟、检索和分析模拟结果，以及如何使用优化算法优化用户代码参数。

Result: Pogosim能够显著降低算法开发成本，通过允许模拟和真实机器人使用完全相同的代码来促进群智能算法的实现。它支持并行模拟、结果分析和参数优化。

Conclusion: Pogosim为Pogobots群机器人研究提供了一个高效的开发和测试平台，极大地减少了直接在机器人上进行实验所需的资源和工作量，加速了群智能算法的开发过程。

Abstract: Pogobots are a new type of open-source/open-hardware robots specifically
designed for swarm robotics research. Their cost-effective and modular design,
complemented by vibration-based and wheel-based locomotion, fast infrared
communication and extensive software architecture facilitate the implementation
of swarm intelligence algorithms. However, testing even simple distributed
algorithms directly on robots is particularly labor-intensive. Scaling to more
complex problems or calibrate user code parameters will have a prohibitively
high strain on available resources. In this article we present Pogosim, a fast
and scalable simulator for Pogobots, designed to reduce as much as possible
algorithm development costs. The exact same code will be used in both
simulation and to experimentally drive real robots. This article details the
software architecture of Pogosim, explain how to write configuration files and
user programs and how simulations approximate or differ from experiments. We
describe how a large set of simulations can be launched in parallel, how to
retrieve and analyze the simulation results, and how to optimize user code
parameters using optimization algorithms.

</details>


### [296] [Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter](https://arxiv.org/abs/2509.10979)
*Dimitri Jacquemont,Carlo Bosio,Teaya Yang,Ruiqi Zhang,Ozgur Orun,Shuai Li,Reza Alam,Thomas M. Schutzius,Simo A. Makiharju,Mark W. Mueller*

Main category: cs.RO

TL;DR: 提出了一种基于四旋翼无人机的自动化系统，用于喷涂光伏（PV）面板的保护涂层，以提高效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 光伏面板效率的小幅提升能产生巨大影响。抗反射和自清洁涂层可增强面板性能，但会随时间降解，需要定期重新涂覆。无人机（UAV）提供了一种比传统手动方法更灵活、自主、更频繁且成本更低的涂层应用方式。

Method: 该系统采用配备液体分散机构的四旋翼无人机。定位堆栈仅使用机载传感器，依赖视觉惯性里程计和相对于四旋翼检测到的光伏面板的相对位置。控制采用基于模型的控制器，该控制器考虑了地面效应和液体分散过程中四旋翼质量的减少。

Result: 通过大量的室内和室外实验，验证了该系统的自主能力。

Conclusion: 所提出的基于四旋翼无人机的系统能够自动化光伏面板保护涂层的应用，从而提高效率并降低成本，为可再生能源领域带来了巨大的潜力。

Abstract: Photovoltaic (PV) panels are becoming increasingly widespread in the domain
of renewable energy, and thus, small efficiency gains can have massive effects.
Anti-reflective and self-cleaning coatings enhance panel performance but
degrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles
(UAVs) offer a flexible and autonomous way to apply protective coatings more
often and at lower cost compared to traditional manual coating methods. In this
letter, we propose a quadcopter-based system, equipped with a liquid dispersion
mechanism, designed to automate such tasks. The localization stack only uses
onboard sensors, relying on visual-inertial odometry and the relative position
of the PV panel detected with respect to the quadcopter. The control relies on
a model-based controller that accounts for the ground effect and the mass
decrease of the quadcopter during liquid dispersion. We validate the autonomy
capabilities of our system through extensive indoor and outdoor experiments.

</details>


### [297] [Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach](https://arxiv.org/abs/2509.11025)
*Peng Chen,Jing Liang,Hui Song,Kang-Jia Qiao,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 针对农业劳动力成本上升背景下果园采摘多机器人系统的高效协调问题，本文定义了一个考虑负载依赖速度和电池限制等实际约束的多目标任务分配（AMERTA）问题，并提出了一种混合分层路径重建算法（HRRA）。实验证明HRRA性能优越，能够探索新的解决方案空间，对理论和实践均有贡献。


<details>
  <summary>Details</summary>
Motivation: 农业劳动力成本不断增加，加速了多机器人系统在果园采摘中的应用。然而，由于完工时间与能耗之间复杂的相互作用，以及负载依赖的速度变化和电池限制等实际约束，有效协调这些系统面临巨大挑战，且这些实际约束常被忽视。

Method: 本文定义了多目标农业多电动机器人任务分配（AMERTA）问题，该问题系统地纳入了实际约束（如负载依赖的速度变化和电池限制）。为解决此问题，提出了一种混合分层路径重建算法（HRRA），该算法集成了分层编码结构、双阶段初始化方法、任务序列优化器和专门的路径重建算子等创新机制。

Result: 在45个测试实例上的大量实验表明，HRRA的性能优于七种最先进的算法。包括威尔科克森符号秩检验和弗里德曼检验在内的统计分析，经验性地验证了HRRA的竞争力及其探索以前无法访问的解决方案空间区域的独特能力。

Conclusion: 这项研究通过提供新颖的问题表述和有效的算法，促进了对多机器人协调的理论理解，同时也为农业自动化提供了实用的见解。

Abstract: The increasing labor costs in agriculture have accelerated the adoption of
multi-robot systems for orchard harvesting. However, efficiently coordinating
these systems is challenging due to the complex interplay between makespan and
energy consumption, particularly under practical constraints like
load-dependent speed variations and battery limitations. This paper defines the
multi-objective agricultural multi-electrical-robot task allocation (AMERTA)
problem, which systematically incorporates these often-overlooked real-world
constraints. To address this problem, we propose a hybrid hierarchical route
reconstruction algorithm (HRRA) that integrates several innovative mechanisms,
including a hierarchical encoding structure, a dual-phase initialization
method, task sequence optimizers, and specialized route reconstruction
operators. Extensive experiments on 45 test instances demonstrate HRRA's
superior performance against seven state-of-the-art algorithms. Statistical
analysis, including the Wilcoxon signed-rank and Friedman tests, empirically
validates HRRA's competitiveness and its unique ability to explore previously
inaccessible regions of the solution space. In general, this research
contributes to the theoretical understanding of multi-robot coordination by
offering a novel problem formulation and an effective algorithm, thereby also
providing practical insights for agricultural automation.

</details>


### [298] [FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers](https://arxiv.org/abs/2509.11109)
*Jiaxin Huang,Hanyu Liu,Yunsheng Ma,Jian Shen,Yilin Zheng,Jiayi Wen,Baishu Wan,Pan Li,Zhigong Song*

Main category: cs.RO

TL;DR: 本研究开发了一个包括人形机器人和外骨骼式远程操作舱的硬件平台，并提出了一个名为FEWT的模仿学习框架，该框架通过结合时域和频域特征，显著提高了人形机器人在模拟和真实世界中的操作成功率。


<details>
  <summary>Details</summary>
Motivation: 具身智能连接物理世界与信息空间，而人形机器人作为其典型物理载体，在机器人学习算法方面展现出巨大潜力。研究旨在实现直观的远程操作、高效收集拟人动作数据，并改进人形机器人的感知表示。

Method: 1. 硬件平台：开发了包括人形机器人和外骨骼式远程操作舱的平台，用于实现直观的远程操作和高效的拟人动作数据收集。2. 模仿学习框架：提出了频率增强小波Transformer (FEWT)，包含两个主要模块：频率增强高效多尺度注意力 (FE-EMA) 和时序离散小波变换 (TS-DWT)。FE-EMA通过结合多尺度小波分解和残差网络，动态融合时域和频域特征，以有效捕捉不同尺度的特征信息。

Result: 实验结果表明，FEWT框架在模拟环境中将最先进算法（ACT基线）的成功率提高了30%，在真实世界中提高了6-12%。

Conclusion: 所开发的硬件平台和FEWT模仿学习框架显著提升了人形机器人的远程操作和数据收集能力，并通过有效融合时频域特征，增强了模型的鲁棒性，在模拟和真实世界中均取得了显著的成功率提升。

Abstract: The embodied intelligence bridges the physical world and information space.
As its typical physical embodiment, humanoid robots have shown great promise
through robot learning algorithms in recent years. In this study, a hardware
platform, including humanoid robot and exoskeleton-style teleoperation cabin,
was developed to realize intuitive remote manipulation and efficient collection
of anthropomorphic action data. To improve the perception representation of
humanoid robot, an imitation learning framework, termed Frequency-Enhanced
Wavelet-based Transformer (FEWT), was proposed, which consists of two primary
modules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and
Time-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale
wavelet decomposition with the residual network, FE-EMA can dynamically fuse
features from both time-domain and frequency-domain. This fusion is able to
capture feature information across various scales effectively, thereby
enhancing model robustness. Experimental performance demonstrates that FEWT
improves the success rate of the state-of-the-art algorithm (Action Chunking
with Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in
real-world.

</details>


### [299] [ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations](https://arxiv.org/abs/2509.11125)
*Zheng Li,Pei Qu,Yufei Jia,Shihui Zhou,Haizhou Ge,Jiahang Cao,Jinni Zhou,Guyue Zhou,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出ManiVID-3D，一种3D视觉强化学习架构，通过自监督解耦特征学习和ViewNet模块，实现视角不变的机器人操作策略，并在视角变化下表现出卓越的鲁棒性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉强化学习策略在真实世界机器人操作中，由于摄像机视角变化（如固定前置摄像头训练的策略在摄像头移动时失效），以及对精确校准的依赖或难以处理大视角变化，导致部署受阻。

Method: 本文提出ManiVID-3D，一个新颖的3D强化学习架构，用于机器人操作。主要方法包括：1) 通过自监督解耦特征学习，学习视角不变的表示。2) 引入ViewNet模块，无需外部校准即可将任意视角的点云观测对齐到统一空间坐标系。3) 开发了一个高效的GPU加速批量渲染模块，实现每秒处理超过5000帧，支持大规模3D视觉强化学习训练。

Result: 在10个模拟任务和5个真实世界任务的广泛评估中，ManiVID-3D在视角变化下比现有最先进方法成功率高44.7%，同时参数量减少80%。该系统对严重透视变化表现出强大的鲁棒性，并具有出色的模拟到真实性能。

Conclusion: 学习几何一致的表示对于在非结构化环境中实现可扩展的机器人操作是有效的，ManiVID-3D在视角变化下的鲁棒性和强大的模拟到真实性能证明了其有效性。

Abstract: Deploying visual reinforcement learning (RL) policies in real-world
manipulation is often hindered by camera viewpoint changes. A policy trained
from a fixed front-facing camera may fail when the camera is shifted--an
unavoidable situation in real-world settings where sensor placement is hard to
manage appropriately. Existing methods often rely on precise camera calibration
or struggle with large perspective changes. To address these limitations, we
propose ManiVID-3D, a novel 3D RL architecture designed for robotic
manipulation, which learns view-invariant representations through
self-supervised disentangled feature learning. The framework incorporates
ViewNet, a lightweight yet effective module that automatically aligns point
cloud observations from arbitrary viewpoints into a unified spatial coordinate
system without the need for extrinsic calibration. Additionally, we develop an
efficient GPU-accelerated batch rendering module capable of processing over
5000 frames per second, enabling large-scale training for 3D visual RL at
unprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world
tasks demonstrates that our approach achieves a 44.7% higher success rate than
state-of-the-art methods under viewpoint variations while using 80% fewer
parameters. The system's robustness to severe perspective changes and strong
sim-to-real performance highlight the effectiveness of learning geometrically
consistent representations for scalable robotic manipulation in unstructured
environments. Our project website can be found in
https://zheng-joe-lee.github.io/manivid3d/.

</details>


### [300] [RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations](https://arxiv.org/abs/2509.11149)
*Mintae Kim,Jiaze Cai,Koushil Sreenath*

Main category: cs.RO

TL;DR: 本文提出RoVerFly，一个统一的基于强化学习的控制框架，用于四旋翼飞行器及其携带柔性缆绳悬挂载荷的精确轨迹跟踪，实现了对不同配置的鲁棒性和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 为四旋翼飞行器设计鲁棒的精确轨迹跟踪控制器极具挑战，尤其是在存在非线性动力学、欠驱动以及柔性缆绳悬挂载荷引入额外自由度和混合特性时。传统的基于模型的控制方法虽然提供稳定性保证，但需要大量调优，且难以适应配置变化（如载荷增减、质量或缆绳长度变化）。

Method: 研究人员提出了RoVerFly，一个统一的基于学习的控制框架。该框架中，一个强化学习（RL）策略被用作鲁棒且通用的跟踪控制器。该控制器通过任务和领域随机化进行训练，以应对干扰和变化的动力学。

Result: RoVerFly控制器对干扰和变化的动力学表现出强大的韧性。它在各种载荷设置下（包括无载荷以及不同质量和缆绳长度）实现了强大的零样本泛化能力，无需控制器切换或重新调优。同时，它保留了反馈跟踪控制器的可解释性和结构。

Conclusion: RoVerFly提供了一个统一的、基于强化学习的解决方案，能够为标准四旋翼飞行器和携带缆绳悬挂载荷的系统提供鲁棒且通用的轨迹跟踪。该方法通过领域随机化训练，展现出对不同配置的卓越适应性和零样本泛化能力，有效解决了传统方法在适应复杂动态和配置变化方面的局限性。

Abstract: Designing robust controllers for precise, arbitrary trajectory tracking with
quadrotors is challenging due to nonlinear dynamics and underactuation, and
becomes harder with flexible cable-suspended payloads that introduce extra
degrees of freedom and hybridness. Classical model-based methods offer
stability guarantees but require extensive tuning and often do not adapt when
the configuration changes, such as when a payload is added or removed, or when
the payload mass or cable length varies. We present RoVerFly, a unified
learning-based control framework in which a reinforcement learning (RL) policy
serves as a robust and versatile tracking controller for standard quadrotors
and for cable-suspended payload systems across a range of configurations.
Trained with task and domain randomization, the controller is resilient to
disturbances and varying dynamics. It achieves strong zero-shot generalization
across payload settings, including no payload as well as varying mass and cable
length, without controller switching or re-tuning, while retaining the
interpretability and structure of a feedback tracking controller. Code and
supplementary materials are available at
https://github.com/mintaeshkim/roverfly

</details>


### [301] [SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators](https://arxiv.org/abs/2509.11185)
*Kai Chen,Zhihai Bi,Guoyang Zhao,Chunxin Zheng,Yulin Li,Hang Zhao,Jun Ma*

Main category: cs.RO

TL;DR: 该论文提出了一种名为SAMP的统一框架，通过在共享空间网格上使用符号距离场（SDF）同时编码机器人和环境，以解决现有神经运动规划方法在同时考虑机器人形状和环境方面的局限性，从而实现更安全、更高效的无碰撞轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 现有的神经运动规划方法难以同时考虑机器人的物理形状和周围环境，常常依赖简化的机器人模型或主要关注障碍物表示，导致碰撞检测不完整，并在复杂场景中性能下降。

Method: 提出SAMP（空间锚点运动策略）框架，该框架在共享空间网格上使用符号距离场（SDF）同时编码环境和机械臂。SAMP包含一个专门的机器人SDF网络，用于精确捕捉机械臂几何形状，实现超越粗略连杆近似的碰撞感知推理。这些表示在空间锚点上融合，并用于训练一个神经运动策略，通过高效的特征对齐策略生成平滑、无碰撞的轨迹。

Result: 在模拟和真实世界环境中的实验表明，SAMP的成功率提高了11%，碰撞率降低了7%，优于现有方法。

Conclusion: 联合建模机器人和环境几何形状具有显著优势和实用价值，特别是在具有挑战性的真实世界环境中。

Abstract: Neural-based motion planning methods have achieved remarkable progress for
robotic manipulators, yet a fundamental challenge lies in simultaneously
accounting for both the robot's physical shape and the surrounding environment
when generating safe and feasible motions. Moreover, existing approaches often
rely on simplified robot models or focus primarily on obstacle representation,
which can lead to incomplete collision detection and degraded performance in
cluttered scenes. To address these limitations, we propose spatial anchor-based
motion policy (SAMP), a unified framework that simultaneously encodes the
environment and the manipulator using signed distance field (SDF) anchored on a
shared spatial grid. SAMP incorporates a dedicated robot SDF network that
captures the manipulator's precise geometry, enabling collision-aware reasoning
beyond coarse link approximations. These representations are fused on spatial
anchors and used to train a neural motion policy that generates smooth,
collision-free trajectories in the proposed efficient feature alignment
strategy. Experiments conducted in both simulated and real-world environments
consistently show that SAMP outperforms existing methods, delivering an 11%
increase in success rate and a 7% reduction in collision rate. These results
highlight the benefits of jointly modelling robot and environment geometry,
demonstrating its practical value in challenging real-world environments.

</details>


### [302] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: DreamNav是一种零样本视觉-语言导航（VLN-CE）方法，它通过稳定自我中心感知、轨迹级规划和主动想象来解决现有方法的成本高、行动语义不匹配和规划短视问题，并在VLN-CE和真实世界测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本视觉-语言导航方法依赖昂贵的感知和被动场景理解，将控制简化为点级选择，导致部署成本高、行动语义不匹配以及规划短视。这些问题促使研究者寻求更高效、更具前瞻性的解决方案。

Method: 本文提出了DreamNav，通过以下三个方面解决问题：1) 使用EgoView Corrector校准视角并稳定自我中心感知，以降低感知成本；2) 引入Trajectory Predictor进行全局轨迹级规划，而非点级动作，以更好地匹配指令语义；3) 设计Imagination Predictor赋予智能体主动思考能力，实现前瞻性、长周期规划。

Result: DreamNav在VLN-CE和真实世界测试中设定了新的零样本最先进（SOTA）性能，在SR和SPL指标上分别比最强的自我中心基线高出7.49%和18.15%。据作者所知，这是首个在仅使用自我中心输入的情况下，统一轨迹级规划和主动想象的零样本VLN方法。

Conclusion: DreamNav成功地通过整合自我中心感知校正、轨迹级规划和主动想象，显著提升了零样本视觉-语言导航的性能和效率，解决了现有方法的关键局限性，并为具身机器人提供了更强大的导航能力。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


### [303] [MEMBOT: Memory-Based Robot in Intermittent POMDP](https://arxiv.org/abs/2509.11225)
*Youzhi Liang,Eyan Noronha*

Main category: cs.RO

TL;DR: MEMBOT是一种模块化、基于记忆的架构，通过解耦信念推断和策略学习，解决了机器人控制任务中间歇性部分可观测性问题，并在观察数据丢失情况下表现出卓越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人系统常在部分或间歇性可观测条件下运行，传感器输入可能嘈杂、被遮挡或完全不可用。传统的强化学习方法假设完全状态可观测性，无法应对这些挑战。

Method: MEMBOT采用模块化、基于记忆的架构，将信念推断与策略学习解耦。训练分为两阶段：1) 离线多任务预训练阶段，通过重建损失学习一个鲁棒、与任务无关的潜在信念编码器；2) 使用行为克隆微调特定任务的策略。信念编码器使用状态空间模型（SSM）和LSTM集成时序观测和动作，推断即使在观测数据丢失时也能保持的潜在状态表示。

Result: MEMBOT在MetaWorld和Robomimic的10个机器人操作基准任务上，在不同观测数据丢失率下进行了评估。结果表明，MEMBOT始终优于无记忆和简单循环基线，在50%观测数据可用性下仍能保持高达80%的峰值性能。

Conclusion: 研究结果强调了显式信念建模在实现现实世界部分可观测机器人系统中的鲁棒、可迁移和数据高效策略方面的有效性。

Abstract: Robotic systems deployed in real-world environments often operate under
conditions of partial and often intermittent observability, where sensor inputs
may be noisy, occluded, or entirely unavailable due to failures or
environmental constraints. Traditional reinforcement learning (RL) approaches
that assume full state observability are ill-equipped for such challenges. In
this work, we introduce MEMBOT, a modular memory-based architecture designed to
address intermittent partial observability in robotic control tasks. MEMBOT
decouples belief inference from policy learning through a two-phase training
process: an offline multi-task learning pretraining stage that learns a robust
task-agnostic latent belief encoder using a reconstruction losses, followed by
fine-tuning of task-specific policies using behavior cloning. The belief
encoder, implemented as a state-space model (SSM) and a LSTM, integrates
temporal sequences of observations and actions to infer latent state
representations that persist even when observations are dropped. We train and
evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and
Robomimic under varying rates of observation dropout. Results show that MEMBOT
consistently outperforms both memoryless and naively recurrent baselines,
maintaining up to 80% of peak performance under 50% observation availability.
These findings highlight the effectiveness of explicit belief modeling in
achieving robust, transferable, and data-efficient policies for real-world
partially observable robotic systems.

</details>


### [304] [CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight](https://arxiv.org/abs/2509.11240)
*Yechen Zhang,Bin Gao,Gang Wang,Jian Sun,Zhuo Li*

Main category: cs.RO

TL;DR: CORB-Planner是一种基于强化学习的实时B样条轨迹规划框架，通过结合安全飞行走廊（SFC）和B样条生成，实现无人机在异构平台上高速自主飞行，克服了模型依赖和平台特异性传感的挑战。


<details>
  <summary>Details</summary>
Motivation: 强化学习在无人机部署中面临挑战，主要原因是对精确动态模型和平台特定传感的依赖，这阻碍了跨平台迁移。

Method: CORB-Planner将B样条轨迹生成与RL策略相结合，通过启发式搜索获得紧凑的安全飞行走廊（SFC）表示，生成连续的控制点。SFC以低维形式抽象障碍物信息。采用从易到难的渐进式训练流程和基于值的软分解批评Q（SDCQ）算法来学习策略。

Result: 在轻量级机载硬件上实现了实时规划，在密集、杂乱环境中支持高达8.2m/s的最大飞行速度，无需外部定位。兼容多种无人机配置（四旋翼、六旋翼），且只需适度的机载计算，展现了其通用性和鲁棒性。

Conclusion: CORB-Planner为异构平台上的高速自主无人机飞行提供了一个通用且鲁棒的实时强化学习轨迹规划解决方案，克服了传统RL部署的局限性。

Abstract: Reinforcement learning (RL) has shown promise in a large number of robotic
control tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)
remains challenging, mainly because of reliance on accurate dynamic models and
platform-specific sensing, which hinders cross-platform transfer. This paper
presents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a
real-time, RL-based trajectory planning framework for high-speed autonomous UAV
flight across heterogeneous platforms. The key idea is to combine B-spline
trajectory generation with the RL policy producing successive control points
with a compact safe flight corridor (SFC) representation obtained via heuristic
search. The SFC abstracts obstacle information in a low-dimensional form,
mitigating overfitting to platform-specific details and reducing sensitivity to
model inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard
progressive training pipeline in simulation. A value-based soft
decomposed-critic Q (SDCQ) algorithm is used to learn effective policies within
approximately ten minutes of training. Benchmarks in simulation and real-world
tests demonstrate real-time planning on lightweight onboard hardware and
support maximum flight speeds up to 8.2m/s in dense, cluttered environments
without external positioning. Compatibility with various UAV configurations
(quadrotors, hexarotors) and modest onboard compute underlines the generality
and robustness of CORB-Planner for practical deployment.

</details>


### [305] [Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP](https://arxiv.org/abs/2509.11270)
*Ziwen He,Zhigang Wang,Yanlong Peng,Pengxu Chang,Hong Yang,Ming Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于神经符号任务与运动规划（TAMP）的持续学习框架，旨在增强具身智能系统在动态环境中的适应性，以解决动力电池在非结构化拆卸场景中机器人感知鲁棒性不足的问题，实验证明其能显著提高任务成功率并减少感知误判。


<details>
  <summary>Details</summary>
Motivation: 新能源汽车产业的快速发展使得动力电池的回收利用成为循环经济的关键挑战。在当前非结构化的拆卸场景中，动态环境严重限制了机器人感知的鲁棒性，从而阻碍了工业应用中的自主拆卸。

Method: 提出了一种基于神经符号任务与运动规划（TAMP）的持续学习框架。该方法将多模态感知交叉验证机制整合到双向推理流中：前向工作流动态优化行动策略，后向学习流自主收集历史任务执行的有效数据以促进系统持续学习和自我优化。

Result: 实验结果表明，所提出的框架将动态拆卸场景中的任务成功率从81.68%提高到100%，同时将平均感知误判次数从3.389次减少到1.128次。

Conclusion: 这项研究为增强具身智能在复杂工业环境中的鲁棒性和适应性提供了一种新范式。

Abstract: With the rapid development of the new energy vehicle industry, the efficient
disassembly and recycling of power batteries have become a critical challenge
for the circular economy. In current unstructured disassembly scenarios, the
dynamic nature of the environment severely limits the robustness of robotic
perception, posing a significant barrier to autonomous disassembly in
industrial applications. This paper proposes a continual learning framework
based on Neuro-Symbolic task and motion planning (TAMP) to enhance the
adaptability of embodied intelligence systems in dynamic environments. Our
approach integrates a multimodal perception cross-validation mechanism into a
bidirectional reasoning flow: the forward working flow dynamically refines and
optimizes action strategies, while the backward learning flow autonomously
collects effective data from historical task executions to facilitate continual
system learning, enabling self-optimization. Experimental results show that the
proposed framework improves the task success rate in dynamic disassembly
scenarios from 81.68% to 100%, while reducing the average number of perception
misjudgments from 3.389 to 1.128. This research provides a new paradigm for
enhancing the robustness and adaptability of embodied intelligence in complex
industrial environments.

</details>


### [306] [Policy Learning for Social Robot-Led Physiotherapy](https://arxiv.org/abs/2509.11297)
*Carl Bettosi,Lynne Ballie,Susan Shenkin,Marta Romeo*

Main category: cs.RO

TL;DR: 该研究通过让33位专家医疗从业者扮演患者角色，构建了一个患者行为模型，克服了患者数据稀缺的问题。在此基础上，他们在模拟环境中训练了一个基于强化学习的策略，使社交机器人能够自适应地指导物理治疗练习，适应个体差异和表现波动。


<details>
  <summary>Details</summary>
Motivation: 社交机器人有望自主指导物理治疗，但需要先进的决策能力来适应患者需求。主要挑战是缺乏患者行为数据来开发强大的策略。

Method: 研究招募了33位专家医疗从业者作为患者代理，利用他们与机器人的互动来构建一个患者行为模型。该模型能够生成运动表现指标和感知劳累的主观评分。随后，研究在模拟环境中训练了一个基于强化学习的策略。

Result: 该强化学习策略能够根据个体劳累承受能力和波动表现调整运动指导。此外，它还适用于处于不同康复阶段、具有不同运动计划的患者。

Conclusion: 通过利用专家代理数据和强化学习，可以为社交机器人开发出适应性强的物理治疗指导策略，有效解决患者数据稀缺的挑战，并实现个性化指导。

Abstract: Social robots offer a promising solution for autonomously guiding patients
through physiotherapy exercise sessions, but effective deployment requires
advanced decision-making to adapt to patient needs. A key challenge is the
scarcity of patient behavior data for developing robust policies. To address
this, we engaged 33 expert healthcare practitioners as patient proxies, using
their interactions with our robot to inform a patient behavior model capable of
generating exercise performance metrics and subjective scores on perceived
exertion. We trained a reinforcement learning-based policy in simulation,
demonstrating that it can adapt exercise instructions to individual exertion
tolerances and fluctuating performance, while also being applicable to patients
at different recovery stages with varying exercise plans.

</details>


### [307] [Brain-Robot Interface for Exercise Mimicry](https://arxiv.org/abs/2509.11306)
*Carl Bettosi,Emilyann Nault,Lynne Baillie,Markus Garschall,Marta Romeo,Beatrix Wais-Zechmann,Nicole Binderlehner,Theodoros Georgio*

Main category: cs.RO

TL;DR: 该研究开发了一种脑机接口（BRI），使社交机器人能够实时模仿患者的运动，以建立融洽关系并提高长期参与度。初步研究表明，该系统成功实现了模仿，并得到了参与者（包括物理治疗师和偏瘫患者）的积极评价。


<details>
  <summary>Details</summary>
Motivation: 为了让社交机器人作为运动指导员保持长期参与度，建立融洽关系至关重要。运动模仿（模仿他人的身体动作）是促进融洽关系的有效工具，并广泛应用于康复锻炼中。因此，研究旨在探索机器人如何通过模仿来增强与患者的互动。

Method: 研究开发了一种新型脑机接口（BRI），该接口允许社交机器人指导员根据患者意图的脑电信号，实时模仿患者的运动。该系统通过一项探索性研究进行评估，共有14名参与者（3名物理治疗师和11名中风或其他损伤恢复期的偏瘫患者）。

Result: 研究发现，该系统在12次会话中成功展示了运动模仿，尽管准确性有所不同。参与者对机器人指导员持有积极看法，信任度和接受度很高，且这些积极看法未受脑机接口技术引入的影响。

Conclusion: 该研究表明，利用脑机接口技术实现机器人对患者运动的实时模仿，是建立融洽关系和促进长期康复锻炼参与的有效途径。尽管模仿准确性有待提高，但参与者对这种交互方式表现出高度的接受度和信任。

Abstract: For social robots to maintain long-term engagement as exercise instructors,
rapport-building is essential. Motor mimicry--imitating one's physical
actions--during social interaction has long been recognized as a powerful tool
for fostering rapport, and it is widely used in rehabilitation exercises where
patients mirror a physiotherapist or video demonstration. We developed a novel
Brain-Robot Interface (BRI) that allows a social robot instructor to mimic a
patient's exercise movements in real-time, using mental commands derived from
the patient's intention. The system was evaluated in an exploratory study with
14 participants (3 physiotherapists and 11 hemiparetic patients recovering from
stroke or other injuries). We found our system successfully demonstrated
exercise mimicry in 12 sessions; however, accuracy varied. Participants had
positive perceptions of the robot instructor, with high trust and acceptance
levels, which were not affected by the introduction of BRI technology.

</details>


### [308] [ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation](https://arxiv.org/abs/2509.11364)
*Sheng Liu,Zhe Li,Weiheng Wang,Han Sun,Heng Zhang,Hongpeng Chen,Yusen Qin,Arash Ajoudani,Yizhao Wang*

Main category: cs.RO

TL;DR: 本文提出一种结合视觉-语言模型（VLM）和“机器人想象力”的主动姿态估计与跟踪方法，通过动态检测和解决歧义，以及主动跟踪移动物体，显著提高了机器人操作的准确性。


<details>
  <summary>Details</summary>
Motivation: 零样本姿态估计方法常因视角模糊性而失效，而固定相机设置难以应对物体移动或自遮挡情况。因此，需要一种更鲁棒的六自由度（6-DoF）物体姿态估计和跟踪方法，以支持可靠的机器人操作。

Method: 该方法包括离线和运行时两个阶段。离线阶段，渲染CAD模型的密集视图，计算FoundationPose熵，并构建包含低（明确）和高（模糊）熵示例的几何感知提示。运行时：1) VLM查询实时图像的模糊性分数；2) 若检测到模糊性，通过渲染虚拟视图“想象”一系列候选相机姿态，结合VLM模糊概率和FoundationPose熵进行评分，然后移动相机到最佳下一视角（NBV）以消除歧义。此外，引入一个主动姿态跟踪模块，通过模仿学习训练扩散策略，生成保持物体可见性并最小化姿态模糊的相机轨迹。

Result: 在仿真和实际实验中，该方法显著优于经典的基线方法。

Conclusion: 所提出的主动姿态估计与跟踪流水线能够有效解决视角引起的姿态歧义，并主动跟踪移动物体，为机器人操作提供了更准确和可靠的姿态信息。

Abstract: Accurate 6-DoF object pose estimation and tracking are critical for reliable
robotic manipulation. However, zero-shot methods often fail under
viewpoint-induced ambiguities and fixed-camera setups struggle when objects
move or become self-occluded. To address these challenges, we propose an active
pose estimation pipeline that combines a Vision-Language Model (VLM) with
"robotic imagination" to dynamically detect and resolve ambiguities in real
time. In an offline stage, we render a dense set of views of the CAD model,
compute the FoundationPose entropy for each view, and construct a
geometric-aware prompt that includes low-entropy (unambiguous) and high-entropy
(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live
image for an ambiguity score; (2) if ambiguity is detected, imagines a discrete
set of candidate camera poses by rendering virtual views, scores each based on
a weighted combination of VLM ambiguity probability and FoundationPose entropy,
and then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated
pose estimation. Furthermore, since moving objects may leave the camera's field
of view, we introduce an active pose tracking module: a diffusion-policy
trained via imitation learning, which generates camera trajectories that
preserve object visibility and minimize pose ambiguity. Experiments in
simulation and real-world show that our approach significantly outperforms
classical baselines.

</details>


### [309] [Quantum deep reinforcement learning for humanoid robot navigation task](https://arxiv.org/abs/2509.11388)
*Romerik Lokossou,Birhanu Shimelis Girma,Ozan K. Tonguz,Ahmed Biyabani*

Main category: cs.RO

TL;DR: 本研究将量子深度强化学习（QDRL）应用于人形机器人，通过混合量子-经典设置，在复杂环境中实现了比经典方法更快的学习速度和更高的回报。


<details>
  <summary>Details</summary>
Motivation: 经典的强化学习方法在复杂、高维、随机的环境中因参数需求大而表现不佳。以往的量子强化学习模型主要集中在较小的环境（如轮式机器人、机械臂），未能解决人形机器人等具有巨大观测和动作空间的复杂任务。

Method: 引入了量子深度强化学习（QDRL）框架，采用参数化量子电路构建混合量子-经典设置。该方法直接导航高维状态空间，避免了传统的映射和规划步骤。通过将量子计算与深度强化学习相结合，在MuJoCo的Humanoid-v4和Walker2d-v4等环境中训练人形智能体，并对比了量子Soft Actor-Critic（SAC）与经典SAC的性能。

Result: 量子SAC在92%更少的步骤后，实现了比经典SAC高8%的平均回报（246.40对比228.36）。这表明量子计算在强化学习任务中具有显著的加速学习潜力。

Conclusion: 量子计算能够显著加速强化学习任务中的学习过程，为人形机器人在复杂导航任务中高效学习提供了新的途径和潜力。

Abstract: Classical reinforcement learning (RL) methods often struggle in complex,
high-dimensional environments because of their extensive parameter requirements
and challenges posed by stochastic, non-deterministic settings. This study
introduces quantum deep reinforcement learning (QDRL) to train humanoid agents
efficiently. While previous quantum RL models focused on smaller environments,
such as wheeled robots and robotic arms, our work pioneers the application of
QDRL to humanoid robotics, specifically in environments with substantial
observation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.
Using parameterized quantum circuits, we explored a hybrid quantum-classical
setup to directly navigate high-dimensional state spaces, bypassing traditional
mapping and planning. By integrating quantum computing with deep RL, we aim to
develop models that can efficiently learn complex navigation tasks in humanoid
robots. We evaluated the performance of the Soft Actor-Critic (SAC) in
classical RL against its quantum implementation. The results show that the
quantum SAC achieves an 8% higher average return (246.40) than the classical
SAC (228.36) after 92% fewer steps, highlighting the accelerated learning
potential of quantum computing in RL tasks.

</details>


### [310] [TRUST 2025: SCRITA and RTSS @ RO-MAN 2025](https://arxiv.org/abs/2509.11402)
*Alessandra Rossi,Patrick Holthaus,Gabriella Lakatos,Sílvia Moros,Ali Fallahi,Murat Kirtay,Marie Postma,Erhan Oztop*

Main category: cs.RO

TL;DR: TRUST研讨会是SCRITA和RTSS两个人机交互研讨会合作的成果，旨在从人类和机器人的双重视角推进信任研究。


<details>
  <summary>Details</summary>
Motivation: 为了汇集两个现有研讨会的互补目标，共同推进人机交互领域中关于信任的研究，并同时考虑人类和机器人的视角。

Method: 通过整合SCRITA（人机交互中的信任、接受度和社会线索）和RTSS（共生社会中的机器人信任）这两个既有研讨会，共同发起并成立了TRUST研讨会。

Result: 成功建立了TRUST研讨会，该研讨会致力于融合人类和机器人视角，全面探讨人机交互中的信任问题。

Conclusion: 此次合作将通过新成立的TRUST研讨会，有效汇聚多方力量，共同推动人机交互信任领域的深入研究与发展。

Abstract: The TRUST workshop is the result of a collaboration between two established
workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance
and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic
Societies). This joint initiative brings together the complementary goals of
these workshops to advance research on trust from both the human and robot
perspectives.
  Website: https://scrita.herts.ac.uk/2025/

</details>


### [311] [Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/abs/2509.11417)
*Shresth Grover,Akshay Gopalkrishnan,Bo Ai,Henrik I. Christensen,Hao Su,Xuanlin Li*

Main category: cs.RO

TL;DR: 本文提出一个框架，通过双编码器设计、基于字符串的动作分词器和协同训练策略，在将视觉-语言-动作（VLA）模型应用于机器人操作时，更好地保留预训练的视觉-语言模型（VLM）特征，从而提高模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由视觉-语言模型（VLM）微调而来的视觉-语言-动作（VLA）模型有望利用丰富的预训练表示来构建通用机器人。然而，直接在机器人数据上进行微调往往会破坏这些表示并限制泛化能力。

Method: 该方法引入了三个关键组件：(i) 双编码器设计，其中一个冻结的视觉编码器用于保留预训练特征，另一个可训练编码器用于任务适应；(ii) 基于字符串的动作分词器，将连续动作转换为与模型预训练领域对齐的字符序列；(iii) 协同训练策略，结合机器人演示数据与强调空间推理和功能属性的视觉-语言数据集。

Result: 在模拟和真实机器人上的评估表明，与基线相比，该方法提高了对视觉扰动的鲁棒性、对新指令和环境的泛化能力以及整体任务成功率。

Conclusion: 该框架成功地在将VLA模型应用于机器人操作时保留了预训练的VLM特征，同时进行了适应性调整，从而显著提升了机器人在各种任务和环境中的性能和泛化能力。

Abstract: Vision-language-action (VLA) models finetuned from vision-language models
(VLMs) hold the promise of leveraging rich pretrained representations to build
generalist robots across diverse tasks and environments. However, direct
fine-tuning on robot data often disrupts these representations and limits
generalization. We present a framework that better preserves pretrained
features while adapting them for robot manipulation. Our approach introduces
three components: (i) a dual-encoder design with one frozen vision encoder to
retain pretrained features and another trainable for task adaptation, (ii) a
string-based action tokenizer that casts continuous actions into character
sequences aligned with the model's pretraining domain, and (iii) a co-training
strategy that combines robot demonstrations with vision-language datasets
emphasizing spatial reasoning and affordances. Evaluations in simulation and on
real robots show that our method improves robustness to visual perturbations,
generalization to novel instructions and environments, and overall task success
compared to baselines.

</details>


### [312] [A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs](https://arxiv.org/abs/2509.11433)
*Pedro Portugal,Damian D. Venghaus,Diego Lopez*

Main category: cs.RO

TL;DR: 该论文提出了一种纯软件框架，用于在基于GRBL的桌面CNC路由器上实现分度旋转加工，无需硬件改造或固件修改，通过定制后处理器和浏览器界面，降低了多轴加工的技术和经济门槛。


<details>
  <summary>Details</summary>
Motivation: 经济实惠的桌面CNC路由器通常缺乏旋转轴，限制了旋转对称或多面零件的制造。现有解决方案往往需要硬件改造、替代控制器或商业CAM软件，增加了成本和复杂性。

Method: 开发了一个纯软件框架，针对基于GRBL的CNC机床。通过定制的后处理器将平面刀路转换为离散的旋转步进，并通过基于浏览器的界面执行这些步进。

Result: 该方法能够在不修改固件的情况下，使用标准、现成的机械部件实现实用的旋转轴加工。尽管不等同于连续四轴加工，但它有效降低了技术和财务障碍，扩展了多轴加工的可及性。

Conclusion: 该框架通过降低技术和经济门槛，扩大了教室、创客空间和小型工作室中多轴加工的应用范围，支持动手学习和快速原型制作。

Abstract: Affordable desktop CNC routers are common in education, prototyping, and
makerspaces, but most lack a rotary axis, limiting fabrication of rotationally
symmetric or multi-sided parts. Existing solutions often require hardware
retrofits, alternative controllers, or commercial CAM software, raising cost
and complexity. This work presents a software-only framework for indexed rotary
machining on GRBL-based CNCs. A custom post-processor converts planar toolpaths
into discrete rotary steps, executed through a browser-based interface. While
not equivalent to continuous 4-axis machining, the method enables practical
rotary-axis fabrication using only standard, off-the-shelf mechanics, without
firmware modification. By reducing technical and financial barriers, the
framework expands access to multi-axis machining in classrooms, makerspaces,
and small workshops, supporting hands-on learning and rapid prototyping.

</details>


### [313] [RAPTOR: A Foundation Policy for Quadrotor Control](https://arxiv.org/abs/2509.11481)
*Jonas Eschmann,Dario Albani,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 本文提出了RAPTOR，一种通过元模仿学习和隐藏层循环训练的四旋翼飞行器基础策略，实现了对多种未见过的真实四旋翼飞行器的零样本适应，仅需毫秒级即可完成。


<details>
  <summary>Details</summary>
Motivation: 人类在适应新环境时数据效率极高，而现代强化学习策略高度专业化，易过拟合，在模拟到现实（Sim2Real）差异等微小变化下就会失效并需要重新训练。因此，需要开发一种能高度自适应的机器人控制系统。

Method: 本文提出了RAPTOR方法，训练一个端到端的神经网络策略来控制多种四旋翼飞行器。该策略是一个包含2084个参数的微型三层网络，通过隐藏层中的循环实现上下文学习（In-Context Learning）。训练过程采用新颖的元模仿学习算法：首先，采样1000种不同的四旋翼飞行器，并为每种飞行器使用强化学习训练一个教师策略；随后，将这1000个教师策略蒸馏成一个单一的、自适应的学生策略。

Result: 该基础策略能够在毫秒内零样本适应10种不同的真实四旋翼飞行器（重量从32克到2.4公斤不等，电机、机架、螺旋桨和飞控类型各异），并且在轨迹跟踪、室内/室外、风扰动、推搡以及不同螺旋桨等多种条件下表现出强大的适应能力。

Conclusion: RAPTOR成功地为四旋翼飞行器控制训练了一个高度自适应的基础策略，该策略能够以极高的数据效率和零样本适应能力，快速应对各种未知的四旋翼平台，显著超越了传统强化学习策略的局限性。

Abstract: Humans are remarkably data-efficient when adapting to new unseen conditions,
like driving a new car. In contrast, modern robotic control systems, like
neural network policies trained using Reinforcement Learning (RL), are highly
specialized for single environments. Because of this overfitting, they are
known to break down even under small differences like the Simulation-to-Reality
(Sim2Real) gap and require system identification and retraining for even
minimal changes to the system. In this work, we present RAPTOR, a method for
training a highly adaptive foundation policy for quadrotor control. Our method
enables training a single, end-to-end neural-network policy to control a wide
variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg
that also differ in motor type (brushed vs. brushless), frame type (soft vs.
rigid), propeller type (2/3/4-blade), and flight controller
(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy
with only 2084 parameters is sufficient for zero-shot adaptation to a wide
variety of platforms. The adaptation through In-Context Learning is made
possible by using a recurrence in the hidden layer. The policy is trained
through a novel Meta-Imitation Learning algorithm, where we sample 1000
quadrotors and train a teacher policy for each of them using Reinforcement
Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive
student policy. We find that within milliseconds, the resulting foundation
policy adapts zero-shot to unseen quadrotors. We extensively test the
capabilities of the foundation policy under numerous conditions (trajectory
tracking, indoor/outdoor, wind disturbance, poking, different propellers).

</details>


### [314] [FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction](https://arxiv.org/abs/2509.11504)
*Yidan Lu,Yinzhao Dong,Jiahui Zhang,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: FR-Net是一个基于学习的框架，使四足机器人能够在复杂地形上从任意跌倒姿势中恢复，其核心是质量-接触预测网络，无需明确地形数据即可实现安全和鲁棒的恢复。


<details>
  <summary>Details</summary>
Motivation: 在复杂地形上，由于地形感知不完整和交互不确定性，传统控制器难以实现四足机器人的跌倒恢复。

Method: 该研究提出了FR-Net，一个基于学习的跌倒恢复框架。其核心是一个质量-接触预测网络，能从有限的传感器输入估计机器人的质量分布和接触状态。通过精心设计的奖励函数，即使在陡峭楼梯上也能确保安全恢复。该框架完全在模拟中使用特权学习进行训练，部署时无需明确的地形数据。

Result: FR-Net在模拟中展示了对不同四足平台的泛化能力，并在Go2机器人上通过10个挑战性场景的广泛真实世界实验验证了其性能。结果表明，明确的质量-接触预测是实现鲁棒跌倒恢复的关键。

Conclusion: 明确的质量-接触预测为可泛化的四足机器人技能的鲁棒跌倒恢复提供了一个有前途的方向。

Abstract: Fall recovery for legged robots remains challenging, particularly on complex
terrains where traditional controllers fail due to incomplete terrain
perception and uncertain interactions. We present \textbf{FR-Net}, a
learning-based framework that enables quadrupedal robots to recover from
arbitrary fall poses across diverse environments. Central to our approach is a
Mass-Contact Predictor network that estimates the robot's mass distribution and
contact states from limited sensory inputs, facilitating effective recovery
strategies. Our carefully designed reward functions ensure safe recovery even
on steep stairs without dangerous rolling motions common to existing methods.
Trained entirely in simulation using privileged learning, our framework guides
policy learning without requiring explicit terrain data during deployment. We
demonstrate the generalization capabilities of \textbf{FR-Net} across different
quadrupedal platforms in simulation and validate its performance through
extensive real-world experiments on the Go2 robot in 10 challenging scenarios.
Our results indicate that explicit mass-contact prediction is key to robust
fall recovery, offering a promising direction for generalizable quadrupedal
skills.

</details>


### [315] [Design and Development of a Remotely Wire-Driven Walking Robot](https://arxiv.org/abs/2509.11506)
*Takahiro Hattori,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 本文提出了一种名为“远程线缆驱动”的新机制，通过线缆远程驱动移动机器人，以实现在恶劣环境下无需电子设备的自主操作。


<details>
  <summary>Details</summary>
Motivation: 机器人在人类难以进入或恶劣的环境中扮演关键角色，但这些环境往往对电子元件构成风险。现有的解决方案（无电子设备自主机器人、液压驱动机器人、线缆驱动机械臂）存在局限性。具体而言，线缆驱动系统尚未用于移动机器人的远程驱动，而移动机器人比机械臂具有更大的覆盖范围和避障能力，线缆机制比液压系统具有更广泛的环境适用性。因此，需要一种新的线缆驱动机制来远程操作移动机器人。

Method: 本文提出了一种名为“远程线缆驱动”（Remote Wire Drive）的新机制，通过线缆实现移动机器人的远程驱动。该机制是一种解耦关节的串联连接，借鉴了线缆驱动机械臂中使用的机制，并针对动力传输进行了调整。研究中还开发了一款线缆驱动的四足机器人。

Result: 通过远程线缆驱动机制成功驱动了所开发的线缆驱动四足机器人，实验验证了该机制的可行性。

Conclusion: 远程线缆驱动机制能够通过线缆远程驱动移动机器人，为在恶劣或难以进入的环境中操作机器人提供了一种无需板载电子设备的有效解决方案。

Abstract: Operating in environments too harsh or inaccessible for humans is one of the
critical roles expected of robots. However, such environments often pose risks
to electronic components as well. To overcome this, various approaches have
been developed, including autonomous mobile robots without electronics,
hydraulic remotely actuated mobile robots, and long-reach robot arms driven by
wires. Among these, electronics-free autonomous robots cannot make complex
decisions, while hydraulically actuated mobile robots and wire-driven robot
arms are used in harsh environments such as nuclear power plants. Mobile robots
offer greater reach and obstacle avoidance than robot arms, and wire mechanisms
offer broader environmental applicability than hydraulics. However, wire-driven
systems have not been used for remote actuation of mobile robots. In this
study, we propose a novel mechanism called Remote Wire Drive that enables
remote actuation of mobile robots via wires. This mechanism is a series
connection of decoupled joints, a mechanism used in wire-driven robot arms,
adapted for power transmission. We experimentally validated its feasibility by
actuating a wire-driven quadruped robot, which we also developed in this study,
through Remote Wire Drive.

</details>


### [316] [PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments](https://arxiv.org/abs/2509.11516)
*Chengjin Wang,Zheng Yan,Yanmin Zhou,Runjie Shen,Zhipeng Wang,Bin Cheng,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种名为PaiP的实时交互式运动规划器，它利用多模态触觉感知来推断物体交互特征，并将其整合到操作成本图中，从而在视觉遮挡和受限空间中实现鲁棒的运动。


<details>
  <summary>Details</summary>
Motivation: 在箱柜场景中，堆叠物体和视觉遮挡导致自由空间受限，传统无碰撞轨迹规划方法在这种情况下常常失效，甚至可能因不可见物体导致灾难性碰撞。

Method: PaiP是一个实时闭环规划框架，利用多模态触觉感知自主推断交互界面的运动效应，从而获取物体交互特征。这些特征被整合到网格图中以生成操作成本图。在此基础上，通过同时优化路径成本和操作成本，扩展了基于采样的规划方法以实现交互式规划。

Result: 实验结果表明，PaiP能够在狭窄空间中实现鲁棒的运动。

Conclusion: PaiP通过结合触觉感知和操作成本优化，有效解决了视觉遮挡和受限空间中的机器人运动规划挑战，实现了在复杂环境中的鲁棒运动。

Abstract: Box/cabinet scenarios with stacked objects pose significant challenges for
robotic motion due to visual occlusions and constrained free space. Traditional
collision-free trajectory planning methods often fail when no collision-free
paths exist, and may even lead to catastrophic collisions caused by invisible
objects. To overcome these challenges, we propose an operational aware
interactive motion planner (PaiP) a real-time closed-loop planning framework
utilizing multimodal tactile perception. This framework autonomously infers
object interaction features by perceiving motion effects at interaction
interfaces. These interaction features are incorporated into grid maps to
generate operational cost maps. Building upon this representation, we extend
sampling-based planning methods to interactive planning by optimizing both path
cost and operational cost. Experimental results demonstrate that PaiP achieves
robust motion in narrow spaces.

</details>


### [317] [Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection](https://arxiv.org/abs/2509.11567)
*Eron Ristich,Jiahe Wang,Lei Zhang,Sultan Haidar Ali,Wanxin Jin,Yi Ren,Jiefeng Sun*

Main category: cs.RO

TL;DR: 本文提出了一种基于数据驱动的Koopman算子方法，实现了对模拟软连续体机器人的实时整体形状控制，解决了现有技术仅能实现末端控制的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的软连续体机器人虽然具有生物兼容性和柔顺运动能力（如章鱼臂），但由于其无限自由度带来的高计算成本，只能实现实时任务空间控制（即尖端控制），无法实现整体形状控制。

Method: 本文针对基于Kirchhoff杆模型的多段肌腱驱动软连续体机器人，提出了一种数据驱动的Koopman算子方法。通过对模拟机器人收集的数据进行逐段投影方案，识别出控制仿射Koopman模型，该模型比无投影方案的准确性高一个数量级。然后，利用这些学习到的Koopman模型，结合线性模型预测控制（MPC）来控制机器人实现各种复杂度的目标形状。

Result: 通过逐段投影方案识别出的Koopman模型比没有投影方案的模型准确性高一个数量级。该方法实现了计算高效的闭环控制，并证明了软机器人实时形状控制的可行性，能够使机器人达到一系列不同复杂度的目标形状。

Conclusion: 这项工作为软连续体机器人的实际形状控制铺平了道路，有望推动其在实际应用中的发展。

Abstract: Soft continuum robots can allow for biocompatible yet compliant motions, such
as the ability of octopus arms to swim, crawl, and manipulate objects. However,
current state-of-the-art continuum robots can only achieve real-time task-space
control (i.e., tip control) but not whole-shape control, mainly due to the high
computational cost from its infinite degrees of freedom. In this paper, we
present a data-driven Koopman operator-based approach for the shape control of
simulated multi-segment tendon-driven soft continuum robots with the Kirchhoff
rod model. Using data collected from these simulated soft robots, we conduct a
per-segment projection scheme on the state of the robots allowing for the
identification of control-affine Koopman models that are an order of magnitude
more accurate than without the projection scheme. Using these learned Koopman
models, we use a linear model predictive control (MPC) to control the robots to
a collection of target shapes of varying complexity. Our method realizes
computationally efficient closed-loop control, and demonstrates the feasibility
of real-time shape control for soft robots. We envision this work can pave the
way for practical shape control of soft continuum robots.

</details>


### [318] [Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems](https://arxiv.org/abs/2509.11688)
*Mostafa Eslami,Maryam Babazadeh*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的框架，将基于张量力学的无坐标、未简化多体动力学和运动学模型与数据辅助控制（DAC）架构相结合，以解决协作机器人系统中数据效率低下和泛化能力差的问题，从而实现更鲁棒和可解释的控制。


<details>
  <summary>Details</summary>
Motivation: 在复杂的协作工作空间中控制机器人系统面临巨大挑战，尤其是在从经验或模拟数据中学习时，鲁棒性和安全性难以保证。主要瓶颈在于依赖坐标相关的模型，这导致数据效率低下，因为它们无法在不同参考系之间泛化物理交互，迫使学习算法在每个新方向上重新发现基本物理原理，从而人为地增加了学习任务的复杂性。

Method: 该研究引入了一个新框架，将基于张量力学的无坐标、未简化多体动力学和运动学模型与数据辅助控制（DAC）架构相结合。它推导了一个非递归、闭式、增强矩阵形式的牛顿-欧拉模型，该模型针对基于张量的控制设计进行了优化。这种结构使得系统能够分解为结构确定、物理基础的部分和不确定、经验性、关注交互的部分，通过虚拟端口变量进行调解。最终，提出了一个完整的、端到端的张量不变建模、控制和学习流水线。结构确定部分的无坐标控制律通过李雅普诺夫分析证明了其稳定性和抽象命令接口。

Result: 该模型和闭环系统通过仿真得到了验证。这项工作为数据高效、帧不变的学习算法（如等变学习）提供了理想的输入，用于学习不确定的交互部分。

Conclusion: 该协同作用直接解决了数据效率低下的问题，提高了可解释性和可解释性，并为在交互环境中实现更鲁棒和更具泛化性的机器人控制铺平了道路。

Abstract: The control of robotic systems in complex, shared collaborative workspaces
presents significant challenges in achieving robust performance and safety when
learning from experienced or simulated data is employed in the pipeline. A
primary bottleneck is the reliance on coordinate-dependent models, which leads
to profound data inefficiency by failing to generalize physical interactions
across different frames of reference. This forces learning algorithms to
rediscover fundamental physical principles in every new orientation,
artificially inflating the complexity of the learning task. This paper
introduces a novel framework that synergizes a coordinate-free, unreduced
multibody dynamics and kinematics model based on tensor mechanics with a
Data-Assisted Control (DAC) architecture. A non-recursive, closed-form
Newton-Euler model in an augmented matrix form is derived that is optimized for
tensor-based control design. This structure enables a principled decomposition
of the system into a structurally certain, physically grounded part and an
uncertain, empirical, and interaction-focused part, mediated by a virtual port
variable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,
control, and learning is proposed. The coordinate-free control laws for the
structurally certain part provide a stable and abstract command interface,
proven via Lyapunov analysis. Eventually, the model and closed-loop system are
validated through simulations. This work provides a naturally ideal input for
data-efficient, frame-invariant learning algorithms, such as equivariant
learning, designed to learn the uncertain interaction. The synergy directly
addresses the data-inefficiency problem, increases explainability and
interpretability, and paves the way for more robust and generalizable robotic
control in interactive environments.

</details>


### [319] [GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning](https://arxiv.org/abs/2509.11594)
*Jizhuo Chen,Diwen Liu,Jiaming Wang,Harold Soh*

Main category: cs.RO

TL;DR: GBPP是一种基于学习的快速评分器，通过RGB-D快照选择机器人抓取的基础姿态。它采用两阶段课程学习（自动标注和模拟精炼），并使用PointNet++风格编码器。在模拟和真实环境中，它优于基线，选择更安全、可达性更高的姿态。


<details>
  <summary>Details</summary>
Motivation: 从单个RGB-D快照中快速选择机器人抓取的基础姿态，避免复杂的任务和运动优化，提高在线选择效率。

Method: 该方法采用两阶段课程学习：1) 使用简单的距离-可见性规则自动标注大型低成本数据集；2) 通过少量高保真模拟试验精炼模型，使其与真实抓取结果匹配。它使用PointNet++风格的点云编码器和多层感知机（MLP）对密集的候选姿态网格进行评分。

Result: 在模拟和真实移动机械臂上，GBPP优于仅基于接近度和几何的基线方法，能够选择更安全、更易于到达的站姿，并且在出错时能平稳降级。结果提供了一种数据高效、几何感知的底座放置实用方案。

Conclusion: 该研究提供了一种实用且数据高效的机器人底座放置方法，用于抓取任务。其核心在于先利用廉价的启发式方法进行广泛覆盖，然后通过有针对性的模拟进行校准和精炼。

Abstract: GBPP is a fast learning based scorer that selects a robot base pose for
grasping from a single RGB-D snapshot. The method uses a two stage curriculum:
(1) a simple distance-visibility rule auto-labels a large dataset at low cost;
and (2) a smaller set of high fidelity simulation trials refines the model to
match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP
scores dense grids of candidate poses, enabling rapid online selection without
full task-and-motion optimization. In simulation and on a real mobile
manipulator, GBPP outperforms proximity and geometry only baselines, choosing
safer and more reachable stances and degrading gracefully when wrong. The
results offer a practical recipe for data efficient, geometry aware base
placement: use inexpensive heuristics for coverage, then calibrate with
targeted simulation.

</details>


### [320] [VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.11930)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: 本文提出了可变规划范围扩散器（VHD）框架，通过学习每个实例的规划范围，解决了现有扩散规划器固定规划范围导致轨迹长度不匹配和性能脆弱的问题，从而提高了规划的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的规划器在训练和推理时都依赖于固定的、预设的规划范围。这种刚性常常导致轨迹长度不匹配（过短或过长），并且在几何或动态难度不同的实例中表现出脆弱性。

Method: VHD框架将规划范围视为一个可学习的变量，而非固定的超参数。它首先使用一个学习到的长度预测器模型，根据起点-目标对预测一个实例特定的规划范围。然后，这个预测的规划范围指导扩散规划器生成所需长度的轨迹。通过初始噪声整形和在随机裁剪的子轨迹上进行训练来控制轨迹长度，从而保持与现有扩散规划器的兼容性，无需改变架构。

Result: 实验结果表明，VHD在迷宫导航和机械臂控制基准测试中提高了成功率和路径效率，对规划范围不匹配和未见过的长度表现出更强的鲁棒性，同时保持了训练的简单性和离线性。

Conclusion: VHD框架通过引入学习到的可变规划范围，有效地解决了扩散规划器中固定规划范围的限制，显著提升了规划的鲁棒性和效率，且无需对现有架构进行修改，训练过程也保持简单和离线。

Abstract: Diffusion-based planners have gained significant recent attention for their
robustness and performance in long-horizon tasks. However, most existing
planners rely on a fixed, pre-specified horizon during both training and
inference. This rigidity often produces length-mismatch (trajectories that are
too short or too long) and brittle performance across instances with varying
geometric or dynamical difficulty. In this paper, we introduce the Variable
Horizon Diffuser (VHD) framework, which treats the horizon as a learned
variable rather than a fixed hyperparameter. Given a start-goal pair, we first
predict an instance-specific horizon using a learned Length Predictor model,
which guides a Diffusion Planner to generate a trajectory of the desired
length. Our design maintains compatibility with existing diffusion planners by
controlling trajectory length through initial noise shaping and training on
randomly cropped sub-trajectories, without requiring architectural changes.
Empirically, VHD improves success rates and path efficiency in maze-navigation
and robot-arm control benchmarks, showing greater robustness to horizon
mismatch and unseen lengths, while keeping training simple and offline-only.

</details>


### [321] [AssemMate: Graph-Based LLM for Robotic Assembly Assistance](https://arxiv.org/abs/2509.11617)
*Qi Zheng,Chaoran Zhang,Zijian Liang,EnTe Lin,Shubo Cui,Qinghongbing Xie,Zhaobo Xu,Long Zeng*

Main category: cs.RO

TL;DR: AssemMate提出了一种基于图的LLM机器人装配辅助系统，通过图谱知识表示和视觉增强，解决了现有方法因文本知识表示导致的实时性和精确推理不足的问题，显著提升了准确性、推理速度并缩短了上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型（LLM）的机器人装配辅助系统主要以自然语言文本形式表示知识。这种方法存在上下文过长和内容冗余的问题，导致难以满足机器人对实时性和精确推理的需求。

Method: AssemMate系统采用图（一种简洁准确的知识表示形式）作为输入，构建了一个基于图的LLM。该系统支持知识图谱问答（KGQA），以实现人机交互和装配任务规划。此外，它还支持感知堆叠场景并执行抓取。具体而言，一个自监督的图卷积网络（GCN）将知识图谱的实体和关系编码到潜在空间，并与LLM的表示对齐，使LLM能够理解图信息。同时，采用视觉增强策略来处理抓取中的堆叠场景。

Result: AssemMate在训练和评估中表现优于现有方法，实现了6.4%的准确率提升，推理速度快3倍，上下文长度缩短28倍，并对随机图表现出强大的泛化能力。此外，该方法在模拟和真实世界的机器人抓取实验中也展现出卓越的性能。

Conclusion: AssemMate通过利用图谱作为知识表示形式和视觉增强策略，成功解决了LLM在机器人装配辅助中实时性和精确推理的挑战，显著提升了系统性能和效率，为未来的人机协作装配提供了新的方向。

Abstract: Large Language Model (LLM)-based robotic assembly assistance has gained
significant research attention. It requires the injection of domain-specific
knowledge to guide the assembly process through natural language interaction
with humans. Despite some progress, existing methods represent knowledge in the
form of natural language text. Due to the long context and redundant content,
they struggle to meet the robots' requirements for real-time and precise
reasoning. In order to bridge this gap, we present AssemMate, which utilizes
the graph\textemdash a concise and accurate form of knowledge
representation\textemdash as input. This graph-based LLM enables knowledge
graph question answering (KGQA), supporting human-robot interaction and
assembly task planning for specific products. Beyond interactive QA, AssemMate
also supports sensing stacked scenes and executing grasping to assist with
assembly. Specifically, a self-supervised Graph Convolutional Network (GCN)
encodes knowledge graph entities and relations into a latent space and aligns
them with LLM's representation, enabling the LLM to understand graph
information. In addition, a vision-enhanced strategy is employed to address
stacked scenes in grasping. Through training and evaluation, AssemMate
outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster
inference, and 28 times shorter context length, while demonstrating strong
generalization ability on random graphs. And our approach further demonstrates
superiority through robotic grasping experiments in both simulated and
real-world settings. More details can be found on the project page:
https://github.com/cristina304/AssemMate.git

</details>


### [322] [Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios](https://arxiv.org/abs/2509.11621)
*Xiangtong Yao,Yirui Zhou,Yuan Meng,Yanwen Liu,Liangyu Dong,Zitao Zhang,Zhenshan Bing,Kai Huang,Fuchun Sun,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种适应-投影策略，使扩散策略能够在推理时零样本适应新型机械臂和动态任务设置，无需重新训练，从而解决了现有扩散策略泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散策略作为强大的视觉运动模型，在机器人操作中表现出色，但它们通常难以泛化到训练中未见的机械臂或末端执行器，并且难以在推理时适应新的任务要求。这通常需要为每种新的硬件或任务配置进行昂贵的数据重新收集和策略重新训练。

Method: 该方法首先使用基础机械臂的演示数据在SE(3)空间中训练一个扩散策略。在在线部署时，它将策略生成的轨迹投影，以满足新硬件和目标所施加的运动学和任务特定约束。此外，这种投影能够动态适应物理差异（例如，工具中心点偏移、夹爪宽度）和任务要求（例如，障碍物高度），确保鲁棒和成功的执行。

Result: 该方法在真实世界的抓取-放置、推动和倾倒任务中进行了验证，使用了包括Franka Panda和Kuka iiwa 14在内的多种机械臂，并配备了各种末端执行器（如柔性夹爪、Robotiq 2F/3F夹爪和各种3D打印设计）。结果表明，在这些跨机械臂场景中，该策略始终保持高成功率。

Conclusion: 所提出的适应-投影策略证明了其在推理时零样本适应新型机械臂和动态任务设置的有效性和实用性，无需任何重新训练，从而显著提高了扩散策略的泛化能力。

Abstract: Diffusion policies are powerful visuomotor models for robotic manipulation,
yet they often fail to generalize to manipulators or end-effectors unseen
during training and struggle to accommodate new task requirements at inference
time. Addressing this typically requires costly data recollection and policy
retraining for each new hardware or task configuration. To overcome this, we
introduce an adaptation-projection strategy that enables a diffusion policy to
perform zero-shot adaptation to novel manipulators and dynamic task settings,
entirely at inference time and without any retraining. Our method first trains
a diffusion policy in SE(3) space using demonstrations from a base manipulator.
During online deployment, it projects the policy's generated trajectories to
satisfy the kinematic and task-specific constraints imposed by the new hardware
and objectives. Moreover, this projection dynamically adapts to physical
differences (e.g., tool-center-point offsets, jaw widths) and task requirements
(e.g., obstacle heights), ensuring robust and successful execution. We validate
our approach on real-world pick-and-place, pushing, and pouring tasks across
multiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped
with a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F
grippers, and various 3D-printed designs. Our results demonstrate consistently
high success rates in these cross-manipulator scenarios, proving the
effectiveness and practicality of our adaptation-projection strategy. The code
will be released after peer review.

</details>


### [323] [ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering](https://arxiv.org/abs/2509.11663)
*Haisheng Wang,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了具身问题回答（EQsA）问题，引入了相应基准，并提出了一个处理该问题的系统ParaEQsA，该系统能并行、感知紧急程度地调度和回答多个异步问题，显著优于传统顺序基线。


<details>
  <summary>Details</summary>
Motivation: 传统的具身问答（EQA）通常只处理单个问题，但在实际部署中，具身智能体需要处理多个可能异步到达且具有不同紧急程度的问题，现有方法无法有效应对。

Method: 本文将此设定形式化为具身问题回答（EQsA），并提出了ParaEQsA框架，该框架利用共享的群组记忆模块减少冗余探索，并使用优先级规划模块动态调度问题。为评估EQsA，作者构建了PAEQs基准数据集（包含40个室内场景和200个问题，具有异步后续问题和紧急性标签），并提出了新的性能指标：直接回答率（DAR）和归一化紧急程度加权延迟（NUWL）。

Result: ParaEQsA系统在PAEQs基准上持续优于从近期EQA系统改编的强大顺序基线，同时显著减少了探索时间和延迟。经验评估还探究了优先级、紧急性建模、空间范围、奖励估计和依赖推理在框架中的相对贡献。

Conclusion: 研究结果表明，在真实的多问题工作负载下，感知紧急程度的并行调度是使具身智能体具有响应性和高效性的关键。

Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem,
introduces a corresponding benchmark, and proposes a system to tackle the
problem. Classical Embodied Question Answering (EQA) is typically formulated as
answering one single question by actively exploring a 3D environment. Real
deployments, however, often demand handling multiple questions that may arrive
asynchronously and carry different urgencies. We formalize this setting as
Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for
parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group
memory module shared among questions to reduce redundant exploration, and a
priority-planning module to dynamically schedule questions. To evaluate this
setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)
benchmark containing 40 indoor scenes and five questions per scene (200 in
total), featuring asynchronous follow-up questions and urgency labels. We
further propose metrics for EQsA performance: Direct Answer Rate (DAR), and
Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency
and responsiveness of this system. ParaEQsA consistently outperforms strong
sequential baselines adapted from recent EQA systems, while reducing
exploration and delay. Empirical evaluations investigate the relative
contributions of priority, urgency modeling, spatial scope, reward estimation,
and dependency reasoning within our framework. Together, these results
demonstrate that urgency-aware, parallel scheduling is key to making embodied
agents responsive and efficient under realistic, multi-question workloads.

</details>


### [324] [From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting](https://arxiv.org/abs/2509.11740)
*Davide Peron,Victor Nan Fernandez-Ayala,Lukas Segelmark*

Main category: cs.RO

TL;DR: 本文介绍了一种用于零售环境的端到端自主货架补货和理货机器人系统，该系统在实验室条件下实现了高成功率，但与人工操作相比，在性能和成本效益方面仍有差距。


<details>
  <summary>Details</summary>
Motivation: 在零售环境中，尤其是在超市里，由于动态的人机交互、有限的空间和多样化的产品几何形状，自主补货面临诸多挑战，这促使了本研究的进行。

Method: 该研究引入了一个高效的端到端机器人系统，集成了市售硬件和可扩展的算法架构。它利用行为树（Behavior Trees, BTs）进行任务规划，使用经过微调的视觉模型进行物体检测，并采用基于ArUco标记的两步模型预测控制（Model Predictive Control, MPC）框架进行精确的货架导航。

Result: 在模拟真实超市条件的实验室实验中，该系统在超过700次补货操作中，拾取和放置成功率超过98%。然而，对比基准测试表明，当前自主系统的性能和成本效益仍低于人工操作。

Conclusion: 该系统在实验室环境中表现出可靠的性能，但与人工操作相比，其性能和成本效益仍有待提高。研究强调了在广泛商业部署之前，仍需在关键领域进行改进并量化所需进展。

Abstract: Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.

</details>


### [325] [Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap](https://arxiv.org/abs/2509.11742)
*Jianping Li,Kaisong Zhu,Zhongyuan Liu,Rui Jin,Xinhang Xu,Pengfei Wan,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了一种自适应激光雷达扫描框架，结合OpenStreetMap (OSM)先验信息和局部可观测性预测，以提高复杂环境中的定位鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的激光雷达-OSM定位方法受限于OSM地图的不完整或过时，且激光雷达有限的视场角以及常速扫描方式导致在特征稀疏区域浪费资源并降低定位精度。

Method: 该研究提出了“OSM引导的自适应激光雷达扫描”框架。它将全局OSM先验与局部可观测性预测相结合，通过增强不确定性感知模型预测控制，引入一个OSM感知项。该方法根据场景相关的可观测性和OSM特征的空间分布，自适应地分配扫描工作量。该方法在ROS中实现，并结合了电动激光雷达里程计后端。

Result: 在模拟和真实世界实验（包括校园道路、室内走廊和城市环境）中，与常速扫描基线相比，该方法显著降低了轨迹误差，同时保持了扫描的完整性。

Conclusion: 研究结果表明，将开源地图（如OSM）与自适应激光雷达扫描相结合，能够实现在复杂环境中鲁棒且高效的定位。

Abstract: LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as
OSM provides lightweight global priors such as building footprints. These
priors enhance global consistency for robot navigation, but OSM is often
incomplete or outdated, limiting its reliability in real-world deployment.
Meanwhile, LiDAR itself suffers from a limited field of view (FoV), where
motorized rotation is commonly used to achieve panoramic coverage. Existing
motorized LiDAR systems, however, typically employ constant-speed scanning that
disregards both scene structure and map priors, leading to wasted effort in
feature-sparse regions and degraded localization accuracy. To address these
challenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework
that integrates global priors with local observability prediction to improve
localization robustness. Specifically, we augment uncertainty-aware model
predictive control with an OSM-aware term that adaptively allocates scanning
effort according to both scene-dependent observability and the spatial
distribution of OSM features. The method is implemented in ROS with a motorized
LiDAR odometry backend and evaluated in both simulation and real-world
experiments. Results on campus roads, indoor corridors, and urban environments
demonstrate significant reductions in trajectory error compared to
constant-speed baselines, while maintaining scan completeness. These findings
highlight the potential of coupling open-source maps with adaptive LiDAR
scanning to achieve robust and efficient localization in complex environments.

</details>


### [326] [Igniting VLMs toward the Embodied Space](https://arxiv.org/abs/2509.11766)
*Andy Zhai,Brae Liu,Bruno Fang,Chalse Cai,Ellie Ma,Ethan Yin,Hao Wang,Hugo Zhou,James Wang,Lights Shi,Lucy Liang,Make Wang,Qian Wang,Roy Gan,Ryan Yu,Shalfun Li,Starrick Liu,Sylas Chen,Vincent Chen,Zach Xu*

Main category: cs.RO

TL;DR: WALL-OSS是一个端到端具身基础模型，通过大规模多模态预训练，解决了现有视觉-语言模型（VLMs）在具身理解方面的局限性，实现了具身感知的视觉-语言理解、强大的语言-动作关联和鲁棒的操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLMs）在空间和具身理解方面存在局限性，将其迁移到具身领域时，模态、预训练分布和训练目标之间存在根本性不匹配，导致动作理解和生成成为实现通用人工智能（AGI）的关键瓶颈。

Method: WALL-OSS采用紧密耦合的架构和多策略训练课程，实现了统一的跨层思维链（Unified Cross-Level CoT），将指令推理、子目标分解和细粒度动作合成无缝地统一在一个可微分框架中。它利用大规模多模态预训练。

Result: WALL-OSS在复杂的长周期操作中取得了高成功率，展示了强大的指令遵循能力、复杂的理解和推理能力，并超越了强大的基线模型。

Conclusion: WALL-OSS为将视觉-语言模型（VLMs）发展为具身基础模型提供了一条可靠且可扩展的路径，有效解决了现有模型的局限性，实现了强大的具身人工智能能力。

Abstract: While foundation models show remarkable progress in language and vision,
existing vision-language models (VLMs) still have limited spatial and
embodiment understanding. Transferring VLMs to embodied domains reveals
fundamental mismatches between modalities, pretraining distributions, and
training objectives, leaving action comprehension and generation as a central
bottleneck on the path to AGI.
  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages
large-scale multimodal pretraining to achieve (1) embodiment-aware
vision-language understanding, (2) strong language-action association, and (3)
robust manipulation capability.
  Our approach employs a tightly coupled architecture and multi-strategies
training curriculum that enables Unified Cross-Level CoT-seamlessly unifying
instruction reasoning, subgoal decomposition, and fine-grained action synthesis
within a single differentiable framework.
  Our results show that WALL-OSS attains high success on complex long-horizon
manipulations, demonstrates strong instruction-following capabilities, complex
understanding and reasoning, and outperforms strong baselines, thereby
providing a reliable and scalable path from VLMs to embodied foundation models.

</details>


### [327] [Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations](https://arxiv.org/abs/2509.11783)
*Shiqi Gong,Sebastian Zudaire,Chi Zhang,Zhen Li*

Main category: cs.RO

TL;DR: 该研究提出了一种增强现实（AR）辅助的机器人遥操作系统，旨在实现直观、非接触式的机器人示教，显著提升任务性能和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统的工业机器人编程复杂且耗时，而示教编程（PbD）虽然更易用，但缺乏直观的机器人控制和示教数据收集界面。因此，需要一种更高效、直观的机器人示教方法。

Method: 本文提出了一种结合AR控制和空间点云渲染的AR增强机器人遥操作系统。该系统允许操作员远程控制机器人，无需进入工作空间或使用传统示教器。该方法在ABB IRB 1200工业机器人和GoFa 5协作机器人上进行了验证。通过用户研究评估了实时环境感知（有点云渲染和无点云渲染）对任务完成准确性、效率和用户信心的影响。

Result: 结果表明，增强的环境感知（有点云渲染）使任务性能显著提高28%，并通过系统可用性量表（SUS）得分提高12%来提升用户体验。

Conclusion: 这项工作推动了工业环境中直观机器人遥操作、AR界面设计、环境感知和遥操作安全机制的发展，用于示教数据收集。所收集的示教数据可作为机器学习应用的宝贵训练数据。

Abstract: Traditional industrial robot programming is often complex and time-consuming,
typically requiring weeks or even months of effort from expert programmers.
Although Programming by Demonstration (PbD) offers a more accessible
alternative, intuitive interfaces for robot control and demonstration
collection remain challenging. To address this, we propose an Augmented Reality
(AR)-enhanced robot teleoperation system that integrates AR-based control with
spatial point cloud rendering, enabling intuitive, contact-free demonstrations.
This approach allows operators to control robots remotely without entering the
workspace or using conventional tools like the teach pendant. The proposed
system is generally applicable and has been demonstrated on ABB robot
platforms, specifically validated with the IRB 1200 industrial robot and the
GoFa 5 collaborative robot. A user study evaluates the impact of real-time
environmental perception, specifically with and without point cloud rendering,
on task completion accuracy, efficiency, and user confidence. Results indicate
that enhanced perception significantly improves task performance by 28% and
enhances user experience, as reflected by a 12% increase in the System
Usability Scale (SUS) score. This work contributes to the advancement of
intuitive robot teleoperation, AR interface design, environmental perception,
and teleoperation safety mechanisms in industrial settings for demonstration
collection. The collected demonstrations may serve as valuable training data
for machine learning applications.

</details>


### [328] [Synthetic vs. Real Training Data for Visual Navigation](https://arxiv.org/abs/2509.11791)
*Lauri Suomela,Sasanka Kuruppu Arachchige,German F. Torres,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 本文证明了通过利用预训练视觉表示的导航策略架构，模拟器训练的策略可以弥合模拟到现实的外观差距，并在真实机器人上实现与现实世界训练策略相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管存在众所周知的模拟到现实（sim-to-real）差距，即模拟器训练的策略在真实世界中表现显著下降，但研究旨在探索模拟器训练的策略能否匹配甚至超越真实世界训练的策略性能。

Method: 核心方法是一种导航策略架构，它利用预训练的视觉表示来弥合模拟到现实的外观差距，并能在机器人硬件上实时运行。

Result: 在轮式移动机器人上，所提出的模拟器训练策略在导航成功率方面比真实世界训练的版本高出31%，比现有最先进的方法高出50%。该策略在无人机上部署也验证了其泛化能力。结果强调了多样化图像编码器预训练对模拟到现实泛化的重要性，并指出在策略学习（on-policy learning）是模拟训练相对于真实数据训练的关键优势。

Conclusion: 多样化的图像编码器预训练对于模拟到现实的泛化至关重要，而策略学习是模拟训练相对于使用真实数据训练的一个关键优势。

Abstract: This paper investigates how the performance of visual navigation policies
trained in simulation compares to policies trained with real-world data.
Performance degradation of simulator-trained policies is often significant when
they are evaluated in the real world. However, despite this well-known
sim-to-real gap, we demonstrate that simulator-trained policies can match the
performance of their real-world-trained counterparts.
  Central to our approach is a navigation policy architecture that bridges the
sim-to-real appearance gap by leveraging pretrained visual representations and
runs real-time on robot hardware. Evaluations on a wheeled mobile robot show
that the proposed policy, when trained in simulation, outperforms its
real-world-trained version by 31% and the prior state-of-the-art methods by 50%
in navigation success rate. Policy generalization is verified by deploying the
same model onboard a drone.
  Our results highlight the importance of diverse image encoder pretraining for
sim-to-real generalization, and identify on-policy learning as a key advantage
of simulated training over training with real data.

</details>


### [329] [UniPilot: Enabling GPS-Denied Autonomy Across Embodiments](https://arxiv.org/abs/2509.11793)
*Mihir Kulkarni,Mihir Dharmadhikari,Nikhil Khedekar,Morten Nissov,Mohit Singh,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了UniPilot，一个紧凑的软硬件一体化自主载荷，旨在使各种机器人平台能在GPS拒绝环境中实现自主操作。


<details>
  <summary>Details</summary>
Motivation: 在GPS信号受限的环境中，单一模态的感知方法往往会失效，因此需要一个集成多模态传感、鲁棒且可跨平台部署的自主系统。

Method: UniPilot集成了多模态传感套件，包括激光雷达、雷达、视觉和惯性传感器。它运行一套完整的自主软件，涵盖多模态感知、探索与检查路径规划以及基于学习的导航策略。该载荷在一个单元中提供了鲁棒的定位、建图、规划、安全和控制能力。

Result: 通过在多种不同环境和各类机器人平台上进行大量实验，验证了UniPilot载荷所实现的建图、规划和安全导航能力的有效性。

Conclusion: UniPilot是一个能够部署在多种机器人平台上的、提供鲁棒定位、建图、规划和控制能力的紧凑型硬件-软件自主载荷，能在GPS拒绝环境中实现自主操作。

Abstract: This paper presents UniPilot, a compact hardware-software autonomy payload
that can be integrated across diverse robot embodiments to enable autonomous
operation in GPS-denied environments. The system integrates a multi-modal
sensing suite including LiDAR, radar, vision, and inertial sensing for robust
operation in conditions where uni-modal approaches may fail. UniPilot runs a
complete autonomy software comprising multi-modal perception, exploration and
inspection path planning, and learning-based navigation policies. The payload
provides robust localization, mapping, planning, and safety and control
capabilities in a single unit that can be deployed across a wide range of
platforms. A large number of experiments are conducted across diverse
environments and on a variety of robot platforms to validate the mapping,
planning, and safe navigation capabilities enabled by the payload.

</details>


### [330] [TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning](https://arxiv.org/abs/2509.11839)
*Jiacheng Liu,Pengxiang Ding,Qihang Zhou,Yuxuan Wu,Da Huang,Zimian Peng,Wei Xiao,Weinan Zhang,Lixin Yang,Cewu Lu,Donglin Wang*

Main category: cs.RO

TL;DR: 模仿学习在长周期、高精度任务中易受累积误差影响。残差策略学习虽有潜力但缺乏全局理解。本文提出KORR框架，利用Koopman算子理论在潜在空间中进行全局动力学建模，指导残差策略更新，从而在长周期机器人任务中显著提升性能、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 模仿学习(IL)在长周期任务和高精度控制中因累积误差而表现不佳。现有的残差策略学习方法主要侧重局部修正，缺乏对状态演化的全局理解，限制了其鲁棒性和对未见场景的泛化能力。

Method: 本文提出将全局动力学建模融入残差策略更新中。具体而言，利用Koopman算子理论在学习到的潜在空间中施加线性时不变结构，以实现可靠的状态转换和改进的外推能力。引入了KORR（Koopman-guided Online Residual Refinement）框架，该框架根据Koopman预测的潜在状态来调整残差修正，从而实现全局信息导向的稳定动作修正。

Result: 在受各种扰动影响的长周期、精细机器人家具组装任务中对KORR进行了评估。结果表明，与强大的基线相比，KORR在性能、鲁棒性和泛化能力方面均取得了持续提升。

Conclusion: KORR是一个简单而有效的框架，能够为残差策略学习提供全局信息导向和稳定的动作修正。研究结果进一步强调了基于Koopman的建模在连接现代学习方法与经典控制理论方面的巨大潜力。

Abstract: Imitation learning (IL) enables efficient skill acquisition from
demonstrations but often struggles with long-horizon tasks and high-precision
control due to compounding errors. Residual policy learning offers a promising,
model-agnostic solution by refining a base policy through closed-loop
corrections. However, existing approaches primarily focus on local corrections
to the base policy, lacking a global understanding of state evolution, which
limits robustness and generalization to unseen scenarios. To address this, we
propose incorporating global dynamics modeling to guide residual policy
updates. Specifically, we leverage Koopman operator theory to impose linear
time-invariant structure in a learned latent space, enabling reliable state
transitions and improved extrapolation for long-horizon prediction and unseen
environments. We introduce KORR (Koopman-guided Online Residual Refinement), a
simple yet effective framework that conditions residual corrections on
Koopman-predicted latent states, enabling globally informed and stable action
refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture
assembly tasks under various perturbations. Results demonstrate consistent
gains in performance, robustness, and generalization over strong baselines. Our
findings further highlight the potential of Koopman-based modeling to bridge
modern learning methods with classical control theory. For more details, please
refer to https://jiachengliu3.github.io/TrajBooster.

</details>


### [331] [Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer](https://arxiv.org/abs/2509.11865)
*Travis Davies,Yiqi Huang,Yunxin Liu,Xiang Chen,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: 本文提出Tenma，一种轻量级扩散-Transformer策略，用于双臂机器人控制。它通过跨肢体归一化器、联合状态-时间编码器和扩散动作解码器，在异构多模态数据上实现了高成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer策略和扩散模型在机器人操作方面取得了进展，但在轻量级、跨肢体学习设置中结合这些技术仍然具有挑战性。

Method: Tenma集成了多视角RGB、本体感受和语言，其核心设计包括：1) 一个跨肢体归一化器，将不同的状态/动作空间映射到共享的潜在空间；2) 一个联合状态-时间编码器，用于时间对齐的观察学习并提升推理速度；3) 一个扩散动作解码器，优化以提高训练稳定性和学习能力。

Result: 在基准测试和同等计算条件下，Tenma在同分布任务中平均成功率达88.95%，远超基线策略（最佳同分布平均成功率18.12%），并在物体和场景变化下保持了强大的性能。尽管使用中等规模数据，Tenma仍实现了鲁棒的操作和泛化。

Conclusion: Tenma展示了多模态和跨肢体学习策略在进一步增强基于Transformer的模仿学习策略能力方面的巨大潜力，并实现了鲁棒的操作和泛化。

Abstract: Scaling Transformer policies and diffusion models has advanced robotic
manipulation, yet combining these techniques in lightweight, cross-embodiment
learning settings remains challenging. We study design choices that most affect
stability and performance for diffusion-transformer policies trained on
heterogeneous, multimodal robot data, and introduce Tenma, a lightweight
diffusion-transformer for bi-manual arm control. Tenma integrates multiview
RGB, proprioception, and language via a cross-embodiment normalizer that maps
disparate state/action spaces into a shared latent space; a Joint State-Time
encoder for temporally aligned observation learning with inference speed
boosts; and a diffusion action decoder optimized for training stability and
learning capacity. Across benchmarks and under matched compute, Tenma achieves
an average success rate of 88.95% in-distribution and maintains strong
performance under object and scene shifts, substantially exceeding baseline
policies whose best in-distribution average is 18.12%. Despite using moderate
data scale, Tenma delivers robust manipulation and generalization, indicating
the great potential for multimodal and cross-embodiment learning strategies for
further augmenting the capacity of transformer-based imitation learning
policies.

</details>


### [332] [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](https://arxiv.org/abs/2509.11964)
*Junyoung Kim,Minsik Jeon,Jihong Min,Kiho Kwak,Junwon Seo*

Main category: cs.RO

TL;DR: 本文提出了一种不确定性感知的语义建图框架，通过整合证据深度学习和贝叶斯核推断，并利用高斯表示和几何对齐核，实现了在复杂户外环境中鲁棒、高效的语义建图。


<details>
  <summary>Details</summary>
Motivation: 现有的语义建图方法在具有挑战性的户外环境中存在多种不确定性来源，这些不确定性严重降低了建图性能，即使贝叶斯核推断（BKI）能够处理稀疏传感器数据的地图推断不连续性，也未能完全解决这些问题。

Method: 该方法通过证据深度学习（Evidential Deep Learning）估计语义预测中的不确定性，并将其整合到贝叶斯核推断（BKI）中进行鲁棒的语义推断。此外，它将噪声观测聚合为连贯的高斯表示以减轻不可靠点的影响，并采用适应复杂场景结构的几何对齐核。这些高斯基元有效地融合了局部几何和语义信息。

Result: 在多样化的越野和城市户外环境中进行的综合评估表明，该框架在建图质量、不确定性校准、表示灵活性和鲁棒性方面持续改进，同时保持了实时效率。

Conclusion: 所提出的不确定性感知语义建图框架能够有效处理多种不确定性来源，在复杂户外场景中实现鲁棒、不确定性感知的建图。

Abstract: Semantic mapping aims to construct a 3D semantic representation of the
environment, providing essential knowledge for robots operating in complex
outdoor settings. While Bayesian Kernel Inference (BKI) addresses
discontinuities of map inference from sparse sensor data, existing semantic
mapping methods suffer from various sources of uncertainties in challenging
outdoor environments. To address these issues, we propose an uncertainty-aware
semantic mapping framework that handles multiple sources of uncertainties,
which significantly degrade mapping performance. Our method estimates
uncertainties in semantic predictions using Evidential Deep Learning and
incorporates them into BKI for robust semantic inference. It further aggregates
noisy observations into coherent Gaussian representations to mitigate the
impact of unreliable points, while employing geometry-aligned kernels that
adapt to complex scene structures. These Gaussian primitives effectively fuse
local geometric and semantic information, enabling robust, uncertainty-aware
mapping in complex outdoor scenarios. Comprehensive evaluation across diverse
off-road and urban outdoor environments demonstrates consistent improvements in
mapping quality, uncertainty calibration, representational flexibility, and
robustness, while maintaining real-time efficiency.

</details>


### [333] [Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study](https://arxiv.org/abs/2509.11971)
*James C. Ward,Alex Bott,Connor York,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 本文提出了一种基于机器学习的对抗模型，用于模拟多机器人巡逻系统中的敌意攻击，以评估其鲁棒性并指导未来的巡逻策略设计。


<details>
  <summary>Details</summary>
Motivation: 研究物理自主系统对攻击的鲁棒性，并指导设计具有漏洞意识的系统。具体而言，多机器人巡逻系统需要一个能模拟真实潜在对手的模型来评估其性能和设计。

Method: 开发了一个基于机器学习的对抗模型。该模型通过观察机器人巡逻行为，试图在有限时间内未被检测地进入安全环境。

Result: 该新模型优于现有基线，提供了更严格的测试。研究人员还评估了其在多种领先的去中心化多机器人巡逻策略下的表现。

Conclusion: 所提出的对抗模型为多机器人巡逻系统提供了一个更真实、更严格的评估工具，能够为未来的巡逻策略设计提供有价值的见解。

Abstract: Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

</details>


### [334] [Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees](https://arxiv.org/abs/2509.12008)
*Yuqing Song,Cesare Tonola,Stefano Savazzi,Sanaz Kianoush,Nicola Pedrocchi,Stephan Sigg*

Main category: cs.RO

TL;DR: 该论文提出了一种基于毫米波雷达的手势控制机械臂系统，解决了传统视觉系统在隐私和环境方面的局限性，并实现了手势识别与机器人控制的实时无缝集成。


<details>
  <summary>Details</summary>
Motivation: 随着机器人普及，对直观高效人机交互的需求增加。手势识别是一种直观、非接触的控制方式。现有基于摄像头的视觉系统存在隐私问题，且在复杂或光线不足环境下表现不佳。毫米波雷达则能保护隐私、不受遮挡和光线影响，并提供丰富的空间数据。

Method: 使用毫米波雷达进行非接触式运动识别。系统能识别九种手势，并将其精确映射为实时命令。该方法将手势识别和机器人控制整合到一个实时管道中，并进行了案例研究以验证系统实用性、性能和可靠性。

Result: 系统能够精确识别九种手势并将其映射为实时命令。案例研究证明了该系统在基于手势的机器人操作中的实用性、性能和可靠性。与以往将手势识别和机器人控制分开处理的工作不同，本系统实现了两者的实时统一。

Conclusion: 该系统通过毫米波雷达实现了可靠、非接触式的手势控制机械臂，提供了一种无缝的人机交互方式。它克服了传统方法的局限性，将手势识别和机器人控制统一到一个实时管道中，提升了人机交互的效率和隐私性。

Abstract: As robots become increasingly prevalent in both homes and industrial
settings, the demand for intuitive and efficient human-machine interaction
continues to rise. Gesture recognition offers an intuitive control method that
does not require physical contact with devices and can be implemented using
various sensing technologies. Wireless solutions are particularly flexible and
minimally invasive. While camera-based vision systems are commonly used, they
often raise privacy concerns and can struggle in complex or poorly lit
environments. In contrast, radar sensing preserves privacy, is robust to
occlusions and lighting, and provides rich spatial data such as distance,
relative velocity, and angle. We present a gesture-controlled robotic arm using
mm-wave radar for reliable, contactless motion recognition. Nine gestures are
recognized and mapped to real-time commands with precision. Case studies are
conducted to demonstrate the system practicality, performance and reliability
for gesture-based robotic manipulation. Unlike prior work that treats gesture
recognition and robotic control separately, our system unifies both into a
real-time pipeline for seamless, contactless human-robot interaction.

</details>


### [335] [Embodied Navigation Foundation Model](https://arxiv.org/abs/2509.12129)
*Jiazhao Zhang,Anqi Li,Yunpeng Qi,Minghan Li,Jiahang Liu,Shaoan Wang,Haoran Liu,Gengze Zhou,Yuze Wu,Xingxing Li,Yuxin Fan,Wenjun Li,Zhibo Chen,Fei Gao,Qi Wu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 本文提出NavFoM，一个跨实体和跨任务的导航基础模型，通过统一架构、标识符令牌和动态采样策略，在多种机器人和导航任务中实现了卓越的泛化能力和最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（VLMs）在通用视觉-语言任务上表现出色，但它们在具身导航中的泛化能力仍受限于狭窄的任务设置和特定实体的架构，无法有效应对不同机器人和任务的复杂性。

Method: NavFoM模型在一个包含四足机器人、无人机、轮式机器人和车辆等多种实体，以及视觉-语言导航、物体搜索、目标跟踪和自动驾驶等多种任务的八百万导航样本数据集上进行训练。它采用统一架构处理多模态导航输入，并通过“标识符令牌”嵌入不同摄像机配置和任务时间上下文信息。为适应实际部署，NavFoM还采用动态调整采样策略来控制观测令牌，以适应有限的令牌长度预算。

Result: NavFoM在公共基准测试中，无需任务特定微调即可在多个导航任务和实体上达到最先进或极具竞争力的性能。额外的真实世界实验进一步证实了其强大的泛化能力和实际适用性。

Conclusion: NavFoM成功地展示了其在具身导航领域中跨实体和跨任务的强大泛化能力和实用性，为未来具身AI的发展奠定了基础。

Abstract: Navigation is a fundamental capability in embodied AI, representing the
intelligence required to perceive and interact within physical environments
following language instructions. Despite significant progress in large
Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance
on general vision-language tasks, their generalization ability in embodied
navigation remains largely confined to narrow task settings and
embodiment-specific architectures. In this work, we introduce a
cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained
on eight million navigation samples that encompass quadrupeds, drones, wheeled
robots, and vehicles, and spanning diverse tasks such as vision-and-language
navigation, object searching, target tracking, and autonomous driving. NavFoM
employs a unified architecture that processes multimodal navigation inputs from
varying camera configurations and navigation horizons. To accommodate diverse
camera setups and temporal horizons, NavFoM incorporates identifier tokens that
embed camera view information of embodiments and the temporal context of tasks.
Furthermore, to meet the demands of real-world deployment, NavFoM controls all
observation tokens using a dynamically adjusted sampling strategy under a
limited token length budget. Extensive evaluations on public benchmarks
demonstrate that our model achieves state-of-the-art or highly competitive
performance across multiple navigation tasks and embodiments without requiring
task-specific fine-tuning. Additional real-world experiments further confirm
the strong generalization capability and practical applicability of our
approach.

</details>


### [336] [Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks](https://arxiv.org/abs/2509.12151)
*Zongyao Yi,Joachim Hertzberg,Martin Atzmueller*

Main category: cs.RO

TL;DR: 本文提出了一种可学习的物理模拟器，通过扩展GNN模型，显著提高了机器人末端执行器在接触密集操作中运动和力矩预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 在接触密集型机器人操作中，需要准确预测机器人末端执行器的运动和力矩，以支持控制和状态估计任务。现有的物理模拟器可能无法满足这一精度要求。

Method: 该研究通过引入新颖的节点和边类型，扩展了最先进的基于GNN的模拟器（FIGNet），使其能够进行动作条件预测。该模型被用于模拟和真实世界的实验。

Result: 在仿真中，使用该模型的MPC代理在“孔中插销”任务中表现与使用真实动力学模型的控制器相当。在真实世界实验中，该模型在运动预测精度上比基线物理模拟器提高了50%，在力矩预测精度上提高了3倍。

Conclusion: 所提出的可学习物理模拟器在接触密集型机器人操作中，显著提高了运动和力矩预测的准确性和精度，在仿真和真实世界环境中均表现出色，有望用于控制和状态估计任务。

Abstract: We present a learnable physics simulator that provides accurate motion and
force-torque prediction of robot end effectors in contact-rich manipulation.
The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)
with novel node and edge types, enabling action-conditional predictions for
control and state estimation tasks. In simulation, the MPC agent using our
model matches the performance of the same controller with the ground truth
dynamics model in a challenging peg-in-hole task, while in the real-world
experiment, our model achieves a 50% improvement in motion prediction accuracy
and 3$\times$ increase in force-torque prediction precision over the baseline
physics simulator. Source code and data are publicly available.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [337] [Generalizable Pareto-Optimal Offloading with Reinforcement Learning in Mobile Edge Computing](https://arxiv.org/abs/2509.10474)
*Ning Yang,Junrui Wen,Meng Zhang,Ming Tang*

Main category: eess.SY

TL;DR: 针对移动边缘计算（MEC）中未知偏好下的多目标卸载问题，本文提出了一种基于离散软演员-评论家（Discrete-SAC）的通用多目标（深度）强化学习（GMORL）框架，显著提升了帕累托前沿的超体积。


<details>
  <summary>Details</summary>
Motivation: 下一代移动网络应用对MEC的延迟和能效等性能指标有很高要求。然而，传统的单目标调度方案无法直接应用于实际系统，因为不同目标之间的偏好（即权重）通常是未知或难以预先设定的。

Method: 本文将MEC中的多边缘卸载问题建模为多目标问题，旨在最小化预期的长期能耗和延迟之和，同时考虑未知偏好。为解决未知偏好和MEC系统多样性挑战，提出了一种基于Discrete-SAC的GMORL任务卸载框架。该框架使用单一策略模型来高效调度任务，适应不同的偏好和异构MEC系统。具体方法包括：引入基于直方图的状态编码方法构建多边缘MEC系统的特征、设计复杂的奖励函数以准确计算延迟和能耗的效用、以及提出一种新颖的神经网络架构以提高泛化能力。

Result: 仿真结果表明，本文提出的GMORL方案与基准方法相比，将帕累托前沿的超体积提高了高达121.0%。

Conclusion: 所提出的GMORL框架能够有效解决MEC中具有未知偏好的多目标卸载问题，通过泛化能力和帕累托最优性的显著提升，展示了其在实际应用中的巨大潜力。

Abstract: Mobile edge computing (MEC) is essential for next-generation mobile network
applications that prioritize various performance metrics, including delays and
energy efficiency. However, conventional single-objective scheduling solutions
cannot be directly applied to practical systems in which the preferences (i.e.,
the weights of different objectives) are often unknown or challenging to
specify in advance. In this study, we formulate a multi-objective offloading
problem for MEC with multiple edges to minimize the sum of expected long-term
energy consumption and delay while considering unknown preferences. To address
the challenge of unknown preferences and the potentially diverse MEC systems,
we propose a generalizable multi-objective (deep) reinforcement learning
(GMORL)-based tasks offloading framework, which employs the Discrete Soft
Actor-Critic (Discrete-SAC) method. Our method uses a single policy model to
efficiently schedule tasks based on varying preferences and adapt to
heterogeneous MEC systems with different CPU frequencies and server quantities.
Under the proposed framework, we introduce a histogram-based state encoding
method for constructing features for multiple edges in MEC systems, a
sophisticated reward function for accurately computing the utilities of delay
and energy consumption, and a novel neural network architecture for improving
generalization. Simulation results demonstrate that our proposed GMORL scheme
enhances the hypervolume of the Pareto front by up to $121.0\%$ compared to
benchmarks. Our code are avavilable at
https://github.com/gracefulning/Generalizable-Pareto-Optimal-Offloading-with-Reinforcement-Learning-in-Mobile-Edge-Computing

</details>


### [338] [Analysis and Design of Spare Strategy for Large-Scale Satellite Constellation Using Direct Insertion under (r,q) Policy](https://arxiv.org/abs/2509.10585)
*Seungyeop Han,Zachary Grieser,Shoji Yoshikawa,Takumi Noro,Takumi Suda,Koki Ho*

Main category: eess.SY

TL;DR: 本文提出了一种基于马尔可夫链的方法，用于分析和优化大型卫星星座的备件管理策略，以最小化运营成本。


<details>
  <summary>Details</summary>
Motivation: 分析和优化大型卫星星座中的备件管理策略，特别是直接部署策略，以应对卫星故障和运载火箭交付时间的不确定性。

Method: 采用基于马尔可夫链的方法，将备件补给建模为周期性审查的再订货点/订货量策略。通过马尔可夫表示捕捉卫星故障和运载火箭交付时间的随机行为。在此框架基础上，构建并求解了一个以最小化运营成本为目标的优化问题。

Result: 所提出的方法提供了一个高效且准确的框架。通过一个真实的巨型星座案例研究，证明了该方法的有效性。

Conclusion: 该基于马尔可夫链的方法能够有效分析和优化大型卫星星座的备件管理策略，从而显著降低运营成本。

Abstract: This paper introduces a Markov chain-based approach for the analysis and
optimization of spare-management policies in large-scale satellite
constellations. Focusing on the direct strategy, we model spare replenishment
as a periodic-review reorder-point/order-quantity policy, where spares are
deployed directly to constellation planes. The stochastic behavior of satellite
failures and launch vehicle lead times is captured through Markov
representations of both failure and replenishment dynamics. Based on this
efficient and accurate framework, we construct and solve an optimization
problem aimed at minimizing operational costs. The effectiveness of the
proposed method is demonstrated through a case study using a real-world
mega-constellation.

</details>


### [339] [Complexity Reduction for TSO-DSO Coordination: Flexibility Aggregation vs. Distributed Optimization](https://arxiv.org/abs/2509.10595)
*Maísa Beraldo Bandeira,Alexander Engelmann,Timm Faulwasser*

Main category: eess.SY

TL;DR: 本文比较了两种基于优化的电力系统复杂性降低方法：通过近似动态规划（ADP）的灵活性聚合和通过交替方向乘子法（ADMM）的分布式优化，分析了它们在通信和解决方案质量方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 电力系统中柔性设备和分布式能源的增加，使得输配电系统的协调变得日益复杂，因此需要有效的优化方法来降低这种复杂性。

Method: 研究讨论并比较了两种方法：1. 通过近似动态规划（ADP）实现的灵活性聚合。2. 通过交替方向乘子法（ADMM）实现的分布式优化。通过结合不同Matpower基准的案例研究对两种方法进行了比较。

Result: 灵活性聚合（ADP）能实现接近最优的解决方案，且通信量最小，但其性能依赖于近似的质量。相比之下，ADMM能获得更接近集中式解决方案的结果，但需要显著更多的通信步骤。

Conclusion: 在降低电力系统复杂性的优化方法中，灵活性聚合（ADP）和分布式优化（ADMM）各有优劣，前者在通信量和近似质量之间权衡，后者则在解决方案质量和通信量之间权衡。

Abstract: The increasing number of flexible devices and distributed energy resources in
power grids renders the coordination of transmission and distribution systems
increasingly complex. In this paper, we discuss and compare two different
approaches to optimization-based complexity reduction: Flexibility aggregation
via Approximate Dynamic Programming (ADP) and distributed optimization via the
Alternating Direction Method of Multipliers (ADMM). Flexibility aggregation
achieves near-optimal solutions with minimal communication. However, its
performance depends on the quality of the approximation used. In contrast, ADMM
attains results closer to the centralized solution but requires significantly
more communication steps. We draw upon a case study combining different
matpower benchmarks to compare both methods.

</details>


### [340] [Optimal Path Planning for Wheel Loader Automation Enabled by Efficient Soil-Tool Interaction Modeling](https://arxiv.org/abs/2509.10642)
*Armin Abdolmohammadi,Navid Mojahed,Bahram Ravani,Shima Nazari*

Main category: eess.SY

TL;DR: 本文提出了一种基于基本土方方程（FEE）的轮式装载机自动化框架，通过在线预测挖掘力并优化铲斗轨迹，实现了显著的能源节约。


<details>
  <summary>Details</summary>
Motivation: 轮式装载机的土方作业需要大量动力，并产生高昂的运营成本，因此需要提高效率和降低能耗。

Method: 该研究基于基本土方方程（FEE）进行土-工具相互作用建模。利用Sobol全局敏感性分析指导的降阶多步参数估计方法，实现了准确的在线挖掘力预测。随后，通过建立最优控制问题，利用上一个挖掘周期识别的土壤参数计算出节能的铲斗轨迹。

Result: 高保真模拟证实了力预测的准确性，并表明与标准路径相比，该框架可节省15-40%的能源。总计算时间与单个挖掘周期相当。

Conclusion: 所提出的自动化框架能够实现实时、能源优化的轮式装载机自动化，具有显著的实际应用潜力，可有效降低土方作业的能耗。

Abstract: Earthmoving operations with wheel loaders require substantial power and incur
high operational costs. This work presents an efficient automation framework
based on the Fundamental Earthmoving Equation (FEE) for soil-tool interaction
modeling. A reduced-order multi-step parameter estimation method guided by
Sobol's global sensitivity analysis is deployed for accurate, online excavation
force prediction. An optimal control problem is then formulated to compute
energy-efficient bucket trajectories using soil parameters identified in the
previous digging cycle. High-fidelity simulations in Algoryx Dynamics confirm
accurate force prediction and demonstrate 15-40% energy savings compared to
standard paths. The total computation time is comparable to a single digging
cycle, highlighting the framework's potential for real-time, energy-optimized
wheel loader automation.

</details>


### [341] [A Linear Programming Framework for Optimal Event-Triggered LQG Control](https://arxiv.org/abs/2509.10671)
*Zahra Hashemi,Dipankar Maity*

Main category: eess.SY

TL;DR: 本文提出一种在网络控制系统中，当数据传输产生开销时，用于传感器到控制器通信的智能调度方法。该方法将调度问题重构为一个混合整数线性规划（MILP），并嵌入模型预测控制（MPC）框架中，证明其性能优于任何确定性策略。


<details>
  <summary>Details</summary>
Motivation: 在标准线性二次高斯（LQG）设置中，当数据传输产生开销时，确定传感器数据传输的最佳时间在计算和分析上都具有挑战性。

Method: 通过重构问题并引入辅助二进制变量，将调度问题转化为计算高效的混合整数线性规划（MILP）。将此方法嵌入模型预测控制（MPC）框架中以实现动态适应。

Result: 该MILP公式不仅简化了分析，还揭示了结构性见解并提供了清晰的决策标准。所产生的调度器性能至少与任何确定性策略（如周期性策略）一样好。仿真结果表明，该方法始终优于传统的周期性调度。

Conclusion: 通过将调度问题转化为MILP并结合MPC框架，本文提出了一种有效的智能调度方法，能够动态适应并提供优于确定性策略的性能，有效解决了网络控制系统中数据传输开销下的通信调度问题。

Abstract: This letter explores intelligent scheduling of sensor-to-controller
communication in networked control systems, particularly when data transmission
incurs a cost. While the optimal controller in a standard linear quadratic
Gaussian (LQG) setup can be computed analytically, determining the optimal
times to transmit sensor data remains computationally and analytically
challenging. We show that, through reformulation and the introduction of
auxiliary binary variables, the scheduling problem can be cast as a
computationally efficient mixed-integer linear program (MILP). This formulation
not only simplifies the analysis but also reveals structural insights and
provides clear decision criteria at each step. Embedding the approach within a
model predictive control (MPC) framework enables dynamic adaptation, and we
prove that the resulting scheduler performs at least as well as any
deterministic strategy (e.g., periodic strategy). Simulation results further
demonstrate that our method consistently outperforms traditional periodic
scheduling.

</details>


### [342] [Combinatorial Control Barrier Functions: Nested Boolean and p-choose-r Compositions of Safety Constraints](https://arxiv.org/abs/2509.10716)
*Pio Ong,Haejoon Lee,Tamas G. Molnar,Dimitra Panagou,Aaron D. Ames*

Main category: eess.SY

TL;DR: 本文提出了一种组合控制障碍函数（CBFs）和矩阵控制障碍函数（MCBFs）的新框架——组合CBFs，用于处理逻辑和组合操作，尤其是或（OR）和更复杂的安全规范，解决了现有方法在非平滑性和组合爆炸方面的问题，并提供了可扩展性和理论保证。


<details>
  <summary>Details</summary>
Motivation: 标准的CBF公式能自然地实现合取（AND）组合，但析取（OR）和更通用的逻辑结构会引入非平滑性，并可能导致逻辑组合数量的组合式爆炸，这限制了CBF在复杂安全规范中的应用。

Method: 本文引入了组合CBFs（combinatorial CBFs）框架，用于处理“p选r”安全规范及其嵌套组合。该框架使用原始数量的基本约束，以可扩展的方式确保精确安全集的安全性。

Result: 所提出的框架能够以可扩展的方式确保精确安全集的安全性，并且仅使用原始数量的基本约束，避免了组合爆炸。此外，论文还建立了在这些组合下安全性的理论保证，并通过多智能体系统中的巡逻问题进行了应用演示。

Conclusion: 组合CBFs框架为处理复杂的逻辑和组合安全规范提供了一种可扩展且理论上可靠的方法，克服了传统CBF在处理非合取逻辑组合时的局限性。

Abstract: This paper investigates the problem of composing multiple control barrier
functions (CBFs) -- and matrix control barrier functions (MCBFs) -- through
logical and combinatorial operations. Standard CBF formulations naturally
enable conjunctive (AND) combinations, but disjunctive (OR) and more general
logical structures introduce nonsmoothness and possibly a combinatorial blow-up
in the number of logical combinations. We introduce the framework of
combinatorial CBFs that addresses p-choose-r safety specifications and their
nested composition. The proposed framework ensures safety for the exact safe
set in a scalable way, using the original number of primitive constraints. We
establish theoretical guarantees on safety under these compositions, and we
demonstrate their use on a patrolling problem in a multi-agent system.

</details>


### [343] [Multi-sectoral Impacts of H2 and Synthetic Fuels Adoption for Heavy-duty Transportation Decarbonization](https://arxiv.org/abs/2509.10734)
*Youssef Shaker,Jun Wen Law,Audun Botterud,Dharik Mallapragada*

Main category: eess.SY

TL;DR: 本研究评估了在深度脱碳背景下，重型车辆（HDV）通过氢能或合成液体燃料实现脱碳对西欧能源系统的影响。结果表明，在缺乏二氧化碳储存的情况下，替代化石液体燃料至关重要；氢能HDV可降低成本和化石燃料需求，但可能增加天然气消耗；而合成燃料则会增加直接空气捕集投资和系统总成本。


<details>
  <summary>Details</summary>
Motivation: 区域经济深度脱碳政策强调电力部门脱碳和终端用途电气化。对于难以电气化的部门（如重型车辆），利用电力生产氢气来替代化石燃料的兴趣日益增长。鉴于重型车辆在轻型车辆电气化后交通排放中占比显著且不断增长，有必要评估其脱碳路径对整体能源系统的影响。

Method: 本研究软链接了两种建模方法：(a) 一个自下而上的交通需求模型，用于生成不同最终能源需求情景；(b) 一个多部门容量扩张模型，在技术和政策约束下共同优化电力、氢气和二氧化碳供应链，以满足外生的最终能源需求。通过以2040年深度脱碳约束下的西欧为案例研究，量化了在有无二氧化碳封存情景下，重型车辆部门采用不同水平的氢能和合成燃料对能源系统的影响。

Result: 在缺乏二氧化碳储存的情况下，重型车辆中替代液体化石燃料对于满足电力、氢气和交通部门的深度脱碳约束至关重要。此外，使用氢能重型车辆可以降低脱碳成本和化石液体燃料需求，但可能增加天然气消耗。虽然氢能重型车辆的采用减少了直接空气捕集（DAC）的需求，但合成燃料的采用增加了DAC投资和系统总成本。

Conclusion: 本研究强调了交通脱碳路径之间的权衡，并强调了在脱碳研究中进行多部门考量的重要性。

Abstract: Policies focused on deep decarbonization of regional economies emphasize
electricity sector decarbonization alongside electrification of end-uses. There
is growing interest in utilizing hydrogen (H2) produced via electricity to
displace fossil fuels in difficult-to-electrify sectors. One such case is
heavy-duty vehicles (HDV), which represent a substantial and growing share of
transport emissions as light-duty vehicles electrify. Here, we assess the bulk
energy system impact of decarbonizing the HDV segment via either H2, or drop-in
synthetic liquid fuels produced from H2 and CO2. Our analysis soft-links two
modeling approaches: (a) a bottom-up transport demand model producing a variety
of final energy demand scenarios for the same service demand and (b) a
multi-sectoral capacity expansion model that co-optimizes power, H2 and CO2
supply chains under technological and policy constraints to meet exogenous
final energy demands. Through a case study of Western Europe in 2040 under deep
decarbonization constraints, we quantify the energy system implications of
different levels of H2 and synthetic fuels adoption in the HDV sector under
scenarios with and without CO2 sequestration. In the absence of CO2 storage,
substitution of liquid fossil fuels in HDVs is essential to meet the deep
decarbonization constraint across the modeled power, H2 and transport sectors.
Additionally, utilizing H2 HDVs reduces decarbonization costs and fossil
liquids demand, but could increase natural gas consumption. While H2 HDV
adoption reduces the need for direct air capture (DAC), synthetic fuel adoption
increases DAC investments and total system costs. The study highlights the
trade-offs across transport decarbonization pathways, and underscores the
importance of multi-sectoral consideration in decarbonization studies.

</details>


### [344] [Experimental Validation of Decentralized Affine Transformation](https://arxiv.org/abs/2509.10791)
*Garegin Mazmanyan,Hossein Rastgoftar*

Main category: eess.SY

TL;DR: 本文通过迷你四旋翼飞行器团队，实验验证了多智能体系统中去中心化仿射变换（AT）的有效性，使其能安全通过障碍物丰富的环境。


<details>
  <summary>Details</summary>
Motivation: 使智能体团队能够在受限、障碍物丰富的环境中安全导航，同时允许智能体间距离进行剧烈变化。

Method: 将二维仿射变换（AT）公式化为去中心化的领导者-跟随者问题。三架领导者四旋翼飞行器位于三角形顶点，跟随者则位于三角形内部。领导者知晓AT规定的期望轨迹，而跟随者通过局部通信（由初始空间配置确定的固定通信权重）推断其轨迹。通过迷你四旋翼飞行器团队进行实验验证。

Result: 实验结果验证了去中心化AT的渐近收敛性，并展示了其安全引导多智能体团队通过障碍物密集环境的能力。

Conclusion: 去中心化仿射变换（AT）能够有效且安全地引导多智能体团队在复杂环境中进行导航，并已通过实验得到验证。

Abstract: This paper presents an experimental validation of decentralized affine
transformation (AT) in multi-agent systems using teams of mini-quadcopters. The
AT framework enables an agent team to safely navigate constrained,
obstacle-rich environments while allowing aggressive changes in inter-agent
distances, which are formally characterized through the decomposition of the AT
transformation matrix. Without loss of generality, we focus on two-dimensional
AT, formulated as a decentralized leader-follower problem. In this formulation,
three leader quadcopters are positioned at the vertices of a triangle, while
all follower quadcopters remain within the triangle. The leaders know the
desired trajectories prescribed by the AT, whereas the followers do not.
Instead, the followers infer their trajectories through local communication
governed by fixed communication weights determined by the initial spatial
configuration of the team. Experimental results validate the asymptotic
convergence of decentralized AT and demonstrate its capability to safely guide
multi-agent teams through obstacle-laden environments.

</details>


### [345] [Control Synthesis for Multiple Reach-Avoid Tasks via Hamilton-Jacobi Reachability Analysis](https://arxiv.org/abs/2509.10896)
*Yu Chen,Shaoyuan Li,Xiang Yin*

Main category: eess.SY

TL;DR: 本文研究了在多重到达-规避（MRA）任务下，连续时间、时变非线性系统带扰动的控制综合问题，并通过求解一系列Hamilton-Jacobi变分不等式来定义值函数，从而合成控制律。


<details>
  <summary>Details</summary>
Motivation: MRA任务要求系统按特定顺序到达一系列目标区域，并在每次到达之间满足状态约束，这比标准的到达-规避任务更具挑战性，因为它需要在规划过程中考虑未来任务的可行性。

Method: 通过求解一系列由Hamilton-Jacobi变分不等式刻画的时变到达-规避问题，定义了一系列值函数。控制律的合成通过确保值函数随时间非负来实现。此外，将线性时序逻辑（LTL）任务控制综合问题转换为MRA任务控制综合问题。

Result: 计算得到的最终值函数的超水平集正是MRA任务的可行集。证明了控制律可以通过确保值函数的非负性有效合成。展示了LTL任务可以通过适当定义MRA任务的目标和状态约束集来转化为MRA任务。

Conclusion: 所提出的方法能够有效解决带扰动的时变非线性系统在MRA任务下的控制综合问题，并可通过转换处理LTL任务。通过机器人规划问题的四个案例研究验证了其有效性。

Abstract: We investigate the control synthesis problem for continuous-time time-varying
nonlinear systems with disturbance under a class of multiple reach-avoid (MRA)
tasks. Specifically, the MRA task requires the system to reach a series of
target regions in a specified order while satisfying state constraints between
each pair of target arrivals. This problem is more challenging than standard
reach-avoid tasks, as it requires considering the feasibility of future
reach-avoid tasks during the planning process. To solve this problem, we define
a series of value functions by solving a cascade of time-varying reach-avoid
problems characterized by Hamilton-Jacobi variational inequalities. We prove
that the super-level set of the final value function computed is exactly the
feasible set of the MRA task. Additionally, we demonstrate that the control law
can be effectively synthesized by ensuring the non-negativeness of the value
functions over time. We also show that the Linear temporal logic task control
synthesis problems can be converted to a collection of MRA task control
synthesis problems by properly defining each target and state constraint set of
MRA tasks. The effectiveness of the proposed approach is illustrated through
four case studies on robot planning problems under time-varying nonlinear
systems with disturbance.

</details>


### [346] [Uncertainty Quantification on State-Based Conflict Detection and Resolution Algorithms](https://arxiv.org/abs/2509.10899)
*Muhammad Fazlur Rahman,Joost Ellerbroek,Jacco Hoekstra*

Main category: eess.SY

TL;DR: 本研究探讨了导航不确定性如何影响无人机在U-space中的冲突检测与解决（CD&R）。研究发现，不确定性使检测变为概率性，并比较了两种解决算法MVP和VO。结果表明，MVP在不确定性下更鲁棒，因为它明确最大化最近会合点（CPA）距离，从而提高了冲突预防率和会合后距离。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解导航不确定性对无人机在U-space中冲突检测与解决（CD&R）的影响。

Method: 研究方法包括：将位置和速度误差建模为零均值高斯噪声（与ADS-L精度一致）；通过蒙特卡洛和分析近似方法将误差传播到冲突指标中；比较了两种解决算法：改进电压势（MVP）和速度障碍（VO）；使用BlueSky仿真来验证这些影响。

Result: 主要结果如下：在不确定性下，基于状态的检测变为概率性，其概率取决于不确定性水平和遭遇几何形状，当标称入侵时间等于前瞻时间时，检测概率会降至50%以下。MVP算法在不确定性下比VO算法更鲁棒，因为它明确最大化了最近会合点（CPA）距离，从而在机动过程中保持向外推力并避免反转行为。BlueSky仿真证实，MVP在冲突场景中实现了更高的入侵预防率和更大的解决后失误距离，其优势在低相对速度下最为显著。

Conclusion: 研究结论强调了最大化最近会合点（CPA）距离作为冲突解决策略的重要性。此外，前瞻范围和保护区可以进行调整，以达到所需的目标安全水平。

Abstract: This study investigates how navigation uncertainty affects conflict detection
and resolution (CD&R) for uncrewed aircraft in U-space. Position and velocity
errors are modelled as zero-mean Gaussian noise consistent with ADS-L accuracy,
and propagated through conflict metrics using Monte Carlo and analytical
approximations. Under uncertainty, state-based detection becomes probabilistic.
The probability of detection depends on both the level of uncertainty and the
encounter geometry, and falls below 50% when the nominal intrusion time equals
the look-ahead. Operationally, detection is re-evaluated over time as the
encounter develops, yielding multiple observations with varying probabilities.
Two resolution algorithms are compared: Modified Voltage Potential (MVP) and
Velocity Obstacle (VO). MVP proves more robust under uncertainty because it
explicitly maximises distance at the closest point of approach (CPA). By
maximising CPA distance, MVP maintains an outward push and avoids reversal
behaviour during the manoeuvre, whereas VO performance degrades at low relative
speeds and shallow angles. BlueSky simulations confirm these effects: MVP
achieves higher intrusion-prevention rates and larger post-resolution miss
distances across conflict scenarios, with its advantage most pronounced at low
relative velocity. The findings highlight the importance of maximising CPA
distance as a conflict resolution strategy. Moreover, the look-ahead horizon
and protected zone can be tuned to achieve a desired target level of safety.

</details>


### [347] [A Highly Compact Direct-Injection Power-Flow Controller and Line-Voltage Regulator with Shared Magnetics and Partial-Power Conversion for Full-Power Control](https://arxiv.org/abs/2509.10955)
*Davood Keshavarzi,Alexander Koehler,Stefan M. Goetz*

Main category: eess.SY

TL;DR: 本文提出了一种紧凑、高电流密度的电力潮流控制器，用于解决低压电网中分布式能源集成带来的挑战，其通过结合SiC/Si晶体管和部分功率拓扑实现，并可在馈线之间进行潮流控制。


<details>
  <summary>Details</summary>
Motivation: 光伏单元、电动汽车充电器、热泵和储能系统日益集成到低压电网中，导致电压越限、稳定性丧失、线路过载以及电力管理问题。传统通用电力潮流控制器（UPFC）体积庞大、响应慢且成本高昂，无法有效应对这些挑战。

Method: 该研究提出了一种基于系统性结合碳化硅（SiC）和硅（Si）晶体管的紧凑型电力潮流控制器。它采用围绕多有源桥构建的严格部分功率拓扑结构，将有源前端变换器作为并联级，通过多有源桥变换器与低压串联注入模块双向连接。该拓扑能以小功率控制高电流，多有源桥充当多输入输出的电力路由器。

Result: 所提出的电力潮流控制器设计经过数学评估，并在仿真和实际系统中验证了其性能。结果表明，该控制器能够以较小的功率控制通过低压串联注入模块的高电流，有效解决低压电网的电力管理问题。

Conclusion: 该论文成功开发并验证了一种高度紧凑、高电流密度的电力潮流控制器，适用于低压配电网不同馈线之间的潮流控制。它通过创新的拓扑结构和SiC/Si技术的结合，有效克服了传统UPFC的缺点，为解决现代电网挑战提供了可行方案。

Abstract: An increasing integration of photovoltaic units, electric vehicle chargers,
heat pumps, and energy storage systems challenges low-voltage power grids and
can cause voltage range violation, loss of stability, (local) overload of
lines, and power management problems. Research suggested universal power-flow
control (UPFC) to solve power management problems. In contrast to bulky, slow,
and costly conventional UPFCs with their shunt and series transformers, this
paper presents a highly compact and current-dense power-flow controller, which
can serve between different feeders in the low-voltage power grids. The enabler
is a systematic combination of silicon car-bide (SiC) with silicon (Si)
transistors and a strict partial-power topology built around a multi-active
bridge. The circuit links an active-front-end converter as a shunt stage
through a multi-active-bridge converter bidirectionally with low-voltage
series-injection modules floating with their respective phases. The topology
can use small power to control high currents through the low-voltage
series-injection modules. The multi-active bridge serves as a
multi-input-output power router that exchanges energy between all elements. We
assess the design as well as the implementation considerations of the proposed
power-flow controller mathematically and verify its performance in simulation
and real systems.

</details>


### [348] [Factor Graph Optimization for Leak Localization in Water Distribution Networks](https://arxiv.org/abs/2509.10982)
*Paul Irofti,Luis Romero-Ben,Florin Stoican,Vicenç Puig*

Main category: eess.SY

TL;DR: 本文首次提出使用因子图优化技术进行供水网络泄漏定位，通过融合压力和需求传感器数据，并估计网络状态演变，相比现有方法在速度和定位精度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 供水网络泄漏检测和定位具有重要的环境、经济和社会影响。传统的卡尔曼滤波和插值方法只能估计当前网络状态，且可能存在速度和精度上的局限性。

Method: 该方法引入了特定的供水网络因子，并提出了一种由两个因子图组成的新架构：一个用于无泄漏状态估计，另一个用于泄漏定位。通过因子图优化技术，实现了压力和需求传感器读数的融合，并能更新当前和过去的网络状态。

Result: 在Modena、L-TOWN和合成网络上的测试结果表明，因子图方法比非线性卡尔曼滤波（如UKF）快得多，并且与最先进的估计-定位方法相比，在定位精度方面也有所改进。

Conclusion: 因子图优化技术是供水网络泄漏定位的有效方法，其在处理传感器融合、估计网络状态演变以及提供更快的速度和更高的定位精度方面优于现有的卡尔曼滤波和其他插值方法。

Abstract: Detecting and localizing leaks in water distribution network systems is an
important topic with direct environmental, economic, and social impact. Our
paper is the first to explore the use of factor graph optimization techniques
for leak localization in water distribution networks, enabling us to perform
sensor fusion between pressure and demand sensor readings and to estimate the
network's temporal and structural state evolution across all network nodes. The
methodology introduces specific water network factors and proposes a new
architecture composed of two factor graphs: a leak-free state estimation factor
graph and a leak localization factor graph. When a new sensor reading is
obtained, unlike Kalman and other interpolation-based methods, which estimate
only the current network state, factor graphs update both current and past
states. Results on Modena, L-TOWN and synthetic networks show that factor
graphs are much faster than nonlinear Kalman-based alternatives such as the
UKF, while also providing improvements in localization compared to
state-of-the-art estimation-localization approaches. Implementation and
benchmarks are available at https://github.com/pirofti/FGLL.

</details>


### [349] [Real-Time Defense Against Coordinated Cyber-Physical Attacks: A Robust Constrained Reinforcement Learning Approach](https://arxiv.org/abs/2509.10999)
*Saman Mazaheri Khamaneh,Tong Wu,Wei Sun,Cong Chen*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的三层鲁棒约束强化学习（RCRL）框架，旨在实时识别并抵御电力系统中的N-K网络物理攻击，通过快速响应（0.21毫秒）恢复系统约束，从而显著增强关键基础设施的韧性。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统越来越容易受到超出传统N-1偶发事件框架的复杂网络物理攻击。现有安全范式在高效识别最坏情况和快速协调防御响应方面存在计算密集和时间延迟的瓶颈，这可能导致级联故障的传播。

Method: 本文提出一个三层鲁棒约束强化学习（RCRL）框架。该框架通过AC-OPF公式生成多样化的系统状态，为每个状态识别最坏的N-K攻击场景，并训练策略以在所有运行条件下缓解这些场景，无需预定义攻击模式。在训练期间，通过基于Beta-blending投影的可行动作映射技术处理约束满足；在部署时，采用原始-对偶增广拉格朗日优化。

Result: 在IEEE基准系统上的验证表明，该框架能有效抵御导致全网范围级联故障的协调N-K攻击。学习到的策略能够以0.21毫秒的推理时间快速响应，成功地将全系统约束恢复正常。

Conclusion: 该RCRL框架为关键基础设施保护建立了卓越的韧性，提供了一种实时控制观察到的网络物理攻击并快速恢复系统约束的有效方法。

Abstract: Modern power systems face increasing vulnerability to sophisticated
cyber-physical attacks beyond traditional N-1 contingency frameworks. Existing
security paradigms face a critical bottleneck: efficiently identifying
worst-case scenarios and rapidly coordinating defensive responses are hindered
by intensive computation and time delays, during which cascading failures can
propagate. This paper presents a novel tri-level robust constrained
reinforcement learning (RCRL) framework for robust power system security. The
framework generates diverse system states through AC-OPF formulations,
identifies worst-case N-K attack scenarios for each state, and trains policies
to mitigate these scenarios across all operating conditions without requiring
predefined attack patterns. The framework addresses constraint satisfaction
through Beta-blending projection-based feasible action mapping techniques
during training and primal-dual augmented Lagrangian optimization for
deployment. Once trained, the RCRL policy learns how to control observed
cyber-physical attacks in real time. Validation on IEEE benchmark systems
demonstrates effectiveness against coordinated N-K attacks, causing widespread
cascading failures throughout the network. The learned policy can successfully
respond rapidly to recover system-wide constraints back to normal within 0.21
ms inference times, establishing superior resilience for critical
infrastructure protection.

</details>


### [350] [General Decentralized Stochastic Optimal Control via Change of Measure: Applications to the Witsenhausen Counterexample](https://arxiv.org/abs/2509.11013)
*Bhagyashri Telsang,Seddik Djouadi,Charalambos D. Charalambous*

Main category: eess.SY

TL;DR: 本文提出了分散式随机动态最优控制问题的全局和逐人最优性条件，并将其应用于Witsenhausen反例，证明了其解的存在唯一性，并通过数值方法验证了线性与非线性策略的相对优劣。


<details>
  <summary>Details</summary>
Motivation: 研究分散式随机动态最优控制问题，特别是解决Witsenhausen反例中策略选择的复杂性，并为这类问题提供通用的最优性条件和可行的求解方法。

Method: 使用Girsanov测度变换的离散时间版本来推导全局和逐人（PbP）最优性条件；将PbP条件应用于Witsenhausen反例，得到两个耦合的非线性积分方程；通过函数空间中的不动点定理证明积分方程解的存在性和唯一性；采用Gauss Hermite求积方案进行数值求解。

Result: 推导出了应用于Witsenhausen反例的两个耦合非线性积分方程；证明了这些积分方程解的存在性和唯一性；数值解证实了Witsenhausen的观察，即在某些参数选择下线性或仿射策略是最优的，而在其他情况下非线性策略优于仿射策略。

Conclusion: 本文为一般分散式随机动态最优控制问题提供了有效的最优性条件和求解框架，并通过Witsenhausen反例的分析和数值结果，验证了在不同参数设置下线性与非线性策略的相对性能，强调了非线性策略在某些情况下的优越性。

Abstract: In this paper we present global and person-by-person (PbP) optimality
conditions for general decentralized stochastic dynamic optimal control
problems, using a discrete-time version of Girsanov's change of measure. The
PbP optimality conditions are applied to the Witsenhausen counterexample to
show that the two strategies satisfy two coupled nonlinear integral equations.
Further, we prove a fixed point theorem in a function space, establishing
existence and uniqueness of solutions to the integral equations. We also
provide numerical solutions of the two integral equations using the Gauss
Hermite Quadrature scheme, and include a detail comparison to other numerical
methods of the literature. The numerical solutions confirm Witsehausen's
observation that, for certain choices of parameters, linear or affine
strategies are optimal, while for other choices of parameters nonlinear
strategies outperformed affine strategies.

</details>


### [351] [Privacy-Preserving Uncertainty Disclosure for Facilitating Enhanced Energy Storage Dispatch](https://arxiv.org/abs/2509.11022)
*Ning Qi,Xiaolong Jin,Kai Hou,Zeyu Liu,Hongjie Jia,Wei Wei*

Main category: eess.SY

TL;DR: 本文提出了一种保护隐私的不确定性披露框架，通过发布边际价值函数边界来减少区间预测的保守性并缓解过度保留，从而优化储能调度和提高社会福利。


<details>
  <summary>Details</summary>
Motivation: 现有区间预测过于保守，导致储能调度效率低下和过度保留行为。研究旨在通过披露不确定性信息来解决这些问题，从而提高储能响应能力和系统效益。

Method: 该研究提出了一种基于随机动态规划的风险规避分析型储能套利模型，明确考虑了价值函数训练中的不确定性区间。利用滚动时域机会约束经济调度公式，推导出实时边际价值函数边界。理论证明了这些边界能可靠地限制真实机会成本并动态收敛于事后价值。

Result: 研究证明，所提出的边际价值函数及其边界随荷电状态单调递减，随不确定性单调递增。通过ISO-NE测试系统的代理仿真验证，该框架在50%可再生能源和35%储能容量下，将储能响应提高了38.91%，将最优性差距缩小到3.91%。此外，通过缓解过度保留，平均系统成本降低了0.23%，平均储能利润增加了13.22%。这些效益随预测保守性、储能容量和系统不确定性的增加而进一步放大。

Conclusion: 所提出的隐私保护不确定性披露框架有效提升了储能调度能力和系统效益，通过提供可靠的边际价值函数边界，减少了预测保守性，缓解了过度保留，从而提高了储能响应、降低了系统成本并增加了储能利润。

Abstract: This paper proposes a novel privacy-preserving uncertainty disclosure
framework, enabling system operators to release marginal value function bounds
to reduce the conservativeness of interval forecast and mitigate excessive
withholding, thereby enhancing storage dispatch and social welfare. We propose
a risk-averse analytical storage arbitrage model based on stochastic dynamic
programming and explicitly account for uncertainty intervals in value function
training. We derive real-time marginal value function bounds using a
rolling-horizon chance-constrained economic dispatch formulation. We rigorously
prove that the bounds reliably cap the true opportunity cost and dynamically
converge to the hindsight value. We verify that both the marginal value
function and its bounds monotonically decrease with the state of charge and
increase with uncertainty, providing a theoretical basis for risk-averse
strategic behaviors and SoC-dependent designs. We validate the effectiveness of
the proposed framework via an agent-based simulation on the ISO-NE test system.
Under 50% renewable capacity and 35% storage capacity, the proposed bounds
enhance storage response by 38.91% and reduce the optimality gap to 3.91%
through improved interval predictions. Additionally, by mitigating excessive
withholding, the bounds yield an average system cost reduction of 0.23% and an
average storage profit increase of 13.22%. These benefits further scale with
higher prediction conservativeness, storage capacity, and system uncertainty.

</details>


### [352] [A Signed Friedkin-Johnsen Model for Arbitrary Network Topologies](https://arxiv.org/abs/2509.11038)
*Aashi Shrinate,Twinkle Tripathy*

Main category: eess.SY

TL;DR: 本文提出了一种基于对立规则的符号Friedkin-Johnsen (SFJ)模型，用于分析带符号交互和顽固代理的网络中意见演化。研究发现符号交互会改变意见领袖和顽固代理的影响力，并提出了一种新的绝对影响力中心性度量来量化代理影响力。


<details>
  <summary>Details</summary>
Motivation: 研究在任意网络拓扑结构中，当存在符号交互和顽固代理时，代理的意见如何演化，以及哪些代理（影响力代理）对最终意见贡献最大。

Method: 提出了一个基于对立规则的符号Friedkin-Johnsen (SFJ)模型。分析了意见收敛性。将代理分为意见领袖和追随者。提出了一种适用于任何网络拓扑、考虑顽固性和符号交互效应的“绝对影响力中心性”度量。通过Bitcoin Alpha数据集进行了仿真。

Result: 符号交互可以使意见领袖变得不那么有影响力或影响力减弱。顽固代理虽然始终保持影响力，但也可能因符号交互而影响力减弱。符号交互可以使代理的意见超出其初始意见的凸包范围。提出的绝对影响力中心性度量能够量化网络中所有代理的整体影响力并识别最有影响力的代理。

Conclusion: 符号交互在意见演化中扮演着关键角色，它能显著改变意见领袖和顽固代理的影响力，甚至导致意见超出初始凸包。新提出的绝对影响力中心性度量提供了一种有效工具来识别和量化网络中的关键影响力代理。

Abstract: The paper presents an opposing rule-based signed Friedkin-Johnsen (SFJ) model
for the evolution of opinions in arbitrary network topologies with signed
interactions and stubborn agents. The primary objective of the paper is to
analyse the emergent behaviours of the agents under the proposed rule and to
identify the key agents which contribute to the final opinions, characterised
as influential agents. We start by presenting some convergence results which
show how the opinions of the agents evolve for a signed network with any
arbitrary topology. Throughout the paper, we classify the agents as opinion
leaders (sinks in the associated condensation graph) and followers (the rest).
In general, it has been shown in the literature that opinion leaders and
stubborn agents drive the opinions of the group. However, the addition of
signed interactions reveals interesting behaviours wherein opinion leaders can
now become non-influential or less influential. Further, while the stubborn
agents always continue to remain influential, they might become less
influential owing to signed interactions. Additionally, the signed interactions
can drive the opinions of the agents outside of the convex hull of their
initial opinions. Thereafter, we propose the absolute influence centrality
measure, which allows us to quantify the overall influence of all the agents in
the network and also identify the most influential agents. Unlike most of the
existing measures, it is applicable to any network topology and considers the
effect of both stubbornness and signed interactions. Finally, simulations are
presented for the Bitcoin Alpha dataset to elaborate the proposed results.

</details>


### [353] [Opinion Clustering under the Friedkin-Johnsen Model: Agreement in Disagreement](https://arxiv.org/abs/2509.11045)
*Aashi Shrinate,Twinkle Tripathy*

Main category: eess.SY

TL;DR: 本文探讨了Friedkin-Johnsen (FJ) 框架中意见聚类的拓扑条件，引入了“局部拓扑说服力代理人 (LTP)”的概念，并展示了如何通过它们在任意有向图中实现期望的意见聚类，且聚类结果与边权重和代理人固执程度无关。


<details>
  <summary>Details</summary>
Motivation: Friedkin-Johnsen (FJ) 框架中的意见收敛已得到充分研究，但导致意见聚类的拓扑条件却鲜有探索。本文旨在弥合这一差距，研究拓扑结构在网络中意见聚类形成中的作用。

Method: 引入“局部拓扑说服力代理人 (LTP)”的概念，每个LTP代理人与其附近的独特一组（非影响力）代理人相关联。利用LTP代理人，本文提出了在任意连接的有向图中FJ框架下获得意见聚类的条件。通过仿真结果，展示了通过适当放置LTP代理人可以设计实现任何期望意见聚类的网络。

Result: 提出了在任意连接的有向图中FJ框架下获得意见聚类的条件。这些意见聚类独立于边权重和代理人的固执程度。仿真结果表明，通过适当放置LTP代理人，可以设计出实现任何期望意见聚类的网络。

Conclusion: 拓扑结构在FJ框架的意见聚类中起着关键作用。通过引入LTP代理人，本文为在任意有向图中理解和设计意见聚类提供了条件，且聚类结果不受传统参数（如边权重和代理人固执程度）的影响。

Abstract: The convergence of opinions in the Friedkin-Johnsen (FJ) framework is well
studied, but the topological conditions leading to opinion clustering remain
less explored. To bridge this gap, we examine the role of topology in the
emergence of opinion clusters within the network. The key contribution of the
paper lies in the introduction of the notion of topologically prominent agents,
referred to as Locally Topologically Persuasive (LTP) agents. Interestingly,
each LTP agent is associated with a unique set of (non-influential) agents in
its vicinity. Using them, we present conditions to obtain opinion clusters in
the FJ framework in any arbitrarily connected digraph. A key advantage of the
proposed result is that the resulting opinion clusters are independent of the
edge weights and the stubbornness of the agents. Finally, we demonstrate using
simulation results that, by suitably placing LTP agents, one can design
networks that achieve any desired opinion clustering.

</details>


### [354] [BERT4beam: Large AI Model Enabled Generalized Beamforming Optimization](https://arxiv.org/abs/2509.11056)
*Yuhang Li,Yang Lu,Wei Chen,Bo Ai,Zhiguo Ding,Dusit Niyato*

Main category: eess.SY

TL;DR: 该论文提出了一种名为BERT4beam的新型框架，将波束成形优化问题建模为基于Transformer的双向编码器表示（BERT）的序列学习任务，以实现对6G无线通信系统中不同系统效用和规模的适应性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）有望成为未来第六代（6G）无线通信系统的关键推动力。然而，目前针对无线通信的大型AI模型研究主要集中于对预训练大型语言模型（LLM）进行特定任务的微调，缺乏针对波束成形优化的大规模AI模型，使其能够适应和泛化到由不同系统效用和规模定义的各种任务。

Method: 该研究提出了一种基于BERT的BERT4beam框架。其核心方法包括：将波束成形优化问题表述为令牌级序列学习任务；对信道状态信息进行令牌化处理；构建BERT模型；以及实施任务特定的预训练和微调策略。基于此框架，论文提出了两种BERT方法，分别用于单任务和多任务波束成形优化。其中，多任务方法被称为UBERT，采用了更细粒度的令牌化策略。

Result: 仿真结果表明，所提出的两种方法均能达到接近最优的性能，并优于现有AI模型。这两种方法都可泛化到不同用户规模。此外，单任务方法通过重新配置BERT模型的输入和输出模块，可以适应不同的系统效用和天线配置；而多任务方法（UBERT）由于其更细粒度的令牌化策略，可以直接泛化到各种任务，展现出强大的适应性和泛化能力。

Conclusion: 该研究提出的BERT4beam框架及其两种BERT方法（包括UBERT）在波束成形优化方面表现出卓越的性能、强大的适应性和泛化能力，能够有效地处理不同系统效用、天线配置和用户规模下的各种波束成形优化任务，为6G无线通信系统中的AI应用提供了新的方向。

Abstract: Artificial intelligence (AI) is anticipated to emerge as a pivotal enabler
for the forthcoming sixth-generation (6G) wireless communication systems.
However, current research efforts regarding large AI models for wireless
communications primarily focus on fine-tuning pre-trained large language models
(LLMs) for specific tasks. This paper investigates the large-scale AI model
designed for beamforming optimization to adapt and generalize to diverse tasks
defined by system utilities and scales. We propose a novel framework based on
bidirectional encoder representations from transformers (BERT), termed
BERT4beam. We aim to formulate the beamforming optimization problem as a
token-level sequence learning task, perform tokenization of the channel state
information, construct the BERT model, and conduct task-specific pre-training
and fine-tuning strategies. Based on the framework, we propose two BERT-based
approaches for single-task and multi-task beamforming optimization,
respectively. Both approaches are generalizable for varying user scales.
Moreover, the former can adapt to varying system utilities and antenna
configurations by re-configuring the input and output module of the BERT model,
while the latter, termed UBERT, can directly generalize to diverse tasks, due
to a finer-grained tokenization strategy. Extensive simulation results
demonstrate that the two proposed approaches can achieve near-optimal
performance and outperform existing AI models across various beamforming
optimization tasks, showcasing strong adaptability and generalizability.

</details>


### [355] [Fundamental limitations of sensitivity metrics for anomaly impact analysis in LTI systems](https://arxiv.org/abs/2509.11194)
*Jingwei Dong,Kangkang Zhang,Anh Tung Nguyen,André M. H. Teixeira*

Main category: eess.SY

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: This study establishes a connection between the output-to-output gain (OOG),
a sensitivity metric quantifying the impact of stealthy attacks, and a novel
input-to-input gain (IIG) introduced to evaluate fault sensitivity under
disturbances, and investigates their fundamental performance limitations
arising from the transmission zeros of the underlying dynamical system.
Inspired by the OOG, which characterizes the maximum performance loss caused by
stealthy attacks, the IIG is proposed as a new measure of robust fault
sensitivity, and is defined as the maximum energy of undetectable faults for a
given disturbance intensity. Then, using right (for OOG) and left (for IIG)
co-prime factorizations, both metrics are expressed as
the~$\mathcal{H}_{\infty}$ norm of a ratio of the numerator factors. This
unified representation facilitates a systematic analysis of their fundamental
limitations. Subsequently, by utilizing the Poisson integral relation,
theoretical bounds for the IIG and OOG are derived, explicitly characterizing
their fundamental limitations imposed by system \mbox{non-minimum} phase (NMP)
zeros. Finally, a numerical example is employed to validate the results.

</details>


### [356] [Dynamic Modeling, Analysis, and Validation of Dual-Port Grid-Forming Control for Hybrid AC/DC Systems](https://arxiv.org/abs/2509.11318)
*Irina Subotić,Dominic Groß,Alexander Winkens,Julian Jansen,Florian Klein-Helmkamp,Andreas Ulbig*

Main category: eess.SY

TL;DR: 本文研究了混合交直流系统在双端口构网型(GFM)控制下的暂态和动态行为。通过引入广义建模框架，并结合硬件实验验证，结果表明双端口GFM控制的主动阻尼能有效改善暂态响应和抑制振荡，同时提高低中压配电网络的运行灵活性。


<details>
  <summary>Details</summary>
Motivation: 研究混合交直流系统在双端口构网型(GFM)控制下的暂态和动态行为，旨在改善其暂态响应，抑制振荡，并增强配电网络的运行灵活性。

Method: 首先引入了一个广义的混合交直流网络建模框架，该框架考虑了变流器、控制和网络电路的动态以及任意网络拓扑。然后将此模型应用于低压网络，分析双端口GFM控制的性能。最后，通过针对三种典型系统架构的硬件实验验证了动态模型和结果。

Result: 结果表明，双端口GFM控制的主动阻尼能有效改善暂态响应并抑制振荡。稳态响应特性可以独立调整，且对阻尼特性影响极小。此外，使用双端口GFM控制的交直流变流器接口的低压直流配电，可以作为交流配电系统之间唯一的互连方式，也可以与交流连接并行，从而增强低中压配电网络的运行灵活性。

Conclusion: 双端口构网型(GFM)控制在混合交直流系统中表现出优异的性能，其主动阻尼能显著改善暂态响应和抑制振荡，同时允许独立调整稳态特性。该控制策略还通过提供灵活的互连选项，有效增强了低中压配电网络的运行灵活性。

Abstract: This work investigates the transient and dynamical behavior of hybrid AC/DC
systems using dual-port grid-forming (GFM) control. A generalized modeling
framework for hybrid AC/DC networks is first introduced that accounts for
converter, control, and network circuit dynamics and arbitrary network
topologies. This modeling framework is applied to low-voltage networks to
analyze the performance of dual-port grid-forming (GFM) control. The results
demonstrate that active damping by dual-port GFM control is effective at
improving the transient response and mitigating oscillations. In contrast, the
steady-state response characteristics can be adjusted independently with
minimal impact on damping characteristics. The dynamic model and results are
validated through hardware experiments for three prototypical system
architectures. Furthermore, we demonstrate that low-voltage DC distribution
interfaced by AC/DC converters using dual-port GFM control, can serve both as
the sole interconnection between AC distribution systems and in parallel to an
AC connection, thereby enhancing the operational flexibility of low- and
medium-voltage distribution networks.

</details>


### [357] [Large-Scale Self-Powered Vibration Control: Theory and Experiment](https://arxiv.org/abs/2509.11346)
*Connor Ligeikis,Heath Hofmann,Jeff Scruggs*

Main category: eess.SY

TL;DR: 本文设计并实验验证了一种自供电振动控制系统原型，适用于大规模应用（功率>1W，力约1kN），该系统通过考虑硬件寄生效应导致的更严格可行性条件，实现了可行的控制设计。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够通过从外部扰动中收集能量来自供电的振动控制系统，并将其应用于需要较大功率和力的工业级场景。

Method: 该原型系统由一个线性滚珠丝杠与永磁同步电机耦合构成。使用定制的三相逆变器控制功率流，并使用定制的半桥DC-DC电源转换器促进与储能电容器之间的功率流动。控制设计采用了一种可行的控制方法，该方法考虑了由于控制硬件寄生效应而导致的比单纯无源性更严格的系统可行性条件。通过针对随机激励调谐吸振器的硬件在环实验进行验证。

Result: 所提出的控制设计通过硬件在环实验得到了验证，实验结果证明了其在处理随机激励调谐吸振器方面的有效性。

Conclusion: 成功设计并实验验证了一种适用于大规模应用的自供电振动控制系统原型及其控制策略，该策略有效地解决了实际控制硬件中的寄生效应带来的可行性限制。

Abstract: A self-powered system is a control technology that powers itself by
harvesting energy from exogenous disturbances. This article details the design
and experimental validation of a prototype self-powered vibration control
system, for larger-scale applications (i.e., power flows above 1W and forces on
the order of 1kN.) The prototype consists of a linear ballscrew coupled with a
permanent-magnet synchronous machine. A custom three-phase inverter is used to
control power flow, and a custom half-bridge DC-DC power converter is used to
facilitate power flow to and from a storage capacitor. Due to parasitics in the
control hardware, feedback laws for self-powered systems must adhere to a
feasibility condition tighter than mere passivity. This article implements a
tractable control design approach that accounts for this feasibility
constraint. The control design is validated via hardware-in-the-loop
experiments pertaining to a stochastically-excited tuned vibration absorber.

</details>


### [358] [A Goal-Oriented Approach for Active Object Detection with Exploration-Exploitation Balance](https://arxiv.org/abs/2509.11467)
*Yalei Yu,Matthew Coombes,Wen-Hua Chen,Cong Sun,Myles Flanagan,Jingjing Jiang,Pramod Pashupathy,Masoud Sotoodeh-Bahraini,Peter Kinnell,Niels Lohse*

Main category: eess.SY

TL;DR: 本文提出了一种名为DCEE（探索与利用双重控制）的算法，用于实现高效的主动目标检测，通过平衡探索和利用来优化视点选择，并在多种场景下表现出卓越的适应性和优越性。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中执行制造任务（如装配操作）等自主机器人应用中，主动目标检测对于现实世界的视觉感知至关重要。

Method: DCEE算法在面向目标的控制系统中实现，通过将基于方差的不确定性估计纳入成本函数来利用主动学习。该方法采用探索-利用平衡的成本函数来主动指导下一个视点的选择。通过开发一个奖励函数来编码目标置信度随视点位置的变化知识，并识别该函数的未知参数，系统生成最优视点规划策略。DCEE集成了奖励函数参数估计和视点规划，平衡了知识利用和主动探索。

Result: DCEE算法在主动目标检测方面展现出显著效果，能有效处理不同位置的乐高积木检测，并在各种场景下保持一致的配置设置和固定数量的参数，体现了其高效性和鲁棒性。通过广泛的数值研究、高保真虚拟仿真和真实世界实验验证，DCEE在主动目标检测方面表现出优于现有方法（包括模型预测控制和熵方法）的性能。

Conclusion: DCEE是一种有效、鲁棒且性能优越的主动目标检测算法，通过平衡探索与利用的策略，能够高效地识别感兴趣的目标，并适应多样化的应用场景。

Abstract: Active object detection, which aims to identify objects of interest through
controlled camera movements, plays a pivotal role in real-world visual
perception for autonomous robotic applications, such as manufacturing tasks
(e.g., assembly operations) performed in unknown environments. A dual control
for exploration and exploitation (DCEE) algorithm is presented within
goal-oriented control systems to achieve efficient active object detection,
leveraging active learning by incorporating variance-based uncertainty
estimation in the cost function. This novel method employs an
exploration-exploitation balanced cost function to actively guide the selection
of the next viewpoint. Specifically, active object detection is achieved
through the development of a reward function that encodes knowledge about the
confidence variation of objects as a function of viewpoint position within a
given domain. By identifying the unknown parameters of this function, the
system generates an optimal viewpoint planning strategy. DCEE integrates
parameter estimation of the reward function and view planning, ensuring a
balanced trade-off between the exploitation of learned knowledge and active
exploration during the planning process. Moreover, it demonstrates remarkable
adaptability across diverse scenarios, effectively handling LEGO brick
detection at varying locations. Importantly, the algorithm maintains consistent
configuration settings and a fixed number of parameters across various
scenarios, underscoring its efficiency and robustness. To validate the proposed
approach, extensive numerical studies, high-fidelity virtual simulations, and
real-world experiments under various scenarios were conducted. The results
confirm the effectiveness of DCEE in active object detection, showcasing
superior performance compared to existing methods, including model predictive
control (MPC) and entropy approaches.

</details>


### [359] [Partitioning techniques for non-centralized predictive control: A systematic review and novel theoretical insights](https://arxiv.org/abs/2509.11470)
*Alessandro Riccardi,Luca Laurenti,Bart De Schutter*

Main category: eess.SY

TL;DR: 本综述对非集中式模型预测控制（MPC）中大规模系统划分方法进行了系统分类，提出了统一的图论形式、数学重构和质量指标，并分析了现有方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 划分问题对于设计和实现大规模系统的非集中式模型预测控制（包括去中心化、分布式、分层和联盟MPC）至关重要，旨在优化子系统定义和控制器分配以最大化性能指标。

Method: 本研究提出了一种将文献中的划分方法系统化为五大类（基于优化、算法、基于社区检测、博弈论导向和启发式）的新方法。开发了统一的图论形式、基于混合整数规划的数学重构、预测性划分和多拓扑表示等新概念，并提出了质量指标的方法论公式。分析了不同划分技术的优缺点，并讨论了代表性案例研究。

Result: 本研究成功地将非集中式MPC的划分方法系统化为五类，提出了支持分类和未来发展的统一形式、数学重构、新概念和质量指标。通过对不同方法的分析和案例研究，展示了其在电力系统、水网络、风电场等多个领域的应用。

Conclusion: 本综述为非集中式MPC中的划分问题提供了一个全面的分析框架和新颖的分类体系，阐明了现有方法的优势和局限性，并展望了未来的挑战，有助于该领域的进一步发展。

Abstract: The partitioning problem is of central relevance for designing and
implementing non-centralized Model Predictive Control (MPC) strategies for
large-scale systems. These control approaches include decentralized MPC,
distributed MPC, hierarchical MPC, and coalitional MPC. Partitioning a system
for the application of non-centralized MPC consists of finding the best
definition of the subsystems, and their allocation into groups for the
definition of local controllers, to maximize the relevant performance
indicators. The present survey proposes a novel systematization of the
partitioning approaches in the literature in five main classes:
optimization-based, algorithmic, community-detection-based,
game-theoretic-oriented, and heuristic approaches. A unified graph-theoretical
formalism, a mathematical re-formulation of the problem in terms of
mixed-integer programming, the novel concepts of predictive partitioning and
multi-topological representations, and a methodological formulation of quality
metrics are developed to support the classification and further developments of
the field. We analyze the different classes of partitioning techniques, and we
present an overview of their strengths and limitations, which include a
technical discussion about the different approaches. Representative case
studies are discussed to illustrate the application of partitioning techniques
for non-centralized MPC in various sectors, including power systems, water
networks, wind farms, chemical processes, transportation systems, communication
networks, industrial automation, smart buildings, and cyber-physical systems.
An outlook of future challenges completes the survey.

</details>


### [360] [Model Predictive Control with High-Probability Safety Guarantee for Nonlinear Stochastic Systems](https://arxiv.org/abs/2509.11584)
*Zishun Liu,Liqian Ma,Yongxin Chen*

Main category: eess.SY

TL;DR: 本文提出了一种针对非线性随机系统的模型预测控制（MPC）框架，通过集合侵蚀将概率安全约束转换为确定性约束，从而在高概率下保证系统安全，并兼容现有确定性MPC算法。


<details>
  <summary>Details</summary>
Motivation: 大多数现有随机MPC方案难以处理概率安全约束，且需要一种能与现有确定性MPC算法兼容、适用于安全关键型复杂非线性系统的解决方案。

Method: 采用模型预测控制（MPC）框架，通过“集合侵蚀”（set-erosion）技术，将概率安全约束转换为在确定性动力学下，在更小安全集上的可处理的确定性安全约束。该方法利用随机轨迹围绕其标称轨迹的随机波动的紧密界限，并兼容任何现成的确定性MPC算法。

Result: 该方法能够以高概率（例如99.99%）保证系统安全，具有可扩展性，并特别适用于安全关键型应用。通过严格分析建立了理论安全保证，并通过数值实验验证了方法的有效性。

Conclusion: 所提出的MPC方法通过创新的约束转换技术，为涉及复杂非线性动力学的安全关键型应用提供了高概率的安全保证，且具有理论支持和实验验证。

Abstract: We present a model predictive control (MPC) framework for nonlinear
stochastic systems that ensures safety guarantee with high probability. Unlike
most existing stochastic MPC schemes, our method adopts a set-erosion that
converts the probabilistic safety constraint into a tractable deterministic
safety constraint on a smaller safe set over deterministic dynamics. As a
result, our method is compatible with any off-the-shelf deterministic MPC
algorithm. The key to the effectiveness of our method is a tight bound on the
stochastic fluctuation of a stochastic trajectory around its nominal version.
Our method is scalable and can guarantee safety with high probability level
(e.g., 99.99%), making it particularly suitable for safety-critical
applications involving complex nonlinear dynamics. Rigorous analysis is
conducted to establish a theoretical safety guarantee, and numerical
experiments are provided to validate the effectiveness of the proposed MPC
method.

</details>


### [361] [$ε$-Optimal Multi-Agent Patrol using Recurrent Strategy](https://arxiv.org/abs/2509.11640)
*Deepak Mallya,Arpita Sinha,Leena Vachhani*

Main category: eess.SY

TL;DR: 本文研究多智能体巡逻问题，指出现有文献未探讨最优解的本质。作者证明了存在一个近似最优的循环巡逻策略，并提供了一种算法来确定它。


<details>
  <summary>Details</summary>
Motivation: 多智能体巡逻问题已被研究二十多年，但现有文献尚未阐明该问题最优解的本质。

Method: 首先证明了对于任何可行的巡逻策略，都存在一个 $\epsilon$-近似循环巡逻策略。然后，建立了通用巡逻问题存在一个 $\epsilon$-最优循环巡逻策略的结论。此外，提出了一种算法方法来确定由文献中任何方法创建的巡逻策略的 $\epsilon$-近似循环巡逻策略。通过基于真实环境的图进行广泛模拟来验证了结论。

Result: 对于每个可行的巡逻策略，都存在一个 $\epsilon$-近似循环巡逻策略。通用巡逻问题存在一个 $\epsilon$-最优循环巡逻策略，其中 $\epsilon$ 与离散化常数 $D$ 成正比（可以任意小，且与巡逻智能体数量和环境大小无关）。该结果适用于多种已研究的问题公式。还提供了一种算法方法来确定 $\epsilon$-近似循环巡逻策略。

Conclusion: 本文证明了多智能体巡逻问题中 $\epsilon$-近似和 $\epsilon$-最优循环巡逻策略的存在性，并提供了一种算法方法来确定这些策略。这些发现通过在真实环境中的模拟得到了验证。

Abstract: The multi-agent patrol problem refers to repeatedly visiting different
locations in an environment using multiple autonomous agents. For over two
decades, researchers have studied this problem in various settings. While
providing valuable insights into the problem, the works in existing literature
have not commented on the nature of the optimal solutions to the problem. We
first show that an $\epsilon$-approximate recurrent patrol strategy exists for
every feasible patrol strategy. Then, we establish the existence of a recurrent
patrol strategy that is an $\epsilon$-optimal solution to the General Patrol
Problem. The factor $\epsilon$ is proportional to the discretisation constant
$D$, which can be arbitrarily small and is independent of the number of patrol
agents and the size of the environment. This result holds for a variety of
problem formulations already studied. We also provide an algorithmic approach
to determine an $\epsilon$-approximate recurrent patrol strategy for a patrol
strategy created by any method from the literature. We perform extensive
simulations in graphs based on real-life environments to validate the claims
made in this work.

</details>


### [362] [Continuous-Time Distributed Learning for Collective Wisdom Maximization](https://arxiv.org/abs/2509.11808)
*Luka Baković,Giacomo Como,Fabio Fagnani,Anton Proskurnikov,Emma Tegling*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的学习动力学模型，作为Abelson意见动力学模型的补充。该模型通过意见交换实现共识，并研究了如何找到最优参数（如意见改变的敏感性）以最小化共识值的方差。研究还提出了一种分布式学习最优参数的动力学，并分析证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于“集体智慧胜于个体智慧”的普遍观点。

Method: 代理人首先独立猜测真实世界状态，然后进行意见交换以达成共识。研究旨在找到最小化共识值方差的最优参数，特别是意见改变的敏感性。提出了一种用于分布式学习最优参数的动力学，并通过共识理论的既有结果分析证明了其收敛性。最后，通过数值例子提供了系统行为和证明方法的直观理解。

Result: 本文提出了一种新颖的学习动力学模型。研究找到了意见交换的最优参数，这些参数能最小化共识值的方差。所提出的分布式学习最优参数的动力学被分析证明在所有相关初始条件下均能收敛。

Conclusion: 本文提出并分析了一种新的学习动力学，用于通过意见交换实现集体智慧。该动力学能够找到最优的意见交换参数，并且其分布式学习过程被证明是收敛的，从而为理解和优化集体决策提供了新的视角。

Abstract: Motivated by the well established idea that collective wisdom is greater than
that of an individual, we propose a novel learning dynamics as a sort of
companion to the Abelson model of opinion dynamics. Agents are assumed to make
independent guesses about the true state of the world after which they engage
in opinion exchange leading to consensus. We investigate the problem of finding
the optimal parameters for this exchange, e.g. those that minimize the variance
of the consensus value. Specifically, the parameter we examine is
susceptibility to opinion change. We propose a dynamics for distributed
learning of the optimal parameters and analytically show that it converges for
all relevant initial conditions by linking to well established results from
consensus theory. Lastly, a numerical example provides intuition on both system
behavior and our proof methods.

</details>


### [363] [Varying Horizon Learning Economic MPC With Unknown Costs of Disturbed Nonlinear Systems](https://arxiv.org/abs/2509.11823)
*Weiliang Xiong,Defeng He,Haiping Du,Jianbin Mu*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的变时域经济模型预测控制（EMPC）方案，适用于具有加性扰动和未知经济成本的受约束非线性系统，无需终端约束。该方案利用混合核回归重构未知成本，在线自适应调整时域，并通过时域相关的收缩约束确保闭环系统收敛性，并验证了其鲁棒性、经济性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在受约束非线性系统存在加性扰动和未知经济成本的情况下，传统的经济模型预测控制（EMPC）面临挑战，如需要终端约束和固定时域，影响其在实际应用中的灵活性和性能。

Method: 1. 采用混合核的通用回归学习框架来重构未知经济成本。2. 开发在线迭代过程以自适应调整预测时域。3. 设计时域相关的收缩约束以确保闭环系统收敛到期望稳态的邻域。4. 建立了确保递归可行性和输入到状态稳定性的充分条件。

Result: 1. 建立了闭环EMPC系统的递归可行性和输入到状态稳定性。2. 通过连续搅拌釜反应器和四罐系统的仿真验证了所提方案在鲁棒性、经济性能和在线计算负担方面的优势。

Conclusion: 所提出的变时域经济模型预测控制方案，无需终端约束，能够有效处理具有加性扰动和未知经济成本的受约束非线性系统，并展现出良好的鲁棒性、经济性能和较低的在线计算负担。

Abstract: This paper proposes a novel varying horizon economic model predictive control
(EMPC) scheme without terminal constraints for constrained nonlinear systems
with additive disturbances and unknown economic costs. The general regression
learning framework with mixed kernels is first used to reconstruct the unknown
cost. Then an online iterative procedure is developed to adjust the horizon
adaptively. Again, an elegant horizon-dependent contraction constraint is
designed to ensure the convergence of the closed-loop system to a neighborhood
of the desired steady state. Moreover, sufficient conditions ensuring recursive
feasibility and input-to-state stability are established for the system in
closed-loop with the EMPC. The merits of the proposed scheme are verified by
the simulations of a continuous stirred tank reactor and a four-tank system in
terms of robustness, economic performance and online computational burden.

</details>


### [364] [High Effort, Low Gain: Fundamental Limits of Active Learning for Linear Dynamical Systems](https://arxiv.org/abs/2509.11907)
*Nicolas Chatzikiriakos,Kevin Jamieson,Andrea Iannelli*

Main category: eess.SY

TL;DR: 本文研究了从有限假设类中识别未知线性动力系统的问题，重点分析了激励输入对样本复杂度的影响，提出了上下界和一种主动学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究激励输入如何影响从有限假设类中高概率识别真实系统的样本复杂度。

Method: 分析了激励输入对样本复杂度的影响；提出了包含激励输入选择的样本复杂度下界；基于下界分析，提出了针对该设置的持久激励 (PE) 条件；利用该条件建立了样本复杂度上界；最后，提出了一种主动学习算法，该算法根据当前估计值顺序最优地激励系统。

Result: 给出了捕捉激励输入选择的样本复杂度下界，并由此引出了一个系统理论条件来确定实验设计的潜在益处。提出了一种比无限假设类情况更弱的PE条件，允许模块化分析不同的激励输入。上下界在关键问题参数上具有相同的依赖性。为所提出的主动学习算法提供了样本复杂度保证。仿真结果展示了该算法的有效性。

Conclusion: 该研究通过理论分析（下界、上界、定制PE条件）和实践（主动学习算法）为从有限假设类中识别线性动力系统提供了深入理解和有效解决方案，并得到了实验验证。

Abstract: In this work, we consider the problem of identifying an unknown linear
dynamical system given a finite hypothesis class. In particular, we analyze the
effect of the excitation input on the sample complexity of identifying the true
system with high probability. To this end, we present sample complexity lower
bounds that capture the choice of the selected excitation input. The sample
complexity lower bound gives rise to a system theoretic condition to determine
the potential benefit of experiment design. Informed by the analysis of the
sample complexity lower bound, we propose a persistent excitation (PE)
condition tailored to the considered setting, which we then use to establish
sample complexity upper bounds. Notably, the \acs{PE} condition is weaker than
in the case of an infinite hypothesis class and allows analyzing different
excitation inputs modularly. Crucially, the lower and upper bounds share the
same dependency on key problem parameters. Finally, we leverage these insights
to propose an active learning algorithm that sequentially excites the system
optimally with respect to the current estimate, and provide sample complexity
guarantees for the presented algorithm. Concluding simulations showcase the
effectiveness of the proposed algorithm.

</details>


### [365] [Distributed Finite-Horizon Optimal Control for Consensus with Differential Privacy Guarantees](https://arxiv.org/abs/2509.11917)
*Yuwen Ma,Yongqiang Wang,Sarah K. Spurgeon,Boli Chen*

Main category: eess.SY

TL;DR: 本文提出了一种基于差分隐私的分布式有限时域LQR框架，用于多智能体系统(MAS)的隐私保护共识控制，特别是在共享状态信息的同时保护敏感的局部成对权重矩阵。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，局部成对权重矩阵被视为敏感数据，直接关联到每个智能体的私有成本函数和控制偏好。保护这些矩阵的机密性是实现隐私保护共识控制的关键问题。

Method: 研究方法包括：1) 提出一种新颖的分布式有限时域线性二次调节器(LQR)框架；2) 在通信的状态信息中注入依赖于共识误差的拉普拉斯噪声；3) 在局部成本函数中采用精心设计的时间相关缩放因子；4) 该方法不依赖于特定的噪声分布假设。

Result: 主要结果是：1) 保证了有界共识；2) 在不依赖特定噪声分布假设的情况下，为权重矩阵实现了严格的ε-差分隐私；3) 分析性地刻画了共识精度和隐私级别之间的权衡关系；4) 提供了通过适当缩放LQR权重矩阵和隐私预算来提高共识性能的明确指导。

Conclusion: 本文成功提出了一种新颖的隐私保护共识控制方案，该方案在保护敏感权重矩阵隐私的同时，保证了多智能体系统的有界共识。此外，还量化了共识精度与隐私级别之间的权衡，为实际应用提供了指导。

Abstract: This paper addresses the problem of privacy-preserving consensus control for
multi-agent systems (MAS) using differential privacy. We propose a novel
distributed finite-horizon linear quadratic regulator (LQR) framework, in which
agents share individual state information while preserving the confidentiality
of their local pairwise weight matrices, which are considered sensitive data in
MAS. Protecting these matrices effectively safeguards each agent's private cost
function and control preferences. Our solution injects consensus
error-dependent Laplace noise into the communicated state information and
employs a carefully designed time-dependent scaling factor in the local cost
functions. {This approach guarantees bounded consensus and achieves rigorous
$\epsilon$-differential privacy for the weight matrices without relying on
specific noise distribution assumptions.} Additionally, we analytically
characterize the trade-off between consensus accuracy and privacy level,
offering clear guidelines on how to enhance consensus performance through
appropriate scaling of the LQR weight matrices and the privacy budget.

</details>


### [366] [Compositional shield synthesis for safe reinforcement learning in partial observability](https://arxiv.org/abs/2509.12085)
*Steven Carr,Georgios Bakirtzis,Ufuk Topcu*

Main category: eess.SY

TL;DR: 本文提出了一种组合式安全防护（shield）合成方法，通过将安全要求分块建模，显著提高了强化学习（RL）代理在不确定和部分可观察环境（POMDPs）中的安全性、学习效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）代理在不确定和部分可观察环境（POMDPs）中经常进入不安全状态。传统的安全防护合成方法在复杂部署场景中计算成本高昂，难以扩展。

Method: 通过将安全要求分块建模，提出了一种组合式安全防护合成方法。该方法应用于使用RL算法的POMDPs问题，以确保代理的策略符合安全要求。

Result: 实验结果表明，配备组合式安全防护的RL代理不仅能保持安全，还能收敛到更高的预期奖励值。与未受防护的代理相比，它需要更少的训练回合，尤其是在稀疏奖励设置下。具体而言，组合式安全防护合成使RL代理能在比其他最先进的基于模型方法大两个数量级的环境中保持安全。

Conclusion: 组合式安全防护合成是一种有效的方法，可以在不确定和部分可观察的环境中，显著提高强化学习代理的安全性、学习效率，并使其能够处理更大规模的环境。

Abstract: Agents controlled by the output of reinforcement learning (RL) algorithms
often transition to unsafe states, particularly in uncertain and partially
observable environments. Partially observable Markov decision processes
(POMDPs) provide a natural setting for studying such scenarios with limited
sensing. Shields filter undesirable actions to ensure safe RL by preserving
safety requirements in the agents' policy. However, synthesizing holistic
shields is computationally expensive in complex deployment scenarios. We
propose the compositional synthesis of shields by modeling safety requirements
by parts, thereby improving scalability. In particular, problem formulations in
the form of POMDPs using RL algorithms illustrate that an RL agent equipped
with the resulting compositional shielding, beyond being safe, converges to
higher values of expected reward. By using subproblem formulations, we preserve
and improve the ability of shielded agents to require fewer training episodes
than unshielded agents, especially in sparse-reward settings. Concretely, we
find that compositional shield synthesis allows an RL agent to remain safe in
environments two orders of magnitude larger than other state-of-the-art
model-based approaches.

</details>


### [367] [Control Analysis and Design for Autonomous Vehicles Subject to Imperfect AI-Based Perception](https://arxiv.org/abs/2509.12137)
*Tao Yan,Zheyu Zhang,Jingjing Jiang,Wen-Hua Chen*

Main category: eess.SY

TL;DR: 本文提出了一种新的建模、分析和综合工具，通过将AI感知模块产生的感知误差（漏检和测量噪声）建模为连续时间马尔可夫链和维纳过程，从而解决了AI驱动自动驾驶汽车（AV）的闭环稳定性分析和性能保证问题，并设计了性能有保证的输出反馈控制器。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的安全性至关重要，尤其是当涉及基于AI的感知模块时。然而，AI算法的黑箱特性使得闭环分析和综合（如建立闭环稳定性和确保性能）变得极具挑战性，而这些对于自动驾驶汽车的安全性至关重要。

Method: 研究方法包括：1. 关注并表征AI感知模块产生的感知误差，而非直接建模AI感知过程。2. 将AI引起的感知误差分为漏检和测量噪声两类，分别使用连续时间马尔可夫链和维纳过程进行建模。3. 提出了一个增强感知误差模型（PEM）的驾驶模型。4. 利用随机微积分建立了一类AI驱动自动驾驶系统的闭环稳定性。5. 提出了一种性能有保证的输出反馈控制综合方法，该方法被表述为一个凸优化问题，可实现高效数值求解。

Result: 主要结果包括：1. 成功建立了一类AI驱动自动驾驶系统的闭环稳定性。2. 提出了一种能够同时确保稳定性和满意性能的输出反馈控制综合方法。3. 该方法被公式化为凸优化问题，支持高效的数值求解。4. 将所提出的方法应用于自适应巡航控制（ACC）场景，验证了其在感知受损和误导情况下的有效性和鲁棒性。

Conclusion: 本文为基于AI的自动驾驶汽车开发了新的建模、分析和综合工具。通过对AI引起的感知误差进行建模和表征，研究成功地建立了闭环稳定性，并提供了一种性能有保证的控制综合方法，有效提升了自动驾驶系统的安全性和可靠性，即使在感知信息不完善的情况下。

Abstract: Safety is a critical concern in autonomous vehicle (AV) systems, especially
when AI-based sensing and perception modules are involved. However, due to the
black box nature of AI algorithms, it makes closed-loop analysis and synthesis
particularly challenging, for example, establishing closed-loop stability and
ensuring performance, while they are fundamental to AV safety. To approach this
difficulty, this paper aims to develop new modeling, analysis, and synthesis
tools for AI-based AVs. Inspired by recent developments in perception error
models (PEMs), the focus is shifted from directly modeling AI-based perception
processes to characterizing the perception errors they produce. Two key classes
of AI-induced perception errors are considered: misdetection and measurement
noise. These error patterns are modeled using continuous-time Markov chains and
Wiener processes, respectively. By means of that, a PEM-augmented driving model
is proposed, with which we are able to establish the closed-loop stability for
a class of AI-driven AV systems via stochastic calculus. Furthermore, a
performance-guaranteed output feedback control synthesis method is presented,
which ensures both stability and satisfactory performance. The method is
formulated as a convex optimization problem, allowing for efficient numerical
solutions. The results are then applied to an adaptive cruise control (ACC)
scenario, demonstrating their effectiveness and robustness despite the
corrupted and misleading perception.

</details>


### [368] [Design and Optimization of EV Charging Infrastructure with Battery in Commercial Buildings](https://arxiv.org/abs/2509.12160)
*Quan Nguyen,Christine Holland,Siddharth Sridhar*

Main category: eess.SY

TL;DR: 本论文评估了在电动汽车普及背景下，商业建筑中服务变压器和电池储能系统的优化配置以及电动汽车充电、储能系统运行和建筑需求之间的协调策略，以支持电网可靠供电和经济运行。


<details>
  <summary>Details</summary>
Motivation: 随着各国推动电动汽车普及以实现脱碳目标，在建筑物中安装电动汽车充电站已不可避免。这可能导致电网基础设施升级和控制增强的需求，以确保对终端负载的可靠供电和整体经济运行。

Method: 本研究采用滚动窗口优化方法，解决两个主要问题：1) 服务变压器和电池储能系统（BESS）的优化规模确定；2) 电动汽车充电、BESS运行和建筑需求之间的优化协调。这些策略在一个包含建筑和电动汽车充电负荷的学校园区环境中进行应用，并通过20年时间跨度（电动汽车数量逐年增加）进行验证，同时进行经济分析以评估中长期投资的成本和效益。

Result: 研究将所提出的设计和控制策略应用于学校园区环境，以说明商业建筑中电动汽车车队的能源管理。这些策略在20年时间范围内（电动汽车数量逐年增加）得到了验证，并且进行了经济分析，展示了每种设计作为中长期投资的成本和效益。

Conclusion: 本论文提出的策略能够有效应对电动汽车普及带来的电网基础设施升级、可靠供电和经济运行需求，并通过对服务变压器和BESS的优化配置以及能源协调管理，为商业建筑电动汽车车队提供能源管理解决方案，并从经济角度评估了其长期投资价值。

Abstract: The installation of electric vehicle (EV) charging stations in buildings is
inevitable, as states push for increased EV adoption to support decarbonization
efforts. This transition could force the need for grid infrastructure upgrades
and enhanced controls to support reliable power delivery to end-use loads, and
overall economic operation. This paper evaluates strategies that address these
needs on two fronts: i) optimal sizing of service transformers and battery
energy storage systems (BESS), and ii) optimized coordination between EV
charging, BESS operation, and building demand. These strategies are applied to
a school campus setting, consisting of building and EV charging loads, to
provide an illustration of energy management in commercial buildings with EV
fleets. A rolling-window optimization approach is applied to determine i)
optimal sizing of the service transformer and BESS and ii) optimal control of
EV charging and BESS charge/discharge schedules. The design and control
strategies are validated in a 20-year time horizon with an annually increasing
number of EVs (buses and vans). In addition, an economic analysis is also
carried out to show the costs and benefits of each design as a medium- and
long-term investment.

</details>


### [369] [Approaches to Analysis and Design of AI-Based Autonomous Vehicles](https://arxiv.org/abs/2509.12169)
*Tao Yan,Zheyu Zhang,Jingjing Jiang,Wen-Hua Chen*

Main category: eess.SY

TL;DR: 本文针对自动驾驶中AI感知引入的不确定性，提出了一种新颖的建模方法，并严格分析了AI自动驾驶车辆的闭环随机稳定性、鲁棒性和性能，提供了基于LMI的控制器综合和优化设计方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型在自动驾驶车辆（AV）中处理复杂感知任务时变得至关重要，但基于AI的反馈由于对AI驱动感知过程机制理解有限，可能对自动驾驶的可靠性构成重大风险。

Method: 本文通过分析AI驱动感知过程的误差特性，提出了一种新颖的建模方法。具体识别并建模了三种基本的AI诱导感知不确定性（马尔可夫链、高斯过程、有界扰动）。在此基础上，建立了均方意义上的闭环随机稳定性（SS），并提出了一种基于线性矩阵不等式（LMI）的SS控制综合方法。此外，通过随机保证成本讨论了鲁棒性和性能，并给出了测试鲁棒性水平的标准。最后，研究了随机最优保证成本控制，并开发了基于LMI技术和凸优化的有效设计程序，并通过跟车控制示例进行了验证。

Result: 开发了AI驱动感知过程的新颖建模方法，识别并建模了三种AI诱导感知不确定性。建立了AI自动驾驶车辆的闭环均方随机稳定性，并提出了基于LMI的SS控制综合方法。提供了在存在AI诱导不确定性时测试AV鲁棒性水平的准则，并开发了基于LMI和凸优化的随机最优保证成本控制的高效设计程序。仿真结果验证了所开发方法的有效性。

Conclusion: 本文成功开发了用于AI自动驾驶车辆建模、分析和综合的工具，严格研究了其闭环特性（如稳定性、鲁棒性和性能），并提供了有效的控制器设计方法，能够有效应对AI感知引入的不确定性，并通过实际案例验证了其有效性。

Abstract: Artificial intelligence (AI) models are becoming key components in an
autonomous vehicle (AV), especially in handling complicated perception tasks.
However, closing the loop through AI-based feedback may pose significant risks
on reliability of autonomous driving due to very limited understanding about
the mechanism of AI-driven perception processes. To overcome it, this paper
aims to develop tools for modeling, analysis, and synthesis for a class of
AI-based AV; in particular, their closed-loop properties, e.g., stability,
robustness, and performance, are rigorously studied in the statistical sense.
First, we provide a novel modeling means for the AI-driven perception processes
by looking at their error characteristics. Specifically, three fundamental
AI-induced perception uncertainties are recognized and modeled by Markov
chains, Gaussian processes, and bounded disturbances, respectively. By means of
that, the closed-loop stochastic stability (SS) is established in the sense of
mean square, and then, an SS control synthesis method is presented within the
framework of linear matrix inequalities (LMIs). Besides the SS properties, the
robustness and performance of AI-based AVs are discussed in terms of a
stochastic guaranteed cost, and criteria are given to test the robustness level
of an AV when in the presence of AI-induced uncertainties. Furthermore, the
stochastic optimal guaranteed cost control is investigated, and an efficient
design procedure is developed innovatively based on LMI techniques and convex
optimization. Finally, to illustrate the effectiveness, the developed results
are applied to an example of car following control, along with extensive
simulation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [370] [MIDOG 2025 Track 2: A Deep Learning Model for Classification of Atypical and Normal Mitotic Figures under Class and Hardness Imbalances](https://arxiv.org/abs/2509.10502)
*Sujatha Kotte,Vangala Govindakrishnan Saipradeep,Vidushi Walia,Dhandapani Nandagopal,Thomas Joseph,Naveen Sivadasan,Bhagat Singh Lali*

Main category: eess.IV

TL;DR: 该研究提出了一种基于ResNet和专门分类头的新型深度学习方法，用于数字病理学中正常和非典型有丝分裂像的分类，有效解决了数据不平衡和形态差异细微的挑战，并取得了良好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在数字病理学中，准确分类有丝分裂像（正常与非典型）对于肿瘤预后至关重要。然而，由于形态差异细微、类别和难度严重不平衡，开发鲁棒的深度学习模型面临挑战。

Method: 该方法基于ResNet骨干网络，并配备了专门的分类头，能够同时建模有丝分裂像的表型和实例难度。为应对多样组织类型、扫描仪差异和数据不平衡，采用了焦点损失（focal loss）来缓解类别不平衡问题，并实施了全面的数据增强管道以提高模型的鲁棒性和泛化能力。

Result: 在MIDOG 2025 Track 2数据集上进行5折交叉验证，该方法实现了0.8744 +/- 0.0093的平均平衡准确率和0.9505 +/- 0.029的ROC AUC。在初步排行榜评估中，模型表现出鲁棒的泛化能力，总体平衡准确率为0.8736 +/- 0.0204。

Conclusion: 所提出的方法为非典型和正常有丝分裂像的分类提供了一个可靠且可泛化的解决方案。通过解决真实世界数据固有的挑战，该方法有潜力支持临床实践中的精确预后评估，并提高病理诊断的一致性。

Abstract: Motivation: Accurate classification of mitotic figures into normal and
atypical types is crucial for tumor prognostication in digital pathology.
However, developing robust deep learning models for this task is challenging
due to the subtle morphological differences, as well as significant class and
hardness imbalances in real-world histopathology datasets. Methods: We propose
a novel deep learning approach based on a ResNet backbone with specialized
classification heads. Our architecture uniquely models both the mitotic figure
phenotype and the instance difficulty simultaneously. This method is
specifically designed to handle the challenges of diverse tissue types, scanner
variability, and imbalanced data. We employed focal loss to effectively
mitigate the pronounced class imbalance, and a comprehensive data augmentation
pipeline was implemented to enhance the model's robustness and
generalizability. Results: Our approach demonstrated strong and consistent
performance. In a 5-fold cross-validation on the MIDOG 2025 Track 2 dataset, it
achieved a mean balanced accuracy of 0.8744 +/- 0.0093 and an ROC AUC of 0.9505
+/- 0.029. The model showed robust generalization across preliminary
leaderboard evaluations, achieving an overall balanced accuracy of 0.8736 +/-
0.0204. Conclusion: The proposed method offers a reliable and generalizable
solution for the classification of atypical and normal mitotic figures. By
addressing the inherent challenges of real world data, our approach has the
potential to support precise prognostic assessments in clinical practice and
improve consistency in pathological diagnosis.

</details>


### [371] [FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules for Interpretable Medical Image Classification](https://arxiv.org/abs/2509.10510)
*Prajit Sengupta,Islem Rekik*

Main category: eess.IV

TL;DR: 提出FireGNN，一个将可训练模糊规则集成到图神经网络（GNN）中的可解释学习框架，用于医学图像分类。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类不仅需要高预测性能，还需要可解释性以获得临床信任和采用。然而，标准的GNN通常是黑盒模型，限制了透明度和可用性，尤其是在临床环境中。

Method: FireGNN框架将可训练模糊规则集成到GNN中。这些规则使用可学习的阈值和锐度参数嵌入拓扑描述符（节点度、聚类系数、标签一致性），以实现内在的符号推理。此外，还探索了辅助自监督任务（如同质性预测、相似性熵）作为基准，以评估拓扑学习的贡献。

Result: 该模糊规则增强模型在五个MedMNIST基准和合成数据集MorphoMNIST上取得了强大的性能，同时生成了可解释的基于规则的解释。据作者所知，这是首次将可训练模糊规则集成到GNN中。

Conclusion: FireGNN提供了一个新颖、可解释的基于图的学习框架，通过将可训练模糊规则集成到GNN中，成功应用于医学图像分类，解决了临床应用对透明度的需求。

Abstract: Medical image classification requires not only high predictive performance
but also interpretability to ensure clinical trust and adoption. Graph Neural
Networks (GNNs) offer a powerful framework for modeling relational structures
within datasets; however, standard GNNs often operate as black boxes, limiting
transparency and usability, particularly in clinical settings. In this work, we
present an interpretable graph-based learning framework named FireGNN that
integrates trainable fuzzy rules into GNNs for medical image classification.
These rules embed topological descriptors - node degree, clustering
coefficient, and label agreement - using learnable thresholds and sharpness
parameters to enable intrinsic symbolic reasoning. Additionally, we explore
auxiliary self-supervised tasks (e.g., homophily prediction, similarity
entropy) as a benchmark to evaluate the contribution of topological learning.
Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST
benchmarks and the synthetic dataset MorphoMNIST, while also generating
interpretable rule-based explanations. To our knowledge, this is the first
integration of trainable fuzzy rules within a GNN.

</details>


### [372] [Data-Efficient Psychiatric Disorder Detection via Self-supervised Learning on Frequency-enhanced Brain Networks](https://arxiv.org/abs/2509.10524)
*Mujie Liu,Mengchu Zhu,Qichao Dong,Ting Dang,Jiangang Ma,Jing Ren,Feng Xia*

Main category: eess.IV

TL;DR: FENet是一种新颖的自监督学习框架，通过整合时域和频域信息，提高了小样本fMRI数据中精神疾病的检测准确性，并强调了高频信息的重要性。


<details>
  <summary>Details</summary>
Motivation: 精神疾病涉及复杂的神经活动变化，fMRI数据是关键诊断证据。然而，fMRI数据稀缺且信息多样，现有的基于图的自监督学习方法主要关注时域表示，往往忽略了频域中丰富的潜在信息。

Method: 本文提出了FENet框架：1. 构建基于fMRI数据固有特性的多视图脑网络，明确将频率信息整合到表示学习中。2. 采用领域特定编码器（包括高效的频域编码器）来捕获时空-频谱特征，突出疾病相关的频率特征。3. 引入领域一致性引导的学习目标，平衡多样化信息的利用，生成频率增强的脑图表示。

Result: 在两个真实世界医疗数据集上的实验表明，FENet优于现有最先进的方法，并在数据量极小的情况下仍能保持强大的性能。此外，研究分析了各种频域特征与精神疾病之间的相关性，强调了高频信息在疾病检测中的关键作用。

Conclusion: FENet通过有效整合时域和频域信息，显著提升了小样本fMRI数据中精神疾病的检测能力，特别是揭示了高频信息在疾病诊断中的关键价值。

Abstract: Psychiatric disorders involve complex neural activity changes, with
functional magnetic resonance imaging (fMRI) data serving as key diagnostic
evidence. However, data scarcity and the diverse nature of fMRI information
pose significant challenges. While graph-based self-supervised learning (SSL)
methods have shown promise in brain network analysis, they primarily focus on
time-domain representations, often overlooking the rich information embedded in
the frequency domain. To overcome these limitations, we propose
Frequency-Enhanced Network (FENet), a novel SSL framework specially designed
for fMRI data that integrates time-domain and frequency-domain information to
improve psychiatric disorder detection in small-sample datasets. FENet
constructs multi-view brain networks based on the inherent properties of fMRI
data, explicitly incorporating frequency information into the learning process
of representation. Additionally, it employs domain-specific encoders to capture
temporal-spectral characteristics, including an efficient frequency-domain
encoder that highlights disease-relevant frequency features. Finally, FENet
introduces a domain consistency-guided learning objective, which balances the
utilization of diverse information and generates frequency-enhanced brain graph
representations. Experiments on two real-world medical datasets demonstrate
that FENet outperforms state-of-the-art methods while maintaining strong
performance in minimal data conditions. Furthermore, we analyze the correlation
between various frequency-domain features and psychiatric disorders,
emphasizing the critical role of high-frequency information in disorder
detection.

</details>


### [373] [An Interpretable Ensemble Framework for Multi-Omics Dementia Biomarker Discovery Under HDLSS Conditions](https://arxiv.org/abs/2509.10527)
*Byeonghee Lee,Joonsung Kang*

Main category: eess.IV

TL;DR: 本文提出了一种结合图注意力网络、多组学变分自编码器、弹性网络稀疏回归和Storey's FDR的集成方法，用于在样本量低的情况下，从高维多组学数据中发现神经退行性疾病的生物标志物，并在模拟数据和ADNI数据集上表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 神经退行性疾病的生物标志物发现需要鲁棒、可解释的框架，该框架能够整合低样本条件下的高维多组学数据。

Method: 本文提出了一种新颖的集成方法，结合了图注意力网络（GAT）、多组学变分自编码器（MOVE）、弹性网络稀疏回归和Storey's假发现率（FDR）。该框架与DIABLO、MOCAT、AMOGEL和MOMLIN等现有先进方法进行了基准测试，并使用模拟多组学数据和阿尔茨海默病神经影像学倡议（ADNI）数据集进行性能评估。

Result: 该方法在预测准确性、特征选择精度和生物学相关性方面表现出卓越的性能。从两个数据集中得到的生物标志物基因图谱被可视化和解释，为痴呆症潜在的分子机制提供了见解。

Conclusion: 该集成方法能够有效地发现神经退行性疾病的生物标志物，并为理解痴呆症的潜在分子机制提供了深入的见解。

Abstract: Biomarker discovery in neurodegenerative diseases requires robust,
interpretable frameworks capable of integrating high-dimensional multi-omics
data under low-sample conditions. We propose a novel ensemble approach
combining Graph Attention Networks (GAT), MultiOmics Variational AutoEncoder
(MOVE), Elastic-net sparse regression, and Storey's False Discovery Rate (FDR).
This framework is benchmarked against state-of-the-art methods including
DIABLO, MOCAT, AMOGEL, and MOMLIN. We evaluate performance using both simulated
multi-omics data and the Alzheimer's Disease Neuroimaging Initiative (ADNI)
dataset. Our method demonstrates superior predictive accuracy, feature
selection precision, and biological relevance. Biomarker gene maps derived from
both datasets are visualized and interpreted, offering insights into latent
molecular mechanisms underlying dementia.

</details>


### [374] [Automated Cervical Os Segmentation for Camera-Guided, Speculum-Free Screening](https://arxiv.org/abs/2509.10593)
*Aoife McDonald-Bowyer,Anjana Wijekoon,Ryan Laurance Love,Katie Allan,Scott Colvin,Aleksandra Gentry-Maharaj,Adeola Olaitan,Danail Stoyanov,Agostino Stilli,Sophia Bano*

Main category: eess.IV

TL;DR: 本研究评估了深度学习方法在无窥器宫颈筛查设备中实时分割宫颈口的能力，发现基于视觉Transformer的模型表现最佳，为提高筛查可及性奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 宫颈癌可预防，但筛查障碍限制了消除目标。无窥器设备（集成成像和采样）能改善可及性，尤其在资源匮乏地区，但需要可靠的视觉引导（如宫颈口识别）。

Method: 本研究比较了五种编码器-解码器深度学习架构，用于阴道镜图像中宫颈口的实时分割。使用IARC宫颈图像数据集中的913帧（200个病例），由妇科医生标注。通过IoU、DICE、检测率和距离指标进行性能评估，并采用十折交叉验证。

Result: 在手术视频上预训练的视觉Transformer模型EndoViT/DPT表现最佳，DICE得分最高（0.50 ± 0.31），检测率最高（0.87 ± 0.33），优于基于CNN的方法。通过模型数据进行的外部验证显示，在可变条件下，该模型能以21.5 FPS的速度进行鲁棒分割，支持实时可行性。

Conclusion: 这些结果为将自动宫颈口识别集成到无窥器宫颈筛查设备中奠定了基础，以支持非专业人员在高资源和低资源环境中的使用。

Abstract: Cervical cancer is highly preventable, yet persistent barriers to screening
limit progress toward elimination goals. Speculum-free devices that integrate
imaging and sampling could improve access, particularly in low-resource
settings, but require reliable visual guidance. This study evaluates deep
learning methods for real-time segmentation of the cervical os in transvaginal
endoscopic images. Five encoder-decoder architectures were compared using 913
frames from 200 cases in the IARC Cervical Image Dataset, annotated by
gynaecologists. Performance was assessed using IoU, DICE, detection rate, and
distance metrics with ten-fold cross-validation. EndoViT/DPT, a vision
transformer pre-trained on surgical video, achieved the highest DICE (0.50 \pm
0.31) and detection rate (0.87 \pm 0.33), outperforming CNN-based approaches.
External validation with phantom data demonstrated robust segmentation under
variable conditions at 21.5 FPS, supporting real-time feasibility. These
results establish a foundation for integrating automated os recognition into
speculum-free cervical screening devices to support non-expert use in both
high- and low-resource contexts.

</details>


### [375] [Language-based Color ISP Tuning](https://arxiv.org/abs/2509.10765)
*Owen Mayer,Shohei Noguchi,Alexander Berestov,Jiro Takatori*

Main category: eess.IV

TL;DR: 本文提出了一种使用语言提示来调整图像信号处理器（ISP）中颜色调整算法模块参数的方法，使用户能够通过文本描述为图像赋予特定的视觉风格。


<details>
  <summary>Details</summary>
Motivation: 研究动机是使用户能够简单地通过文本描述来为ISP处理后的图像赋予特定的视觉风格，从而简化颜色调整参数的设置过程。

Method: 该方法首先以可微分的方式实现ISP模块，然后使用预训练的视觉-语言模型（VLM）定义一个目标函数，使得当ISP处理后的图像与输入语言提示在视觉上最相似时，该目标函数最小化。最后，使用梯度下降优化ISP参数。

Result: 实验结果表明，该方法能够使用不同的语言提示来调整ISP参数，并比较了不同预训练VLM和优化策略的性能。

Conclusion: 该研究成功展示了通过语言提示对ISP参数进行调整的可行性，实现了文本驱动的图像视觉风格定制。

Abstract: We propose a method for tuning the parameters of a color adjustment Image
Signal Processor (ISP) algorithmic "block" using language prompts. This enables
the user to impart a particular visual style to the ISP-processed image simply
by describing it through a text prompt. To do this, we first implement the ISP
block in a differentiable manner. Then, we define an objective function using
an off-the-shelf, pretrained vision-language model (VLM) such that the
objective is minimized when the ISP processed image is most visually similar to
the input language prompt. Finally, we optimize the ISP parameters using
gradient descent. Experimental results demonstrate tuning of ISP parameters
with different language prompts, and compare the performance of different
pretrained VLMs and optimization strategies.

</details>


### [376] [Adapting Medical Vision Foundation Models for Volumetric Medical Image Segmentation via Active Learning and Selective Semi-supervised Fine-tuning](https://arxiv.org/abs/2509.10784)
*Jin Yang,Daniel S. Marcus,Aristeidis Sotiras*

Main category: eess.IV

TL;DR: 本文提出了一种主动无源域适应（ASFDA）方法，通过设计新颖的主动学习策略，高效地将医学视觉基础模型（Med-VFMs）适应到目标域，用于体视医学图像分割，以最小化样本选择预算最大化性能。


<details>
  <summary>Details</summary>
Motivation: 医学视觉基础模型（Med-VFMs）在解释医学图像方面表现出色，但将其高效适应到下游评估任务（尤其是分割）仍缺乏深入探索。当前随机选择少量样本进行微调的方式效率低下，因此迫切需要设计一种通过选择信息量大的样本来最大化目标域适应性能的高效微调方法。

Method: 本文提出主动无源域适应（ASFDA）方法。该方法采用新颖的主动学习（AL）策略，通过“主动测试时样本查询”来选择目标域中最具信息量的样本进行微调，且无需访问源预训练样本。该查询策略使用两个度量标准：1) 多样化知识散度（DKD），用于衡量源-目标知识差距和域内多样性；2) 解剖分割难度（ASD），通过自适应测量前景区域的预测熵来评估解剖结构分割的难度。此外，ASFDA还采用选择性半监督微调，通过识别未查询样本中可靠性高的样本来提高微调性能和效率。

Result: 该方法旨在通过选择信息量最大的样本，在最小化样本选择预算的情况下，最大化医学视觉基础模型在目标域上的适应性能，特别是在体视医学图像分割任务中的表现和效率。

Conclusion: 本文提出了一种名为ASFDA的主动无源域适应方法，通过结合DKD和ASD两种查询指标的主动学习策略，以及选择性半监督微调，实现了医学视觉基础模型在体视医学图像分割任务中高效、高性能的域适应。

Abstract: Medical Vision Foundation Models (Med-VFMs) have superior capabilities of
interpreting medical images due to the knowledge learned from self-supervised
pre-training with extensive unannotated images. To improve their performance on
adaptive downstream evaluations, especially segmentation, a few samples from
target domains are selected randomly for fine-tuning them. However, there lacks
works to explore the way of adapting Med-VFMs to achieve the optimal
performance on target domains efficiently. Thus, it is highly demanded to
design an efficient way of fine-tuning Med-VFMs by selecting informative
samples to maximize their adaptation performance on target domains. To achieve
this, we propose an Active Source-Free Domain Adaptation (ASFDA) method to
efficiently adapt Med-VFMs to target domains for volumetric medical image
segmentation. This ASFDA employs a novel Active Learning (AL) method to select
the most informative samples from target domains for fine-tuning Med-VFMs
without the access to source pre-training samples, thus maximizing their
performance with the minimal selection budget. In this AL method, we design an
Active Test Time Sample Query strategy to select samples from the target
domains via two query metrics, including Diversified Knowledge Divergence (DKD)
and Anatomical Segmentation Difficulty (ASD). DKD is designed to measure the
source-target knowledge gap and intra-domain diversity. It utilizes the
knowledge of pre-training to guide the querying of source-dissimilar and
semantic-diverse samples from the target domains. ASD is designed to evaluate
the difficulty in segmentation of anatomical structures by measuring predictive
entropy from foreground regions adaptively. Additionally, our ASFDA method
employs a Selective Semi-supervised Fine-tuning to improve the performance and
efficiency of fine-tuning by identifying samples with high reliability from
unqueried ones.

</details>


### [377] [Branched Broomrape Detection in Tomato Farms Using Satellite Imagery and Time-Series Analysis](https://arxiv.org/abs/2509.10804)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Parastoo Farajpoor,Hamid Jafarbiglu,Mohsen Mesgaran*

Main category: eess.IV

TL;DR: 该研究利用Sentinel-2卫星图像和时间序列分析，通过LSTM网络成功开发了一个端到端管道，用于在番茄田中大规模检测列当寄生植物，取得了87%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 分枝列当是一种对番茄生产构成严重威胁的寄生植物，可导致高达80%的产量损失。其地下生命周期和大量长寿命种子的特点，使得早期检测对于控制其蔓延至关重要。

Method: 研究构建了一个端到端管道：首先，利用Sentinel-2卫星图像（选择云量低于10%的图像）和农户报告的侵染区域定义感兴趣区。接着，处理12个光谱波段和日地传感器几何，计算20个植被指数（如NDVI、NDMI），并使用经过地面真值和合成数据校准的神经网络推导出5种植物性状（如叶面积指数、冠层叶绿素含量）。然后，利用冠层叶绿素含量趋势划定移植到收获期，并使用生长积温校准物候。最后，分割植被像素，并使用长短期记忆（LSTM）网络在48个生长积温时间点上的18,874个像素进行训练。通过置换特征重要性评估了特征的贡献。

Result: 该模型在训练集上达到了88%的准确率，在测试集上达到了87%的准确率，查准率0.86，查全率0.92，F1分数0.89。置换特征重要性分析表明，NDMI、冠层叶绿素含量、光合有效辐射吸收分数（FAPAR）和叶绿素红边指数是最具信息量的特征，这与列当侵染的生理效应一致。

Conclusion: 研究结果表明，卫星驱动的时间序列建模在番茄农场中大规模检测寄生胁迫方面具有广阔前景。

Abstract: Branched broomrape (Phelipanche ramosa (L.) Pomel) is a chlorophyll-deficient
parasitic plant that threatens tomato production by extracting nutrients from
the host, with reported yield losses up to 80 percent. Its mostly subterranean
life cycle and prolific seed production (more than 200,000 seeds per plant,
viable for up to 20 years) make early detection essential. We present an
end-to-end pipeline that uses Sentinel-2 imagery and time-series analysis to
identify broomrape-infested tomato fields in California. Regions of interest
were defined from farmer-reported infestations, and images with less than 10
percent cloud cover were retained. We processed 12 spectral bands and
sun-sensor geometry, computed 20 vegetation indices (e.g., NDVI, NDMI), and
derived five plant traits (Leaf Area Index, Leaf Chlorophyll Content, Canopy
Chlorophyll Content, Fraction of Absorbed Photosynthetically Active Radiation,
and Fractional Vegetation Cover) using a neural network calibrated with
ground-truth and synthetic data. Trends in Canopy Chlorophyll Content
delineated transplanting-to-harvest periods, and phenology was aligned using
growing degree days. Vegetation pixels were segmented and used to train a Long
Short-Term Memory (LSTM) network on 18,874 pixels across 48 growing-degree-day
time points. The model achieved 88 percent training accuracy and 87 percent
test accuracy, with precision 0.86, recall 0.92, and F1 0.89. Permutation
feature importance ranked NDMI, Canopy Chlorophyll Content, FAPAR, and a
chlorophyll red-edge index as most informative, consistent with the
physiological effects of infestation. Results show the promise of
satellite-driven time-series modeling for scalable detection of parasitic
stress in tomato farms.

</details>


### [378] [The Microwave Rainbow: How Geometry Paints Colours in Microwave Vision](https://arxiv.org/abs/2509.11099)
*Huizhang Yang*

Main category: eess.IV

TL;DR: 研究揭示了合成孔径雷达（SAR）图像中人造结构的“微波彩虹”效应，这是一种几何色散现象。通过建立几何-物理模型，将目标几何与观测到的“颜色”特征联系起来，从而实现从太空直接测绘物体几何形态的新型遥感方式。


<details>
  <summary>Details</summary>
Motivation: 尽管星载SAR提供了全天候、昼夜观测地球的能力，但其信号中仍有大量信息未被破译。高分辨率图像显示人造结构呈现出“颜色光谱”，其物理起源尚不清楚。

Method: 引入了一个几何-物理模型。该模型将结构视为固有的衍射光栅，解释了“微波彩虹”是几何色散的一种形式。它提供了目标几何与其观测到的“颜色”特征之间的直接分析联系，涵盖了从曲面上的连续颜色梯度（零级衍射）到周期性结构的重复光谱模式（高级衍射）。

Result: 该模型定量解释了观察到的所有“颜色”特征，将“颜色”从视觉假象转变为物理形态的精确度量。这使得关键基础设施和自然现象的几何形状能够直接从太空进行测绘。

Conclusion: 这项工作确立了“微波彩色视觉”作为一种新型遥感模式的物理基础，为我们感知世界的方式开辟了新领域，能够直接从太空测绘物体几何形态。

Abstract: Microwave vision from spaceborne synthetic aperture radar (SAR) provides an
all-weather, day-and-night capability to observe Earth, yet much of the
information encoded in its signals remains undeciphered. Recent high-resolution
imagery has revealed a striking phenomenon: man-made structures systematically
appear in a spectrum of colours, the physical origin of which has been an open
question. Here we show that this effect, which we term the microwave rainbow,
is a form of geometric dispersion arising from structures acting as intrinsic
diffraction gratings. We introduce a geometric-physical model that provides a
direct analytical link between a target's geometry and its observed colour
signature. This model quantitatively explains the full range of signatures,
from continuous colour gradients on curved surfaces (zero-order diffraction) to
repeating spectral patterns from periodic structures (high-order diffraction).
This work transforms colour from a visual artefact into a precise measure of
physical form, enabling the geometry of both critical infrastructure and
natural phenomena to be mapped directly from space. Our findings establish the
physical basis for a new remote sensing modality: microwave colour vision, and
open a new frontier in how we perceive our world.

</details>


### [379] [UltraUPConvNet: A UPerNet- and ConvNeXt-Based Multi-Task Network for Ultrasound Tissue Segmentation and Disease Prediction](https://arxiv.org/abs/2509.11108)
*Zhi Chen*

Main category: eess.IV

TL;DR: UltraUPConvNet是一种计算高效的通用框架，用于超声图像分类和分割，在大规模数据集上实现了先进性能且计算开销较低。


<details>
  <summary>Details</summary>
Motivation: 超声成像在临床中广泛应用，但当前的AI研究将疾病预测（分类）和组织分割视为独立任务，且模型需要大量的计算开销。

Method: 引入UltraUPConvNet，一个计算高效的通用框架，旨在同时处理超声图像分类和分割。该模型在一个包含超过9,700个注释（涵盖七个不同解剖区域）的大规模数据集上进行训练。

Result: 该模型在某些数据集上以较低的计算开销实现了最先进的性能。

Conclusion: UltraUPConvNet是一个计算高效且性能优越的通用框架，能够有效地处理超声图像的分类和分割任务。

Abstract: Ultrasound imaging is widely used in clinical practice due to its
cost-effectiveness, mobility, and safety. However, current AI research often
treats disease prediction and tissue segmentation as two separate tasks and
their model requires substantial computational overhead. In such a situation,
we introduce UltraUPConvNet, a computationally efficient universal framework
designed for both ultrasound image classification and segmentation. Trained on
a large-scale dataset containing more than 9,700 annotations across seven
different anatomical regions, our model achieves state-of-the-art performance
on certain datasets with lower computational overhead. Our model weights and
codes are available at https://github.com/yyxl123/UltraUPConvNet

</details>


### [380] [EMeRALDS: Electronic Medical Record Driven Automated Lung Nodule Detection and Classification in Thoracic CT Images](https://arxiv.org/abs/2509.11714)
*Hafza Eman,Furqan Shaukat,Muhammad Hamza Zafar,Syed Muhammad Anwar*

Main category: eess.IV

TL;DR: 本研究开发了一个基于大型视觉-语言模型（VLMs）的计算机辅助诊断（CAD）系统，用于CT扫描中肺结节的准确检测和分类，结合放射组学和合成电子病历，在零样本设置下表现出色。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症相关死亡的主要原因，主要由于诊断延迟和早期检测不力。因此，开发一个能够准确检测和分类肺部结节的计算机辅助诊断（CAD）系统至关重要。

Method: 该研究提出了一个端到端的CAD流程，包含两个模块：(i) 检测模块（CADe），基于Segment Anything Model 2 (SAM2)，将标准视觉提示替换为由CLIP编码的文本提示；(ii) 诊断模块（CADx），计算分割结节与放射组学特征之间的相似性分数。为了增加临床背景，通过专家放射科医师的放射组学评估生成合成电子病历（EMRs），并与相似性分数结合进行最终分类。该方法在LIDC-IDRI数据集（1,018次CT扫描）上进行了测试。

Result: 该方法在零样本肺结节分析中表现出强大的性能。CADe模块在结节分割方面实现了0.92的Dice分数和0.85的IoU。CADx模块在恶性肿瘤分类方面达到了0.97的特异性，超越了现有的全监督方法。

Conclusion: 将视觉-语言模型与放射组学和合成电子病历相结合，可以实现CT扫描中肺结节的准确且具有临床相关性的计算机辅助诊断。该系统在增强早期肺癌检测、提高诊断信心和改善常规临床工作流程中的患者管理方面显示出巨大潜力。

Abstract: Objective: Lung cancer is a leading cause of cancer-related mortality
worldwide, primarily due to delayed diagnosis and poor early detection. This
study aims to develop a computer-aided diagnosis (CAD) system that leverages
large vision-language models (VLMs) for the accurate detection and
classification of pulmonary nodules in computed tomography (CT) scans.
  Methods: We propose an end-to-end CAD pipeline consisting of two modules: (i)
a detection module (CADe) based on the Segment Anything Model 2 (SAM2), in
which the standard visual prompt is replaced with a text prompt encoded by CLIP
(Contrastive Language-Image Pretraining), and (ii) a diagnosis module (CADx)
that calculates similarity scores between segmented nodules and radiomic
features. To add clinical context, synthetic electronic medical records (EMRs)
were generated using radiomic assessments by expert radiologists and combined
with similarity scores for final classification. The method was tested on the
publicly available LIDC-IDRI dataset (1,018 CT scans).
  Results: The proposed approach demonstrated strong performance in zero-shot
lung nodule analysis. The CADe module achieved a Dice score of 0.92 and an IoU
of 0.85 for nodule segmentation. The CADx module attained a specificity of 0.97
for malignancy classification, surpassing existing fully supervised methods.
  Conclusions: The integration of VLMs with radiomics and synthetic EMRs allows
for accurate and clinically relevant CAD of pulmonary nodules in CT scans. The
proposed system shows strong potential to enhance early lung cancer detection,
increase diagnostic confidence, and improve patient management in routine
clinical workflows.

</details>


### [381] [Impact of a Sharpness Based Loss Function for Removing Out-of-Focus Blur](https://arxiv.org/abs/2509.11735)
*Uditangshu Aurangabadkar,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 本文探索了使用明确处理清晰度的Q损失函数来微调最先进的去模糊模型，并提出了一种新的全参考图像质量度量Omega，该度量结合了PSNR和Q，对振铃伪影敏感。实验结果显示，与使用标准损失函数相比，清晰度（Q）提高了15%，Omega提高了10%。


<details>
  <summary>Details</summary>
Motivation: 现有的标准图像质量度量（如PSNR或SSIM）无法区分清晰度和振铃伪影，这使得评估去模糊算法的性能变得不公平。同时，最近的研究正在探索更复杂的去模糊损失函数。

Method: 1. 探索了先前引入的Q损失函数，该函数明确处理图像清晰度。2. 使用Q损失函数对最先进（SOTA）的去模糊模型进行微调。3. 提出了一种新的全参考图像质量度量Omega，该度量结合了PSNR和Q。Omega对振铃伪影敏感，但对轻微的清晰度增加不敏感。

Result: 与使用标准损失函数相比，本文的方法在清晰度（Q）方面提高了15%，在Omega度量方面提高了10%。

Conclusion: 明确处理清晰度的Q损失函数能有效提升去模糊模型的性能，使其生成更清晰的图像。新提出的Omega度量结合了PSNR和Q，能更公平地评估去模糊机制的恢复效果，因为它能区分清晰度和振铃伪影，从而为去模糊算法的比较提供了一个更好的标准。

Abstract: Recent research has explored complex loss functions for deblurring. In this
work, we explore the impact of a previously introduced loss function - Q which
explicitly addresses sharpness and employ it to fine-tune State-of-the-Art
(SOTA) deblurring models. Standard image quality metrics such as PSNR or SSIM
do not distinguish sharpness from ringing. Therefore, we propose a novel
full-reference image quality metric Omega that combines PSNR with Q. This
metric is sensitive to ringing artefacts, but not to a slight increase in
sharpness, thus making it a fair metric for comparing restorations from
deblurring mechanisms. Our approach shows an increase of 15 percent in
sharpness (Q) and up to 10 percent in Omega over the use of standard losses.

</details>


### [382] [EyeNexus: Adaptive Gaze-Driven Quality and Bitrate Streaming for Seamless VR Cloud Gaming Experiences](https://arxiv.org/abs/2509.11807)
*Ze Wu,Ahmad Alhilal,Yuk Hang Tsui,Matti Siekkinen,Pan Hui*

Main category: eess.IV

TL;DR: EyeNexus是一个创新的VR云游戏系统，它结合了实时注视点驱动的空间压缩和视频编码，并动态调整注视点区域以适应网络条件，显著提升了性能、视觉质量和用户体验，同时减少了延迟和晕动症。


<details>
  <summary>Details</summary>
Motivation: 在VR云游戏中传输高分辨率游戏场景面临网络性能波动的挑战。现有注视点渲染和编码方法未能有效整合实时注视点数据或适应网络变化，导致用户体验不佳。

Method: 本文提出了EyeNexus系统，它结合了实时注视点驱动的空间压缩（FSC）和注视点驱动的视频编码（FVE），并精确对齐和注视点化注视点。该系统提出了一种新颖的注视点模型，根据实时带宽和注视点数据动态调整注视点区域，从而简化了FVE中的网络感知质量分配，确保平滑且不易察觉的质量梯度。

Result: EyeNexus将延迟降低高达70.9%，感知视觉质量提高高达24.6%。用户研究表明，EyeNexus在可玩性和视觉质量方面最高提升了48%，并消除了晕动症。

Conclusion: EyeNexus通过结合实时注视点驱动的空间压缩和视频编码，以及动态调整注视点区域以适应网络条件，显著提高了VR云游戏的性能、感知视觉质量和用户体验，同时有效减少了延迟和消除了晕动症。

Abstract: Virtual Reality (VR) cloud gaming systems render the 3D graphics on cloud
servers for playing graphically demanding games on VR headsets. Delivering
high-resolution game scenes is challenging due to variation in network
performance. By leveraging the non-uniform human vision perception, foveated
rendering and encoding have proven effective for optimized streaming in
constrained networks. SoTA foveation methods either do not incorporate
real-time gaze data or are unable to handle variations in network conditions,
resulting in a suboptimal user experience. We introduce EyeNexus, a pioneering
system that combines real-time gaze-driven spatial compression (FSC) with
gaze-driven video encoding (FVE), transforming the gaze point for precise
alignment and foveation. We propose a novel foveation model that dynamically
adjusts the foveation region based on real-time bandwidth and gaze data. The
model simplifies network-aware quality assignment in FVE, ensuring smooth and
imperceptible quality gradients. We evaluate EyeNexus using objective and
subjective measures with different network conditions and games. EyeNexus
reduces latency by up to 70.9% and improves perceptual visual quality by up to
24.6%. Our IRB-approved user study shows that EyeNexus achieves the highest
playability and visual quality, with improvements of up to 48%, while
eliminating motion sickness.

</details>


### [383] [The Filter Echo: A General Tool for Filter Visualisation](https://arxiv.org/abs/2509.11932)
*Daniel Gaa,Joachim Weickert,Iva Farag,Özgün Çiçek*

Main category: eess.IV

TL;DR: 本文将扩散回波泛化为滤波器回波，并提出一种压缩方法，以扩展其应用范围并显著降低存储需求，使其成为分析和改进各类滤波器的实用工具。


<details>
  <summary>Details</summary>
Motivation: 扩散回波对于理解非线性扩散滤波器的工作原理很有用，但在文献中受关注较少。原因有二：一是概念局限于扩散滤波器，应用范围有限；二是存储需求大，实用性受限。本文旨在解决这两个问题。

Method: 引入“滤波器回波”作为扩散回波的泛化，并将其应用于自适应平滑之外的任务，如图像修复、渗透和变分光流计算。同时，提出了滤波器回波的压缩方法。

Result: 滤波器回波可用于超越自适应平滑的多种应用，并提供了一个可视化和检查不同滤波器回波的框架。压缩方法将存储需求降低了20到100倍。

Conclusion: 滤波器回波是一个更通用且实用的工具，能够帮助深入理解和改进各种滤波器，其存储效率通过压缩得到了显著提升，拓宽了其应用前景。

Abstract: To select suitable filters for a task or to improve existing filters, a deep
understanding of their inner workings is vital. Diffusion echoes, which are
space-adaptive impulse responses, are useful to visualise the effect of
nonlinear diffusion filters. However, they have received little attention in
the literature. There may be two reasons for this: Firstly, the concept was
introduced specifically for diffusion filters, which might appear too limited.
Secondly, diffusion echoes have large storage requirements, which restricts
their practicality. This work addresses both problems. We introduce the filter
echo as a generalisation of the diffusion echo and use it for applications
beyond adaptive smoothing, such as image inpainting, osmosis, and variational
optic flow computation. We provide a framework to visualise and inspect echoes
from various filters with different applications. Furthermore, we propose a
compression approach for filter echoes, which reduces storage requirements by a
factor of 20 to 100.

</details>


### [384] [Data-driven Smile Design: Personalized Dental Aesthetics Outcomes Using Deep Learning](https://arxiv.org/abs/2509.12001)
*Marcus Lin,Jennifer Lai*

Main category: eess.IV

TL;DR: 本研究提出一个整合人工智能、大数据和识别技术的综合系统，旨在自动化微笑设计过程，帮助牙科专业人员轻松实现美观的微笑设计，并克服传统方法和现有数字工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的微笑设计高度依赖牙医经验，使用石膏模型和手绘，导致结果不确定。尽管数字技术和AI有所进步，但仍受限于操作者偏见和训练数据，可能无法为个体用户提供最佳方案。研究旨在解决美学和功能平衡的难题，并提高患者满意度。

Method: 本研究建议开发一个综合系统，该系统整合了人工智能、大数据和识别技术，以实现微笑设计的自动化。该系统包含一个面部特征提取模块和一个图像生成模块，旨在满足不同的医生和患者需求。

Result: 该系统旨在使有经验和无经验的牙医都能轻松生成令人满意的美学设计，从而自动化微笑设计过程。它能够满足多样化的从业者和患者需求。

Conclusion: 该系统有望通过自动化微笑设计过程，提高牙科专业人员的工作效率和设计质量。未来的研究将整合用户数据进行优化，探索虚拟和增强现实技术进行实时预览，并利用收集到的数据进行美学偏好分析，以增进对微笑设计的理解。

Abstract: A healthy smile plays a significant role in functional as well as esthetic
considerations, improving confidence. It is difficult for dental professionals
to strike a balance between esthetic requirements and functional requirements.
Traditional smile design has had heavy reliance on dentist expertise and used
plaster models and hand drawings, raising questions about the outcome for
patients. Digital technology, led by Dr. Christian Coachman in 2007, allows
photographic and videographic assessments, enabling improved intercommunication
among specialists and patients. Advances in artificial intelligence (AI) and
big data have supported analysis of facial features and development of
personalized smile designs in the last few years. Outputs are, however,
susceptible to practitioner bias or limitations of training data, and may be
suboptimal for individual users. The study presented here suggests a
comprehensive system integrating AI, big data, and recognition technologies to
automate the smile design process so that both experienced and inexperienced
dentists can generate pleasing aesthetics with ease. The system has a Facial
Feature Extraction Module and an Image Generation Module, serving diverse
practitioner and patient needs. User data can be incorporated in future
research for design optimization and testing of virtual and augmented reality
for real-time previewing. Data gathered can also be employed in aesthetic
preference analyses, which can enhance our knowledge of smile design in dental
practice.

</details>
