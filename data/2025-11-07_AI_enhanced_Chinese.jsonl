{"id": "2511.03762", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.03762", "abs": "https://arxiv.org/abs/2511.03762", "authors": ["Yundi Zhang", "Nil Stolt-Ansó", "Jiazhen Pan", "Wenqi Huang", "Kerstin Hammernik", "Daniel Rueckert"], "title": "Reconstruction-free segmentation from undersampled k-space using transformers", "comment": "Accepted by the conference ISMRM 2024\n  (https://archive.ismrm.org/2024/0656_WR8CHcQx6.html)", "summary": "Motivation: High acceleration factors place a limit on MRI image\nreconstruction. This limit is extended to segmentation models when treating\nthese as subsequent independent processes.\n  Goal: Our goal is to produce segmentations directly from sparse k-space\nmeasurements without the need for intermediate image reconstruction.\n  Approach: We employ a transformer architecture to encode global k-space\ninformation into latent features. The produced latent vectors condition queried\ncoordinates during decoding to generate segmentation class probabilities.\n  Results: The model is able to produce better segmentations across high\nacceleration factors than image-based segmentation baselines.\n  Impact: Cardiac segmentation directly from undersampled k-space samples\ncircumvents the need for an intermediate image reconstruction step. This allows\nthe potential to assess myocardial structure and function on higher\nacceleration factors than methods that rely on images as input.", "AI": {"tldr": "本文提出一种基于Transformer的方法，可直接从稀疏k空间数据进行心脏分割，无需中间图像重建，在高加速因子下表现优于基于图像的传统方法。", "motivation": "高加速因子限制了MRI图像重建的质量，进而影响后续的分割模型。研究目标是突破这一限制，直接从稀疏k空间测量中生成分割结果。", "method": "该方法采用Transformer架构将全局k空间信息编码为潜在特征。生成的潜在向量在解码过程中条件化查询坐标，以生成分割类别概率。", "result": "在高加速因子下，该模型能够生成比基于图像的分割基线更好的分割结果。", "conclusion": "直接从欠采样k空间样本进行心脏分割，避免了中间图像重建步骤。这使得在比依赖图像作为输入的方法更高的加速因子下评估心肌结构和功能成为可能。"}}
{"id": "2511.03931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03931", "abs": "https://arxiv.org/abs/2511.03931", "authors": ["Iman Adibnazari", "Harsh Sharma", "Myungsun Park", "Jacobo Cervera-Torralba", "Boris Kramer", "Michael T. Tolley"], "title": "Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction", "comment": "20 Pages, 8 Figures", "summary": "Soft robots have shown immense promise in settings where they can leverage\ndynamic control of their entire bodies. However, effective dynamic shape\ncontrol requires a controller that accounts for the robot's high-dimensional\ndynamics--a challenge exacerbated by a lack of general-purpose tools for\nmodeling soft robots amenably for control. In this work, we conduct a\ncomparative study of data-driven model reduction techniques for generating\nlinear models amendable to dynamic shape control. We focus on three\nmethods--the eigensystem realization algorithm, dynamic mode decomposition with\ncontrol, and the Lagrangian operator inference (LOpInf) method. Using each\nclass of model, we explored their efficacy in model predictive control policies\nfor the dynamic shape control of a simulated eel-inspired soft robot in three\nexperiments: 1) tracking simulated reference trajectories guaranteed to be\nfeasible, 2) tracking reference trajectories generated from a biological model\nof eel kinematics, and 3) tracking reference trajectories generated by a\nreduced-scale physical analog. In all experiments, the LOpInf-based policies\ngenerated lower tracking errors than policies based on other models.", "AI": {"tldr": "本文比较了三种数据驱动的模型降阶技术（ERA、DMDc、LOpInf）在软体机器人动态形状控制中的性能。结果显示，基于LOpInf的模型在模型预测控制中实现了最低的跟踪误差。", "motivation": "软体机器人具有巨大的潜力，但其高维动态特性使得有效的动态形状控制面临挑战。目前缺乏通用的工具来为控制目的建模软体机器人。", "method": "研究对比了三种数据驱动的模型降阶技术：特征系统实现算法（ERA）、带控制的动态模态分解（DMDc）和拉格朗日算子推断（LOpInf）方法。利用这些模型，研究人员探索了它们在模型预测控制（MPC）策略中用于模拟鳗鱼形软体机器人动态形状控制的有效性。实验包括跟踪可行的模拟轨迹、跟踪生物鳗鱼运动学轨迹以及跟踪缩减尺寸物理模拟器生成的轨迹。", "result": "在所有实验中，基于LOpInf策略的模型预测控制产生的跟踪误差均低于基于其他模型（ERA和DMDc）的策略。", "conclusion": "LOpInf方法在为软体机器人动态形状控制生成线性模型方面表现出优越性，能够实现更低的跟踪误差，从而为软体机器人控制提供了一种更有效的数据驱动建模方法。"}}
{"id": "2511.03890", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03890", "abs": "https://arxiv.org/abs/2511.03890", "authors": ["Linchen Qian", "Jiasong Chen", "Ruonan Gong", "Wei Sun", "Minliang Liu", "Liang Liang"], "title": "Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images", "comment": null, "summary": "Accurate geometric modeling of the aortic valve from 3D CT images is\nessential for biomechanical analysis and patient-specific simulations to assess\nvalve health or make a preoperative plan. However, it remains challenging to\ngenerate aortic valve meshes with both high-quality and consistency across\ndifferent patients. Traditional approaches often produce triangular meshes with\nirregular topologies, which can result in poorly shaped elements and\ninconsistent correspondence due to inter-patient anatomical variation. In this\nwork, we address these challenges by introducing a template-fitting pipeline\nwith deep neural networks to generate structured quad (i.e., quadrilateral)\nmeshes from 3D CT images to represent aortic valve geometries. By remeshing\naortic valves of all patients with a common quad mesh template, we ensure a\nuniform mesh topology with consistent node-to-node and element-to-element\ncorrespondence across patients. This consistency enables us to simplify the\nlearning objective of the deep neural networks, by employing a loss function\nwith only two terms (i.e., a geometry reconstruction term and a smoothness\nregularization term), which is sufficient to preserve mesh smoothness and\nelement quality. Our experiments demonstrate that the proposed approach\nproduces high-quality aortic valve surface meshes with improved smoothness and\nshape quality, while requiring fewer explicit regularization terms compared to\nthe traditional methods. These results highlight that using structured quad\nmeshes for the template and neural network training not only ensures mesh\ncorrespondence and quality but also simplifies the training process, thus\nenhancing the effectiveness and efficiency of aortic valve modeling.", "AI": {"tldr": "本文提出了一种基于深度神经网络的模板拟合管道，用于从3D CT图像生成结构化四边形网格，以实现主动脉瓣的精确几何建模，确保网格质量和患者间一致性，并简化训练过程。", "motivation": "从3D CT图像精确建模主动脉瓣对于生物力学分析和患者特异性模拟至关重要。然而，传统方法生成的三角网格存在拓扑不规则、单元形状不佳以及患者间对应关系不一致的问题，影响网格质量和一致性。", "method": "引入了一个基于深度神经网络的模板拟合管道，用于从3D CT图像生成结构化四边形网格。通过使用一个通用的四边形网格模板对所有患者的主动脉瓣进行重网格化，确保了统一的网格拓扑和一致的节点与单元对应关系。深度神经网络的损失函数仅包含几何重建和平滑度正则化两项，足以保持网格平滑度和单元质量。", "result": "所提出的方法能够生成高质量的主动脉瓣表面网格，显著改善了平滑度和形状质量。与传统方法相比，该方法所需的显式正则化项更少。", "conclusion": "使用结构化四边形网格作为模板和神经网络训练不仅能确保网格的对应性和质量，还能简化训练过程，从而提高主动脉瓣建模的有效性和效率。"}}
{"id": "2511.03876", "categories": ["eess.IV", "cs.CV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2511.03876", "abs": "https://arxiv.org/abs/2511.03876", "authors": ["Jinyuxuan Guo", "Gurnoor Singh Khurana", "Alejandro Gonzalo Grande", "Juan C. del Alamo", "Francisco Contijoch"], "title": "Computed Tomography (CT)-derived Cardiovascular Flow Estimation Using Physics-Informed Neural Networks Improves with Sinogram-based Training: A Simulation Study", "comment": null, "summary": "Background: Non-invasive imaging-based assessment of blood flow plays a\ncritical role in evaluating heart function and structure. Computed Tomography\n(CT) is a widely-used imaging modality that can robustly evaluate\ncardiovascular anatomy and function, but direct methods to estimate blood flow\nvelocity from movies of contrast evolution have not been developed.\n  Purpose: This study evaluates the impact of CT imaging on Physics-Informed\nNeural Networks (PINN)-based flow estimation and proposes an improved\nframework, SinoFlow, which uses sinogram data directly to estimate blood flow.\n  Methods: We generated pulsatile flow fields in an idealized 2D vessel\nbifurcation using computational fluid dynamics and simulated CT scans with\nvarying gantry rotation speeds, tube currents, and pulse mode imaging settings.\nWe compared the performance of PINN-based flow estimation using reconstructed\nimages (ImageFlow) to SinoFlow.\n  Results: SinoFlow significantly improved flow estimation performance by\navoiding propagating errors introduced by filtered backprojection. SinoFlow was\nrobust across all tested gantry rotation speeds and consistently produced lower\nmean squared error and velocity errors than ImageFlow. Additionally, SinoFlow\nwas compatible with pulsed-mode imaging and maintained higher accuracy with\nshorter pulse widths.\n  Conclusions: This study demonstrates the potential of SinoFlow for CT-based\nflow estimation, providing a more promising approach for non-invasive blood\nflow assessment. The findings aim to inform future applications of PINNs to CT\nimages and provide a solution for image-based estimation, with reasonable\nacquisition parameters yielding accurate flow estimates.", "AI": {"tldr": "本研究提出了一种名为SinoFlow的新框架，通过直接使用CT正弦图数据而非重建图像，显著提高了基于物理信息神经网络（PINN）的血流估计性能，避免了图像重建引入的误差。", "motivation": "CT是评估心血管解剖和功能的常用成像方式，但目前缺乏直接从造影剂演变电影中估计血流速度的方法。现有基于PINN的血流估计方法可能受到CT成像参数的影响，且传统方法通过重建图像进行估计会引入误差。", "method": "研究通过计算流体动力学（CFD）在一个理想化的二维血管分叉中生成了脉动流场，并模拟了不同机架旋转速度、管电流和脉冲模式成像设置下的CT扫描。随后，比较了使用重建图像的PINN基流估计方法（ImageFlow）与直接使用正弦图数据的SinoFlow框架的性能。", "result": "SinoFlow通过避免滤波反投影引入的误差，显著提升了血流估计性能。SinoFlow在所有测试的机架旋转速度下均表现出鲁棒性，并且比ImageFlow产生了更低均方误差和速度误差。此外，SinoFlow兼容脉冲模式成像，并能在更短的脉冲宽度下保持更高的准确性。", "conclusion": "本研究证明了SinoFlow在CT血流估计方面的潜力，为无创血流评估提供了一种更有前景的方法。研究结果旨在为未来PINN在CT图像上的应用提供信息，并提供一个图像基估计解决方案，即通过合理的采集参数获得准确的血流估计。"}}
{"id": "2511.04510", "categories": ["eess.IV", "cs.CV", "physics.optics", "68T07, 78A46, 78A70, 92C55", "I.2.10; I.4.5"], "pdf": "https://arxiv.org/pdf/2511.04510", "abs": "https://arxiv.org/abs/2511.04510", "authors": ["Shihan Zhao", "Jianru Zhang", "Yanan Wu", "Linlin Li", "Siyuan Shen", "Xingjun Zhu", "Guoyan Zheng", "Jiahua Jiang", "Wuwei Ren"], "title": "$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography via Implicit Neural Representation", "comment": null, "summary": "Fluorescence Molecular Tomography (FMT) is a promising technique for\nnon-invasive 3D visualization of fluorescent probes, but its reconstruction\nremains challenging due to the inherent ill-posedness and reliance on\ninaccurate or often-unknown tissue optical properties. While deep learning\nmethods have shown promise, their supervised nature limits generalization\nbeyond training data. To address these problems, we propose $\\mu$NeuFMT, a\nself-supervised FMT reconstruction framework that integrates implicit\nneural-based scene representation with explicit physical modeling of photon\npropagation. Its key innovation lies in jointly optimize both the fluorescence\ndistribution and the optical properties ($\\mu$) during reconstruction,\neliminating the need for precise prior knowledge of tissue optics or\npre-conditioned training data. We demonstrate that $\\mu$NeuFMT robustly\nrecovers accurate fluorophore distributions and optical coefficients even with\nseverely erroneous initial values (0.5$\\times$ to 2$\\times$ of ground truth).\nExtensive numerical, phantom, and in vivo validations show that $\\mu$NeuFMT\noutperforms conventional and supervised deep learning approaches across diverse\nheterogeneous scenarios. Our work establishes a new paradigm for robust and\naccurate FMT reconstruction, paving the way for more reliable molecular imaging\nin complex clinically related scenarios, such as fluorescence guided surgery.", "AI": {"tldr": "本文提出了一种名为μNeuFMT的自监督荧光分子断层扫描（FMT）重建框架，它通过联合优化荧光分布和光学特性，解决了传统FMT重建中固有的病态性、对光学参数的依赖以及监督深度学习泛化能力受限的问题。", "motivation": "FMT重建面临挑战，原因在于其固有的病态性以及对不准确或未知组织光学特性的依赖。虽然深度学习有前景，但其监督性质限制了在训练数据之外的泛化能力。", "method": "本文提出了μNeuFMT，一个自监督的FMT重建框架。它将隐式神经场景表示与光子传播的显式物理建模相结合，并在重建过程中联合优化荧光分布和光学特性（μ），从而无需精确的组织光学先验知识或预处理的训练数据。", "result": "μNeuFMT即使在初始值严重错误（0.5倍至2倍于真实值）的情况下，也能稳健地恢复准确的荧光团分布和光学系数。广泛的数值、体模和体内验证表明，在各种异质场景中，μNeuFMT优于传统方法和监督深度学习方法。", "conclusion": "μNeuFMT为稳健和准确的FMT重建建立了一个新范式，为复杂临床相关场景（如荧光引导手术）中更可靠的分子成像铺平了道路。"}}
{"id": "2511.03738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03738", "abs": "https://arxiv.org/abs/2511.03738", "authors": ["Pranav Bhandari", "Nicolas Fay", "Sanjeevan Selvaganapathy", "Amitava Datta", "Usman Naseem", "Mehwish Nasim"], "title": "Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs", "comment": null, "summary": "Large Language Models exhibit implicit personalities in their generation, but\nreliably controlling or aligning these traits to meet specific needs remains an\nopen challenge. The need for effective mechanisms for behavioural manipulation\nof the model during generation is a critical gap in the literature that needs\nto be fulfilled. Personality-aware LLMs hold a promising direction towards this\nobjective. However, the relationship between these psychological constructs and\ntheir representations within LLMs remains underexplored and requires further\ninvestigation. Moreover, it is intriguing to understand and study the use of\nthese representations to steer the models' behaviour. We propose a novel\npipeline that extracts hidden state activations from transformer layers using\nthe Big Five Personality Traits (Openness, Conscientiousness, Extraversion,\nAgreeableness and Neuroticism), which is a comprehensive and empirically\nvalidated framework to model human personality applies low-rank subspace\ndiscovery methods, and identifies trait-specific optimal layers across\ndifferent model architectures for robust injection. The resulting\npersonality-aligned directions are then operationalised through a flexible\nsteering framework with dynamic layer selection, enabling precise control of\ntrait expression in LLM outputs. Our findings reveal that personality traits\noccupy a low-rank shared subspace, and that these latent structures can be\ntransformed into actionable mechanisms for effective steering through careful\nperturbations without impacting the fluency, variance and general capabilities,\nhelping to bridge the gap between psychological theory and practical model\nalignment.", "AI": {"tldr": "本文提出一种新颖的流水线，通过提取Transformer隐藏层激活并利用大五人格特质，识别模型中与人格特质相关的低秩共享子空间，并开发出一种灵活的引导框架，实现对LLM输出中人格特质表达的精确控制，且不影响模型性能。", "motivation": "大型语言模型（LLMs）在生成中展现出隐式人格，但可靠地控制或调整这些特质以满足特定需求仍是一个未解决的挑战。文献中缺乏有效的机制来在生成过程中操纵模型的行为，这是一个关键空白。人格感知型LLM为此提供了一个有前景的方向，但心理学构建与其在LLM中表示之间的关系尚未得到充分探索，且如何利用这些表示来引导模型行为也值得深入研究。", "method": "研究提出了一种新颖的流水线，该流水线利用大五人格特质（开放性、尽责性、外向性、宜人性、神经质）从Transformer层中提取隐藏状态激活，应用低秩子空间发现方法，并在不同模型架构中识别特质特定的最优层，以实现鲁棒的注入。然后，通过一个具有动态层选择的灵活引导框架，将生成的人格对齐方向进行操作化，从而精确控制LLM输出中的特质表达。", "result": "研究发现人格特质占据一个低秩共享子空间，并且这些潜在结构可以通过细致的扰动转化为可操作的机制，实现有效的引导，同时不影响模型的流畅性、多样性和通用能力。", "conclusion": "该研究成功地弥合了心理学理论与实际模型对齐之间的鸿沟，为LLM中人格特质的精确控制提供了一种有效且不损害模型性能的方法。"}}
{"id": "2511.03734", "categories": ["eess.SY", "cs.SY", "math.DS", "93B30, 15A18, 93C10, 37M25"], "pdf": "https://arxiv.org/pdf/2511.03734", "abs": "https://arxiv.org/abs/2511.03734", "authors": ["Philipp Schmitz", "Lea Bold", "Friedrich M. Philipp", "Mario Rosenfelder", "Peter Eberhard", "Henrik Ebel", "Karl Worthmann"], "title": "On excitation of control-affine systems and its use for data-driven Koopman approximants", "comment": null, "summary": "The Koopman operator and extended dynamic mode decomposition (EDMD) as a\ndata-driven technique for its approximation have attracted considerable\nattention as a key tool for modeling, analysis, and control of complex\ndynamical systems. However, extensions towards control-affine systems resulting\nin bilinear surrogate models are prone to demanding data requirements rendering\ntheir applicability intricate. In this paper, we propose a framework for\ndata-fitting of control-affine mappings to increase the robustness margin in\nthe associated system identification problem and, thus, to provide more\nreliable bilinear EDMD schemes. In particular, guidelines for input selection\nbased on subspace angles are deduced such that a desired threshold with respect\nto the minimal singular value is ensured. Moreover, we derive necessary and\nsufficient conditions of optimality for maximizing the minimal singular value.\nFurther, we demonstrate the usefulness of the proposed approach using bilinear\nEDMD with control for non-holonomic robots.", "AI": {"tldr": "本文提出了一种数据拟合框架，用于提高控制仿射映射的鲁棒性，从而改善双线性扩展动态模式分解（EDMD）方案的可靠性。通过基于子空间角度的输入选择指南和最大化最小奇异值的最优性条件，解决了控制仿射系统在双线性代理模型中数据需求高的问题，并以非完整机器人为例进行了验证。", "motivation": "Koopman算子和EDMD是复杂动力系统建模、分析和控制的关键工具。然而，将其扩展到控制仿射系统时，产生的双线性代理模型对数据要求很高，导致其适用性复杂。", "method": "本文提出了一种用于控制仿射映射数据拟合的框架，以提高相关系统识别问题的鲁棒性。具体方法包括：1) 基于子空间角度推导出输入选择指南，以确保最小奇异值达到期望阈值。2) 推导出最大化最小奇异值的必要和充分最优性条件。", "result": "所提出的框架提高了系统识别问题的鲁棒性，从而提供了更可靠的双线性EDMD方案。通过基于子空间角度的输入选择指南，可以确保达到期望的最小奇异值阈值。此外，还推导了最大化最小奇异值的必要和充分最优性条件。该方法在非完整机器人的双线性EDMD控制中得到了有效验证。", "conclusion": "通过提出一个数据拟合框架、基于子空间角度的输入选择指南以及最大化最小奇异值的最优性条件，本文成功提高了控制仿射系统双线性EDMD方案的鲁棒性和可靠性，有效解决了其数据需求高的问题。"}}
{"id": "2511.03765", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03765", "abs": "https://arxiv.org/abs/2511.03765", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Jae-Jin Lee", "Woojoo Lee"], "title": "LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices", "comment": "8 pages, 6 figures, 2 tables, DATE 2026 accepted paper", "summary": "On-device fine-tuning of CNNs is essential to withstand domain shift in edge\napplications such as Human Activity Recognition (HAR), yet full fine-tuning is\ninfeasible under strict memory, compute, and energy budgets. We present\nLoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on\nLow-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies\nTensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional\nlayers, (ii) selectively updates only the output-side core with\nzero-initialization to keep the auxiliary path inactive at the start, and (iii)\nfuses the update back into dense kernels, leaving inference cost unchanged.\nThis design preserves convolutional structure and reduces the number of\ntrainable parameters by up to two orders of magnitude compared to full\nfine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves\naccuracy within 4.7% of full fine-tuning while updating at most 1.49% of\nparameters, consistently outperforming prior parameter-efficient baselines\nunder similar budgets. On a Jetson Orin Nano, TT-SVD initialization and\nselective-core training yield 1.4-3.8x faster convergence to target F1.\nLoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN\nadaptation practical for edge platforms.", "AI": {"tldr": "LoRA-Edge是一种面向边缘设备的参数高效微调(PEFT)方法，它结合了LoRA和张量列车分解，显著减少了CNN微调所需的参数量，同时保持了高精度和更快的收敛速度，使其在资源受限的边缘平台成为可能。", "motivation": "在人体活动识别(HAR)等边缘应用中，CNN需要进行设备上微调以应对域偏移。然而，由于严格的内存、计算和能耗预算，进行全面的微调是不可行的。", "method": "LoRA-Edge是基于低秩适应(LoRA)并辅以张量列车分解的PEFT方法。具体包括：(i) 对预训练的卷积层应用张量列车奇异值分解(TT-SVD)；(ii) 仅选择性地更新输出侧核心，并进行零初始化，以确保辅助路径在开始时处于非活动状态；(iii) 将更新融合回密集核中，从而保持推理成本不变。这种设计保留了卷积结构。", "result": "与全量微调相比，LoRA-Edge将可训练参数数量减少了多达两个数量级。在各种HAR数据集和CNN骨干网络上，其精度与全量微调的差距在4.7%以内，而更新的参数最多仅占总参数的1.49%。在相似预算下，它始终优于先前的参数高效基线。在Jetson Orin Nano设备上，TT-SVD初始化和选择性核心训练使收敛到目标F1的速度提高了1.4-3.8倍。", "conclusion": "LoRA-Edge使得结构对齐、参数高效的设备上CNN适应在边缘平台上变得实用可行。"}}
{"id": "2511.03773", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03773", "abs": "https://arxiv.org/abs/2511.03773", "authors": ["Zhaorun Chen", "Zhuokai Zhao", "Kai Zhang", "Bo Liu", "Qi Qi", "Yifan Wu", "Tarun Kalluri", "Sara Cao", "Yuanhao Xiong", "Haibo Tong", "Huaxiu Yao", "Hengduo Li", "Jiacheng Zhu", "Xian Li", "Dawn Song", "Bo Li", "Jason Weston", "Dat Huynh"], "title": "Scaling Agent Learning via Experience Synthesis", "comment": null, "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.", "AI": {"tldr": "DreamGym是一个统一框架，通过推理模型合成多样化经验、结合经验回放和自适应任务生成，解决了大型语言模型（LLM）智能体强化学习（RL）中昂贵数据收集的挑战，显著提升了RL训练效果和模拟到现实的迁移能力。", "motivation": "强化学习（RL）赋能大型语言模型（LLM）智能体面临实际应用挑战，包括昂贵的试错、任务多样性不足、奖励信号不可靠以及基础设施复杂性，这些都阻碍了可扩展经验数据的收集。", "method": "DreamGym提出一个统一框架，通过以下方式合成可扩展的经验：1) 使用基于推理的经验模型，而非昂贵的真实环境试错，推导一致的状态转移和反馈信号；2) 利用经验回放缓冲区，用离线真实数据初始化并持续更新，以提高转移的稳定性和质量；3) 自适应生成新任务以挑战当前智能体策略，实现更有效的在线课程学习。", "result": "DreamGym在多样化环境和智能体骨干上显著改进了RL训练，无论是在纯合成设置还是模拟到现实的迁移场景。在WebArena等非RL就绪任务上，其性能超越所有基线30%以上。在RL就绪但成本高昂的设置中，仅使用合成交互就能达到GRPO和PPO的性能。在将纯粹通过合成经验训练的策略迁移到真实环境RL时，DreamGym在所需真实世界交互显著减少的情况下，提供了额外的显著性能提升，为通用RL提供了一种可扩展的预热策略。", "conclusion": "DreamGym通过可扩展地合成多样化经验，成功解决了LLM智能体在线RL训练中的关键挑战，并在各种任务和迁移场景中展现出卓越的性能，为通用RL提供了一种高效的预热和训练方法。"}}
{"id": "2511.03893", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.03893", "abs": "https://arxiv.org/abs/2511.03893", "authors": ["Adam M. Saunders", "Lucas W. Remedios", "Elyssa M. McMaster", "Jongyeon Yoon", "Gaurav Rudravaram", "Adam Sadriddinov", "Praitayini Kanakaraj", "Bennett A. Landman", "Adam W. Anderson"], "title": "DeepFixel: Crossing white matter fiber identification through spherical convolutional neural networks", "comment": "11 pages, 6 figures. Accepted to SPIE Medical Imaging 2026: Clinical\n  and Biomedical Imaging", "summary": "Diffusion-weighted magnetic resonance imaging allows for reconstruction of\nmodels for structural connectivity in the brain, such as fiber orientation\ndistribution functions (ODFs) that describe the distribution, direction, and\nvolume of white matter fiber bundles in a voxel. Crossing white matter fibers\nin voxels complicate analysis and can lead to errors in downstream tasks like\ntractography. We introduce one option for separating fiber ODFs by performing a\nnonlinear optimization to fit ODFs to the given data and penalizing terms that\nare not symmetric about the axis of the fiber. However, this optimization is\nnon-convex and computationally infeasible across an entire image (approximately\n1.01 x 106 ms per voxel). We introduce DeepFixel, a spherical convolutional\nneural network approximation for this nonlinear optimization. We model the\nprobability distribution of fibers as a spherical mesh with higher angular\nresolution than a truncated spherical harmonic representation. To validate\nDeepFixel, we compare to the nonlinear optimization and a fixel-based\nseparation algorithm of two-fiber and three-fiber ODFs. The median angular\ncorrelation coefficient is 1 (interquartile range of 0.00) using the nonlinear\noptimization algorithm, 0.988 (0.317) using a fiber bundle elements or\n\"fixel\"-based separation algorithm, and 0.973 (0.004) using DeepFixel.\nDeepFixel is more computationally efficient than the non-convex optimization\n(0.32 ms per voxel). DeepFixel's spherical mesh representation is successful at\ndisentangling at smaller angular separations and smaller volume fractions than\nthe fixel-based separation algorithm.", "AI": {"tldr": "该论文提出DeepFixel，一种球形卷积神经网络，用于高效分离扩散加权磁共振成像中交叉的白质纤维方向分布函数(ODFs)。它在计算效率上显著优于非线性优化，并能更好地处理小角度分离和低体积分数的纤维，同时保持较高的准确性。", "motivation": "扩散加权磁共振成像中的交叉白质纤维会使大脑结构连接分析复杂化，并导致诸如纤维束成像等下游任务出现错误。现有的非线性优化方法虽然能分离ODFs，但计算成本过高，无法在整个图像上应用。", "method": "研究首先提出一种非线性优化方法来分离纤维ODFs，通过拟合ODFs到给定数据并惩罚不对称项。由于该优化计算量大，研究引入了DeepFixel，一个球形卷积神经网络，以近似该非线性优化。DeepFixel将纤维概率分布建模为具有高角度分辨率的球形网格。通过与非线性优化和基于fixel的分离算法在两纤维和三纤维ODFs上的比较来验证DeepFixel。", "result": "非线性优化算法的中位角相关系数为1（四分位距0.00），但计算时间为每体素约1.01 x 10^6毫秒。基于fixel的分离算法的中位角相关系数为0.988（0.317）。DeepFixel的中位角相关系数为0.973（0.004），但计算效率更高，每体素仅需0.32毫秒。DeepFixel的球形网格表示在解缠绕小角度分离和低体积分数的纤维方面优于基于fixel的分离算法。", "conclusion": "DeepFixel作为一种球形卷积神经网络，成功地为分离交叉白质纤维ODFs提供了一个高效且准确的替代方案。它显著降低了计算成本，同时在处理具有挑战性的纤维配置（如小角度分离和低体积分数）方面表现出色，优于传统的基于fixel的分离算法。"}}
{"id": "2511.04009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04009", "abs": "https://arxiv.org/abs/2511.04009", "authors": ["Chenzui Li", "Yiming Chen", "Xi Wu", "Giacinto Barresi", "Fei Chen"], "title": "Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration", "comment": "7 pages, 7 figures, IROS 2025 accepted", "summary": "This paper introduces an upper limb postural optimization method for\nenhancing physical ergonomics and force manipulability during bimanual\nhuman-robot co-carrying tasks. Existing research typically emphasizes human\nsafety or manipulative efficiency, whereas our proposed method uniquely\nintegrates both aspects to strengthen collaboration across diverse conditions\n(e.g., different grasping postures of humans, and different shapes of objects).\nSpecifically, the joint angles of a simplified human skeleton model are\noptimized by minimizing the cost function to prioritize safety and manipulative\ncapability. To guide humans towards the optimized posture, the reference\nend-effector poses of the robot are generated through a transformation module.\nA bimanual model predictive impedance controller (MPIC) is proposed for our\nhuman-like robot, CURI, to recalibrate the end effector poses through planned\ntrajectories. The proposed method has been validated through various subjects\nand objects during human-human collaboration (HHC) and human-robot\ncollaboration (HRC). The experimental results demonstrate significant\nimprovement in muscle conditions by comparing the activation of target muscles\nbefore and after optimization.", "AI": {"tldr": "本文提出了一种上肢姿态优化方法，用于双臂人机协作搬运任务，旨在同时提升物理人体工程学和力操纵能力。", "motivation": "现有研究通常侧重于人类安全或操纵效率的单一方面，且缺乏在不同抓握姿态和物体形状等多样条件下的通用性，未能有效整合安全性和协作能力。", "method": "该方法通过最小化成本函数来优化简化人类骨骼模型的关节角度，以优先考虑安全性和操纵能力。然后，通过转换模块生成机器人的参考末端执行器姿态。最后，为类人机器人CURI提出了一种双臂模型预测阻抗控制器（MPIC），通过规划轨迹重新校准末端执行器姿态。", "result": "通过人-人协作（HHC）和人-机协作（HRC）实验，在不同受试者和物体上验证了该方法。实验结果表明，优化前后目标肌肉的激活程度对比显示，肌肉状况有显著改善。", "conclusion": "该方法成功地在双臂人机协作搬运任务中提升了物理人体工程学和力操纵能力，并通过优化上肢姿态有效平衡了安全性和协作效率。"}}
{"id": "2511.04304", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.04304", "abs": "https://arxiv.org/abs/2511.04304", "authors": ["Robin Spanier", "Thorsten Hoeser", "Claudia Kuenzer"], "title": "Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data", "comment": "14 pages, 9 figures", "summary": "The recent and ongoing expansion of marine infrastructure, including offshore\nwind farms, oil and gas platforms, artificial islands, and aquaculture\nfacilities, highlights the need for effective monitoring systems. The\ndevelopment of robust models for offshore infrastructure detection relies on\ncomprehensive, balanced datasets, but falls short when samples are scarce,\nparticularly for underrepresented object classes, shapes, and sizes. By\ntraining deep learning-based YOLOv10 object detection models with a combination\nof synthetic and real Sentinel-1 satellite imagery acquired in the fourth\nquarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of\nGuinea, and Coast of Brazil), this study investigates the use of synthetic\ntraining data to enhance model performance. We evaluated this approach by\napplying the model to detect offshore platforms in three unseen regions (Gulf\nof Mexico, North Sea, Persian Gulf) and thereby assess geographic\ntransferability. This region-holdout evaluation demonstrated that the model\ngeneralises beyond the training areas. In total, 3,529 offshore platforms were\ndetected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and\n1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which\nimproved to 0.90 upon incorporating synthetic data. We analysed how synthetic\ndata enhances the representation of unbalanced classes and overall model\nperformance, taking a first step toward globally transferable detection of\noffshore infrastructure. This study underscores the importance of balanced\ndatasets and highlights synthetic data generation as an effective strategy to\naddress common challenges in remote sensing, demonstrating the potential of\ndeep learning for scalable, global offshore infrastructure monitoring.", "AI": {"tldr": "本研究利用合成数据与真实Sentinel-1卫星图像结合训练YOLOv10深度学习模型，以提高海洋基础设施（特别是海上平台）的检测性能和地理泛化能力，实现F1分数从0.85提升至0.90，并成功检测了3,529个平台。", "motivation": "随着海洋基础设施（如海上风电场、油气平台）的持续扩张，对有效监测系统的需求日益增加。然而，开发稳健的离岸基础设施检测模型面临数据不足的挑战，特别是对于代表性不足的物体类别、形状和尺寸，导致数据集不全面或不平衡。", "method": "研究采用YOLOv10深度学习目标检测模型，并结合合成数据和2023年第四季度从四个区域（里海、南海、几内亚湾、巴西海岸）获取的真实Sentinel-1卫星图像进行训练。通过将模型应用于三个未见区域（墨西哥湾、北海、波斯湾）来评估其地理可迁移性，并分析合成数据如何增强不平衡类别的表示和整体模型性能。", "result": "模型成功检测了总计3,529个海上平台，其中北海411个，墨西哥湾1,519个，波斯湾1,593个。模型在未见区域表现出良好的泛化能力，初始F1分数为0.85，在整合合成数据后提高到0.90。合成数据有效增强了不平衡类别的表示，并显著提升了整体模型性能。", "conclusion": "本研究强调了平衡数据集的重要性，并证明了合成数据生成是解决遥感领域常见挑战的有效策略。通过利用深度学习和合成数据，可以实现可扩展的全球海洋基础设施监测，朝着全球可迁移的离岸基础设施检测迈出了第一步。"}}
{"id": "2511.03825", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03825", "abs": "https://arxiv.org/abs/2511.03825", "authors": ["Ahmed Mostafa", "Raisul Arefin Nahid", "Samuel Mulder"], "title": "How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis", "comment": "Publication Notice. This paper was published in the BAR 2025 Workshop\n  (with NDSS 2025) and is for research and educational use. Copyright\n  \\c{opyright} 2025 Internet Society. All rights reserved. Personal/classroom\n  reproduction is permitted with this notice and full paper citation. All other\n  uses, including commercial, require prior written permission from the\n  Internet Society", "summary": "Tokenization is fundamental in assembly code analysis, impacting intrinsic\ncharacteristics like vocabulary size, semantic coverage, and extrinsic\nperformance in downstream tasks. Despite its significance, tokenization in the\ncontext of assembly code remains an underexplored area. This study aims to\naddress this gap by evaluating the intrinsic properties of Natural Language\nProcessing (NLP) tokenization models and parameter choices, such as vocabulary\nsize. We explore preprocessing customization options and pre-tokenization rules\ntailored to the unique characteristics of assembly code. Additionally, we\nassess their impact on downstream tasks like function signature prediction -- a\ncritical problem in binary code analysis.\n  To this end, we conduct a thorough study on various tokenization models,\nsystematically analyzing their efficiency in encoding assembly instructions and\ncapturing semantic nuances. Through intrinsic evaluations, we compare\ntokenizers based on tokenization efficiency, vocabulary compression, and\nrepresentational fidelity for assembly code. Using state-of-the-art pre-trained\nmodels such as the decoder-only Large Language Model (LLM) Llama 3.2, the\nencoder-only transformer BERT, and the encoder-decoder model BART, we evaluate\nthe effectiveness of these tokenizers across multiple performance metrics.\nPreliminary findings indicate that tokenizer choice significantly influences\ndownstream performance, with intrinsic metrics providing partial but incomplete\npredictability of extrinsic evaluation outcomes. These results reveal complex\ntrade-offs between intrinsic tokenizer properties and their utility in\npractical assembly code tasks. Ultimately, this study provides valuable\ninsights into optimizing tokenization models for low-level code analysis,\ncontributing to the robustness and scalability of Natural Language Model\n(NLM)-based binary analysis workflows.", "AI": {"tldr": "本研究探讨了汇编代码分析中分词（tokenization）的重要性，评估了NLP分词模型及其参数的内在特性和对下游任务（如函数签名预测）的影响。结果显示分词器选择显著影响性能，揭示了内在属性与实际效用之间的复杂权衡。", "motivation": "分词在汇编代码分析中至关重要，影响词汇量、语义覆盖和下游任务性能，但该领域在汇编代码背景下尚未得到充分探索。", "method": "研究评估了NLP分词模型的内在特性和参数选择（如词汇量），探索了针对汇编代码的预处理定制和预分词规则。通过对多种分词模型进行深入研究，分析了它们在编码汇编指令和捕捉语义细微差别方面的效率。通过内在评估比较了分词效率、词汇压缩和表示保真度。使用Llama 3.2、BERT和BART等预训练模型评估了分词器在函数签名预测等下游任务中的有效性。", "result": "初步结果表明，分词器选择显著影响下游任务性能。内在指标对外部评估结果具有部分但非完全的预测性。研究揭示了分词器内在属性与其在实际汇编代码任务中的实用性之间存在的复杂权衡。", "conclusion": "本研究为优化低级代码分析的分词模型提供了宝贵见解，有助于提高基于自然语言模型（NLM）的二进制分析工作流的鲁棒性和可扩展性。"}}
{"id": "2511.03737", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03737", "abs": "https://arxiv.org/abs/2511.03737", "authors": ["Dániel István Németh", "Kálmán Tornai"], "title": "Hybrid ILM-NILM Smart Plug System", "comment": "13 pages, 9 figures. This is the original version of a manuscript\n  currently under review at the International Journal of Electrical Power &\n  Energy Systems", "summary": "Electrical load classification is generally divided into intrusive and\nnon-intrusive approaches, both having their limitations and advantages. With\nthe non-intrusive approach, controlling appliances is not possible, but the\ninstallation cost of a single measurement device is cheap. In comparison,\nintrusive, smart plug-based solutions offer individual appliance control, but\nthe installation cost is much higher. There have been very few approaches\naiming to combine these methods. In this paper we show that extending a smart\nplug-based solution to multiple loads per plug can reduce control granularity\nin favor of lowering the system's installation costs. Connecting various loads\nto a Smart Plug through an extension cord is seldom considered in the\nliterature, even though it is common in households. This scenario is also\nhandled by the hybrid load classification solution presented in this paper.", "AI": {"tldr": "该论文提出一种混合式负载分类方案，通过扩展智能插座支持多负载连接，以降低安装成本并处理家庭中常见的排插连接场景。", "motivation": "现有的侵入式和非侵入式负载分类方法各有优缺点，但很少有结合两者的方案。侵入式智能插座提供精细控制但成本高，而非侵入式成本低但无法单独控制。此外，智能插座通常只考虑单个负载，而家庭中通过延长线连接多个负载的情况很常见，但文献中鲜有涉及。", "method": "本文提出一种混合负载分类解决方案，该方案扩展了基于智能插座的解决方案，使其能够支持每个插座连接多个负载。这种方法以降低控制粒度为代价，实现了系统安装成本的降低。同时，该方案也能处理通过延长线连接多种负载的常见家庭场景。", "result": "论文表明，将基于智能插座的解决方案扩展到每个插座连接多个负载，可以在降低系统安装成本的同时，降低控制粒度。此外，所提出的混合负载分类解决方案能够处理通过延长线连接多个负载的常见家庭场景。", "conclusion": "通过扩展智能插座以支持多负载连接的混合式负载分类方案，可以在控制粒度和系统安装成本之间取得平衡，并有效解决家庭中常见的通过延长线连接多个电器的问题。"}}
{"id": "2511.03739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03739", "abs": "https://arxiv.org/abs/2511.03739", "authors": ["Eugenius Mario Situmorang", "Adila Alfa Krisnadhi", "Ari Wibisono"], "title": "TextualVerifier: Verify TextGrad Step-by-Step", "comment": null, "summary": "TextGrad is a novel approach to text-based automatic differentiation that\nenables composite AI systems to perform optimization without explicit numerical\nequations. However, it currently lacks self-verification mechanisms that ensure\nreasoning validity in text-based decision making. This research introduces\nTextualVerifier, a verification framework that leverages chain-of-thought\nreasoning and majority voting with large language models to address this\nverification gap. TextualVerifier implements a four-stage workflow:\nchain-of-thought decomposition, variant generation, majority voting, and\nconsensus aggregation. It integrates non-invasively with TextGrad at both the\nloss function and optimization result verification stages. Experimental\nevaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)\nstandalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad\non GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically\nsignificant improvements (p < 0.001). In phase one, TextualVerifier improves\nthe validity of reasoning steps by 29 percent. In phase two, integration into\nTextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4\npercent with a moderate overhead of 5.9 LLM calls on average. Further\nevaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92\npercentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.\nTextualVerifier thus presents the first self-verification framework for\nTextGrad through LLM-based techniques without requiring numerical gradients,\nenabling more reliable reasoning and opening new directions for verification in\ntext-based optimization.", "AI": {"tldr": "TextualVerifier是一个基于大型语言模型（LLM）的自验证框架，通过链式思考和多数投票来验证TextGrad的文本推理过程，显著提高了推理的有效性和TextGrad的性能。", "motivation": "TextGrad作为一种新颖的文本自动微分方法，目前缺乏确保其文本决策推理有效性的自验证机制。", "method": "本研究引入了TextualVerifier框架，它利用大型语言模型（LLM）的链式思考（chain-of-thought）推理和多数投票机制来填补验证空白。该框架包含四个阶段：链式思考分解、变体生成、多数投票和共识聚合。它以非侵入性方式集成到TextGrad的损失函数和优化结果验证阶段。实验使用Gemini 1.5 Pro模型，分为两个阶段进行评估：1）在PRM800K上进行独立评估；2）与TextGrad集成在GPQA-Diamond、MMLU-ML和MMLU-CP基准上进行评估。", "result": "实验结果显示出统计学上显著的改进（p < 0.001）。在第一阶段，TextualVerifier使推理步骤的有效性提高了29%。在第二阶段，集成到TextGrad损失函数中，性能从68.2%提高到70.4%，增加了2.2个百分点，平均仅增加了5.9次LLM调用。对TextualVerifier版本化的进一步评估显示，在GPQA、MMLU-ML和MMLU-CP上分别实现了8.08、10.71和3.92个百分点的改进。", "conclusion": "TextualVerifier是第一个为TextGrad提供自验证的框架，它利用基于LLM的技术，无需数值梯度，从而实现了更可靠的推理，并为文本优化中的验证开辟了新方向。"}}
{"id": "2511.03855", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03855", "abs": "https://arxiv.org/abs/2511.03855", "authors": ["Duong Mai", "Lawrence Hall"], "title": "Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets", "comment": "Abstract accepted for oral presentation at SPIE Medical Imaging 2026:\n  Computer-Aided Diagnosis", "summary": "Deep learned (DL) models for image recognition have been shown to fail to\ngeneralize to data from different devices, populations, etc. COVID-19 detection\nfrom Chest X-rays (CXRs), in particular, has been shown to fail to generalize\nto out-of-distribution (OOD) data from new clinical sources not covered in the\ntraining set. This occurs because models learn to exploit shortcuts -\nsource-specific artifacts that do not translate to new distributions - rather\nthan reasonable biomarkers to maximize performance on in-distribution (ID)\ndata. Rendering the models more robust to distribution shifts, our study\ninvestigates the use of fundamental noise injection techniques (Gaussian,\nSpeckle, Poisson, and Salt and Pepper) during training. Our empirical results\ndemonstrate that this technique can significantly reduce the performance gap\nbetween ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results\naveraged over ten random seeds across key metrics such as AUC, F1, accuracy,\nrecall and specificity. Our source code is publicly available at\nhttps://github.com/Duongmai127/Noisy-ood", "AI": {"tldr": "深度学习模型在图像识别（特别是COVID-19胸部X光检测）中难以泛化到分布外数据，因为它们利用了捷径。本研究发现，在训练期间注入基础噪声（如高斯、散斑、泊松、椒盐噪声）可以显著缩小模型在分布内和分布外数据上的性能差距，从而提高泛化能力。", "motivation": "深度学习模型在图像识别（如COVID-19胸部X光检测）中存在泛化性差的问题，尤其是在面对来自不同设备、人群或新的临床来源的分布外数据时。这归因于模型学习了特定于来源的伪影（捷径），而非真正的生物标志物，以最大化在分布内数据上的性能。研究旨在提高模型对分布变化的鲁棒性。", "method": "本研究调查了在训练期间使用基础噪声注入技术（包括高斯噪声、散斑噪声、泊松噪声和椒盐噪声）来提高模型对分布变化的鲁棒性。", "result": "经验结果表明，该技术能显著将分布内（ID）和分布外（OOD）评估之间的性能差距从0.10-0.20缩小到0.01-0.06。这些结果是基于十个随机种子，在AUC、F1、准确率、召回率和特异性等关键指标上平均得出的。", "conclusion": "在训练过程中注入基础噪声是一种有效的方法，可以显著减少深度学习模型在分布内和分布外数据之间的性能差距，从而提高模型对分布变化的鲁棒性，有助于解决模型在学习捷径而非合理生物标志物的问题。"}}
{"id": "2511.03845", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03845", "abs": "https://arxiv.org/abs/2511.03845", "authors": ["Tianning Dong", "Luyi Ma", "Varun Vasudevan", "Jason Cho", "Sushant Kumar", "Kannan Achan"], "title": "To See or To Read: User Behavior Reasoning in Multimodal LLMs", "comment": "Accepted by the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Workshop: Efficient Reasoning", "summary": "Multimodal Large Language Models (MLLMs) are reshaping how modern agentic\nsystems reason over sequential user-behavior data. However, whether textual or\nimage representations of user behavior data are more effective for maximizing\nMLLM performance remains underexplored. We present \\texttt{BehaviorLens}, a\nsystematic benchmarking framework for assessing modality trade-offs in\nuser-behavior reasoning across six MLLMs by representing transaction data as\n(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a\nreal-world purchase-sequence dataset, we find that when data is represented as\nimages, MLLMs next-purchase prediction accuracy is improved by 87.5% compared\nwith an equivalent textual representation without any additional computational\ncost.", "AI": {"tldr": "本文提出`BehaviorLens`框架，系统性评估多模态大语言模型（MLLMs）在用户行为推理中不同数据模态（文本、散点图、流程图）的权衡。研究发现，将交易数据表示为图像可显著提升MLLM的下一购买预测准确率达87.5%，且无需额外计算成本。", "motivation": "多模态大语言模型（MLLMs）正在重塑现代智能体系统对序列用户行为数据的推理方式。然而，目前尚未充分探索文本或图像表示的用户行为数据哪种能更有效地最大化MLLM性能。", "method": "研究引入了`BehaviorLens`，一个系统性基准测试框架，用于评估六种MLLM在用户行为推理中的模态权衡。该框架将交易数据表示为三种形式：(1) 文本段落，(2) 散点图（图像），和 (3) 流程图（图像）。实验使用了一个真实世界的购买序列数据集。", "result": "当数据以图像形式表示时（散点图或流程图），MLLMs的下一购买预测准确率比等效的文本表示提高了87.5%，且未增加额外的计算成本。", "conclusion": "将用户行为数据表示为图像（如散点图或流程图）能显著提高多模态大语言模型在下一购买预测任务中的性能，相较于文本表示具有巨大优势，并且不会带来额外的计算负担。"}}
{"id": "2511.03996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03996", "abs": "https://arxiv.org/abs/2511.03996", "authors": ["Yushi Wang", "Changsheng Luo", "Penghui Chen", "Jianran Liu", "Weijian Sun", "Tong Guo", "Kechang Yang", "Biao Hu", "Yangang Zhang", "Mingguo Zhao"], "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots", "comment": "Project page: https://humanoid-kick.github.io", "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.", "AI": {"tldr": "本文提出了一种基于统一强化学习的控制器，通过直接整合视觉感知和运动控制，使仿人机器人能够在动态环境中学习并执行反应灵敏的足球技能。", "motivation": "现有的仿人足球系统通常依赖解耦模块，导致在动态环境中响应延迟和行为不连贯，且实际感知限制进一步加剧了这些问题。", "method": "研究采用统一的强化学习控制器，将视觉感知与运动控制直接集成。该方法将对抗运动先验（Adversarial Motion Priors）扩展到真实世界动态环境中的感知设置。引入了编码器-解码器架构，并结合了模拟真实世界视觉特征的虚拟感知系统，使策略能从不完美观测中恢复特权状态，并建立感知与动作之间的主动协调。", "result": "所提出的控制器展现出强大的反应能力，在包括真实RoboCup比赛在内的各种场景中，都能持续执行连贯且稳健的足球行为。", "conclusion": "通过直接整合视觉感知和运动控制，本文的统一强化学习控制器成功克服了现有系统的局限性，使仿人机器人能够习得高度反应性的足球技能，并在复杂动态环境中表现出色。"}}
{"id": "2511.03740", "categories": ["eess.SY", "cs.MA", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03740", "abs": "https://arxiv.org/abs/2511.03740", "authors": ["Xinyi Wang", "Devansh R. Agrawal", "Dimitra Panagou"], "title": "Kalman-Bucy Filtering with Randomized Sensing: Fundamental Limits and Sensor Network Design for Field Estimation", "comment": null, "summary": "Stability analysis of the Kalman filter under randomly lost measurements has\nbeen widely studied. We revisit this problem in a general continuous-time\nframework, where both the measurement matrix and noise covariance evolve as\nrandom processes, capturing variability in sensing locations. Within this\nsetting, we derive a closed-form upper bound on the expected estimation\ncovariance for continuous-time Kalman filtering. We then apply this framework\nto spatiotemporal field estimation, where the field is modeled as a Gaussian\nprocess observed by randomly located, noisy sensors. Using clarity, introduced\nin our earlier work as a rescaled form of the differential entropy of a random\nvariable, we establish a grid-independent lower bound on the spatially averaged\nexpected clarity. This result exposes fundamental performance limits through a\ncomposite sensing parameter that jointly captures the effects of the number of\nsensors, noise level, and measurement frequency. Simulations confirm that the\nproposed bound is tight for the discrete-time Kalman filter, approaching it as\nthe measurement rate decreases, while avoiding the recursive computations\nrequired in the discrete-time formulation. Most importantly, the derived limits\nprovide principled and efficient guidelines for sensor network design problem\nprior to deployment.", "AI": {"tldr": "本文针对连续时间卡尔曼滤波器在随机测量丢失情况下的稳定性进行了分析，推导了期望估计协方差和清晰度的闭式界限，为传感器网络设计提供了基础性指导。", "motivation": "现有关于卡尔曼滤波器在随机测量丢失下稳定性的研究多集中于离散时间框架。本文旨在建立一个更通用的连续时间框架，其中测量矩阵和噪声协方差也作为随机过程演化，以更真实地捕捉传感位置的变异性。", "method": "1. 在广义连续时间框架下，推导了连续时间卡尔曼滤波器期望估计协方差的闭式上限。2. 将此框架应用于时空场估计，将场建模为高斯过程，由随机定位的噪声传感器观测。3. 利用“清晰度”（之前工作中引入的微分熵的重标形式），建立了空间平均期望清晰度的与网格无关的下限。", "result": "1. 得到了连续时间卡尔曼滤波器期望估计协方差的闭式上限。2. 建立了空间平均期望清晰度的与网格无关的下限，通过一个综合感知参数（结合传感器数量、噪声水平和测量频率）揭示了基本的性能限制。3. 仿真证实，所提出的界限对于离散时间卡尔曼滤波器是紧密的，尤其是在测量速率降低时，并避免了离散时间公式所需的递归计算。", "conclusion": "所推导的性能界限为传感器网络在部署前期的设计问题提供了有原则且高效的指导方针。"}}
{"id": "2511.03819", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "SILVI是一款开源视频标注软件，旨在弥补现有工具在动物行为和互动标注方面的不足，它能同时实现个体定位和行为互动标注，并生成结构化数据以训练计算机视觉模型。", "motivation": "现有计算机视觉方法主要关注个体动作检测，而对理解社交和个体动物行为至关重要的互动检测和标注关注较少。目前的开源标注工具要么支持行为标注但无个体定位，要么支持定位但无法捕获互动，存在功能上的空白。", "method": "本文介绍了一个名为SILVI的开源标注软件。它整合了个体定位和行为/互动标注的功能，允许研究人员直接在视频数据中进行标注，并生成适合训练和验证计算机视觉模型的结构化输出。", "result": "SILVI使研究人员能够对视频中的动物行为和互动进行精细标注，包括个体定位。它生成的结构化输出可用于开发和验证计算机视觉模型，从而促进细粒度行为分析的自动化方法。该工具不仅适用于动物行为研究，也可能适用于需要提取动态场景图的人类互动标注。", "conclusion": "SILVI通过提供一个集成的标注工具，弥合了行为生态学与计算机视觉之间的差距。它能有效标注动物行为和互动，对推动自动化行为分析至关重要，并具有更广泛的应用潜力。"}}
{"id": "2511.04042", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04042", "abs": "https://arxiv.org/abs/2511.04042", "authors": ["Kailun Ji", "Xiaoyu Hu", "Xinyu Zhang", "Jun Chen"], "title": "An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue", "comment": null, "summary": "Large-scale disaster Search And Rescue (SAR) operations are persistently\nchallenged by complex terrain and disrupted communications. While Unmanned\nAerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area\nsearch and supply delivery, yet their effective coordination places a\nsignificant cognitive burden on human operators. The core human-machine\ncollaboration bottleneck lies in the ``intention-to-action gap'', which is an\nerror-prone process of translating a high-level rescue objective into a\nlow-level swarm command under high intensity and pressure. To bridge this gap,\nthis study proposes a novel LLM-CRF system that leverages Large Language Models\n(LLMs) to model and augment human-swarm teaming cognition. The proposed\nframework initially captures the operator's intention through natural and\nmulti-modal interactions with the device via voice or graphical annotations. It\nthen employs the LLM as a cognitive engine to perform intention comprehension,\nhierarchical task decomposition, and mission planning for the UAV swarm. This\nclosed-loop framework enables the swarm to act as a proactive partner,\nproviding active feedback in real-time while reducing the need for manual\nmonitoring and control, which considerably advances the efficacy of the SAR\ntask. We evaluate the proposed framework in a simulated SAR scenario.\nExperimental results demonstrate that, compared to traditional order and\ncommand-based interfaces, the proposed LLM-driven approach reduced task\ncompletion time by approximately $64.2\\%$ and improved task success rate by\n$7\\%$. It also leads to a considerable reduction in subjective cognitive\nworkload, with NASA-TLX scores dropping by $42.9\\%$. This work establishes the\npotential of LLMs to create more intuitive and effective human-swarm\ncollaborations in high-stakes scenarios.", "AI": {"tldr": "该研究提出一种基于LLM-CRF的系统，通过自然多模态交互和LLM驱动的认知引擎，弥合人类操作员与无人机蜂群之间在灾难搜救（SAR）任务中的“意图-行动鸿沟”，显著提升任务效率并减轻操作员认知负荷。", "motivation": "大规模灾难搜救中，无人机蜂群在复杂地形和通信中断环境下难以有效协调，人类操作员面临巨大的认知负担，将高层级救援目标转化为低层级蜂群指令存在“意图-行动鸿沟”，易出错且压力巨大。", "method": "提出LLM-CRF系统，通过语音或图形标注等自然多模态交互捕捉操作员意图。系统利用LLM作为认知引擎，进行意图理解、分层任务分解和无人机蜂群任务规划。这是一个闭环框架，使蜂群能作为主动伙伴提供实时反馈，减少手动监控和控制。", "result": "在模拟SAR场景中，与传统指令式界面相比，所提出的LLM驱动方法将任务完成时间缩短约64.2%，任务成功率提高7%，主观认知负荷（NASA-TLX得分）降低42.9%。", "conclusion": "这项工作证实了大型语言模型（LLMs）在创建高风险场景下更直观、更有效的人机蜂群协作方面的潜力。"}}
{"id": "2511.03878", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA", "I.2.7; I.2.0"], "pdf": "https://arxiv.org/pdf/2511.03878", "abs": "https://arxiv.org/abs/2511.03878", "authors": ["Suraj Prasai", "Mengnan Du", "Ying Zhang", "Fan Yang"], "title": "KnowThyself: An Agentic Assistant for LLM Interpretability", "comment": "5 pages, 1 figure, Accepted for publication at the Demonstration\n  Track of the 40th AAAI Conference on Artificial Intelligence (AAAI 26)", "summary": "We develop KnowThyself, an agentic assistant that advances large language\nmodel (LLM) interpretability. Existing tools provide useful insights but remain\nfragmented and code-intensive. KnowThyself consolidates these capabilities into\na chat-based interface, where users can upload models, pose natural language\nquestions, and obtain interactive visualizations with guided explanations. At\nits core, an orchestrator LLM first reformulates user queries, an agent router\nfurther directs them to specialized modules, and the outputs are finally\ncontextualized into coherent explanations. This design lowers technical\nbarriers and provides an extensible platform for LLM inspection. By embedding\nthe whole process into a conversational workflow, KnowThyself offers a robust\nfoundation for accessible LLM interpretability.", "AI": {"tldr": "KnowThyself是一个智能体助手，通过整合现有工具并提供对话式界面和交互式可视化，降低了大型语言模型（LLM）可解释性的技术门槛。", "motivation": "现有的LLM可解释性工具分散且需要大量编码，用户难以有效利用。研究旨在开发一个统一、易于访问的平台，简化LLM的检查和理解过程。", "method": "KnowThyself的核心是一个编排LLM，它首先重构用户查询，然后一个智能体路由器将查询定向到专门的模块。最终，输出被语境化为连贯的解释，并通过聊天界面提供交互式可视化。", "result": "该设计降低了技术障碍，提供了一个可扩展的LLM检查平台。通过将整个过程嵌入到对话式工作流中，KnowThyself使LLM可解释性变得更加易于访问。", "conclusion": "KnowThyself为可访问的LLM可解释性提供了一个坚实的基础，通过其对话式工作流和整合能力，使得用户能够更轻松地理解和检查LLM。"}}
{"id": "2511.03741", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03741", "abs": "https://arxiv.org/abs/2511.03741", "authors": ["Xiachong Lin", "Arian Prabowo", "Imran Razzak", "Hao Xue", "Matthew Amos", "Sam Behrens", "Flora D. Salim"], "title": "Electric Vehicle Charging Load Modeling: A Survey, Trends, Challenges and Opportunities", "comment": "14 pages, 7 figures", "summary": "The evolution of electric vehicles (EVs) is reshaping the automotive\nindustry, advocating for more sustainable transportation practices. Accurately\npredicting EV charging behavior is essential for effective infrastructure\nplanning and optimization. However, the charging load of EVs is significantly\ninfluenced by uncertainties and randomness, posing challenges for accurate\nestimation. Furthermore, existing literature reviews lack a systematic analysis\nof modeling approaches focused on information fusion. This paper\ncomprehensively reviews EV charging load models from the past five years. We\ncategorize state-of-the-art modeling methods into statistical, simulated, and\ndata-driven approaches, examining the advantages and drawbacks of each.\nAdditionally, we analyze the three bottom-up level operations of information\nfusion in existing models. We conclude by discussing the challenges and\nopportunities in the field, offering guidance for future research endeavors to\nadvance our understanding and explore practical research directions.", "AI": {"tldr": "本文全面回顾了过去五年电动汽车充电负荷模型，系统分析了统计、模拟和数据驱动方法，并探讨了信息融合在模型中的应用，指出了未来的挑战与机遇。", "motivation": "电动汽车普及推动可持续交通发展，但其充电行为的不确定性给基础设施规划带来挑战。现有文献综述缺乏对侧重信息融合的建模方法的系统分析，因此需要对充电负荷模型进行全面回顾。", "method": "本文对过去五年电动汽车充电负荷模型进行了全面综述。将现有建模方法归类为统计、模拟和数据驱动方法，并分析了各自的优缺点。此外，还分析了现有模型中信息融合的三个自下而上的操作层次。", "result": "论文对最先进的充电负荷建模方法进行了分类（统计、模拟、数据驱动），并探讨了它们的优缺点。同时，分析了现有模型中信息融合的三种自下而上的操作级别。", "conclusion": "文章总结了该领域面临的挑战和机遇，为未来的研究工作提供了指导，以增进理解并探索实用的研究方向。"}}
{"id": "2511.03882", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03882", "abs": "https://arxiv.org/abs/2511.03882", "authors": ["Florence Klitzner", "Blanca Inigo", "Benjamin D. Killeen", "Lalithkumar Seenivasan", "Michelle Song", "Axel Krieger", "Mathias Unberath"], "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures", "comment": null, "summary": "Imitation learning-based robot control policies are enjoying renewed interest\nin video-based robotics. However, it remains unclear whether this approach\napplies to X-ray-guided procedures, such as spine instrumentation. This is\nbecause interpretation of multi-view X-rays is complex. We examine\nopportunities and challenges for imitation policy learning in bi-plane-guided\ncannula insertion. We develop an in silico sandbox for scalable, automated\nsimulation of X-ray-guided spine procedures with a high degree of realism. We\ncurate a dataset of correct trajectories and corresponding bi-planar X-ray\nsequences that emulate the stepwise alignment of providers. We then train\nimitation learning policies for planning and open-loop control that iteratively\nalign a cannula solely based on visual information. This precisely controlled\nsetup offers insights into limitations and capabilities of this method. Our\npolicy succeeded on the first attempt in 68.5% of cases, maintaining safe\nintra-pedicular trajectories across diverse vertebral levels. The policy\ngeneralized to complex anatomy, including fractures, and remained robust to\nvaried initializations. Rollouts on real bi-planar X-rays further suggest that\nthe model can produce plausible trajectories, despite training exclusively in\nsimulation. While these preliminary results are promising, we also identify\nlimitations, especially in entry point precision. Full closed-look control will\nrequire additional considerations around how to provide sufficiently frequent\nfeedback. With more robust priors and domain knowledge, such models may provide\na foundation for future efforts toward lightweight and CT-free robotic\nintra-operative spinal navigation.", "AI": {"tldr": "本文探讨了模仿学习在X射线引导下脊柱手术中的应用，通过模拟沙盒训练了基于视觉信息的套管针对齐策略，并在复杂解剖结构中取得了有希望的初步结果。", "motivation": "基于视频的机器人模仿学习备受关注，但其在解释多视图X射线复杂的X射线引导手术（如脊柱器械植入）中的适用性尚不明确。", "method": "研究开发了一个高度逼真的、可扩展的X射线引导脊柱手术自动化模拟沙盒。他们策划了一个包含正确轨迹和双平面X射线序列的数据集，并训练了模仿学习策略，用于仅基于视觉信息进行规划和开环控制，以迭代对齐套管针。", "result": "该策略在68.5%的案例中首次尝试成功，在不同椎骨水平上保持了安全的椎弓根内轨迹。它能泛化到包括骨折在内的复杂解剖结构，并对不同的初始设置保持鲁棒性。在真实双平面X射线上的测试表明，尽管仅在模拟中训练，模型也能生成合理的轨迹。", "conclusion": "初步结果表明，该方法在实现轻量级、无CT的机器人术中脊柱导航方面具有潜力。但也存在局限性，特别是在入路点精度方面，并且完全闭环控制需要更频繁的反馈。未来工作需要更强大的先验知识和领域知识。"}}
{"id": "2511.03742", "categories": ["eess.SY", "cs.SE", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03742", "abs": "https://arxiv.org/abs/2511.03742", "authors": ["Angelos Alexopoulos", "Agorakis Bompotas", "Nikitas Rigas Kalogeropoulos", "Panagiotis Kechagias", "Athanasios P. Kalogeras", "Christos Alexakos"], "title": "A Model-Based Approach to Automated Digital Twin Generation in Manufacturing", "comment": "Accepted for presentation to 10th South-East Europe Design\n  Automation, Computer Engineering, Computer Networks and Social Media\n  Conference (SEEDA-CECNSM 2025)", "summary": "Modern manufacturing demands high flexibility and reconfigurability to adapt\nto dynamic production needs. Model-based Engineering (MBE) supports rapid\nproduction line design, but final reconfiguration requires simulations and\nvalidation. Digital Twins (DTs) streamline this process by enabling real-time\nmonitoring, simulation, and reconfiguration. This paper presents a novel\nplatform that automates DT generation and deployment using AutomationML-based\nfactory plans. The platform closes the loop with a GAI-powered simulation\nscenario generator and automatic physical line reconfiguration, enhancing\nefficiency and adaptability in manufacturing.", "AI": {"tldr": "本文提出一个新颖的平台，利用AutomationML自动化数字孪生生成与部署，并通过GAI驱动的仿真场景生成器和物理产线自动重构，实现制造过程的闭环，提高效率和适应性。", "motivation": "现代制造业要求高灵活性和可重构性以适应动态生产需求。模型化工程（MBE）支持快速产线设计，但最终重构需要仿真和验证。数字孪生（DTs）通过实时监控、仿真和重构简化了这一过程。", "method": "该平台通过基于AutomationML的工厂规划自动化数字孪生（DT）的生成和部署。它还集成了一个由GAI（生成式人工智能）驱动的仿真场景生成器，并支持物理产线的自动重构，从而形成闭环。", "result": "该平台通过自动化数字孪生生成、部署、GAI驱动的仿真和自动物理产线重构，显著提高了制造过程的效率和适应性。", "conclusion": "所提出的平台通过自动化数字孪生生成与部署，并结合GAI驱动的仿真场景生成和物理产线自动重构，成功实现了制造过程的闭环，从而提升了制造效率和适应性。"}}
{"id": "2511.03772", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03772", "abs": "https://arxiv.org/abs/2511.03772", "authors": ["Stergios Chatzikyriakidis", "Dimitris Papadakis", "Sevasti-Ioanna Papaioannou", "Erofili Psaltaki"], "title": "GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation", "comment": null, "summary": "We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the\nexisting GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern\nGreek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian\nGreek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a\ndataset with total size 6,374,939 words and 10 varieties. This is the first\ndataset with such variation and size to date. We conduct a number of\nfine-tuning experiments to see the effect of good quality dialectal data on a\nnumber of LLMs. We fine-tune three model architectures (Llama-3-8B,\nLlama-3.1-8B, Krikri-8B) and compare the results to frontier models\n(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).", "AI": {"tldr": "本文介绍了扩展的希腊方言数据集GRDD+，包含637万词和10种方言，并使用该数据集对多种LLM进行了微调实验，以评估高质量方言数据对模型性能的影响。", "motivation": "现有希腊方言数据集有限，且缺乏具有如此多样性和规模的数据集。研究旨在通过创建更全面的数据集，探索高质量方言数据对大型语言模型（LLM）性能的影响。", "method": "研究方法包括：1. 构建GRDD+数据集，通过补充现有GRDD数据并新增六种方言（如Greco-Corsican, Griko, Tsakonian等），最终达到6,374,939词和10种方言的规模。2. 使用GRDD+数据集对Llama-3-8B、Llama-3.1-8B和Krikri-8B三种LLM架构进行微调实验。3. 将微调结果与前沿模型（如Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5）进行比较。", "result": "研究结果是创建了一个迄今为止规模最大、方言种类最丰富的希腊方言数据集GRDD+（6,374,939词，10种方言）。此外，研究还通过一系列微调实验，展示了高质量方言数据对多种LLM性能的影响。", "conclusion": "高质量的方言数据对LLM的性能具有显著影响。GRDD+数据集是迄今为止第一个具有如此多样性和规模的希腊方言数据集，为未来方言相关的LLM研究提供了宝贵的资源。"}}
{"id": "2511.03888", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03888", "abs": "https://arxiv.org/abs/2511.03888", "authors": ["Abdulmumin Sa'ad", "Sulaimon Oyeniyi Adebayo", "Abdul Jabbar Siddiqui"], "title": "Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model", "comment": "8 pages", "summary": "The global waste crisis is escalating, with solid waste generation expected\nto increase by 70% by 2050. Traditional waste collection methods, particularly\nin remote or harsh environments like deserts, are labor-intensive, inefficient,\nand often hazardous. Recent advances in computer vision and deep learning have\nopened the door to automated waste detection systems, yet most research focuses\non urban environments and recyclable materials, overlooking organic and\nhazardous waste and underexplored terrains such as deserts. In this work, we\npropose an enhanced real-time object detection framework based on a pruned,\nlightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)\nand specialized data augmentation strategies. Using the DroneTrashNet dataset,\nwe demonstrate significant improvements in precision, recall, and mean average\nprecision (mAP), while achieving low latency and compact model size suitable\nfor deployment on resource-constrained aerial drones. Benchmarking our model\nagainst state-of-the-art lightweight YOLO variants further highlights its\noptimal balance of accuracy and efficiency. Our results validate the\neffectiveness of combining data-centric and model-centric enhancements for\nrobust, real-time waste detection in desert environments.", "AI": {"tldr": "该研究提出了一种基于剪枝轻量级YOLOv12的实时目标检测框架，结合自对抗训练和数据增强，用于在沙漠环境中通过无人机高效检测垃圾，并在准确性和效率之间取得了最佳平衡。", "motivation": "全球垃圾危机日益严重，传统垃圾收集方法效率低下且危险，尤其是在沙漠等偏远或恶劣环境中。现有研究多集中于城市环境和可回收物，忽视了有机和危险废物以及沙漠等未充分探索的地形，且缺乏适用于资源受限无人机的实时检测系统。", "method": "该研究提出了一种增强的实时目标检测框架，其核心是经过剪枝的轻量级YOLOv12版本。该框架集成了自对抗训练（SAT）和专门的数据增强策略。模型在DroneTrashNet数据集上进行训练和评估。", "result": "该模型在精度（precision）、召回率（recall）和平均精度均值（mAP）方面取得了显著提升，同时实现了低延迟和紧凑的模型尺寸，适用于部署在资源受限的空中无人机上。与现有最先进的轻量级YOLO变体进行基准测试表明，该模型在准确性和效率之间取得了最佳平衡。", "conclusion": "结合以数据为中心和以模型为中心的增强方法，能够有效实现沙漠环境中鲁棒、实时的垃圾检测。"}}
{"id": "2511.03823", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03823", "abs": "https://arxiv.org/abs/2511.03823", "authors": ["Jan Kocoń", "Maciej Piasecki", "Arkadiusz Janz", "Teddy Ferdinan", "Łukasz Radliński", "Bartłomiej Koptyra", "Marcin Oleksy", "Stanisław Woźniak", "Paweł Walkowiak", "Konrad Wojtasik", "Julia Moska", "Tomasz Naskręt", "Bartosz Walkowiak", "Mateusz Gniewkowski", "Kamil Szyc", "Dawid Motyka", "Dawid Banach", "Jonatan Dalasiński", "Ewa Rudnicka", "Bartłomiej Alberski", "Tomasz Walkowiak", "Aleksander Szczęsny", "Maciej Markiewicz", "Tomasz Bernaś", "Hubert Mazur", "Kamil Żyta", "Mateusz Tykierko", "Grzegorz Chodak", "Tomasz Kajdanowicz", "Przemysław Kazienko", "Agnieszka Karlińska", "Karolina Seweryn", "Anna Kołos", "Maciej Chrabąszcz", "Katarzyna Lorenc", "Aleksandra Krasnodębska", "Artur Wilczek", "Katarzyna Dziewulska", "Paula Betscher", "Zofia Cieślińska", "Katarzyna Kowol", "Daria Mikoś", "Maciej Trzciński", "Dawid Krutul", "Marek Kozłowski", "Sławomir Dadas", "Rafał Poświata", "Michał Perełkiewicz", "Małgorzata Grębowiec", "Maciej Kazuła", "Marcin Białas", "Roman Roszko", "Danuta Roszko", "Jurgita Vaičenonienė", "Andrius Utka", "Paweł Levchuk", "Paweł Kowalski", "Irena Prawdzic-Jankowska", "Maciej Ogrodniczuk", "Monika Borys", "Anna Bulińska", "Wiktoria Gumienna", "Witold Kieraś", "Dorota Komosińska", "Katarzyna Krasnowska-Kieraś", "Łukasz Kobyliński", "Martyna Lewandowska", "Marek Łaziński", "Mikołaj Łątkowski", "Dawid Mastalerz", "Beata Milewicz", "Agnieszka Anna Mykowiecka", "Angelika Peljak-Łapińska", "Sandra Penno", "Zuzanna Przybysz", "Michał Rudolf", "Piotr Rybak", "Karolina Saputa", "Aleksandra Tomaszewska", "Aleksander Wawer", "Marcin Woliński", "Joanna Wołoszyn", "Alina Wróblewska", "Bartosz Żuk", "Filip Żarnecki", "Konrad Kaczyński", "Anna Cichosz", "Zuzanna Deckert", "Monika Garnys", "Izabela Grabarczyk", "Wojciech Janowski", "Sylwia Karasińska", "Aleksandra Kujawiak", "Piotr Misztela", "Maria Szymańska", "Karolina Walkusz", "Igor Siek", "Jakub Kwiatkowski", "Piotr Pęzik"], "title": "PLLuM: A Family of Polish Large Language Models", "comment": "83 pages, 19 figures", "summary": "Large Language Models (LLMs) play a central role in modern artificial\nintelligence, yet their development has been primarily focused on English,\nresulting in limited support for other languages. We present PLLuM (Polish\nLarge Language Model), the largest open-source family of foundation models\ntailored specifically for the Polish language. Developed by a consortium of\nmajor Polish research institutions, PLLuM addresses the need for high-quality,\ntransparent, and culturally relevant language models beyond the English-centric\ncommercial landscape. We describe the development process, including the\nconstruction of a new 140-billion-token Polish text corpus for pre-training, a\n77k custom instructions dataset, and a 100k preference optimization dataset. A\nkey component is a Responsible AI framework that incorporates strict data\ngovernance and a hybrid module for output correction and safety filtering. We\ndetail the models' architecture, training procedures, and alignment techniques\nfor both base and instruction-tuned variants, and demonstrate their utility in\na downstream task within public administration. By releasing these models\npublicly, PLLuM aims to foster open research and strengthen sovereign AI\ntechnologies in Poland.", "AI": {"tldr": "PLLuM是为波兰语量身定制的最大开源基础模型系列，旨在解决现有大模型以英语为中心的问题，通过构建大规模波兰语语料库和数据集，并结合负责任AI框架开发而成，已公开用于促进波兰的开放研究和主权AI技术。", "motivation": "当前大型语言模型（LLMs）主要关注英语，导致对其他语言的支持有限。研究动机是为了满足对高质量、透明且与波兰文化相关的语言模型的需求，超越以英语为中心的商业模型。", "method": "开发了PLLuM（Polish Large Language Model）系列，包括构建了一个新的1400亿token的波兰语预训练文本语料库，一个7.7万条自定义指令数据集，以及一个10万条偏好优化数据集。关键组成部分是一个负责任AI框架，包含严格的数据治理和用于输出校正与安全过滤的混合模块。详细描述了模型架构、训练过程以及基础模型和指令微调模型的对齐技术。", "result": "成功开发了PLLuM，这是为波兰语量身定制的最大开源基础模型系列。该模型在公共管理领域的下游任务中展示了其效用，并已公开发布。", "conclusion": "PLLuM的发布旨在促进波兰的开放研究，并加强波兰的主权AI技术，为波兰语提供了高质量、透明且文化相关的语言模型。"}}
{"id": "2511.04052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04052", "abs": "https://arxiv.org/abs/2511.04052", "authors": ["Kyongsik Yun", "David Bayard", "Gerik Kubiak", "Austin Owens", "Andrew Johnson", "Ryan Johnson", "Dan Scharf", "Thomas Lu"], "title": "Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors", "comment": null, "summary": "Future planetary exploration missions demand high-performance, fault-tolerant\ncomputing to enable autonomous Guidance, Navigation, and Control (GNC) and\nLander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).\nThis paper evaluates the deployment of GNC and LVS algorithms on\nnext-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx\nVersal--demonstrating up to 15x speedup for LVS image processing and over 250x\nspeedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory\noptimization compared to legacy spaceflight hardware. To ensure computational\nreliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for\nTrusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that\nperforms real-time fault detection and correction across redundant cores.\nARBITER is validated in both static optimization tasks (GFOLD) and dynamic\nclosed-loop control (Attitude Control System). A fault injection study further\nidentifies the gradient computation stage in GFOLD as the most sensitive to\nbit-level errors, motivating selective protection strategies and vector-based\noutput arbitration. This work establishes a scalable and energy-efficient\narchitecture for future missions, including Mars Sample Return, Enceladus\nOrbilander, and Ceres Sample Return, where onboard autonomy, low latency, and\nfault resilience are critical.", "AI": {"tldr": "本文评估了下一代多核处理器在行星探索任务中实现自主制导、导航与控制(GNC)和着陆器视觉系统(LVS)的性能和容错能力。通过引入多核投票机制ARBITER，实现了显著的速度提升和实时故障检测与纠正，为未来任务提供了可扩展、节能且容错的计算架构。", "motivation": "未来的行星探索任务，尤其是在进入、下降和着陆(EDL)阶段，需要高性能、容错的计算能力来支持自主GNC和LVS操作。传统空间飞行硬件已无法满足这些需求。", "method": "研究方法包括：1) 在HPSC、Snapdragon VOXL2和AMD Xilinx Versal等多核处理器上部署并评估GNC和LVS算法。2) 提出ARBITER（异步冗余行为检查以实现可信执行和恢复），一种多核投票(MV)机制，用于跨冗余核心进行实时故障检测和纠正。3) 在静态优化任务（GFOLD）和动态闭环控制（姿态控制系统）中验证ARBITER。4) 进行故障注入研究，以识别对位级错误最敏感的计算阶段。", "result": "主要成果包括：1) LVS图像处理速度提升高达15倍。2) GFOLD轨迹优化速度比传统硬件提升超过250倍。3) ARBITER机制有效实现了跨冗余核心的实时故障检测和纠正。4) 故障注入研究发现GFOLD中的梯度计算阶段对位级错误最敏感，这促使研究者提出选择性保护策略和基于向量的输出仲裁。", "conclusion": "本研究为未来行星任务（如火星样本返回、土卫二轨道着陆器、谷神星样本返回）建立了一个可扩展、节能且容错的计算架构，这些任务对机载自主性、低延迟和故障恢复能力有严格要求。"}}
{"id": "2511.03948", "categories": ["cs.AI", "cs.HC", "I.2.6; K.3.1"], "pdf": "https://arxiv.org/pdf/2511.03948", "abs": "https://arxiv.org/abs/2511.03948", "authors": ["Kevin Hong", "Kia Karbasi", "Gregory Pottie"], "title": "Extracting Causal Relations in Deep Knowledge Tracing", "comment": "Accepted for publication in the Proceedings of the 18th International\n  Conference on Educational Data Mining, 6 pages, 1 figure", "summary": "A longstanding goal in computational educational research is to develop\nexplainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which\nleverages a Recurrent Neural Network (RNN) to predict student knowledge and\nperformance on exercises, has been proposed as a major advancement over\ntraditional KT methods. Several studies suggest that its performance gains stem\nfrom its ability to model bidirectional relationships between different\nknowledge components (KCs) within a course, enabling the inference of a\nstudent's understanding of one KC from their performance on others. In this\npaper, we challenge this prevailing explanation and demonstrate that DKT's\nstrength lies in its implicit ability to model prerequisite relationships as a\ncausal structure, rather than bidirectional relationships. By pruning exercise\nrelation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal\nsubsets of the Assistments dataset, we show that DKT's predictive capabilities\nalign strongly with these causal structures. Furthermore, we propose an\nalternative method for extracting exercise relation DAGs using DKT's learned\nrepresentations and provide empirical evidence supporting our claim. Our\nfindings suggest that DKT's effectiveness is largely driven by its capacity to\napproximate causal dependencies between KCs rather than simple relational\nmappings.", "AI": {"tldr": "本文挑战了深度知识追踪（DKT）模型通过建模知识成分（KC）之间的双向关系来提高性能的普遍解释。研究表明，DKT的真正优势在于其隐式地建模了KC之间的先决条件/因果关系，而非双向关系。", "motivation": "计算教育研究的一个长期目标是开发可解释的知识追踪（KT）模型。DKT被认为是传统KT方法的重大进步，但其性能提升的普遍解释（即建模双向关系）受到质疑。本文旨在揭示DKT性能提升的真正机制。", "method": "研究通过将练习关系图修剪成有向无环图（DAGs），并在Assistments数据集的因果子集上训练DKT来证明DKT的预测能力与因果结构高度一致。此外，还提出了一种利用DKT学习到的表示来提取练习关系DAGs的替代方法。", "result": "实验结果表明，DKT的预测能力与因果结构（即先决条件关系）高度吻合。通过DKT学习到的表示提取练习关系DAGs的方法也提供了经验证据支持其主张，即DKT能够近似KC之间的因果依赖关系。", "conclusion": "研究得出结论，DKT的有效性主要源于其近似KC之间因果依赖关系的能力，而非简单的关系映射或双向关系。这为理解DKT的工作原理提供了新的视角。"}}
{"id": "2511.03743", "categories": ["eess.SY", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SP", "68T05 (Learning and adaptive systems) 93C95 (Neural networks in\n  control theory)", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.03743", "abs": "https://arxiv.org/abs/2511.03743", "authors": ["Marios Impraimakis"], "title": "A convolutional neural network deep learning method for model class selection", "comment": "31 pages, 16 figures, published in Earthquake Engineering &\n  Structural Dynamics", "summary": "The response-only model class selection capability of a novel deep\nconvolutional neural network method is examined herein in a simple, yet\neffective, manner. Specifically, the responses from a unique degree of freedom\nalong with their class information train and validate a one-dimensional\nconvolutional neural network. In doing so, the network selects the model class\nof new and unlabeled signals without the need of the system input information,\nor full system identification. An optional physics-based algorithm enhancement\nis also examined using the Kalman filter to fuse the system response signals\nusing the kinematics constraints of the acceleration and displacement data.\nImportantly, the method is shown to select the model class in slight signal\nvariations attributed to the damping behavior or hysteresis behavior on both\nlinear and nonlinear dynamic systems, as well as on a 3D building finite\nelement model, providing a powerful tool for structural health monitoring\napplications.", "AI": {"tldr": "本文提出一种新颖的深度一维卷积神经网络方法，仅利用响应信号即可进行模型类别选择，并通过卡尔曼滤波器增强，在结构健康监测中对线性和非线性系统均有效。", "motivation": "研究如何仅利用系统响应信号及其类别信息，在无需系统输入或完整系统识别的情况下，选择新信号的模型类别，以应用于结构健康监测。", "method": "使用来自单一自由度的响应及其类别信息训练和验证一维卷积神经网络。该网络能够为新的、未标记的信号选择模型类别，无需系统输入信息或完整的系统识别。此外，还探索了一种可选的基于物理的算法增强方法，即使用卡尔曼滤波器通过加速度和位移数据的运动学约束来融合系统响应信号。", "result": "该方法被证明能够识别由阻尼行为或滞后行为引起的微小信号变化，成功选择模型类别。它在线性和非线性动态系统上以及在三维建筑有限元模型上均有效。", "conclusion": "该方法为结构健康监测应用提供了一个强大的工具，能够有效地进行模型类别选择。"}}
{"id": "2511.04109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04109", "abs": "https://arxiv.org/abs/2511.04109", "authors": ["Yanbo Pang", "Qingkai Li", "Mingguo Zhao"], "title": "CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN", "comment": null, "summary": "As robotic arm applications extend beyond industrial settings into\nhealthcare, service, and daily life, existing control algorithms struggle to\nachieve the agile manipulation required for complex environments with dynamic\ntrajectories, unpredictable interactions, and diverse objects. This paper\npresents a biomimetic control framework based on Spiking Neural Networks (SNN),\ninspired by the human Central Nervous System (CNS), to achieve agile control in\nsuch environments. The proposed framework features five control modules\n(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three\nhierarchical control levels (first-order, second-order, third-order), and two\ninformation pathways (ascending, descending). Each module is fully implemented\nusing SNN. The spinal cord module uses spike encoding and Leaky\nIntegrate-and-Fire (LIF) neurons for feedback control. The brainstem module\nemploys a network of LIF and non-spiking LIF neurons to dynamically adjust\nspinal cord parameters via reinforcement learning. The thalamus module\nsimilarly adjusts the cerebellum's torque outputs. The cerebellum module uses a\nrecurrent SNN to learn the robotic arm's dynamics through regression, providing\nfeedforward gravity compensation torques. The framework is validated both in\nsimulation and on real-world robotic arm platform under various loads and\ntrajectories. Results demonstrate that our method outperforms the\nindustrial-grade position control in manipulation agility.", "AI": {"tldr": "本文提出了一种基于脉冲神经网络（SNN）的仿生控制框架，灵感来源于人类中枢神经系统（CNS），旨在为机器人手臂在复杂动态环境中实现敏捷操控，并实验证明其优于工业级位置控制。", "motivation": "现有机器人手臂控制算法难以在医疗、服务和日常生活中复杂、动态、不可预测且物体多样的环境中实现所需的敏捷操控。", "method": "该研究提出一个仿生控制框架，包含五个控制模块（大脑皮层、小脑、丘脑、脑干、脊髓）、三个分层控制级别和两条信息通路，所有模块均使用SNN实现。脊髓模块采用脉冲编码和LIF神经元进行反馈控制。脑干模块通过强化学习动态调整脊髓参数。丘脑模块调整小脑的扭矩输出。小脑模块使用循环SNN学习机械臂动力学，提供前馈重力补偿扭矩。该框架在仿真和实际机器人手臂平台上进行了验证。", "result": "实验结果表明，该方法在操作敏捷性方面优于工业级位置控制。", "conclusion": "所提出的基于SNN的仿生控制框架能够有效实现机器人手臂在复杂环境中的敏捷控制。"}}
{"id": "2511.03744", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03744", "abs": "https://arxiv.org/abs/2511.03744", "authors": ["Navid Mojahed", "Mahdis Rabbani", "Shima Nazari"], "title": "Predictive Compensation in Finite-Horizon LQ Games under Gauss-Markov Deviations", "comment": "10 pages, 4 Figures, Preprint submitted to Control Engineering\n  Practice (Elsevier)", "summary": "This paper presents a predictive compensation framework for finite-horizon\ndiscrete-time linear quadratic dynamic games in the presence of Gauss-Markov\ndeviations from feedback Nash strategies. One player experiences correlated\nstochastic deviations, modeled via a first-order autoregressive process, while\nthe other compensates using a predictive strategy that anticipates the effect\nof future correlation. Closed-form recursions for mean and covariance\npropagation are derived, and the resulting performance improvement is analyzed\nthrough the sensitivity of expected cost.", "AI": {"tldr": "本文提出了一种预测补偿框架，用于处理有限时域离散时间线性二次动态博弈中，当一方存在高斯-马尔可夫偏差时，另一方进行补偿。", "motivation": "研究动机是解决动态博弈中，当一方玩家偏离反馈纳什策略时，其偏差表现为相关随机过程（高斯-马尔可夫），另一方如何通过预测策略进行有效补偿以提高性能。", "method": "该研究通过一阶自回归过程建模一方玩家的相关随机偏差。另一方则采用预测策略进行补偿，该策略能预见未来相关性的影响。文章推导了均值和协方差传播的闭式递归公式。", "result": "研究结果包括推导了均值和协方差传播的闭式递归公式，并通过预期成本的敏感性分析了由此带来的性能改进。", "conclusion": "该预测补偿框架能有效应对动态博弈中存在相关随机偏差的情况，通过预测未来相关性，显著提高了补偿方的博弈性能。"}}
{"id": "2511.03980", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03980", "abs": "https://arxiv.org/abs/2511.03980", "authors": ["Bram Bulté", "Ayla Rigouts Terryn"], "title": "LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing", "comment": "Preprint under review at Computational Linguistics. Accepted with\n  minor revisions (10/10/2025); second round", "summary": "Large Language Models (LLMs) are rapidly being adopted by users across the\nglobe, who interact with them in a diverse range of languages. At the same\ntime, there are well-documented imbalances in the training data and\noptimisation objectives of this technology, raising doubts as to whether LLMs\ncan represent the cultural diversity of their broad user base. In this study,\nwe look at LLMs and cultural values and examine how prompt language and\ncultural framing influence model responses and their alignment with human\nvalues in different countries. We probe 10 LLMs with 63 items from the Hofstede\nValues Survey Module and World Values Survey, translated into 11 languages, and\nformulated as prompts with and without different explicit cultural\nperspectives. Our study confirms that both prompt language and cultural\nperspective produce variation in LLM outputs, but with an important caveat:\nWhile targeted prompting can, to a certain extent, steer LLM responses in the\ndirection of the predominant values of the corresponding countries, it does not\novercome the models' systematic bias toward the values associated with a\nrestricted set of countries in our dataset: the Netherlands, Germany, the US,\nand Japan. All tested models, regardless of their origin, exhibit remarkably\nsimilar patterns: They produce fairly neutral responses on most topics, with\nselective progressive stances on issues such as social tolerance. Alignment\nwith cultural values of human respondents is improved more with an explicit\ncultural perspective than with a targeted prompt language. Unexpectedly,\ncombining both approaches is no more effective than cultural framing with an\nEnglish prompt. These findings reveal that LLMs occupy an uncomfortable middle\nground: They are responsive enough to changes in prompts to produce variation,\nbut too firmly anchored to specific cultural defaults to adequately represent\ncultural diversity.", "AI": {"tldr": "研究发现，尽管提示语言和文化框架能影响大型语言模型（LLMs）的输出，但LLMs仍存在系统性偏见，倾向于特定国家（荷兰、德国、美国、日本）的价值观，难以充分代表全球文化多样性。", "motivation": "LLMs在全球范围内被不同语言用户广泛采用，但其训练数据和优化目标可能存在不平衡，引发了对LLMs能否代表其广泛用户群文化多样性的质疑。", "method": "本研究使用Hofstede价值观调查模块和世界价值观调查中的63个项目，将其翻译成11种语言，并以包含或不包含明确文化视角的提示形式，对10个LLMs进行了探测。", "result": "提示语言和文化视角都会导致LLM输出的变化。虽然有针对性的提示能在一定程度上使LLM响应趋向相应国家的主导价值观，但无法克服模型对荷兰、德国、美国和日本等少数国家价值观的系统性偏见。所有测试模型都表现出相似的模式：对大多数话题持中立态度，但在社会容忍等问题上持选择性进步立场。明确的文化视角比有针对性的提示语言更能提高模型与人类文化价值观的一致性。出乎意料的是，结合这两种方法并不比使用英语提示进行文化框架更有效。", "conclusion": "LLMs处于一个“令人不安的中间地带”：它们对提示的变化足够敏感以产生差异，但又过于牢固地锚定于特定的文化默认值，无法充分代表文化多样性。"}}
{"id": "2511.03830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03830", "abs": "https://arxiv.org/abs/2511.03830", "authors": ["Mikołaj Langner", "Jan Eliasz", "Ewa Rudnicka", "Jan Kocoń"], "title": "Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification", "comment": "9 pages, 8 figures", "summary": "We introduce a method for efficient multi-label text classification with\nlarge language models (LLMs), built on reformulating classification tasks as\nsequences of dichotomic (yes/no) decisions. Instead of generating all labels in\na single structured response, each target dimension is queried independently,\nwhich, combined with a prefix caching mechanism, yields substantial efficiency\ngains for short-text inference without loss of accuracy. To demonstrate the\napproach, we focus on affective text analysis, covering 24 dimensions including\nemotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator\nmodel (DeepSeek-V3) provides multiple annotations per text, which are\naggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,\nGemma3-1B). The fine-tuned models show significant improvements over zero-shot\nbaselines, particularly on the dimensions seen during training. Our findings\nsuggest that decomposing multi-label classification into dichotomic queries,\ncombined with distillation and cache-aware inference, offers a scalable and\neffective framework for LLM-based classification. While we validate the method\non affective states, the approach is general and applicable across domains.", "AI": {"tldr": "该研究提出一种高效的多标签文本分类方法，通过将任务分解为一系列二分决策，并结合前缀缓存和LLM-to-SLM蒸馏，在不损失准确性的前提下显著提升了推理效率。", "motivation": "动机是提高大型语言模型（LLMs）在多标签文本分类任务中的效率，尤其是在短文本推理场景下，同时保持或提高分类准确性。", "method": "该方法将多标签分类任务重构为一系列二分（是/否）决策。每个目标维度独立查询，并结合前缀缓存机制以提高效率。此外，采用LLM-to-SLM蒸馏技术，使用强大的LLM（DeepSeek-V3）生成多重标注，聚合后用于微调较小的模型（如HerBERT-Large、CLARIN-1B、PLLuM-8B、Gemma3-1B）。研究以情感文本分析（24个维度）为例进行验证。", "result": "结果显示，该方法在短文本推理方面取得了显著的效率提升，且未损失准确性。经过微调的小型模型在零样本基线上表现出显著改进，尤其是在训练期间见过的维度上。这些发现表明，将多标签分类分解为二分查询，结合蒸馏和缓存感知推理，提供了一个可扩展且有效的LLM分类框架。", "conclusion": "将多标签分类分解为二分查询，结合蒸馏和缓存感知推理，为基于LLM的分类提供了一个可扩展且有效的框架。尽管该方法在情感状态分析中得到验证，但其具有通用性，可应用于不同领域。"}}
{"id": "2511.03891", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.03891", "abs": "https://arxiv.org/abs/2511.03891", "authors": ["Hlali Azzeddine", "Majid Ben Yakhlef", "Soulaiman El Hazzat"], "title": "Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition", "comment": null, "summary": "Small, imbalanced datasets and poor input image quality can lead to high\nfalse predictions rates with deep learning models. This paper introduces\nClass-Based Image Composition, an approach that allows us to reformulate\ntraining inputs through a fusion of multiple images of the same class into\ncombined visual composites, named Composite Input Images (CoImg). That enhances\nthe intra-class variance and improves the valuable information density per\ntraining sample and increases the ability of the model to distinguish between\nsubtle disease patterns. Our method was evaluated on the Optical Coherence\nTomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et\nal., 2024), which contains 2,064 high-resolution optical coherence tomography\n(OCT) scans of the human retina, representing seven distinct diseases with a\nsignificant class imbalance. We constructed a perfectly class-balanced version\nof this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout\ncomposite image. To assess the effectiveness of this new representation, we\nconducted a comparative analysis between the original dataset and its variant\nusing a VGG16 model. A fair comparison was ensured by utilizing the identical\nmodel architecture and hyperparameters for all experiments. The proposed\napproach markedly improved diagnostic results.The enhanced Dataset achieved\nnear-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared\nto a baseline model trained on raw dataset. The false prediction rate was also\nsignificantly lower, this demonstrates that the method can producehigh-quality\npredictions even for weak datasets affected by class imbalance or small sample\nsize.", "AI": {"tldr": "本文提出了一种名为“基于类的图像合成”（Class-Based Image Composition）的方法，通过将同一类别的多张图像融合成复合输入图像（CoImg），增强了类内方差和信息密度，显著提高了深度学习模型在小型、不平衡或低质量数据集上的诊断准确性，降低了错误预测率。", "motivation": "深度学习模型在处理小型、不平衡数据集或输入图像质量差时，往往会导致较高的错误预测率。", "method": "该研究引入了“基于类的图像合成”方法，将同一类别的多张图像融合为复合输入图像（CoImg），以增强类内方差并提高每个训练样本的有价值信息密度。该方法在光学相干断层扫描数据集（OCTDL）上进行评估，构建了一个完全类别平衡的Co-OCTDL数据集，其中每个扫描都表示为3x1布局的复合图像。研究使用VGG16模型对原始数据集及其变体进行了比较分析，确保了模型架构和超参数的一致性以进行公平比较。", "result": "所提出的方法显著改善了诊断结果。经过增强的数据集（Co-OCTDL）实现了接近完美的准确率（99.6%），F1分数（0.995）和AUC（0.9996），远优于在原始数据集上训练的基线模型。错误预测率也显著降低。", "conclusion": "该方法即使对于受类别不平衡或样本量小影响的“弱数据集”，也能产生高质量的预测，证明了其有效性。"}}
{"id": "2511.03827", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03827", "abs": "https://arxiv.org/abs/2511.03827", "authors": ["Mohammad Atif Quamar", "Mohammad Areeb", "Mikhail Kuznetsov", "Muslum Ozgur Ozmen", "Z. Berkay Celik"], "title": "STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models", "comment": "Presented at the 2nd Workshop on Frontiers in Probabilistic\n  Inference: Sampling Meets Learning (NeurIPS 2025)", "summary": "Aligning large language models with human values is crucial for their safe\ndeployment; however, existing methods, such as fine-tuning, are computationally\nexpensive and suboptimal. In contrast, inference-time approaches like Best-of-N\nsampling require practically infeasible computation to achieve optimal\nalignment. We propose STARS: Segment-level Token Alignment with Rejection\nSampling, a decoding-time algorithm that steers model generation by iteratively\nsampling, scoring, and rejecting/accepting short, fixed-size token segments.\nThis allows for early correction of the generation path, significantly\nimproving computational efficiency and boosting alignment quality. Across a\nsuite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)\nby up to 14.9 percentage points and Direct Preference Optimization (DPO) by up\nto 4.3 percentage points on win-rates, while remaining highly competitive with\nstrong Best-of-N baselines. Our work establishes granular, reward-guided\nsampling as a generalizable, robust, and efficient alternative to traditional\nfine-tuning and full-sequence ranking methods for aligning LLMs.", "AI": {"tldr": "本文提出STARS，一种解码时算法，通过迭代采样、评分和拒绝/接受固定大小的令牌片段，在计算效率和对齐质量上显著优于现有微调方法，并与强大的Best-of-N基线竞争，实现LLM与人类价值观的对齐。", "motivation": "现有的大语言模型对齐方法，如微调，计算成本高昂且次优；而像Best-of-N这样的推理时方法，要实现最佳对齐则需要不切实际的计算量。", "method": "本文提出STARS（Segment-level Token Alignment with Rejection Sampling），一种解码时算法。它通过迭代地采样、评分和拒绝/接受短的、固定大小的令牌片段来引导模型生成。这种方法允许对生成路径进行早期修正。", "result": "在六个LLM上，STARS在胜率方面比监督微调（SFT）高出多达14.9个百分点，比直接偏好优化（DPO）高出多达4.3个百分点，同时与强大的Best-of-N基线保持高度竞争力。", "conclusion": "STARS确立了细粒度、奖励引导的采样作为一种通用、鲁棒且高效的替代方案，用于对齐LLM，优于传统的微调和全序列排序方法。"}}
{"id": "2511.04131", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04131", "abs": "https://arxiv.org/abs/2511.04131", "authors": ["Yitang Li", "Zhengyi Luo", "Tonghe Zhang", "Cunxi Dai", "Anssi Kanervisto", "Andrea Tirinzoni", "Haoyang Weng", "Kris Kitani", "Mateusz Guzek", "Ahmed Touati", "Alessandro Lazaric", "Matteo Pirotta", "Guanya Shi"], "title": "BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning", "comment": null, "summary": "Building Behavioral Foundation Models (BFMs) for humanoid robots has the\npotential to unify diverse control tasks under a single, promptable generalist\npolicy. However, existing approaches are either exclusively deployed on\nsimulated humanoid characters, or specialized to specific tasks such as\ntracking. We propose BFM-Zero, a framework that learns an effective shared\nlatent representation that embeds motions, goals, and rewards into a common\nspace, enabling a single policy to be prompted for multiple downstream tasks\nwithout retraining. This well-structured latent space in BFM-Zero enables\nversatile and robust whole-body skills on a Unitree G1 humanoid in the real\nworld, via diverse inference methods, including zero-shot motion tracking, goal\nreaching, and reward optimization, and few-shot optimization-based adaptation.\nUnlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds\nupon recent advancements in unsupervised RL and Forward-Backward (FB) models,\nwhich offer an objective-centric, explainable, and smooth latent representation\nof whole-body motions. We further extend BFM-Zero with critical reward shaping,\ndomain randomization, and history-dependent asymmetric learning to bridge the\nsim-to-real gap. Those key design choices are quantitatively ablated in\nsimulation. A first-of-its-kind model, BFM-Zero establishes a step toward\nscalable, promptable behavioral foundation models for whole-body humanoid\ncontrol.", "AI": {"tldr": "BFM-Zero是一个为人形机器人设计的框架，它通过学习共享的潜在表示，将运动、目标和奖励统一在一个空间中，从而实现单一策略在真实世界中无需重新训练即可处理多种全身控制任务，并结合无监督强化学习和前向-后向模型来弥合仿真与现实的差距。", "motivation": "现有的人形机器人控制方法要么仅限于模拟环境，要么专门用于特定任务（如跟踪），缺乏一个能统一处理多样化控制任务的、可提示的通用策略。", "method": "BFM-Zero框架学习一个共享的潜在表示，将运动、目标和奖励嵌入共同空间。它基于无监督强化学习和前向-后向（FB）模型，提供客观、可解释和平滑的潜在表示。通过多种推理方法实现任务，包括零样本运动跟踪、目标达成和奖励优化，以及少样本基于优化的适应。为弥合仿真与现实差距，还引入了奖励塑形、域随机化和依赖历史的非对称学习。", "result": "BFM-Zero在真实世界的Unitree G1人形机器人上实现了多功能且鲁棒的全身技能。其结构良好的潜在空间使得单个策略无需重新训练即可应对多个下游任务，并成功展示了零样本运动跟踪、目标达成、奖励优化以及少样本适应能力。关键设计选择在仿真中得到了定量验证。", "conclusion": "BFM-Zero是构建可扩展、可提示的全身人形机器人行为基础模型的重要一步，通过统一的潜在表示和先进的学习方法，实现了在真实世界中的多任务控制。"}}
{"id": "2511.03985", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03985", "abs": "https://arxiv.org/abs/2511.03985", "authors": ["Zhuowen Yuan", "Tao Liu", "Yang Yang", "Yang Wang", "Feng Qi", "Kaushik Rangadurai", "Bo Li", "Shuang Yang"], "title": "ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering", "comment": null, "summary": "Recent LLM-based agents have demonstrated strong capabilities in automated ML\nengineering. However, they heavily rely on repeated full training runs to\nevaluate candidate solutions, resulting in significant computational overhead,\nlimited scalability to large search spaces, and slow iteration cycles. To\naddress these challenges, we introduce ArchPilot, a multi-agent system that\nintegrates architecture generation, proxy-based evaluation, and adaptive search\ninto a unified framework. ArchPilot consists of three specialized agents: an\norchestration agent that coordinates the search process using a Monte Carlo\nTree Search (MCTS)-inspired novel algorithm with a restart mechanism and\nmanages memory of previous candidates; a generation agent that iteratively\ngenerates, improves, and debugs candidate architectures; and an evaluation\nagent that executes proxy training runs, generates and optimizes proxy\nfunctions, and aggregates the proxy scores into a fidelity-aware performance\nmetric. This multi-agent collaboration allows ArchPilot to prioritize\nhigh-potential candidates with minimal reliance on expensive full training\nruns, facilitating efficient ML engineering under limited budgets. Experiments\non MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE\nand ML-Master, validating the effectiveness of our multi-agent system.", "AI": {"tldr": "ArchPilot是一个多智能体系统，通过集成架构生成、代理评估和自适应搜索，显著降低了基于LLM的ML工程中昂贵的完整训练运行的计算开销。", "motivation": "现有的基于LLM的智能体在自动化ML工程中表现出色，但它们过度依赖重复的完整训练运行来评估候选方案，导致计算开销巨大、对大规模搜索空间的可扩展性有限以及迭代周期缓慢。", "method": "ArchPilot是一个多智能体系统，包含三个专业智能体：一个编排智能体，使用受蒙特卡洛树搜索（MCTS）启发的算法协调搜索过程并管理记忆；一个生成智能体，迭代生成、改进和调试候选架构；一个评估智能体，执行代理训练、生成和优化代理函数，并将代理分数聚合成一个考虑保真度的性能指标。这种多智能体协作结合了代理评估和自适应搜索。", "result": "在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML-Master等SOTA基线，验证了其多智能体系统的有效性。", "conclusion": "ArchPilot通过优先处理高潜力候选方案，最大限度地减少对昂贵完整训练运行的依赖，从而在有限预算下实现高效的ML工程。"}}
{"id": "2511.03912", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03912", "abs": "https://arxiv.org/abs/2511.03912", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "title": "I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging", "comment": null, "summary": "Unknown anomaly detection in medical imaging remains a fundamental challenge\ndue to the scarcity of labeled anomalies and the high cost of expert\nsupervision. We introduce an unsupervised, oracle-free framework that\nincrementally expands a trusted set of normal samples without any anomaly\nlabels. Starting from a small, verified seed of normal images, our method\nalternates between lightweight adapter updates and uncertainty-gated sample\nadmission. A frozen pretrained vision backbone is augmented with tiny\nconvolutional adapters, ensuring rapid domain adaptation with negligible\ncomputational overhead. Extracted embeddings are stored in a compact coreset\nenabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during\nincremental expansion is enforced by dual probabilistic gates, a sample is\nadmitted into the normal memory only if its distance to the existing coreset\nlies within a calibrated z-score threshold, and its SWAG-based epistemic\nuncertainty remains below a seed-calibrated bound. This mechanism prevents\ndrift and false inclusions without relying on generative reconstruction or\nreplay buffers. Empirically, our system steadily refines the notion of\nnormality as unlabeled data arrive, producing substantial gains over baselines.\nOn COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on\nPneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,\nROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These\nresults highlight the effectiveness and efficiency of the proposed framework\nfor real-world, label-scarce medical imaging applications.", "AI": {"tldr": "本文提出了一种无监督、无需预言机的框架，用于医学图像中的未知异常检测。该框架通过轻量级适配器更新和不确定性门控样本准入，逐步扩展正常样本集，实现了显著的性能提升。", "motivation": "医学图像中未知异常检测面临标注异常稀缺和专家监督成本高昂的挑战。", "method": "该方法从少量经过验证的正常图像种子集开始，无监督地逐步扩展可信的正常样本集。它交替进行轻量级适配器更新和不确定性门控样本准入。通过微小卷积适配器增强冻结的预训练视觉骨干网络，实现快速域适应。提取的嵌入存储在紧凑的核心集（coreset）中，以实现高效的k-NN异常评分。通过双重概率门（z-score阈值距离和基于SWAG的认知不确定性阈值）确保增量扩展的安全性，避免了生成式重建或重放缓冲区。", "result": "该系统持续优化正常性概念，在COVID-CXR数据集上，ROC-AUC从0.9489提升至0.9982；在肺炎CXR数据集上，ROC-AUC从0.6834提升至0.8968；在脑部MRI ND-5数据集上，ROC-AUC从0.6041提升至0.7269，PR-AUC从0.7539提升至0.8211，均显著优于基线方法。", "conclusion": "所提出的框架对于真实世界中标签稀缺的医学图像应用具有高效性和有效性。"}}
{"id": "2511.03745", "categories": ["eess.SY", "cs.CE", "cs.SY", "65L06, 37B10, 68W30, 70E60, 770E15, 70-10", "G.1.3; G.1.7; G.1.10; I.6.3; J.2; J.6"], "pdf": "https://arxiv.org/pdf/2511.03745", "abs": "https://arxiv.org/abs/2511.03745", "authors": ["Osama A. Marzouk"], "title": "InvSim algorithm for pre-computing airplane flight controls in limited-range autonomous missions, and demonstration via double-roll maneuver of Mirage III fighters", "comment": "47 pages, 20 figures, 10 tables, published journal article,\n  peer-reviewed, open access", "summary": "In this work, we start with a generic mathematical framework for the\nequations of motion (EOM) in flight mechanics with six degrees of freedom\n(6-DOF) for a general (not necessarily symmetric) fixed-wing aircraft. This\nmathematical framework incorporates (1) body axes (fixed in the airplane at its\ncenter of gravity), (2) inertial axes (fixed in the earth/ground at the\ntake-off point), wind axes (aligned with the flight path/course), (3) spherical\nflight path angles (azimuth angle measured clockwise from the geographic north,\nand elevation angle measured above the horizon plane), and (4) spherical flight\nangles (angle of attack and sideslip angle). We then manipulate these equations\nof motion to derive a customized version suitable for inverse simulation flight\nmechanics, where a target flight trajectory is specified while a set of\ncorresponding necessary flight controls to achieve that maneuver are predicted.\nWe then present a numerical procedure for integrating the developed inverse\nsimulation (InvSim) system in time; utilizing (1) symbolic mathematics, (2)\nexplicit fourth-order Runge-Kutta (RK4) numerical integration technique, and\n(3) expressions based on the finite difference method (FDM); such that the four\nnecessary control variables (engine thrust force, ailerons' deflection angle,\nelevators' deflection angle, and rudder's deflection angle) are computed as\ndiscrete values over the entire maneuver time, and these calculated control\nvalues enable the airplane to achieve the desired flight trajectory, which is\nspecified by three inertial Cartesian coordinates of the airplane, in addition\nto the Euler's roll angle. We finally demonstrate the proposed numerical\nprocedure of flight mechanics inverse simulation (InvSim).", "AI": {"tldr": "本文提出了一种针对六自由度固定翼飞机运动方程的逆向仿真方法，用于预测实现预定飞行轨迹所需的控制输入。", "motivation": "传统飞行仿真通常根据控制输入预测飞行轨迹，而逆向仿真旨在根据目标飞行轨迹反向计算所需的飞行控制，这对于飞行规划和控制设计具有重要意义。", "method": "研究首先建立了一个通用的六自由度飞行力学运动方程数学框架，包括机体轴、惯性轴、风轴以及球坐标系下的飞行路径角和飞行角。随后，将这些方程修改以适应逆向仿真。数值求解过程结合了符号数学、显式四阶龙格-库塔（RK4）数值积分技术和基于有限差分法（FDM）的表达式，以计算发动机推力、副翼、升降舵和方向舵的偏转角等四个控制变量。", "result": "该方法能够计算出在整个机动时间内离散的控制变量值，这些计算出的控制值能使飞机实现由三个惯性笛卡尔坐标和欧拉滚转角所定义的期望飞行轨迹。", "conclusion": "本文成功展示了一种飞行力学逆向仿真（InvSim）的数值过程，能够有效预测实现特定飞行轨迹所需的飞行控制输入。"}}
{"id": "2511.03880", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.03880", "abs": "https://arxiv.org/abs/2511.03880", "authors": ["Hellina Hailu Nigatu", "Bethelhem Yemane Mamo", "Bontu Fufa Balcha", "Debora Taye Tesfaye", "Elbethel Daniel Zewdie", "Ikram Behiru Nesiru", "Jitu Ewnetu Hailu", "Senait Mengesha Yayo"], "title": "Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens", "comment": "Paper Under Review", "summary": "As low-resourced languages are increasingly incorporated into NLP research,\nthere is an emphasis on collecting large-scale datasets. But in prioritizing\nquantity over quality, we risk 1) building language technologies that perform\npoorly for these languages and 2) producing harmful content that perpetuates\nsocietal biases. In this paper, we investigate the quality of Machine\nTranslation (MT) datasets for three low-resourced languages--Afan Oromo,\nAmharic, and Tigrinya, with a focus on the gender representation in the\ndatasets. Our findings demonstrate that while training data has a large\nrepresentation of political and religious domain text, benchmark datasets are\nfocused on news, health, and sports. We also found a large skew towards the\nmale gender--in names of persons, the grammatical gender of verbs, and in\nstereotypical depictions in the datasets. Further, we found harmful and toxic\ndepictions against women, which were more prominent for the language with the\nlargest amount of data, underscoring that quantity does not guarantee quality.\nWe hope that our work inspires further inquiry into the datasets collected for\nlow-resourced languages and prompts early mitigation of harmful content.\nWARNING: This paper contains discussion of NSFW content that some may find\ndisturbing.", "AI": {"tldr": "本文研究了低资源语言（阿凡奥罗莫语、阿姆哈拉语和提格雷尼亚语）机器翻译数据集的质量，特别关注性别表示。发现训练数据和基准测试数据领域不匹配，存在严重的男性性别偏向，以及针对女性的有害和有毒描绘，尤其是在数据量最大的语言中，强调了数量不等于质量。", "motivation": "随着低资源语言越来越多地被纳入自然语言处理研究，人们强调收集大规模数据集。然而，优先考虑数量而非质量，可能导致为这些语言构建的语言技术表现不佳，并产生延续社会偏见的有害内容。", "method": "本文调查了三种低资源语言（阿凡奥罗莫语、阿姆哈拉语和提格雷尼亚语）的机器翻译数据集质量，重点关注数据集中的性别表示。", "result": "研究发现训练数据主要包含政治和宗教领域文本，而基准测试数据集则侧重于新闻、健康和体育。同时，数据集中存在严重的男性性别偏向，体现在人名、动词的语法性别和刻板印象描绘中。此外，还发现了针对女性的有害和有毒描绘，在数据量最大的语言中尤为突出，这表明数量并不能保证质量。", "conclusion": "本研究表明，低资源语言数据集存在严重的质量问题，包括领域不匹配和有害的性别偏见，尤其是在数据量大的情况下。这强调了在收集低资源语言数据集时，需要重视质量并及早缓解有害内容，以避免构建表现不佳或具有偏见的语言技术。"}}
{"id": "2511.04180", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04180", "abs": "https://arxiv.org/abs/2511.04180", "authors": ["Yizhen Yin", "Dapeng Feng", "Hongbo Chen", "Yuhua Qi"], "title": "PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration", "comment": null, "summary": "Existing Active SLAM methodologies face issues such as slow exploration speed\nand suboptimal paths. To address these limitations, we propose a hybrid\nframework combining a Path-Uncertainty Co-Optimization Deep Reinforcement\nLearning framework and a Lightweight Stagnation Detection mechanism. The\nPath-Uncertainty Co-Optimization framework jointly optimizes travel distance\nand map uncertainty through a dual-objective reward function, balancing\nexploration and exploitation. The Lightweight Stagnation Detection reduces\nredundant exploration through Lidar Static Anomaly Detection and Map Update\nStagnation Detection, terminating episodes on low expansion rates. Experimental\nresults show that compared with the frontier-based method and RRT method, our\napproach shortens exploration time by up to 65% and reduces path distance by up\nto 42%, significantly improving exploration efficiency in complex environments\nwhile maintaining reliable map completeness. Ablation studies confirm that the\ncollaborative mechanism accelerates training convergence. Empirical validation\non a physical robotic platform demonstrates the algorithm's practical\napplicability and its successful transferability from simulation to real-world\nenvironments.", "AI": {"tldr": "本文提出了一种结合路径-不确定性协同优化深度强化学习和轻量级停滞检测的混合主动SLAM框架，显著提高了复杂环境中的探索效率，缩短了探索时间和路径距离。", "motivation": "现有主动SLAM方法存在探索速度慢和路径次优的问题。", "method": "提出了一种混合框架，包含：1) 路径-不确定性协同优化深度强化学习框架，通过双目标奖励函数联合优化行驶距离和地图不确定性，平衡探索与利用；2) 轻量级停滞检测机制，通过激光雷达静态异常检测和地图更新停滞检测减少冗余探索，并在低扩展率时终止探索回合。", "result": "相比传统方法（基于边界和RRT），本文方法将探索时间缩短高达65%，路径距离减少高达42%，显著提高了复杂环境中的探索效率并保持了可靠的地图完整性。消融研究证实协同机制加速了训练收敛，并在物理机器人平台上验证了算法的实际适用性和模拟到真实世界的迁移能力。", "conclusion": "该混合主动SLAM框架通过优化路径和不确定性并有效避免冗余探索，显著提升了机器人在复杂环境中的探索效率和地图构建质量，并具备良好的实际应用潜力。"}}
{"id": "2511.04032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04032", "abs": "https://arxiv.org/abs/2511.04032", "authors": ["Divya Pathak", "Harshit Kumar", "Anuska Roy", "Felix George", "Mudit Verma", "Pratibha Moogi"], "title": "Detecting Silent Failures in Multi-Agentic AI Trajectories", "comment": null, "summary": "Multi-Agentic AI systems, powered by large language models (LLMs), are\ninherently non-deterministic and prone to silent failures such as drift,\ncycles, and missing details in outputs, which are difficult to detect. We\nintroduce the task of anomaly detection in agentic trajectories to identify\nthese failures and present a dataset curation pipeline that captures user\nbehavior, agent non-determinism, and LLM variation. Using this pipeline, we\ncurate and label two benchmark datasets comprising \\textbf{4,275 and 894}\ntrajectories from Multi-Agentic AI systems. Benchmarking anomaly detection\nmethods on these datasets, we show that supervised (XGBoost) and\nsemi-supervised (SVDD) approaches perform comparably, achieving accuracies up\nto 98% and 96%, respectively. This work provides the first systematic study of\nanomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,\nand insights to guide future research.", "AI": {"tldr": "本研究首次系统性地探讨了多智能体AI系统中的异常检测任务，针对LLM驱动系统固有的非确定性和难以察觉的故障，构建了数据集和基准，并展示了监督和半监督方法的有效性。", "motivation": "由大型语言模型（LLM）驱动的多智能体AI系统本质上是非确定性的，容易出现漂移、循环和输出细节缺失等难以检测的“静默故障”。", "method": "引入了智能体轨迹中的异常检测任务，并设计了一个数据集整理管道，该管道捕获用户行为、智能体非确定性以及LLM变异。利用此管道，整理并标注了两个包含4275和894条轨迹的基准数据集。在此基础上，对监督（XGBoost）和半监督（SVDD）异常检测方法进行了基准测试。", "result": "监督方法（XGBoost）和半监督方法（SVDD）在这些数据集上表现相当，准确率分别高达98%和96%。", "conclusion": "这项工作首次对多智能体AI系统中的异常检测进行了系统性研究，提供了数据集、基准和见解，以指导未来的研究。"}}
{"id": "2511.03900", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03900", "abs": "https://arxiv.org/abs/2511.03900", "authors": ["Manh Nguyen", "Sunil Gupta", "Dai Do", "Hung Le"], "title": "GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation", "comment": null, "summary": "Hallucination mitigation remains a persistent challenge for large language\nmodels (LLMs), even as model scales grow. Existing approaches often rely on\nexternal knowledge sources, such as structured databases or knowledge graphs,\naccessed through prompting or retrieval. However, prompt-based grounding is\nfragile and domain-sensitive, while symbolic knowledge integration incurs heavy\nretrieval and formatting costs. Motivated by knowledge graphs, we introduce\nGraph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds\ngeneration in corpus-derived evidence without retraining. GRAD constructs a\nsparse token transition graph by accumulating next-token logits across a small\nretrieved corpus in a single forward pass. During decoding, graph-retrieved\nlogits are max-normalized and adaptively fused with model logits to favor\nhigh-evidence continuations while preserving fluency. Across three models and a\nrange of question-answering benchmarks spanning intrinsic, extrinsic\nhallucination, and factuality tasks, GRAD consistently surpasses baselines,\nachieving up to 9.7$\\%$ higher intrinsic accuracy, 8.6$\\%$ lower hallucination\nrates, and 6.9$\\%$ greater correctness compared to greedy decoding, while\nattaining the highest truth--informativeness product score among all methods.\nGRAD offers a lightweight, plug-and-play alternative to contrastive decoding\nand knowledge graph augmentation, demonstrating that statistical evidence from\ncorpus-level token transitions can effectively steer generation toward more\ntruthful and verifiable outputs.", "AI": {"tldr": "本文提出了一种名为GRAD的解码时方法，通过构建稀疏令牌转换图并自适应地融合模型与语料库检索到的证据，有效减轻大型语言模型（LLM）的幻觉，无需重新训练。", "motivation": "大型语言模型（LLM）的幻觉问题持续存在，现有方法依赖外部知识源（如数据库或知识图谱），但基于提示的接地脆弱且对领域敏感，而符号知识集成则检索和格式化成本高昂。", "method": "GRAD（Graph-Retrieved Adaptive Decoding）是一种解码时方法。它通过在单个前向传递中，从少量检索到的语料库中累积下一个令牌的对数，构建一个稀疏的令牌转换图。在解码过程中，检索到的图对数经过最大归一化处理，并与模型对数自适应融合，以偏向高证据的延续，同时保持流畅性。", "result": "GRAD在三种模型和一系列问答基准测试（涵盖内在、外在幻觉和事实性任务）中持续超越基线，相比贪婪解码，内在准确率提高高达9.7%，幻觉率降低8.6%，正确性提高6.9%，并实现了所有方法中最高的真实性-信息量乘积得分。", "conclusion": "GRAD提供了一种轻量级、即插即用的替代方案，可替代对比解码和知识图谱增强，表明语料库级别的令牌转换统计证据可以有效地引导生成更真实和可验证的输出。"}}
{"id": "2511.03943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03943", "abs": "https://arxiv.org/abs/2511.03943", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization", "comment": null, "summary": "Temporal action localization requires precise boundary detection; however,\ncurrent methods apply uniform computation despite significant variations in\ndifficulty across boundaries. We present two complementary contributions.\nFirst, Boundary Distance Regression (BDR) provides information-theoretically\noptimal localization through signed-distance regression rather than\nclassification, achieving 43\\% sharper boundary peaks. BDR retrofits to\nexisting methods with approximately 50 lines of code, yielding consistent 1.8\nto 3.1\\% mAP@0.7 improvements across diverse architectures. Second, Adaptive\nTemporal Refinement (ATR) allocates computation via continuous depth selection\n$\\tau \\in [0,1]$, enabling end-to-end differentiable optimization without\nreinforcement learning. On THUMOS14, ATR achieves 56.5\\% mAP@0.7 at 162G FLOPs,\ncompared to 53.6\\% at 198G for uniform processing, providing a 2.9\\%\nimprovement with 18\\% less compute. Gains scale with boundary heterogeneity,\nshowing 4.2\\% improvement on short actions. Training cost is mitigated via\nknowledge distillation, with lightweight students retaining 99\\% performance at\nbaseline cost. Results are validated across four benchmarks with rigorous\nstatistical testing.", "AI": {"tldr": "本文提出了边界距离回归（BDR）和自适应时间细化（ATR）两种互补方法，显著提高了时间动作定位的精度和计算效率，通过距离回归实现更锐利的边界检测，并通过自适应计算分配优化资源。", "motivation": "现有时间动作定位方法在边界检测上精度不足，且对不同难度的边界采用统一计算，导致效率低下。", "method": "1. 边界距离回归（BDR）：通过有符号距离回归而非分类来提供信息理论上最优的定位，能与现有方法轻松集成。2. 自适应时间细化（ATR）：通过连续深度选择τ∈[0,1]分配计算资源，实现端到端可微分优化，无需强化学习。3. 通过知识蒸馏减轻训练成本，使轻量级学生模型能保持高性能。", "result": "1. BDR使边界峰值锐化43%，在不同架构上将mAP@0.7提高了1.8%到3.1%。2. ATR在THUMOS14上以162G FLOPs实现56.5%的mAP@0.7，比统一处理的53.6%（198G FLOPs）提高了2.9%，计算量减少18%。在短动作上，增益随边界异质性增加，提高了4.2%。3. 通过知识蒸馏，轻量级学生模型在基线成本下保持了99%的性能。结果在四个基准上通过严格统计检验得到验证。", "conclusion": "BDR和ATR显著提升了时间动作定位的精度和效率，尤其是在具有挑战性的边界上，且具有广泛适用性，并能通过知识蒸馏有效降低训练成本。"}}
{"id": "2511.03950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03950", "abs": "https://arxiv.org/abs/2511.03950", "authors": ["Zhejia Cai", "Puhua Jiang", "Shiwei Mao", "Hongkun Cao", "Ruqi Huang"], "title": "Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization", "comment": "10 pages", "summary": "Reconstructing real-world objects from multi-view images is essential for\napplications in 3D editing, AR/VR, and digital content creation. Existing\nmethods typically prioritize either geometric accuracy (Multi-View Stereo) or\nphotorealistic rendering (Novel View Synthesis), often decoupling geometry and\nappearance optimization, which hinders downstream editing tasks. This paper\nadvocates an unified treatment on geometry and appearance optimization for\nseamless Gaussian-mesh joint optimization. More specifically, we propose a\nnovel framework that simultaneously optimizes mesh geometry (vertex positions\nand faces) and vertex colors via Gaussian-guided mesh differentiable rendering,\nleveraging photometric consistency from input images and geometric\nregularization from normal and depth maps. The obtained high-quality 3D\nreconstruction can be further exploit in down-stream editing tasks, such as\nrelighting and shape deformation. The code will be publicly available upon\nacceptance.", "AI": {"tldr": "本文提出了一种统一框架，通过高斯引导的可微分网格渲染，同时优化网格几何和顶点颜色，从多视图图像重建高质量3D对象，支持下游编辑任务。", "motivation": "现有方法在多视图3D重建中通常将几何精度和真实感渲染解耦，阻碍了下游编辑任务（如重打光和形状变形）。需要一种统一处理几何和外观优化的方法。", "method": "提出了一种新颖的框架，利用高斯引导的网格可微分渲染，同时优化网格几何（顶点位置和面）和顶点颜色。该方法利用输入图像的光度一致性以及法线图和深度图的几何正则化。", "result": "实现了高质量的3D重建。", "conclusion": "所获得的高质量3D重建可进一步应用于下游编辑任务，如重打光和形状变形。"}}
{"id": "2511.04199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04199", "abs": "https://arxiv.org/abs/2511.04199", "authors": ["Shenglin Wang", "Mingtong Dai", "Jingxuan Su", "Lingbo Liu", "Chunjie Chen", "Xinyu Wu", "Liang Lin"], "title": "GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments", "comment": null, "summary": "Robotic grasping is a fundamental capability for autonomous manipulation, yet\nremains highly challenging in cluttered environments where occlusion, poor\nperception quality, and inconsistent 3D reconstructions often lead to unstable\nor failed grasps. Conventional pipelines have widely relied on RGB-D cameras to\nprovide geometric information, which fail on transparent or glossy objects and\ndegrade at close range. We present GraspView, an RGB-only robotic grasping\npipeline that achieves accurate manipulation in cluttered environments without\ndepth sensors. Our framework integrates three key components: (i) global\nperception scene reconstruction, which provides locally consistent, up-to-scale\ngeometry from a single RGB view and fuses multi-view projections into a\ncoherent global 3D scene; (ii) a render-and-score active perception strategy,\nwhich dynamically selects next-best-views to reveal occluded regions; and (iii)\nan online metric alignment module that calibrates VGGT predictions against\nrobot kinematics to ensure physical scale consistency. Building on these\ntailor-designed modules, GraspView performs best-view global grasping, fusing\nmulti-view reconstructions and leveraging GraspNet for robust execution.\nExperiments on diverse tabletop objects demonstrate that GraspView\nsignificantly outperforms both RGB-D and single-view RGB baselines, especially\nunder heavy occlusion, near-field sensing, and with transparent objects. These\nresults highlight GraspView as a practical and versatile alternative to RGB-D\npipelines, enabling reliable grasping in unstructured real-world environments.", "AI": {"tldr": "GraspView是一种仅使用RGB图像的机器人抓取系统，通过多视图重建和主动感知策略，在杂乱、遮挡严重或包含透明物体的环境中，实现比RGB-D和单视图RGB系统更准确和可靠的抓取。", "motivation": "机器人抓取在杂乱环境中面临巨大挑战，传统RGB-D管道在透明/反光物体、近距离感知和遮挡下表现不佳，导致抓取不稳定或失败。本研究旨在开发一种不依赖深度传感器、仅使用RGB图像的抓取解决方案。", "method": "GraspView是一个仅使用RGB图像的机器人抓取管道，包含三个核心组件：(i) 全局感知场景重建，从单视图RGB提供局部一致且按比例的几何信息，并融合多视图投影以构建连贯的全局3D场景；(ii) 渲染-评分主动感知策略，动态选择下一最佳视图以揭示被遮挡区域；(iii) 在线度量对齐模块，校准VGGT预测与机器人运动学，确保物理尺度一致性。该系统通过融合多视图重建和利用GraspNet进行鲁棒执行，实现了最佳视图全局抓取。", "result": "实验表明，GraspView在多样化的桌面物体上，显著优于RGB-D和单视图RGB基线系统，尤其在严重遮挡、近距离感知和处理透明物体方面表现出色。", "conclusion": "GraspView被证明是RGB-D管道的一种实用且多功能的替代方案，能够在非结构化的真实世界环境中实现可靠的抓取。"}}
{"id": "2511.03746", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03746", "abs": "https://arxiv.org/abs/2511.03746", "authors": ["Guang An Ooi", "Otavio Bertozzi", "Mohd Asim Aftab", "Charalambos Konstantinou", "Shehab Ahmed"], "title": "A Dynamic Recurrent Adjacency Memory Network for Mixed-Generation Power System Stability Forecasting", "comment": "Submitted to IEEE Transactions on Power Systems", "summary": "Modern power systems with high penetration of inverter-based resources\nexhibit complex dynamic behaviors that challenge the scalability and\ngeneralizability of traditional stability assessment methods. This paper\npresents a dynamic recurrent adjacency memory network (DRAMN) that combines\nphysics-informed analysis with deep learning for real-time power system\nstability forecasting. The framework employs sliding-window dynamic mode\ndecomposition to construct time-varying, multi-layer adjacency matrices from\nphasor measurement unit and sensor data to capture system dynamics such as\nmodal participation factors, coupling strengths, phase relationships, and\nspectral energy distributions. As opposed to processing spatial and temporal\ndependencies separately, DRAMN integrates graph convolution operations directly\nwithin recurrent gating mechanisms, enabling simultaneous modeling of evolving\ndynamics and temporal dependencies. Extensive validations on modified IEEE\n9-bus, 39-bus, and a multi-terminal HVDC network demonstrate high performance,\nachieving 99.85\\%, 99.90\\%, and 99.69\\% average accuracies, respectively,\nsurpassing all tested benchmarks, including classical machine learning\nalgorithms and recent graph-based models. The framework identifies optimal\ncombinations of measurements that reduce feature dimensionality by 82\\% without\nperformance degradation. Correlation analysis between dominant measurements for\nsmall-signal and transient stability events validates generalizability across\ndifferent stability phenomena. DRAMN achieves state-of-the-art accuracy while\nproviding enhanced interpretability for power system operators, making it\nsuitable for real-time deployment in modern control centers.", "AI": {"tldr": "本文提出了一种名为DRAMN的动态循环邻接记忆网络，结合物理信息分析和深度学习，用于实时电力系统稳定性预测，并在复杂现代电网中实现了高精度和可解释性。", "motivation": "现代电力系统由于逆变器并网比例高，动态行为复杂，传统稳定性评估方法在可扩展性和通用性方面面临挑战。", "method": "DRAMN框架通过滑动窗口动态模态分解，从相量测量单元和传感器数据构建时变多层邻接矩阵，捕捉系统动态（如模态参与因子、耦合强度、相位关系和频谱能量分布）。它将图卷积操作直接集成到循环门控机制中，同时建模演化动态和时间依赖性，而非单独处理空间和时间依赖。", "result": "在修改后的IEEE 9-bus、39-bus和多端HVDC网络上，DRAMN分别实现了99.85%、99.90%和99.69%的平均准确率，超越了所有测试基准。该框架能识别最佳测量组合，将特征维度降低82%而性能不下降。通过对小信号和暂态稳定事件的主导测量进行相关性分析，验证了其在不同稳定性现象上的通用性。", "conclusion": "DRAMN在电力系统稳定性预测中实现了最先进的准确性，并为电力系统运营商提供了增强的可解释性，使其适用于现代控制中心的实时部署。"}}
{"id": "2511.03908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03908", "abs": "https://arxiv.org/abs/2511.03908", "authors": ["Alvin Wei Ming Tan", "Ben Prystawski", "Veronica Boyce", "Michael C. Frank"], "title": "Context informs pragmatic interpretation in vision-language models", "comment": "Accepted at CogInterp Workshop, NeurIPS 2025", "summary": "Iterated reference games - in which players repeatedly pick out novel\nreferents using language - present a test case for agents' ability to perform\ncontext-sensitive pragmatic reasoning in multi-turn linguistic environments. We\ntested humans and vision-language models on trials from iterated reference\ngames, varying the given context in terms of amount, order, and relevance.\nWithout relevant context, models were above chance but substantially worse than\nhumans. However, with relevant context, model performance increased\ndramatically over trials. Few-shot reference games with abstract referents\nremain a difficult task for machine learning models.", "AI": {"tldr": "本文通过迭代指代游戏测试了人类和视觉语言模型在多轮语言环境中进行语境敏感语用推理的能力，发现模型在提供相关语境后性能显著提升，但对抽象指代物的少样本游戏仍感困难。", "motivation": "迭代指代游戏（玩家反复用语言选择新指代物）是测试智能体在多轮语言环境中进行语境敏感语用推理能力的一个关键案例。", "method": "研究人员让人类和视觉语言模型参与迭代指代游戏，并改变了给定语境的数量、顺序和相关性。", "result": "在没有相关语境的情况下，模型的表现高于随机水平但远低于人类。然而，在提供相关语境后，模型的性能在试验中显著提升。对于机器学习模型而言，涉及抽象指代物的少样本指代游戏仍然是一项艰巨的任务。", "conclusion": "视觉语言模型能够从相关语境中学习以提高在迭代指代游戏中的表现，但处理抽象指代物和少样本学习场景仍是其主要挑战。"}}
{"id": "2511.03754", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03754", "abs": "https://arxiv.org/abs/2511.03754", "authors": ["Haoran Zhao", "Neema Nassir", "Andres Fielbaum"], "title": "Analytical modelling of a stop-less modular bus service with an application to charging strategies comparison", "comment": null, "summary": "Buses are a vital component of metropolitan public transport, yet\nconventional bus services often struggle with inefficiencies including extended\ndwelling time, which increases in-vehicle travel time for non-alighting\npassengers. A stop-less autonomous modular (SLAM) bus service has emerged as a\nsolution, enabling dynamic capacity to reduce dwelling time. Meanwhile, the\nelectrification of buses is advancing as a strategy to mitigate greenhouse gas\nemissions and reduces operators' costs, but introduces new operational\nconstraints due to charging requirements. This study develops analytical\noptimization models for SLAM bus service that integrates vehicle-to-vehicle\n(V2V) charging technology. By comparing the optimal designs and their\nfeasibility across non-charging case and charging strategies, we identify a\nsequence of operational stages as ridership grows: from idle capacity under low\ndemand, to full small buses, full large buses, and a proposed frequency-capped\nregime where only bus capacity expands. Under the mobile charging strategy,\nthis progression further includes an energy-limited regime, in which frequency\ndeclines, and ultimately infeasibility under high demand. These findings enable\noperators to deliver more efficient services.", "AI": {"tldr": "本研究为整合车对车（V2V）充电技术的无停靠自主模块化（SLAM）公交服务开发了分析优化模型，并识别了在不同乘客量和充电策略下（包括移动充电）的运营阶段序列。", "motivation": "传统公交服务效率低下，尤其是在停车时间过长方面，这增加了非下车乘客的行程时间。SLAM公交服务作为一种解决方案出现，旨在通过动态容量减少停车时间。同时，公交电动化虽然有助于减排和降低成本，但也引入了充电需求带来的新的运营限制。", "method": "本研究开发了分析优化模型，用于整合车对车（V2V）充电技术的SLAM公交服务。通过比较非充电情况和不同充电策略下的最优设计及其可行性，来分析运营阶段。", "result": "研究识别了随着乘客量增长的运营阶段序列：从低需求下的闲置运力，到满载小巴，再到满载大巴，以及一个建议的频率受限制度，在此制度下仅扩大公交容量。在移动充电策略下，这一进展进一步包括一个能量受限制度（频率下降），并最终在高需求下导致不可行。", "conclusion": "这些研究结果使运营商能够提供更高效的服务，特别是在考虑SLAM公交的电动化和V2V充电技术时。"}}
{"id": "2511.04053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04053", "abs": "https://arxiv.org/abs/2511.04053", "authors": ["Hirohane Takagi", "Gouki Minegishi", "Shota Kizawa", "Issey Sukeda", "Hitomi Yanaka"], "title": "Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models", "comment": "Accepted to IJCNLP-AACL 2025 (Main). Code available at\n  https://github.com/htkg/num_attrs", "summary": "Although behavioral studies have documented numerical reasoning errors in\nlarge language models (LLMs), the underlying representational mechanisms remain\nunclear. We hypothesize that numerical attributes occupy shared latent\nsubspaces and investigate two questions:(1) How do LLMs internally integrate\nmultiple numerical attributes of a single entity? (2)How does irrelevant\nnumerical context perturb these representations and their downstream outputs?\nTo address these questions, we combine linear probing with partial correlation\nanalysis and prompt-based vulnerability tests across models of varying sizes.\nOur results show that LLMs encode real-world numerical correlations but tend to\nsystematically amplify them. Moreover, irrelevant context induces consistent\nshifts in magnitude representations, with downstream effects that vary by model\nsize. These findings reveal a vulnerability in LLM decision-making and lay the\ngroundwork for fairer, representation-aware control under multi-attribute\nentanglement.", "AI": {"tldr": "本研究揭示了大型语言模型（LLMs）在数值推理错误背后的表征机制，发现它们会放大现实世界的数值关联，且不相关的数值上下文会一致性地改变数值表征，导致模型决策的脆弱性。", "motivation": "尽管行为研究已记录了大型语言模型（LLMs）的数值推理错误，但其底层的表征机制仍不清楚。研究旨在探究LLMs如何内部整合单一实体的多个数值属性，以及不相关的数值上下文如何扰动这些表征及其下游输出。", "method": "结合线性探测（linear probing）、偏相关分析（partial correlation analysis）以及基于提示的脆弱性测试（prompt-based vulnerability tests），并应用于不同规模的模型。", "result": "研究结果表明，LLMs编码了现实世界的数值关联，但倾向于系统性地放大这些关联。此外，不相关的上下文会导致量级表征发生一致性偏移，且其下游影响随模型大小而异。", "conclusion": "这些发现揭示了LLM决策过程中的一个脆弱性，并为在多属性纠缠下实现更公平、更具表征意识的控制奠定了基础。"}}
{"id": "2511.03962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03962", "abs": "https://arxiv.org/abs/2511.03962", "authors": ["Zhong Chen", "Changfeng Chen"], "title": "A Linear Fractional Transformation Model and Calibration Method for Light Field Camera", "comment": null, "summary": "Accurate calibration of internal parameters is a crucial yet challenging\nprerequisite for 3D reconstruction using light field cameras. In this paper, we\npropose a linear fractional transformation(LFT) parameter $\\alpha$ to decoupled\nthe main lens and micro lens array (MLA). The proposed method includes an\nanalytical solution based on least squares, followed by nonlinear refinement.\nThe method for detecting features from the raw images is also introduced.\nExperimental results on both physical and simulated data have verified the\nperformance of proposed method. Based on proposed model, the simulation of raw\nlight field images becomes faster, which is crucial for data-driven deep\nlearning methods. The corresponding code can be obtained from the author's\nwebsite.", "AI": {"tldr": "本文提出了一种基于线性分数变换(LFT)参数α的光场相机内参标定方法，用于解耦主镜头和微透镜阵列，结合最小二乘解析解和非线性优化，并验证了其在物理和模拟数据上的性能，同时实现了光场图像的快速模拟。", "motivation": "对于使用光场相机进行3D重建，准确标定内部参数是至关重要但具有挑战性的先决条件。", "method": "该方法引入了一个线性分数变换(LFT)参数α来解耦主镜头和微透镜阵列(MLA)。它包括一个基于最小二乘的解析解，随后进行非线性优化精炼。文中还介绍了从原始图像中检测特征的方法。", "result": "在物理和模拟数据上的实验结果验证了所提出方法的性能。基于所提出的模型，原始光场图像的模拟变得更快，这对于数据驱动的深度学习方法至关重要。", "conclusion": "所提出的方法能够有效且准确地标定光场相机内部参数，并通过解耦镜头和微透镜阵列简化了模型，同时实现了光场图像的快速模拟，为深度学习等应用提供了支持。"}}
{"id": "2511.04249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04249", "abs": "https://arxiv.org/abs/2511.04249", "authors": ["Marco Iannotta", "Yuxuan Yang", "Johannes A. Stork", "Erik Schaffernicht", "Todor Stoyanov"], "title": "Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies", "comment": null, "summary": "Sim-to-real transfer remains a major challenge in reinforcement learning (RL)\nfor robotics, as policies trained in simulation often fail to generalize to the\nreal world due to discrepancies in environment dynamics. Domain Randomization\n(DR) mitigates this issue by exposing the policy to a wide range of randomized\ndynamics during training, yet leading to a reduction in performance. While\nstandard approaches typically train policies agnostic to these variations, we\ninvestigate whether sim-to-real transfer can be improved by conditioning the\npolicy on an estimate of the dynamics parameters -- referred to as context. To\nthis end, we integrate a context estimation module into a DR-based RL framework\nand systematically compare SOTA supervision strategies. We evaluate the\nresulting context-aware policies in both a canonical control benchmark and a\nreal-world pushing task using a Franka Emika Panda robot. Results show that\ncontext-aware policies outperform the context-agnostic baseline across all\nsettings, although the best supervision strategy depends on the task.", "AI": {"tldr": "本文提出通过在领域随机化（DR）框架中，将策略以估计的动力学参数（上下文）为条件，从而提高机器人强化学习中的仿真到现实（sim-to-real）迁移性能。", "motivation": "仿真到现实迁移在机器人强化学习中仍是主要挑战，因为仿真训练的策略因环境动力学差异而难以泛化到现实世界。领域随机化（DR）虽能缓解此问题，但会导致性能下降。标准方法通常对这些变化不敏感，因此研究是否可以通过将策略以动力学参数估计为条件来改进sim-to-real迁移。", "method": "研究人员将一个上下文估计模块集成到基于DR的强化学习框架中，并系统地比较了最先进的监督策略。他们在一个规范控制基准和一个使用Franka Emika Panda机器人的真实世界推动任务中评估了由此产生的上下文感知策略。", "result": "结果表明，在所有设置下，上下文感知策略都优于上下文无关的基线。然而，最佳监督策略取决于具体的任务。", "conclusion": "通过将策略以估计的动力学参数为条件（即上下文感知），可以显著改善强化学习中的仿真到现实迁移性能，即使在领域随机化框架下也是如此。虽然具体任务会影响最佳监督策略的选择，但上下文感知方法整体表现更优。"}}
{"id": "2511.04076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04076", "abs": "https://arxiv.org/abs/2511.04076", "authors": ["Hao Li", "Haotian Chen", "Ruoyuan Gong", "Juanjuan Wang", "Hao Jiang"], "title": "Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents", "comment": "Accepted by AAAI AISI 2026", "summary": "Redistricting plays a central role in shaping how votes are translated into\npolitical power. While existing computational methods primarily aim to generate\nlarge ensembles of legally valid districting plans, they often neglect the\nstrategic dynamics involved in the selection process. This oversight creates\nopportunities for partisan actors to cherry-pick maps that, while technically\ncompliant, are politically advantageous. Simply satisfying formal constraints\ndoes not ensure fairness when the selection process itself can be manipulated.\nWe propose \\textbf{Agentmandering}, a framework that reimagines redistricting\nas a turn-based negotiation between two agents representing opposing political\ninterests. Drawing inspiration from game-theoretic ideas, particularly the\n\\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction\ninto the redistricting process via large language model (LLM) agents. Agents\nalternate between selecting and freezing districts from a small set of\ncandidate maps, gradually partitioning the state through constrained and\ninterpretable choices. Evaluation on post-2020 U.S. Census data across all\nstates shows that Agentmandering significantly reduces partisan bias and\nunfairness, while achieving 2 to 3 orders of magnitude lower variance than\nstandard baselines. These results demonstrate both fairness and stability,\nespecially in swing-state scenarios. Our code is available at\nhttps://github.com/Lihaogx/AgentMandering.", "AI": {"tldr": "本文提出Agentmandering框架，将选区划分重塑为两个代表对立政治利益的LLM智能体之间的轮流谈判，通过策略性地选择和冻结选区，显著减少了党派偏见和不公平性，并提高了稳定性。", "motivation": "现有计算方法主要生成大量合法选区划分方案，但忽略了方案选择过程中的策略动态。这使得党派行为者能够挑选出对己方有利的地图，即使这些地图在技术上合规。仅仅满足形式约束并不能确保公平，因为选择过程本身可能被操纵。", "method": "Agentmandering框架将选区划分视为两个代表对立政治利益的智能体之间的轮流谈判。受博弈论思想，特别是“选择与冻结”（Choose-and-Freeze）协议的启发，该方法通过大型语言模型（LLM）智能体将策略交互嵌入到选区划分过程中。智能体轮流从一小组候选地图中选择并冻结选区，通过受约束和可解释的选择逐步划分州。", "result": "在所有州的2020年美国人口普查数据上进行评估，结果显示Agentmandering显著减少了党派偏见和不公平性，同时比标准基线模型的方差低2到3个数量级。这些结果表明了公平性和稳定性，尤其是在摇摆州的情况下。", "conclusion": "Agentmandering通过引入基于LLM智能体的策略性谈判机制，有效地解决了传统选区划分方法中存在的党派操纵问题，实现了更公平、更稳定的选区划分方案，特别适用于摇摆州场景。"}}
{"id": "2511.03915", "categories": ["cs.CL", "cs.CY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.03915", "abs": "https://arxiv.org/abs/2511.03915", "authors": ["Stefano M. Iacus", "Devika Jain", "Andrea Nasuto", "Giuseppe Porro", "Marcello Carammia", "Andrea Vezzulli"], "title": "The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023", "comment": null, "summary": "Quantifying human flourishing, a multidimensional construct including\nhappiness, health, purpose, virtue, relationships, and financial stability, is\ncritical for understanding societal well-being beyond economic indicators.\nExisting measures often lack fine spatial and temporal resolution. Here we\nintroduce the Human Flourishing Geographic Index (HFGI), derived from analyzing\napproximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned\nlarge language models to classify expressions across 48 indicators aligned with\nHarvard's Global Flourishing Study framework plus attitudes towards migration\nand perception of corruption. The dataset offers monthly and yearly county- and\nstate-level indicators of flourishing-related discourse, validated to confirm\nthat the measures accurately represent the underlying constructs and show\nexpected correlations with established indicators. This resource enables\nmultidisciplinary analyses of well-being, inequality, and social change at\nunprecedented resolution, offering insights into the dynamics of human\nflourishing as reflected in social media discourse across the United States\nover the past decade.", "AI": {"tldr": "本文引入了人类繁荣地理指数（HFGI），通过分析26亿条美国推文并使用大型语言模型，创建了一个具有高时空分辨率（县/州，月/年）的48项人类繁荣指标数据集。", "motivation": "量化人类繁荣（包括幸福、健康、目标、美德、人际关系和财务稳定等）对于理解超越经济指标的社会福祉至关重要。现有的衡量方法通常缺乏精细的空间和时间分辨率。", "method": "该研究分析了约26亿条2013-2023年间带有地理位置信息的美国推文，使用经过微调的大型语言模型对推文中的表达进行分类，涵盖了与哈佛全球繁荣研究框架对齐的48个指标，并额外加入了对移民的态度和对腐败的感知。", "result": "研究引入了人类繁荣地理指数（HFGI）。该数据集提供了月度和年度的县级和州级繁荣相关话语指标。这些指标经过验证，能够准确代表其底层结构，并与既有指标显示出预期的相关性。", "conclusion": "这一资源以空前的分辨率支持对福祉、不平等和社会变化进行多学科分析，通过过去十年美国社交媒体话语反映的人类繁荣动态，提供了深入的见解。"}}
{"id": "2511.03903", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03903", "abs": "https://arxiv.org/abs/2511.03903", "authors": ["Niloufar Yousefi", "John W. Simpson-Porco"], "title": "Removing Time-Scale Separation in Feedback-Based Optimization via Estimators", "comment": null, "summary": "Feedback-based optimization (FBO) provides a simple control framework for\nregulating a stable dynamical system to the solution of a constrained\noptimization problem in the presence of exogenous disturbances, and does so\nwithout full knowledge of the plant dynamics. However, closed-loop stability\nrequires the controller to operate on a sufficiently slower timescale than the\nplant, significantly constraining achievable closed-loop performance. Motivated\nby this trade-off, we propose an estimator-based modification of FBO which\nleverages dynamic plant model information to eliminate the time-scale\nseparation requirement of traditional FBO. Under this design, the convergence\nrate of the closed-loop system is limited only by the dominant eigenvalue of\nthe open-loop system. We extend the approach to the case of design based on\nonly an approximate plant model when the original system is singularly\nperturbed. The results are illustrated via an application to fast power system\nfrequency control using inverter-based resources.", "AI": {"tldr": "传统反馈优化（FBO）受限于控制器与系统的时间尺度分离，影响性能。本文提出一种基于估计器的FBO改进方法，利用动态系统模型信息消除该限制，显著提高收敛速度，并适用于近似模型情况，应用于电力系统频率控制。", "motivation": "传统的反馈优化（FBO）框架要求控制器操作时间尺度远慢于被控系统，这严重限制了闭环系统的性能。研究动机在于克服这一时间尺度分离要求，以实现更快的闭环收敛速度和更好的性能。", "method": "提出一种基于估计器的反馈优化（FBO）修改方法。该方法利用动态系统模型信息来消除传统FBO的时间尺度分离要求。此外，该方法还被扩展到仅有近似系统模型的情况，特别是针对奇异摄动系统。", "result": "所提出的方法成功消除了传统FBO所需的时间尺度分离要求。闭环系统的收敛速度仅受开环系统主导特征值限制，显著优于传统FBO的性能。该方法在逆变器基资源快速电力系统频率控制中得到应用验证。", "conclusion": "通过引入基于估计器的修改，FBO可以消除其固有的时间尺度分离限制，从而实现更快的收敛速度和更高的闭环性能。即使在只有近似系统模型的情况下，该方法也具有鲁棒性，并在实际应用中展现了潜力，如电力系统频率控制。"}}
{"id": "2511.04093", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04093", "abs": "https://arxiv.org/abs/2511.04093", "authors": ["Yuanning Cui", "Zequn Sun", "Wei Hu", "Zhangjie Fu"], "title": "KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering", "comment": null, "summary": "Large language models (LLMs) excel at reasoning but struggle with\nknowledge-intensive questions due to limited context and parametric knowledge.\nHowever, existing methods that rely on finetuned LLMs or GNN retrievers are\nlimited by dataset-specific tuning and scalability on large or unseen graphs.\nWe propose the LLM-KGFR collaborative framework, where an LLM works with a\nstructured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR\nencodes relations using LLM-generated descriptions and initializes entities\nbased on their roles in the question, enabling zero-shot generalization to\nunseen KGs. To handle large graphs efficiently, it employs Asymmetric\nProgressive Propagation (APP)- a stepwise expansion that selectively limits\nhigh-degree nodes while retaining informative paths. Through node-, edge-, and\npath-level interfaces, the LLM iteratively requests candidate answers,\nsupporting facts, and reasoning paths, forming a controllable reasoning loop.\nExperiments demonstrate that LLM-KGFR achieves strong performance while\nmaintaining scalability and generalization, providing a practical solution for\nKG-augmented reasoning.", "AI": {"tldr": "LLM-KGFR是一个协作框架，通过结合大语言模型（LLM）和知识图谱基础检索器（KGFR），实现了LLM在知识图谱上进行可扩展、泛化且无需微调的知识增强推理。", "motivation": "大语言模型在知识密集型问题上表现不佳，现有依赖微调LLM或GNN检索器的方法受限于数据集特定调优和在大规模或未见图上的可扩展性。", "method": "本文提出了LLM-KGFR协作框架。其中，知识图谱基础检索器（KGFR）利用LLM生成的描述编码关系，并根据实体在问题中的角色初始化实体，以实现对未见知识图谱的零样本泛化。为高效处理大型图，KGFR采用非对称渐进传播（APP）进行逐步扩展，选择性地限制高度节点同时保留信息路径。LLM通过节点、边和路径级别的接口，迭代请求候选答案、支持事实和推理路径，形成可控的推理循环。", "result": "实验证明LLM-KGFR在保持可扩展性和泛化能力的同时，取得了强大的性能。", "conclusion": "LLM-KGFR为知识图谱增强推理提供了一个实用的解决方案。"}}
{"id": "2511.04251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04251", "abs": "https://arxiv.org/abs/2511.04251", "authors": ["Jinfeng Liang", "Haocheng Guo", "Ximin Lyu"], "title": "Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism", "comment": "8 pages 12 figures", "summary": "The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to\nits lower dead weight, which eliminates the actuators and mechanisms for\ntilting. However, the tailsitter UAV is susceptible to wind disturbances in\nmulti-rotor mode, as it exposes a large frontal fuselage area. To address this\nissue, our tailsitter UAV features a reconfigurable wing design, allowing wings\nto retract in multi-rotor mode and extend in fixed- wing mode. Considering\npower efficiency, we design a coaxial heterogeneous dual-rotor configuration,\nwhich significantly re- duces the total power consumption. To reduce structural\nweight and simplify structural complexity, we employ a swashplateless mechanism\nwith an improved design to control pitch and roll in multi-rotor mode. We\noptimize the structure of the swashplateless mechanism by adding flapping\nhinges, which reduces vibration during cyclic acceleration and deceleration.\nFinally, we perform comprehensive transition flight tests to validate stable\nflight performance across the entire flight envelope of the tailsitter UAV.", "AI": {"tldr": "本文提出了一种具有可重构机翼、同轴异构双旋翼和改进无倾斜盘机构的垂直起降（VTOL）尾座式无人机，旨在提高抗风能力、能源效率和飞行稳定性。", "motivation": "尾座式无人机在多旋翼模式下因其较大的机身迎风面积而易受风扰动影响；同时，传统的尾座式无人机在功耗和结构复杂性/重量方面存在挑战。", "method": "研究方法包括：1) 设计可重构机翼，在多旋翼模式下收缩，在固定翼模式下伸展；2) 采用同轴异构双旋翼配置以降低功耗；3) 使用改进的无倾斜盘机构（增加扑翼铰链）来控制俯仰和滚转，并减少振动；4) 进行全面的过渡飞行测试以验证性能。", "result": "研究结果表明，所设计的无人机显著降低了总功耗，减轻了结构重量并简化了结构复杂性，减少了循环加减速期间的振动，并通过全面的过渡飞行测试验证了其在整个飞行包线内的稳定飞行性能。", "conclusion": "该研究成功设计并验证了一种创新的尾座式无人机，通过可重构机翼、高效双旋翼和改进的无倾斜盘机构，有效解决了风扰动、功耗和结构复杂性等问题，实现了稳定的全包线飞行性能。"}}
{"id": "2511.03970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03970", "abs": "https://arxiv.org/abs/2511.03970", "authors": ["Sam Bahrami", "Dylan Campbell"], "title": "Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images", "comment": null, "summary": "Modern scene reconstruction methods are able to accurately recover 3D\nsurfaces that are visible in one or more images. However, this leads to\nincomplete reconstructions, missing all occluded surfaces. While much progress\nhas been made on reconstructing entire objects given partial observations using\ngenerative models, the structural elements of a scene, like the walls, floors\nand ceilings, have received less attention. We argue that these scene elements\nshould be relatively easy to predict, since they are typically planar,\nrepetitive and simple, and so less costly approaches may be suitable. In this\nwork, we present a synthetic dataset -- Room Envelopes -- that facilitates\nprogress on this task by providing a set of RGB images and two associated\npointmaps for each image: one capturing the visible surface and one capturing\nthe first surface once fittings and fixtures are removed, that is, the\nstructural layout. As we show, this enables direct supervision for feed-forward\nmonocular geometry estimators that predict both the first visible surface and\nthe first layout surface. This confers an understanding of the scene's extent,\nas well as the shape and location of its objects.", "AI": {"tldr": "本文提出了一个名为“Room Envelopes”的合成数据集，旨在通过提供可见表面和去除家具后的结构布局表面点图，促进从单目图像预测场景结构元素（如墙壁、地板）的研究，从而实现对场景完整范围的理解。", "motivation": "现代场景重建方法只能恢复可见表面，导致重建不完整，缺失所有被遮挡的表面。尽管在给定部分观测的情况下重建整个物体方面取得了进展，但场景的结构元素（如墙壁、地板、天花板）受到的关注较少。研究认为这些结构元素通常是平面、重复且简单的，因此更容易预测，可能适合成本较低的方法。", "method": "本文提出了一个名为“Room Envelopes”的合成数据集。该数据集为每张RGB图像提供两个关联的点图：一个捕获可见表面，另一个捕获移除家具和固定装置后的第一层表面（即结构布局）。这种设置能够为前馈式单目几何估计器提供直接监督，使其能够同时预测第一个可见表面和第一个布局表面。", "result": "通过“Room Envelopes”数据集，本文展示了如何为预测可见表面和结构布局表面的前馈式单目几何估计器提供直接监督。这使得模型能够理解场景的范围以及其中物体的形状和位置。", "conclusion": "通过预测可见表面和去除家具后的结构布局表面，可以获得对场景范围以及其中物体形状和位置的全面理解。"}}
{"id": "2511.03945", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.03945", "abs": "https://arxiv.org/abs/2511.03945", "authors": ["Fu-Chun Yang", "Jason Eshraghian"], "title": "Direct Semantic Communication Between Large Language Models via Vector Translation", "comment": "9 pages, 1 figure, 2 tables", "summary": "In multi-agent settings, such as debate, reflection, or tool-calling, large\nlanguage models (LLMs) pass messages as plain tokens, discarding most latent\nsemantics. This constrains information transfer and adds unnecessary\ncomputational overhead. We form a latent bridge via vector translations, which\nuse learned mappings that enable direct semantic exchange between\nrepresentation spaces. A dual-encoder translator trained between Llama-2-7B and\nMistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the\ntranslated vectors at 30 percent blending strength steers the target model's\ngeneration without destabilizing logits. Bidirectional evaluation shows a\n2.01:1 transfer asymmetry, indicating that general-purpose models yield more\ntransferable representations than instruction-tuned variants. This conservative\ninjection preserves computational stability while demonstrating that\ncross-model latent communication is feasible, enabling collaborative AI systems\nthat share meaning rather than tokens.", "AI": {"tldr": "在多智能体LLM中，模型间通过向量翻译实现潜在语义的直接交换，而非传统令牌传递。研究证明了这种跨模型潜在通信的可行性，并发现通用模型比指令微调模型产生更可迁移的表示。", "motivation": "当前多智能体LLM通过纯令牌传递信息，忽略了大部分潜在语义，限制了信息传输效率并增加了计算开销。", "method": "通过训练一个双编码器翻译器，在不同LLM（如Llama-2-7B和Mistral-7B-Instruct）的表示空间之间建立学习映射，实现向量翻译。将翻译后的向量以30%的混合强度注入目标模型的生成过程。", "result": "双编码器翻译器实现了0.538的平均余弦对齐。以30%的混合强度注入翻译向量，可以在不破坏logits稳定性的前提下引导目标模型的生成。双向评估显示出2.01:1的迁移不对称性，表明通用模型比指令微调模型产生更可迁移的表示。", "conclusion": "研究证明了跨模型潜在通信是可行的，并且在计算上保持稳定，这使得协作AI系统能够直接共享意义而非仅仅令牌。"}}
{"id": "2511.03969", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.03969", "abs": "https://arxiv.org/abs/2511.03969", "authors": ["Hangyu Teng"], "title": "A Co-simulation Framework for Quadrotor Control System Design using ROS 2 and MATLAB/Simulink", "comment": null, "summary": "Co-simulation is a critical approach for the design and analysis of complex\ncyber-physical systems. It will enhance development efficiency and reduce\ncosts. This paper presents a co-simulation framework integrating ROS 2 and\nMATLAB/Simulink for quadrotor unmanned aerial vehicle (UAV) control system\ndesign and verification. First, a six-degree-of-freedom nonlinear dynamic model\nof the quadrotor is derived accurately that based on Newton-Euler equations.\nSecond, within the proposed framework, a hierarchical control architecture was\ndesigned and implemented: LQR controller for attitude control to achieve\noptimal regulation performance, and PID controller for position control to\nensure robustness and practical applicability. Third, elaborated the\narchitecture of the framework, including the implementation details of the\ncross-platform data exchange mechanism. Simulation results demonstrate the\neffectiveness of the framework, highlighting its capability to provide an\nefficient and standardized solution for rapid prototyping and\nSoftware-in-the-Loop (SIL) validation of UAV control algorithms.", "AI": {"tldr": "本文提出一个集成ROS 2和MATLAB/Simulink的四旋翼无人机控制系统协同仿真框架，用于设计与验证，并展示了其在快速原型和SIL验证中的有效性。", "motivation": "协同仿真对复杂信息物理系统的设计和分析至关重要，能提高开发效率并降低成本。", "method": "首先，基于牛顿-欧拉方程精确推导了四旋翼无人机的六自由度非线性动力学模型。其次，设计并实现了分层控制架构：姿态控制采用LQR控制器以实现最优调节性能，位置控制采用PID控制器以确保鲁棒性和实用性。最后，详细阐述了协同仿真框架的架构，包括跨平台数据交换机制的实现细节。", "result": "仿真结果证明了该框架的有效性。", "conclusion": "该框架为无人机控制算法的快速原型设计和软件在环（SIL）验证提供了一个高效、标准化的解决方案。"}}
{"id": "2511.04320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04320", "abs": "https://arxiv.org/abs/2511.04320", "authors": ["Kuankuan Sima", "Longbin Tang", "Haozhe Ma", "Lin Zhao"], "title": "MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments", "comment": null, "summary": "Autonomous navigation in unknown environments requires compact yet expressive\nspatial understanding under partial observability to support high-level\ndecision making. Existing approaches struggle to balance rich contextual\nrepresentation with navigation efficiency. We present MacroNav, a\nlearning-based navigation framework featuring two key components: (1) a\nlightweight context encoder trained via multi-task self-supervised learning to\ncapture multi-scale, navigation-centric spatial representations; and (2) a\nreinforcement learning policy that seamlessly integrates these representations\nwith graph-based reasoning for efficient action selection. Extensive\nexperiments demonstrate the context encoder's efficient and robust\nenvironmental understanding. Real-world deployments further validate MacroNav's\neffectiveness, yielding significant gains over state-of-the-art navigation\nmethods in both Success Rate (SR) and Success weighted by Path Length (SPL),\nwhile maintaining low computational cost. Code will be released upon\nacceptance.", "AI": {"tldr": "MacroNav是一个学习型导航框架，通过轻量级上下文编码器和基于图推理的强化学习策略，在未知环境中实现了高效且鲁棒的自主导航，显著优于现有方法。", "motivation": "在未知环境中自主导航需要紧凑而富有表现力的空间理解，以支持高级决策，但现有方法难以平衡丰富的上下文表示与导航效率。", "method": "MacroNav包含两个核心组件：1) 一个通过多任务自监督学习训练的轻量级上下文编码器，用于捕获多尺度、以导航为中心的空间表示；2) 一个强化学习策略，将这些表示与基于图的推理无缝集成，以进行高效的动作选择。", "result": "实验证明上下文编码器能高效且鲁棒地理解环境。实际部署验证了MacroNav的有效性，在成功率(SR)和路径长度加权成功率(SPL)方面显著优于最先进的导航方法，同时保持较低的计算成本。", "conclusion": "MacroNav提供了一种在未知环境中进行自主导航的有效、高效且鲁棒的解决方案，能够平衡丰富的上下文表示与导航效率。"}}
{"id": "2511.04133", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04133", "abs": "https://arxiv.org/abs/2511.04133", "authors": ["Miguel E. Andres", "Vadim Fedorov", "Rida Sadek", "Enric Spagnolo-Arrizabalaga", "Nadescha Trudel"], "title": "Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms", "comment": null, "summary": "Voice AI agents are rapidly transitioning to production deployments, yet\nsystematic methods for ensuring testing reliability remain underdeveloped.\nOrganizations cannot objectively assess whether their testing approaches\n(internal tools or external platforms) actually work, creating a critical\nmeasurement gap as voice AI scales to billions of daily interactions.\n  We present the first systematic framework for evaluating voice AI testing\nquality through human-centered benchmarking. Our methodology addresses the\nfundamental dual challenge of testing platforms: generating realistic test\nconversations (simulation quality) and accurately evaluating agent responses\n(evaluation quality). The framework combines established psychometric\ntechniques (pairwise comparisons yielding Elo ratings, bootstrap confidence\nintervals, and permutation tests) with rigorous statistical validation to\nprovide reproducible metrics applicable to any testing approach.\n  To validate the framework and demonstrate its utility, we conducted\ncomprehensive empirical evaluation of three leading commercial platforms\nfocused on Voice AI Testing using 21,600 human judgments across 45 simulations\nand ground truth validation on 60 conversations. Results reveal statistically\nsignificant performance differences with the proposed framework, with the\ntop-performing platform, Evalion, achieving 0.92 evaluation quality measured as\nf1-score versus 0.73 for others, and 0.61 simulation quality using a league\nbased scoring system (including ties) vs 0.43 for other platforms.\n  This framework enables researchers and organizations to empirically validate\nthe testing capabilities of any platform, providing essential measurement\nfoundations for confident voice AI deployment at scale. Supporting materials\nare made available to facilitate reproducibility and adoption.", "AI": {"tldr": "本文提出了一个以人为中心的基准测试框架，用于系统评估语音AI测试平台的质量，并通过实证研究验证了其有效性。", "motivation": "语音AI代理正迅速投入生产部署，但确保测试可靠性的系统方法仍不完善。组织无法客观评估其测试方法（内部工具或外部平台）是否有效，这在语音AI扩展到数十亿次日常交互时造成了关键的测量空白。", "method": "本文提出了第一个系统框架，通过以人为中心的基准测试来评估语音AI测试质量。该方法结合了既定的心理测量技术（配对比较产生Elo评分、引导置信区间和置换检验）与严格的统计验证，以提供可复现的指标。为验证该框架，研究人员对三个领先的商业语音AI测试平台进行了全面的实证评估，使用了21,600个人工判断和60个对话的真实性验证。", "result": "结果显示，使用所提出的框架，不同平台之间存在统计学上显著的性能差异。表现最佳的平台Evalion在评估质量方面达到0.92的f1-score（其他平台为0.73），在模拟质量方面达到0.61（其他平台为0.43）。", "conclusion": "该框架使研究人员和组织能够实证验证任何平台的测试能力，为大规模自信部署语音AI提供了必要的测量基础。支持材料已公开以促进重现性和采纳。"}}
{"id": "2511.04020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04020", "abs": "https://arxiv.org/abs/2511.04020", "authors": ["Shiyin Lin"], "title": "Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises", "comment": null, "summary": "Large Language Models (LLMs) enhanced with retrieval -- commonly referred to\nas Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance\nin knowledge-intensive tasks. However, RAG pipelines often fail when retrieved\nevidence is incomplete, leaving gaps in the reasoning process. In such cases,\n\\emph{abductive inference} -- the process of generating plausible missing\npremises to explain observations -- offers a principled approach to bridge\nthese gaps. In this paper, we propose a framework that integrates abductive\ninference into retrieval-augmented LLMs. Our method detects insufficient\nevidence, generates candidate missing premises, and validates them through\nconsistency and plausibility checks. Experimental results on abductive\nreasoning and multi-hop QA benchmarks show that our approach improves both\nanswer accuracy and reasoning faithfulness. This work highlights abductive\ninference as a promising direction for enhancing the robustness and\nexplainability of RAG systems.", "AI": {"tldr": "本文提出一个将溯因推理整合到检索增强型大型语言模型（RAG）中的框架，以解决检索证据不完整导致推理中断的问题。该方法通过生成和验证缺失的前提来弥补知识空白，从而提高了答案准确性和推理忠实度。", "motivation": "检索增强生成（RAG）系统在知识密集型任务中表现出色，但当检索到的证据不完整时，其推理过程会产生空白并导致失败。溯因推理（生成合理解释观察结果的缺失前提）提供了一种弥补这些空白的原则性方法。", "method": "该研究提出一个将溯因推理整合到检索增强型大型语言模型中的框架。具体方法包括：检测证据不足、生成候选缺失前提，并通过一致性和合理性检查来验证这些前提。", "result": "在溯因推理和多跳问答基准测试上的实验结果表明，该方法显著提高了答案的准确性以及推理的忠实度。", "conclusion": "这项工作强调了溯因推理是增强RAG系统鲁棒性和可解释性的一个有前景的方向。"}}
{"id": "2511.04357", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04357", "abs": "https://arxiv.org/abs/2511.04357", "authors": ["Maëlic Neau", "Zoe Falomir", "Paulo E. Santos", "Anne-Gwenn Bosser", "Cédric Buche"], "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies", "comment": null, "summary": "Deploying autonomous robots that can learn new skills from demonstrations is\nan important challenge of modern robotics. Existing solutions often apply\nend-to-end imitation learning with Vision-Language Action (VLA) models or\nsymbolic approaches with Action Model Learning (AML). On the one hand, current\nVLA models are limited by the lack of high-level symbolic planning, which\nhinders their abilities in long-horizon tasks. On the other hand, symbolic\napproaches in AML lack generalization and scalability perspectives. In this\npaper we present a new neuro-symbolic approach, GraSP-VLA, a framework that\nuses a Continuous Scene Graph representation to generate a symbolic\nrepresentation of human demonstrations. This representation is used to generate\nnew planning domains during inference and serves as an orchestrator for\nlow-level VLA policies, scaling up the number of actions that can be reproduced\nin a row. Our results show that GraSP-VLA is effective for modeling symbolic\nrepresentations on the task of automatic planning domain generation from\nobservations. In addition, results on real-world experiments show the potential\nof our Continuous Scene Graph representation to orchestrate low-level VLA\npolicies in long-horizon tasks.", "AI": {"tldr": "本文提出了一种名为GraSP-VLA的神经符号方法，它利用连续场景图从人类演示中生成符号表示，用于规划领域生成和协调低级视觉-语言-动作（VLA）策略，从而解决长周期机器人任务中的挑战。", "motivation": "现有端到端视觉-语言-动作（VLA）模型缺乏高层符号规划能力，限制了其在长周期任务中的表现；而符号动作模型学习（AML）方法则缺乏泛化性和可扩展性。因此，需要一种结合两者优势的新方法。", "method": "GraSP-VLA框架使用连续场景图（Continuous Scene Graph）来生成人类演示的符号表示。这种表示在推理阶段用于生成新的规划领域，并作为低级VLA策略的协调器，从而扩展可连续执行的动作数量。", "result": "研究结果表明，GraSP-VLA在从观察中自动生成规划领域的符号表示建模方面是有效的。此外，在真实世界的实验中，其连续场景图表示展示了在长周期任务中协调低级VLA策略的潜力。", "conclusion": "GraSP-VLA通过其神经符号方法和连续场景图表示，成功地弥补了现有VLA模型和符号AML方法的不足，为机器人从演示中学习新技能提供了有效且可扩展的解决方案，尤其适用于长周期任务。"}}
{"id": "2511.03988", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.03988", "abs": "https://arxiv.org/abs/2511.03988", "authors": ["Wenshuo Qin", "Leyla Isik"], "title": "Simple 3D Pose Features Support Human and Machine Social Scene Understanding", "comment": "28 pages, 6 figures", "summary": "Humans can quickly and effortlessly extract a variety of information about\nothers' social interactions from visual input, ranging from visuospatial cues\nlike whether two people are facing each other to higher-level information. Yet,\nthe computations supporting these abilities remain poorly understood, and\nsocial interaction recognition continues to challenge even the most advanced AI\nvision systems. Here, we hypothesized that humans rely on 3D visuospatial pose\ninformation to make social interaction judgments, which is absent in most AI\nvision models. To test this, we combined state-of-the-art pose and depth\nestimation algorithms to extract 3D joint positions of people in short video\nclips depicting everyday human actions and compared their ability to predict\nhuman social interaction judgments with current AI vision models. Strikingly,\n3D joint positions outperformed most current AI vision models, revealing that\nkey social information is available in explicit body position but not in the\nlearned features of most vision models, including even the layer-wise\nembeddings of the pose models used to extract joint positions. To uncover the\ncritical pose features humans use to make social judgments, we derived a\ncompact set of 3D social pose features describing only the 3D position and\ndirection of faces in the videos. We found that these minimal descriptors\nmatched the predictive strength of the full set of 3D joints and significantly\nimproved the performance of off-the-shelf AI vision models when combined with\ntheir embeddings. Moreover, the degree to which 3D social pose features were\nrepresented in each off-the-shelf AI vision model predicted the model's ability\nto match human social judgments. Together, our findings provide strong evidence\nthat human social scene understanding relies on explicit representations of 3D\npose and can be supported by simple, structured visuospatial primitives.", "AI": {"tldr": "人类在理解社会互动时依赖3D视觉空间姿态信息，尤其是面部位置和方向，这比大多数AI模型表现更好。研究发现，这种结构化的3D姿态数据至关重要，并能显著提升AI模型的性能。", "motivation": "人类能轻松从视觉输入中提取关于他人社会互动的信息，但其计算机制尚不清楚，且即使最先进的AI视觉系统也难以识别社会互动。本研究假设人类依赖3D视觉空间姿态信息进行社会互动判断，而这在大多数AI视觉模型中是缺失的。", "method": "研究结合了最先进的姿态和深度估计算法，从描绘日常人类活动的短视频片段中提取了人物的3D关节位置。然后，将这些3D关节位置预测人类社会互动判断的能力与当前AI视觉模型进行了比较。此外，研究还推导出一组紧凑的3D社会姿态特征（仅描述视频中面部的3D位置和方向），并测试了这些最小描述符的预测能力是否与完整3D关节集相当，以及它们与现成AI视觉模型嵌入结合后能否提高AI性能。最后，评估了3D社会姿态特征在每个现成AI模型中的表示程度如何预测模型匹配人类社会判断的能力。", "result": "3D关节位置在预测人类社会互动判断方面显著优于大多数当前AI视觉模型，表明关键的社会信息存在于明确的身体位置中，而非大多数视觉模型（包括用于提取关节位置的姿态模型的层级嵌入）的学习特征中。一个紧凑的3D社会姿态特征集（面部3D位置和方向）与完整3D关节集的预测能力相当，并且在与现成AI视觉模型的嵌入结合时，显著提高了它们的性能。此外，3D社会姿态特征在每个现成AI视觉模型中的表示程度预示了该模型匹配人类社会判断的能力。", "conclusion": "研究结果提供了强有力的证据，表明人类对社会场景的理解依赖于3D姿态的明确表示，并且可以通过简单、结构化的视觉空间原语来支持。"}}
{"id": "2511.03992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03992", "abs": "https://arxiv.org/abs/2511.03992", "authors": ["Yuwen Tao", "Kanglei Zhou", "Xin Tan", "Yuan Xie"], "title": "CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation", "comment": null, "summary": "Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret\nfree-form language expressions and localize the corresponding 3D regions in\nGaussian fields. While recent advances have introduced cross-modal alignment\nbetween language and 3D geometry, existing pipelines still struggle with\ncross-view consistency due to their reliance on 2D rendered pseudo supervision\nand view specific feature learning. In this work, we present Camera Aware\nReferring Field (CaRF), a fully differentiable framework that operates directly\nin the 3D Gaussian space and achieves multi view consistency. Specifically,\nCaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates\ncamera geometry into Gaussian text interactions to explicitly model view\ndependent variations and enhance geometric reasoning. Building on this, In\nTraining Paired View Supervision (ITPVS) is proposed to align per Gaussian\nlogits across calibrated views during training, effectively mitigating single\nview overfitting and exposing inter view discrepancies for optimization.\nExtensive experiments on three representative benchmarks demonstrate that CaRF\nachieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of\nthe art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.\nMoreover, this work promotes more reliable and view consistent 3D scene\nunderstanding, with potential benefits for embodied AI, AR/VR interaction, and\nautonomous perception.", "AI": {"tldr": "本文提出了一种名为CaRF的3D高斯场参考分割框架，旨在解决现有方法在跨视图一致性方面的不足。CaRF通过引入高斯场相机编码和训练中配对视图监督，直接在3D高斯空间操作并实现了多视图一致性，在多个基准测试上显著优于现有技术。", "motivation": "现有3D高斯场参考分割（R3DGS）方法依赖2D渲染的伪监督和视图特定的特征学习，导致在跨视图一致性方面表现不佳，难以将语言表达式与3D高斯场中的对应区域进行可靠的定位。", "method": "本文提出了Camera Aware Referring Field (CaRF)，一个完全可微分的框架，直接在3D高斯空间中操作以实现多视图一致性。具体方法包括：1) Gaussian Field Camera Encoding (GFCE)，将相机几何信息融入高斯文本交互中，以显式建模视图相关变化并增强几何推理。2) In Training Paired View Supervision (ITPVS)，在训练期间对校准视图中的每个高斯逻辑进行对齐，以减轻单视图过拟合并优化视图间差异。", "result": "CaRF在三个代表性基准测试上实现了显著改进。在Ref LERF、LERF OVS和3D OVS数据集上，mIoU分别比现有最先进方法平均提高了16.8%、4.3%和2.0%。", "conclusion": "CaRF促进了更可靠和视图一致的3D场景理解，有望为具身智能、AR/VR交互和自主感知等领域带来潜在益处。"}}
{"id": "2511.04054", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04054", "abs": "https://arxiv.org/abs/2511.04054", "authors": ["Sheikh A. Tahmid", "Gennaro Notomista"], "title": "Necessary and Sufficient Conditions for the Optimization-Based Concurrent Execution of Learned Robotic Tasks", "comment": "currently in review", "summary": "In this work, we consider the problem of executing multiple tasks encoded by\nvalue functions, each learned through Reinforcement Learning, using an\noptimization-based framework. Prior works develop such a framework, but left\nunanswered a fundamental question of when learned value functions can be\nconcurrently executed. The main contribution of this work is to present\ntheorems which provide necessary and sufficient conditions to concurrently\nexecute sets of learned tasks within subsets of the state space, using a\npreviously proposed min-norm controller. These theorems provide insight into\nwhen learned control tasks are possible to be made concurrently executable,\nwhen they might already inherently be concurrently executable and when it is\nnot possible at all to make a set of learned tasks concurrently executable\nusing the previously proposed methods. Additional contributions of this work\ninclude extending the optimization-based framework to execute multiple tasks\nencoded by value functions to also account for value functions trained with a\ndiscount factor, making the overall framework more compatible with standard RL\npractices.", "AI": {"tldr": "本研究提出了在优化框架下并发执行多个通过强化学习学习到的任务（由值函数编码）的充要条件，并扩展了该框架以兼容带折扣因子的值函数。", "motivation": "先前的研究虽然开发了基于优化的多任务执行框架，但未能解决一个基本问题：何时可以并发执行学习到的值函数。", "method": "本研究使用一个先前提出的最小范数控制器，推导并提出了关于在状态空间子集内并发执行学习任务集合的充要条件定理。此外，还扩展了基于优化的框架，使其能够处理使用折扣因子训练的值函数，从而更兼容标准的强化学习实践。", "result": "研究提供了关于何时可以将学习到的控制任务并发执行、何时它们可能本身就可并发执行以及何时完全不可能并发执行的深入见解。同时，通过兼容折扣因子，使整体框架与标准RL实践更加兼容。", "conclusion": "本研究通过提供并发执行学习任务的充要条件，并扩展框架以兼容折扣因子，显著推进了基于优化框架下的多任务强化学习，解决了先前框架中的一个关键未解问题。"}}
{"id": "2511.04177", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04177", "abs": "https://arxiv.org/abs/2511.04177", "authors": ["Claire Yang", "Maya Cakmak", "Max Kleiman-Weiner"], "title": "When Empowerment Disempowers", "comment": null, "summary": "Empowerment, a measure of an agent's ability to control its environment, has\nbeen proposed as a universal goal-agnostic objective for motivating assistive\nbehavior in AI agents. While multi-human settings like homes and hospitals are\npromising for AI assistance, prior work on empowerment-based assistance assumes\nthat the agent assists one human in isolation. We introduce an open source\nmulti-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we\nempirically show that assistive RL agents optimizing for one human's\nempowerment can significantly reduce another human's environmental influence\nand rewards - a phenomenon we formalize as disempowerment. We characterize when\ndisempowerment occurs in these environments and show that joint empowerment\nmitigates disempowerment at the cost of the user's reward. Our work reveals a\nbroader challenge for the AI alignment community: goal-agnostic objectives that\nseem aligned in single-agent settings can become misaligned in multi-agent\ncontexts.", "AI": {"tldr": "研究表明，在多人类环境中，为单一人类赋能（empowerment）的AI代理可能导致其他人类的赋能被剥夺（disempowerment），揭示了目标无关性目标在多代理情境下的潜在失调问题。", "motivation": "赋能被提议作为AI代理激励辅助行为的通用、目标无关性目标。然而，先前的赋能辅助研究假设代理只协助一个孤立的人类。现实生活中，如家庭和医院等，多人类环境对AI辅助具有巨大潜力，因此需要研究赋能在多人类场景下的表现。", "method": "引入了一个开源的多人类网格世界测试套件Disempower-Grid。使用该套件，通过经验性地展示优化单个任务的赋能的强化学习代理如何显著减少另一个任务的环境影响力及奖励，并将其形式化为“赋能剥夺”（disempowerment）。研究并描述了赋能剥夺在这些环境中发生的条件，并探讨了联合赋能（joint empowerment）如何缓解赋能剥夺。", "result": "经验性结果表明，优化单个任务赋能的辅助强化学习代理可以显著减少另一个任务的环境影响力及奖励，这一现象被形式化为赋能剥夺。研究还发现，联合赋能可以缓解赋能剥夺，但代价是用户的奖励会降低。", "conclusion": "该工作揭示了AI对齐领域的一个更广泛挑战：在单代理设置中看似对齐的目标无关性目标，在多代理环境中可能会变得失调。这表明需要重新思考多人类AI辅助系统中的目标设计。"}}
{"id": "2511.04035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04035", "abs": "https://arxiv.org/abs/2511.04035", "authors": ["Dongji Gao", "Chenda Liao", "Changliang Liu", "Matthew Wiesner", "Leibny Paola Garcia", "Daniel Povey", "Sanjeev Khudanpur", "Jian Wu"], "title": "WST: Weakly Supervised Transducer for Automatic Speech Recognition", "comment": null, "summary": "The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in\nend-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily\non large-scale, high-quality annotated data, which are often costly and\ndifficult to obtain. To mitigate this reliance, we propose a Weakly Supervised\nTransducer (WST), which integrates a flexible training graph designed to\nrobustly handle errors in the transcripts without requiring additional\nconfidence estimation or auxiliary pre-trained models. Empirical evaluations on\nsynthetic and industrial datasets reveal that WST effectively maintains\nperformance even with transcription error rates of up to 70%, consistently\noutperforming existing Connectionist Temporal Classification (CTC)-based weakly\nsupervised approaches, such as Bypass Temporal Classification (BTC) and\nOmni-Temporal Classification (OTC). These results demonstrate the practical\nutility and robustness of WST in realistic ASR settings. The implementation\nwill be publicly available.", "AI": {"tldr": "本文提出一种弱监督Transducer (WST)，通过灵活的训练图有效处理转录错误，在高达70%的错误率下仍能保持高性能，优于现有弱监督方法，解决了RNN-T对大量高质量标注数据的依赖。", "motivation": "RNN-T在端到端自动语音识别 (ASR) 中广泛应用，但其性能严重依赖于昂贵且难以获取的大规模高质量标注数据。", "method": "提出弱监督Transducer (WST)，该方法集成了一个灵活的训练图，旨在鲁棒地处理转录中的错误，且无需额外的置信度估计或辅助预训练模型。", "result": "在合成和工业数据集上的评估表明，WST在转录错误率高达70%的情况下仍能有效保持性能。它持续优于现有的基于CTC的弱监督方法，如Bypass Temporal Classification (BTC) 和 Omni-Temporal Classification (OTC)。", "conclusion": "WST在实际ASR设置中展现出实用性和鲁棒性，有效缓解了对大量高质量标注数据的依赖。"}}
{"id": "2511.04246", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04246", "abs": "https://arxiv.org/abs/2511.04246", "authors": ["Sander De Witte", "Tom Lefebvre", "Thomas Neve", "Andras Retzler", "Guillaume Crevecoeur"], "title": "Differential Flatness of Quasi-Static Slider-Pusher Models with Applications in Control", "comment": null, "summary": "This paper investigates the dynamic properties of planar slider-pusher\nsystems as a motion primitive in manipulation tasks. To that end, we construct\na differential kinematic model deriving from the limit surface approach under\nthe quasi-static assumption and with negligible contact friction. The\nquasi-static model applies to generic slider shapes and circular pusher\ngeometries, enabling a differential kinematic representation of the system.\nFrom this model, we analyze differential flatness - a property advantageous for\ncontrol synthesis and planning - and find that slider-pusher systems with\npolygon sliders and circular pushers exhibit flatness with the centre of mass\nas a flat output. Leveraging this property, we propose two control strategies\nfor trajectory tracking: a cascaded quasi-static feedback strategy and a\ndynamic feedback linearization approach. We validate these strategies through\nclosed-loop simulations incorporating perturbed models and input noise, as well\nas experimental results using a physical setup with a finger-like pusher and\nvision-based state detection. The real-world experiments confirm the\napplicability of the simulation gains, highlighting the potential of the\nproposed methods for", "AI": {"tldr": "本文研究了平面推杆-滑块系统的动力学特性，构建了准静态模型，发现其具有微分平坦性，并提出了两种控制策略进行轨迹跟踪，通过仿真和实验验证了其有效性。", "motivation": "研究平面推杆-滑块系统作为操作任务中的基本运动原语的动态特性，并为其开发有效的控制策略。", "method": "构建了基于极限表面方法、准静态假设和可忽略接触摩擦的微分运动学模型，适用于通用滑块形状和圆形推杆。分析了系统的微分平坦性。提出了两种控制策略：级联准静态反馈策略和动态反馈线性化方法。通过闭环仿真和物理实验验证了这些策略。", "result": "具有多边形滑块和圆形推杆的推杆-滑块系统表现出微分平坦性，其质心是一个平坦输出。所提出的两种控制策略（级联准静态反馈和动态反馈线性化）在有扰动模型和输入噪声的仿真以及实际物理设置中都得到了验证，仿真增益适用于实际实验。", "conclusion": "所提出的控制方法利用了平面推杆-滑块系统的微分平坦性，在轨迹跟踪方面表现出潜力，并通过仿真和实际实验得到了有效验证。"}}
{"id": "2511.04220", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04220", "abs": "https://arxiv.org/abs/2511.04220", "authors": ["Alan Seroul", "Théo Fagnoni", "Inès Adnani", "Dana O. Mohamed", "Phillip Kingston"], "title": "Opus: A Quantitative Framework for Workflow Evaluation", "comment": null, "summary": "This paper introduces the Opus Workflow Evaluation Framework, a\nprobabilistic-normative formulation for quantifying Workflow quality and\nefficiency. It integrates notions of correctness, reliability, and cost into a\ncoherent mathematical model that enables direct comparison, scoring, and\noptimization of Workflows. The framework combines the Opus Workflow Reward, a\nprobabilistic function estimating expected performance through success\nlikelihood, resource usage, and output gain, with the Opus Workflow Normative\nPenalties, a set of measurable functions capturing structural and informational\nquality across Cohesion, Coupling, Observability, and Information Hygiene. It\nsupports automated Workflow assessment, ranking, and optimization within modern\nautomation systems such as Opus and can be integrated into Reinforcement\nLearning loops to guide Workflow discovery and refinement. In this paper, we\nintroduce the Opus Workflow Reward model that formalizes Workflow success as a\nprobabilistic expectation over costs and outcomes. We define measurable Opus\nWorkflow Normative Penalties capturing structural, semantic, and signal-related\nproperties of Workflows. Finally, we propose a unified optimization formulation\nfor identifying and ranking optimal Workflows under joint Reward-Penalty\ntrade-offs.", "AI": {"tldr": "本文提出了Opus工作流评估框架，一个概率-规范性模型，用于量化工作流的质量和效率，并支持其比较、评分和优化。", "motivation": "研究动机是为了量化工作流的质量和效率，将正确性、可靠性和成本整合到一个连贯的数学模型中，以便直接比较、评分和优化工作流。", "method": "该框架结合了Opus工作流奖励（一个估算预期性能的概率函数，考虑成功可能性、资源使用和产出增益）和Opus工作流规范性惩罚（一组衡量内聚性、耦合性、可观察性和信息卫生等结构和信息质量的可测量函数）。它支持自动化评估、排名和优化，并可集成到强化学习循环中。", "result": "本文介绍了Opus工作流奖励模型，将工作流成功形式化为成本和结果的概率期望；定义了Opus工作流规范性惩罚，用于捕捉工作流的结构、语义和信号相关属性；并提出了一个统一的优化公式，用于在奖励-惩罚权衡下识别和排名最优工作流。", "conclusion": "Opus工作流评估框架通过整合概率性奖励和规范性惩罚，实现对工作流质量和效率的量化，支持自动化评估、排名和优化，并能指导工作流的发现和改进。"}}
{"id": "2511.04375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04375", "abs": "https://arxiv.org/abs/2511.04375", "authors": ["Anna Mészáros", "Javier Alonso-Mora", "Jens Kober"], "title": "Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories", "comment": null, "summary": "Effectively capturing the joint distribution of all agents in a scene is\nrelevant for predicting the true evolution of the scene and in turn providing\nmore accurate information to the decision processes of autonomous vehicles.\nWhile new models have been developed for this purpose in recent years, it\nremains unclear how to best represent the joint distributions particularly from\nthe perspective of the interactions between agents. Thus far there is no clear\nconsensus on how best to represent interactions between agents; whether they\nshould be learned implicitly from data by neural networks, or explicitly\nmodeled using the spatial and temporal relations that are more grounded in\nhuman decision-making. This paper aims to study various means of describing\ninteractions within the same network structure and their effect on the final\nlearned joint distributions. Our findings show that more often than not, simply\nallowing a network to establish interactive connections between agents based on\ndata has a detrimental effect on performance. Instead, having well defined\ninteractions (such as which agent of an agent pair passes first at an\nintersection) can often bring about a clear boost in performance.", "AI": {"tldr": "本研究探讨了自动驾驶场景中智能体交互的不同建模方式。结果显示，明确定义交互（如“谁先通过”）比神经网络隐式学习交互更能提升性能。", "motivation": "有效捕捉场景中所有智能体的联合分布对于预测场景演变和为自动驾驶车辆提供准确信息至关重要。尽管已有新模型，但如何最好地表示联合分布，特别是智能体之间的交互，仍不明确，尤其是在隐式学习与显式建模之间缺乏共识。", "method": "本文在相同的网络结构内，研究了多种描述智能体交互的方式，并分析它们对最终学习到的联合分布的影响。具体比较了基于数据让网络建立交互连接（隐式学习）和使用明确定义的交互（显式建模）。", "result": "研究发现，简单地允许网络基于数据建立智能体间的交互连接往往会对性能产生不利影响。相反，拥有明确定义的交互（例如，在十字路口哪对智能体中的哪个智能体先行）通常能显著提升性能。", "conclusion": "为了更准确地预测场景演变，在自动驾驶领域中，明确建模智能体之间的交互（例如基于人类决策的空间和时间关系）比让神经网络隐式学习这些交互更为有效。"}}
{"id": "2511.03997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.03997", "abs": "https://arxiv.org/abs/2511.03997", "authors": ["Peiyao Wang", "Weining Wang", "Qi Li"], "title": "PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection", "comment": null, "summary": "Recent advances in text-to-video generation have achieved impressive\nperceptual quality, yet generated content often violates fundamental principles\nof physical plausibility - manifesting as implausible object dynamics,\nincoherent interactions, and unrealistic motion patterns. Such failures hinder\nthe deployment of video generation models in embodied AI, robotics, and\nsimulation-intensive domains. To bridge this gap, we propose PhysCorr, a\nunified framework for modeling, evaluating, and optimizing physical consistency\nin video generation. Specifically, we introduce PhysicsRM, the first\ndual-dimensional reward model that quantifies both intra-object stability and\ninter-object interactions. On this foundation, we develop PhyDPO, a novel\ndirect preference optimization pipeline that leverages contrastive feedback and\nphysics-aware reweighting to guide generation toward physically coherent\noutputs. Our approach is model-agnostic and scalable, enabling seamless\nintegration into a wide range of video diffusion and transformer-based\nbackbones. Extensive experiments across multiple benchmarks demonstrate that\nPhysCorr achieves significant improvements in physical realism while preserving\nvisual fidelity and semantic alignment. This work takes a critical step toward\nphysically grounded and trustworthy video generation.", "AI": {"tldr": "PhysCorr是一个统一框架，用于建模、评估和优化视频生成中的物理一致性，通过引入PhysicsRM奖励模型和PhyDPO优化管道，显著提升了生成视频的物理真实感。", "motivation": "当前文本到视频生成模型虽然在感知质量上有所进步，但常违反物理原理，表现为不合理的物体动态、不连贯的交互和不真实的运动模式，这阻碍了其在具身AI、机器人和模拟密集型领域的应用。", "method": "本文提出了PhysCorr框架。具体而言，引入了PhysicsRM，这是首个量化对象内部稳定性与对象间交互的双维度奖励模型。在此基础上，开发了PhyDPO，一种新颖的直接偏好优化管道，利用对比反馈和物理感知重加权来指导生成物理连贯的输出。该方法与模型无关且可扩展。", "result": "在多个基准测试中，PhysCorr显著提升了物理真实感，同时保持了视觉保真度和语义对齐。", "conclusion": "这项工作是迈向物理基础且值得信赖的视频生成关键一步。"}}
{"id": "2511.04070", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04070", "abs": "https://arxiv.org/abs/2511.04070", "authors": ["Shreya Havaldar", "Helen Jin", "Chaehyeon Kim", "Anton Xue", "Weiqiu You", "Marco Gatti", "Bhuvnesh Jain", "Helen Qu", "Daniel A Hashimoto", "Amin Madani", "Rajat Deo", "Sameed Ahmed M. Khatana", "Gary E. Weissman", "Lyle Ungar", "Eric Wong"], "title": "T-FIX: Text-Based Explanations with Features Interpretable to eXperts", "comment": null, "summary": "As LLMs are deployed in knowledge-intensive settings (e.g., surgery,\nastronomy, therapy), users expect not just answers, but also meaningful\nexplanations for those answers. In these settings, users are often domain\nexperts (e.g., doctors, astrophysicists, psychologists) who require\nexplanations that reflect expert-level reasoning. However, current evaluation\nschemes primarily emphasize plausibility or internal faithfulness of the\nexplanation, which fail to capture whether the content of the explanation truly\naligns with expert intuition. We formalize expert alignment as a criterion for\nevaluating explanations with T-FIX, a benchmark spanning seven\nknowledge-intensive domains. In collaboration with domain experts, we develop\nnovel metrics to measure the alignment of LLM explanations with expert\njudgment.", "AI": {"tldr": "在知识密集型领域，大型语言模型（LLM）的解释需要与专家直觉对齐。本文提出了T-FIX基准和新指标，用于衡量LLM解释与专家判断的对齐程度。", "motivation": "当LLM应用于手术、天文学、治疗等知识密集型场景时，用户（通常是领域专家）期望LLM不仅提供答案，还能提供有意义且符合专家级推理的解释。然而，当前评估方案主要关注解释的合理性或内部忠实度，未能捕捉解释内容是否真正符合专家直觉。", "method": "本文将“专家对齐”正式化为评估解释的标准，并开发了T-FIX基准，该基准涵盖七个知识密集型领域。研究与领域专家合作，开发了衡量LLM解释与专家判断对齐程度的新指标。", "result": "提出了T-FIX基准以及一套新颖的度量指标，用于评估LLM解释与领域专家判断的对齐程度。", "conclusion": "通过T-FIX基准和新的度量指标，可以更有效地评估LLM在知识密集型环境中解释的专家对齐性，从而确保其解释能够满足领域专家的需求。"}}
{"id": "2511.04381", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04381", "abs": "https://arxiv.org/abs/2511.04381", "authors": ["Dexin wang", "Faliang Chang", "Chunsheng Liu"], "title": "ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation", "comment": null, "summary": "Efficiently leveraging simulation to acquire advanced manipulation skills is\nboth challenging and highly significant. We introduce \\textit{ForeRobo}, a\ngenerative robotic agent that utilizes generative simulations to autonomously\nacquire manipulation skills driven by envisioned goal states. Instead of\ndirectly learning low-level policies, we advocate integrating generative\nparadigms with classical control. Our approach equips a robotic agent with a\nself-guided \\textit{propose-generate-learn-actuate} cycle. The agent first\nproposes the skills to be acquired and constructs the corresponding simulation\nenvironments; it then configures objects into appropriate arrangements to\ngenerate skill-consistent goal states (\\textit{ForeGen}). Subsequently, the\nvirtually infinite data produced by ForeGen are used to train the proposed\nstate generation model (\\textit{ForeFormer}), which establishes point-wise\ncorrespondences by predicting the 3D goal position of every point in the\ncurrent state, based on the scene state and task instructions. Finally,\nclassical control algorithms are employed to drive the robot in real-world\nenvironments to execute actions based on the envisioned goal states. Compared\nwith end-to-end policy learning methods, ForeFormer offers superior\ninterpretability and execution efficiency. We train and benchmark ForeFormer\nacross a variety of rigid-body and articulated-object manipulation tasks, and\nobserve an average improvement of 56.32\\% over the state-of-the-art state\ngeneration models, demonstrating strong generality across different\nmanipulation patterns. Moreover, in real-world evaluations involving more than\n20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits\nremarkable generalization capabilities, attaining an average success rate of\n79.28\\%.", "AI": {"tldr": "ForeRobo是一个生成式机器人代理，它利用生成式仿真来自主学习操作技能。它通过预测三维目标状态并结合经典控制来实现零样本模拟到真实世界的迁移，展现出卓越的泛化能力。", "motivation": "高效利用仿真来获取高级操作技能既具挑战性又意义重大。直接学习低级策略效率不高。", "method": "ForeRobo采用“提议-生成-学习-执行”的自引导循环。首先，代理提议技能并构建仿真环境；接着，通过ForeGen生成与技能一致的目标状态，产生无限数据。然后，使用这些数据训练ForeFormer模型，该模型根据场景状态和任务指令预测当前状态中每个点的三维目标位置。最后，利用经典控制算法在真实世界中根据设想的目标状态驱动机器人执行动作。", "result": "与端到端策略学习方法相比，ForeFormer提供了卓越的可解释性和执行效率。在各种刚体和关节对象操作任务中，ForeFormer比最先进的状态生成模型平均提高了56.32%。在超过20项真实世界机器人任务中，ForeRobo实现了零样本模拟到真实世界的迁移，并取得了79.28%的平均成功率，展现出显著的泛化能力。", "conclusion": "ForeRobo通过将生成范式与经典控制相结合，能够高效、可解释地利用生成式仿真自主获取操作技能。它在模拟和真实世界中均表现出强大的泛化能力和零样本迁移潜力。"}}
{"id": "2511.04235", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.04235", "abs": "https://arxiv.org/abs/2511.04235", "authors": ["Zhengru Fang", "Yu Guo", "Jingjing Wang", "Yuang Zhang", "Haonan An", "Yinhai Wang", "Yuguang Fang"], "title": "Shared Spatial Memory Through Predictive Coding", "comment": "We have prepared the open-source code and video demonstration pages:\n  1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html", "summary": "Sharing and reconstructing a consistent spatial memory is a critical\nchallenge in multi-agent systems, where partial observability and limited\nbandwidth often lead to catastrophic failures in coordination. We introduce a\nmulti-agent predictive coding framework that formulate coordination as the\nminimization of mutual uncertainty among agents. Instantiated as an information\nbottleneck objective, it prompts agents to learn not only who and what to\ncommunicate but also when. At the foundation of this framework lies a\ngrid-cell-like metric as internal spatial coding for self-localization,\nemerging spontaneously from self-supervised motion prediction. Building upon\nthis internal spatial code, agents gradually develop a bandwidth-efficient\ncommunication mechanism and specialized neural populations that encode\npartners' locations: an artificial analogue of hippocampal social place cells\n(SPCs). These social representations are further enacted by a hierarchical\nreinforcement learning policy that actively explores to reduce joint\nuncertainty. On the Memory-Maze benchmark, our approach shows exceptional\nresilience to bandwidth constraints: success degrades gracefully from 73.5% to\n64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast\nbaseline collapses from 67.6% to 28.6%. Our findings establish a theoretically\nprincipled and biologically plausible basis for how complex social\nrepresentations emerge from a unified predictive drive, leading to social\ncollective intelligence.", "AI": {"tldr": "本文提出了一种多智能体预测编码框架，通过最小化智能体间的相互不确定性来实现一致的空间记忆和协调。该框架利用类网格细胞的空间编码和人工社交地点细胞，实现了在低带宽条件下的高效通信和卓越的协调性能。", "motivation": "在多智能体系统中，由于部分可观测性和有限带宽，共享和重建一致的空间记忆是一个关键挑战，常常导致协调失败。", "method": "该研究引入了一个多智能体预测编码框架，将协调表述为最小化智能体间的相互不确定性。通过信息瓶颈目标，智能体学习何时、与谁、通信什么。该框架的基础是自监督运动预测中自发产生的类网格细胞度量作为内部空间编码。在此基础上，智能体发展出带宽高效的通信机制和编码伙伴位置的专用神经网络群（人工海马社交地点细胞）。这些社交表征通过分层强化学习策略进一步实现，主动探索以减少联合不确定性。", "result": "在Memory-Maze基准测试中，该方法对带宽限制表现出卓越的韧性：当带宽从128位/步缩减到4位/步时，成功率从73.5%优雅地下降到64.4%，而全广播基线则从67.6%骤降至28.6%。", "conclusion": "研究结果为复杂社交表征如何从统一的预测驱动中出现，并导致社交集体智能，建立了理论上合理且生物学上可信的基础。"}}
{"id": "2511.04072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04072", "abs": "https://arxiv.org/abs/2511.04072", "authors": ["Xinying Qian", "Ying Zhang", "Yu Zhao", "Baohang Zhou", "Xuhui Sui", "Xiaojie Yuan"], "title": "Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering", "comment": "Submitted to the IEEE for possible publication", "summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer\ntime-sensitive questions by leveraging factual information from Temporal\nKnowledge Graphs (TKGs). While previous studies have employed pre-trained TKG\nembeddings or graph neural networks to inject temporal knowledge, they fail to\nfully understand the complex semantic information of time constraints.\nRecently, Large Language Models (LLMs) have shown remarkable progress,\nbenefiting from their strong semantic understanding and reasoning\ngeneralization capabilities. However, their temporal reasoning ability remains\nlimited. LLMs frequently suffer from hallucination and a lack of knowledge. To\naddress these limitations, we propose the Plan of Knowledge framework with a\ncontrastive temporal retriever, which is named PoK. Specifically, the proposed\nPlan of Knowledge module decomposes a complex temporal question into a sequence\nof sub-objectives from the pre-defined tools, serving as intermediate guidance\nfor reasoning exploration. In parallel, we construct a Temporal Knowledge Store\n(TKS) with a contrastive retrieval framework, enabling the model to selectively\nretrieve semantically and temporally aligned facts from TKGs. By combining\nstructured planning with temporal knowledge retrieval, PoK effectively enhances\nthe interpretability and factual consistency of temporal reasoning. Extensive\nexperiments on four benchmark TKGQA datasets demonstrate that PoK significantly\nimproves the retrieval precision and reasoning accuracy of LLMs, surpassing the\nperformance of the state-of-the-art TKGQA methods by 56.0% at most.", "AI": {"tldr": "该研究提出了PoK框架，通过结合结构化规划和对比时间知识检索，显著提升了大型语言模型（LLMs）在时间知识图谱问答（TKGQA）中的时间推理能力、事实一致性和可解释性。", "motivation": "现有TKGQA方法未能充分理解时间约束的复杂语义信息。尽管LLMs具有强大的语义理解和推理能力，但其时间推理能力有限，常出现幻觉和知识缺乏问题。", "method": "本文提出了名为PoK的“知识规划”（Plan of Knowledge）框架，包含两个核心模块：1) 知识规划模块：将复杂的时间问题分解为一系列预定义工具的子目标，作为推理探索的中间指导。2) 时间知识存储（TKS）模块：构建一个带有对比检索框架的TKS，使模型能够从时间知识图谱中选择性地检索语义和时间对齐的事实。PoK通过结合结构化规划和时间知识检索来增强时间推理。", "result": "PoK有效提升了时间推理的可解释性和事实一致性。在四个基准TKGQA数据集上的广泛实验表明，PoK显著提高了LLMs的检索精度和推理准确性，性能最多超越了现有最先进的TKGQA方法56.0%。", "conclusion": "PoK框架通过结合结构化规划和对比时间知识检索，成功解决了LLMs在TKGQA中时间推理能力不足、幻觉和知识缺乏的问题，显著提升了模型性能、可解释性和事实一致性。"}}
{"id": "2511.04330", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04330", "abs": "https://arxiv.org/abs/2511.04330", "authors": ["Christian Portilla", "Arviandy G Aribowo", "Ramachandran Anantharaman", "César A Gómez-Pérez", "Leyla Özkan"], "title": "Data-Driven Modeling of Photosynthesis Regulation Under Oscillating Light Condition - Part I: In-Silico Exploration", "comment": "10 pages, 14 figures", "summary": "This paper explores the application of data-driven system identification\ntechniques in the frequency domain to obtain simplified, control-oriented\nmodels of photosynthesis regulation under oscillating light conditions.\nIn-silico datasets are generated using simulations of the physics-based Basic\nDREAM Model (BDM) Funete et al.[2024], with light intensity signals --\ncomprising DC (static) and AC (modulated) components as input and chlorophyll\nfluorescence (ChlF) as output. Using these data, the Best Linear Approximation\n(BLA) method is employed to estimate second-order linear time-invariant (LTI)\ntransfer function models across different operating conditions defined by DC\nlevels and modulation frequencies of light intensity. Building on these local\nmodels, a Linear Parameter-Varying (LPV) representation is constructed, in\nwhich the scheduling parameter is defined by the DC values of the light\nintensity, providing a compact state-space representation of the system\ndynamics.", "AI": {"tldr": "本文利用数据驱动的系统辨识技术，基于物理模型仿真数据，在频域内为振荡光照下光合作用调控建立了简化、面向控制的线性时不变（LTI）和线性参数变化（LPV）模型。", "motivation": "研究动机是为了获得简化且便于控制设计的模型，以描述振荡光照条件下光合作用的调控机制，这比复杂的物理模型更适合控制应用。", "method": "研究方法包括：1. 使用基于物理的Basic DREAM Model (BDM) 生成仿真数据集，输入为包含直流（DC）和交流（AC）分量的光强度信号，输出为叶绿素荧光（ChlF）。2. 采用最佳线性近似（BLA）方法，在不同操作条件下（由光强度的DC水平和调制频率定义）估计二阶线性时不变（LTI）传递函数模型。3. 基于这些局部模型，构建了一个线性参数变化（LPV）表示，其中调度参数由光强度的DC值定义。", "result": "主要结果是获得了在不同操作条件下光合作用调控的二阶LTI传递函数模型，并构建了一个以光强度DC值为调度参数的紧凑LPV状态空间表示，用于描述系统动力学。", "conclusion": "结论是成功地运用数据驱动的系统辨识技术，在频域内为振荡光照下的光合作用调控开发了简化的、面向控制的LTI和LPV模型。"}}
{"id": "2511.04016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04016", "abs": "https://arxiv.org/abs/2511.04016", "authors": ["Mahmoud Soliman", "Islam Osman", "Mohamed S. Shehata", "Rasika Rajapakshe"], "title": "MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging", "comment": "10 pages, 2 figures", "summary": "The performance of vision models in medical imaging is often hindered by the\nprevailing paradigm of fine-tuning backbones pre-trained on out-of-domain\nnatural images. To address this fundamental domain gap, we propose MedDChest, a\nnew foundational Vision Transformer (ViT) model optimized specifically for\nthoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,\nmultimodal dataset of over 1.2 million images, encompassing different\nmodalities including Chest X-ray and Computed Tomography (CT) compiled from 10\npublic sources. A core technical contribution of our work is Guided Random\nResized Crops, a novel content-aware data augmentation strategy that biases\nsampling towards anatomically relevant regions, overcoming the inefficiency of\nstandard cropping techniques on medical scans. We validate our model's\neffectiveness by fine-tuning it on a diverse set of downstream diagnostic\ntasks. Comprehensive experiments empirically demonstrate that MedDChest\nsignificantly outperforms strong, publicly available ImageNet-pretrained\nmodels. By establishing the superiority of large-scale, in-domain pre-training\ncombined with domain-specific data augmentation, MedDChest provides a powerful\nand robust feature extractor that serves as a significantly better starting\npoint for a wide array of thoracic diagnostic tasks. The model weights will be\nmade publicly available to foster future research and applications.", "AI": {"tldr": "MedDChest是一个专门针对胸腔影像优化的新型基础Vision Transformer模型，通过在大规模多模态胸腔影像数据集上从头预训练，并结合内容感知的数据增强策略，显著优于ImageNet预训练模型，为胸腔诊断任务提供更优的起点。", "motivation": "当前医学影像视觉模型的性能受限于将预训练于域外自然图像的骨干模型进行微调，导致存在显著的领域差距。", "method": "本文提出了MedDChest模型，在一个包含超过120万张胸部X射线和CT图像的大规模、多模态、精选数据集中从头开始预训练。核心技术贡献是“引导随机裁剪”（Guided Random Resized Crops），这是一种新颖的、内容感知的数据增强策略，能将采样偏向解剖学相关区域，克服了标准裁剪技术在医学扫描上的低效性。", "result": "通过在多样化的下游诊断任务上进行微调验证，MedDChest在经验上显著优于强大的、公开可用的ImageNet预训练模型。模型权重将公开可用。", "conclusion": "大规模、域内预训练结合领域特定数据增强的方法被证实具有优越性。MedDChest提供了一个强大且稳健的特征提取器，可作为各种胸腔诊断任务的显著更优的起点。"}}
{"id": "2511.04421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04421", "abs": "https://arxiv.org/abs/2511.04421", "authors": ["Yueyang Weng", "Xiaopeng Zhang", "Yongjin Mu", "Yingcong Zhu", "Yanjie Li", "Qi Liu"], "title": "Temporal Action Selection for Action Chunking", "comment": null, "summary": "Action chunking is a widely adopted approach in Learning from Demonstration\n(LfD). By modeling multi-step action chunks rather than single-step actions,\naction chunking significantly enhances modeling capabilities for human expert\npolicies. However, the reduced decision frequency restricts the utilization of\nrecent observations, degrading reactivity - particularly evident in the\ninadequate adaptation to sensor noise and dynamic environmental changes.\nExisting efforts to address this issue have primarily resorted to trading off\nreactivity against decision consistency, without achieving both. To address\nthis limitation, we propose a novel algorithm, Temporal Action Selector (TAS),\nwhich caches predicted action chunks from multiple timesteps and dynamically\nselects the optimal action through a lightweight selector network. TAS achieves\nbalanced optimization across three critical dimensions: reactivity, decision\nconsistency, and motion coherence. Experiments across multiple tasks with\ndiverse base policies show that TAS significantly improves success rates -\nyielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a\nbase policy with residual reinforcement learning (RL) substantially enhances\ntraining efficiency and elevates the performance plateau. Experiments in both\nsimulation and physical robots confirm the method's efficacy.", "AI": {"tldr": "动作分块在模仿学习中提高了建模能力但降低了反应性。本文提出时间动作选择器（TAS），通过缓存和动态选择动作块，平衡了反应性、决策一致性和运动连贯性，显著提高了成功率，并能有效提升残差强化学习的训练效率和性能。", "motivation": "动作分块（Action chunking）在模仿学习（LfD）中虽能增强对人类专家策略的建模能力，但其降低的决策频率限制了对最新观测的利用，导致反应性下降，尤其在应对传感器噪声和动态环境变化时表现不佳。现有解决方案往往以牺牲决策一致性来换取反应性，未能同时实现两者。", "method": "本文提出一种新算法——时间动作选择器（Temporal Action Selector, TAS）。该方法缓存来自多个时间步的预测动作块，并通过一个轻量级的选择器网络动态选择最优动作。TAS旨在同时优化反应性、决策一致性和运动连贯性三个关键维度。", "result": "TAS在多项任务和不同基础策略下显著提高了成功率，绝对增益高达73.3%。此外，将TAS作为基础策略与残差强化学习（RL）结合，能大幅提升训练效率和性能上限。该方法的有效性已在仿真和物理机器人实验中得到证实。", "conclusion": "TAS算法成功解决了动作分块在模仿学习中反应性不足的问题，通过动态选择机制，有效平衡了反应性、决策一致性和运动连贯性。这不仅显著提升了任务成功率，也为强化学习的训练效率和性能带来了实质性改善。"}}
{"id": "2511.04285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04285", "abs": "https://arxiv.org/abs/2511.04285", "authors": ["Zeng Zhiyuan", "Jiashuo Liu", "Zhangyue Yin", "Ge Zhang", "Wenhao Huang", "Xipeng Qiu"], "title": "RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization", "comment": null, "summary": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for\ntraining large reasoning models, its training dynamics harbor a critical\nchallenge: RL overfitting, where models gain training rewards but lose\ngeneralization. Our analysis reveals this is driven by policy\nover-specialization and catastrophic forgetting of diverse solutions generated\nduring training. Standard optimization discards this valuable inter-step policy\ndiversity. To address this, we introduce RLoop, a self-improving framework\nbuilt on iterative policy initialization. RLoop transforms the standard\ntraining process into a virtuous cycle: it first uses RL to explore the\nsolution space from a given policy, then filters the successful trajectories to\ncreate an expert dataset. This dataset is used via Rejection-sampling\nFine-Tuning (RFT) to refine the initial policy, creating a superior starting\npoint for the next iteration. This loop of exploration and exploitation via\niterative re-initialization effectively converts transient policy variations\ninto robust performance gains. Our experiments show RLoop mitigates forgetting\nand substantially improves generalization, boosting average accuracy by 9% and\npass@32 by over 15% compared to vanilla RL.", "AI": {"tldr": "RLVR训练大模型存在过拟合问题，导致泛化性下降。本文提出RLoop框架，通过迭代策略初始化，结合RL探索和拒绝采样微调（RFT），将成功轨迹转化为专家数据集以优化策略，有效缓解遗忘并显著提升泛化能力。", "motivation": "可验证奖励强化学习（RLVR）在训练大型推理模型时面临严重的RL过拟合挑战，模型在获得训练奖励的同时却失去了泛化能力。这主要是由策略过度专业化和对训练过程中产生的多样化解决方案的灾难性遗忘所驱动的。标准的优化方法会丢弃这种有价值的中间步骤策略多样性。", "method": "本文提出了RLoop，一个基于迭代策略初始化的自改进框架。RLoop将标准训练过程转化为良性循环：首先使用RL从给定策略探索解决方案空间，然后过滤成功的轨迹以创建专家数据集。接着，通过拒绝采样微调（RFT）使用该数据集来优化初始策略，为下一次迭代创建一个更好的起点。这种通过迭代重新初始化进行的探索和利用循环，有效地将瞬态策略变化转化为稳健的性能提升。", "result": "实验结果表明，RLoop有效缓解了遗忘问题，并显著提高了泛化能力。与传统的RL方法相比，RLoop的平均准确率提高了9%，pass@32指标提高了15%以上。", "conclusion": "RLoop框架通过迭代策略初始化和结合探索与利用的策略优化循环，成功解决了RLVR中的过拟合和灾难性遗忘问题，将瞬态策略变化转化为持久的性能增益，从而显著提升了模型的泛化能力和整体表现。"}}
{"id": "2511.04008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04008", "abs": "https://arxiv.org/abs/2511.04008", "authors": ["Mahmoud Soliman", "Omar Abdelaziz", "Ahmed Radwan", "Anand", "Mohamed Shehata"], "title": "GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization", "comment": "6 pages, 3 figures", "summary": "Domain generalization (DG) seeks robust Vision Transformer (ViT) performance\non unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;\nstandard fine-tuning is costly and can impair generalization. We propose\nGNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a\nMixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead\nof token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,\nSAGE) operates on inter-patch graphs to dynamically assign patches to\nspecialized experts. This context-aware GNN routing leverages inter-patch\nrelationships for better adaptation to domain shifts. GNN-MoE achieves\nstate-of-the-art or competitive DG benchmark performance with high parameter\nefficiency, highlighting the utility of graph-based contextual routing for\nrobust, lightweight DG.", "AI": {"tldr": "该论文提出GNN-MoE，一种用于域泛化（DG）的参数高效微调（PEFT）方法，通过图神经网络（GNN）路由器在图像块间图上动态分配图像块给专家模型，以实现鲁棒且轻量级的ViT性能。", "motivation": "在未见领域上实现Vision Transformer（ViT）的鲁棒性能是域泛化（DG）的目标。然而，有效适应预训练ViT以进行DG具有挑战性，因为标准的微调方法成本高昂且可能损害泛化能力。", "method": "本文提出了GNN-MoE，通过结合参数高效微调（PEFT）和混合专家（MoE）框架来增强DG。该方法利用高效的Kronecker适配器。其核心创新是一个新颖的图神经网络（GNN）路由器（可以是GCN、GAT或SAGE），它在图像块间图上操作，而不是传统的基于token的路由，动态地将图像块分配给专业的专家。这种上下文感知的GNN路由利用图像块间的关系，以更好地适应域偏移。", "result": "GNN-MoE在DG基准测试中取得了最先进或具有竞争力的性能，同时保持了高参数效率。", "conclusion": "研究结果突出了基于图的上下文路由对于实现鲁棒、轻量级域泛化任务的实用性。"}}
{"id": "2511.04293", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04293", "abs": "https://arxiv.org/abs/2511.04293", "authors": ["Jovana Kovačević", "Felix Langner", "Erfan Tajalli-Ardekani", "Marvin Dorn", "Simon Waczowicz", "Ralf Mikut", "Jörg Matthes", "Hüseyin K. Çakmak", "Veit Hagenmeyer"], "title": "ComEMS4Build: Comfort-Oriented Energy Management System for Residential Buildings using Hydrogen for Seasonal Storage", "comment": "30 pages, 14 figures, Submitted to Applied Energy Journal", "summary": "Integrating flexible loads and storage systems into the residential sector\ncontributes to the alignment of volatile renewable generation with demand.\nBesides batteries serving as a short-term storage solution, residential\nbuildings can benefit from a Hydrogen (H2) storage system, allowing seasonal\nshifting of renewable energy. However, as the initial costs of H2 systems are\nhigh, coupling a Fuel Cell (FC) with a Heat Pump (HP) can contribute to the\nsize reduction of the H2 system. The present study develops a Comfort-Oriented\nEnergy Management System for Residential Buildings (ComEMS4Build) comprising\nPhotovoltaics (PV), Battery Energy Storage System (BESS), and H2 storage, where\nFC and HP are envisioned as complementary technologies. The fuzzy-logic-based\nComEMS4Build is designed and evaluated over a period of 12 weeks in winter for\na family household building in Germany using a semi-synthetic modeling\napproach. The Rule-Based Control (RBC), which serves as a lower benchmark, is a\nscheduler designed to require minimal inputs for operation. The Model\nPredictive Control (MPC) is intended as a cost-optimal benchmark with an ideal\nforecast. The results show that ComEMS4Build, similar to MPC, does not violate\nthe thermal comfort of occupants in 10 out of 12 weeks, while RBC has a\nslightly higher median discomfort of 0.68 Kh. The ComEMS4Build increases the\nweekly electricity costs by 12.06 EUR compared to MPC, while RBC increases the\nweekly costs by 30.14 EUR. The ComEMS4Build improves the Hybrid Energy Storage\nSystem (HESS) utilization and energy exchange with the main grid compared to\nthe RBC. However, when it comes to the FC operation, the RBC has an advantage,\nas it reduces the toggling counts by 3.48% and working hours by 7.59% compared\nto MPC...", "AI": {"tldr": "本研究开发了一种基于模糊逻辑的面向舒适度的住宅建筑能量管理系统 (ComEMS4Build)，集成光伏、电池和氢能存储（燃料电池与热泵辅助），在冬季评估中，其在保持热舒适性和降低电力成本方面优于基于规则的控制，但成本略高于理想模型预测控制。", "motivation": "将灵活负荷和储能系统整合到住宅部门有助于平衡不稳定的可再生能源发电与需求。除了短期电池储能外，氢能系统可实现可再生能源的季节性转移。然而，由于氢能系统初始成本高昂，将燃料电池与热泵结合可有助于减小氢能系统规模，从而激发了开发更高效能量管理系统的需求。", "method": "本研究开发了ComEMS4Build，一个包含光伏、电池储能系统和氢能存储（燃料电池和热泵为补充技术）的面向舒适度的住宅建筑能量管理系统。该系统基于模糊逻辑设计，并采用半合成建模方法，在德国一个家庭住宅建筑中，对12个冬季周进行了评估。同时，将其性能与作为下限基准的基于规则的控制（RBC）和作为成本最优基准的理想模型预测控制（MPC）进行了比较。", "result": "结果显示，ComEMS4Build与MPC类似，在12周中有10周未违反居住者的热舒适度，而RBC的中位数不适度略高（0.68 Kh）。ComEMS4Build每周电费比MPC高12.06欧元，而RBC则高30.14欧元。与RBC相比，ComEMS4Build提高了混合储能系统的利用率和与主电网的能量交换。然而，在燃料电池运行方面，RBC具有优势，与MPC相比，它将燃料电池的开关次数减少了3.48%，工作时间减少了7.59%。", "conclusion": "ComEMS4Build在住宅建筑中实现了舒适度与成本的平衡，其性能在热舒适性和电费方面优于基于规则的控制。尽管其电力成本略高于具有理想预测的MPC，但在提高混合储能系统利用率和电网交互方面表现良好。在燃料电池运行效率方面，RBC在减少开关次数和工作时间上表现出一定的优势。"}}
{"id": "2511.04077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04077", "abs": "https://arxiv.org/abs/2511.04077", "authors": ["Špela Vintar", "Jan Jona Javoršek"], "title": "The truth is no diaper: Human and AI-generated associations to emotional words", "comment": "6 pages, 1 figure. Presented at ICCC'25, Campinas, Brazil", "summary": "Human word associations are a well-known method of gaining insight into the\ninternal mental lexicon, but the responses spontaneously offered by human\nparticipants to word cues are not always predictable as they may be influenced\nby personal experience, emotions or individual cognitive styles. The ability to\nform associative links between seemingly unrelated concepts can be the driving\nmechanisms of creativity. We perform a comparison of the associative behaviour\nof humans compared to large language models. More specifically, we explore\nassociations to emotionally loaded words and try to determine whether large\nlanguage models generate associations in a similar way to humans. We find that\nthe overlap between humans and LLMs is moderate, but also that the associations\nof LLMs tend to amplify the underlying emotional load of the stimulus, and that\nthey tend to be more predictable and less creative than human ones.", "AI": {"tldr": "该研究比较了人类与大型语言模型（LLM）的词语联想行为，发现LLM与人类联想有中等程度重叠，但LLM倾向于放大情感负荷，且更可预测、缺乏创造性。", "motivation": "词语联想是了解人类心智词典的重要方法，但人类联想受个人因素影响且难以预测。本研究旨在探究大型语言模型（LLM）是否能像人类一样进行联想，特别是对带有情感负荷的词语的联想，并考察LLM联想与创造力的关系。", "method": "通过比较人类和大型语言模型（LLM）对词语（特别是情感词）的联想行为来执行。", "result": "人类与LLM的联想重叠度适中。LLM的联想倾向于放大刺激词语的潜在情感负荷，并且比人类的联想更可预测、更缺乏创造性。", "conclusion": "尽管与人类联想有中等程度的重叠，大型语言模型在词语联想方面表现出与人类不同的特征，尤其是在情感放大、可预测性和创造性方面。"}}
{"id": "2511.04029", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.04029", "abs": "https://arxiv.org/abs/2511.04029", "authors": ["Yihao Luo", "Xianglong He", "Chuanyu Pan", "Yiwen Chen", "Jiaqi Wu", "Yangguang Li", "Wanli Ouyang", "Yuanming Hu", "Guang Yang", "ChoonHwai Yap"], "title": "Near-Lossless 3D Voxel Representation Free from Iso-surface", "comment": null, "summary": "Accurate and efficient voxelized representations of 3D meshes are the\nfoundation of 3D reconstruction and generation. However, existing\nrepresentations based on iso-surface heavily rely on water-tightening or\nrendering optimization, which inevitably compromise geometric fidelity. We\npropose Faithful Contouring, a sparse voxelized representation that supports\n2048+ resolutions for arbitrary meshes, requiring neither converting meshes to\nfield functions nor extracting the isosurface during remeshing. It achieves\nnear-lossless fidelity by preserving sharpness and internal structures, even\nfor challenging cases with complex geometry and topology. The proposed method\nalso shows flexibility for texturing, manipulation, and editing. Beyond\nrepresentation, we design a dual-mode autoencoder for Faithful Contouring,\nenabling scalable and detail-preserving shape reconstruction. Extensive\nexperiments show that Faithful Contouring surpasses existing methods in\naccuracy and efficiency for both representation and reconstruction. For direct\nrepresentation, it achieves distance errors at the $10^{-5}$ level; for mesh\nreconstruction, it yields a 93\\% reduction in Chamfer Distance and a 35\\%\nimprovement in F-score over strong baselines, confirming superior fidelity as a\nrepresentation for 3D learning tasks.", "AI": {"tldr": "提出了一种名为“忠实轮廓”（Faithful Contouring）的稀疏体素化表示方法，可实现任意网格的近乎无损的几何保真度，并设计了双模态自编码器用于可扩展的形状重建。", "motivation": "现有的基于等值面的三维网格体素化表示方法，因依赖水密化或渲染优化而牺牲了几何保真度。需要一种更准确高效、能保持几何细节的体素化表示方法。", "method": "提出了“忠实轮廓”方法，这是一种稀疏体素化表示，支持2048+分辨率，无需将网格转换为场函数或在重新网格化时提取等值面。它通过保留锐度和内部结构实现近乎无损的保真度。此外，还设计了一个双模态自编码器，用于可扩展且细节保留的形状重建。", "result": "在直接表示方面，实现了$10^{-5}$级别的距离误差。在网格重建方面，与强基线相比，倒角距离（Chamfer Distance）减少了93%，F-score提高了35%。在准确性和效率上均超越现有方法。", "conclusion": "“忠实轮廓”方法在三维网格表示和重建方面展现出卓越的保真度和效率，使其成为三维学习任务的优秀表示方法。"}}
{"id": "2511.04555", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04555", "abs": "https://arxiv.org/abs/2511.04555", "authors": ["Tao Lin", "Yilei Zhong", "Yuxin Du", "Jingjing Zhang", "Jiting Liu", "Yinxinyu Chen", "Encheng Gu", "Ziyan Liu", "Hongyi Cai", "Yanwen Zou", "Lixing Zou", "Zhaoye Zhou", "Gen Li", "Bo Zhao"], "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment", "comment": "Github: https://github.com/MINT-SJTU/Evo-1", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models.", "AI": {"tldr": "Evo-1是一种轻量级VLA模型，无需大规模机器人数据预训练，通过新颖的架构和两阶段训练范式，实现了高性能和高部署效率，并在多个基准测试和真实世界评估中达到最先进水平。", "motivation": "当前的VLA模型参数量大，严重依赖大规模机器人数据预训练，导致训练成本高、实时推理部署受限。此外，大多数训练范式会损害视觉-语言骨干网络的感知表征，导致过拟合和下游任务泛化能力差。", "method": "Evo-1构建于原生多模态视觉-语言模型（VLM）之上，融合了新颖的交叉调制扩散Transformer和一个优化的集成模块，形成有效的架构。它还引入了两阶段训练范式，逐步将动作与感知对齐，同时保留了VLM的表征。该模型仅有0.77亿参数，无需机器人数据预训练。", "result": "Evo-1在Meta-World和RoboTwin套件上取得了最先进的结果，分别超越现有最佳模型12.4%和6.9%，在LIBERO上达到94.8%的竞争力结果。在真实世界评估中，Evo-1以高推理频率和低内存开销实现了78%的成功率，优于所有基线方法。", "conclusion": "Evo-1证明了轻量级VLA模型可以在不进行机器人数据预训练的情况下，实现强大的性能和高效的部署。这为未来研究轻量级高效VLA模型提供了有前景的方向。"}}
{"id": "2511.04079", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04079", "abs": "https://arxiv.org/abs/2511.04079", "authors": ["Eva Prakash", "Maayane Attias", "Pierre Chambon", "Justin Xu", "Steven Truong", "Jean-Benoit Delbrouck", "Tessa Cook", "Curtis Langlotz"], "title": "Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods", "comment": "In submission to JAMIA", "summary": "Objective: To enhance automated de-identification of radiology reports by\nscaling transformer-based models through extensive training datasets and\nbenchmarking performance against commercial cloud vendor systems for protected\nhealth information (PHI) detection. Materials and Methods: In this\nretrospective study, we built upon a state-of-the-art, transformer-based, PHI\nde-identification pipeline by fine-tuning on two large annotated radiology\ncorpora from Stanford University, encompassing chest X-ray, chest CT,\nabdomen/pelvis CT, and brain MR reports and introducing an additional PHI\ncategory (AGE) into the architecture. Model performance was evaluated on test\nsets from Stanford and the University of Pennsylvania (Penn) for token-level\nPHI detection. We further assessed (1) the stability of synthetic PHI\ngeneration using a \"hide-in-plain-sight\" method and (2) performance against\ncommercial systems. Precision, recall, and F1 scores were computed across all\nPHI categories. Results: Our model achieved overall F1 scores of 0.973 on the\nPenn dataset and 0.996 on the Stanford dataset, outperforming or maintaining\nthe previous state-of-the-art model performance. Synthetic PHI evaluation\nshowed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50\nindependently de-identified Penn datasets. Our model outperformed all vendor\nsystems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).\nDiscussion: Large-scale, multimodal training improved cross-institutional\ngeneralization and robustness. Synthetic PHI generation preserved data utility\nwhile ensuring privacy. Conclusion: A transformer-based de-identification model\ntrained on diverse radiology datasets outperforms prior academic and commercial\nsystems in PHI detection and establishes a new benchmark for secure clinical\ntext processing.", "AI": {"tldr": "本研究通过大规模多模态训练，显著提升了基于Transformer的放射学报告去识别模型性能，并在PHI检测方面超越了现有学术和商业系统，建立了新的基准。", "motivation": "旨在通过扩展训练数据集和基准测试商业云系统，增强放射学报告的自动化去识别能力，以更好地检测受保护健康信息（PHI）。", "method": "本回顾性研究在现有基于Transformer的PHI去识别管道基础上，利用斯坦福大学的两个大型放射学报告标注语料库（涵盖胸部X光、胸部CT、腹部/盆腔CT和脑部MR报告）进行微调，并引入了额外的PHI类别（AGE）。模型性能在斯坦福和宾夕法尼亚大学的测试集上进行评估，并进一步评估了“隐匿”方法合成PHI的稳定性以及与商业系统的性能对比。计算了所有PHI类别的精确率、召回率和F1分数。", "result": "模型在宾夕法尼亚大学数据集上实现了0.973的总F1分数，在斯坦福数据集上实现了0.996的总F1分数，超越或保持了先前最先进模型的性能。合成PHI评估显示，在50个独立去识别的宾夕法尼亚大学数据集上，检测一致性良好（总F1：0.959 [0.958-0.960]）。模型在合成宾夕法尼亚大学报告上超越了所有商业系统（总F1：0.960 vs. 0.632-0.754）。", "conclusion": "基于Transformer且在多样化放射学数据集上训练的去识别模型在PHI检测方面优于先前的学术和商业系统，并为安全的临床文本处理建立了新的基准。"}}
{"id": "2511.04307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04307", "abs": "https://arxiv.org/abs/2511.04307", "authors": ["Jian Mu", "Chaoyun Zhang", "Chiming Ni", "Lu Wang", "Bo Qiao", "Kartik Mathur", "Qianhui Wu", "Yuhang Xie", "Xiaojun Ma", "Mengyu Zhou", "Si Qin", "Liqun Li", "Yu Kang", "Minghua Ma", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents", "comment": null, "summary": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360$^\\circ$ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.", "AI": {"tldr": "GUI-360°是一个大规模、综合的数据集和基准套件，旨在解决计算机使用代理（CUA）在真实任务稀缺、多模态轨迹自动化收集和标注缺失、以及统一基准评估不足的问题。它通过LLM增强的自动化流程构建，包含120万+动作步骤，支持GUI定位、屏幕解析和动作预测，并揭示现有模型与人类水平的差距。", "motivation": "现有计算机使用代理（CUA）面临三大挑战：1) 缺乏真实世界的CUA任务；2) 缺乏多模态轨迹的自动化收集和标注流程；3) 缺乏一个能联合评估GUI定位、屏幕解析和动作预测的统一基准。", "method": "引入GUI-360°数据集，通过LLM增强的、大部分自动化的流程来解决上述问题，包括查询源获取、环境模板构建、任务实例化、批量执行和LLM驱动的质量过滤。数据集包含流行的Windows办公应用中的数千条轨迹，超过120万个执行动作步骤，包括全分辨率截图、可访问性元数据、实例化目标、中间推理轨迹以及成功和失败的动作轨迹。它支持GUI定位、屏幕解析和动作预测三个典型任务，以及混合GUI+API动作空间。", "result": "在GUI-360°上对最先进的视觉-语言模型进行基准测试显示，在定位和动作预测方面存在显著的开箱即用不足。尽管监督微调和强化学习带来了显著改进，但仍未达到人类水平的可靠性。", "conclusion": "GUI-360°数据集及其配套代码的发布，旨在促进可复现研究，加速健壮桌面CUA的进展。数据集揭示了当前AI模型在复杂GUI任务上的局限性，并为未来研究设定了方向。"}}
{"id": "2511.04370", "categories": ["eess.SY", "cs.FL", "cs.SE", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04370", "abs": "https://arxiv.org/abs/2511.04370", "authors": ["Dennis Hendriks", "Michel Reniers", "Wan Fokkink", "Wytse Oortwijn"], "title": "Overview and Performance Evaluation of Supervisory Controller Synthesis with Eclipse ESCET v4.0", "comment": null, "summary": "Supervisory controllers control cyber-physical systems to ensure their\ncorrect and safe operation. Synthesis-based engineering (SBE) is an approach to\nlargely automate their design and implementation. SBE combines model-based\nengineering with computer-aided design, allowing engineers to focus on 'what'\nthe system should do (the requirements) rather than 'how' it should do it\n(design and implementation). In the Eclipse Supervisory Control Engineering\nToolkit (ESCET) open-source project, a community of users, researchers, and\ntool vendors jointly develop a toolkit to support the entire SBE process,\nparticularly through the CIF modeling language and tools. In this paper, we\nfirst provide a description of CIF's symbolic supervisory controller synthesis\nalgorithm, and thereby include aspects that are often omitted in the\nliterature, but are of great practical relevance, such as the prevention of\nruntime errors, handling different types of requirements, and supporting input\nvariables (to connect to external inputs). Secondly, we introduce and describe\nCIF's benchmark models, a collection of 23 freely available industrial and\nacademic models of various sizes and complexities. Thirdly, we describe recent\nimprovements between ESCET versions v0.8 (December 2022) and v4.0 (June 2024)\nthat affect synthesis performance, evaluate them on our benchmark models, and\nshow the current practical synthesis performance of CIF. Fourthly, we briefly\nlook at multi-level synthesis, a non-monolithic synthesis approach, evaluate\nits gains, and show that while it can help to further improve synthesis\nperformance, further performance improvements are still needed to synthesize\ncomplex models.", "AI": {"tldr": "本文描述了CIF的符号监督控制器综合算法，介绍了其基准模型集，评估了ESCET工具包中从v0.8到v4.0的综合性能改进，并探讨了多级综合方法及其对复杂模型性能的潜在提升。", "motivation": "为确保信息物理系统（CPS）的正确和安全运行，需要监督控制器。综合工程（SBE）旨在自动化其设计和实现，使工程师能专注于系统“应该做什么”（需求）而非“如何实现”（设计和实现）。本文旨在详细阐述CIF语言及其在ESCET项目中的工具如何支持SBE，并展示其在实际应用中的性能。", "method": "1. 详细描述了CIF的符号监督控制器综合算法，包括运行时错误预防、不同类型需求处理和输入变量支持等实用方面。 2. 介绍了CIF的基准模型，这是一个包含23个不同规模和复杂度的工业和学术模型的集合。 3. 描述并评估了ESCET版本v0.8至v4.0之间影响综合性能的改进，并使用基准模型展示了当前的实际综合性能。 4. 简要探讨了多级综合（一种非单一综合方法），评估了其性能提升。", "result": "1. 提供了CIF综合算法的全面描述，涵盖了文献中常被忽略的实用细节。 2. 提供了23个可自由获取的工业和学术基准模型。 3. 展示了ESCET在版本更新后，CIF的综合性能得到了显著提升，并给出了当前的实际性能数据。 4. 多级综合能够进一步提高综合性能，但对于复杂模型而言，仍需进一步的性能改进。", "conclusion": "CIF的符号监督控制器综合算法结合ESCET工具包，通过SBE方法为信息物理系统提供了强大的支持，并在实际应用中展现了良好的综合性能。尽管多级综合有助于提升性能，但对于处理更复杂的模型，仍需持续进行性能优化。"}}
{"id": "2511.04437", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04437", "abs": "https://arxiv.org/abs/2511.04437", "authors": ["Patrik Valábek", "Michaela Horváthová", "Martin Klaučo"], "title": "Deep Koopman Economic Model Predictive Control of a Pasteurisation Unit", "comment": null, "summary": "This paper presents a deep Koopman-based Economic Model Predictive Control\n(EMPC) for efficient operation of a laboratory-scale pasteurization unit (PU).\nThe method uses Koopman operator theory to transform the complex, nonlinear\nsystem dynamics into a linear representation, enabling the application of\nconvex optimization while representing the complex PU accurately. The deep\nKoopman model utilizes neural networks to learn the linear dynamics from\nexperimental data, achieving a 45% improvement in open-loop prediction accuracy\nover conventional N4SID subspace identification. Both analyzed models were\nemployed in the EMPC formulation that includes interpretable economic costs,\nsuch as energy consumption, material losses due to inadequate pasteurization,\nand actuator wear. The feasibility of EMPC is ensured using slack variables.\nThe deep Koopman EMPC and N4SID EMPC are numerically validated on a nonlinear\nmodel of multivariable PU under external disturbance. The disturbances include\nfeed pump fail-to-close scenario and the introduction of a cold batch to be\npastuerized. These results demonstrate that the deep Koopmand EMPC achieves a\n32% reduction in total economic cost compared to the N4SID baseline. This\nimprovement is mainly due to the reductions in material losses and energy\nconsumption. Furthermore, the steady-state operation via Koopman-based EMPC\nrequires 10.2% less electrical energy. The results highlight the practical\nadvantages of integrating deep Koopman representations with economic\noptimization to achieve resource-efficient control of thermal-intensive plants.", "AI": {"tldr": "本文提出了一种基于深度Koopman算子的经济模型预测控制（EMPC）方法，用于巴氏杀菌装置的有效运行。该方法通过将复杂非线性系统线性化，显著提高了预测精度，并在经济成本（如能源消耗和物料损失）方面比传统方法降低了32%。", "motivation": "动机在于高效运行实验室规模的巴氏杀菌装置等复杂非线性系统，并降低其运行中的经济成本，包括能源消耗、因杀菌不充分造成的物料损失以及执行器磨损。", "method": "该研究采用深度Koopman算子理论，通过神经网络从实验数据中学习，将复杂的非线性系统动力学转化为线性表示。这种线性模型被集成到经济模型预测控制（EMPC）框架中，该框架考虑了能源消耗、物料损失和执行器磨损等经济成本，并使用松弛变量确保可行性。研究将深度Koopman EMPC与传统的N4SID EMPC进行了比较，并在存在外部扰动（如进料泵故障和冷批次引入）的非线性多变量巴氏杀菌装置模型上进行了数值验证。", "result": "结果显示，深度Koopman模型在开环预测精度上比传统的N4SID子空间辨识提高了45%。在EMPC应用中，深度Koopman EMPC相比N4SID基线，总经济成本降低了32%，主要得益于物料损失和能源消耗的减少。此外，通过基于Koopman的EMPC实现的稳态运行所需的电能减少了10.2%。", "conclusion": "研究表明，将深度Koopman表示与经济优化相结合，为热密集型工厂实现资源高效控制提供了实际优势。"}}
{"id": "2511.04664", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04664", "abs": "https://arxiv.org/abs/2511.04664", "authors": ["Phat Nguyen", "Erfan Aasi", "Shiva Sreeram", "Guy Rosman", "Andrew Silva", "Sertac Karaman", "Daniela Rus"], "title": "SAFe-Copilot: Unified Shared Autonomy Framework", "comment": null, "summary": "Autonomous driving systems remain brittle in rare, ambiguous, and\nout-of-distribution scenarios, where human driver succeed through contextual\nreasoning. Shared autonomy has emerged as a promising approach to mitigate such\nfailures by incorporating human input when autonomy is uncertain. However, most\nexisting methods restrict arbitration to low-level trajectories, which\nrepresent only geometric paths and therefore fail to preserve the underlying\ndriving intent. We propose a unified shared autonomy framework that integrates\nhuman input and autonomous planners at a higher level of abstraction. Our\nmethod leverages Vision Language Models (VLMs) to infer driver intent from\nmulti-modal cues -- such as driver actions and environmental context -- and to\nsynthesize coherent strategies that mediate between human and autonomous\ncontrol. We first study the framework in a mock-human setting, where it\nachieves perfect recall alongside high accuracy and precision. A human-subject\nsurvey further shows strong alignment, with participants agreeing with\narbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive\nbenchmark demonstrates a substantial reduction in collision rate and\nimprovement in overall performance compared to pure autonomy. Arbitration at\nthe level of semantic, language-based representations emerges as a design\nprinciple for shared autonomy, enabling systems to exercise common-sense\nreasoning and maintain continuity with human intent.", "AI": {"tldr": "本文提出一个统一的共享自主驾驶框架，利用视觉语言模型（VLMs）从多模态线索推断驾驶员意图，并在更高抽象层次上整合人机控制，以提高自动驾驶在复杂场景下的鲁棒性和安全性。", "motivation": "自动驾驶系统在罕见、模糊和分布外场景中表现脆弱，而人类驾驶员能通过情境推理成功应对。现有共享自主方法将仲裁限制在低级轨迹，未能保留潜在的驾驶意图。", "method": "提出一个统一的共享自主框架，在更高抽象层次上整合人类输入和自主规划器。该方法利用视觉语言模型（VLMs）从驾驶员行为和环境上下文等多模态线索推断驾驶员意图，并合成协调人机控制的连贯策略。", "result": "在模拟人类设置中，该框架实现了完美的召回率以及高准确率和精确度。一项人类受试者调查显示，参与者在92%的案例中同意仲裁结果，表明高度一致性。在Bench2Drive基准测试中，与纯自主驾驶相比，碰撞率显著降低，整体性能得到改善。", "conclusion": "基于语义、语言表示的仲裁成为共享自主驾驶的设计原则，使系统能够进行常识推理并保持与人类意图的连续性。"}}
{"id": "2511.04037", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.04037", "abs": "https://arxiv.org/abs/2511.04037", "authors": ["Arfina Rahman", "Mahesh Banavar"], "title": "A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals", "comment": "This work has been submitted to IEEE Transactions on Biometrics,\n  Behavior, and Identity Science (TBIOM) for possible publication", "summary": "Photoplethysmography (PPG) signals, which measure changes in blood volume in\nthe skin using light, have recently gained attention in biometric\nauthentication because of their non-invasive acquisition, inherent liveness\ndetection, and suitability for low-cost wearable devices. However, PPG signal\nquality is challenged by motion artifacts, illumination changes, and\ninter-subject physiological variability, making robust feature extraction and\nclassification crucial. This study proposes a lightweight and cost-effective\nbiometric authentication framework based on PPG signals extracted from\nlow-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings\nfrom 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The\nraw PPG signals undergo a standard preprocessing pipeline involving baseline\ndrift removal, motion artifact suppression using Principal Component Analysis\n(PCA), bandpass filtering, Fourier-based resampling, and amplitude\nnormalization. To generate robust representations, each one-dimensional PPG\nsegment is converted into a two-dimensional time-frequency scalogram via the\nContinuous Wavelet Transform (CWT), effectively capturing transient\ncardiovascular dynamics. We developed a hybrid deep learning model, termed\nCVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision\nTransformer (CVT) and ConvMixer branches with temporal features from a Long\nShort-Term Memory network (LSTM). The experimental results on 46 subjects\ndemonstrate an authentication accuracy of 98%, validating the robustness of the\nmodel to noise and variability between subjects. Due to its efficiency,\nscalability, and inherent liveness detection capability, the proposed system is\nwell-suited for real-world mobile and embedded biometric security applications.", "AI": {"tldr": "本研究提出了一种基于低帧率指尖视频PPG信号的轻量级生物识别认证框架，通过结合CWT转换的2D时频图和混合深度学习模型CVT-ConvMixer-LSTM，实现了98%的认证准确率，适用于移动和嵌入式安全应用。", "motivation": "PPG信号因其无创、活体检测和低成本可穿戴设备的适用性，在生物识别认证中受到关注。然而，PPG信号质量易受运动伪影、光照变化和个体生理差异的影响，因此需要鲁棒的特征提取和分类方法。", "method": "该研究利用CFIHSR数据集（46名受试者，14 Hz采样率的PPG记录）。原始PPG信号经过基线漂移去除、基于PCA的运动伪影抑制、带通滤波、傅里叶重采样和幅度归一化等预处理。为生成鲁棒表示，将一维PPG段通过连续小波变换（CWT）转换为二维时频标量图。开发了一个混合深度学习模型CVT-ConvMixer-LSTM，结合了卷积视觉Transformer（CVT）和ConvMixer分支的空间特征与长短期记忆网络（LSTM）的时间特征。", "result": "在46名受试者上的实验结果表明，该认证框架实现了98%的认证准确率。该模型对噪声和受试者间的变异性表现出鲁棒性。", "conclusion": "所提出的系统因其高效、可扩展性以及固有的活体检测能力，非常适用于实际的移动和嵌入式生物识别安全应用。"}}
{"id": "2511.04451", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04451", "abs": "https://arxiv.org/abs/2511.04451", "authors": ["Patrik Valábek", "Marek Wadinger", "Michal Kvasnica", "Martin Klaučo"], "title": "Deep Dictionary-Free Method for Identifying Linear Model of Nonlinear System with Input Delay", "comment": null, "summary": "Nonlinear dynamical systems with input delays pose significant challenges for\nprediction, estimation, and control due to their inherent complexity and the\nimpact of delays on system behavior. Traditional linear control techniques\noften fail in these contexts, necessitating innovative approaches. This paper\nintroduces a novel approach to approximate the Koopman operator using an\nLSTM-enhanced Deep Koopman model, enabling linear representations of nonlinear\nsystems with time delays. By incorporating Long Short-Term Memory (LSTM)\nlayers, the proposed framework captures historical dependencies and efficiently\nencodes time-delayed system dynamics into a latent space. Unlike traditional\nextended Dynamic Mode Decomposition (eDMD) approaches that rely on predefined\ndictionaries, the LSTM-enhanced Deep Koopman model is dictionary-free, which\nmitigates the problems with the underlying dynamics being known and\nincorporated into the dictionary. Quantitative comparisons with extended eDMD\non a simulated system demonstrate highly significant performance gains in\nprediction accuracy in cases where the true nonlinear dynamics are unknown and\nachieve comparable results to eDMD with known dynamics of a system.", "AI": {"tldr": "本文提出了一种结合LSTM的深度Koopman模型，用于近似具有输入延迟的非线性动力学系统的Koopman算子，从而实现对延迟系统动力学的线性表示，并在预测精度上表现出色。", "motivation": "具有输入延迟的非线性动力学系统在预测、估计和控制方面面临巨大挑战，传统线性控制技术往往失效。现有扩展动态模态分解（eDMD）方法依赖预定义字典，限制了其在未知动力学系统中的应用。", "method": "引入了一种LSTM增强的深度Koopman模型来近似Koopman算子。通过结合LSTM层，该模型能够捕获历史依赖性，并将时间延迟的系统动力学有效编码到潜在空间中。与传统的eDMD不同，该方法是无字典的。", "result": "在模拟系统上的定量比较表明，当真实非线性动力学未知时，LSTM增强的深度Koopman模型在预测精度上比eDMD有显著提高；当系统动力学已知时，其性能与eDMD相当。", "conclusion": "LSTM增强的深度Koopman模型为处理带有输入延迟的非线性动力学系统提供了一种创新且有效的解决方案，尤其在系统动力学未知的情况下，展现出优越的预测能力。"}}
{"id": "2511.04665", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04665", "abs": "https://arxiv.org/abs/2511.04665", "authors": ["Kaifeng Zhang", "Shuo Sha", "Hanxiao Jiang", "Matthew Loper", "Hyunjong Song", "Guangyan Cai", "Zhuo Xu", "Xiaochen Hu", "Changxi Zheng", "Yunzhu Li"], "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions", "comment": "Website: https://real2sim-eval.github.io/", "summary": "Robotic manipulation policies are advancing rapidly, but their direct\nevaluation in the real world remains costly, time-consuming, and difficult to\nreproduce, particularly for tasks involving deformable objects. Simulation\nprovides a scalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of soft-body\ninteractions. We present a real-to-sim policy evaluation framework that\nconstructs soft-body digital twins from real-world videos and renders robots,\nobjects, and environments with photorealistic fidelity using 3D Gaussian\nSplatting. We validate our approach on representative deformable manipulation\ntasks, including plush toy packing, rope routing, and T-block pushing,\ndemonstrating that simulated rollouts correlate strongly with real-world\nexecution performance and reveal key behavioral patterns of learned policies.\nOur results suggest that combining physics-informed reconstruction with\nhigh-quality rendering enables reproducible, scalable, and accurate evaluation\nof robotic manipulation policies. Website: https://real2sim-eval.github.io/", "AI": {"tldr": "本文提出一个实时到模拟的策略评估框架，通过从真实视频构建软体数字孪生，并使用3D高斯泼溅实现逼真渲染，从而实现可变形物体操作策略的可复现、可扩展和准确评估。", "motivation": "机器人操作策略的真实世界评估成本高、耗时且难以复现，尤其对于涉及可变形物体的任务。现有模拟器未能捕捉软体交互的视觉和物理复杂性。", "method": "该研究提出了一个实时到模拟的策略评估框架，通过从真实世界视频构建软体数字孪生，并使用3D高斯泼溅技术以逼真的保真度渲染机器人、物体和环境。", "result": "该方法在毛绒玩具打包、绳索布线和T型块推动等代表性可变形操作任务上进行了验证。结果表明，模拟执行与真实世界执行性能高度相关，并揭示了学习策略的关键行为模式。", "conclusion": "结合物理信息重建和高质量渲染，可以实现机器人操作策略的可复现、可扩展和准确评估。"}}
{"id": "2511.04103", "categories": ["cs.CL", "cs.AI", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04103", "abs": "https://arxiv.org/abs/2511.04103", "authors": ["Moses Charikar", "Chirag Pabbaraju", "Ambuj Tewari"], "title": "A Characterization of List Language Identification in the Limit", "comment": null, "summary": "We study the problem of language identification in the limit, where given a\nsequence of examples from a target language, the goal of the learner is to\noutput a sequence of guesses for the target language such that all the guesses\nbeyond some finite time are correct. Classical results of Gold showed that\nlanguage identification in the limit is impossible for essentially any\ninteresting collection of languages. Later, Angluin gave a precise\ncharacterization of language collections for which this task is possible.\nMotivated by recent positive results for the related problem of language\ngeneration, we revisit the classic language identification problem in the\nsetting where the learner is given the additional power of producing a list of\n$k$ guesses at each time step. The goal is to ensure that beyond some finite\ntime, one of the guesses is correct at each time step.\n  We give an exact characterization of collections of languages that can be\n$k$-list identified in the limit, based on a recursive version of Angluin's\ncharacterization (for language identification with a list of size $1$). This\nfurther leads to a conceptually appealing characterization: A language\ncollection can be $k$-list identified in the limit if and only if the\ncollection can be decomposed into $k$ collections of languages, each of which\ncan be identified in the limit (with a list of size $1$). We also use our\ncharacterization to establish rates for list identification in the statistical\nsetting where the input is drawn as an i.i.d. stream from a distribution\nsupported on some language in the collection. Our results show that if a\ncollection is $k$-list identifiable in the limit, then the collection can be\n$k$-list identified at an exponential rate, and this is best possible. On the\nother hand, if a collection is not $k$-list identifiable in the limit, then it\ncannot be $k$-list identified at any rate that goes to zero.", "AI": {"tldr": "本文重新审视了极限语言识别问题，引入了每次猜测k个列表的机制，并给出了可k-列表识别的语言集合的精确刻画，发现其等价于将集合分解为k个可识别的子集合。在统计设置下，可k-列表识别的集合能以指数速率识别。", "motivation": "Gold的经典结果表明，几乎所有有趣的语言集合都无法在极限中进行语言识别。Angluin后来给出了可识别语言集合的精确特征。受最近语言生成相关问题积极结果的启发，本文在学习器可以输出k个猜测列表的设定下，重新审视了经典的语言识别问题。", "method": "本文基于Angluin特征的递归版本，给出了可在极限中进行k-列表识别的语言集合的精确特征。在此基础上，进一步推导出一个概念上吸引人的特征：一个语言集合可在极限中进行k-列表识别，当且仅当该集合可以分解为k个语言集合，每个子集合都可以在极限中被识别（列表大小为1）。此外，本文还将此特征应用于统计设置，其中输入是来自集合中某种语言的独立同分布流。", "result": "研究结果表明，一个语言集合可在极限中进行k-列表识别，当且仅当该集合可以分解为k个可在极限中识别的语言集合。在统计设置下，如果一个集合可在极限中进行k-列表识别，那么它能以指数速率进行k-列表识别，且这是最优的。反之，如果一个集合不能在极限中进行k-列表识别，那么它不能以任何趋于零的速率进行k-列表识别。", "conclusion": "本文为可在极限中进行k-列表识别的语言集合提供了精确的特征，并揭示了其与将集合分解为k个可识别子集合的等价性。同时，在统计设置下，证明了可k-列表识别的集合能够以最优的指数速率进行识别，而不可识别的集合则无法以任何趋零的速率识别。"}}
{"id": "2511.04316", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04316", "abs": "https://arxiv.org/abs/2511.04316", "authors": ["Tim Beyer", "Jonas Dornbusch", "Jakob Steimle", "Moritz Ladenburger", "Leo Schwinn", "Stephan Günnemann"], "title": "AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research", "comment": null, "summary": "The rapid expansion of research on Large Language Model (LLM) safety and\nrobustness has produced a fragmented and oftentimes buggy ecosystem of\nimplementations, datasets, and evaluation methods. This fragmentation makes\nreproducibility and comparability across studies challenging, hindering\nmeaningful progress. To address these issues, we introduce AdversariaLLM, a\ntoolbox for conducting LLM jailbreak robustness research. Its design centers on\nreproducibility, correctness, and extensibility. The framework implements\ntwelve adversarial attack algorithms, integrates seven benchmark datasets\nspanning harmfulness, over-refusal, and utility evaluation, and provides access\nto a wide range of open-weight LLMs via Hugging Face. The implementation\nincludes advanced features for comparability and reproducibility such as\ncompute-resource tracking, deterministic results, and distributional evaluation\ntechniques. \\name also integrates judging through the companion package\nJudgeZoo, which can also be used independently. Together, these components aim\nto establish a robust foundation for transparent, comparable, and reproducible\nresearch in LLM safety.", "AI": {"tldr": "AdversariaLLM是一个用于大型语言模型(LLM)越狱鲁棒性研究的工具箱，旨在解决当前研究生态系统的碎片化问题，提高研究的可复现性和可比较性。", "motivation": "LLM安全和鲁棒性研究的快速发展导致了实现、数据集和评估方法的高度碎片化和错误频发，这使得研究的可复现性和可比较性面临挑战，阻碍了有意义的进展。", "method": "本文介绍了AdversariaLLM工具箱，其设计核心是可复现性、正确性和可扩展性。它实现了12种对抗性攻击算法，集成了7个涵盖有害性、过度拒绝和效用评估的基准数据集，并通过Hugging Face提供对各种开源LLM的访问。该实现还包括计算资源跟踪、确定性结果和分布式评估技术等高级功能，以增强可比较性和可复现性。此外，它还通过配套包JudgeZoo集成了判决功能。", "result": "AdversariaLLM提供了一个全面的框架，用于进行LLM越狱鲁棒性研究。它集成了多种攻击算法、基准数据集和LLM访问，并具备计算资源跟踪、确定性结果和分布式评估等高级功能，确保了研究的可复现性、正确性和可扩展性。", "conclusion": "AdversariaLLM及其配套包旨在为LLM安全领域的透明、可比较和可复现研究建立一个坚实的基础。"}}
{"id": "2511.04108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04108", "abs": "https://arxiv.org/abs/2511.04108", "authors": ["Wenmo Qiu", "Saurabh Srivastava"], "title": "Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models", "comment": null, "summary": "Recent work has explored batch prompting as a strategy to amortize inference\ncost in large language models (LLMs). In this paper, we show that batching\noffers an additional, underappreciated benefit: it regularizes model behavior\nduring multi-step reasoning for Large Reasoning Models (LRMs). We conduct a\ncomprehensive study across 13 diverse benchmarks and observe that batching\nimproves accuracy while substantially reducing reasoning token usage, often by\n3x-5x. Through detailed behavioral analysis, we find that batching suppresses\noverthinking, reduces hedging language (e.g., repetitive self-corrections), and\nencourages more decisive answers. Surprisingly, we also observe emergent\ncollective effects in batched inference: models often generalize patterns from\nearlier examples to solve harder ones in the same batch. These findings\nposition batching not just as a throughput optimization, but as a powerful\ninference-time regularizer for more efficient and reliable LLM reasoning.", "AI": {"tldr": "本文发现，对大型语言模型（LLMs）进行批处理不仅能分摊推理成本，还能在多步推理中作为正则化器，显著提高推理准确性并减少token使用，同时抑制过度思考和犹豫语言。", "motivation": "以往研究主要关注批处理在LLMs中分摊推理成本的优势。本文旨在探索批处理的另一个被低估的益处：在多步推理中正则化模型行为。", "method": "研究在13个不同基准测试上进行了全面研究，并进行了详细的行为分析。", "result": "批处理提高了推理准确性，同时大幅减少了推理token使用（通常为3-5倍）。行为分析发现，批处理抑制了过度思考，减少了犹豫语言（如重复的自我修正），并鼓励更果断的回答。此外，还观察到批处理推理中出现了集体效应：模型能够从批次中较早的例子中泛化模式，以解决批次中较难的问题。", "conclusion": "批处理不仅是一种吞吐量优化策略，更是一种强大的推理时正则化器，能够使LLM的推理更高效、更可靠。"}}
{"id": "2511.04312", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04312", "abs": "https://arxiv.org/abs/2511.04312", "authors": ["Jacob Lysnæs-Larsen", "Marte Eggen", "Inga Strümke"], "title": "Probing the Probes: Methods and Metrics for Concept Alignment", "comment": "29 pages, 17 figures", "summary": "In explainable AI, Concept Activation Vectors (CAVs) are typically obtained\nby training linear classifier probes to detect human-understandable concepts as\ndirections in the activation space of deep neural networks. It is widely\nassumed that a high probe accuracy indicates a CAV faithfully representing its\ntarget concept. However, we show that the probe's classification accuracy alone\nis an unreliable measure of concept alignment, i.e., the degree to which a CAV\ncaptures the intended concept. In fact, we argue that probes are more likely to\ncapture spurious correlations than they are to represent only the intended\nconcept. As part of our analysis, we demonstrate that deliberately misaligned\nprobes constructed to exploit spurious correlations, achieve an accuracy close\nto that of standard probes. To address this severe problem, we introduce a\nnovel concept localization method based on spatial linear attribution, and\nprovide a comprehensive comparison of it to existing feature visualization\ntechniques for detecting and mitigating concept misalignment. We further\npropose three classes of metrics for quantitatively assessing concept\nalignment: hard accuracy, segmentation scores, and augmentation robustness. Our\nanalysis shows that probes with translation invariance and spatial alignment\nconsistently increase concept alignment. These findings highlight the need for\nalignment-based evaluation metrics rather than probe accuracy, and the\nimportance of tailoring probes to both the model architecture and the nature of\nthe target concept.", "AI": {"tldr": "本文指出CAV探针分类精度不足以衡量概念对齐度，探针易捕捉虚假相关性。为此，我们提出一种新的概念定位方法和三类对齐度评估指标，并强调需基于对齐度而非精度进行评估。", "motivation": "在可解释AI中，人们普遍认为高探针精度意味着CAV忠实地代表其目标概念。然而，本文发现探针的分类精度本身并不可靠，探针更有可能捕获虚假相关性而非仅代表预期概念，导致概念对齐度不佳。", "method": "本文通过构建故意错位的探针（利用虚假相关性）来证明其仍能达到接近标准探针的精度。为解决此问题，我们引入了一种基于空间线性归因的新型概念定位方法，并将其与现有特征可视化技术进行比较。此外，我们提出了三类用于定量评估概念对齐度的新指标：硬精度、分割分数和增强鲁棒性。", "result": "研究表明，故意错位的探针可以达到与标准探针接近的精度。分析发现，具有平移不变性和空间对齐的探针能持续提高概念对齐度。这些发现强调了需要基于对齐度而非探针精度来评估CAV。", "conclusion": "探针精度不足以评估CAV的概念对齐度，应采用基于对齐度的评估指标。同时，根据模型架构和目标概念的性质来定制探针至关重要。"}}
{"id": "2511.04078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04078", "abs": "https://arxiv.org/abs/2511.04078", "authors": ["Zehui Feng", "Chenqi Zhang", "Mingru Wang", "Minuo Wei", "Shiwei Cheng", "Cuntai Guan", "Ting Han"], "title": "Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment", "comment": "30 pages, 16 figures, under review as a conference paper", "summary": "Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI\nremains a fundamental challenge due to subject variability and the entangled\nnature of visual features. Existing approaches primarily align neural activity\ndirectly with visual embeddings, but visual-only representations often fail to\ncapture latent semantic dimensions, limiting interpretability and deep\nrobustness. To address these limitations, we propose Bratrix, the first\nend-to-end framework to achieve multimodal Language-Anchored Vision-Brain\nalignment. Bratrix decouples visual stimuli into hierarchical visual and\nlinguistic semantic components, and projects both visual and brain\nrepresentations into a shared latent space, enabling the formation of aligned\nvisual-language and brain-language embeddings. To emulate human-like perceptual\nreliability and handle noisy neural signals, Bratrix incorporates a novel\nuncertainty perception module that applies uncertainty-aware weighting during\nalignment. By leveraging learnable language-anchored semantic matrices to\nenhance cross-modal correlations and employing a two-stage training strategy of\nsingle-modality pretraining followed by multimodal fine-tuning, Bratrix-M\nimproves alignment precision. Extensive experiments on EEG, MEG, and fMRI\nbenchmarks demonstrate that Bratrix improves retrieval, reconstruction, and\ncaptioning performance compared to state-of-the-art methods, specifically\nsurpassing 14.3% in 200-way EEG retrieval task. Code and model are available.", "AI": {"tldr": "Bratrix是一种端到端的多模态语言锚定视觉-大脑对齐框架。它通过解耦视觉和语言语义、投影到共享潜在空间、引入不确定性感模块以及两阶段训练策略，显著提升了从EEG、MEG和fMRI信号中进行检索、重建和图像描述的性能。", "motivation": "从EEG、MEG和fMRI等神经信号中揭示视觉语义面临主体变异性和视觉特征纠缠的挑战。现有方法主要将神经活动直接与视觉嵌入对齐，但纯视觉表示无法捕捉潜在的语义维度，限制了解释性和深度鲁棒性。", "method": "Bratrix是首个实现多模态语言锚定视觉-大脑对齐的端到端框架。它将视觉刺激解耦为分层的视觉和语言语义组件，并将视觉和大脑表征投影到共享潜在空间，形成对齐的视觉-语言和大脑-语言嵌入。为模拟人类感知可靠性并处理噪声神经信号，Bratrix整合了一个新颖的不确定性感模块，在对齐过程中应用不确定性感知加权。此外，它利用可学习的语言锚定语义矩阵增强跨模态相关性，并采用单模态预训练后进行多模态微调的两阶段训练策略。", "result": "Bratrix在EEG、MEG和fMRI基准测试中，相较于现有最先进方法，显著提升了检索、重建和图像描述的性能。特别是在200路EEG检索任务中，性能提升超过14.3%。", "conclusion": "Bratrix通过其创新的多模态语言锚定视觉-大脑对齐方法，有效克服了从神经信号中揭示视觉语义的现有挑战。其结合语义解耦、共享潜在空间、不确定性感知和两阶段训练的策略，在多个任务中展现出卓越的性能，为理解视觉神经编码提供了更深层次的洞察。"}}
{"id": "2511.04461", "categories": ["eess.SY", "cs.CE", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04461", "abs": "https://arxiv.org/abs/2511.04461", "authors": ["Giorgio Palma", "Andrea Serani", "Matteo Diez"], "title": "Data-driven uncertainty-aware seakeeping prediction of the Delft 372 catamaran using ensemble Hankel dynamic mode decomposition", "comment": null, "summary": "In this study, we present and validate an ensemble-based Hankel Dynamic Mode\nDecomposition with control (HDMDc) for uncertainty-aware seakeeping predictions\nof a high-speed catamaran, namely the Delft 372 model. Experimental\nmeasurements (time histories) of wave elevation at the longitudinal center of\ngravity, heave, pitch, notional flight-deck velocity, notional bridge\nacceleration, and total resistance were collected from irregular wave basin\ntests on a 1:33.3 scale replica of the Delft 372 model under sea state 5\nconditions at Fr = 0.425, and organized into training, validation, and test\nsets. The HDMDc algorithm constructs an equation-free linear reduced-order\nmodel of the seakeeping vessel by augmenting states and inputs with their\ntime-lagged copies to capture nonlinear and memory effects. Two ensembling\nstrategies, namely Bayesian HDMDc (BHDMDc), which samples hyperparameters\nconsidered stochastic variables with prior distribution to produce posterior\nmean forecasts with confidence intervals, and Frequentist HDMDc (FHDMDc), which\naggregates multiple model obtained over data subsets, are compared in providing\nseakeeping prediction and uncertainty quantification. The FHDMDc approach is\nfound to improve the accuracy of the predictions compared to the deterministic\ncounterpart, also providing robust uncertainty estimation; whereas the\napplication of BHDMDc to the present test case is not found beneficial in\ncomparison to the deterministic model. FHDMDc-derived probability density\nfunctions for the motions closely match both experimental data and URANS\nresults, demonstrating reliable and computationally efficient seakeeping\nprediction for design and operational support.", "AI": {"tldr": "本研究提出并验证了一种基于集成Hankel动态模态分解与控制（HDMDc）的方法，用于高速双体船（Delft 372模型）的适航性预测及不确定性量化。", "motivation": "需要对高速船只的适航性进行不确定性感知的预测，以更好地支持设计和操作决策。", "method": "研究使用Delft 372模型（1:33.3比例）在海况5、弗劳德数0.425条件下进行的非规则波浪水池试验数据。数据包括波浪高、垂荡、纵摇、甲板速度、桥梁加速度和总阻力。采用HDMDc算法构建无方程的线性降阶模型，通过增加带有时间滞后副本的状态和输入来捕捉非线性和记忆效应。比较了两种集成策略：贝叶斯HDMDc（BHDMDc）和频率论HDMDc（FHDMDc），用于提供适航性预测和不确定性量化。", "result": "频率论HDMDc（FHDMDc）方法在预测准确性方面优于确定性模型，并提供了稳健的不确定性估计；而贝叶斯HDMDc（BHDMDc）在此测试案例中与确定性模型相比未显示出益处。FHDMDc导出的运动概率密度函数与实验数据和URANS结果高度吻合。", "conclusion": "FHDMDc为高速双体船的适航性预测提供了一种可靠且计算高效的方法，能够有效进行不确定性量化，对设计和操作支持具有重要意义。"}}
{"id": "2511.04671", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04671", "abs": "https://arxiv.org/abs/2511.04671", "authors": ["Maximus A. Pace", "Prithwish Dan", "Chuanruo Ning", "Atiksh Bhardwaj", "Audrey Du", "Edward W. Duan", "Wei-Chiu Ma", "Kushal Kedia"], "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations", "comment": null, "summary": "Human videos can be recorded quickly and at scale, making them an appealing\nsource of training data for robot learning. However, humans and robots differ\nfundamentally in embodiment, resulting in mismatched action execution. Direct\nkinematic retargeting of human hand motion can therefore produce actions that\nare physically infeasible for robots. Despite these low-level differences,\nhuman demonstrations provide valuable motion cues about how to manipulate and\ninteract with objects. Our key idea is to exploit the forward diffusion\nprocess: as noise is added to actions, low-level execution differences fade\nwhile high-level task guidance is preserved. We present X-Diffusion, a\nprincipled framework for training diffusion policies that maximally leverages\nhuman data without learning dynamically infeasible motions. X-Diffusion first\ntrains a classifier to predict whether a noisy action is executed by a human or\nrobot. Then, a human action is incorporated into policy training only after\nadding sufficient noise such that the classifier cannot discern its embodiment.\nActions consistent with robot execution supervise fine-grained denoising at low\nnoise levels, while mismatched human actions provide only coarse guidance at\nhigher noise levels. Our experiments show that naive co-training under\nexecution mismatches degrades policy performance, while X-Diffusion\nconsistently improves it. Across five manipulation tasks, X-Diffusion achieves\na 16% higher average success rate than the best baseline. The project website\nis available at https://portal-cornell.github.io/X-Diffusion/.", "AI": {"tldr": "X-Diffusion是一个利用前向扩散过程的框架，它通过在人类动作中添加噪声来模糊人类和机器人执行差异，从而有效地利用人类视频数据进行机器人学习，同时避免学习到机器人无法执行的动作，显著提高了操纵任务的成功率。", "motivation": "人类视频是机器人学习的丰富训练数据来源，但人类和机器人身体结构（embodiment）的根本差异导致直接将人类动作映射到机器人上会产生不可行的动作。研究旨在解决如何在存在这种执行差异的情况下，有效利用人类演示数据。", "method": "核心思想是利用前向扩散过程：随着噪声的增加，低级执行差异会消失，而高级任务指导得以保留。X-Diffusion首先训练一个分类器来判断带噪声的动作是由人类还是机器人执行。然后，只有当人类动作被添加了足够的噪声，以至于分类器无法区分其来源时，才将其纳入策略训练。机器人动作用于监督低噪声水平下的精细去噪，而失配的人类动作则在高噪声水平下提供粗略指导。", "result": "实验表明，在执行不匹配的情况下进行简单协同训练会降低策略性能，而X-Diffusion持续改进了性能。在五项操纵任务中，X-Diffusion比最佳基线平均成功率高出16%。", "conclusion": "X-Diffusion提供了一个原则性的框架，通过利用扩散过程来有效整合人类数据进行机器人学习，成功克服了人类与机器人之间执行不匹配的挑战。它在不学习动态不可行动作的前提下，最大限度地利用了人类演示数据，显著提升了机器人操纵任务的成功率。"}}
{"id": "2511.04328", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04328", "abs": "https://arxiv.org/abs/2511.04328", "authors": ["Jiahao Zhao", "Luxin Xu", "Minghuan Tan", "Lichao Zhang", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang"], "title": "RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation", "comment": "To appear in BIBM2025", "summary": "Numerous medical systems powered by Large Language Models (LLMs) have\nachieved remarkable progress in diverse healthcare tasks. However, research on\ntheir medication safety remains limited due to the lack of real world datasets,\nconstrained by privacy and accessibility issues. Moreover, evaluation of LLMs\nin realistic clinical consultation settings, particularly regarding medication\nsafety, is still underexplored. To address these gaps, we propose a framework\nthat simulates and evaluates clinical consultations to systematically assess\nthe medication safety capabilities of LLMs. Within this framework, we generate\ninquiry diagnosis dialogues with embedded medication risks and construct a\ndedicated medication safety database, RxRisk DB, containing 6,725\ncontraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.\nA two-stage filtering strategy ensures clinical realism and professional\nquality, resulting in the benchmark RxSafeBench with 2,443 high-quality\nconsultation scenarios. We evaluate leading open-source and proprietary LLMs\nusing structured multiple choice questions that test their ability to recommend\nsafe medications under simulated patient contexts. Results show that current\nLLMs struggle to integrate contraindication and interaction knowledge,\nespecially when risks are implied rather than explicit. Our findings highlight\nkey challenges in ensuring medication safety in LLM-based systems and provide\ninsights into improving reliability through better prompting and task-specific\ntuning. RxSafeBench offers the first comprehensive benchmark for evaluating\nmedication safety in LLMs, advancing safer and more trustworthy AI-driven\nclinical decision support.", "AI": {"tldr": "该研究提出了一个模拟临床咨询的框架，并构建了首个全面评估大型语言模型（LLMs）药物安全能力的基准RxSafeBench。结果显示LLMs在整合禁忌症和药物相互作用知识方面存在困难，尤其是在风险隐含而非明确时。", "motivation": "尽管LLMs在医疗领域取得显著进展，但其药物安全性研究仍受限于缺乏真实世界数据（因隐私和可及性问题），且在实际临床咨询环境中，特别是药物安全方面的评估尚不充分。", "method": "研究提出了一个模拟临床咨询的框架，用于系统评估LLMs的药物安全能力。具体方法包括：生成嵌入药物风险的问诊对话；构建药物安全数据库RxRisk DB（包含6,725条禁忌症、28,781条药物相互作用和14,906对适应症-药物对）；采用两阶段过滤策略确保临床真实性和专业质量，最终创建了包含2,443个高质量咨询场景的RxSafeBench基准；通过结构化多项选择题评估了领先的开源和专有LLMs在模拟患者情境下推荐安全药物的能力。", "result": "评估结果表明，当前LLMs在整合禁忌症和药物相互作用知识方面存在困难，尤其当药物风险是隐含而非明确时。这凸显了确保基于LLM的系统药物安全性的关键挑战。", "conclusion": "研究结果揭示了LLM-based系统在药物安全方面面临的关键挑战，并为通过更好的提示工程和任务特定微调来提高可靠性提供了见解。RxSafeBench作为首个全面评估LLMs药物安全性的基准，将推动更安全、更值得信赖的AI驱动临床决策支持的发展。"}}
{"id": "2511.04083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04083", "abs": "https://arxiv.org/abs/2511.04083", "authors": ["Abu Hanif Muhammad Syarubany"], "title": "Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score", "comment": null, "summary": "We study CT image denoising in the unpaired and self-supervised regimes by\nevaluating two strong, training-data-efficient paradigms: a CycleGAN-based\nresidual translator and a Noise2Score (N2S) score-matching denoiser. Under a\ncommon evaluation protocol, a configuration sweep identifies a simple standard\nU-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =\n64) as the most reliable setting; we then train it to convergence with a longer\nschedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234\nSSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an\nunseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly\nbehind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,\nhighlighting its utility when clean pairs are unavailable. Overall, CycleGAN\noffers the strongest final image quality, whereas Noise2Score provides a robust\npair-free alternative with competitive performance. Source code is available at\nhttps://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.", "AI": {"tldr": "本文研究了在非配对和自监督CT图像去噪中，CycleGAN和Noise2Score两种方法的性能。结果显示，CycleGAN在最终图像质量上表现最佳，而Noise2Score在处理极度噪声输入时表现出鲁棒性。", "motivation": "研究CT图像去噪，尤其是在缺乏干净-噪声图像对（非配对和自监督）的情况下，寻找高效且数据利用率高的去噪范式。", "method": "评估了两种强大的、训练数据高效的范式：基于CycleGAN的残差转换器（采用U-Net骨干网络）和Noise2Score (N2S) 分数匹配去噪器。在共同评估协议下，通过配置扫描确定了CycleGAN的最佳设置，并将其训练至收敛。", "result": "经过优化的CycleGAN将噪声输入从34.66 dB / 0.9234 SSIM提升至38.913 dB / 0.971 SSIM，并取得了1.9441的估计分数和1.9343的Kaggle排行榜分数。Noise2Score在绝对PSNR/SSIM上略逊一筹，但在处理极度噪声输入时获得了显著提升。总体而言，CycleGAN提供了最强的最终图像质量。", "conclusion": "CycleGAN在最终图像质量方面表现最佳。Noise2Score提供了一种鲁棒的、无需配对的替代方案，具有竞争力的性能，尤其适用于干净图像对不可用且输入噪声极大的情况。"}}
{"id": "2511.04120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04120", "abs": "https://arxiv.org/abs/2511.04120", "authors": ["Xinyuan Li", "Murong Xu", "Wenbiao Tao", "Hanlun Zhu", "Yike Zhao", "Jipeng Zhang", "Yunshi Lan"], "title": "RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning", "comment": null, "summary": "Large language models (LLMs) achieve high performance on mathematical\nreasoning, but these results can be inflated by training data leakage or\nsuperficial pattern matching rather than genuine reasoning. To this end, an\nadversarial perturbation-based evaluation is needed to measure true\nmathematical reasoning ability. Current rule-based perturbation methods often\ngenerate ill-posed questions and impede the systematic evaluation of question\ndifficulty and the evolution of benchmarks. To bridge this gap, we propose\nRIDE, a novel adversarial question-rewriting framework that leverages Item\nResponse Theory (IRT) to rigorously measure question difficulty and to generate\nintrinsically more challenging, well-posed variations of mathematical problems.\nWe employ 35 LLMs to simulate students and build a difficulty ranker from their\nresponses. This ranker provides a reward signal during reinforcement learning\nand guides a question-rewriting model to reformulate existing questions across\ndifficulty levels. Applying RIDE to competition-level mathematical benchmarks\nyields perturbed versions that degrade advanced LLM performance, with\nexperiments showing an average 21.73% drop across 26 models, thereby exposing\nlimited robustness in mathematical reasoning and confirming the validity of our\nevaluation approach.", "AI": {"tldr": "本文提出RIDE框架，利用项目反应理论（IRT）和强化学习，生成更具挑战性且结构良好的数学问题，以对抗性评估大型语言模型（LLM）真实的数学推理能力，揭示了LLM在此方面的鲁棒性不足。", "motivation": "LLM在数学推理上的高表现可能因训练数据泄露或表面模式匹配而虚高，而非真正的推理能力。现有基于规则的扰动方法常生成不适定问题，阻碍了系统性评估问题难度和基准演进，因此需要一种更严谨的方法来衡量真实数学推理能力。", "method": "本文提出了RIDE框架，一个新颖的对抗性问题重写框架。它利用项目反应理论（IRT）来严格衡量问题难度，并生成内在更具挑战性、结构良好的数学问题变体。通过使用35个LLM模拟学生并根据其响应构建一个难度排序器，该排序器作为强化学习的奖励信号，指导问题重写模型在不同难度级别上重构现有问题。", "result": "将RIDE应用于竞赛级别的数学基准测试，生成了扰动版本，导致先进LLM的性能下降。实验显示，26个模型平均性能下降21.73%。", "conclusion": "RIDE生成的扰动问题暴露了LLM在数学推理方面有限的鲁棒性，并证实了本文评估方法的有效性，即能够更准确地衡量LLM的真实数学推理能力。"}}
{"id": "2511.04679", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04679", "abs": "https://arxiv.org/abs/2511.04679", "authors": ["Qingzhou Lu", "Yao Feng", "Baiyu Shi", "Michael Piseno", "Zhenan Bao", "C. Karen Liu"], "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction", "comment": "Home page: https://gentle-humanoid.axell.top", "summary": "Humanoid robots are expected to operate in human-centered environments where\nsafe and natural physical interaction is essential. However, most recent\nreinforcement learning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are typically\nrestricted to base or end-effector control and focus on resisting extreme\nforces rather than enabling compliance. We introduce GentleHumanoid, a\nframework that integrates impedance control into a whole-body motion tracking\npolicy to achieve upper-body compliance. At its core is a unified spring-based\nformulation that models both resistive contacts (restoring forces when pressing\nagainst surfaces) and guiding contacts (pushes or pulls sampled from human\nmotion data). This formulation ensures kinematically consistent forces across\nthe shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through task-adjustable\nforce thresholds. We evaluate our approach in both simulation and on the\nUnitree G1 humanoid across tasks requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and safe object\nmanipulation. Compared to baselines, our policy consistently reduces peak\ncontact forces while maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward humanoid robots\nthat can safely and effectively collaborate with humans and handle objects in\nreal-world environments.", "AI": {"tldr": "该论文提出GentleHumanoid框架，通过将阻抗控制融入全身运动跟踪策略，使仿人机器人在与人交互时能实现上身柔顺性，从而实现安全自然的物理交互。", "motivation": "现有强化学习策略通常强调刚性跟踪并抑制外部力，而现有的阻抗增强方法通常局限于基座或末端执行器控制，并侧重于抵抗极端力而非实现柔顺性，这阻碍了仿人机器人在以人为中心的环境中进行安全自然的物理交互。", "method": "引入GentleHumanoid框架，将阻抗控制集成到全身运动跟踪策略中，以实现上身柔顺性。核心是一个统一的基于弹簧的公式，用于模拟抵抗性接触（按压表面时的恢复力）和引导性接触（来自人类运动数据的推拉力）。该公式确保了肩部、肘部和手腕之间运动学上一致的力，并使策略能够应对多样化的交互场景。通过任务可调的力阈值进一步支持安全性。", "result": "在模拟和Unitree G1仿人机器人上，对于需要不同柔顺性水平的任务（如轻柔拥抱、站立辅助和安全物体操作），与基线相比，该策略在保持任务成功的同时，始终能降低峰值接触力，从而实现更平滑、更自然的交互。", "conclusion": "这些结果标志着仿人机器人在真实世界环境中安全有效地与人类协作并处理物体方面迈出了重要一步。"}}
{"id": "2511.04470", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04470", "abs": "https://arxiv.org/abs/2511.04470", "authors": ["Amin Hashemi-Zadeh", "Nima Tashakor", "Sandun Hettiarachchi", "Stefan Goetz"], "title": "AI-Driven Phase-Shifted Carrier Optimization for Cascaded Bridge Converters, Modular Multilevel Converters, and Reconfigurable Batteries", "comment": "10 pages, 11 figures", "summary": "Phase-shifted carrier pulse-width modulation (PSC-PWM) is a widely adopted\nscheduling algorithm in cascaded bridge converters, modular multilevel\nconverters, and reconfigurable batteries. However, non-uniformed pulse widths\nfor the modules with fixed phase shift angles lead to significant ripple\ncurrent and output-voltage distortion. Voltage uniformity instead would require\noptimization of the phase shifts of the individual carriers. However, the\ncomputational burden for such optimization is beyond the capabilities of any\nsimple embedded controller. This paper proposes a neural network that emulates\nthe behavior of an instantaneous optimizer with significantly reduced\ncomputational burden. The proposed method has the advantages of stable\nperformance in predicting the optimum phase-shift angles under balanced battery\nmodules with non-identical modulation indices without requiring extensive\nlookup tables, slow numerical optimization, or complex controller tuning. With\nonly one (re)training session for any specified number of modules, the proposed\nmethod is readily adaptable to different system sizes. Furthermore, the\nproposed framework also includes a simple scaling strategy that allows a neural\nnetwork trained for fewer modules to be reused for larger systems by grouping\nmodules and adjusting their phase shifts. The scaling strategy eliminates the\nneed for retraining. Large-scale assessment, simulations, and experiments\ndemonstrate that, on average, the proposed approach can reduce the current\nripple and the weighted total harmonic distortion by up to 50 % in real time\nand is 100 to 500 thousand times faster than a conventional optimizer (e.g.,\ngenetic algorithms), making it the only solution for an online application.", "AI": {"tldr": "本文提出了一种基于神经网络的方法，用于实时优化移相载波脉宽调制（PSC-PWM）中的相移角，以减少级联变换器中的电流纹波和电压失真，其计算速度远超传统优化器，并具有良好的可扩展性。", "motivation": "移相载波脉宽调制（PSC-PWM）在级联变换器中广泛应用，但固定相移角会导致模块脉宽不均匀，从而产生显著的纹波电流和输出电压失真。优化单个载波的相移角可以实现电压均匀性，但其计算负担对于简单的嵌入式控制器来说过大。", "method": "本文提出了一种神经网络，能够模拟瞬时优化器的行为，以显著降低计算负担。该方法通过一次训练即可预测最佳相移角，适用于模块平衡但调制指数不同的情况，无需大量的查找表、缓慢的数值优化或复杂的控制器调谐。此外，还提出了一个简单的扩展策略，允许为少量模块训练的神经网络通过分组和调整相移来重用于更大的系统，无需重新训练。", "result": "大规模评估、仿真和实验表明，该方法平均可将电流纹波和加权总谐波失真降低高达50%，且实时性能比传统优化器（如遗传算法）快10万到50万倍。这使其成为唯一适用于在线应用的解决方案。", "conclusion": "所提出的神经网络方法为级联变换器中PSC-PWM的相移角优化提供了一个快速、稳定且可扩展的解决方案，能够显著提高性能并满足实时在线应用的需求。"}}
{"id": "2511.04084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04084", "abs": "https://arxiv.org/abs/2511.04084", "authors": ["Nishchal Sapkota", "Haoyan Shi", "Yejia Zhang", "Xianshi Ma", "Bofang Zheng", "Danny Z. Chen"], "title": "When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation is critical for accurate diagnostics and treatment\nplanning, but remains challenging due to complex anatomical structures and\nlimited annotated training data. CNN-based segmentation methods excel at local\nfeature extraction, but struggle with modeling long-range dependencies.\nTransformers, on the other hand, capture global context more effectively, but\nare inherently data-hungry and computationally expensive. In this work, we\nintroduce UKAST, a U-Net like architecture that integrates rational-function\nbased Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By\nleveraging rational base functions and Group Rational KANs (GR-KANs) from the\nKolmogorov-Arnold Transformer (KAT), our architecture addresses the\ninefficiencies of vanilla spline-based KANs, yielding a more expressive and\ndata-efficient framework with reduced FLOPs and only a very small increase in\nparameter count compared to SwinUNETR. UKAST achieves state-of-the-art\nperformance on four diverse 2D and 3D medical image segmentation benchmarks,\nconsistently surpassing both CNN- and Transformer-based baselines. Notably, it\nattains superior accuracy in data-scarce settings, alleviating the data-hungry\nlimitations of standard Vision Transformers. These results show the potential\nof KAN-enhanced Transformers to advance data-efficient medical image\nsegmentation. Code is available at: https://github.com/nsapkota417/UKAST", "AI": {"tldr": "本文提出UKAST，一种将基于有理函数的Kolmogorov-Arnold网络（KANs）集成到Swin Transformer编码器中的U-Net类架构，旨在实现数据高效的医学图像分割，并在多个基准测试中达到SOTA性能。", "motivation": "医学图像分割对于诊断和治疗规划至关重要，但面临复杂解剖结构和标注数据有限的挑战。传统的CNN方法难以建模长距离依赖，而Transformer虽然能捕捉全局上下文，却存在数据需求大和计算成本高的缺点。", "method": "本文引入UKAST，一种U-Net类架构，将基于有理函数的Kolmogorov-Arnold网络（KANs）集成到Swin Transformer编码器中。通过利用Kolmogorov-Arnold Transformer (KAT) 中的有理基函数和Group Rational KANs (GR-KANs)，UKAST解决了传统样条基KANs的低效问题，实现了更具表达力和数据效率的框架，与SwinUNETR相比，FLOPs更少，参数量仅有微小增加。", "result": "UKAST在四个不同的2D和3D医学图像分割基准测试中取得了最先进的性能，持续超越了基于CNN和Transformer的基线方法。尤其是在数据稀缺的环境中，UKAST展现出卓越的准确性，有效缓解了标准Vision Transformer对数据量大的依赖。", "conclusion": "这些结果表明，KAN增强型Transformer在推动数据高效医学图像分割方面具有巨大潜力。"}}
{"id": "2511.04341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04341", "abs": "https://arxiv.org/abs/2511.04341", "authors": ["Nick Oh", "Fernand Gobet"], "title": "Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning", "comment": "To-be presented at the Workshop on the Foundations of Reasoning in\n  Language Models at NeurIPS 2025 (non-archival)", "summary": "Test-time reasoning architectures such as those following the Generate-Verify\nparadigm -- where a model iteratively refines or verifies its own generated\noutputs -- prioritise generation and verification but exclude the monitoring\nprocesses that determine when and how reasoning should begin. This omission may\ncontribute to the prefix dominance trap, in which models commit early to\nsuboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy\nloss. We address this architectural gap by formalising Flavell's and Nelson and\nNarens' metacognitive theories into computational specifications, proposing the\nMonitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify\nparadigm by adding explicit monitoring that captures metacognitive experiences\n(from difficulty assessments to confidence judgements) before generation begins\nand refines future monitoring through verification feedback. Though we present\nno empirical validation, this work provides the first systematic computational\ntranslation of foundational metacognitive theories, offering a principled\nvocabulary for understanding reasoning system failures and suggesting specific\narchitectural interventions for future test-time reasoning designs.", "AI": {"tldr": "本文提出Monitor-Generate-Verify (MGV)框架，通过整合元认知监控来解决现有生成-验证推理架构中因缺乏监控导致的“前缀主导陷阱”问题，从而提高推理系统的准确性。", "motivation": "现有的测试时推理架构（如生成-验证范式）忽视了决定何时以及如何开始推理的监控过程，这导致模型容易陷入“前缀主导陷阱”，过早地选择次优推理路径且难以恢复，造成约20%的准确率损失。", "method": "作者将Flavell以及Nelson和Narens的元认知理论形式化为计算规范，并在此基础上提出了Monitor-Generate-Verify (MGV)框架。MGV通过在生成开始前加入明确的监控（包括难度评估和置信度判断），并通过验证反馈来完善未来的监控，从而扩展了生成-验证范式。", "result": "尽管本文未提供实证验证，但它首次系统地将基础元认知理论转化为计算规范，为理解推理系统故障提供了一套有原则的词汇，并为未来的测试时推理设计提出了具体的架构干预建议。", "conclusion": "MGV框架通过在推理过程中显式地整合元认知监控，弥补了现有生成-验证架构的不足。它为设计更鲁棒、能有效避免“前缀主导陷阱”的测试时推理系统提供了理论基础和架构方向。"}}
{"id": "2511.04388", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04388", "abs": "https://arxiv.org/abs/2511.04388", "authors": ["Chang Liu", "Juan Li", "Sheng Zhang", "Chang Liu", "Jie Li", "Xu Zhang"], "title": "BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems", "comment": "8 pages, 5 figures, published to IROS 2025", "summary": "Depth estimation is one of the key technologies for realizing 3D perception\nin unmanned systems. Monocular depth estimation has been widely researched\nbecause of its low-cost advantage, but the existing methods face the challenges\nof poor depth estimation performance and blurred object boundaries on embedded\nsystems. In this paper, we propose a novel monocular depth estimation model,\nBoRe-Depth, which contains only 8.7M parameters. It can accurately estimate\ndepth maps on embedded systems and significantly improves boundary quality.\nFirstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which\nadaptively fuses depth features to enhance boundary detail representation.\nSecondly, we integrate semantic knowledge into the encoder to improve the\nobject recognition and boundary perception capabilities. Finally, BoRe-Depth is\ndeployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We\ndemonstrate that the proposed model significantly outperforms previous\nlightweight models on multiple challenging datasets, and we provide detailed\nablation studies for the proposed methods. The code is available at\nhttps://github.com/liangxiansheng093/BoRe-Depth.", "AI": {"tldr": "本文提出了一种名为BoRe-Depth的单目深度估计算法，专为嵌入式系统设计，参数量仅8.7M。它能准确估计深度图并显著改善物体边界质量，在NVIDIA Jetson Orin上以50.7 FPS高效运行，性能优于现有轻量级模型。", "motivation": "现有应用于嵌入式系统的单目深度估计算法存在深度估计性能不佳和物体边界模糊的问题。", "method": "本文提出了BoRe-Depth模型，包含8.7M参数。方法包括：1) 设计增强特征自适应融合模块(EFAF)，自适应融合深度特征以增强边界细节表示；2) 将语义知识整合到编码器中，以提高物体识别和边界感知能力。", "result": "BoRe-Depth模型能在嵌入式系统上准确估计深度图，并显著改善边界质量。它在NVIDIA Jetson Orin上以50.7 FPS高效运行，并在多个挑战性数据集上显著优于之前的轻量级模型。", "conclusion": "BoRe-Depth是一种高效且准确的单目深度估计算法，特别适用于嵌入式系统，能够有效解决现有方法的性能和边界质量问题，为无人系统中的3D感知提供了低成本解决方案。"}}
{"id": "2511.04531", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04531", "abs": "https://arxiv.org/abs/2511.04531", "authors": ["Arkadeep Saha", "Pieter van Goor", "Antonio Franchi", "Ravi Banavar"], "title": "Synchronous Observer Design for Landmark-Inertial SLAM with Almost-Global Convergence", "comment": "13 pages, 3 figures, This work has been submitted to IFAC World\n  Congress 2026 for possible publication", "summary": "Landmark Inertial Simultaneous Localisation and Mapping (LI-SLAM) is the\nproblem of estimating the locations of landmarks in the environment and the\nrobot's pose relative to those landmarks using landmark position measurements\nand measurements from Inertial Measurement Unit (IMU). This paper proposes a\nnonlinear observer for LI-SLAM posed in continuous time and analyses the\nobserver in a base space that encodes all the observable states of LI-SLAM. The\nlocal exponential stability and almost-global asymptotic stability of the error\ndynamics in base space is established in the proof section and validated using\nsimulations.", "AI": {"tldr": "本文提出了一种用于地标惯性同步定位与建图（LI-SLAM）的非线性观测器，并在编码所有可观测状态的基空间中分析了其误差动力学的局部指数稳定性和几乎全局渐近稳定性，并通过仿真验证。", "motivation": "LI-SLAM的目标是利用地标位置测量和惯性测量单元（IMU）数据，估计环境中地标的位置以及机器人相对于这些地标的姿态。", "method": "本文提出了一种在连续时间下构建的非线性观测器，用于解决LI-SLAM问题。该观测器在编码所有LI-SLAM可观测状态的基空间中进行分析。", "result": "在证明部分，建立了基空间中误差动力学的局部指数稳定性和几乎全局渐近稳定性，并通过仿真验证了这些结果。", "conclusion": "所提出的非线性观测器能有效且稳定地解决LI-SLAM问题，实现机器人姿态和地标位置的估计，并具备良好的误差收敛性。"}}
{"id": "2511.04112", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04112", "abs": "https://arxiv.org/abs/2511.04112", "authors": ["Biao Liu", "Yuanzhi Liang"], "title": "SpatialLock: Precise Spatial Control in Text-to-Image Synthesis", "comment": "Work in progress", "summary": "Text-to-Image (T2I) synthesis has made significant advancements in recent\nyears, driving applications such as generating datasets automatically. However,\nprecise control over object localization in generated images remains a\nchallenge. Existing methods fail to fully utilize positional information,\nleading to an inadequate understanding of object spatial layouts. To address\nthis issue, we propose SpatialLock, a novel framework that leverages perception\nsignals and grounding information to jointly control the generation of spatial\nlocations. SpatialLock incorporates two components: Position-Engaged Injection\n(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial\ninformation through an attention layer, encouraging the model to learn the\ngrounding information effectively. PoG employs perception-based supervision to\nfurther refine object localization. Together, these components enable the model\nto generate objects with precise spatial arrangements and improve the visual\nquality of the generated images. Experiments show that SpatialLock sets a new\nstate-of-the-art for precise object positioning, achieving IOU scores above 0.9\nacross multiple datasets.", "AI": {"tldr": "SpatialLock是一个新颖的框架，通过结合感知信号和定位信息，解决了文本到图像生成中物体精确定位的问题，显著提高了空间布局的控制和图像质量。", "motivation": "文本到图像合成在物体定位的精确控制方面仍面临挑战，现有方法未能充分利用位置信息，导致对物体空间布局的理解不足。", "method": "本文提出了SpatialLock框架，包含两个核心组件：1. Position-Engaged Injection (PoI) 通过注意力层直接整合空间信息，促进模型有效学习定位信息。2. Position-Guided Learning (PoG) 采用基于感知的监督进一步细化物体定位。", "result": "实验表明，SpatialLock在精确定位方面达到了新的最先进水平，在多个数据集上实现了超过0.9的IOU分数，并提升了生成图像的视觉质量。", "conclusion": "SpatialLock通过其独特的方法有效解决了文本到图像生成中的物体精确定位问题，能够生成具有精确空间排列的物体，并显著提高图像质量。"}}
{"id": "2511.04139", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.04139", "abs": "https://arxiv.org/abs/2511.04139", "authors": ["Dazhong Chen", "Yi-Cheng Lin", "Yuchen Huang", "Ziwei Gong", "Di Jiang", "Zeying Xie", "Yi R.", "Fung"], "title": "CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese", "comment": null, "summary": "Automatic speech recognition (ASR) is critical for language accessibility,\nyet low-resource Cantonese remains challenging due to limited annotated data,\nsix lexical tones, tone sandhi, and accent variation. Existing ASR models, such\nas Whisper, often suffer from high word error rates. Large audio-language\nmodels (LALMs), in contrast, can leverage broader contextual reasoning but\nstill require explicit tonal and prosodic acoustic cues. We introduce CantoASR,\na collaborative ASR-LALM error correction framework that integrates forced\nalignment for acoustic feature extraction, a LoRA-finetuned Whisper for\nimproved tone discrimination, and an instruction-tuned Qwen-Audio for\nprosody-aware correction. Evaluations on spontaneous Cantonese data show\nsubstantial CER gains over Whisper-Large-V3. These findings suggest that\nintegrating acoustic cues with LALM reasoning provides a scalable strategy for\nlow-resource tonal and dialectal ASR.", "AI": {"tldr": "CantoASR是一个协同ASR-LALM错误纠正框架，它结合了强制对齐、LoRA微调的Whisper和指令微调的Qwen-Audio，以解决粤语低资源ASR的挑战，并在自发粤语数据上显著优于Whisper-Large-V3。", "motivation": "自动语音识别（ASR）对语言可及性至关重要，但由于标注数据有限、六个声调、变调和口音差异，低资源粤语ASR仍然具有挑战性。现有模型如Whisper的词错误率较高，而大型音频语言模型（LALM）虽能利用更广泛的上下文推理，但仍需要明确的声调和韵律声学线索。", "method": "本文引入了CantoASR，一个协同ASR-LALM错误纠正框架。它整合了用于声学特征提取的强制对齐、用于改善声调辨别的LoRA微调Whisper，以及用于韵律感知纠正的指令微调Qwen-Audio。", "result": "在自发粤语数据上的评估显示，CantoASR在字符错误率（CER）方面比Whisper-Large-V3有显著提升。", "conclusion": "研究结果表明，将声学线索与LALM推理相结合，为低资源声调和方言ASR提供了一种可扩展的策略。"}}
{"id": "2511.04153", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04153", "abs": "https://arxiv.org/abs/2511.04153", "authors": ["Fahim Ahmed", "Md Mubtasim Ahasan", "Jahir Sadik Monon", "Muntasir Wahed", "M Ashraful Amin", "A K M Mahbubur Rahman", "Amin Ahsan Ali"], "title": "BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation", "comment": null, "summary": "Text-to-SQL systems provide a natural language interface that can enable even\nlaymen to access information stored in databases. However, existing Large\nLanguage Models (LLM) struggle with SQL generation from natural instructions\ndue to large schema sizes and complex reasoning. Prior work often focuses on\ncomplex, somewhat impractical pipelines using flagship models, while smaller,\nefficient models remain overlooked. In this work, we explore three multi-agent\nLLM pipelines, with systematic performance benchmarking across a range of small\nto large open-source models: (1) Multi-agent discussion pipeline, where agents\niteratively critique and refine SQL queries, and a judge synthesizes the final\nanswer; (2) Planner-Coder pipeline, where a thinking model planner generates\nstepwise SQL generation plans and a coder synthesizes queries; and (3)\nCoder-Aggregator pipeline, where multiple coders independently generate SQL\nqueries, and a reasoning agent selects the best query. Experiments on the\nBird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small\nmodel performance, with up to 10.6% increase in Execution Accuracy for\nQwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,\nthe LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B\nand QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest\nscore of 56.4%. Codes are available at\nhttps://github.com/treeDweller98/bappa-sql.", "AI": {"tldr": "本文探索了三种多智能体LLM管道，旨在提升Text-to-SQL系统性能，特别是针对小型开源模型，并通过系统基准测试发现“规划器-编码器”管道效果最佳，显著提高了SQL生成准确性。", "motivation": "现有大型语言模型在Text-to-SQL任务中面临挑战，主要原因是大规模数据库模式和复杂的推理需求。此外，以往研究多集中于使用旗舰级模型构建复杂而不切实际的管道，而忽略了小型高效模型的潜力。", "method": "研究探索了三种多智能体LLM管道：1) 多智能体讨论管道，智能体迭代地评论和完善SQL查询，由一个判断器给出最终答案；2) 规划器-编码器管道，一个“思考”模型生成分步SQL生成计划，然后由一个编码器合成查询；3) 编码器-聚合器管道，多个编码器独立生成SQL查询，一个推理智能体选择最佳查询。这些管道在从小到大的开源模型范围内进行了系统性性能基准测试。", "result": "在Bird-Bench Mini-Dev数据集上的实验显示，多智能体讨论管道能提升小型模型性能，例如Qwen2.5-7b-Instruct在三轮讨论后执行准确率提高了10.6%。在所有管道中，“LLM推理器-编码器”管道表现最佳，其中DeepSeek-R1-32B和QwQ-32B规划器将Gemma 3 27B IT的准确率从52.4%提升至最高的56.4%。", "conclusion": "多智能体LLM管道能够有效提升Text-to-SQL系统的性能，特别是对于小型模型。其中，“规划器-编码器”管道展现出最佳效果，通过规划模型指导查询生成，显著提高了SQL的生成准确性，为数据库的自然语言访问提供了更可靠的解决方案。"}}
{"id": "2511.04439", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04439", "abs": "https://arxiv.org/abs/2511.04439", "authors": ["Anisha Garg", "Ganesh Venkatesh"], "title": "The Peril of Preference: Why GRPO fails on Ordinal Rewards", "comment": null, "summary": "Group-relative Policy Optimization's (GRPO) simplicity makes it highly\ndesirable for adapting LLMs to become experts at specific tasks. But this\nsimplicity also makes it ill-specified as we seek to enhance RL training with\nricher, non-binary feedback. When using ordinal rewards to give partial credit,\nGRPO's simplicity starts to hurt, as its group-average baseline often assigns a\npositive advantage to failed trajectories and reinforces incorrect behavior.\n  We introduce Correctness Relative Policy Optimization (CoRPO), a new\nformulation that solves this flaw. CoRPO uses an adaptive baseline that\nenforces a minimum quality threshold, ensuring failed solutions are never\npositively reinforced. Once the policy consistently meets this threshold, the\nbaseline automatically transitions to a relative preference mode, pushing the\nmodel to find optimal solutions rather than just \"acceptable\" ones. We\nempirically validate CoRPO on a code verification task, where it demonstrates\nmore stable convergence and better out-of-domain generalization.\n  This work represents a critical step in our broader research program to\nenable LLMs to learn genuinely new capabilities through reinforcement learning.\nWe achieve this by enabling LLMs to learn from rich, multi-dimensional feedback\n- progressing from binary to ordinal rewards in this work, and onward to\ndenser, per-step supervision.", "AI": {"tldr": "针对使用非二元（序数）奖励时，GRPO在强化学习训练中可能错误地强化失败轨迹的问题，本文提出了CoRPO。CoRPO采用自适应基线，先确保满足最低质量阈值，再转向相对偏好模式，从而实现更稳定的收敛和更好的泛化能力。", "motivation": "将大型语言模型（LLMs）通过强化学习（RL）适应特定任务时，GRPO因其简洁性而受欢迎。然而，当使用更丰富的非二元（序数）奖励（而非简单二元反馈）进行训练时，GRPO的简单性导致其平均基线可能错误地为失败轨迹分配正优势，从而强化不正确的行为。", "method": "本文引入了正确性相对策略优化（CoRPO）。CoRPO使用一个自适应基线：首先强制执行一个最低质量阈值，确保失败的解决方案永远不会得到正向强化；一旦策略持续达到此阈值，基线便自动转换为相对偏好模式，推动模型寻找最优解而非仅仅“可接受”的方案。", "result": "在代码验证任务上的实验表明，CoRPO展示了更稳定的收敛性，并具有更好的域外泛化能力。", "conclusion": "这项工作是LLMs通过强化学习学习新能力的关键一步，通过使LLMs能够从丰富、多维度的反馈中学习（从二元奖励到序数奖励，并进一步向更密集的每步监督发展），从而实现这一目标。"}}
{"id": "2511.04393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04393", "abs": "https://arxiv.org/abs/2511.04393", "authors": ["Chanwoo Park", "Ziyang Chen", "Asuman Ozdaglar", "Kaiqing Zhang"], "title": "Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as \"agents\" for\ndecision-making (DM) in interactive and dynamic environments. Yet, since they\nwere not originally designed for DM, recent studies show that LLMs can struggle\neven in basic online DM problems, failing to achieve low regret or an effective\nexploration-exploitation tradeoff. To address this, we introduce Iterative\nRegret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure\nthat repeatedly distills low-regret decision trajectories back into the base\nmodel. At each iteration, the model rolls out multiple decision trajectories,\nselects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior\nmethods that (a) distill action sequences from known DM algorithms or (b) rely\non manually crafted chain-of-thought templates, our approach leverages the\nregret metric to elicit the model's own DM ability and reasoning rationales.\nThis reliance on model-generated reasoning avoids rigid output engineering and\nprovides more flexible, natural-language training signals. Empirical results\nshow that Iterative RMFT improves LLMs' DM performance across diverse models -\nfrom Transformers with numerical input/output, to open-weight LLMs, and\nadvanced closed-weight models like GPT-4o mini. Its flexibility in output and\nreasoning formats enables generalization across tasks with varying horizons,\naction spaces, reward processes, and natural-language contexts. Finally, we\nprovide theoretical insight showing that a single-layer Transformer under this\nparadigm can act as a no-regret learner in a simplified setting. Overall,\nIterative RMFT offers a principled and general post-training framework for\nenhancing LLMs' decision-making capabilities.", "AI": {"tldr": "LLM在决策制定中表现不佳，本文提出Iterative RMFT，通过迭代地将模型自身生成的低遗憾决策轨迹蒸馏回模型进行微调，显著提升LLM的决策能力。", "motivation": "大型语言模型（LLM）被越来越多地部署为决策制定（DM）的“代理”，但在交互式和动态环境中，它们在基本的在线决策问题上表现不佳，无法实现低遗憾或有效的探索-利用权衡，因为它们并非为此设计。", "method": "引入迭代遗憾最小化微调（Iterative RMFT），这是一种后训练程序。在每次迭代中，模型会生成多条决策轨迹，选择其中遗憾值最低的k条，并用这些轨迹对自身进行微调。与以往方法不同，Iterative RMFT利用遗憾指标来激发模型自身的决策能力和推理逻辑，避免了对已知算法或手动链式思考模板的依赖，从而提供更灵活、自然的训练信号。", "result": "实验结果表明，Iterative RMFT显著提升了多种LLM（包括带有数值输入/输出的Transformer、开源LLM以及GPT-4o mini等先进模型）的决策性能。其输出和推理格式的灵活性使其能够泛化到不同时间范围、动作空间、奖励过程和自然语言上下文的任务。此外，理论分析表明，在简化设置下，一个单层Transformer在此范式下可以作为无遗憾学习器。", "conclusion": "Iterative RMFT为增强LLM的决策能力提供了一个原则性且通用的后训练框架。"}}
{"id": "2511.04117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04117", "abs": "https://arxiv.org/abs/2511.04117", "authors": ["Yunghee Lee", "Byeonghyun Pak", "Junwha Hong", "Hoseong Kim"], "title": "Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration", "comment": "21 pages, 8 figures. NeurIPS 2025. Project page:\n  https://yhlee-add.github.io/THG", "summary": "In this paper, we propose Tortoise and Hare Guidance (THG), a training-free\nstrategy that accelerates diffusion sampling while maintaining high-fidelity\ngeneration. We demonstrate that the noise estimate and the additional guidance\nterm exhibit markedly different sensitivity to numerical error by reformulating\nthe classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our\nerror-bound analysis shows that the additional guidance branch is more robust\nto approximation, revealing substantial redundancy that conventional solvers\nfail to exploit. Building on this insight, THG significantly reduces the\ncomputation of the additional guidance: the noise estimate is integrated with\nthe tortoise equation on the original, fine-grained timestep grid, while the\nadditional guidance is integrated with the hare equation only on a coarse grid.\nWe also introduce (i) an error-bound-aware timestep sampler that adaptively\nselects step sizes and (ii) a guidance-scale scheduler that stabilizes large\nextrapolation spans. THG reduces the number of function evaluations (NFE) by up\nto 30% with virtually no loss in generation fidelity ($\\Delta$ImageReward\n$\\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free\naccelerators under identical computation budgets. Our findings highlight the\npotential of multirate formulations for diffusion solvers, paving the way for\nreal-time high-quality image synthesis without any model retraining. The source\ncode is available at https://github.com/yhlee-add/THG.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2511.04626", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04626", "abs": "https://arxiv.org/abs/2511.04626", "authors": ["Zihao Song", "Shirantha Welikala", "Panos J. Antsaklis", "Hai Lin"], "title": "Funnel-Based Online Recovery Control for Nonlinear Systems With Unknown Dynamics", "comment": "13 pages, 14 figures", "summary": "In this paper, we focus on recovery control of nonlinear systems from attacks\nor failures. The main challenges of this problem lie in (1) learning the\nunknown dynamics caused by attacks or failures with formal guarantees, and (2)\nfinding the invariant set of states to formally ensure the state deviations\nallowed from the nominal trajectory. To solve this problem, we propose to apply\nthe Recurrent Equilibrium Networks (RENs) to learn the unknown dynamics using\nthe data from the real-time system states. The input-output property of this\nREN model is guaranteed by incremental integral quadratic constraints (IQCs).\nThen, we propose a funnel-based control method to achieve system recovery from\nthe deviated states. In particular, a sufficient condition for nominal\ntrajectory stabilization is derived together with the invariant funnels along\nthe nominal trajectory. Eventually, the effectiveness of our proposed control\nmethod is illustrated by a simulation example of a DC microgrid control\napplication.", "AI": {"tldr": "本文提出了一种针对遭受攻击或故障的非线性系统的恢复控制方法。该方法结合使用循环平衡网络（REN）来学习未知动力学，并通过漏斗型控制实现系统从偏差状态的恢复，同时提供形式化保证。", "motivation": "该研究旨在解决非线性系统在遭受攻击或故障后恢复控制的挑战，主要包括：1) 如何在形式化保证下学习由攻击或故障引起的未知动力学；2) 如何找到状态的不变集，以形式化地确保状态偏离标称轨迹的允许范围。", "method": "本文提出以下方法：1) 应用循环平衡网络（RENs）从实时系统状态数据中学习未知动力学；2) 通过增量积分二次约束（IQCs）来保证REN模型的输入输出特性；3) 提出一种基于漏斗的控制方法，以实现系统从偏离状态的恢复；4) 推导了标称轨迹稳定性的充分条件以及沿标称轨迹的不变漏斗。", "result": "通过一个直流微电网控制应用的仿真示例，验证了所提出的控制方法的有效性。", "conclusion": "该研究成功地提出了一种结合REN学习未知动力学和漏斗型控制实现非线性系统恢复的方法，并在仿真中展示了其有效性。"}}
{"id": "2511.04123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04123", "abs": "https://arxiv.org/abs/2511.04123", "authors": ["Tengjie Li", "Shikui Tu", "Lei Xu"], "title": "Text to Sketch Generation with Multi-Styles", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advances in vision-language models have facilitated progress in sketch\ngeneration. However, existing specialized methods primarily focus on generic\nsynthesis and lack mechanisms for precise control over sketch styles. In this\nwork, we propose a training-free framework based on diffusion models that\nenables explicit style guidance via textual prompts and referenced style\nsketches. Unlike previous style transfer methods that overwrite key and value\nmatrices in self-attention, we incorporate the reference features as auxiliary\ninformation with linear smoothing and leverage a style-content guidance\nmechanism. This design effectively reduces content leakage from reference\nsketches and enhances synthesis quality, especially in cases with low\nstructural similarity between reference and target sketches. Furthermore, we\nextend our framework to support controllable multi-style generation by\nintegrating features from multiple reference sketches, coordinated via a joint\nAdaIN module. Extensive experiments demonstrate that our approach achieves\nhigh-quality sketch generation with accurate style alignment and improved\nflexibility in style control. The official implementation of M3S is available\nat https://github.com/CMACH508/M3S.", "AI": {"tldr": "本文提出了一种基于扩散模型的免训练框架，通过文本提示和参考草图实现对草图风格的精确控制和多风格生成，有效减少内容泄漏并提高生成质量。", "motivation": "现有的草图生成方法主要侧重于通用合成，缺乏对草图风格的精确控制机制。", "method": "本研究提出一个基于扩散模型的免训练框架，通过文本提示和参考风格草图实现显式风格指导。与以往覆盖自注意力K/V矩阵的方法不同，该方法通过线性平滑将参考特征作为辅助信息整合，并利用风格-内容指导机制。此外，该框架通过整合来自多个参考草图的特征，并由联合AdaIN模块协调，支持可控的多风格生成。", "result": "广泛的实验表明，该方法实现了高质量的草图生成，具有准确的风格对齐和更高的风格控制灵活性。它有效减少了参考草图的内容泄漏，并在参考与目标草图结构相似度较低的情况下显著提高了合成质量。", "conclusion": "所提出的M3S框架通过创新的风格指导机制，成功解决了现有草图生成方法在风格控制方面的局限性，实现了高精度、灵活的单风格和多风格草图生成。"}}
{"id": "2511.04464", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04464", "abs": "https://arxiv.org/abs/2511.04464", "authors": ["Carnot Braun", "Rafael O. Jarczewski", "Gabriel U. Talasso", "Leandro A. Villas", "Allan M. de Souza"], "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context", "comment": null, "summary": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization.", "AI": {"tldr": "PAVe系统结合传统路径规划算法与大型语言模型（LLM），实现个性化、情境感知的城市车辆路径规划，以更好地理解和整合人类驾驶员的复杂需求。", "motivation": "传统车辆路径系统擅长优化单一或有限的多目标指标（如时间、距离），但在解释和整合人类驾驶员的复杂、语义化和动态情境（如多步骤任务、情境约束、紧急需求）方面能力不足。", "method": "PAVe采用混合代理助手，首先通过多目标（时间、CO2）Dijkstra算法生成一组候选路线。然后，一个LLM代理利用预处理的城市兴趣点（POI）地理空间缓存，根据用户提供的任务、偏好和避让规则评估这些候选路线，并进行路径修改。", "result": "在现实城市场景的基准测试中，PAVe成功地将复杂的用户意图转化为适当的路线修改，其初始路线选择准确率超过88%（使用本地模型）。", "conclusion": "将经典路径规划算法与基于LLM的语义推理层相结合，是创建个性化、自适应和可扩展的城市出行优化解决方案的强大而有效的方法。"}}
{"id": "2511.04184", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04184", "abs": "https://arxiv.org/abs/2511.04184", "authors": ["Mohammed Musthafa Rafi", "Adarsh Krishnamurthy", "Aditya Balu"], "title": "Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains", "comment": "10 pages, 4 figures. Submitted to IEEE DISTILL 2025 (co-located with\n  IEEE TPS 2025)", "summary": "The proliferation of AI-generated content has created an absurd communication\ntheater where senders use LLMs to inflate simple ideas into verbose content,\nrecipients use LLMs to compress them back into summaries, and as a consequence\nneither party engage with authentic content. LAAC (LLM as a Communicator)\nproposes a paradigm shift - positioning LLMs as intelligent communication\nintermediaries that capture the sender's intent through structured dialogue and\nfacilitate genuine knowledge exchange with recipients. Rather than perpetuating\ncycles of AI-generated inflation and compression, LAAC enables authentic\ncommunication across diverse contexts including academic papers, proposals,\nprofessional emails, and cross-platform content generation. However, deploying\nLLMs as trusted communication intermediaries raises critical questions about\ninformation fidelity, consistency, and reliability. This position paper\nsystematically evaluates the trustworthiness requirements for LAAC's deployment\nacross multiple communication domains. We investigate three fundamental\ndimensions: (1) Information Capture Fidelity - accuracy of intent extraction\nduring sender interviews across different communication types, (2)\nReproducibility - consistency of structured knowledge across multiple\ninteraction instances, and (3) Query Response Integrity - reliability of\nrecipient-facing responses without hallucination, source conflation, or\nfabrication. Through controlled experiments spanning multiple LAAC use cases,\nwe assess these trust dimensions using LAAC's multi-agent architecture.\nPreliminary findings reveal measurable trust gaps that must be addressed before\nLAAC can be reliably deployed in high-stakes communication scenarios.", "AI": {"tldr": "本文提出LAAC（将大型语言模型作为通信者）范式，旨在利用LLM作为智能通信中介，通过结构化对话捕获发送者意图，促进接收者之间的真实知识交流，并评估了其部署中的信任度需求和挑战。", "motivation": "当前AI生成内容导致了一种荒谬的通信现象：发送者用LLM膨胀简单想法，接收者用LLM压缩内容，导致双方都无法接触到真实内容。这种现象促使研究者寻求一种能促进真实交流的解决方案。", "method": "LAAC提出将LLM定位为智能通信中介，通过结构化对话捕获发送者意图。本文系统评估了LAAC部署的信任度要求，包括三个维度：(1) 信息捕获保真度——发送者意图提取的准确性；(2) 可复现性——结构化知识在多次交互中的一致性；(3) 查询响应完整性——接收者响应的可靠性（无幻觉、来源混淆或捏造）。通过跨多个LAAC用例的受控实验，评估了LAAC多智能体架构的这些信任维度。", "result": "初步研究结果揭示了可测量的信任差距，这些差距必须在LAAC可靠部署于高风险通信场景之前得到解决。", "conclusion": "LAAC为实现真实通信提供了一种范式转变，但要将其作为可信的通信中介广泛部署，特别是在高风险场景中，必须解决信息保真度、一致性和可靠性方面的显著信任挑战。"}}
{"id": "2511.04126", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04126", "abs": "https://arxiv.org/abs/2511.04126", "authors": ["Venkata Manikanta Desu", "Syed Fawaz Ali"], "title": "Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)", "comment": "14 pages, 11 figures, planning to submit for a coneference", "summary": "This study presents a complete pipeline for automated tennis match analysis.\nOur framework integrates multiple deep learning models to detect and track\nplayers and the tennis ball in real time, while also identifying court\nkeypoints for spatial reference. Using YOLOv8 for player detection, a\ncustom-trained YOLOv5 model for ball tracking, and a ResNet50-based\narchitecture for court keypoint detection, our system provides detailed\nanalytics including player movement patterns, ball speed, shot accuracy, and\nplayer reaction times. The experimental results demonstrate robust performance\nin varying court conditions and match scenarios. The model outputs an annotated\nvideo along with detailed performance metrics, enabling coaches, broadcasters,\nand players to gain actionable insights into the dynamics of the game.", "AI": {"tldr": "该研究提出了一个完整的自动化网球比赛分析流程，整合了多个深度学习模型，用于实时检测和跟踪球员与网球，并识别球场关键点，以提供详细的比赛分析。", "motivation": "旨在为教练、广播公司和球员提供详细的比赛分析和可操作的见解，以深入了解比赛动态。", "method": "该框架整合了多个深度学习模型：使用YOLOv8进行球员检测，使用定制训练的YOLOv5模型进行网球跟踪，以及基于ResNet50的架构进行球场关键点检测。", "result": "实验结果表明，在不同的球场条件和比赛场景下均表现出稳健的性能。该系统输出带有注释的视频和详细的性能指标，包括球员移动模式、球速、击球准确性和球员反应时间。", "conclusion": "该系统能够为教练、广播公司和球员提供关于比赛动态的可操作见解，从而提升对比赛的理解和分析能力。"}}
{"id": "2511.04644", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.04644", "abs": "https://arxiv.org/abs/2511.04644", "authors": ["Stephen Ampleman", "Himanshu Sharma", "Sayak Mukherjee", "Sonja Glavaski"], "title": "Control Affine Hybrid Power Plant Subsystem Modeling for Supervisory Control Design", "comment": "7 pages, 3 figures", "summary": "Hybrid power plants (HPPs) combine multiple power generators\n(conventional/variable) and energy storage capabilities to support generation\ninadequacy and grid demands. This paper introduces a modeling and control\ndesign framework for hybrid power plants (HPPs) consisting of a wind farm,\nsolar plant, and battery storage. Specifically, this work adapts established\nmodeling paradigms for wind farms, solar plants and battery models into a\ncontrol affine form suitable for control design at the supervisory level. In\nthe case of wind and battery models, generator torque and cell current control\nlaws are developed using nonlinear control and control barrier function\ntechniques to track a command from a supervisory control law while maintaining\nsafe and stable operation. The utility of this modeling and control framework\nis illustrated through a test case using a utility demand signal for tracking,\ntime varying wind and irradiance data, and a rule-based supervisory control\nlaw.", "AI": {"tldr": "本文提出了一种针对风电场、太阳能电站和电池储能组成的混合式电厂（HPP）的建模与控制设计框架，将现有模型转化为控制仿射形式，并利用非线性控制和控制障碍函数技术开发了风机和电池的控制律，通过案例验证了其有效性。", "motivation": "混合式电厂（HPPs）结合多种发电机（常规/可变）和储能能力，以应对发电不足和电网需求，因此需要一个有效的建模和控制设计框架来管理这些复杂系统。", "method": "该研究将风电场、太阳能电站和电池的现有模型转化为适用于监督层控制设计的控制仿射形式。对于风机和电池模型，采用非线性控制和控制障碍函数（CBF）技术开发了发电机转矩和电池电流控制律，以跟踪监督控制指令并保持安全稳定运行。通过一个包含电网需求信号、时变风速和辐照数据以及基于规则的监督控制律的测试案例来验证该框架的实用性。", "result": "该建模和控制框架的实用性通过一个包含公用事业需求信号跟踪、时变风能和辐照数据以及基于规则的监督控制律的测试案例得到了验证。", "conclusion": "该研究成功提出并展示了一个用于混合式电厂（HPPs）的建模和控制设计框架，能够有效管理风能、太阳能和电池储能，以满足电网需求并确保系统安全稳定运行。"}}
{"id": "2511.04481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04481", "abs": "https://arxiv.org/abs/2511.04481", "authors": ["Lars Krupp", "Daniel Geißler", "Vishal Banwari", "Paul Lukowicz", "Jakob Karolus"], "title": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis", "comment": "Accepted by AAAI 2026 AISI", "summary": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful\nagentic systems pushing the boundaries of Large Language Models (LLM). They can\nautonomously interact with the internet at the user's behest, such as\nnavigating websites, filling search masks, and comparing price lists. Though\nweb agent research is thriving, induced sustainability issues remain largely\nunexplored. To highlight the urgency of this issue, we provide an initial\nexploration of the energy and $CO_2$ cost associated with web agents from both\na theoretical -via estimation- and an empirical perspective -by benchmarking.\nOur results show how different philosophies in web agent creation can severely\nimpact the associated expended energy, and that more energy consumed does not\nnecessarily equate to better results. We highlight a lack of transparency\nregarding disclosing model parameters and processes used for some web agents as\na limiting factor when estimating energy consumption. Our work contributes\ntowards a change in thinking of how we evaluate web agents, advocating for\ndedicated metrics measuring energy consumption in benchmarks.", "AI": {"tldr": "本文探讨了Web代理（如OpenAI Operator和Google Project Mariner）的能源和碳排放可持续性问题，通过理论估算和实证基准测试发现不同设计对能耗影响显著，且高能耗不等于高效果，呼吁将能耗作为评估Web代理的关键指标。", "motivation": "Web代理系统（如OpenAI Operator和Google Project Mariner）虽然强大且研究活跃，但其引发的可持续性问题（能源消耗和二氧化碳排放）却在很大程度上未被探索。为了强调这一问题的紧迫性，作者进行了初步探索。", "method": "研究方法包括两个方面：理论估算和实证基准测试。通过这两种方式，评估了Web代理相关的能源和二氧化碳成本。", "result": "研究结果表明，Web代理创建的不同设计理念会严重影响其能源消耗。此外，消耗更多能源并不一定意味着更好的结果。研究还指出，缺乏关于某些Web代理模型参数和使用过程的透明度是估算能耗的一个限制因素。", "conclusion": "本文倡导改变Web代理的评估方式，主张在基准测试中加入专门的能耗测量指标，以促进对Web代理可持续性的更全面考量。"}}
{"id": "2511.04195", "categories": ["cs.CL", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.04195", "abs": "https://arxiv.org/abs/2511.04195", "authors": ["Nicolò Pagan", "Petter Törnberg", "Christopher A. Bail", "Anikó Hannák", "Christopher Barrie"], "title": "Computational Turing Test Reveals Systematic Differences Between Human and AI Language", "comment": null, "summary": "Large language models (LLMs) are increasingly used in the social sciences to\nsimulate human behavior, based on the assumption that they can generate\nrealistic, human-like text. Yet this assumption remains largely untested.\nExisting validation efforts rely heavily on human-judgment-based evaluations --\ntesting whether humans can distinguish AI from human output -- despite evidence\nthat such judgments are blunt and unreliable. As a result, the field lacks\nrobust tools for assessing the realism of LLM-generated text or for calibrating\nmodels to real-world data. This paper makes two contributions. First, we\nintroduce a computational Turing test: a validation framework that integrates\naggregate metrics (BERT-based detectability and semantic similarity) with\ninterpretable linguistic features (stylistic markers and topical patterns) to\nassess how closely LLMs approximate human language within a given dataset.\nSecond, we systematically compare nine open-weight LLMs across five calibration\nstrategies -- including fine-tuning, stylistic prompting, and context retrieval\n-- benchmarking their ability to reproduce user interactions on X (formerly\nTwitter), Bluesky, and Reddit. Our findings challenge core assumptions in the\nliterature. Even after calibration, LLM outputs remain clearly distinguishable\nfrom human text, particularly in affective tone and emotional expression.\nInstruction-tuned models underperform their base counterparts, and scaling up\nmodel size does not enhance human-likeness. Crucially, we identify a trade-off:\noptimizing for human-likeness often comes at the cost of semantic fidelity, and\nvice versa. These results provide a much-needed scalable framework for\nvalidation and calibration in LLM simulations -- and offer a cautionary note\nabout their current limitations in capturing human communication.", "AI": {"tldr": "该研究提出了一种计算图灵测试框架来评估大型语言模型（LLM）生成文本的真实性。结果显示，LLM输出与人类文本仍有明显区别，尤其在情感表达上，并且在追求人类相似性与语义忠实度之间存在权衡。", "motivation": "LLMs在社会科学中被广泛用于模拟人类行为，但其生成文本“人类般”的真实性这一核心假设尚未得到充分检验。现有验证方法依赖于不可靠的人类判断，导致缺乏评估LLM文本真实性和校准模型的鲁棒工具。", "method": "1. 引入计算图灵测试：一个结合聚合指标（基于BERT的可检测性、语义相似性）和可解释语言特征（文体标记、主题模式）的验证框架，用于评估LLM逼近人类语言的程度。2. 系统比较九个开源LLM：采用五种校准策略（包括微调、风格提示、上下文检索），基准测试它们在重现X（Twitter）、Bluesky和Reddit用户交互方面的能力。", "result": "1. LLM输出与人类文本仍有明显区别，尤其在情感基调和情绪表达方面。2. 指令微调模型表现不如其基础模型。3. 扩大模型规模并不能增强文本的人类相似性。4. 存在一个关键的权衡：优化人类相似性往往以牺牲语义忠实度为代价，反之亦然。", "conclusion": "该研究提供了一个急需的、可扩展的LLM模拟验证和校准框架。同时，对LLM目前在捕捉人类交流方面的局限性提出了警示。"}}
{"id": "2511.04500", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04500", "abs": "https://arxiv.org/abs/2511.04500", "authors": ["Andrea Cera Palatsi", "Samuel Martin-Gutierrez", "Ana S. Cardenal", "Max Pellert"], "title": "Large language models replicate and predict human cooperation across experiments in game theory", "comment": null, "summary": "Large language models (LLMs) are increasingly used both to make decisions in\ndomains such as health, education and law, and to simulate human behavior. Yet\nhow closely LLMs mirror actual human decision-making remains poorly understood.\nThis gap is critical: misalignment could produce harmful outcomes in practical\napplications, while failure to replicate human behavior renders LLMs\nineffective for social simulations. Here, we address this gap by developing a\ndigital twin of game-theoretic experiments and introducing a systematic\nprompting and probing framework for machine-behavioral evaluation. Testing\nthree open-source models (Llama, Mistral and Qwen), we find that Llama\nreproduces human cooperation patterns with high fidelity, capturing human\ndeviations from rational choice theory, while Qwen aligns closely with Nash\nequilibrium predictions. Notably, we achieved population-level behavioral\nreplication without persona-based prompting, simplifying the simulation\nprocess. Extending beyond the original human-tested games, we generate and\npreregister testable hypotheses for novel game configurations outside the\noriginal parameter grid. Our findings demonstrate that appropriately calibrated\nLLMs can replicate aggregate human behavioral patterns and enable systematic\nexploration of unexplored experimental spaces, offering a complementary\napproach to traditional research in the social and behavioral sciences that\ngenerates new empirical predictions about human social decision-making.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在博弈论实验中模拟人类决策行为的准确性，发现某些LLMs能高度复现人类行为模式，并可用于探索新的实验空间。", "motivation": "LLMs越来越多地用于决策和模拟人类行为，但它们与真实人类决策的一致性尚不清楚。这种不一致可能导致实际应用中的有害结果，或使社会模拟无效。", "method": "开发了博弈论实验的“数字孪生”，并引入了系统的提示和探测框架进行机器行为评估。测试了Llama、Mistral和Qwen三款开源模型。", "result": "Llama模型能高度复现人类合作模式，捕捉到人类偏离理性选择理论的行为；Qwen模型则与纳什均衡预测高度一致。研究在没有基于角色的提示下实现了群体层面的行为复现，并为超出原始参数网格的新博弈配置生成了可测试的假设。", "conclusion": "经过适当校准的LLMs能够复制聚合的人类行为模式，并系统性地探索未知的实验空间，为社会和行为科学的传统研究提供补充方法，并能生成关于人类社会决策的新经验预测。"}}
{"id": "2511.04128", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04128", "abs": "https://arxiv.org/abs/2511.04128", "authors": ["Shengyu Tang", "Zeyuan Lu", "Jiazhi Dong", "Changdong Yu", "Xiaoyu Wang", "Yaohui Lyu", "Weihao Xia"], "title": "DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms", "comment": "Updated version of the Ocean Engineering (Elsevier, 2025) paper with\n  minor corrections", "summary": "Accurate perception of the marine environment through robust multi-object\ntracking (MOT) is essential for ensuring safe vessel navigation and effective\nmaritime surveillance. However, the complicated maritime environment often\ncauses camera motion and subsequent visual degradation, posing significant\nchallenges to MOT. To address this challenge, we propose an efficient\nDual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the\nframework is a parallel tracker with affine compensation, which incorporates an\nobject detection and re-identification (ReID) branch, along with a dedicated\nbranch for dynamic camera motion estimation. Specifically, a Reversible\nColumnar Detection Network (RCDN) is integrated into the detection module to\nleverage multi-level visual features for robust object detection. Furthermore,\na lightweight Transformer-based appearance extractor (Li-TAE) is designed to\ncapture global contextual information and generate robust appearance features.\nAnother branch decouples platform-induced and target-intrinsic motion by\nconstructing a projective transformation, applying platform-motion compensation\nwithin the Kalman filter, and thereby stabilizing true object trajectories.\nFinally, a clustering-optimized feature fusion module effectively combines\nmotion and appearance cues to ensure identity consistency under noise,\nocclusion, and drift. Extensive evaluations on the Singapore Maritime Dataset\ndemonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT\nattains the fastest runtime among existing ReID-based MOT frameworks while\nmaintaining high identity consistency and robustness to jitter and occlusion.\nCode is available at:\nhttps://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.", "AI": {"tldr": "本文提出了一种名为DMSORT的高效双分支海事多目标跟踪（MOT）方法，通过并行跟踪器、仿射补偿、目标检测与ReID以及动态摄像机运动估计，解决了复杂海事环境下摄像机运动和视觉退化带来的挑战，实现了最先进的性能和快速运行速度。", "motivation": "确保船舶安全导航和有效海事监控需要准确感知海洋环境，但复杂的海事环境常导致摄像机运动和视觉退化，给多目标跟踪（MOT）带来了重大挑战。", "method": "本文提出DMSORT方法，其核心是一个带有仿射补偿的并行跟踪器。该框架包含一个目标检测和重识别（ReID）分支，以及一个专门用于动态摄像机运动估计的分支。具体而言，检测模块集成了可逆柱状检测网络（RCDN）以利用多级视觉特征进行鲁棒目标检测；设计了轻量级基于Transformer的外观特征提取器（Li-TAE）以捕捉全局上下文信息并生成鲁棒外观特征。另一个分支通过构建投影变换来解耦平台诱导运动和目标固有运动，并在卡尔曼滤波器中应用平台运动补偿，从而稳定真实目标轨迹。最后，一个聚类优化的特征融合模块有效结合运动和外观线索，确保在噪声、遮挡和漂移下的身份一致性。", "result": "在新加坡海事数据集上的广泛评估表明，DMSORT实现了最先进的性能。值得注意的是，DMSORT在现有基于ReID的MOT框架中运行速度最快，同时保持了高身份一致性以及对抖动和遮挡的鲁棒性。", "conclusion": "DMSORT在复杂海事环境中有效解决了多目标跟踪的挑战，通过其创新的双分支并行跟踪架构和模块设计，在保持高身份一致性和鲁棒性的同时，实现了卓越的跟踪性能和运行效率。"}}
{"id": "2511.04228", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2511.04228", "abs": "https://arxiv.org/abs/2511.04228", "authors": ["Liran Cohen", "Yaniv Nemcovesky", "Avi Mendelson"], "title": "REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs", "comment": "Pre-print version under review", "summary": "Machine unlearning aims to remove the influence of specific training data\nfrom a model without requiring full retraining. This capability is crucial for\nensuring privacy, safety, and regulatory compliance. Therefore, verifying\nwhether a model has truly forgotten target data is essential for maintaining\nreliability and trustworthiness. However, existing evaluation methods often\nassess forgetting at the level of individual inputs. This approach may overlook\nresidual influence present in semantically similar examples. Such influence can\ncompromise privacy and lead to indirect information leakage. We propose REMIND\n(Residual Memorization In Neighborhood Dynamics), a novel evaluation method\naiming to detect the subtle remaining influence of unlearned data and classify\nwhether the data has been effectively forgotten. REMIND analyzes the model's\nloss over small input variations and reveals patterns unnoticed by single-point\nevaluations. We show that unlearned data yield flatter, less steep loss\nlandscapes, while retained or unrelated data exhibit sharper, more volatile\npatterns. REMIND requires only query-based access, outperforms existing methods\nunder similar constraints, and demonstrates robustness across different models,\ndatasets, and paraphrased inputs, making it practical for real-world\ndeployment. By providing a more sensitive and interpretable measure of\nunlearning effectiveness, REMIND provides a reliable framework to assess\nunlearning in language models. As a result, REMIND offers a novel perspective\non memorization and unlearning.", "AI": {"tldr": "REMIND是一种新型评估方法，通过分析模型在输入微小变化上的损失平坦度，检测机器学习遗忘中语义相似样本的残余影响，以更敏感地验证数据是否被有效遗忘。", "motivation": "现有机器学习遗忘评估方法仅在单个输入层面评估，可能忽略语义相似示例中存在的残余影响，从而损害隐私并导致间接信息泄露。因此，需要更可靠的方法来验证模型是否真正遗忘了目标数据。", "method": "本文提出了REMIND（Residual Memorization In Neighborhood Dynamics）方法。它通过分析模型在小输入变化上的损失（即损失景观）来检测未学习数据的微妙残余影响。REMIND仅需要基于查询的访问，并通过观察损失景观的平坦度来区分已遗忘数据和未遗忘数据。", "result": "研究发现，被遗忘的数据会产生更平坦、不那么陡峭的损失景观，而保留或不相关的数据则表现出更尖锐、更波动的模式。REMIND在类似限制下优于现有方法，并对不同的模型、数据集和释义输入表现出鲁棒性，使其适用于实际部署。", "conclusion": "REMIND为评估语言模型中的遗忘有效性提供了一个更敏感、可解释且可靠的框架，从而对记忆和遗忘提出了新的视角。"}}
{"id": "2511.04137", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04137", "abs": "https://arxiv.org/abs/2511.04137", "authors": ["Yujian Liu", "Ze Wang", "Hao Chen", "Ximeng Sun", "Xiaodong Yu", "Jialian Wu", "Jiang Liu", "Emad Barsoum", "Zicheng Liu", "Shiyu Chang"], "title": "Learning from Online Videos at Inference Time for Computer-Use Agents", "comment": null, "summary": "Computer-use agents can operate computers and automate laborious tasks, but\ndespite recent rapid progress, they still lag behind human users, especially\nwhen tasks require domain-specific procedural knowledge about particular\napplications, platforms, and multi-step workflows. Humans can bridge this gap\nby watching video tutorials: we search, skim, and selectively imitate short\nsegments that match our current subgoal. In this paper, we study how to enable\ncomputer-use agents to learn from online videos at inference time effectively.\nWe propose a framework that retrieves and filters tutorial videos, converts\nthem into structured demonstration trajectories, and dynamically selects\ntrajectories as in-context guidance during execution. Particularly, using a\nVLM, we infer UI actions, segment videos into short subsequences of actions,\nand assign each subsequence a textual objective. At inference time, a two-stage\nselection mechanism dynamically chooses a single trajectory to add in context\nat each step, focusing the agent on the most helpful local guidance for its\nnext decision. Experiments on two widely used benchmarks show that our\nframework consistently outperforms strong base agents and variants that use\nonly textual tutorials or transcripts. Analyses highlight the importance of\ntrajectory segmentation and selection, action filtering, and visual\ninformation, suggesting that abundant online videos can be systematically\ndistilled into actionable guidance that improves computer-use agents at\ninference time. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/video_demo.", "AI": {"tldr": "本文提出一个框架，使计算机使用代理能够在推理时从在线视频教程中学习，通过将视频转换为结构化轨迹并动态选择作为上下文指导，显著优于仅使用文本的基线模型。", "motivation": "尽管计算机使用代理进步迅速，但在需要特定领域程序知识的任务上仍落后于人类。人类可以通过观看视频教程来弥补这一差距，因此研究如何让代理有效利用在线视频学习成为关键。", "method": "该框架包括检索和筛选教程视频、将视频转换为结构化演示轨迹，以及在执行时动态选择轨迹作为上下文指导。具体方法是：使用VLM推断UI动作，将视频分割成短动作子序列并赋予文本目标，并在推理时通过两阶段选择机制动态选择单个轨迹作为每一步的局部指导。", "result": "在两个广泛使用的基准测试中，该框架持续优于强大的基础代理和仅使用文本教程或转录本的变体。分析表明，轨迹分割与选择、动作过滤以及视觉信息的重要性，证明在线视频能被系统地提炼成可操作的指导。", "conclusion": "大量的在线视频可以系统地提炼成可操作的指导，从而在推理时有效提升计算机使用代理的性能。"}}
{"id": "2511.04556", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.04556", "abs": "https://arxiv.org/abs/2511.04556", "authors": ["Zihang Ding", "Kun Zhang"], "title": "Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach", "comment": "32 pages (including supplementary information), 11 figures (and 7\n  figures in supplementary). Submitted to Nature Water. Partially presented at\n  HydroML 2025 Symposium, Minnesota Water Resources Conference 2025, and will\n  be presented at AGU Fall Meeting 2025", "summary": "Urban surface water flooding, triggered by intense rainfall overwhelming\ndrainage systems, is increasingly frequent and widespread. While flood\nprediction and monitoring in high spatial-temporal resolution are desired,\npractical constraints in time, budget, and technology hinder its full\nimplementation. How to monitor urban drainage networks and predict flow\nconditions under constrained resource is a major challenge. This study presents\na data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to\noptimize sensor placement and reconstruct peak flowrates in a stormwater\nsystem, using the Woodland Avenue catchment in Duluth, Minnesota, as a case\nstudy. We utilized a SWMM model to generate a training dataset of peak flowrate\nprofiles across the stormwater network. Furthermore, we applied DSS -\nleveraging singular value decomposition for dimensionality reduction and QR\nfactorization for sensor allocation - to identify the optimal monitoring nodes\nbased on the simulated training dataset. We then validated the\nrepresentativeness of these identified monitoring nodes by comparing the\nDSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three\noptimally placed sensors among 77 nodes achieved satisfactory reconstruction\nperformance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to\n75th percentiles). In addition, the model showed good robustness to uncertainty\nin measurements. Its robustness to sensor failures is location-dependent and\nimproves with the number of sensors deployed. The framework balances\ncomputational efficiency and physical interpretability, enabling high-accuracy\nflow reconstruction with minimal sensors. This DSS framework can be further\nintegrated with predictive models to realize flood early warning and real-time\ncontrol under limited sensing and monitoring resource.", "AI": {"tldr": "本研究提出了一种数据驱动的稀疏传感（DSS）框架，结合EPA-SWMM模型，以优化城市雨水系统中的传感器部署，并在资源受限的情况下高精度重建峰值流量。", "motivation": "城市地表洪水日益频繁且普遍，但高时空分辨率的洪水预测和监测受时间、预算和技术限制。在资源有限的情况下，如何监测城市排水网络并预测流量是一个重大挑战。", "method": "本研究使用EPA-SWMM模型生成雨水网络中峰值流量剖面的训练数据集。然后，应用DSS框架（利用奇异值分解进行降维和QR分解进行传感器分配）来识别最佳监测节点。最后，通过比较DSS重建的峰值流量剖面与SWMM结果，验证了所识别监测节点的代表性。", "result": "在77个节点中，仅通过3个优化放置的传感器，就实现了令人满意的流量重建性能，Nash-Sutcliffe效率（NSE）值为0.92-0.95。该模型对测量不确定性表现出良好的鲁棒性，且对传感器故障的鲁棒性取决于位置并随传感器数量的增加而提高。", "conclusion": "DSS框架平衡了计算效率和物理可解释性，能够以最少的传感器实现高精度的流量重建。该框架可进一步与预测模型集成，以在有限的传感和监测资源下实现洪水预警和实时控制。"}}
{"id": "2511.04205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04205", "abs": "https://arxiv.org/abs/2511.04205", "authors": ["Michał Karp", "Anna Kubaszewska", "Magdalena Król", "Robert Król", "Aleksander Smywiński-Pohl", "Mateusz Szymański", "Witold Wydmański"], "title": "LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal", "comment": null, "summary": "This study provides an empirical assessment of whether current large language\nmodels (LLMs) can pass the official qualifying examination for membership in\nPoland's National Appeal Chamber (Krajowa Izba Odwo{\\l}awcza). The authors\nexamine two related ideas: using LLM as actual exam candidates and applying the\n'LLM-as-a-judge' approach, in which model-generated answers are automatically\nevaluated by other models. The paper describes the structure of the exam, which\nincludes a multiple-choice knowledge test on public procurement law and a\nwritten judgment, and presents the hybrid information recovery and extraction\npipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4\nSonnet and Bielik-11B-v2.6) were tested in closed-book and various\nRetrieval-Augmented Generation settings. The results show that although the\nmodels achieved satisfactory scores in the knowledge test, none met the passing\nthreshold in the practical written part, and the evaluations of the\n'LLM-as-a-judge' often diverged from the judgments of the official examining\ncommittee. The authors highlight key limitations: susceptibility to\nhallucinations, incorrect citation of legal provisions, weaknesses in logical\nargumentation, and the need for close collaboration between legal experts and\ntechnical teams. The findings indicate that, despite rapid technological\nprogress, current LLMs cannot yet replace human judges or independent examiners\nin Polish public procurement adjudication.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在波兰国家上诉院官方资格考试中的表现，发现LLMs在选择题部分表现尚可，但在书面判决部分均未能通过，且“LLM作为评判者”的评估结果与官方委员会存在差异，表明当前LLMs尚不能替代人类法官。", "motivation": "本研究旨在实证评估当前大型语言模型（LLMs）是否能通过波兰国家上诉院的官方资格考试，并探讨将LLMs作为实际考试候选人以及利用“LLM作为评判者”方法的可行性。", "method": "研究测试了包括GPT-4.1、Claude 4 Sonnet和Bielik-11B-v2.6在内的多种LLMs。考试结构包括公共采购法多项选择知识测试和书面判决。测试设置包括闭卷和多种检索增强生成（RAG）模式。同时，构建了混合信息检索和提取管道来支持模型。评估方法包括将LLM作为考生和使用其他模型自动评估LLM生成的答案（“LLM作为评判者”）。", "result": "LLMs在知识测试中取得了令人满意的分数，但所有模型在实际书面判决部分均未达到及格线。此外，“LLM作为评判者”的评估结果经常与官方考试委员会的判断存在分歧。主要局限性包括：易受幻觉影响、法律条文引用不正确、逻辑论证薄弱。", "conclusion": "尽管技术进步迅速，但当前的大型语言模型尚不能在波兰公共采购裁决中取代人类法官或独立的审查员。研究强调了法律专家与技术团队之间密切合作的必要性。"}}
{"id": "2511.04161", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04161", "abs": "https://arxiv.org/abs/2511.04161", "authors": ["Suranjan Goswami", "Abhinav Ravi", "Raja Kolla", "Ali Faraz", "Shaharukh Khan", "Akash", "Chandra Khatri", "Shubham Agarwal"], "title": "Seeing Straight: Document Orientation Detection for Efficient OCR", "comment": null, "summary": "Despite significant advances in document understanding, determining the\ncorrect orientation of scanned or photographed documents remains a critical\npre-processing step in the real world settings. Accurate rotation correction is\nessential for enhancing the performance of downstream tasks such as Optical\nCharacter Recognition (OCR) where misalignment commonly arises due to user\nerrors, particularly incorrect base orientations of the camera during capture.\nIn this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for\nevaluating OCR robustness to image rotations, comprising (i) ORB-En, built from\nrotation-transformed structured and free-form English OCR datasets, and (ii)\nORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource\nlanguages. We also present a fast, robust and lightweight rotation\nclassification pipeline built on the vision encoder of Phi-3.5-Vision model\nwith dynamic image cropping, fine-tuned specifically for 4-class rotation task\nin a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy\non identifying the rotations respectively on both the datasets. Beyond\nclassification, we demonstrate the critical role of our module in boosting OCR\nperformance: closed-source (up to 14%) and open-weights models (up to 4x) in\nthe simulated real-world setting.", "AI": {"tldr": "本文提出OCR-Rotation-Bench (ORB) 新基准用于评估OCR对图像旋转的鲁棒性，并开发了一个基于Phi-3.5-Vision的快速轻量级旋转分类管道，该方法在旋转识别上达到高精度，并显著提升了OCR性能。", "motivation": "在现实世界中，扫描或拍摄文档的错误方向是一个关键的预处理问题，它通常由用户错误（如相机捕获时方向不正确）引起，严重影响光学字符识别（OCR）等下游任务的性能。", "method": "1. 引入了OCR-Rotation-Bench (ORB) 新基准，包括：a) ORB-En，由旋转转换后的结构化和自由格式英文OCR数据集构建；b) ORB-Indic，一个涵盖11种中低资源印度语言的新型多语言数据集。2. 提出了一个快速、鲁棒、轻量级的旋转分类管道，该管道基于Phi-3.5-Vision模型的视觉编码器，结合动态图像裁剪技术，并针对4类旋转任务进行了独立微调。", "result": "该方法在ORB-En数据集上实现了96%的旋转识别准确率，在ORB-Indic数据集上实现了92%的准确率。此外，在模拟的真实世界环境中，该模块显著提升了OCR性能：闭源模型提升高达14%，开源模型提升高达4倍。", "conclusion": "所提出的旋转分类模块在识别文档旋转方面表现出高精度和鲁棒性，并通过作为关键预处理步骤，显著提高了OCR模型（包括闭源和开源）在多语言和真实世界场景中的性能。"}}
{"id": "2511.04248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04248", "abs": "https://arxiv.org/abs/2511.04248", "authors": ["Salma Mekaoui", "Hiba Sofyan", "Imane Amaaz", "Imane Benchrif", "Arsalane Zarghili", "Ilham Chaker", "Nikola S. Nikolov"], "title": "Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models", "comment": null, "summary": "Extracting topics from text has become an essential task, especially with the\nrapid growth of unstructured textual data. Most existing works rely on highly\ncomputational methods to address this challenge. In this paper, we argue that\nprobabilistic and statistical approaches, such as topic modeling (TM), can\noffer effective alternatives that require fewer computational resources. TM is\na statistical method that automatically discovers topics in large collections\nof unlabeled text; however, it produces topics as distributions of\nrepresentative words, which often lack clear interpretability. Our objective is\nto perform topic labeling by assigning meaningful labels to these sets of\nwords. To achieve this without relying on computationally expensive models, we\npropose a graph-based approach that not only enriches topic words with\nsemantically related terms but also explores the relationships among them. By\nanalyzing these connections within the graph, we derive suitable labels that\naccurately capture each topic's meaning. We present a comparative study between\nour proposed method and several benchmarks, including ChatGPT-3.5, across two\ndifferent datasets. Our method achieved consistently better results than\ntraditional benchmarks in terms of BERTScore and cosine similarity and produced\nresults comparable to ChatGPT-3.5, while remaining computationally efficient.\nFinally, we discuss future directions for topic labeling and highlight\npotential research avenues for enhancing interpretability and automation.", "AI": {"tldr": "本文提出了一种图基方法，通过丰富主题词并探索其关系来为主题模型生成的主题词分配有意义的标签，从而提高主题的可解释性，同时保持计算效率，并取得了优于传统基准和与ChatGPT-3.5相当的结果。", "motivation": "随着非结构化文本数据快速增长，主题提取变得至关重要。现有方法通常计算成本高昂。虽然概率统计方法（如主题建模）计算效率更高，但它们生成的主题是词汇分布，缺乏清晰的可解释性。研究目标是为这些词汇集合分配有意义的标签，且不依赖计算密集型模型。", "method": "提出了一种图基方法。该方法不仅用语义相关的词汇丰富主题词，还探索这些词汇之间的关系。通过分析图中的连接，从而推导出能准确捕捉每个主题含义的标签。", "result": "在两个不同数据集上的比较研究表明，所提出的方法在BERTScore和余弦相似度方面始终优于传统基准，并取得了与ChatGPT-3.5相当的结果，同时保持了计算效率。", "conclusion": "所提出的图基主题标注方法在提高主题可解释性方面表现出色，且计算效率高。未来研究方向包括进一步增强主题标注的解释性和自动化。"}}
{"id": "2511.04584", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04584", "abs": "https://arxiv.org/abs/2511.04584", "authors": ["Daniel Gomm", "Cornelius Wolff", "Madelon Hulsebos"], "title": "Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis", "comment": "Accepted to the AI for Tabular Data workshop at EurIPS 2025", "summary": "Natural language interfaces to tabular data must handle ambiguities inherent\nto queries. Instead of treating ambiguity as a deficiency, we reframe it as a\nfeature of cooperative interaction, where the responsibility of query\nspecification is shared among the user and the system. We develop a principled\nframework distinguishing cooperative queries, i.e., queries that yield a\nresolvable interpretation, from uncooperative queries that cannot be resolved.\nApplying the framework to evaluations for tabular question answering and\nanalysis, we analyze the queries in 15 popular datasets, and observe an\nuncontrolled mixing of query types neither adequate for evaluating a system's\nexecution accuracy nor for evaluating interpretation capabilities. Our\nframework and analysis of queries shifts the perspective from fixing ambiguity\nto embracing cooperation in resolving queries. This reflection enables more\ninformed design and evaluation for natural language interfaces for tabular\ndata, for which we outline implications and directions for future research.", "AI": {"tldr": "本文将自然语言接口（NLI）中对表格数据的模糊查询重新定义为合作交互的特征，提出一个框架来区分可解析和不可解析的查询。通过分析现有数据集，发现查询类型混合，不利于评估。作者主张在查询解析中拥抱合作，以指导NLI的设计和评估。", "motivation": "目前的自然语言接口在处理表格数据查询时面临固有的歧义性。研究动机在于不将歧义视为缺陷，而是将其重构为合作交互的一个特征，即用户和系统共同承担查询规范的责任。", "method": "开发了一个原则性框架，用于区分“合作查询”（可解析的）和“非合作查询”（无法解析的）。将该框架应用于表格问答和分析的评估中，分析了15个流行数据集中的查询。", "result": "通过分析发现，现有数据集中查询类型混合失控，既不足以评估系统的执行准确性，也不足以评估其解释能力。该框架和查询分析将视角从“修复歧义”转向“在查询解析中拥抱合作”。", "conclusion": "这项反思能够为表格数据的自然语言接口带来更明智的设计和评估，并为未来的研究提供了启示和方向。通过理解和利用合作性，可以更好地构建和评估NLIs。"}}
{"id": "2511.04583", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04583", "abs": "https://arxiv.org/abs/2511.04583", "authors": ["Atsuyuki Miyai", "Mashiro Toyooka", "Takashi Otonari", "Zaiying Zhao", "Kiyoharu Aizawa"], "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper", "comment": "Issues, comments, and questions are all welcome in\n  https://github.com/Agent4Science-UTokyo/Jr.AI-Scientist", "summary": "Understanding the current capabilities and risks of AI Scientist systems is\nessential for ensuring trustworthy and sustainable AI-driven scientific\nprogress while preserving the integrity of the academic ecosystem. To this end,\nwe develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system\nthat mimics the core research workflow of a novice student researcher: Given\nthe baseline paper from the human mentor, it analyzes its limitations,\nformulates novel hypotheses for improvement, validates them through rigorous\nexperimentation, and writes a paper with the results. Unlike previous\napproaches that assume full automation or operate on small-scale code, Jr. AI\nScientist follows a well-defined research workflow and leverages modern coding\nagents to handle complex, multi-file implementations, leading to scientifically\nvaluable contributions. For evaluation, we conducted automated assessments\nusing AI Reviewers, author-led evaluations, and submissions to Agents4Science,\na venue dedicated to AI-driven scientific contributions. The findings\ndemonstrate that Jr. AI Scientist generates papers receiving higher review\nscores than existing fully automated systems. Nevertheless, we identify\nimportant limitations from both the author evaluation and the Agents4Science\nreviews, indicating the potential risks of directly applying current AI\nScientist systems and key challenges for future research. Finally, we\ncomprehensively report various risks identified during development. We hope\nthese insights will deepen understanding of current progress and risks in AI\nScientist development.", "AI": {"tldr": "本文开发了Jr. AI Scientist，一个模拟新手研究员工作流程的自主AI科学家系统，能分析论文局限、提出假设、实验验证并撰写论文。评估显示其表现优于现有自动化系统，但也揭示了当前AI科学家系统的局限性和潜在风险。", "motivation": "为了确保AI驱动的科学进步值得信赖和可持续，同时维护学术生态系统的完整性，理解AI科学家系统的当前能力和风险至关重要。", "method": "开发了Jr. AI Scientist系统，它模仿新手学生研究员的核心工作流程：分析导师论文的局限性、提出改进假设、通过实验验证，并撰写包含结果的论文。该系统利用现代编码代理处理复杂的多文件实现。评估方法包括使用AI审稿人进行自动化评估、作者主导的评估以及向Agents4Science提交论文。", "result": "研究发现，Jr. AI Scientist生成的论文获得了比现有全自动化系统更高的审稿分数。然而，作者评估和Agents4Science的审稿结果也指出了重要的局限性，表明当前AI科学家系统直接应用的潜在风险，并揭示了未来研究的关键挑战。", "conclusion": "Jr. AI Scientist展示了AI科学家系统在模拟研究流程方面的进步，但同时也暴露了其显著的局限性和潜在风险。这些发现加深了对AI科学家发展现状和风险的理解，为未来的研究指明了方向。"}}
{"id": "2511.04190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04190", "abs": "https://arxiv.org/abs/2511.04190", "authors": ["Josef Mayr", "Anna Reithmeir", "Maxime Di Folco", "Julia A. Schnabel"], "title": "Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification", "comment": "Preprint. Submitted to the IEEE International Symposium on Biomedical\n  Imaging (ISBI) 2026", "summary": "Covariance descriptors capture second-order statistics of image features.\nThey have shown strong performance in general computer vision tasks, but remain\nunderexplored in medical imaging. We investigate their effectiveness for both\nconventional and learning-based medical image classification, with a particular\nfocus on SPDNet, a classification network specifically designed for symmetric\npositive definite (SPD) matrices. We propose constructing covariance\ndescriptors from features extracted by pre-trained general vision encoders\n(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and\nMedSAM - are evaluated across eleven binary and multi-class datasets from the\nMedMNSIT benchmark. Our results show that covariance descriptors derived from\nGVE features consistently outperform those derived from handcrafted features.\nMoreover, SPDNet yields superior performance to state-of-the-art methods when\ncombined with DINOv2 features. Our findings highlight the potential of\ncombining covariance descriptors with powerful pretrained vision encoders for\nmedical image analysis.", "AI": {"tldr": "本研究探索了协方差描述符在医学图像分类中的应用，发现结合预训练通用视觉编码器（如DINOv2）的协方差描述符，特别是与SPDNet结合时，能显著优于手工特征和现有最先进方法。", "motivation": "协方差描述符在通用计算机视觉任务中表现出色，但在医学成像领域尚未得到充分探索。本研究旨在调查其在医学图像分类中的有效性，并特别关注为对称正定（SPD）矩阵设计的分类网络SPDNet。", "method": "研究从预训练的通用视觉编码器（GVEs，如DINOv2和MedSAM）提取的特征构建协方差描述符，并与手工描述符进行比较。评估在MedMNSIT基准测试的十一个二分类和多分类数据集上进行。性能通过SPDNet以及其他最先进方法进行比较。", "result": "结果显示，源自GVE特征的协方差描述符始终优于源自手工特征的描述符。此外，当SPDNet与DINOv2特征结合时，其性能超越了最先进的方法。", "conclusion": "研究结果强调了将协方差描述符与强大的预训练视觉编码器相结合，在医学图像分析中具有巨大潜力。"}}
{"id": "2511.04234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04234", "abs": "https://arxiv.org/abs/2511.04234", "authors": ["Alex Fang", "Thomas Voice", "Ruoming Pang", "Ludwig Schmidt", "Tom Gunter"], "title": "Reusing Pre-Training Data at Test Time is a Compute Multiplier", "comment": null, "summary": "Large language models learn from their vast pre-training corpora, gaining the\nability to solve an ever increasing variety of tasks; yet although researchers\nwork to improve these datasets, there is little effort to understand how\nefficient the pre-training apparatus is at extracting ideas and knowledge from\nthe data. In this work, we use retrieval augmented generation along with\ntest-time compute as a way to quantify how much dataset value was left behind\nby the process of pre-training, and how this changes across scale. We\ndemonstrate that pre-training then retrieving from standard and largely\nopen-sourced datasets results in significant accuracy gains in MMLU, Math-500,\nand SimpleQA, which persist through decontamination. For MMLU we observe that\nretrieval acts as a ~5x compute multiplier versus pre-training alone. We show\nthat these results can be further improved by leveraging additional compute at\ntest time to parse the retrieved context, demonstrating a 10 percentage point\nimprovement on MMLU for the public LLaMA 3.1 8B model. Overall, our results\nsuggest that today's pre-training methods do not make full use of the\ninformation in existing pre-training datasets, leaving significant room for\nprogress.", "AI": {"tldr": "研究表明，当前大型语言模型的预训练方法未能充分利用训练数据中的信息。通过检索增强生成（RAG），模型在多个基准测试中获得显著的准确性提升，相当于预训练计算量的约5倍效率提升，揭示了预训练数据价值的巨大未开发潜力。", "motivation": "尽管研究人员致力于改进大型语言模型的预训练数据集，但很少有工作关注预训练过程从数据中提取思想和知识的效率。本研究旨在量化预训练过程遗留了多少数据集价值，以及这种效率如何随模型规模变化。", "method": "本研究采用检索增强生成（RAG）结合测试时计算，来量化预训练过程未充分利用的数据集价值。通过在标准开源数据集上进行预训练后检索，并在MMLU、Math-500和SimpleQA等基准测试上评估模型的准确性增益，并进行去污染处理以确保结果的有效性。此外，还探索了在测试时投入额外计算来解析检索到的上下文对性能的进一步提升。", "result": "结果显示，在标准数据集上进行预训练后使用检索增强，可在MMLU、Math-500和SimpleQA上实现显著的准确性提升，且这种提升在去污染后依然存在。对于MMLU，检索的作用相当于单独预训练的约5倍计算量效率提升。通过在测试时利用额外计算来解析检索到的上下文，性能可进一步提高，例如在MMLU上使公开的LLaMA 3.1 8B模型获得10个百分点的提升。", "conclusion": "本研究结果表明，当前大型语言模型的预训练方法未能充分利用现有预训练数据集中的全部信息，存在巨大的改进空间。这暗示了通过更有效的数据利用策略，可以显著提升模型的性能和效率。"}}
{"id": "2511.04256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04256", "abs": "https://arxiv.org/abs/2511.04256", "authors": ["Kun Yang", "Zikang chen", "Yanmeng Wang", "Zhigen Li"], "title": "SSPO: Subsentence-level Policy Optimization", "comment": null, "summary": "As a significant part of post-training of the Large Language Models (LLMs),\nReinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'\nreasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative\nPolicy Optimization) and GSPO (Group Sequence Policy Optimization), are\nobserved to suffer from unstable policy updates and low usage of sampling data,\nrespectively. The importance ratio of GRPO is calculated at the token level,\nwhich focuses more on optimizing a single token. This will be easily affected\nby outliers, leading to model training collapse. GSPO proposed the calculation\nof the response level importance ratio, which solves the problem of high\nvariance and training noise accumulation in the calculation of the GRPO\nimportance ratio. However, since all the response tokens share a common\nimportance ratio, extreme values can easily raise or lower the overall mean,\nleading to the entire response being mistakenly discarded, resulting in a\ndecrease in the utilization of sampled data. This paper introduces SSPO, which\napplies sentence-level importance ratio, taking the balance between GRPO and\nGSPO. SSPO not only avoids training collapse and high variance, but also\nprevents the whole response tokens from being abandoned by the clipping\nmechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily\nadjust the clipping bounds, encouraging high-entropy tokens to explore and\nnarrow the clipping range of low-entropy tokens. In particular, SSPO achieves\nan average score of 46.57 across five datasets, surpassing GRPO (43.01) and\nGSPO (44.42), and wins state-of-the-art performance on three datasets. These\nresults highlight SSPO's effectiveness in leveraging generated data by taking\nthe essence of GSPO but rejecting its shortcomings.", "AI": {"tldr": "本文提出SSPO算法，通过引入句子级重要性比例和句子熵调整PPO-CLIP，解决了现有RLVR算法GRPO和GSPO在训练稳定性差和数据利用率低方面的问题，并在多个数据集上取得了最先进的性能。", "motivation": "现有LLM后训练中的RLVR算法存在缺陷：GRPO的token级重要性比例易受异常值影响导致训练崩溃；GSPO的response级重要性比例虽解决了高方差问题，但可能因极端值导致整个响应被错误丢弃，降低了采样数据利用率。因此，需要一种平衡训练稳定性和数据利用率的方法。", "method": "本文提出了SSPO（Sentence-level Sequence Policy Optimization）算法，它采用句子级重要性比例，旨在平衡GRPO和GSPO的优缺点。此外，SSPO将句子熵应用于PPO-CLIP，以动态调整裁剪边界，鼓励高熵token进行探索，并缩小低熵token的裁剪范围。", "result": "SSPO在五个数据集上的平均得分达到46.57，超过了GRPO（43.01）和GSPO（44.42），并在其中三个数据集上取得了最先进的性能。这些结果表明SSPO在利用生成数据方面的有效性，它汲取了GSPO的优点并避免了其缺点。", "conclusion": "SSPO通过采用句子级重要性比例和句子熵调整PPO-CLIP，有效解决了RLVR算法中训练不稳定和数据利用率低的问题，显著提高了LLM的推理能力，并取得了优于现有方法的性能。"}}
{"id": "2511.04171", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04171", "abs": "https://arxiv.org/abs/2511.04171", "authors": ["Fatemehzahra Darzi", "Rodrigo Escobar Diaz Guerrero", "Thomas Bocklitz"], "title": "Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology", "comment": "14 pages, 7 Figures", "summary": "Image registration refers to the process of spatially aligning two or more\nimages by mapping them into a common coordinate system, so that corresponding\nanatomical or tissue structures are matched across images. In digital\npathology, registration enables direct comparison and integration of\ninformation from different stains or imaging modalities, sup-porting\napplications such as biomarker analysis and tissue reconstruction. Accurate\nregistration of images from different modalities is an essential step in\ndigital pathology. In this study, we investigated how various color\ntransformation techniques affect image registration between hematoxylin and\neosin (H&E) stained images and non-linear multimodal images. We used a dataset\nof 20 tissue sample pairs, with each pair undergoing several preprocessing\nsteps, including different color transformation (CycleGAN, Macenko, Reinhard,\nVahadane), inversion, contrast adjustment, intensity normalization, and\ndenoising. All images were registered using the VALIS registration method,\nwhich first applies rigid registration and then performs non-rigid registration\nin two steps on both low and high-resolution images. Registration performance\nwas evaluated using the relative Target Registration Error (rTRE). We reported\nthe median of median rTRE values (MMrTRE) and the average of median rTRE values\n(AMrTRE) for each method. In addition, we performed a custom point-based\nevaluation using ten manually selected key points. Registration was done\nseparately for two scenarios, using either the original or inverted multimodal\nimages. In both scenarios, CycleGAN color transformation achieved the lowest\nregistration errors, while the other methods showed higher errors. These\nfindings show that applying color transformation before registration improves\nalignment between images from different modalities and supports more reliable\nanalysis in digital pathology.", "AI": {"tldr": "本研究调查了不同颜色转换技术对H&E染色图像与多模态图像配准性能的影响，发现CycleGAN在配准前应用时能显著降低配准误差。", "motivation": "数字病理学中，准确配准来自不同染色或成像模态的图像是生物标志物分析和组织重建等应用的关键步骤，因此需要研究如何优化异模态图像的配准效果。", "method": "研究使用了20对组织样本，每对样本都经过多种预处理步骤，包括不同的颜色转换（CycleGAN、Macenko、Reinhard、Vahadane）、反转、对比度调整、强度归一化和去噪。所有图像均使用VALIS方法进行配准（先刚性后两步非刚性）。配准性能通过相对目标配准误差（rTRE）以及自定义点基评估（10个手动选择的关键点）进行评估。配准在原始和反转多模态图像两种情景下分别进行。", "result": "在原始和反转多模态图像两种情景下，CycleGAN颜色转换均实现了最低的配准误差，而其他方法显示出更高的误差。", "conclusion": "研究结果表明，在配准前应用颜色转换，特别是CycleGAN，可以显著改善不同模态图像之间的对齐效果，从而支持更可靠的数字病理学分析。"}}
{"id": "2511.04406", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04406", "abs": "https://arxiv.org/abs/2511.04406", "authors": ["Mohammad Amin Ghanizadeh", "Mohammad Javad Dousti"], "title": "Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning", "comment": null, "summary": "Data quality and its effective selection are fundamental to improving the\nperformance of machine translation models, serving as cornerstones for\nachieving robust and reliable translation systems. This paper presents a data\nselection methodology specifically designed for fine-tuning machine translation\nsystems, which leverages the synergy between a learner model and a pre-trained\nreference model to enhance overall training effectiveness. By defining a\nlearnability score, our approach systematically evaluates the utility of data\npoints for training, ensuring that only the most relevant and impactful\nexamples contribute to the fine-tuning process. Furthermore, our method employs\na batch selection strategy which considers interdependencies among data points,\noptimizing the efficiency of the training process while maintaining a focus on\ndata relevance. Experiments on English to Persian and several other language\npairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that\nour method can achieve up to a fivefold improvement in data efficiency compared\nto an iid baseline. Experimental results indicate that our approach improves\ncomputational efficiency by 24 when utilizing cached embeddings, as it requires\nfewer training data points. Additionally, it enhances generalization, resulting\nin superior translation performance compared to random selection method.", "AI": {"tldr": "本文提出一种针对机器翻译系统微调的数据选择方法，通过学习能力分数和批次选择策略，显著提升了数据和计算效率，并改善了翻译性能和泛化能力。", "motivation": "数据质量和有效选择对于提升机器翻译模型性能，构建稳健可靠的翻译系统至关重要。", "method": "该方法利用学习器模型和预训练参考模型之间的协同作用，通过定义“学习能力分数”系统评估数据点的效用。此外，采用批次选择策略，考虑数据点之间的相互依赖性，以优化训练效率和数据相关性。", "result": "在英波语对及其他语言对上，使用mBART模型在CCMatrix数据集上进行微调的实验表明，该方法相比iid基线，数据效率提高了五倍。利用缓存嵌入时，计算效率提高了24%，并且泛化能力更强，翻译性能优于随机选择方法。", "conclusion": "所提出的数据选择方法能够显著提高机器翻译系统微调的数据效率和计算效率，同时增强模型的泛化能力，从而获得更优越的翻译性能。"}}
{"id": "2511.04192", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04192", "abs": "https://arxiv.org/abs/2511.04192", "authors": ["Hanmo Chen", "Chenghao Xu", "Jiexi Yan", "Cheng Deng"], "title": "AStF: Motion Style Transfer via Adaptive Statistics Fusor", "comment": null, "summary": "Human motion style transfer allows characters to appear less rigidity and\nmore realism with specific style. Traditional arbitrary image style transfer\ntypically process mean and variance which is proved effective. Meanwhile,\nsimilar methods have been adapted for motion style transfer. However, due to\nthe fundamental differences between images and motion, relying on mean and\nvariance is insufficient to fully capture the complex dynamic patterns and\nspatiotemporal coherence properties of motion data. Building upon this, our key\ninsight is to bring two more coefficient, skewness and kurtosis, into the\nanalysis of motion style. Specifically, we propose a novel Adaptive Statistics\nFusor (AStF) which consists of Style Disentanglement Module (SDM) and\nHigh-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in\nconjunction with a Motion Consistency Regularization (MCR) discriminator.\nExperimental results show that, by providing a more comprehensive model of the\nspatiotemporal statistical patterns inherent in dynamic styles, our proposed\nAStF shows proficiency superiority in motion style transfers over\nstate-of-the-arts. Our code and model are available at\nhttps://github.com/CHMimilanlan/AStF.", "AI": {"tldr": "本文提出了一种新颖的自适应统计融合器（AStF），通过引入偏度（skewness）和峰度（kurtosis）等高阶统计量，克服了传统均值和方差方法在运动风格迁移中的局限性，实现了更逼真和精确的运动风格迁移。", "motivation": "传统的图像风格迁移方法（基于均值和方差）在运动风格迁移中表现不足，因为运动数据的复杂动态模式和时空连贯性无法仅通过一阶和二阶统计量完全捕捉。为了更全面地捕捉运动风格的内在统计模式，需要引入更高阶的统计分析。", "method": "本文提出了一种名为自适应统计融合器（AStF）的新模型，该模型包含风格解耦模块（SDM）和高阶多统计注意力（HOS-Attn）模块。AStF的核心思想是将偏度和峰度这两个高阶统计系数引入运动风格分析。此外，AStF与一个运动一致性正则化（MCR）判别器协同训练。", "result": "实验结果表明，通过更全面地建模动态风格中固有的时空统计模式，所提出的AStF在运动风格迁移方面显示出优于现有最先进方法的卓越性能和熟练度。", "conclusion": "通过引入偏度和峰度等高阶统计量，并结合新颖的AStF模型和MCR判别器，本文成功解决了传统方法在捕捉复杂运动风格方面的不足，显著提升了运动风格迁移的真实感和性能。"}}
{"id": "2511.04588", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.04588", "abs": "https://arxiv.org/abs/2511.04588", "authors": ["Soham De", "Lodewijk Gelauff", "Ashish Goel", "Smitha Milli", "Ariel Procaccia", "Alice Siu"], "title": "Question the Questions: Auditing Representation in Online Deliberative Processes", "comment": null, "summary": "A central feature of many deliberative processes, such as citizens'\nassemblies and deliberative polls, is the opportunity for participants to\nengage directly with experts. While participants are typically invited to\npropose questions for expert panels, only a limited number can be selected due\nto time constraints. This raises the challenge of how to choose a small set of\nquestions that best represent the interests of all participants. We introduce\nan auditing framework for measuring the level of representation provided by a\nslate of questions, based on the social choice concept known as justified\nrepresentation (JR). We present the first algorithms for auditing JR in the\ngeneral utility setting, with our most efficient algorithm achieving a runtime\nof $O(mn\\log n)$, where $n$ is the number of participants and $m$ is the number\nof proposed questions. We apply our auditing methods to historical\ndeliberations, comparing the representativeness of (a) the actual questions\nposed to the expert panel (chosen by a moderator), (b) participants' questions\nchosen via integer linear programming, (c) summary questions generated by large\nlanguage models (LLMs). Our results highlight both the promise and current\nlimitations of LLMs in supporting deliberative processes. By integrating our\nmethods into an online deliberation platform that has been used for over\nhundreds of deliberations across more than 50 countries, we make it easy for\npractitioners to audit and improve representation in future deliberations.", "AI": {"tldr": "本文提出一个基于正当代表性（JR）的审计框架和算法，用于衡量审议过程中专家提问环节中问题集的代表性。研究将该框架应用于历史审议数据，比较了主持人选择、整数线性规划和大型语言模型（LLM）生成的问题的代表性，并揭示了LLM在此方面的潜力和局限性。最终，该方法被整合到一个在线审议平台中，以帮助实践者提升未来审议的代表性。", "motivation": "在公民大会和审议性民意调查等许多审议过程中，参与者有机会直接与专家互动。然而，由于时间限制，只能从大量提议问题中选择有限数量的问题，这引发了一个挑战：如何选择一小组问题，使其能最好地代表所有参与者的利益。", "method": "研究引入了一个基于社会选择概念“正当代表性”（JR）的审计框架，用于衡量问题集的代表性。开发了首个在通用效用设置下审计JR的算法，其中最有效的算法运行时间为O(mn log n)。将审计方法应用于历史审议数据，比较了三种问题选择方法的代表性：(a) 专家小组实际提问（由主持人选择），(b) 通过整数线性规划选择的参与者问题，(c) 大型语言模型（LLM）生成的摘要问题。最终，将这些方法整合到一个已被广泛使用的在线审议平台中。", "result": "研究首次提出了在通用效用设置下审计正当代表性（JR）的算法，其中最有效的算法运行时间为O(mn log n)。通过对历史审议数据的应用，比较了主持人选择、ILP选择和LLM生成的问题的代表性。结果突出了大型语言模型在支持审议过程方面的潜力和当前的局限性。", "conclusion": "本文为衡量和改进审议过程中问题集的代表性提供了新的审计框架和高效算法。研究结果表明，虽然大型语言模型在支持此类过程方面具有前景，但仍存在局限性。通过将这些方法整合到在线审议平台中，实践者可以更容易地审计和提升未来审议的代表性。"}}
{"id": "2511.04646", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.04646", "abs": "https://arxiv.org/abs/2511.04646", "authors": ["Narjes Nourzad", "Hanqing Yang", "Shiyu Chen", "Carlee Joe-Wong"], "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration", "comment": null, "summary": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies.", "AI": {"tldr": "DR. WELL是一个去中心化的神经符号框架，通过两阶段协商协议和符号规划，结合共享世界模型，实现多智能体协同规划，避免了轨迹层面的脆弱性，提高了任务完成率和效率。", "motivation": "合作多智能体规划面临信息不完全、通信受限以及轨迹层面协调容易失败的问题。符号规划通过提高抽象级别和提供最小行动词汇，可以缓解这些挑战，实现同步和集体进展。", "method": "DR. WELL框架采用两阶段协商协议：智能体首先提出候选角色并进行推理，然后在共识和环境约束下承诺联合分配。承诺后，每个智能体独立生成并执行其角色的符号计划。计划通过共享的世界模型与执行结果相结合，该模型编码当前状态并随智能体行动而更新。", "result": "DR. WELL通过对符号计划而非原始轨迹进行推理，避免了脆弱的步级对齐，实现了可重用、可同步、可解释的高级操作。在合作推块任务中的实验表明，智能体能跨回合适应，动态世界模型捕获可重用模式，提高了任务完成率和效率。通过协商和自我完善，动态世界模型改进了任务完成和效率，以时间开销换取了不断演进的、更高效的协作策略。", "conclusion": "DR. WELL框架通过去中心化的神经符号方法、两阶段协商和共享动态世界模型，有效解决了多智能体协同规划中的挑战，实现了鲁棒、高效且可解释的协作，并能通过学习演进更优的协作策略。"}}
{"id": "2511.04255", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04255", "abs": "https://arxiv.org/abs/2511.04255", "authors": ["Marawan Elbatel", "Anbang Wang", "Keyuan Liu", "Kaouther Mouheb", "Enrique Almar-Munoz", "Lizhuo Lin", "Yanqi Yang", "Karim Lekadir", "Xiaomeng Li"], "title": "MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection", "comment": null, "summary": "This paper does not introduce a novel architecture; instead, it revisits a\nfundamental yet overlooked baseline: adapting human-centric foundation models\nfor anatomical landmark detection in medical imaging. While landmark detection\nhas traditionally relied on domain-specific models, the emergence of\nlarge-scale pre-trained vision models presents new opportunities. In this\nstudy, we investigate the adaptation of Sapiens, a human-centric foundation\nmodel designed for pose estimation, to medical imaging through multi-dataset\npretraining, establishing a new state of the art across multiple datasets. Our\nproposed model, MedSapiens, demonstrates that human-centric foundation models,\ninherently optimized for spatial pose localization, provide strong priors for\nanatomical landmark detection, yet this potential has remained largely\nuntapped. We benchmark MedSapiens against existing state-of-the-art models,\nachieving up to 5.26% improvement over generalist models and up to 21.81%\nimprovement over specialist models in the average success detection rate (SDR).\nTo further assess MedSapiens adaptability to novel downstream tasks with few\nannotations, we evaluate its performance in limited-data settings, achieving\n2.69% improvement over the few-shot state of the art in SDR. Code and model\nweights are available at https://github.com/xmed-lab/MedSapiens .", "AI": {"tldr": "本文提出MedSapiens，通过多数据集预训练将以人为中心的姿态估计基础模型Sapiens应用于医学图像解剖地标检测，并在多项任务上取得了最先进的性能。", "motivation": "解剖地标检测传统上依赖于领域特定模型，但大规模预训练视觉模型的出现带来了新机遇。特别是，以人为中心的模型在空间姿态定位方面具有内在优势，但其在医学图像地标检测中的潜力尚未被充分发掘。", "method": "研究团队将Sapiens（一个以人为中心的姿态估计基础模型）通过多数据集预训练的方式，适配到医学图像领域，并将其命名为MedSapiens。该方法旨在利用Sapiens模型固有的空间姿态定位能力，作为解剖地标检测的强先验。", "result": "MedSapiens在多项数据集上建立了新的最先进水平。与通用模型相比，平均成功检测率（SDR）提高了5.26%；与专业模型相比，SDR提高了21.81%。在有限数据（few-shot）设置下，MedSapiens的SDR也比现有最先进方法提高了2.69%。", "conclusion": "研究表明，以人为中心的基础模型，由于其在空间姿态定位方面的优化，能为解剖地标检测提供强大的先验知识，其在医学图像领域的潜力值得进一步开发和利用。"}}
{"id": "2511.04662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04662", "abs": "https://arxiv.org/abs/2511.04662", "authors": ["Yu Feng", "Nathaniel Weir", "Kaj Bostrom", "Sam Bayless", "Darion Cassel", "Sapana Chaudhary", "Benjamin Kiesl-Reiter", "Huzefa Rangwala"], "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks", "comment": null, "summary": "LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but\nthey cannot reliably verify their own logic. Even when they reach correct\nanswers, the underlying reasoning may be flawed, undermining trust in\nhigh-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a\nneuro-symbolic method that extracts and verifies formal logical arguments from\nCoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order\nlogic and identifies premises that ground the argument in source context,\ncommonsense knowledge, or prior reasoning steps. The symbolic representation\nenables automated solvers to verify logical validity while the NL premises\nallow humans and systems to identify ungrounded or fallacious reasoning steps.\nExperiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT\neffectively identifies flawed reasoning, and serves as a strong predictor of\nfinal answer correctness. We also leverage VeriCoT's verification signal for\n(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on\nVeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct\npreference optimization (DPO) using verification-based pairwise rewards,\nfurther improving reasoning validity and accuracy.", "AI": {"tldr": "VeriCoT是一种神经符号方法，通过将大型语言模型（LLM）的思维链（CoT）推理形式化并验证其逻辑有效性，从而提高LLM推理的可信度和准确性。", "motivation": "LLM可以通过CoT进行多步推理，但它们无法可靠地验证自己的逻辑。即使答案正确，底层推理也可能存在缺陷，这在关键场景中会损害信任。", "method": "VeriCoT从CoT推理中提取并验证形式化的逻辑论证。它将每个CoT推理步骤形式化为一阶逻辑，并识别支撑论证的前提（源上下文、常识或先前的推理步骤）。符号表示允许自动化求解器验证逻辑有效性，而自然语言前提则允许人类和系统识别无根据或错误的推理步骤。此外，VeriCoT的验证信号被用于推理时的自我反思、在VeriCoT蒸馏数据集上的监督微调（SFT）以及使用基于验证的成对奖励进行直接偏好优化（DPO）的偏好微调（PFT）。", "result": "在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别有缺陷的推理，并能很好地预测最终答案的正确性。通过利用VeriCoT的验证信号进行自我反思、SFT和PFT，进一步提高了推理的有效性和准确性。", "conclusion": "VeriCoT作为一种神经符号方法，通过形式化和验证LLM的CoT推理，有效解决了LLM推理逻辑不可靠的问题，显著提升了推理的信任度、有效性和准确性。"}}
{"id": "2511.04476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04476", "abs": "https://arxiv.org/abs/2511.04476", "authors": ["Fabian Schmidt", "Seyedehmoniba Ravan", "Vladimir Vlassov"], "title": "Probabilistic Textual Time Series Depression Detection", "comment": "14 pages, 8 figures, 4 tables", "summary": "Accurate and interpretable predictions of depression severity are essential\nfor clinical decision support, yet existing models often lack uncertainty\nestimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time\nSeries Depression Detection framework that predicts PHQ-8 scores from\nutterance-level clinical interviews while modeling uncertainty over time. PTTSD\nincludes sequence-to-sequence and sequence-to-one variants, both combining\nbidirectional LSTMs, self-attention, and residual connections with Gaussian or\nStudent-t output heads trained via negative log-likelihood. Evaluated on E-DAIC\nand DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only\nsystems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated\nprediction intervals. Ablations confirm the value of attention and\nprobabilistic modeling, while comparisons with MentalBERT establish generality.\nA three-part calibration analysis and qualitative case studies further\nhighlight the interpretability and clinical relevance of uncertainty-aware\nforecasting.", "AI": {"tldr": "PTTSD是一个概率文本时间序列抑郁检测框架，通过建模不确定性来预测PHQ-8分数，并在文本专用系统中达到了最先进的性能，同时提供可解释的预测区间。", "motivation": "现有的抑郁症严重程度预测模型通常缺乏不确定性估计和时间序列建模能力，而这对于临床决策支持至关重要。", "method": "本文提出了PTTSD框架，它包括序列到序列和序列到一的变体。该框架结合了双向LSTM、自注意力机制和残差连接，并使用高斯或Student-t输出头，通过负对数似然进行训练，以预测PHQ-8分数并建模随时间变化的不确定性。", "result": "PTTSD在E-DAIC和DAIC-WOZ数据集上，在文本专用系统中取得了最先进的性能（例如，E-DAIC上的MAE为3.85，DAIC上的MAE为3.55），并生成了校准良好的预测区间。消融实验证实了注意力机制和概率建模的价值，与MentalBERT的比较则证明了其通用性。", "conclusion": "PTTSD通过提供不确定性估计的预测，增强了抑郁症严重程度预测的准确性和可解释性，并通过校准分析和定性案例研究进一步突出了其在临床上的相关性。"}}
{"id": "2511.04432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04432", "abs": "https://arxiv.org/abs/2511.04432", "authors": ["Lars Bungum", "Charles Yijia Huang", "Abeer Kashar"], "title": "If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs", "comment": "8 pages, 1 figure, 3 tables, submitted to aconference", "summary": "In this study, we experiment with the ability of LLMs to do temporal\nreasoning. Using a Norwegian book from 1940 containing trivia questions, we\nprompt the LLMs to answer the questions as if it were 1940. We also pose the\nquestions in both English and Norwegian. Correct answers are often presented as\nsentences, and grading is done by means of LLM-as-judge, with sampled checks by\na native speaker. Prompting in English consistently gave better results than in\nNorwegian, an unexpected result. In contrast, using larger LLMs improved\nresults. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,\nand also the largest available LLM especially crafted for Norwegian.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）的时间推理能力，发现使用1940年的挪威问答集时，英语提示比挪威语提示效果更好，且更大的LLM表现更佳。", "motivation": "研究旨在实验LLMs进行时间推理的能力，即在特定历史背景（1940年）下回答问题的表现。", "method": "研究使用了一本1940年的挪威问答书，要求LLMs模拟1940年的情境回答问题。问题同时以英语和挪威语提出。答案由LLM作为裁判进行评分，并由母语者抽样检查。测试的模型包括DeepSeek-R1、Gemma3、Qwen3、Llama3.1系列以及一个专门为挪威语设计的最大LLM。", "result": "结果显示，英语提示始终比挪威语提示获得更好的表现，这是一个出乎意料的发现。此外，使用更大的LLM能够提高结果。", "conclusion": "LLMs在时间推理方面表现出不同能力，其中英语提示在处理特定历史背景的挪威语内容时表现优于挪威语提示，且模型规模对性能有积极影响。"}}
{"id": "2511.04479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04479", "abs": "https://arxiv.org/abs/2511.04479", "authors": ["Surapon Nonesung", "Teetouch Jaknamon", "Sirinya Chaiophat", "Natapong Nitarach", "Chanakan Wittayasakpan", "Warit Sirichotedumrong", "Adisai Na-Thalang", "Kunat Pipatanakul"], "title": "ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai", "comment": "Accepted at the IJCNLP-AACL 2025 (Main)", "summary": "We present ThaiOCRBench, the first comprehensive benchmark for evaluating\nvision-language models (VLMs) on Thai text-rich visual understanding tasks.\nDespite recent progress in multimodal modeling, existing benchmarks\npredominantly focus on high-resource languages, leaving Thai underrepresented,\nespecially in tasks requiring document structure understanding. ThaiOCRBench\naddresses this gap by offering a diverse, human-annotated dataset comprising\n2,808 samples across 13 task categories. We evaluate a wide range of\nstate-of-the-art VLMs in a zero-shot setting, spanning both proprietary and\nopen-source systems. Results show a significant performance gap, with\nproprietary models (e.g., Gemini 2.5 Pro) outperforming open-source\ncounterparts. Notably, fine-grained text recognition and handwritten content\nextraction exhibit the steepest performance drops among open-source models.\nThrough detailed error analysis, we identify key challenges such as language\nbias, structural mismatch, and hallucinated content. ThaiOCRBench provides a\nstandardized framework for assessing VLMs in low-resource, script-complex\nsettings, and provides actionable insights for improving Thai-language document\nunderstanding.", "AI": {"tldr": "该论文提出了ThaiOCRBench，首个用于评估视觉语言模型在泰语文本理解任务上的综合基准。它包含2,808个人工标注样本，涵盖13个任务类别，并揭示了专有模型与开源模型之间的显著性能差距，尤其是在泰语文档结构理解方面。", "motivation": "尽管多模态建模取得了进展，但现有基准主要集中在高资源语言，导致泰语等低资源语言，特别是在需要文档结构理解的任务中，代表性不足。", "method": "研究人员构建了ThaiOCRBench，一个包含2,808个样本、涵盖13个任务类别的人工标注数据集。他们使用零样本设置评估了广泛的最新视觉语言模型（包括专有和开源系统），并进行了详细的错误分析。", "result": "评估结果显示，专有模型（如Gemini 2.5 Pro）在性能上显著优于开源模型。在开源模型中，细粒度文本识别和手写内容提取任务的性能下降最为剧烈。研究还发现了语言偏差、结构不匹配和内容幻觉等关键挑战。", "conclusion": "ThaiOCRBench为在低资源、复杂脚本环境中评估视觉语言模型提供了一个标准化框架，并为改进泰语文档理解提供了可行的见解。"}}
{"id": "2511.04281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04281", "abs": "https://arxiv.org/abs/2511.04281", "authors": ["Yujie Yang", "Shuang Li", "Jun Ye", "Neng Dong", "Fan Li", "Huafeng Li"], "title": "DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification", "comment": null, "summary": "Video-based Visible-Infrared person re-identification (VVI-ReID) aims to\nretrieve the same pedestrian across visible and infrared modalities from video\nsequences. Existing methods tend to exploit modality-invariant visual features\nbut largely overlook gait features, which are not only modality-invariant but\nalso rich in temporal dynamics, thus limiting their ability to model the\nspatiotemporal consistency essential for cross-modal video matching. To address\nthese challenges, we propose a DINOv2-Driven Gait Representation Learning\n(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn\ngait features complementary to appearance cues, facilitating robust\nsequence-level representations for cross-modal retrieval. Specifically, we\nintroduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which\ngenerates and enhances silhouette representations with general-purpose semantic\npriors from DINOv2 and jointly optimizes them with the ReID objective to\nachieve semantically enriched and task-adaptive gait feature learning.\nFurthermore, we develop a Progressive Bidirectional Multi-Granularity\nEnhancement (PBMGE) module, which progressively refines feature representations\nby enabling bidirectional interactions between gait and appearance streams\nacross multiple spatial granularities, fully leveraging their complementarity\nto enhance global representations with rich local details and produce highly\ndiscriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets\ndemonstrate the superiority of our approach, significantly outperforming\nexisting state-of-the-art methods.", "AI": {"tldr": "本文提出DinoGRL框架，利用DINOv2的视觉先验学习步态特征，并结合外观线索，以增强视频可见光-红外跨模态行人重识别（VVI-ReID）的序列级表示。", "motivation": "现有VVI-ReID方法倾向于利用模态不变的视觉特征，但忽略了步态特征。步态特征不仅模态不变，而且富含时间动态性，对于建模跨模态视频匹配所需的时空一致性至关重要，因此现有方法在这方面存在局限性。", "method": "提出DINOv2驱动的步态表示学习（DinoGRL）框架。具体包括：1) 语义感知剪影与步态学习（SASGL）模型，利用DINOv2的通用语义先验生成和增强剪影表示，并与ReID目标联合优化以学习语义丰富且任务自适应的步态特征。2) 渐进式双向多粒度增强（PBMGE）模块，通过在步态和外观流之间进行多空间粒度上的双向交互，渐进式地细化特征表示，充分利用它们的互补性，以丰富全局表示并生成高判别性特征。", "result": "在HITSZ-VCM和BUPT数据集上进行了广泛实验，结果表明该方法优于现有最先进的方法。", "conclusion": "所提出的DinoGRL框架通过有效利用DINOv2的视觉先验和结合步态与外观特征的互补性，显著提高了视频可见光-红外行人重识别的性能，成功解决了跨模态视频匹配中时空一致性建模的挑战。"}}
{"id": "2511.04491", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04491", "abs": "https://arxiv.org/abs/2511.04491", "authors": ["Nikhil Abhyankar", "Purvi Chaurasia", "Sanchit Kabra", "Ananya Srivastava", "Vivek Gupta", "Chandan K. Reddy"], "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables", "comment": null, "summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform\ntables, underrepresenting the complexity of real-world data and giving an\nincomplete view of Large Language Models' (LLMs) reasoning abilities. Real\ntables are long, heterogeneous, and domain-specific, mixing structured fields\nwith free text and requiring multi-hop reasoning across thousands of tokens. To\naddress this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from\n2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)\nand ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates\nLLMs jointly across scale, heterogeneity, domain specificity, and reasoning\ncomplexity. Experiments with open-source and proprietary models show that LLMs\nstruggle with heterogeneous schemas and complex multi-hop inference, revealing\npersistent weaknesses in current architectures and prompting strategies.\nRUST-BENCH establishes a challenging new testbed for advancing tabular\nreasoning research.", "AI": {"tldr": "RUST-BENCH是一个新的基准测试，用于评估大型语言模型（LLMs）在复杂、异构的真实世界表格上的推理能力，并揭示了LLMs在此类任务中的不足。", "motivation": "现有的表格推理基准测试主要针对小型、统一的表格，未能充分代表真实世界数据的复杂性，导致对LLMs推理能力的评估不完整。真实表格通常较长、异构、领域特定，并混合了结构化字段和自由文本，需要跨数千个token进行多跳推理。", "method": "引入RUST-BENCH，一个包含7966个问题、来自2031个真实世界表格的基准测试，涵盖两个领域：RB-Science（NSF拨款记录）和RB-Sports（NBA统计数据）。该基准测试在规模、异构性、领域特异性和推理复杂性方面对LLMs进行联合评估。", "result": "对开源和专有模型的实验表明，LLMs在异构模式和复杂的多跳推理方面表现不佳，揭示了当前架构和提示策略中持续存在的弱点。", "conclusion": "RUST-BENCH为推进表格推理研究建立了一个具有挑战性的新测试平台。"}}
{"id": "2511.04283", "categories": ["cs.CV", "68T40(Primary)68T45, 68U99 (Secondary)", "I.4.8; I.3.7"], "pdf": "https://arxiv.org/pdf/2511.04283", "abs": "https://arxiv.org/abs/2511.04283", "authors": ["Shiwei Ren", "Tianci Wen", "Yongchun Fang", "Biao Lu"], "title": "FastGS: Training 3D Gaussian Splatting in 100 Seconds", "comment": "Project page: https://fastgs.github.io/", "summary": "The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to\nproperly regulate the number of Gaussians during training, causing redundant\ncomputational time overhead. In this paper, we propose FastGS, a novel, simple,\nand general acceleration framework that fully considers the importance of each\nGaussian based on multi-view consistency, efficiently solving the trade-off\nbetween training time and rendering quality. We innovatively design a\ndensification and pruning strategy based on multi-view consistency, dispensing\nwith the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &\nTemples, and Deep Blending datasets demonstrate that our method significantly\noutperforms the state-of-the-art methods in training speed, achieving a\n3.32$\\times$ training acceleration and comparable rendering quality compared\nwith DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\\times$ acceleration\ncompared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that\nFastGS exhibits strong generality, delivering 2-7$\\times$ training acceleration\nacross various tasks, including dynamic scene reconstruction, surface\nreconstruction, sparse-view reconstruction, large-scale reconstruction, and\nsimultaneous localization and mapping. The project page is available at\nhttps://fastgs.github.io/", "AI": {"tldr": "FastGS提出了一种基于多视角一致性的3D高斯泼溅加速框架，通过高效的稠密化和剪枝策略，显著提升了训练速度并保持了渲染质量，同时具有强大的通用性。", "motivation": "现有的3D高斯泼溅加速方法未能有效控制高斯数量，导致冗余的计算时间开销。", "method": "FastGS提出了一种新颖、简单且通用的加速框架，该框架基于多视角一致性充分考虑每个高斯的重要性。它创新性地设计了基于多视角一致性的稠密化和剪枝策略，摒弃了预算机制，以有效平衡训练时间和渲染质量。", "result": "FastGS在Mip-NeRF 360数据集上实现了3.32倍的训练加速，渲染质量与DashGaussian相当；在Deep Blending数据集上比原始3DGS加速15.45倍。它在动态场景重建、表面重建、稀疏视角重建、大规模重建和同步定位与建图等多种任务中展现出强大的通用性，实现了2-7倍的训练加速。", "conclusion": "FastGS是一个高效且通用的3D高斯泼溅加速框架，通过创新的多视角一致性策略，显著提升了训练速度，同时保持了高质量的渲染效果，并能广泛应用于多种三维重建任务。"}}
{"id": "2511.04495", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04495", "abs": "https://arxiv.org/abs/2511.04495", "authors": ["Cuong Huynh", "Jie Cao"], "title": "OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation", "comment": "Accepted to TSAR 2025 Workshop at EMNLP2025", "summary": "This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task\n(Alva-Manchego et al., 2025), designed for readability-controlled text\nsimplification using LLM-prompting-based generation. Based on the analysis of\nprompt-based text simplification methods, we discovered an interesting finding\nthat text simplification performance is highly related to the gap between the\nsource CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by\nthis finding, we propose two multi-round simplification methods and generate\nthem via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based\nLLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.\nLater improvements with MRS-Joint show that taking the LLM simplified\ncandidates as the starting point could further boost the multi-round\nsimplification performance.", "AI": {"tldr": "本文介绍了OUNLP系统，该系统使用基于LLM提示的多轮简化方法进行可读性控制的文本简化，其灵感来源于简化性能与源文本和目标文本的CEFR级别差距高度相关的发现。", "motivation": "研究动机是实现可读性控制的文本简化，并发现文本简化性能与源文本和目标文本之间的CEFR级别差距密切相关。", "method": "该研究采用基于LLM提示的生成（使用GPT-4o），并提出了两种多轮简化方法：基于规则的简化（MRS-Rule）和联合基于规则的LLM简化（MRS-Joint）。后续改进是将LLM简化的候选文本作为起点，进一步提升多轮简化性能。", "result": "OUNLP系统在TSAR-2025共享任务中排名第7（共20支队伍）。后期通过MRS-Joint方法（将LLM简化候选文本作为起点）的改进，进一步提升了多轮简化性能。", "conclusion": "多轮简化方法，特别是以LLM简化候选文本作为起点的策略，能有效提高可读性控制的文本简化性能，且源文本与目标文本的CEFR级别差距是影响简化性能的重要因素。"}}
{"id": "2511.04260", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04260", "abs": "https://arxiv.org/abs/2511.04260", "authors": ["Claudio Giusti", "Luca Guarnera", "Sebastiano Battiato"], "title": "Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery", "comment": "13 pages, 6 figures, 5 tables", "summary": "The growing sophistication of synthetic image and deepfake generation models\nhas turned source attribution and authenticity verification into a critical\nchallenge for modern computer vision systems. Recent studies suggest that\ndiffusion pipelines unintentionally imprint persistent statistical traces,\nknown as signal leaks, within their outputs, particularly in latent\nrepresentations. Building on this observation, we propose Proto-LeakNet, a\nsignal-leak-aware and interpretable attribution framework that integrates\nclosed-set classification with a density-based open-set evaluation on the\nlearned embeddings, enabling analysis of unseen generators without retraining.\nOperating in the latent domain of diffusion models, our method re-simulates\npartial forward diffusion to expose residual generator-specific cues. A\ntemporal attention encoder aggregates multi-step latent features, while a\nfeature-weighted prototype head structures the embedding space and enables\ntransparent attribution. Trained solely on closed data and achieving a Macro\nAUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under\npost-processing, surpassing state-of-the-art methods, and achieves strong\nseparability between known and unseen generators. These results demonstrate\nthat modeling signal-leak bias in latent space enables reliable and\ninterpretable AI-image and deepfake forensics. The code for the whole work will\nbe available upon submission.", "AI": {"tldr": "Proto-LeakNet是一个可解释的归因框架，它利用扩散模型在潜在空间中留下的“信号泄漏”来识别AI生成图像的来源，即使是未知的生成器也能有效识别，并对图像后处理具有鲁棒性。", "motivation": "随着合成图像和深度伪造生成模型的日益复杂，确定图像来源和验证其真实性成为计算机视觉领域的关键挑战。现有研究表明，扩散模型在其输出中会无意中留下持久的统计痕迹（即“信号泄漏”），尤其是在潜在表示中，这为源头归因提供了可能。", "method": "本文提出了Proto-LeakNet框架。该方法在扩散模型的潜在域中操作，通过重新模拟部分前向扩散来揭示生成器特定的残余线索。它采用一个时间注意力编码器来聚合多步潜在特征，并使用一个特征加权原型头来构建嵌入空间，实现透明归因。该框架结合了闭集分类和基于密度的开集评估，使其能够在不重新训练的情况下分析未知的生成器。", "result": "Proto-LeakNet仅在闭集数据上训练，实现了98.13%的Macro AUC。其学习到的潜在几何结构在经过后处理后仍保持鲁棒性，性能超越了现有最先进的方法。该方法在已知和未知生成器之间展现出强大的可分离性。", "conclusion": "研究结果表明，在潜在空间中对“信号泄漏”偏差进行建模，能够实现可靠且可解释的AI图像和深度伪造取证。"}}
{"id": "2511.04506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04506", "abs": "https://arxiv.org/abs/2511.04506", "authors": ["Paloma Rabaey", "Jong Hak Moon", "Jung-Oh Lee", "Min Gwan Kim", "Hangyul Yoon", "Thomas Demeester", "Edward Choi"], "title": "Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways", "comment": null, "summary": "Radiology reports are invaluable for clinical decision-making and hold great\npotential for automated analysis when structured into machine-readable formats.\nThese reports often contain uncertainty, which we categorize into two distinct\ntypes: (i) Explicit uncertainty reflects doubt about the presence or absence of\nfindings, conveyed through hedging phrases. These vary in meaning depending on\nthe context, making rule-based systems insufficient to quantify the level of\nuncertainty for specific findings; (ii) Implicit uncertainty arises when\nradiologists omit parts of their reasoning, recording only key findings or\ndiagnoses. Here, it is often unclear whether omitted findings are truly absent\nor simply unmentioned for brevity. We address these challenges with a two-part\nframework. We quantify explicit uncertainty by creating an expert-validated,\nLLM-based reference ranking of common hedging phrases, and mapping each finding\nto a probability value based on this reference. In addition, we model implicit\nuncertainty through an expansion framework that systematically adds\ncharacteristic sub-findings derived from expert-defined diagnostic pathways for\n14 common diagnoses. Using these methods, we release Lunguage++, an expanded,\nuncertainty-aware version of the Lunguage benchmark of fine-grained structured\nradiology reports. This enriched resource enables uncertainty-aware image\nclassification, faithful diagnostic reasoning, and new investigations into the\nclinical impact of diagnostic uncertainty.", "AI": {"tldr": "本文提出一个两部分框架，量化放射学报告中的显式和隐式不确定性，并发布了Lunguage++，一个扩展的、不确定性感知的结构化放射学报告基准。", "motivation": "放射学报告对临床决策至关重要，但其中包含的显式（通过模糊词表达）和隐式（省略推理）不确定性阻碍了自动化分析和结构化。现有的基于规则的系统不足以量化这些不确定性。", "method": "针对显式不确定性，本文创建了一个专家验证的、基于大型语言模型（LLM）的常用模糊短语参考排名，并将每个发现映射到基于该排名的概率值。针对隐式不确定性，本文通过一个扩展框架，系统地从14种常见诊断的专家定义诊断路径中添加特征性子发现。", "result": "通过上述方法，本文发布了Lunguage++，这是Lunguage基准的一个扩展的、不确定性感知的版本，其中包含细粒度的结构化放射学报告。", "conclusion": "Lunguage++这一丰富资源能够实现不确定性感知的图像分类、忠实的诊断推理，并为诊断不确定性的临床影响提供新的研究途径。"}}
{"id": "2511.04317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04317", "abs": "https://arxiv.org/abs/2511.04317", "authors": ["Xiangjun Zhang", "Litong Gong", "Yinglin Zheng", "Yansong Liu", "Wentao Jiang", "Mingyi Xu", "Biao Wang", "Tiezheng Ge", "Ming Zeng"], "title": "RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation", "comment": "17 pages, 16 figures", "summary": "Most text-to-video(T2V) diffusion models depend on pre-trained text encoders\nfor semantic alignment, yet they often fail to maintain video quality when\nprovided with concise prompts rather than well-designed ones. The primary issue\nlies in their limited textual semantics understanding. Moreover, these text\nencoders cannot rephrase prompts online to better align with user intentions,\nwhich limits both the scalability and usability of the models, To address these\nchallenges, we introduce RISE-T2V, which uniquely integrates the processes of\nprompt rephrasing and semantic feature extraction into a single and seamless\nstep instead of two separate steps. RISE-T2V is universal and can be applied to\nvarious pre-trained LLMs and video diffusion models(VDMs), significantly\nenhancing their capabilities for T2V tasks. We propose an innovative module\ncalled the Rephrasing Adapter, enabling diffusion models to utilize text hidden\nstates during the next token prediction of the LLM as a condition for video\ngeneration. By employing a Rephrasing Adapter, the video generation model can\nimplicitly rephrase basic prompts into more comprehensive representations that\nbetter match the user's intent. Furthermore, we leverage the powerful\ncapabilities of LLMs to enable video generation models to accomplish a broader\nrange of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a\nversatile framework applicable to different video diffusion model\narchitectures, significantly enhancing the ability of T2V models to generate\nhigh-quality videos that align with user intent. Visual results are available\non the webpage at https://rise-t2v.github.io.", "AI": {"tldr": "RISE-T2V 提出了一种新颖的框架，通过将提示词改写和语义特征提取整合为一个步骤，显著提升了文本到视频（T2V）扩散模型在处理简洁提示词时生成高质量视频的能力。", "motivation": "现有的文本到视频扩散模型依赖预训练文本编码器，但在面对简洁而非精心设计的提示词时，视频质量难以保持，主要原因是它们对文本语义理解有限。此外，这些模型无法在线改写提示词以更好地符合用户意图，这限制了模型的可扩展性和可用性。", "method": "本文提出了 RISE-T2V 框架，它独特地将提示词改写和语义特征提取整合为一个无缝的步骤。核心是一个创新的“改写适配器”（Rephrasing Adapter），使扩散模型能够利用大型语言模型（LLM）在预测下一个词元时的隐藏状态作为视频生成的条件，从而隐式地将基本提示词改写为更全面的表示，以更好地匹配用户意图。", "result": "实验证明，RISE-T2V 是一个通用的框架，适用于各种预训练的 LLM 和视频扩散模型，显著增强了 T2V 模型生成与用户意图一致的高质量视频的能力。它在不同视频扩散模型架构上均表现出多功能性。", "conclusion": "RISE-T2V 通过集成提示词改写和语义特征提取，并利用 LLM 的强大能力，有效解决了现有 T2V 模型在理解简洁提示词方面的局限性，从而显著提高了视频生成质量和与用户意图的对齐程度，是一个通用且有效的 T2V 框架。"}}
{"id": "2511.04288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04288", "abs": "https://arxiv.org/abs/2511.04288", "authors": ["Leire Benito-Del-Valle", "Artzai Picón", "Daniel Mugica", "Manuel Ramos", "Eva Portillo", "Javier Romero", "Carlos Javier Jimenez", "Ramón Navarra-Mestre"], "title": "Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment", "comment": null, "summary": "Herbicide field trials require accurate identification of plant species and\nassessment of herbicide-induced damage across diverse environments. While\ngeneral-purpose vision foundation models have shown promising results in\ncomplex visual domains, their performance can be limited in agriculture, where\nfine-grained distinctions between species and damage types are critical.\n  In this work, we adapt a general-purpose vision foundation model to herbicide\ntrial characterization. Trained using a self-supervised learning approach on a\nlarge, curated agricultural dataset, the model learns rich and transferable\nrepresentations optimized for herbicide trials images.\n  Our domain-specific model significantly outperforms the best general-purpose\nfoundation model in both species identification (F1 score improvement from 0.91\nto 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions\n(new locations and other time), it achieves even greater gains (species\nidentification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In\ndomain-shift scenarios, such as drone imagery, it maintains strong performance\n(species classification from 0.49 to 0.60).\n  Additionally, we show that domain-specific pretraining enhances segmentation\naccuracy, particularly in low-annotation regimes. An annotation-efficiency\nanalysis reveals that, under unseen conditions, the domain-specific model\nachieves 5.4% higher F1 score than the general-purpose model, while using 80%\nfewer labeled samples.\n  These results demonstrate the generalization capabilities of domain-specific\nfoundation models and their potential to significantly reduce manual annotation\nefforts, offering a scalable and automated solution for herbicide trial\nanalysis.", "AI": {"tldr": "本研究通过自监督学习，在一个大型农业数据集上训练了一个领域特定的视觉基础模型，显著提升了除草剂试验中植物物种识别和损害分类的准确性，尤其是在未见条件和数据漂移场景下，并能有效减少标注工作。", "motivation": "除草剂田间试验需要准确识别植物物种和评估除草剂造成的损害，但通用视觉基础模型在农业领域面临挑战，难以区分细粒度的物种和损害类型。", "method": "本研究采用自监督学习方法，在一个大型精选农业数据集上训练了一个通用的视觉基础模型，使其适应除草剂试验的特性，学习针对该领域优化的丰富且可迁移的表征。", "result": "该领域特定模型在物种识别（F1从0.91提升到0.94）和损害分类（F1从0.26提升到0.33）方面均显著优于最佳通用基础模型。在未见条件（新地点和时间）下，性能提升更显著（物种识别F1从0.56提升到0.66；损害分类F1从0.17提升到0.27）。在领域漂移场景（如无人机图像）中，其性能依然强劲（物种分类F1从0.49提升到0.60）。此外，领域特定预训练提高了分割精度，尤其是在低标注情况下，并在使用80%更少标注样本的情况下，在未见条件下F1分数比通用模型高5.4%。", "conclusion": "领域特定的基础模型展现出强大的泛化能力，能够显著减少人工标注工作，为除草剂试验分析提供了一个可扩展且自动化的解决方案。"}}
{"id": "2511.04527", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04527", "abs": "https://arxiv.org/abs/2511.04527", "authors": ["Amir Zur", "Atticus Geiger", "Ekdeep Singh Lubana", "Eric Bigelow"], "title": "Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics", "comment": null, "summary": "When a language model generates text, the selection of individual tokens\nmight lead it down very different reasoning paths, making uncertainty difficult\nto quantify. In this work, we consider whether reasoning language models\nrepresent the alternate paths that they could take during generation. To test\nthis hypothesis, we use hidden activations to control and predict a language\nmodel's uncertainty during chain-of-thought reasoning. In our experiments, we\nfind a clear correlation between how uncertain a model is at different tokens,\nand how easily the model can be steered by controlling its activations. This\nsuggests that activation interventions are most effective when there are\nalternate paths available to the model -- in other words, when it has not yet\ncommitted to a particular final answer. We also find that hidden activations\ncan predict a model's future outcome distribution, demonstrating that models\nimplicitly represent the space of possible paths.", "AI": {"tldr": "本研究发现，语言模型的隐藏激活可以反映并控制其在推理过程中的不确定性，这表明模型在最终确定答案之前会隐式地表示潜在的替代推理路径。", "motivation": "语言模型在生成文本时，不同token的选择可能导致截然不同的推理路径，使得量化不确定性变得困难。研究旨在探讨推理语言模型是否在生成过程中表示了它们可能采取的替代路径。", "method": "通过使用隐藏激活来控制和预测语言模型在思维链推理过程中的不确定性。实验测试了模型在不同token处的不确定性与通过控制激活对其进行引导的难易程度之间的相关性。此外，还探究了隐藏激活是否能预测模型未来的结果分布。", "result": "研究发现，模型在不同token处的不确定性与其通过激活干预被引导的难易程度之间存在明显关联。这表明当模型存在替代路径时（即尚未确定最终答案时），激活干预最有效。同时，隐藏激活能够预测模型的未来结果分布。", "conclusion": "语言模型隐式地表示了可能的推理路径空间，并且隐藏激活能够反映并预测模型未来的行为。当模型尚未确定最终答案时，其内部状态（通过激活表示）会更灵活，更容易被引导。"}}
{"id": "2511.04499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04499", "abs": "https://arxiv.org/abs/2511.04499", "authors": ["Christos-Nikolaos Zacharopoulos", "Revekka Kyriakoglou"], "title": "Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering", "comment": "Accepted at IJCNLP-AACL 2025", "summary": "As Large Language Models (LLMs) become integral to human-centered\napplications, understanding their personality-like behaviors is increasingly\nimportant for responsible development and deployment. This paper systematically\nevaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to\nassess trait expressions under varying sampling temperatures. We find\nsignificant differences across four of the five personality dimensions, with\nNeuroticism and Extraversion susceptible to temperature adjustments. Further,\nhierarchical clustering reveals distinct model clusters, suggesting that\narchitectural features may predispose certain models toward stable trait\nprofiles. Taken together, these results offer new insights into the emergence\nof personality-like patterns in LLMs and provide a new perspective on model\ntuning, selection, and the ethical governance of AI systems. We share the data\nand code for this analysis here:\nhttps://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1", "AI": {"tldr": "本研究使用大五人格量表（BFI-2）评估了六个大型语言模型（LLMs）的“人格”表现，发现不同模型在人格维度上存在显著差异，且抽样温度会影响神经质和外向性。模型架构可能预设了稳定的特质，为模型调优和AI伦理治理提供了新视角。", "motivation": "大型语言模型（LLMs）在以人为中心的应用中日益重要，因此理解其类似人格的行为对于负责任的开发和部署至关重要。", "method": "系统性地评估了六个LLMs，应用大五人格量表（BFI-2）框架，在不同的抽样温度下评估其特质表达。此外，还采用了层次聚类方法。", "result": "在五个主要人格维度中的四个上发现了显著差异，其中神经质和外向性易受温度调整的影响。层次聚类揭示了不同的模型簇，表明架构特征可能使某些模型倾向于稳定的特质剖面。", "conclusion": "这些结果为LLMs中人格样模式的出现提供了新见解，并为模型调优、选择和AI系统的伦理治理提供了新视角。"}}
{"id": "2511.04344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04344", "abs": "https://arxiv.org/abs/2511.04344", "authors": ["Muhammad Annas Shaikh", "Hamza Zaman", "Arbaz Asif"], "title": "Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset", "comment": null, "summary": "This paper presents a comprehensive evaluation of nine convolutional neural\nnetwork architectures for binary classification of horses and motorcycles in\nthe VOC 2008 dataset. We address the significant class imbalance problem by\nimplementing minority-class augmentation techniques. Our experiments compare\nmodern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and\nVision Transformer across multiple performance metrics. Results demonstrate\nsubstantial performance variations, with ConvNeXt-Tiny achieving the highest\nAverage Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle\ndetection. We observe that data augmentation significantly improves minority\nclass detection, particularly benefiting deeper architectures. This study\nprovides insights into architecture selection for imbalanced binary\nclassification tasks and quantifies the impact of data augmentation strategies\nin mitigating class imbalance issues in object detection.", "AI": {"tldr": "本文评估了九种CNN架构在VOC 2008数据集上对马匹和摩托车进行二分类的性能，并通过少数类数据增强解决了类别不平衡问题。研究发现ConvNeXt-Tiny表现最佳，且数据增强显著提升了少数类检测。", "motivation": "研究旨在解决VOC 2008数据集中马匹和摩托车二分类任务中的显著类别不平衡问题，并评估不同卷积神经网络架构和数据增强策略对性能的影响。", "method": "评估了包括ResNet-50、ConvNeXt-Tiny、DenseNet-121和Vision Transformer在内的九种现代CNN架构。通过实施少数类数据增强技术来解决类别不平衡问题。在VOC 2008数据集上进行实验，并比较了多种性能指标。", "result": "结果显示性能存在显著差异，其中ConvNeXt-Tiny在马匹检测中取得了95.53%的最高平均精度（AP），在摩托车检测中取得了89.12%的AP。数据增强显著改善了少数类检测，尤其对更深层的架构益处更大。", "conclusion": "本研究为不平衡二分类任务中的架构选择提供了见解，并量化了数据增强策略在缓解目标检测中类别不平衡问题上的影响。"}}
{"id": "2511.04334", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04334", "abs": "https://arxiv.org/abs/2511.04334", "authors": ["Saúl Alonso-Monsalve", "Leigh H. Whitehead", "Adam Aurisano", "Lorena Escudero Sanchez"], "title": "Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography", "comment": "12 pages, 5 figures", "summary": "The accurate delineation of tumours in radiological images like Computed\nTomography is a very specialised and time-consuming task, and currently a\nbottleneck preventing quantitative analyses to be performed routinely in the\nclinical setting. For this reason, developing methods for the automated\nsegmentation of tumours in medical imaging is of the utmost importance and has\ndriven significant efforts in recent years. However, challenges regarding the\nimpracticality of 3D scans, given the large amount of voxels to be analysed,\nusually requires the downsampling of such images or using patches thereof when\napplying traditional convolutional neural networks. To overcome this problem,\nin this paper we propose a new methodology that uses, divided into two stages,\nvoxel sparsification and submanifold sparse convolutional networks. This method\nallows segmentations to be performed with high-resolution inputs and a native\n3D model architecture, obtaining state-of-the-art accuracies while\nsignificantly reducing the computational resources needed in terms of GPU\nmemory and time. We studied the deployment of this methodology in the context\nof Computed Tomography images of renal cancer patients from the KiTS23\nchallenge, and our method achieved results competitive with the challenge\nwinners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%\nfor tumours + cysts, and 80.3% for tumours alone. Crucially, our method also\noffers significant computational improvements, achieving up to a 60% reduction\nin inference time and up to a 75\\% reduction in VRAM usage compared to an\nequivalent dense architecture, across both CPU and various GPU cards tested.", "AI": {"tldr": "本文提出了一种基于体素稀疏化和子流形稀疏卷积网络的3D肿瘤自动分割方法，在保持高分辨率和领先精度的同时，显著降低了计算资源消耗，解决了传统方法在处理大规模3D医学图像时的挑战。", "motivation": "在放射影像中准确勾勒肿瘤边界是一项专业且耗时的工作，是阻碍临床常规进行定量分析的瓶颈。传统卷积神经网络在处理3D扫描时，由于体素量大，通常需要降采样或使用图像块，导致资源消耗高且可能丢失信息。", "method": "本文提出了一种分两阶段的新方法：首先进行体素稀疏化（voxel sparsification），然后应用子流形稀疏卷积网络（submanifold sparse convolutional networks）。这种方法允许使用高分辨率的原始3D模型架构进行分割。", "result": "该方法在KiTS23肾癌CT图像数据集上取得了与挑战赛获胜者相媲美的结果，肾脏+肿块的Dice相似系数为95.8%，肿瘤+囊肿为85.7%，单独肿瘤为80.3%。在计算资源方面，推理时间减少高达60%，VRAM使用量减少高达75%，显著优于等效的密集架构。", "conclusion": "所提出的方法能够以高分辨率和原生3D模型架构实现肿瘤的精确分割，同时显著减少了GPU内存和时间等计算资源需求。这为在临床环境中常规进行自动肿瘤分割提供了高效且准确的解决方案。"}}
{"id": "2511.04502", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04502", "abs": "https://arxiv.org/abs/2511.04502", "authors": ["Joshua Gao", "Quoc Huy Pham", "Subin Varghese", "Silwal Saurav", "Vedhus Hoskere"], "title": "RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a critical technique for grounding\nLarge Language Models (LLMs) in factual evidence, yet evaluating RAG systems in\nspecialized, safety-critical domains remains a significant challenge. Existing\nevaluation frameworks often rely on heuristic-based metrics that fail to\ncapture domain-specific nuances and other works utilize LLM-as-a-Judge\napproaches that lack validated alignment with human judgment. This paper\nintroduces RAGalyst, an automated, human-aligned agentic framework designed for\nthe rigorous evaluation of domain-specific RAG systems. RAGalyst features an\nagentic pipeline that generates high-quality, synthetic question-answering (QA)\ndatasets from source documents, incorporating an agentic filtering step to\nensure data fidelity. The framework refines two key LLM-as-a-Judge\nmetrics-Answer Correctness and Answerability-using prompt optimization to\nachieve a strong correlation with human annotations. Applying this framework to\nevaluate various RAG components across three distinct domains (military\noperations, cybersecurity, and bridge engineering), we find that performance is\nhighly context-dependent. No single embedding model, LLM, or hyperparameter\nconfiguration proves universally optimal. Additionally, we provide an analysis\non the most common low Answer Correctness reasons in RAG. These findings\nhighlight the necessity of a systematic evaluation framework like RAGalyst,\nwhich empowers practitioners to uncover domain-specific trade-offs and make\ninformed design choices for building reliable and effective RAG systems.\nRAGalyst is available on our Github.", "AI": {"tldr": "RAGalyst是一个自动化、与人类判断对齐的代理框架，用于严格评估特定领域的RAG系统，通过生成高质量的合成问答数据集和优化LLM-as-a-Judge指标来识别领域特定的RAG性能权衡。", "motivation": "在专业、安全关键领域评估RAG系统面临巨大挑战。现有评估框架依赖启发式指标，无法捕捉领域细微差别，或使用LLM-as-a-Judge方法，但其与人类判断的对齐性未经充分验证。", "method": "本文引入了RAGalyst框架，它包含一个代理管道，能从源文档生成高质量的合成问答（QA）数据集，并通过代理过滤步骤确保数据保真度。该框架通过提示优化，改进了“答案正确性”和“可回答性”这两个关键的LLM-as-a-Judge指标，使其与人类标注高度相关。", "result": "将RAGalyst应用于军事行动、网络安全和桥梁工程三个领域，评估了各种RAG组件，发现性能高度依赖于上下文，没有单一的嵌入模型、LLM或超参数配置是普遍最优的。此外，论文还分析了RAG中导致答案正确性低的最常见原因。", "conclusion": "这些发现强调了RAGalyst这类系统化评估框架的必要性，它能帮助从业者发现领域特定的权衡，并做出明智的设计选择，以构建可靠有效的RAG系统。"}}
{"id": "2511.04538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04538", "abs": "https://arxiv.org/abs/2511.04538", "authors": ["Cyril Vallez", "Alexander Sternfeld", "Andrei Kucharavy", "Ljiljana Dolamic"], "title": "From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting", "comment": null, "summary": "As the role of Large Language Models (LLM)-based coding assistants in\nsoftware development becomes more critical, so does the role of the bugs they\ngenerate in the overall cybersecurity landscape. While a number of LLM code\nsecurity benchmarks have been proposed alongside approaches to improve the\nsecurity of generated code, it remains unclear to what extent they have\nimpacted widely used coding LLMs. Here, we show that even the latest\nopen-weight models are vulnerable in the earliest reported vulnerability\nscenarios in a realistic use setting, suggesting that the safety-functionality\ntrade-off has until now prevented effective patching of vulnerabilities. To\nhelp address this issue, we introduce a new severity metric that reflects the\nrisk posed by an LLM-generated vulnerability, accounting for vulnerability\nseverity, generation chance, and the formulation of the prompt that induces\nvulnerable code generation - Prompt Exposure (PE). To encourage the mitigation\nof the most serious and prevalent vulnerabilities, we use PE to define the\nModel Exposure (ME) score, which indicates the severity and prevalence of\nvulnerabilities a model generates.", "AI": {"tldr": "研究发现最新的开源大语言模型在早期报告的漏洞场景中仍然存在安全漏洞，这表明安全性与功能性之间存在权衡。为解决此问题，论文引入了新的严重性指标——提示暴露（PE）和模型暴露（ME），以量化和优先处理LLM生成的漏洞风险。", "motivation": "随着基于大语言模型（LLM）的编码助手在软件开发中变得越来越重要，它们生成的错误对网络安全构成了关键风险。尽管已提出许多LLM代码安全基准和改进代码安全性的方法，但这些方法对广泛使用的编码LLM的影响程度尚不明确。", "method": "研究通过在实际使用场景中测试最新的开源模型，评估它们在最早报告的漏洞场景中的脆弱性。为了解决现有问题，论文引入了新的严重性度量标准——提示暴露（PE），该指标综合考虑了漏洞严重性、生成概率以及诱导生成漏洞代码的提示表述。在此基础上，定义了模型暴露（ME）得分，用于指示模型生成漏洞的严重性和普遍性。", "result": "研究显示，即使是最新的开源模型，在现实使用环境中，也对最早报告的漏洞场景表现出脆弱性。这表明在LLM中，安全性与功能性之间的权衡至今仍阻碍了有效漏洞修复。论文还成功引入了新的度量标准——提示暴露（PE）和模型暴露（ME），以量化和评估LLM生成漏洞的风险。", "conclusion": "最新的开源大语言模型在生成安全代码方面仍存在显著缺陷，尤其是在已知的漏洞场景中。这可能源于模型设计中安全性与功能性之间的固有权衡。通过引入提示暴露（PE）和模型暴露（ME）等新指标，可以更有效地识别、评估和优先处理LLM生成的严重且普遍的漏洞，从而鼓励缓解这些风险。"}}
{"id": "2511.04528", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04528", "abs": "https://arxiv.org/abs/2511.04528", "authors": ["Kaveh Eskandari Miandoab", "Katharine Kowalyshyn", "Kabir Pamnani", "Anesu Gavhera", "Vasanth Sarathy", "Matthias Scheutz"], "title": "IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection", "comment": "Accepted for the 40th Annual AAAI Conference on Artificial\n  Intelligence (2026) - Demonstration Track", "summary": "We present IntelliProof, an interactive system for analyzing argumentative\nessays through LLMs. IntelliProof structures an essay as an argumentation\ngraph, where claims are represented as nodes, supporting evidence is attached\nas node properties, and edges encode supporting or attacking relations. Unlike\nexisting automated essay scoring systems, IntelliProof emphasizes the user\nexperience: each relation is initially classified and scored by an LLM, then\nvisualized for enhanced understanding. The system provides justifications for\nclassifications and produces quantitative measures for essay coherence. It\nenables rapid exploration of argumentative quality while retaining human\noversight. In addition, IntelliProof provides a set of tools for a better\nunderstanding of an argumentative essay and its corresponding graph in natural\nlanguage, bridging the gap between the structural semantics of argumentative\nessays and the user's understanding of a given text. A live demo and the system\nare available here to try: \\textbf{https://intelliproof.vercel.app}", "AI": {"tldr": "IntelliProof是一个交互式系统，利用大型语言模型（LLMs）将议论文结构化为论证图，以可视化方式分析论证质量，并提供量化指标和自然语言解释，同时保留人工监督。", "motivation": "现有自动化论文评分系统缺乏用户体验，难以深入理解论证结构。研究旨在弥合议论文结构语义与用户理解之间的鸿沟，提供一个既能快速探索论证质量又保留人工监督的系统。", "method": "IntelliProof将议论文构建为论证图：论点是节点，支持证据是节点属性，边表示支持或攻击关系。LLM对每种关系进行分类和评分，并进行可视化展示。系统提供分类依据、文章连贯性的量化指标，并提供工具以自然语言解释论证图。", "result": "IntelliProof能够快速探索议论文的论证质量，同时保留人工监督。它为分类提供理由，生成文章连贯性的量化指标，并通过自然语言工具弥合了论证结构语义与用户理解之间的差距。", "conclusion": "IntelliProof是一个有效的交互式系统，通过LLMs和可视化论证图，显著增强了用户对议论文论证质量的理解和分析能力，同时兼顾了自动化效率与人工监督的重要性。"}}
{"id": "2511.04347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04347", "abs": "https://arxiv.org/abs/2511.04347", "authors": ["Sanjay Kumar", "Tim Brophy", "Eoin Martino Grua", "Ganesh Sistu", "Valentina Donzella", "Ciaran Eising"], "title": "Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection", "comment": null, "summary": "Accurate 3D object detection is essential for automated vehicles to navigate\nsafely in complex real-world environments. Bird's Eye View (BEV)\nrepresentations, which project multi-sensor data into a top-down spatial\nformat, have emerged as a powerful approach for robust perception. Although\nBEV-based fusion architectures have demonstrated strong performance through\nmultimodal integration, the effects of sensor occlusions, caused by\nenvironmental conditions such as fog, haze, or physical obstructions, on 3D\ndetection accuracy remain underexplored. In this work, we investigate the\nimpact of occlusions on both camera and Light Detection and Ranging (LiDAR)\noutputs using the BEVFusion architecture, evaluated on the nuScenes dataset.\nDetection performance is measured using mean Average Precision (mAP) and the\nnuScenes Detection Score (NDS). Our results show that moderate camera\nocclusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is\nbased only on the camera. On the other hand, LiDAR sharply drops in performance\nonly under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),\nwith a severe impact on long-range detection. In fused settings, the effect\ndepends on which sensor is occluded: occluding the camera leads to a minor 4.1%\ndrop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%\ndrop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task\nof 3D object detection. Our results highlight the need for future research into\nocclusion-aware evaluation methods and improved sensor fusion techniques that\ncan maintain detection accuracy in the presence of partial sensor failure or\ndegradation due to adverse environmental conditions.", "AI": {"tldr": "本研究调查了遮挡对基于BEV的相机、LiDAR和融合3D目标检测性能的影响。结果显示，相机对中度遮挡敏感，LiDAR对重度遮挡敏感，且融合模型更依赖LiDAR。", "motivation": "自动驾驶车辆需要准确的3D目标检测以安全导航，而基于BEV的多传感器融合方法表现出色。然而，传感器遮挡（如雾、霾或物理障碍）对3D检测精度的影响尚未得到充分探索。", "method": "研究人员使用BEVFusion架构，在nuScenes数据集上评估了遮挡对相机和LiDAR输出的影响。检测性能通过平均精度（mAP）和nuScenes检测分数（NDS）进行衡量。", "result": "中度相机遮挡导致纯相机检测的mAP下降41.3%。LiDAR仅在重度遮挡下性能急剧下降，mAP下降47.3%，严重影响长距离检测。在融合设置中，相机遮挡导致mAP小幅下降4.1%，而LiDAR遮挡导致mAP大幅下降26.8%，表明模型更依赖LiDAR。", "conclusion": "研究结果强调了未来需要开发遮挡感知评估方法和改进传感器融合技术，以在恶劣环境条件下传感器部分失效或性能下降时仍能保持检测精度。"}}
{"id": "2511.04560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04560", "abs": "https://arxiv.org/abs/2511.04560", "authors": ["Sadia Sultana", "Saiyma Sittul Muna", "Mosammat Zannatul Samarukh", "Ajwad Abrar", "Tareque Mohmud Chowdhury"], "title": "BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering", "comment": "Under Review", "summary": "Developing accurate biomedical Question Answering (QA) systems in\nlow-resource languages remains a major challenge, limiting equitable access to\nreliable medical knowledge. This paper introduces BanglaMedQA and\nBanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice\nQuestion (MCQ) datasets designed to evaluate reasoning and retrieval in medical\nartificial intelligence (AI). The study applies and benchmarks several\nRetrieval-Augmented Generation (RAG) strategies, including Traditional,\nZero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining\ntextbook-based and web retrieval with generative reasoning to improve factual\naccuracy. A key novelty lies in integrating a Bangla medical textbook corpus\nthrough Optical Character Recognition (OCR) and implementing an Agentic RAG\npipeline that dynamically selects between retrieval and reasoning strategies.\nExperimental results show that the Agentic RAG achieved the highest accuracy\n89.54% with openai/gpt-oss-120b, outperforming other configurations and\ndemonstrating superior rationale quality. These findings highlight the\npotential of RAG-based methods to enhance the reliability and accessibility of\nBangla medical QA, establishing a foundation for future research in\nmultilingual medical artificial intelligence.", "AI": {"tldr": "该研究引入了首个大规模孟加拉语生物医学多项选择题数据集BanglaMedQA和BanglaMMedBench，并提出了一种Agentic RAG策略，通过结合教科书和网络检索以及生成式推理，显著提高了孟加拉语低资源环境下生物医学问答的准确性和可靠性。", "motivation": "在低资源语言中开发准确的生物医学问答系统是一个重大挑战，限制了人们公平获取可靠医疗知识的机会。本研究旨在解决孟加拉语环境下这一问题。", "method": "研究引入了BanglaMedQA和BanglaMMedBench两个孟加拉语生物医学MCQ数据集。应用并基准测试了多种检索增强生成（RAG）策略，包括传统RAG、零样本回退RAG、代理RAG（Agentic RAG）、迭代反馈RAG和聚合RAG。关键创新在于通过光学字符识别（OCR）整合了孟加拉语医学教科书语料库，并实现了能够动态选择检索或推理策略的Agentic RAG管道。", "result": "实验结果显示，Agentic RAG结合openai/gpt-oss-120b模型达到了89.54%的最高准确率，优于其他配置，并展现出卓越的推理质量。", "conclusion": "这些发现突出了基于RAG的方法在提高孟加拉语医疗问答的可靠性和可及性方面的潜力，为未来多语言医疗人工智能研究奠定了基础。"}}
{"id": "2511.04349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04349", "abs": "https://arxiv.org/abs/2511.04349", "authors": ["Puneet Mishra", "Martijntje Vollebregt", "Yizhou Ma", "Maria Font-i-Furnols"], "title": "A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications", "comment": null, "summary": "Background In analytical chemistry, spatial information about materials is\ncommonly captured through imaging techniques, such as traditional color cameras\nor with advanced hyperspectral cameras and microscopes. However, efficiently\nextracting and analyzing this spatial information for exploratory and\npredictive purposes remains a challenge, especially when using traditional\nchemometric methods. Recent advances in deep learning and artificial\nintelligence have significantly enhanced image processing capabilities,\nenabling the extraction of multiscale deep features that are otherwise\nchallenging to capture with conventional image processing techniques. Despite\nthe wide availability of open-source deep learning models, adoption in\nanalytical chemistry remains limited because of the absence of structured,\nstep-by-step guidance for implementing these models.\n  Results This tutorial aims to bridge this gap by providing a step-by-step\nguide for applying deep learning approaches to extract spatial information from\nimaging data and integrating it with other data sources, such as spectral\ninformation. Importantly, the focus of this work is not on training deep\nlearning models for image processing but on using existing open source models\nto extract deep features from imaging data.\n  Significance The tutorial provides MATLAB code tutorial demonstrations,\nshowcasing the processing of imaging data from various imaging modalities\ncommonly encountered in analytical chemistry. Readers must run the tutorial\nsteps on their own datasets using the codes presented in this tutorial.", "AI": {"tldr": "本教程提供了一份MATLAB代码指南，介绍如何利用现有开源深度学习模型从分析化学成像数据中提取空间特征，并将其与光谱信息等其他数据源整合，以克服传统化学计量学方法的挑战。", "motivation": "分析化学中，从成像数据中高效提取和分析空间信息仍然是挑战，传统化学计量学方法难以捕捉多尺度深层特征。尽管深度学习在图像处理方面取得了显著进展且有开源模型可用，但由于缺乏结构化的实施指导，其在分析化学领域的应用仍受限。", "method": "本教程通过提供分步指南和MATLAB代码演示，重点在于如何应用现有的开源深度学习模型来从成像数据中提取深层特征，并将其与光谱信息等其他数据源整合。该方法不涉及训练新的深度学习模型，而是利用预训练模型。", "result": "本教程展示了如何处理分析化学中常见的各种成像模式的数据。读者可以使用教程中提供的代码，在自己的数据集上运行这些步骤，以提取和分析空间信息。", "conclusion": "本教程旨在通过提供实用的分步指导和MATLAB代码，弥合深度学习在分析化学领域应用的鸿沟，帮助研究人员有效地从成像数据中提取和利用空间信息，从而增强探索性和预测能力。"}}
{"id": "2511.04654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04654", "abs": "https://arxiv.org/abs/2511.04654", "authors": ["Mohammad Atif Quamar", "Mohammad Areeb"], "title": "Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning", "comment": "Presented at the 1st Workshop on Efficient Reasoning (NeurIPS 2025)", "summary": "Chain-of-Thought (CoT) prompting is a key technique for enabling complex\nreasoning in large language models. However, generating full, fixed-length\nrationales is computationally wasteful, inflating both token usage and latency.\nWe introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free\ndecoding algorithm that adaptively halts rationale generation. LEASH monitors\ntwo intrinsic signals: the slope of token-level entropy and the improvement in\nthe top-logit margin. It terminates the generation once both signals plateau,\nindicating the model has reached a stable reasoning state. Across four\ninstruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces\naverage token generation by 30--35% and latency by 27%, while incurring a 10\np.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no\nadditional training or supervision, offering a simple and efficient alternative\nto CoT decoding.", "AI": {"tldr": "LEASH是一种无需训练的解码算法，通过监控逻辑熵和最高逻辑值裕度的变化，自适应地停止Chain-of-Thought（CoT）推理过程，旨在减少令牌使用和延迟，同时保持相对较高的准确率。", "motivation": "Chain-of-Thought（CoT）提示在大型语言模型中实现复杂推理非常有效，但生成完整、固定长度的理由计算成本高昂，导致令牌使用和延迟增加，造成资源浪费。", "method": "本文提出LEASH（Logit-Entropy Adaptive Stopping Heuristic），这是一种无需训练的解码算法，可自适应地停止理由生成。LEASH监测两个内在信号：令牌级熵的斜率和最高逻辑值裕度的改进。当这两个信号都趋于平稳时，表明模型已达到稳定的推理状态，LEASH便终止生成。", "result": "在GSM8K和AQuA-RAT基准测试中，LEASH在四种指令微调模型上平均减少了30-35%的令牌生成和27%的延迟。相对于CoT，其准确率下降了10个百分点。", "conclusion": "LEASH是一种模型无关、无需额外训练或监督的简单高效的CoT解码替代方案。它通过自适应停止理由生成，显著提高了计算效率，尽管会带来一定的准确率下降，但仍提供了一个有价值的权衡。"}}
{"id": "2511.04384", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04384", "abs": "https://arxiv.org/abs/2511.04384", "authors": ["Itbaan Safwan", "Muhammad Annas Shaikh", "Muhammad Haaris", "Ramail Khan", "Muhammad Atif Tahir"], "title": "Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA", "comment": "This is a working paper submitted for Medico 2025: Visual Question\n  Answering (with multimodal explanations) for Gastrointestinal Imaging at\n  MediaEval 2025. 5 pages, 3 figures and 1 table", "summary": "We present a multi-task framework for the MediaEval Medico 2025 challenge,\nleveraging a LoRA-tuned Florence-2 model for simultaneous visual question\nanswering (VQA), explanation generation, and visual grounding. The proposed\nsystem integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer\nlearning, (2) a synthetically enriched explanation dataset offering structured\nmedical reasoning, and (3) text-to-region pairs linking visual features with\nsegmentation masks. This multi-task setup enables the model to jointly learn\nvisual grounding, reasoning, and interpretation, producing responses that are\nboth accurate and interpretable. Extensive evaluation demonstrates that our\napproach substantially improves over single-task baselines in both answer\naccuracy and visual localization, highlighting the effectiveness of grounded\nmulti-task learning for medical VQA applications.", "AI": {"tldr": "本文提出了一个基于LoRA微调Florence-2模型的多任务框架，用于MediaEval Medico 2025挑战，实现医学视觉问答（VQA）、解释生成和视觉定位，并在准确性和定位方面显著优于单任务基线。", "motivation": "为MediaEval Medico 2025挑战赛开发一个能够同时进行医学VQA、解释生成和视觉定位的系统，以提供准确且可解释的响应。", "method": "采用多任务框架，使用LoRA微调的Florence-2模型。整合了三个数据集：Kvasir-VQA-x1（用于问答学习）、一个合成的解释数据集（提供结构化医学推理）和文本到区域对（连接视觉特征与分割掩码）。该设置使模型能够共同学习视觉定位、推理和解释。", "result": "该方法在答案准确性和视觉定位方面均显著优于单任务基线。", "conclusion": "接地多任务学习对于医学VQA应用是有效的，能够产生准确且可解释的响应。"}}
{"id": "2511.04570", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04570", "abs": "https://arxiv.org/abs/2511.04570", "authors": ["Jingqi Tong", "Yurong Mou", "Hangcheng Li", "Mingzhe Li", "Yongzhuo Yang", "Ming Zhang", "Qiguang Chen", "Tianyi Liang", "Xiaomeng Hu", "Yining Zheng", "Xinchi Chen", "Jun Zhao", "Xuanjing Huang", "Xipeng Qiu"], "title": "Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm", "comment": "36 pages, 14 figures", "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly\nimprove the reasoning ability of large language models (LLMs) and Vision\nLanguage Models (VLMs). However, these paradigms have inherent limitations. (1)\nImages capture only single moments and fail to represent dynamic processes or\ncontinuous changes, and (2) The separation of text and vision as distinct\nmodalities, hindering unified multimodal understanding and generation. To\novercome these limitations, we introduce \"Thinking with Video\", a new paradigm\nthat leverages video generation models, such as Sora-2, to bridge visual and\ntextual reasoning in a unified temporal framework. To support this exploration,\nwe developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench\nencompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing\nPuzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our\nevaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,\nSora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even\nsurpasses VLMs on several tasks, such as Eyeballing Games. On text-centric\ntasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.\nFurthermore, we systematically analyse the source of these abilities. We also\nfind that self-consistency and in-context learning can improve Sora-2's\nperformance. In summary, our findings demonstrate that the video generation\nmodel is the potential unified multimodal understanding and generation model,\npositions \"thinking with video\" as a unified multimodal reasoning paradigm.", "AI": {"tldr": "本文提出“视频思维”范式，利用Sora-2等视频生成模型弥合视觉与文本推理，克服现有“文本思维”和“图像思维”的局限。通过VideoThinkBench基准测试，Sora-2在视觉和文本任务上均表现出色，证明视频生成模型作为统一多模态理解与生成模型的潜力。", "motivation": "现有的“文本思维”和“图像思维”范式存在局限性：图像无法捕捉动态过程或连续变化，且文本和视觉作为独立模态阻碍了统一的多模态理解和生成。", "method": "引入“视频思维”新范式，利用Sora-2等视频生成模型在一个统一的时间框架内连接视觉和文本推理。为此，开发了VideoThinkBench基准测试，包含视觉中心任务（如“眼球谜题”）和文本中心任务（如GSM8K、MMMU子集）。评估了Sora-2的能力，并系统分析了其能力来源，同时探究了自洽性和上下文学习对性能的影响。", "result": "Sora-2被证明是一个有能力的推理器。在视觉中心任务上，Sora-2与最先进的视觉语言模型（VLM）相当，甚至在某些任务（如“眼球游戏”）上超越了VLM。在文本中心任务上，Sora-2在MATH上达到92%的准确率，在MMMU上达到75.53%的准确率。此外，自洽性和上下文学习可以提高Sora-2的性能。", "conclusion": "研究结果表明，视频生成模型是潜在的统一多模态理解和生成模型，将“视频思维”定位为一个统一的多模态推理范式。"}}
{"id": "2511.04450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04450", "abs": "https://arxiv.org/abs/2511.04450", "authors": ["Yaniv Ohayon", "Ofir Itzhak Shahar", "Ohad Ben-Shahar"], "title": "Solving Convex Partition Visual Jigsaw Puzzles", "comment": null, "summary": "Jigsaw puzzle solving requires the rearrangement of unordered pieces into\ntheir original pose in order to reconstruct a coherent whole, often an image,\nand is known to be an intractable problem. While the possible impact of\nautomatic puzzle solvers can be disruptive in various application domains, most\nof the literature has focused on developing solvers for square jigsaw puzzles,\nseverely limiting their practical use. In this work, we significantly expand\nthe types of puzzles handled computationally, focusing on what is known as\nConvex Partitions, a major subset of polygonal puzzles whose pieces are convex.\nWe utilize both geometrical and pictorial compatibilities, introduce a greedy\nsolver, and report several performance measures next to the first benchmark\ndataset of such puzzles.", "AI": {"tldr": "本文提出了一种解决凸多边形拼图的贪婪算法，结合几何和图像兼容性，并创建了首个此类拼图的基准数据集，显著扩展了自动拼图求解器的应用范围。", "motivation": "现有拼图求解器主要针对方形拼图，限制了其实际应用。研究旨在扩展计算处理的拼图类型，特别是处理更通用的多边形拼图，如凸分区拼图。", "method": "专注于凸分区（多边形拼图的一个主要子集）。同时利用几何兼容性和图像兼容性。引入了一种贪婪求解器。", "result": "成功开发并报告了针对凸分区拼图的贪婪求解器的多项性能指标。创建并发布了首个此类拼图的基准数据集。", "conclusion": "该工作显著扩展了计算处理的拼图类型，为凸分区拼图提供了一种新的求解方法和基准数据集，克服了以往仅限于方形拼图的局限性。"}}
{"id": "2511.04394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04394", "abs": "https://arxiv.org/abs/2511.04394", "authors": ["Ke Du", "Yimin Peng", "Chao Gao", "Fan Zhou", "Siqiao Xue"], "title": "DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale", "comment": "code: https://github.com/wuji3/DORAEMON", "summary": "DORAEMON is an open-source PyTorch library that unifies visual object\nmodeling and representation learning across diverse scales. A single\nYAML-driven workflow covers classification, retrieval and metric learning; more\nthan 1000 pretrained backbones are exposed through a timm-compatible interface,\ntogether with modular losses, augmentations and distributed-training utilities.\nReproducible recipes match or exceed reference results on ImageNet-1K,\nMS-Celeb-1M and Stanford online products, while one-command export to ONNX or\nHuggingFace bridges research and deployment. By consolidating datasets, models,\nand training techniques into one platform, DORAEMON offers a scalable\nfoundation for rapid experimentation in visual recognition and representation\nlearning, enabling efficient transfer of research advances to real-world\napplications. The repository is available at https://github.com/wuji3/DORAEMON.", "AI": {"tldr": "DORAEMON是一个开源PyTorch库，旨在统一视觉对象建模和表示学习，提供分类、检索和度量学习功能，集成了大量预训练模型、模块化组件和便捷的部署工具。", "motivation": "研究动机是为视觉识别和表示学习提供一个可扩展的、统一的平台，以加速快速实验，并高效地将研究进展转化为实际应用。", "method": "该库基于PyTorch，采用YAML驱动的单一工作流，支持分类、检索和度量学习。它提供了超过1000个timm兼容的预训练骨干网络，以及模块化的损失函数、数据增强和分布式训练工具。此外，它支持一键导出到ONNX或HuggingFace。", "result": "DORAEMON在ImageNet-1K、MS-Celeb-1M和Stanford online products等数据集上达到了或超越了参考结果，并成功地在研究与部署之间搭建了桥梁。它为视觉识别和表示学习的快速实验提供了一个可扩展的基础。", "conclusion": "DORAEMON通过整合数据集、模型和训练技术，提供了一个可扩展的平台，极大地促进了视觉识别和表示学习领域的快速实验，并有助于将研究成果高效地应用于实际场景。"}}
{"id": "2511.04643", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04643", "abs": "https://arxiv.org/abs/2511.04643", "authors": ["Alamgir Munir Qazi", "John P. McCrae", "Jamal Abdul Nasir"], "title": "When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection", "comment": null, "summary": "The proliferation of misinformation necessitates robust yet computationally\nefficient fact verification systems. While current state-of-the-art approaches\nleverage Large Language Models (LLMs) for generating explanatory rationales,\nthese methods face significant computational barriers and hallucination risks\nin real-world deployments. We present DeReC (Dense Retrieval Classification), a\nlightweight framework that demonstrates how general-purpose text embeddings can\neffectively replace autoregressive LLM-based approaches in fact verification\ntasks. By combining dense retrieval with specialized classification, our system\nachieves better accuracy while being significantly more efficient. DeReC\noutperforms explanation-generating LLMs in efficiency, reducing runtime by 95%\non RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%\non LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),\nshowcasing its effectiveness across varying dataset sizes. On the RAWFC\ndataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art\nmethod L-Defense (61.20%). Our results demonstrate that carefully engineered\nretrieval-based systems can match or exceed LLM performance in specialized\ntasks while being significantly more practical for real-world deployment.", "AI": {"tldr": "DeReC是一个轻量级的事实核查框架，它通过结合密集检索和专门分类，利用通用文本嵌入有效替代了基于LLM的方法，显著提高了效率和准确性，超越了现有最先进的LLM模型。", "motivation": "当前事实核查系统面临信息误传泛滥的挑战，需要既强大又计算高效的解决方案。现有最先进的LLM方法虽然能生成解释性理由，但在实际部署中存在显著的计算障碍和幻觉风险。", "method": "本文提出了DeReC（密集检索分类）框架，它使用通用文本嵌入来代替自回归LLM方法进行事实核查。该系统通过结合密集检索和专门分类来实现其功能。", "result": "DeReC在效率上显著优于生成解释的LLM，在RAWFC数据集上运行时长减少95%，在LIAR-RAW上减少92%。在RAWFC数据集上，DeReC的F1分数达到65.58%，超过了最先进的L-Defense方法（61.20%）。", "conclusion": "精心设计的基于检索的系统在特定任务中可以达到或超越LLM的性能，同时在实际部署中更具实用性。"}}
{"id": "2511.04474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04474", "abs": "https://arxiv.org/abs/2511.04474", "authors": ["Wenwen Li", "Sizhe Wang", "Hyunho Lee", "Chenyan Lu", "Sujit Roy", "Rahul Ramachandran", "Chia-Yu Hsu"], "title": "Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability", "comment": null, "summary": "Landslides cause severe damage to lives, infrastructure, and the environment,\nmaking accurate and timely mapping essential for disaster preparedness and\nresponse. However, conventional deep learning models often struggle when\napplied across different sensors, regions, or under conditions of limited\ntraining data. To address these challenges, we present a three-axis analytical\nframework of sensor, label, and domain for adapting geospatial foundation\nmodels (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a\nseries of experiments, we show that it consistently outperforms task-specific\nCNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other\nGeoFMs (TerraMind, SatMAE). The model, built on global pretraining,\nself-supervision, and adaptable fine-tuning, proved resilient to spectral\nvariation, maintained accuracy under label scarcity, and generalized more\nreliably across diverse datasets and geographic settings. Alongside these\nstrengths, we also highlight remaining challenges such as computational cost\nand the limited availability of reusable AI-ready training data for landslide\nresearch. Overall, our study positions GeoFMs as a step toward more robust and\nscalable approaches for landslide risk reduction and environmental monitoring.", "AI": {"tldr": "本研究提出并验证了基于地理空间基础模型（GeoFMs）的Prithvi-EO-2.0模型，用于滑坡制图，该模型在传感器、标签和领域适应性方面表现出色，优于传统深度学习模型和其他GeoFMs，并对滑坡风险管理具有重要意义。", "motivation": "传统的深度学习模型在跨传感器、跨区域或训练数据有限的情况下，难以进行准确及时的滑坡制图，这阻碍了灾害准备和响应。", "method": "研究提出了一个围绕传感器、标签和领域的三轴分析框架，用于调整地理空间基础模型（GeoFMs），重点关注Prithvi-EO-2.0。该模型基于全球预训练、自监督学习和自适应微调。", "result": "Prithvi-EO-2.0模型在滑坡制图方面持续优于任务特定的CNN（U-Net, U-Net++）、视觉Transformer（Segformer, SwinV2-B）以及其他GeoFMs（TerraMind, SatMAE）。它对光谱变化具有弹性，在标签稀缺的情况下仍能保持准确性，并能更可靠地泛化到多样化的数据集和地理环境。研究也指出计算成本高和可复用AI训练数据有限等挑战。", "conclusion": "本研究表明，地理空间基础模型（GeoFMs）是迈向更稳健和可扩展的滑坡风险降低和环境监测方法的重要一步，尽管仍面临计算成本和数据可用性等挑战。"}}
{"id": "2511.04460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04460", "abs": "https://arxiv.org/abs/2511.04460", "authors": ["Runqi Qiao", "Qiuna Tan", "Minghan Yang", "Guanting Dong", "Peiqing Yang", "Shiqiang Lang", "Enhui Wan", "Xiaowan Wang", "Yida Xu", "Lan Yang", "Chong Sun", "Chen Li", "Honggang Zhang"], "title": "V-Thinker: Interactive Thinking with Images", "comment": "Working in progress", "summary": "Empowering Large Multimodal Models (LMMs) to deeply integrate image\ninteraction with long-horizon reasoning capabilities remains a long-standing\nchallenge in this field. Recent advances in vision-centric reasoning explore a\npromising \"Thinking with Images\" paradigm for LMMs, marking a shift from\nimage-assisted reasoning to image-interactive thinking. While this milestone\nenables models to focus on fine-grained image regions, progress remains\nconstrained by limited visual tool spaces and task-specific workflow designs.\nTo bridge this gap, we present V-Thinker, a general-purpose multimodal\nreasoning assistant that enables interactive, vision-centric thinking through\nend-to-end reinforcement learning. V-Thinker comprises two key components: (1)\na Data Evolution Flywheel that automatically synthesizes, evolves, and verifies\ninteractive reasoning datasets across three dimensions-diversity, quality, and\ndifficulty; and (2) a Visual Progressive Training Curriculum that first aligns\nperception via point-level supervision, then integrates interactive reasoning\nthrough a two-stage reinforcement learning framework. Furthermore, we introduce\nVTBench, an expert-verified benchmark targeting vision-centric interactive\nreasoning tasks. Extensive experiments demonstrate that V-Thinker consistently\noutperforms strong LMM-based baselines in both general and interactive\nreasoning scenarios, providing valuable insights for advancing\nimage-interactive reasoning applications.", "AI": {"tldr": "本文提出V-Thinker，一个通用的多模态推理助手，通过数据演化和渐进式训练，增强大型多模态模型（LMMs）的图像交互式推理能力，并引入了VTBench基准。", "motivation": "当前LMMs在将图像交互与长程推理深度融合方面面临挑战，现有“图像思维”范式受限于视觉工具空间和任务特定工作流设计，无法满足细粒度图像区域交互需求。", "method": "V-Thinker包含两个核心组件：1) 数据演化飞轮，自动合成、演化和验证多样性、高质量、高难度的交互式推理数据集；2) 视觉渐进式训练课程，首先通过点级监督对齐感知，然后通过两阶段强化学习框架整合交互式推理。此外，还引入了VTBench，一个专家验证的图像中心交互式推理任务基准。", "result": "V-Thinker在通用和交互式推理场景中，持续超越了强大的基于LMM的基线模型。", "conclusion": "V-Thinker为推进图像交互式推理应用提供了有价值的见解和解决方案，有效弥合了当前LMMs在深度图像交互和长程推理之间的差距。"}}
{"id": "2511.04426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04426", "abs": "https://arxiv.org/abs/2511.04426", "authors": ["Alan de Aguiar", "Michaella Pereira Andrade", "Charles Morphy D. Santos", "João Paulo Gois"], "title": "HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats", "comment": null, "summary": "Analyzing octopuses in their natural habitats is challenging due to their\ncamouflage capability, rapid changes in skin texture and color, non-rigid body\ndeformations, and frequent occlusions, all of which are compounded by variable\nunderwater lighting and turbidity. Addressing the lack of large-scale annotated\ndatasets, this paper introduces HideAndSeg, a novel, minimally supervised\nAI-based tool for segmenting videos of octopuses. It establishes a quantitative\nbaseline for this task. HideAndSeg integrates SAM2 with a custom-trained\nYOLOv11 object detector. First, the user provides point coordinates to generate\nthe initial segmentation masks with SAM2. These masks serve as training data\nfor the YOLO model. After that, our approach fully automates the pipeline by\nproviding a bounding box prompt to SAM2, eliminating the need for further\nmanual intervention. We introduce two unsupervised metrics - temporal\nconsistency $DICE_t$ and new component count $NC_t$ - to quantitatively\nevaluate segmentation quality and guide mask refinement in the absence of\nground-truth data, i.e., real-world information that serves to train, validate,\nand test AI models. Results show that HideAndSeg achieves satisfactory\nperformance, reducing segmentation noise compared to the manually prompted\napproach. Our method can re-identify and segment the octopus even after periods\nof complete occlusion in natural environments, a scenario in which the manually\nprompted model fails. By reducing the need for manual analysis in real-world\nscenarios, this work provides a practical tool that paves the way for more\nefficient behavioral studies of wild cephalopods.", "AI": {"tldr": "本文提出了一种名为 HideAndSeg 的新型最小监督AI工具，用于在具有挑战性的自然环境中分割章鱼视频，并引入了无监督评估指标，实现了满意的性能，即使在完全遮挡后也能重新识别和分割章鱼。", "motivation": "由于章鱼的伪装能力、快速的皮肤纹理和颜色变化、非刚性身体变形、频繁遮挡以及水下光照和浊度变化，在自然栖息地分析章鱼极具挑战性。此外，目前缺乏大规模的带标注数据集。", "method": "HideAndSeg 集成了 SAM2 和自定义训练的 YOLOv11 对象检测器。首先，用户提供点坐标，通过 SAM2 生成初始分割掩码，这些掩码用于训练 YOLO 模型。之后，该方法通过向 SAM2 提供边界框提示实现完全自动化，无需进一步手动干预。此外，引入了两个无监督指标——时间一致性 $DICE_t$ 和新组件计数 $NC_t$——用于在缺乏真实数据的情况下定量评估分割质量和指导掩码细化。", "result": "HideAndSeg 取得了令人满意的性能，与手动提示方法相比，减少了分割噪声。该方法甚至能在自然环境中章鱼完全遮挡后重新识别和分割章鱼，而手动提示模型在此情景下会失败。", "conclusion": "该工作提供了一个实用的工具，通过减少实际场景中的手动分析需求，为更高效地研究野生头足类动物的行为铺平了道路。"}}
{"id": "2511.04520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04520", "abs": "https://arxiv.org/abs/2511.04520", "authors": ["Nabyl Quignon", "Baptiste Chopin", "Yaohui Wang", "Antitza Dantcheva"], "title": "THEval. Evaluation Framework for Talking Head Video Generation", "comment": null, "summary": "Video generation has achieved remarkable progress, with generated videos\nincreasingly resembling real ones. However, the rapid advance in generation has\noutpaced the development of adequate evaluation metrics. Currently, the\nassessment of talking head generation primarily relies on limited metrics,\nevaluating general video quality, lip synchronization, and on conducting user\nstudies. Motivated by this, we propose a new evaluation framework comprising 8\nmetrics related to three dimensions (i) quality, (ii) naturalness, and (iii)\nsynchronization. In selecting the metrics, we place emphasis on efficiency, as\nwell as alignment with human preferences. Based on this considerations, we\nstreamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as\nwell as face quality. Our extensive experiments on 85,000 videos generated by\n17 state-of-the-art models suggest that while many algorithms excel in lip\nsynchronization, they face challenges with generating expressiveness and\nartifact-free details. These videos were generated based on a novel real\ndataset, that we have curated, in order to mitigate bias of training data. Our\nproposed benchmark framework is aimed at evaluating the improvement of\ngenerative methods. Original code, dataset and leaderboards will be publicly\nreleased and regularly updated with new methods, in order to reflect progress\nin the field.", "AI": {"tldr": "本文提出了一套新的评估框架，包含8个指标，用于更全面、细致地评估说话人头部视频生成模型的质量、自然度和同步性，并发现现有模型在表达力和细节方面存在挑战。", "motivation": "视频生成技术（特别是说话人头部生成）发展迅速，但现有评估指标（如通用视频质量、唇部同步和用户研究）不足以充分衡量其进步，缺乏细致且与人类偏好一致的评估方法。", "method": "研究者提出了一个包含8个指标的新评估框架，涵盖质量、自然度和同步性三个维度。这些指标侧重于效率和与人类偏好的一致性，并分析头部、嘴巴和眉毛的精细动态以及面部质量。此外，他们还整理了一个新的真实数据集，以减少训练数据的偏差。", "result": "对由17个最先进模型生成的85,000个视频进行的广泛实验表明，许多算法在唇部同步方面表现出色，但在生成表达力和无伪影细节方面仍面临挑战。", "conclusion": "所提出的基准评估框架旨在促进生成方法的改进。原始代码、数据集和排行榜将公开，并定期更新新方法，以反映该领域的进展。"}}
{"id": "2511.04525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04525", "abs": "https://arxiv.org/abs/2511.04525", "authors": ["Dimitrios Anastasiou", "Santiago Barbarisi", "Lucy Culshaw", "Jayna Patel", "Evangelos B. Mazomenos", "Imanol Luengo", "Danail Stoyanov"], "title": "Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy", "comment": null, "summary": "Purpose: Accurate assessment of surgical complexity is essential in\nLaparoscopic Cholecystectomy (LC), where severe inflammation is associated with\nlonger operative times and increased risk of postoperative complications. The\nParkland Grading Scale (PGS) provides a clinically validated framework for\nstratifying inflammation severity; however, its automation in surgical videos\nremains largely unexplored, particularly in realistic scenarios where complete\nvideos must be analyzed without prior manual curation. Methods: In this work,\nwe introduce STC-Net, a novel framework for SingleTimestamp-based Complexity\nestimation in LC via the PGS, designed to operate under weak temporal\nsupervision. Unlike prior methods limited to static images or manually trimmed\nclips, STC-Net operates directly on full videos. It jointly performs temporal\nlocalization and grading through a localization, window proposal, and grading\nmodule. We introduce a novel loss formulation combining hard and soft\nlocalization objectives and background-aware grading supervision. Results:\nEvaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy\nof 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by\nover 10% in both metrics and highlighting the effectiveness of weak supervision\nfor surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable\nand effective approach for automated PGS-based surgical complexity estimation\nfrom full LC videos, making it promising for post-operative analysis and\nsurgical training.", "AI": {"tldr": "本文提出STC-Net，一个基于单时间戳的框架，用于在腹腔镜胆囊切除术（LC）中通过Parkland分级量表（PGS）自动评估手术复杂性。该框架能在弱时间监督下直接处理完整视频，并显著优于非局部化基线。", "motivation": "在腹腔镜胆囊切除术中，准确评估手术复杂性至关重要，因为严重的炎症与更长的手术时间和更高的术后并发症风险相关。Parkland分级量表（PGS）是临床验证的炎症严重程度分级框架，但其在手术视频中的自动化（尤其是在需要分析完整视频而无需手动筛选的实际场景中）仍未被充分探索。", "method": "本文引入了STC-Net框架，该框架通过PGS实现LC中的单时间戳复杂性估计，并在弱时间监督下运行。与仅限于静态图像或手动裁剪片段的现有方法不同，STC-Net直接处理完整视频。它通过定位、窗口提议和分级模块共同执行时间定位和分级。研究还引入了一种结合硬性和软性定位目标以及背景感知分级监督的新型损失函数。", "result": "在包含1,859个LC视频的私有数据集上进行评估，STC-Net的准确率达到62.11%，F1分数达到61.42%。这在两项指标上均比非局部化基线高出10%以上，突显了弱监督在手术复杂性评估中的有效性。", "conclusion": "STC-Net展示了一种可扩展且有效的方法，用于从完整的LC视频中自动进行基于PGS的手术复杂性估计，这使其在术后分析和手术培训方面具有广阔前景。"}}
{"id": "2511.04601", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.04601", "abs": "https://arxiv.org/abs/2511.04601", "authors": ["Yicheng Xiao", "Yu Chen", "Haoxuan Ma", "Jiale Hong", "Caorui Li", "Lingxiang Wu", "Haiyun Guo", "Jinqiao Wang"], "title": "PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning", "comment": null, "summary": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved\nremarkable success in a variety of downstream vison language understanding\ntasks, enhancing its capability for fine-grained image-text alignment remains\nan active research focus. To this end, most existing works adopt the strategy\nof explicitly increasing the granularity of visual information processing,\ne.g., incorporating visual prompts to guide the model focus on specific local\nregions within the image. Meanwhile, researches on Multimodal Large Language\nModels(MLLMs) have demonstrated that training with long and detailed textual\ndescriptions can effectively improve the model's fine-grained vision-language\nalignment. However, the inherent token length limitation of CLIP's text encoder\nfundamentally limits CLIP to process more granular textual information embedded\nin long text sequences. To synergistically leverage the advantages of enhancing\nboth visual and textual content processing granularity, we propose PixCLIP, a\nnovel framework designed to concurrently accommodate visual prompt inputs and\nprocess lengthy textual descriptions. Specifically, we first establish an\nautomated annotation pipeline capable of generating pixel-level localized,\nlong-form textual descriptions for images. Utilizing this pipeline, we\nconstruct LongGRIT, a high-quality dataset comprising nearly 1.5 million\nsamples. Secondly, we replace CLIP's original text encoder with the LLM and\npropose a three-branch pixel-text alignment learning framework, facilitating\nfine-grained alignment between image regions and corresponding textual\ndescriptions at arbitrary granularity. Experiments demonstrate that PixCLIP\nshowcases breakthroughs in pixel-level interaction and handling long-form\ntexts, achieving state-of-the-art performance.", "AI": {"tldr": "PixCLIP是一个新颖的框架，旨在通过同时处理视觉提示和长文本描述来增强CLIP模型在细粒度图像-文本对齐方面的能力，并取得了最先进的性能。", "motivation": "CLIP模型在多种视觉语言理解任务中表现出色，但在细粒度图像-文本对齐方面仍有提升空间。现有方法主要通过增加视觉信息处理粒度或利用长文本描述来改进，但CLIP的文本编码器固有的token长度限制了其处理更细粒度长文本信息的能力。本研究旨在协同利用增强视觉和文本内容处理粒度的优势。", "method": "1. 提出了PixCLIP框架，能够同时处理视觉提示输入和长文本描述。2. 建立了一个自动标注管道，用于生成像素级局部化的长文本图像描述。3. 利用该管道构建了LongGRIT数据集（包含近150万个样本）。4. 将CLIP的原始文本编码器替换为大型语言模型（LLM）。5. 提出了一个三分支像素-文本对齐学习框架，以促进图像区域与相应文本描述在任意粒度上的细粒度对齐。", "result": "实验证明，PixCLIP在像素级交互和处理长文本方面取得了突破，实现了最先进的性能。", "conclusion": "PixCLIP通过协同增强视觉和文本内容处理粒度，成功克服了CLIP在处理长文本方面的局限性，显著提升了模型在细粒度图像-文本对齐任务中的表现。"}}
{"id": "2511.04628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04628", "abs": "https://arxiv.org/abs/2511.04628", "authors": ["Kylie Cancilla", "Alexander Moore", "Amar Saini", "Carmen Carrano"], "title": "NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment", "comment": null, "summary": "Video quality assessment (VQA) is vital for computer vision tasks, but\nexisting approaches face major limitations: full-reference (FR) metrics require\nclean reference videos, and most no-reference (NR) models depend on training on\ncostly human opinion labels. Moreover, most opinion-unaware NR methods are\nimage-based, ignoring temporal context critical for video object detection. In\nthis work, we present a scalable, streaming-based VQA model that is both\nno-reference and opinion-unaware. Our model leverages synthetic degradations of\nthe DAVIS dataset, training a temporal-aware convolutional architecture to\npredict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without\nreferences at inference. We show that our streaming approach outperforms our\nown image-based baseline by generalizing across diverse degradations,\nunderscoring the value of temporal modeling for scalable VQA in real-world\nvision systems. Additionally, we demonstrate that our model achieves higher\ncorrelation with full-reference metrics compared to BRISQUE, a widely-used\nopinion-aware image quality assessment baseline, validating the effectiveness\nof our temporal, opinion-unaware approach.", "AI": {"tldr": "本文提出了一种可扩展、基于流的无参考、无主观意见的视频质量评估（VQA）模型，该模型利用合成退化和时间感知卷积架构，直接从退化视频中预测全参考（FR）指标，无需参考视频，并在各种退化条件下表现出色。", "motivation": "现有VQA方法存在局限性：全参考（FR）指标需要干净的参考视频；大多数无参考（NR）模型依赖于昂贵的人类主观意见标签进行训练；且大多数无主观意见的NR方法是基于图像的，忽略了对视频目标检测至关重要的时间上下文。", "method": "本研究提出了一种可扩展、基于流的VQA模型，该模型是无参考且无主观意见的。它利用DAVIS数据集的合成退化数据，训练一个时间感知卷积架构，直接从退化视频中预测FR指标（LPIPS、PSNR、SSIM），在推理时无需参考视频。", "result": "所提出的流式方法在各种退化条件下泛化能力优于其自身的图像基线，突出了时间建模对于可扩展VQA的价值。此外，该模型与全参考指标的相关性高于广泛使用的BRISQUE（一种有主观意见的图像质量评估基线）。", "conclusion": "本研究验证了时间感知、无主观意见的方法在可扩展VQA中的有效性，强调了时间建模对于实际视觉系统的重要性，并证明了其与全参考指标的高度相关性。"}}
{"id": "2511.04595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04595", "abs": "https://arxiv.org/abs/2511.04595", "authors": ["Chen Shi", "Shaoshuai Shi", "Xiaoyang Lyu", "Chunyang Liu", "Kehua Sheng", "Bo Zhang", "Li Jiang"], "title": "UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction", "comment": null, "summary": "Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,\nyet existing methods struggle with the joint challenges of sparse,\nnon-overlapping camera views and complex scene dynamics. We present UniSplat, a\ngeneral feed-forward framework that learns robust dynamic scene reconstruction\nthrough unified latent spatio-temporal fusion. UniSplat constructs a 3D latent\nscaffold, a structured representation that captures geometric and semantic\nscene context by leveraging pretrained foundation models. To effectively\nintegrate information across spatial views and temporal frames, we introduce an\nefficient fusion mechanism that operates directly within the 3D scaffold,\nenabling consistent spatio-temporal alignment. To ensure complete and detailed\nreconstructions, we design a dual-branch decoder that generates dynamic-aware\nGaussians from the fused scaffold by combining point-anchored refinement with\nvoxel-based generation, and maintain a persistent memory of static Gaussians to\nenable streaming scene completion beyond current camera coverage. Extensive\nexperiments on real-world datasets demonstrate that UniSplat achieves\nstate-of-the-art performance in novel view synthesis, while providing robust\nand high-quality renderings even for viewpoints outside the original camera\ncoverage.", "AI": {"tldr": "UniSplat是一个通用的前馈框架，通过统一的潜在时空融合和3D潜在支架，实现自动驾驶中鲁棒的动态场景重建，并在新视图合成方面达到最先进的性能。", "motivation": "现有的前馈3D重建方法难以应对自动驾驶中稀疏、非重叠的摄像机视图和复杂的场景动态带来的联合挑战。", "method": "UniSplat构建了一个3D潜在支架，利用预训练的基础模型捕获几何和语义场景上下文。它引入了一种高效的融合机制，直接在3D支架内操作，实现一致的时空对齐。设计了一个双分支解码器，结合点锚定细化和体素生成，从融合的支架生成动态感知的Gaussians，并维护静态Gaussians的持久记忆，以实现超出当前摄像机覆盖范围的流式场景补全。", "result": "在真实世界数据集上的大量实验表明，UniSplat在新视图合成方面取得了最先进的性能，即使对于原始摄像机覆盖范围之外的视点，也能提供鲁棒且高质量的渲染。", "conclusion": "UniSplat成功解决了自动驾驶中动态场景重建的挑战，通过其创新的统一潜在时空融合和双分支解码器设计，实现了卓越的重建质量和泛化能力。"}}
{"id": "2511.04615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04615", "abs": "https://arxiv.org/abs/2511.04615", "authors": ["Tushar Kataria", "Shikha Dubey", "Mary Bronner", "Jolanta Jedrzkiewicz", "Ben J. Brintz", "Shireen Y. Elhabian", "Beatrice S. Knudsen"], "title": "Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality", "comment": null, "summary": "Deep learning models can generate virtual immunohistochemistry (IHC) stains\nfrom hematoxylin and eosin (H&E) images, offering a scalable and low-cost\nalternative to laboratory IHC. However, reliable evaluation of image quality\nremains a challenge as current texture- and distribution-based metrics quantify\nimage fidelity rather than the accuracy of IHC staining. Here, we introduce an\nautomated and accuracy grounded framework to determine image quality across\nsixteen paired or unpaired image translation models. Using color deconvolution,\nwe generate masks of pixels stained brown (i.e., IHC-positive) as predicted by\neach virtual IHC model. We use the segmented masks of real and virtual IHC to\ncompute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly\nquantify correct pixel - level labeling without needing expert manual\nannotations. Our results demonstrate that conventional image fidelity metrics,\nincluding Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),\nand structural similarity (SSIM), correlate poorly with stain accuracy and\npathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE\nachieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based\nmodels are less reliable in providing accurate IHC positive pixel labels.\nMoreover, whole-slide images (WSI) reveal performance declines that are\ninvisible in patch-based evaluations, emphasizing the need for WSI-level\nbenchmarks. Together, this framework defines a reproducible approach for\nassessing the quality of virtual IHC models, a critical step to accelerate\ntranslation towards routine use by pathologists.", "AI": {"tldr": "本文提出了一种自动化、基于准确性的框架，用于评估虚拟免疫组织化学（IHC）图像的质量。研究发现，传统图像保真度指标与染色准确性相关性差，而配对模型表现更佳，且全玻片图像（WSI）评估对于揭示模型性能下降至关重要。", "motivation": "深度学习模型能从H&E图像生成虚拟IHC染色，提供了一种可扩展、低成本的替代方案。然而，当前基于纹理和分布的图像质量评估指标衡量的是图像保真度而非IHC染色的准确性，导致可靠的图像质量评估仍是一个挑战。", "method": "研究引入了一个自动化且基于准确性的框架，用于评估十六种配对或非配对图像转换模型。通过颜色去卷积技术，生成真实和虚拟IHC图像中棕色（即IHC阳性）像素的掩膜。利用这些分割掩膜计算染色准确性指标（Dice、IoU、Hausdorff距离），直接量化像素级别的正确标记，无需专家手动标注。同时，将结果与传统的图像保真度指标（FID、PSNR、SSIM）进行比较，并进行了全玻片图像（WSI）级别的评估。", "result": "研究结果表明，传统的图像保真度指标（FID、PSNR、SSIM）与染色准确性和病理学家评估的相关性很差。配对模型（如PyramidPix2Pix和AdaptiveNCE）实现了最高的染色准确性，而非配对的扩散模型和GAN模型在提供准确的IHC阳性像素标记方面可靠性较低。此外，全玻片图像（WSI）评估揭示了在基于图像块的评估中不可见的性能下降，强调了WSI级别基准测试的必要性。", "conclusion": "该框架定义了一种可重复的方法来评估虚拟IHC模型的质量，这是加速其向病理学家日常应用转化的关键一步。"}}
{"id": "2511.04675", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04675", "abs": "https://arxiv.org/abs/2511.04675", "authors": ["Jinlai Liu", "Jian Han", "Bin Yan", "Hui Wu", "Fengda Zhu", "Xing Wang", "Yi Jiang", "Bingyue Peng", "Zehuan Yuan"], "title": "InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation", "comment": "NeurIPS 2025 Oral", "summary": "We introduce InfinityStar, a unified spacetime autoregressive framework for\nhigh-resolution image and dynamic video synthesis. Building on the recent\nsuccess of autoregressive modeling in both vision and language, our purely\ndiscrete approach jointly captures spatial and temporal dependencies within a\nsingle architecture. This unified design naturally supports a variety of\ngeneration tasks such as text-to-image, text-to-video, image-to-video, and long\ninteractive video synthesis via straightforward temporal autoregression.\nExtensive experiments demonstrate that InfinityStar scores 83.74 on VBench,\noutperforming all autoregressive models by large margins, even surpassing some\ndiffusion competitors like HunyuanVideo. Without extra optimizations, our model\ngenerates a 5s, 720p video approximately 10x faster than leading\ndiffusion-based methods. To our knowledge, InfinityStar is the first discrete\nautoregressive video generator capable of producing industrial level 720p\nvideos. We release all code and models to foster further research in efficient,\nhigh-quality video generation.", "AI": {"tldr": "InfinityStar是一个统一的时空自回归框架，用于高分辨率图像和动态视频合成。它在质量和速度上显著优于现有自回归模型，甚至超越一些扩散模型，支持多种生成任务。", "motivation": "受视觉和语言领域自回归模型成功的启发，研究旨在开发一个纯离散的统一框架，能够共同捕获空间和时间依赖性，以实现高分辨率图像和动态视频合成。", "method": "引入InfinityStar，一个统一的时空自回归框架。它采用纯离散方法，通过单一架构共同捕获空间和时间依赖性。该设计自然支持文本到图像、文本到视频、图像到视频和长交互式视频合成等多种生成任务，通过直接的时间自回归实现。", "result": "InfinityStar在VBENCH上得分83.74，大幅超越所有自回归模型，甚至超过了部分扩散竞争对手（如HunyuanVideo）。在没有额外优化的情况下，模型生成5秒720p视频的速度比领先的扩散方法快约10倍。据称，InfinityStar是首个能够生成工业级720p视频的离散自回归视频生成器。", "conclusion": "InfinityStar是一个高效、高质量的统一离散自回归模型，能够生成工业级720p视频，并在性能和速度上建立了新的基准，为高效、高质量视频生成领域提供了新的研究方向。"}}
{"id": "2511.04652", "categories": ["cs.CV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.04652", "abs": "https://arxiv.org/abs/2511.04652", "authors": ["Mantas Žurauskas", "Tom Bu", "Sanaz Alali", "Beyza Kalkanli", "Derek Shi", "Fernando Alamos", "Gauresh Pandit", "Christopher Mei", "Ali Behrooz", "Ramin Mirjalili", "Dave Stronks", "Alexander Fix", "Dmitri Model"], "title": "Polarization-resolved imaging improves eye tracking", "comment": null, "summary": "Polarization-resolved near-infrared imaging adds a useful optical contrast\nmechanism to eye tracking by measuring the polarization state of light\nreflected by ocular tissues in addition to its intensity. In this paper we\ndemonstrate how this contrast can be used to enable eye tracking. Specifically,\nwe demonstrate that a polarization-enabled eye tracking (PET) system composed\nof a polarization--filter--array camera paired with a linearly polarized\nnear-infrared illuminator can reveal trackable features across the sclera and\ngaze-informative patterns on the cornea, largely absent in intensity-only\nimages. Across a cohort of 346 participants, convolutional neural network based\nmachine learning models trained on data from PET reduced the median\n95th-percentile absolute gaze error by 10--16\\% relative to capacity-matched\nintensity baselines under nominal conditions and in the presence of eyelid\nocclusions, eye-relief changes, and pupil-size variation. These results link\nlight--tissue polarization effects to practical gains in human--computer\ninteraction and position PET as a simple, robust sensing modality for future\nwearable devices.", "AI": {"tldr": "该研究展示了偏振分辨近红外成像（PET）如何通过揭示巩膜和角膜上的独特特征来显著改善眼动追踪的准确性，与仅基于强度的传统方法相比，在多种挑战条件下将凝视误差降低了10-16%。", "motivation": "传统的眼动追踪主要依赖于光线强度，但眼部组织反射光的偏振状态提供了额外的光学对比机制。本研究旨在探索如何利用这种偏振对比来增强眼动追踪能力。", "method": "研究采用了一个偏振使能眼动追踪（PET）系统，该系统由一个偏振滤光阵列相机和一个线性偏振近红外照明器组成。通过卷积神经网络（CNN）机器学习模型对来自PET系统的数据进行训练，以实现眼动追踪。", "result": "PET系统能够揭示巩膜上可追踪的特征和角膜上与凝视相关的模式，这些特征在仅基于强度的图像中几乎不存在。在346名参与者的数据上，基于PET数据训练的CNN模型将中位数95百分位绝对凝视误差降低了10-16%，优于能力匹配的强度基线，即使在眼睑遮挡、眼距变化和瞳孔大小变化等挑战条件下也表现良好。", "conclusion": "光-组织偏振效应为人机交互带来了实际的收益。PET被定位为未来可穿戴设备中一种简单、鲁棒的传感模式。"}}
{"id": "2511.04670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04670", "abs": "https://arxiv.org/abs/2511.04670", "authors": ["Shusheng Yang", "Jihan Yang", "Pinzhi Huang", "Ellis Brown", "Zihao Yang", "Yue Yu", "Shengbang Tong", "Zihan Zheng", "Yifan Xu", "Muhan Wang", "Daohan Lu", "Rob Fergus", "Yann LeCun", "Li Fei-Fei", "Saining Xie"], "title": "Cambrian-S: Towards Spatial Supersensing in Video", "comment": "Website: https://cambrian-mllm.github.io/", "summary": "We argue that progress in true multimodal intelligence calls for a shift from\nreactive, task-driven systems and brute-force long context towards a broader\nparadigm of supersensing. We frame spatial supersensing as four stages beyond\nlinguistic-only understanding: semantic perception (naming what is seen),\nstreaming event cognition (maintaining memory across continuous experiences),\nimplicit 3D spatial cognition (inferring the world behind pixels), and\npredictive world modeling (creating internal models that filter and organize\ninformation). Current benchmarks largely test only the early stages, offering\nnarrow coverage of spatial cognition and rarely challenging models in ways that\nrequire true world modeling. To drive progress in spatial supersensing, we\npresent VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial\nrecall) and VSC (continual visual spatial counting). These tasks require\narbitrarily long video inputs yet are resistant to brute-force context\nexpansion. We then test data scaling limits by curating VSI-590K and training\nCambrian-S, achieving +30% absolute improvement on VSI-Bench without\nsacrificing general capabilities. Yet performance on VSI-SUPER remains limited,\nindicating that scale alone is insufficient for spatial supersensing. We\npropose predictive sensing as a path forward, presenting a proof-of-concept in\nwhich a self-supervised next-latent-frame predictor leverages surprise\n(prediction error) to drive memory and event segmentation. On VSI-SUPER, this\napproach substantially outperforms leading proprietary baselines, showing that\nspatial supersensing requires models that not only see but also anticipate,\nselect, and organize experience.", "AI": {"tldr": "本文提出“超感知”（supersensing）范式以推动多模态智能发展，超越现有任务驱动和暴力长上下文方法。通过引入VSI-SUPER基准测试和“预测感知”方法，证明了仅靠规模不足以实现高级空间认知，并展示了预测感知在空间超感知任务上的优越性。", "motivation": "当前的真多模态智能进展受限于反应式、任务驱动系统和暴力长上下文处理。现有基准测试主要覆盖空间认知的早期阶段，未能有效挑战模型进行真正的世界建模，缺乏对连续经验、隐式3D空间认知和预测世界建模能力的评估。", "method": "本文提出空间超感知的四个阶段：语义感知、流式事件认知、隐式3D空间认知和预测世界建模。为推动进展，引入VSI-SUPER两部分基准测试：VSR（长时程视觉空间回忆）和VSC（持续视觉空间计数），旨在抵抗暴力上下文扩展。通过构建VSI-590K数据集并训练Cambrian-S模型来测试数据扩展的极限。最后，提出“预测感知”作为前进方向，利用自监督的下一潜在帧预测器，通过“惊喜”（预测误差）驱动记忆和事件分割。", "result": "通过VSI-590K训练的Cambrian-S在VSI-Bench上实现了+30%的绝对提升，且未牺牲通用能力。然而，在VSI-SUPER上的性能依然有限，表明仅靠规模不足以实现空间超感知。所提出的预测感知方法在VSI-SUPER上显著优于领先的专有基线。", "conclusion": "真正的空间超感知需要模型不仅能“看”，还能“预期”、“选择”和“组织”经验。预测感知是一种有前途的方法，而仅仅扩大模型规模不足以实现空间超感知。"}}
{"id": "2511.04655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04655", "abs": "https://arxiv.org/abs/2511.04655", "authors": ["Ellis Brown", "Jihan Yang", "Shusheng Yang", "Rob Fergus", "Saining Xie"], "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable Non-Visual Shortcuts", "comment": "Project page: https://cambrian-mllm.github.io", "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models\n(MLLMs). Yet we find that models can ace many multimodal benchmarks without\nstrong visual understanding, instead exploiting biases, linguistic priors, and\nsuperficial patterns. This is especially problematic for vision-centric\nbenchmarks that are meant to require visual inputs. We adopt a diagnostic\nprinciple for benchmark design: if a benchmark can be gamed, it will be.\nDesigners should therefore try to ``game'' their own benchmarks first, using\ndiagnostic and debiasing procedures to systematically identify and mitigate\nnon-visual biases. Effective diagnosis requires directly ``training on the test\nset'' -- probing the released test set for its intrinsic, exploitable patterns.\n  We operationalize this standard with two components. First, we diagnose\nbenchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.\nOur primary diagnostic tool involves fine-tuning a powerful Large Language\nModel via $k$-fold cross-validation on exclusively the non-visual, textual\ninputs of the test set to reveal shortcut performance and assign each sample a\nbias score $s(x)$. We complement this with a lightweight Random Forest-based\ndiagnostic operating on hand-crafted features for fast, interpretable auditing.\nSecond, we debias benchmarks by filtering high-bias samples using an\n``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four\nbenchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive\nnon-visual biases. As a case study, we apply our full framework to create\nVSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider\nvision-blind performance gap than the original.", "AI": {"tldr": "MLLM模型在多模态基准测试中常利用非视觉偏差而非真正的视觉理解。本文提出“测试集压力测试”（TsT）方法诊断这些偏差，并通过“迭代偏差剪枝”（IBP）程序去除高偏差样本，以创建更稳健的基准。", "motivation": "多模态大语言模型（MLLM）在许多多模态基准测试中表现出色，但这种表现并非源于强大的视觉理解能力，而是利用了偏见、语言先验和表面模式。这对于旨在评估视觉输入的基准尤其成问题，导致无法准确评估模型的真实视觉能力。", "method": "本文遵循“如果基准可以被利用，它就会被利用”的诊断原则。首先，通过“测试集压力测试”（TsT）诊断基准的脆弱性，主要方法是利用k折交叉验证，仅在测试集的非视觉文本输入上微调一个强大的大型语言模型，以揭示捷径性能并分配偏差分数。辅以基于随机森林的轻量级诊断工具进行快速审计。其次，通过“迭代偏差剪枝”（IBP）程序过滤高偏差样本，从而消除基准中的偏差。", "result": "将该框架应用于VSI-Bench、CV-Bench、MMMU和VideoMME四个基准，发现普遍存在的非视觉偏差。以VSI-Bench为例，应用完整框架创建了VSI-Bench-Debiased，结果显示其非视觉可解性降低，并且与原始基准相比，盲视性能差距更大，证明了去偏后的基准更具鲁棒性。", "conclusion": "当前的MLLM基准测试普遍存在非视觉偏差，导致模型无需真正的视觉理解即可取得高分。本文提出的诊断和去偏框架（TsT和IBP）能够有效识别和缓解这些偏差，从而创建更稳健、更能准确评估模型视觉理解能力的多模态基准。"}}
{"id": "2511.04678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04678", "abs": "https://arxiv.org/abs/2511.04678", "authors": ["Yihong Sun", "Xinyu Yang", "Jennifer J. Sun", "Bharath Hariharan"], "title": "Tracking and Understanding Object Transformations", "comment": "NeurIPS 2025", "summary": "Real-world objects frequently undergo state transformations. From an apple\nbeing cut into pieces to a butterfly emerging from its cocoon, tracking through\nthese changes is important for understanding real-world objects and dynamics.\nHowever, existing methods often lose track of the target object after\ntransformation, due to significant changes in object appearance. To address\nthis limitation, we introduce the task of Track Any State: tracking objects\nthrough transformations while detecting and describing state changes,\naccompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we\npresent TubeletGraph, a zero-shot system that recovers missing objects after\ntransformation and maps out how object states are evolving over time.\nTubeletGraph first identifies potentially overlooked tracks, and determines\nwhether they should be integrated based on semantic and proximity priors. Then,\nit reasons about the added tracks and generates a state graph describing each\nobserved transformation. TubeletGraph achieves state-of-the-art tracking\nperformance under transformations, while demonstrating deeper understanding of\nobject transformations and promising capabilities in temporal grounding and\nsemantic reasoning for complex object transformations. Code, additional\nresults, and the benchmark dataset are available at\nhttps://tubelet-graph.github.io.", "AI": {"tldr": "本文引入了“跟踪任意状态”任务，旨在跟踪经历状态转换的物体，同时检测和描述这些状态变化。为此，作者提出了一个零样本系统TubeletGraph和一个新基准数据集VOST-TAS，该系统在转换下实现了最先进的跟踪性能。", "motivation": "现实世界中的物体经常发生状态转换（如苹果被切开、蝴蝶破茧而出），但现有方法在物体外观发生显著变化后往往会丢失目标，这限制了对真实世界物体和动态的理解。", "method": "本文提出了“跟踪任意状态”（Track Any State）任务和VOST-TAS基准数据集。为解决此问题，作者提出了TubeletGraph，一个零样本系统。它首先识别可能被忽略的轨迹，并根据语义和邻近先验来决定是否整合它们。然后，它对添加的轨迹进行推理，并生成一个描述每个观察到的转换的状态图。", "result": "TubeletGraph在状态转换下实现了最先进的跟踪性能，同时展示了对物体转换更深层次的理解，并在复杂物体转换的时间定位和语义推理方面展现出良好的能力。", "conclusion": "TubeletGraph成功解决了物体在状态转换后跟踪丢失的问题，通过检测和描述状态变化，实现了卓越的跟踪性能和对物体转换的深入理解，为复杂物体转换的语义推理提供了有前景的解决方案。"}}
{"id": "2511.04680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04680", "abs": "https://arxiv.org/abs/2511.04680", "authors": ["Rafe Loya", "Andrew Hamara", "Benjamin Estell", "Benjamin Kilpatrick", "Andrew C. Freeman"], "title": "Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping", "comment": "Accepted to the Datasets track of VCIP 2025", "summary": "Automatic image cropping is a method for maximizing the human-perceived\nquality of cropped regions in photographs. Although several works have proposed\ntechniques for producing singular crops, little work has addressed the problem\nof producing multiple, distinct crops with aesthetic appeal. In this paper, we\nmotivate the problem with a discussion on modern social media applications,\nintroduce a dataset of 277 relevant images and human labels, and evaluate the\nefficacy of several single-crop models with an image partitioning algorithm as\na pre-processing step. The dataset is available at\nhttps://github.com/RafeLoya/carousel.", "AI": {"tldr": "本文提出了一种解决自动图像裁剪中生成多个具有美学吸引力且独特的裁剪区域的问题，并为此引入了一个新数据集和评估方法。", "motivation": "现有研究大多关注生成单个高质量的裁剪区域，但现代社交媒体应用场景需要能够生成多个不同且美观的裁剪区域。", "method": "作者首先讨论了现代社交媒体应用中该问题的动机，然后引入了一个包含277张相关图像和人工标注的新数据集。接着，他们将图像分区算法作为预处理步骤，评估了多个单裁剪模型在此场景下的有效性。", "result": "通过将图像分区算法作为预处理步骤，本文评估了多个单裁剪模型在生成多个独特裁剪方面的有效性。数据集已公开。", "conclusion": "本文解决了生成多个独特且具有美学吸引力裁剪区域的问题，并通过引入新数据集和评估方法，为该领域的研究奠定了基础。"}}
{"id": "2511.04668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04668", "abs": "https://arxiv.org/abs/2511.04668", "authors": ["Ellis Brown", "Arijit Ray", "Ranjay Krishna", "Ross Girshick", "Rob Fergus", "Saining Xie"], "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding", "comment": "Project page: https://ellisbrown.github.io/sims-v", "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.", "AI": {"tldr": "多模态语言模型在视频空间推理方面表现不佳，主要受限于真实世界数据的获取。本文提出SIMS-V框架，利用3D模拟器生成空间丰富的视频训练数据，并通过消融实验确定了三种核心问题类型，实现了高效且可迁移的空间智能训练，使小型模型在真实世界空间推理基准上超越大型基线模型。", "motivation": "尽管多模态语言模型在高级视频理解方面表现出色，但在跨时空的空间推理方面仍存在困难。当前的空间训练方法依赖真实世界视频数据，但获取多样化且带有精确空间标注的数据是一个瓶颈。", "method": "本文提出了SIMS-V，一个系统性的数据生成框架，利用3D模拟器的特权信息创建空间丰富的视频训练数据。通过系统性地消融问题类型、组合和规模，研究了模拟数据哪些特性能够驱动有效的真实世界迁移。最终确定了三类问题（度量测量、视角依赖推理和时间跟踪）作为开发可迁移空间智能的最有效集合。", "result": "研究发现，度量测量、视角依赖推理和时间跟踪这三种问题类型组成的最小集合，在开发可迁移空间智能方面最为有效，即使问题类型较少，其表现也优于全面覆盖的方法。通过这种高效训练，一个仅用2.5万个模拟示例微调的7B参数视频LLM，其性能超越了更大的72B基线模型，并在严格的真实世界空间推理基准上与专有模型达到竞争水平。该方法展现出强大的泛化能力，在通用视频理解方面保持性能，同时在具身和真实世界空间任务上显示出显著改进。", "conclusion": "利用3D模拟器生成的空间丰富数据，并专注于特定核心问题类型进行训练，可以高效地为多模态语言模型开发可迁移的空间智能。这种方法能够显著提升模型在真实世界空间推理任务上的表现，即使使用更少的训练数据和更小的模型也能取得优异成果。"}}
