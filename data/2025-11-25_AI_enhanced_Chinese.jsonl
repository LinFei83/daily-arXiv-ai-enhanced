{"id": "2511.17576", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17576", "abs": "https://arxiv.org/abs/2511.17576", "authors": ["Rayan Aldajani"], "title": "Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks", "comment": "2 pages, 2 figures, accepted at IEEE CASCON 2025", "summary": "Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.", "AI": {"tldr": "本研究评估了使用正面身体图像和基本人体测量数据构建的AI模型作为低成本替代方案来估算体脂百分比的可行性，并取得了良好的准确性。", "motivation": "黄金标准体脂测量方法（如DEXA扫描）成本高昂且大多数人难以获得，因此需要开发低成本、易于获取的替代方案。", "method": "研究构建了一个包含535个样本的数据集，其中包括253例记录了人体测量数据（体重、身高、颈围、踝围和腕围）的样本，以及282张从Reddit帖子中抓取的带有自报（部分为DEXA测量）体脂百分比的图像。开发了两种方法：1) 基于ResNet的图像模型；2) 使用人体测量数据的回归模型。此外，还提出了一个多模态融合框架以备未来扩展。", "result": "基于图像的模型取得了4.44%的均方根误差（RMSE）和0.807的决定系数（R^2）。", "conclusion": "研究结果表明，AI辅助模型能够提供可及且低成本的体脂估算，支持未来在健康和健身领域的消费者应用。"}}
{"id": "2511.17715", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.17715", "abs": "https://arxiv.org/abs/2511.17715", "authors": ["Siying Li", "Lang Tong", "Timothy D. Mount"], "title": "Risk-Based Capacity Accreditation of Resource-Colocated Large Loads in Capacity Markets", "comment": null, "summary": "We study capacity accreditation of resource-colocated large loads, defined as large demands such as data center and manufacturing loads colocated with behind-the-meter generation and storage resources, synchronously connected to the bulk power system, and capable of participating in the wholesale electricity market as an integrated unit. Because the qualified capacity of a resource portfolio is not equal to the sum of its individual resources' qualified capacities, we propose a novel risk-based capacity accreditation framework that evaluates the collective contribution to system reliability. Grounded in the effective load carrying capability (ELCC) metric, the proposed capacity accreditation employs a convex optimization engine that jointly dispatches colocated resources to minimize reliability risk. We apply the developed methodology to a hydrogen manufacturing facility with colocated renewable generation, storage, and fuel cell resources.", "AI": {"tldr": "本文提出了一种新颖的基于风险的容量认证框架，用于评估资源共址大型负荷（如数据中心、工厂与电源/储能共址）对系统可靠性的集体贡献。", "motivation": "现有容量认证方法无法准确评估共址资源组合的集体贡献，即组合的合格容量不等于其个体资源合格容量之和。因此，需要一个能将这些集成单元视为一个整体，并评估其对系统可靠性贡献的框架。", "method": "该框架基于有效负荷承载能力（ELCC）指标，采用凸优化引擎共同调度共址资源，以最小化可靠性风险。它通过风险评估来确定资源共址大型负荷的容量认证。", "result": "该方法被应用于一个具有共址可再生能源发电、储能和燃料电池资源的制氢设施，证明了其在实际场景中的适用性。", "conclusion": "所提出的框架能够评估资源共址大型负荷作为整体对电力系统可靠性的集体贡献，提供了一种更准确的容量认证方法。"}}
{"id": "2511.17720", "categories": ["cs.RO", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2511.17720", "abs": "https://arxiv.org/abs/2511.17720", "authors": ["Sean Cowan", "Pietro Fanti", "Leon B. S. Williams", "Chit Hong Yam", "Kaneyasu Asakuma", "Yuichiro Nada", "Dario Izzo"], "title": "Vision-Guided Optic Flow Navigation for Small Lunar Missions", "comment": null, "summary": "Private lunar missions are faced with the challenge of robust autonomous navigation while operating under stringent constraints on mass, power, and computational resources. This work proposes a motion-field inversion framework that uses optical flow and rangefinder-based depth estimation as a lightweight CPU-based solution for egomotion estimation during lunar descent. We extend classical optical flow formulations by integrating them with depth modeling strategies tailored to the geometry for lunar/planetary approach, descent, and landing, specifically, planar and spherical terrain approximations parameterized by a laser rangefinder. Motion field inversion is performed through a least-squares framework, using sparse optical flow features extracted via the pyramidal Lucas-Kanade algorithm. We verify our approach using synthetically generated lunar images over the challenging terrain of the lunar south pole, using CPU budgets compatible with small lunar landers. The results demonstrate accurate velocity estimation from approach to landing, with sub-10% error for complex terrain and on the order of 1% for more typical terrain, as well as performances suitable for real-time applications. This framework shows promise for enabling robust, lightweight on-board navigation for small lunar missions.", "AI": {"tldr": "本文提出了一种基于光流和测距仪深度估计的运动场反演框架，为月球着陆器提供轻量级的自运动估计算法，特别适用于资源受限的私人月球任务。", "motivation": "私人月球任务面临在严格的质量、功耗和计算资源限制下实现鲁棒自主导航的挑战。", "method": "该研究提出了一种运动场反演框架，利用光流和基于测距仪的深度估计作为轻量级CPU解决方案，用于月球下降过程中的自运动估计。通过将经典光流公式与针对月球/行星进近、下降和着陆几何形状定制的深度建模策略（由激光测距仪参数化的平面和球形地形近似）相结合。运动场反演通过最小二乘框架执行，并使用金字塔式Lucas-Kanade算法提取的稀疏光流特征。", "result": "该方法通过合成生成的月球图像（包括月球南极的复杂地形）进行了验证，并使用了与小型月球着陆器兼容的CPU预算。结果表明，从进近到着陆，速度估计准确，对于复杂地形误差低于10%，对于典型地形误差约为1%，并且性能适用于实时应用。", "conclusion": "该框架有望为小型月球任务提供鲁棒、轻量级的机载导航能力。"}}
{"id": "2511.17603", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17603", "abs": "https://arxiv.org/abs/2511.17603", "authors": ["Chelsea-Xi Chen", "Zhe Zhang", "Aven-Le Zhou"], "title": "Translating Cultural Choreography from Humanoid Forms to Robotic Arm", "comment": null, "summary": "Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \\textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.", "AI": {"tldr": "本研究提出ROPRA，一个三阶段流程，通过符号姿态转移实现机械臂编舞的文化语义保真和形态可移植性，并以昆曲《牡丹亭》为例进行了评估。", "motivation": "机械臂编舞在轨迹再现时常缺失文化语义。本研究旨在探索符号姿态转移（采用关节空间兼容的符号表示）能否在六自由度机械臂上保持语义保真，并实现跨形态的可移植性。", "method": "研究实现了ROPRA，一个包含三个阶段的管道：编码文化规范姿态、编排符号序列、解码为伺服指令。以昆曲《牡丹亭》中的一个场景作为评估材料。具体流程包括基于语料库的姿态选择、符号评分、直接关节角度执行，以及结合光绘和服装色彩的视觉层。", "result": "结果显示，机械臂能够以预期的时序和文化可读性重现编舞，并得到了专家和观众的认可。", "conclusion": "这项研究指出了非拟人化的文化遗产保护和可移植的创作工作流程。未来的工作将设计舞蹈启发式的过渡曲线，将符号表示扩展到包含触觉、音乐和空间线索的运动，并测试跨平台的可移植性。"}}
{"id": "2511.17540", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17540", "abs": "https://arxiv.org/abs/2511.17540", "authors": ["Ryudai Iwakami", "Bo Peng", "Hiroyuki Hanyu", "Tasuku Ishigooka", "Takuya Azumi"], "title": "AUTOSAR AP and ROS 2 Collaboration Framework", "comment": "9 pages. This version includes minor \\lstlisting configuration adjustments for successful compilation. The page count is now nine pages due to the addition of author information. There are no other significant changes to the content or layout. Originally published at Euromicro Conference DSD 2024", "summary": "The field of autonomous vehicle research is advancing rapidly, necessitating platforms that meet real-time performance, safety, and security requirements for practical deployment. AUTOSAR Adaptive Platform (AUTOSAR AP) is widely adopted in development to meet these criteria; however, licensing constraints and tool implementation challenges limit its use in research. Conversely, Robot Operating System 2 (ROS 2) is predominantly used in research within the autonomous driving domain, leading to a disparity between research and development platforms that hinders swift commercialization. This paper proposes a collaboration framework that enables AUTOSAR AP and ROS 2 to communicate with each other using a Data Distribution Service for Real-Time Systems (DDS). In contrast, AUTOSAR AP uses Scalable service-Oriented Middleware over IP (SOME/IP) for communication. The proposed framework bridges these protocol differences, ensuring seamless interaction between the two platforms. We validate the functionality and performance of our bridge converter through empirical analysis, demonstrating its efficiency in conversion time and ease of integration with ROS 2 tools. Furthermore, the availability of the proposed collaboration framework is improved by automatically generating a configuration file for the proposed bridge converter.", "AI": {"tldr": "本文提出了一个协作框架，通过使用DDS作为通信桥梁，实现AUTOSAR AP和ROS 2之间的通信，旨在弥合自动驾驶领域研究平台与开发平台之间的差距，促进快速商业化。", "motivation": "自动驾驶研究需要满足实时性、安全性和安全性要求的平台。AUTOSAR AP广泛用于开发，但其许可和工具限制了研究使用。ROS 2主要用于研究，导致研究与开发平台之间存在差异，阻碍了快速商业化。", "method": "提出一个协作框架，使AUTOSAR AP和ROS 2能够使用数据分发服务（DDS）进行通信。该框架解决了AUTOSAR AP使用的基于IP的可伸缩面向服务中间件（SOME/IP）与DDS之间的协议差异。此外，还通过自动生成桥接转换器的配置文件来提高框架的可用性。", "result": "通过实证分析验证了桥接转换器的功能和性能，证明了其在转换时间方面的效率以及与ROS 2工具集成的简易性。自动生成配置文件的功能进一步提升了框架的可用性。", "conclusion": "所提出的协作框架成功地弥合了AUTOSAR AP和ROS 2之间的通信协议差异，为自动驾驶领域的研究与开发提供了一个有效的集成解决方案，并提升了可用性，从而有助于加速商业化进程。"}}
{"id": "2511.17559", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.17559", "abs": "https://arxiv.org/abs/2511.17559", "authors": ["Gyubok Lee", "Woosog Chay", "Edward Choi"], "title": "SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering", "comment": "ML4H 2025 Proceedings", "summary": "Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.", "AI": {"tldr": "本文介绍了SCARE，一个用于评估电子健康记录（EHR）问答系统中，文本到SQL模型生成查询后置验证机制的基准，旨在提高临床环境中的安全性。", "motivation": "尽管大型语言模型（LLMs）在文本到SQL方面取得进展，但将它们部署到安全关键的临床EHR问答系统仍面临挑战。不正确的SQL查询可能危及患者护理。现有工作主要关注提高SQL生成准确性或预执行过滤，缺乏统一的基准来评估独立的事后验证机制，而这对于安全部署至关重要。", "method": "研究引入了SCARE基准，用于评估EHR问答系统中作为后置安全层的验证方法。SCARE评估联合任务：1) 分类问题可回答性（可回答、模糊或不可回答），以及 2) 验证或纠正候选SQL查询。该基准包含4,200个问题、候选SQL查询和预期模型输出的三元组，数据来源于MIMIC-III、MIMIC-IV和eICU数据库。候选SQL查询由七个不同的文本到SQL模型生成。研究使用SCARE对两阶段方法和代理框架等多种方法进行了基准测试。", "result": "实验揭示了问题分类和SQL错误纠正之间存在关键的权衡，突出了主要挑战。", "conclusion": "SCARE基准为评估EHR问答系统中的后置安全层提供了工具，并揭示了文本到SQL模型安全部署的关键挑战和未来研究方向。"}}
{"id": "2511.17651", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.17651", "abs": "https://arxiv.org/abs/2511.17651", "authors": ["Tommaso Milanese", "Baris Can Efe", "Claudio Bruschini", "Nobukazu Teranishi", "Edoardo Charbon"], "title": "Reconfigurable, large-format D-ToF/photon-counting SPAD image sensors with embedded FPGA for scene adaptability", "comment": "Presented at the International Image Sensor Workshop 2025", "summary": "CMOS-compatible single-photon avalanche diodes (SPADs) have emerged in many systems as the solution of choice for cameras with photon-number resolution and photon counting capabilities. Being natively digital optical interfaces, SPADs are naturally drawn to in situ logic processing and event-driven computation; they are usually coupled to discrete FPGAs to enable reconfigurability. In this work, we propose to bring the FPGA on-chip, in direct contact with the SPADs at pixel or cluster level. To demonstrate the suitability of this approach, we created an architecture for processing timestamps and photon counts using programmable weighted sums based on an efficient use of look-up tables. The outputs are processed hierarchically, similarly to what is done in FPGAs, reducing power consumption and simplifying I/Os. Finally, we show how artificial neural networks can be designed and reprogrammed by using look-up tables in an efficient way.", "AI": {"tldr": "本文提出了一种将类FPGA的可重构逻辑直接集成到CMOS兼容的单光子雪崩二极管（SPAD）芯片上的架构，用于光子计数和时间戳处理，并通过查找表高效实现人工神经网络。", "motivation": "SPAD是光子数分辨率和光子计数相机的首选方案，它们本质上是数字光学接口，非常适合原位逻辑处理和事件驱动计算。然而，它们通常与分立的FPGA耦合以实现可重构性。研究的动机是将FPGA直接集成到芯片上，与SPAD在像素或簇级别直接接触，以提高效率。", "method": "提出了一种将FPGA集成到芯片上、与SPAD在像素或簇级别直接接触的架构。通过使用查找表（LUT）高效实现的基于可编程加权和，来处理时间戳和光子计数，以证明该方法的适用性。采用分层处理方式，类似于FPGA，以降低功耗并简化I/O。", "result": "该架构能够高效处理时间戳和光子计数。分层处理有效降低了功耗并简化了I/O。此外，展示了如何利用查找表高效设计和重新编程人工神经网络。", "conclusion": "将类FPGA的可重构逻辑直接集成到SPAD芯片上，为光子计数、时间戳处理和事件驱动计算提供了一种高效、低功耗且I/O简化的解决方案，并能有效支持人工神经网络的设计与重编程。"}}
{"id": "2511.17600", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17600", "abs": "https://arxiv.org/abs/2511.17600", "authors": ["Narumasa Tsutsumida", "Rei Mitsuhashi", "Yoshito Sawada", "Akira Kato"], "title": "SALPA: Spaceborne LiDAR Point Adjustment for Enhanced GEDI Footprint Geolocation", "comment": "21 pages, 2 figures", "summary": "Spaceborne Light Detection and Ranging (LiDAR) systems, such as NASA's Global Ecosystem Dynamics Investigation (GEDI), provide forest structure for global carbon assessments. However, geolocation uncertainties (typically 5-15 m) propagate systematically through derived products, undermining forest profile estimates, including carbon stock assessments. Existing correction methods face critical limitations: waveform simulation approaches achieve meter-level accuracy but require high-resolution LiDAR data unavailable in most regions, while terrain-based methods employ deterministic grid searches that may overlook optimal solutions in continuous solution spaces. We present SALPA (Spaceborne LiDAR Point Adjustment), a multi-algorithm optimization framework integrating three optimization paradigms with five distance metrics. Operating exclusively with globally available digital elevation models and geoid data, SALPA explores continuous solution spaces through gradient-based, evolutionary, and swarm intelligence approaches. Validation across contrasting sites: topographically complex Nikko, Japan, and flat Landes, France, demonstrates 15-16% improvements over original GEDI positions and 0.5-2% improvements over the state-of-the-art GeoGEDI algorithm. L-BFGS-B with Area-based metrics achieves optimal accuracy-efficiency trade-offs, while population-based algorithms (genetic algorithms, particle swarm optimization) excel in complex terrain. The platform-agnostic framework facilitates straightforward adaptation to emerging spaceborne LiDAR missions, providing a generalizable foundation for universal geolocation correction essential for reliable global forest monitoring and climate policy decisions.", "AI": {"tldr": "该研究提出了SALPA，一个多算法优化框架，用于校正星载LiDAR（如GEDI）数据的地理定位不确定性。它利用全球可用的DEM和大地水准面数据，通过探索连续解空间，显著提高了森林结构估算的准确性，优于现有方法。", "motivation": "星载LiDAR系统（如GEDI）的地理定位不确定性（通常为5-15米）会系统性地影响森林剖面和碳储量估算。现有校正方法存在局限性：波形模拟方法需要高分辨率LiDAR数据，而地形方法采用确定性网格搜索，可能忽略连续解空间中的最优解。", "method": "本文提出了SALPA（Spaceborne LiDAR Point Adjustment），一个集成了三种优化范式（基于梯度的、进化的和群智能方法）与五种距离度量的多算法优化框架。SALPA仅使用全球可用的数字高程模型和大地水准面数据，探索连续的解空间。", "result": "在地形复杂的日本日光和平坦的法国朗德地区进行验证，结果显示SALPA比原始GEDI定位精度提高了15-16%，比最先进的GeoGEDI算法提高了0.5-2%。L-BFGS-B与基于面积的度量实现了最佳的精度-效率权衡，而基于群体的算法（遗传算法、粒子群优化）在复杂地形中表现出色。", "conclusion": "SALPA是一个平台无关的、可推广的通用地理定位校正框架，对可靠的全球森林监测和气候政策决策至关重要。它提供了一个通用的基础，可轻松适应新兴的星载LiDAR任务。"}}
{"id": "2511.17596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17596", "abs": "https://arxiv.org/abs/2511.17596", "authors": ["Yassir Benhammou", "Suman Kalyan", "Sujay Kumar"], "title": "Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding", "comment": "8 pages, 5 figures, 4 tables", "summary": "Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.", "AI": {"tldr": "本文提出了一种多模态自动编码器（MMAE），通过学习文本、音频和视觉数据的统一表示，实现广播内容元数据提取和语义聚类的端到端自动化，并在聚类和对齐指标上显著优于基线。", "motivation": "广播和媒体组织日益依赖AI自动化内容索引和元数据生成，但现有AI系统通常仅限于单一模态（如视频、音频或文本），限制了它们对广播材料中复杂跨模态关系的理解。", "method": "提出多模态自动编码器（MMAE），旨在学习文本、音频和视觉数据的统一表示。该模型在LUMA数据集上进行训练，通过最小化跨模态的联合重建损失，发现模态不变的语义结构，而无需依赖大型配对或对比数据集。", "result": "与线性基线相比，MMAE在聚类和对齐指标（Silhouette、ARI、NMI）上显示出显著改进。这表明基于重建的多模态嵌入可以作为广播档案中可扩展元数据生成和跨模态检索的基础。", "conclusion": "重建驱动的多模态学习在增强现代广播工作流程中的自动化、可搜索性和内容管理效率方面具有巨大潜力。"}}
{"id": "2511.17730", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.17730", "abs": "https://arxiv.org/abs/2511.17730", "authors": ["Zeinab Nezami", "Shehr Bano", "Abdelaziz Salama", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Safety and Risk Pathways in Cooperative Generative Multi-Agent Systems: A Telecom Perspective", "comment": null, "summary": "Generative multiagent systems are rapidly emerging as transformative tools for scalable automation and adaptive decisionmaking in telecommunications. Despite their promise, these systems introduce novel risks that remain underexplored, particularly when agents operate asynchronously across layered architectures. This paper investigates key safety pathways in telecomfocused Generative MultiAgent Systems (GMAS), emphasizing risks of miscoordination and semantic drift shaped by persona diversity. We propose a modular safety evaluation framework that integrates agentlevel checks on code quality and compliance with systemlevel safety metrics. Using controlled simulations across 32 persona sets, five questions, and multiple iterative runs, we demonstrate progressive improvements in analyzer penalties and AllocatorCoder consistency, alongside persistent vulnerabilities such as policy drift and variability under specific persona combinations. Our findings provide the first domaingrounded evidence that persona design, coding style, and planning orientation directly influence the stability and safety of telecom GMAS, highlighting both promising mitigation strategies and open risks for future deployment.", "AI": {"tldr": "本文研究电信领域生成式多智能体系统（GMAS）的安全风险，特别是异步操作和角色多样性导致的误协调和语义漂移。提出一个模块化安全评估框架，并通过模拟实验证明角色设计、编码风格和规划方向直接影响系统稳定性与安全性，并揭示了改进点和持续存在的漏洞。", "motivation": "生成式多智能体系统在电信领域的自动化和自适应决策方面具有巨大潜力，但其引入的新型风险（特别是当智能体在分层架构中异步操作时）尚未得到充分探索，尤其是由角色多样性引起的误协调和语义漂移风险。", "method": "提出一个模块化安全评估框架，该框架整合了智能体层面的代码质量和合规性检查，以及系统层面的安全指标。通过对32个角色组合、5个问题和多次迭代运行进行受控模拟实验。", "result": "实验结果显示分析器惩罚和 AllocatorCoder 一致性逐步改善，但特定角色组合下仍存在策略漂移和变异等持续性漏洞。研究首次提供了领域实证证据，表明角色设计、编码风格和规划方向直接影响电信GMAS的稳定性和安全性。", "conclusion": "角色设计、编码风格和规划方向是影响电信领域生成式多智能体系统稳定性与安全性的关键因素。研究揭示了有前景的缓解策略，同时也指出了未来部署中仍需解决的开放风险。"}}
{"id": "2511.17608", "categories": ["cs.RO", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.17608", "abs": "https://arxiv.org/abs/2511.17608", "authors": ["Yunlong Guo", "John Canning", "Zenon Chaczko", "Gang-Ding Peng"], "title": "Robot joint characterisation and control using a magneto-optical rotary encoder", "comment": null, "summary": "A robust and compact magneto-optical rotary encoder for the characterisation of robotic rotary joints is demonstrated. The system employs magnetic field-induced optical attenuation in a double-pass configuration using rotating nonuniform magnets around an optical circulator operating in reflection. The encoder tracks continuous 360° rotation with rotation sweep rates from ν = 135 °/s to ν = 370 °/s, and an angular resolution of Δθ = 0.3°. This offers a low-cost and reliable alternative to conventional robot rotation encoders while maintaining competitive performance.", "AI": {"tldr": "本文展示了一种紧凑且鲁棒的磁光旋转编码器，用于机器人旋转关节的特性表征，通过磁场诱导的光学衰减实现，具有0.3°的角分辨率和360°连续跟踪能力。", "motivation": "现有机器人旋转编码器成本较高或可靠性不足，需要一种低成本、可靠且性能具有竞争力的替代方案来表征机器人旋转关节。", "method": "该系统采用磁场诱导的光学衰减原理，通过双程配置实现。具体方法是使用围绕光学环形器（工作在反射模式）旋转的非均匀磁体。", "result": "该编码器能够连续跟踪360°旋转，旋转扫描速率从135 °/s到370 °/s，并实现了0.3°的角分辨率。", "conclusion": "所提出的磁光旋转编码器为传统的机器人旋转编码器提供了一种低成本、可靠的替代方案，同时保持了具有竞争力的性能。"}}
{"id": "2511.17597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17597", "abs": "https://arxiv.org/abs/2511.17597", "authors": ["Zhengsen Xu", "Sibo Cheng", "Hongjie He", "Lanying Wang", "Wentao Sun", "Jonathan Li", "Lincoln Linlin Xu"], "title": "BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction", "comment": "This paper has been accepted by AAAI-26", "summary": "Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire", "AI": {"tldr": "本文提出了一个涵盖25年、日分辨率、多模态驱动因素的野火风险预测基准数据集，覆盖不列颠哥伦比亚省及其周边区域，并评估了多种时间序列预测模型。", "motivation": "野火风险预测因燃料、气象、地形和人类活动的复杂交互而极具挑战性。尽管数据驱动方法日益受到关注，但支持长期时间建模、大规模空间覆盖和多模态驱动因素的公开基准数据集仍然稀缺。", "method": "研究团队构建了一个25年、日分辨率的野火数据集，覆盖2.4亿公顷，包含38个协变量，包括活跃火情探测、气象变量、燃料状况、地形特征和人为因素。基于此基准，他们评估了多种时间序列预测模型，包括基于CNN、线性、Transformer和Mamba的架构，并研究了位置嵌入的有效性以及不同火灾驱动因素的相对重要性。", "result": "该研究提供了一个用于野火风险预测的综合基准数据集。通过评估多种时间序列预测模型，研究探讨了不同模型架构的有效性，并分析了位置嵌入以及不同火灾驱动因素在预测中的相对重要性。", "conclusion": "本文通过发布一个大规模、多模态、长期时间序列的野火数据集及其代码，填补了现有数据资源的空白，为野火风险预测研究提供了一个重要的基准和工具。"}}
{"id": "2511.17744", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17744", "abs": "https://arxiv.org/abs/2511.17744", "authors": ["Jinyi Hao", "Jie Wang", "Kotaro Tsuboi", "Liqin Gao", "Tristan T. Hormel", "Yukun Guo", "An-Lun Wu", "Min Gao", "Christina J. Flaxel", "Steven T. Bailey", "Thomas S. Hwang", "Yali Jia"], "title": "Robust Detection of Retinal Neovascularization in Widefield Optical Coherence Tomography", "comment": "17 pages, 11 figures. Submitted to Optica. Corresponding author: Yali Jia. Affiliations: ((1) Casey Eye Institute, Oregon Health & Science University, USA (2) Department of Ophthalmology, Aichi Medical University, Japan (3) Department of Biomedical Engineering, Oregon Health & Science University, USA (4) Department of Ophthalmology, Mackay Memorial Hospital, Taiwan)", "summary": "Retinal neovascularization (RNV) is a vision threatening development in diabetic retinopathy (DR). Vision loss associated with RNV is preventable with timely intervention, making RNV clinical screening and monitoring a priority. Optical coherence tomography (OCT) angiography (OCTA) provides high-resolution imaging and high-sensitivity detection of RNV lesions. With recent commercial devices introducing widefield OCTA imaging to the clinic, the technology stands to improve early detection of RNV pathology. However, to meet clinical requirements these imaging capabilities must be combined with effective RNV detection and quantification, but existing algorithms for OCTA images are optimized for conventional, i.e. narrow, fields of view. Here, we present a novel approach for RNV diagnosis and staging on widefield OCT/OCTA. Unlike conventional methods dependent on multi-layer retinal segmentation, our model reframes RNV identification as a direct binary localization task. Our fully automated approach was trained and validated on 589 widefield scans (17x17-mm to 26x21-mm) collected from multiple devices at multiple clinics. Our method achieved a device-dependent area under curve (AUC) ranging from 0.96 to 0.99 for RNV diagnosis, and mean intersection over union (IOU) ranging from 0.76 to 0.88 for segmentation. We also demonstrate our method's ability to monitor lesion growth longitudinally. Our results indicate that deep learning-based analysis for widefield OCTA images could offer a valuable means for improving RNV screening and management.", "AI": {"tldr": "本研究提出了一种基于深度学习的自动化方法，用于在宽视野OCT/OCTA图像上进行视网膜新生血管（RNV）的诊断和分期，并取得了高精度，可用于改善RNV筛查和管理。", "motivation": "视网膜新生血管（RNV）是糖尿病视网膜病变（DR）中威胁视力的并发症，早期干预可预防视力丧失。宽视野OCTA技术有望改善RNV的早期检测，但现有算法主要针对窄视野图像，无法满足临床对宽视野图像的检测和量化需求。", "method": "本研究提出了一种新颖的全自动化深度学习方法，将RNV识别重新定义为直接的二元定位任务，而非传统的依赖多层视网膜分割。该模型在来自多个临床机构、多种设备的589例宽视野扫描（17x17-mm至26x21-mm）数据上进行了训练和验证。", "result": "该方法在RNV诊断方面实现了0.96至0.99（取决于设备）的曲线下面积（AUC），在分割方面实现了0.76至0.88的平均交并比（IOU）。研究还证明了该方法能够纵向监测病灶的生长。", "conclusion": "研究结果表明，基于深度学习的宽视野OCTA图像分析可以为改善RNV筛查和管理提供有价值的手段。"}}
{"id": "2511.17765", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17765", "abs": "https://arxiv.org/abs/2511.17765", "authors": ["Darren Chiu", "Zhehui Huang", "Ruohai Ge", "Gaurav S. Sukhatme"], "title": "LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation", "comment": "20 pages, 15 figures", "summary": "Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.", "AI": {"tldr": "本文提出LEARN，一个轻量级、安全引导的两阶段强化学习框架，用于多纳米无人机在杂乱空间中的导航，结合低分辨率ToF传感器和紧凑的注意力策略，实现了高效的板载飞行。", "motivation": "纳米无人机团队具有高度灵活性，但由于机载传感、通信和计算能力受限，面临严峻的导航挑战。现有方法依赖高分辨率视觉或计算密集型规划器，不适用于此类平台。", "method": "LEARN是一个轻量级、两阶段的安全引导强化学习(RL)框架。它结合了低分辨率飞行时间(ToF)传感器、一个简单的运动规划器和一个紧凑的、基于注意力的RL策略。", "result": "在模拟中，LEARN在资源消耗显著减少的情况下，性能优于两种最先进的规划器10%。在六架Crazyflie四旋翼机上，LEARN在各种室内外环境中实现了完全板载飞行，速度高达2.0米/秒，并成功穿越了0.2米的间隙。", "conclusion": "LEARN框架为纳米无人机团队在杂乱空间中的导航提供了一个可行、高效且资源友好的解决方案，通过结合轻量级传感器和强化学习策略，克服了传统方法的局限性。"}}
{"id": "2511.17560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17560", "abs": "https://arxiv.org/abs/2511.17560", "authors": ["Yuechi Zhou", "Yi Su", "Jianxin Zhang", "Juntao Li", "Qingrong Xia", "Zhefeng Wang", "Xinyu Duan", "Baoxing Huai"], "title": "$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\\textbf{A}$ttention-$\\textbf{A}$ware $\\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\\times$.", "AI": {"tldr": "本文提出了一种名为$A^3$（Attention-Aware Accurate KV Cache Fusion）的算法，通过选择性地融合与问题相关的文本块的KV Cache，解决了大型语言模型在处理长上下文时存在的解码延迟和内存开销问题，同时保持了任务性能并显著减少了首个token的生成时间。", "motivation": "大型语言模型（LLMs）虽能处理长上下文，但解码延迟和内存开销巨大，阻碍了实际部署。现有的KV Cache重用方法存在明显的性能下降问题，且重计算的token往往未能与最相关的上下文片段对齐，导致关键上下文表示更新不当。", "method": "通过深入研究基于重计算的重用方法，发现其未能有效对齐相关上下文。因此，本文提出了$A^3$算法，该算法根据文本块与问题的相关性，预先计算并选择性地融合其KV Cache，以实现精确集成并最小化计算开销。", "result": "在各种基准测试和LLMs上的广泛实验表明，$A^3$算法在任务性能上优于四种基线方法，并将首个token的生成时间（TTFT）缩短了2倍。", "conclusion": "$A^3$算法通过注意力感知的KV Cache融合，有效地解决了LLMs在长上下文处理中的效率和性能权衡问题，实现了性能最佳且效率显著提升的效果。"}}
{"id": "2511.17561", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17561", "abs": "https://arxiv.org/abs/2511.17561", "authors": ["Huimin Ren", "Yan Liang", "Baiqiao Su", "Chaobo Sun", "Hengtong Lu", "Kaike Zhang", "Chen Wei"], "title": "LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models", "comment": null, "summary": "The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.", "AI": {"tldr": "本文介绍了LexInstructEval，一个用于评估大型语言模型（LLMs）细粒度词汇指令遵循能力的新基准和评估框架，通过基于规则的语法和程序化验证来克服现有评估方法的局限性。", "motivation": "当前评估LLM遵循复杂细粒度词汇指令能力的方法存在显著挑战：人工评估主观且昂贵；LLM作为判官的系统存在固有偏见和不可靠性；现有程序化基准缺乏表达能力来测试复杂的组合约束。", "method": "研究引入了LexInstructEval框架，该框架基于一个形式化的、基于规则的语法，将复杂指令分解为规范的<Procedure, Relation, Value>三元组。通过多阶段、人工参与的流水线系统生成多样化数据集，并通过透明的程序化引擎进行客观验证。", "result": "研究发布了LexInstructEval数据集和开源评估工具，旨在促进对LLM可控性和可靠性的进一步研究。", "conclusion": "LexInstructEval提供了一个客观、系统且表达力强的评估框架，用于测试LLM遵循细粒度词汇指令的能力，从而有助于推动LLM可控性和可靠性研究的发展。"}}
{"id": "2511.17865", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17865", "abs": "https://arxiv.org/abs/2511.17865", "authors": ["Suk Ki Lee", "Ronnie F. P. Stone", "Max Gao", "Wenlong Zhang", "Zhenghui Sha", "Hyunwoong Ko"], "title": "Generative Model Predictive Control in Manufacturing Processes: A Review", "comment": "24 pages, 5 figures, Review article", "summary": "Manufacturing processes are inherently dynamic and uncertain, with varying parameters and nonlinear behaviors, making robust control essential for maintaining quality and reliability. Traditional control methods often fail under these conditions due to their reactive nature. Model Predictive Control (MPC) has emerged as a more advanced framework, leveraging process models to predict future states and optimize control actions. However, MPC relies on simplified models that often fail to capture complex dynamics, and it struggles with accurate state estimation and handling the propagation of uncertainty in manufacturing environments. Machine learning (ML) has been introduced to enhance MPC by modeling nonlinear dynamics and learning latent representations that support predictive modeling, state estimation, and optimization. Yet existing ML-driven MPC approaches remain deterministic and correlation-focused, motivating the exploration of generative. Generative ML offers new opportunities by learning data distributions, capturing hidden patterns, and inherently managing uncertainty, thereby complementing MPC. This review highlights five representative methods and examines how each has been integrated into MPC components, including predictive modeling, state estimation, and optimization. By synthesizing these cases, we outline the common ways generative ML can systematically enhance MPC and provide a framework for understanding its potential in diverse manufacturing processes. We identify key research gaps, propose future directions, and use a representative case to illustrate how generative ML-driven MPC can extend broadly across manufacturing. Taken together, this review positions generative ML not as an incremental add-on but as a transformative approach to reshape predictive control for next-generation manufacturing systems.", "AI": {"tldr": "该综述探讨了生成式机器学习如何通过学习数据分布、捕捉隐藏模式和固有地管理不确定性，来变革性地增强模型预测控制（MPC），以应对下一代制造系统中的复杂动态和不确定性挑战。", "motivation": "传统控制方法在动态不确定制造环境中表现不佳。模型预测控制（MPC）虽先进，但其简化模型难以捕捉复杂动态，且在准确状态估计和不确定性传播方面存在局限。现有机器学习驱动的MPC方法多为确定性且侧重于相关性，未能充分处理不确定性。因此，需要探索生成式机器学习（Generative ML）来弥补这些不足，以实现更鲁棒的预测控制。", "method": "本文综述了五种代表性的生成式机器学习方法，并分析了它们如何被整合到模型预测控制（MPC）的各个组件中，包括预测建模、状态估计和优化。通过综合这些案例，作者概述了生成式机器学习系统性增强MPC的常见方式，并提供了一个理解其在不同制造过程中潜力的框架。同时，文章还识别了关键研究空白，提出了未来研究方向，并通过一个代表性案例说明了其广泛应用前景。", "result": "研究结果表明，生成式机器学习能够系统性地增强MPC，提供了一个理解其在多样化制造过程中潜力的新框架。它被定位为一种变革性方法，而非简单的增量附加，能够重塑下一代制造系统的预测控制。生成式ML通过学习数据分布、捕捉隐藏模式和固有地管理不确定性，有效解决了传统MPC的局限性。", "conclusion": "生成式机器学习并非对模型预测控制（MPC）的简单补充，而是一种具有变革意义的方法，能够从根本上重塑下一代制造系统的预测控制。它通过有效处理复杂动态和固有的不确定性，为提高制造过程的质量和可靠性提供了新的范式。"}}
{"id": "2511.17578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17578", "abs": "https://arxiv.org/abs/2511.17578", "authors": ["Neelotpal Dutta", "Tianyu Zhang", "Tao Liu", "Yongxue Chen", "Charlie C. L. Wang"], "title": "Implicit Neural Field-Based Process Planning for Multi-Axis Manufacturing: Direct Control over Collision Avoidance and Toolpath Geometry", "comment": null, "summary": "Existing curved-layer-based process planning methods for multi-axis manufacturing address collisions only indirectly and generate toolpaths in a post-processing step, leaving toolpath geometry uncontrolled during optimization. We present an implicit neural field-based framework for multi-axis process planning that overcomes these limitations by embedding both layer generation and toolpath design within a single differentiable pipeline. Using sinusoidally activated neural networks to represent layers and toolpaths as implicit fields, our method enables direct evaluation of field values and derivatives at any spatial point, thereby allowing explicit collision avoidance and joint optimization of manufacturing layers and toolpaths. We further investigate how network hyperparameters and objective definitions influence singularity behavior and topology transitions, offering built-in mechanisms for regularization and stability control. The proposed approach is demonstrated on examples in both additive and subtractive manufacturing, validating its generality and effectiveness.", "AI": {"tldr": "本文提出了一种基于隐式神经场的框架，用于多轴工艺规划，将层生成和刀具路径设计整合到单一可微分管道中，实现了直接碰撞避免和联合优化。", "motivation": "现有的基于曲面层的多轴制造工艺规划方法仅间接处理碰撞，并在后处理步骤中生成刀具路径，导致优化过程中刀具路径几何形状无法控制。", "method": "该方法使用正弦激活神经网络将层和刀具路径表示为隐式场，从而可以直接评估任何空间点的场值和导数。这使得能够进行显式碰撞避免和制造层与刀具路径的联合优化。此外，还研究了网络超参数和目标定义如何影响奇异行为和拓扑转换，提供了内置的正则化和稳定性控制机制。", "result": "所提出的方法在增材制造和减材制造的例子中都得到了验证，展示了其通用性和有效性。它实现了显式碰撞避免和制造层与刀具路径的联合优化。", "conclusion": "该隐式神经场方法成功地将层生成和刀具路径设计集成在一起，克服了现有方法的局限性。它通过直接碰撞避免和联合优化提高了工艺规划的效率和控制力，并提供了正则化和稳定性控制机制，适用于多种制造场景。"}}
{"id": "2511.17565", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17565", "abs": "https://arxiv.org/abs/2511.17565", "authors": ["Sarthak Chakraborty", "Suman Nath", "Xuchao Zhang", "Chetan Bansal", "Indranil Gupta"], "title": "Generative Caching for Structurally Similar Prompts and Responses", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \\ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \\ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \\ourmethod{} achieves 83\\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\\sim$20\\% and reduces end-to-end execution latency by $\\sim$34\\% compared to standard prompt matching.", "AI": {"tldr": "本文提出了一种名为\\ourmethod{}的生成式缓存，旨在解决大型语言模型（LLMs）在处理结构相似但非完全相同的提示时，传统缓存方法（精确匹配和语义缓存）的局限性，从而提高缓存命中率并降低执行延迟。", "motivation": "LLMs在可重复工作流和智能体设置中被广泛应用，其中提示常以相似结构重复使用，但存在细微变体。传统精确提示匹配对此类情况无效，而语义缓存可能因忽略关键差异而产生错误响应。", "method": "\\ourmethod{}是一种生成式缓存，它能够识别相似提示结构中的可重用响应模式，并为新的请求合成定制的输出。该方法旨在生成对提示变体敏感的响应。", "result": "在没有提示重复的数据集上，\\ourmethod{}实现了83%的缓存命中率，且错误命中率极低。在智能体工作流中，与标准提示匹配相比，它将缓存命中率提高了约20%，并将端到端执行延迟降低了约34%。", "conclusion": "\\ourmethod{}通过处理结构相似的提示，显著提高了LLM应用中的缓存效率和性能，有效解决了传统缓存方法在处理提示变体时的不足。"}}
{"id": "2511.17607", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17607", "abs": "https://arxiv.org/abs/2511.17607", "authors": ["Hyakka Nakada", "Yoshiyasu Tanaka"], "title": "Robustness of Structured Data Extraction from Perspectively Distorted Documents", "comment": "8 pages, 12 figures", "summary": "Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.", "AI": {"tldr": "本研究调查了文档的平面内旋转和透视畸变对多模态大语言模型（如Gemini-1.5-pro）数据提取准确性的影响，发现结构识别准确性受畸变影响显著，但可通过简单的旋转校正改善。", "motivation": "光学字符识别（OCR）对智能信息学至关重要，多模态大语言模型在此任务中表现出色。然而，现有研究注意到文档的平面内旋转会影响数据提取准确性，而真实世界文档通常不仅有旋转，还有透视畸变，其对数据提取准确性的影响尚未被充分研究。", "method": "研究观察到文档图像的典型畸变近似等腰梯形变换，从而将原本八个独立的透视参数简化为旋转角度和畸变比两个参数。通过合成生成具有不同这些参数的样本文档，评估了最先进模型Gemini-1.5-pro的性能，并衡量了字符识别准确性（传统OCR指标）和结构识别准确性（阅读顺序正确性）。", "result": "研究发现，文档畸变显著降低了结构识别准确性。此外，通过简单的旋转校正可以有效提高这种准确性。", "conclusion": "文档畸变对多模态大语言模型的OCR任务（特别是结构识别）有显著负面影响，但通过简单的旋转校正可以有效改善性能。这一发现有助于多模态大语言模型在OCR任务中的实际应用。"}}
{"id": "2511.17867", "categories": ["eess.IV", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.17867", "abs": "https://arxiv.org/abs/2511.17867", "authors": ["Samuel Fernández-Menduiña", "Eduardo Pavez", "Antonio Ortega", "Tsung-Wei Huang", "Thuong Nguyen Canh", "Guan-Ming Su", "Peng Yin"], "title": "INT-DTT+: Low-Complexity Data-Dependent Transforms for Video Coding", "comment": null, "summary": "Discrete trigonometric transforms (DTTs), such as the DCT-2 and the DST-7, are widely used in video codecs for their balance between coding performance and computational efficiency. In contrast, data-dependent transforms, such as the Karhunen-Loève transform (KLT) and graph-based separable transforms (GBSTs), offer better energy compaction but lack symmetries that can be exploited to reduce computational complexity. This paper bridges this gap by introducing a general framework to design low-complexity data-dependent transforms. Our approach builds on DTT+, a family of GBSTs derived from rank-one updates of the DTT graphs, which can adapt to signal statistics while retaining a structure amenable to fast computation. We first propose a graph learning algorithm for DTT+ that estimates the rank-one updates for rows and column graphs jointly, capturing the statistical properties of the overall block. Then, we exploit the progressive structure of DTT+ to decompose the kernel into a base DTT and a structured Cauchy matrix. By leveraging low-complexity integer DTTs and sparsifying the Cauchy matrix, we construct an integer approximation to DTT+, termed INT-DTT+. This approximation significantly reduces both computational and memory complexities with respect to the separable KLT with minimal performance loss. We validate our approach in the context of mode-dependent transforms for the VVC standard, following a rate-distortion optimized transform (RDOT) design approach. Integrated into the explicit multiple transform selection (MTS) framework of VVC in a rate-distortion optimization setup, INT-DTT+ achieves more than 3% BD-rate savings over the VVC MTS baseline, with complexity comparable to the integer DCT-2 once the base DTT coefficients are available.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17847", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.17847", "abs": "https://arxiv.org/abs/2511.17847", "authors": ["Xinguo Fang", "Anthony G. Christodoulou"], "title": "Generative MR Multitasking with complex-harmonic cardiac encoding: Bridging the gap between gated imaging and real-time imaging", "comment": "Submitted to Magnetic Resonance in Medicine; 21 pages, 7 figures", "summary": "Purpose: To develop a unified image reconstruction framework that bridges real-time and gated cardiac MRI, including quantitative MRI. Methods: We introduce Generative Multitasking, which learns an implicit neural temporal basis from sequence timings and an interpretable latent space for cardiac and respiratory motion. Cardiac motion is modeled as a complex harmonic, with phase encoding timing and a latent amplitude capturing beat-to-beat functional variability, linking cardiac phase-resolved (\"gated-like\") and time-resolved (\"real-time-like\") views. We implemented the framework using a conditional variational autoencoder (CVAE) and evaluated it for free-breathing, non-ECG-gated radial GRE in three settings: steady-state cine imaging, multicontrast T2prep/IR imaging, and dual-flip-angle T1/T2 mapping, compared with conventional Multitasking. Results: Generative Multitasking provided flexible cardiac motion representation, enabling reconstruction of archetypal cardiac phase-resolved cines (like gating) as well as time-resolved series that reveal beat-to-beat variability (like real-time imaging). Conditioning on the previous k-space angle and modifying this term at inference removed eddy-current artifacts without globally smoothing high temporal frequencies. For quantitative mapping, Generative Multitasking reduced intraseptal T1 and T2 coefficients of variation compared with conventional Multitasking (T1: 0.13 vs. 0.31; T2: 0.12 vs. 0.32; p<0.001), indicating higher SNR. Conclusion: Generative Multitasking uses a CVAE with complex harmonic cardiac coordinates to unify gated and real-time CMR within a single free-breathing, non-ECG-gated acquisition. It allows flexible cardiac motion representation, suppresses trajectory-dependent artifacts, and improves T1 and T2 mapping, suggesting a path toward cine, multicontrast, and quantitative imaging without separate gated and real-time scans.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17774", "abs": "https://arxiv.org/abs/2511.17774", "authors": ["Salma Mozaffari", "Daniel Ruan", "William van den Bogert", "Nima Fazeli", "Sigrid Adriaenssens", "Arash Adel"], "title": "Learning Diffusion Policies for Robotic Manipulation of Timber Joinery under Fabrication Uncertainty", "comment": null, "summary": "Construction uncertainties such as fabrication inaccuracies and material imperfections pose a significant challenge to contact-rich robotic manipulation by hindering precise and robust assembly. In this paper, we explore the performance and robustness of diffusion policy learning as a promising solution for contact-sensitive robotic assembly at construction scale, using timber mortise and tenon joints as a case study. A two-phase study is conducted: first, to evaluate policy performance and applicability; second, to assess robustness in handling fabrication uncertainties simulated as randomized perturbations to the mortise position. The best-performing policy achieved a total average success rate of 75% with perturbations up to 10 mm, including 100% success in unperturbed cases. The results demonstrate the potential of sensory-motor diffusion policies to generalize to a wide range of complex, contact-rich assembly tasks across construction and manufacturing, advancing robotic construction under uncertainty and contributing to safer, more efficient building practices.", "AI": {"tldr": "本文探讨了扩散策略学习在应对建筑不确定性（如制造误差）的接触密集型机器人装配中的性能和鲁棒性，以木材榫卯接头为例，取得了高达10毫米扰动下75%的成功率。", "motivation": "建筑施工中的不确定性，如制造不准确和材料缺陷，对接触密集型机器人操作构成了重大挑战，阻碍了精确和鲁棒的装配。", "method": "研究采用扩散策略学习来解决接触敏感的机器人装配问题，并以木材榫卯接头作为案例研究。进行了两阶段研究：首先评估策略性能和适用性；其次通过模拟随机扰动（高达10毫米）来评估其处理制造不确定性的鲁棒性。", "result": "表现最佳的策略在未受扰动的情况下实现了100%的成功率，在高达10毫米扰动的情况下，总平均成功率达到75%。", "conclusion": "研究结果表明，感觉运动扩散策略有潜力推广到建筑和制造领域中各种复杂的、接触密集型装配任务，从而推动不确定性下的机器人施工，并有助于实现更安全、更高效的建筑实践。"}}
{"id": "2511.17777", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17777", "abs": "https://arxiv.org/abs/2511.17777", "authors": ["Ravi Prakash", "Vincent Y. Wang", "Arpit Mishra", "Devi Yuliarti", "Pei Zhong", "Ryan P. McNabb", "Patrick J. Codd", "Leila J. Bridgeman"], "title": "See, Plan, Cut: MPC-Based Autonomous Volumetric Robotic Laser Surgery with OCT Guidance", "comment": "9 pages, 8 figures", "summary": "Robotic laser systems offer the potential for sub-millimeter, non-contact, high-precision tissue resection, yet existing platforms lack volumetric planning and intraoperative feedback. We present RATS (Robot-Assisted Tissue Surgery), an intelligent opto-mechanical, optical coherence tomography (OCT)-guided robotic platform designed for autonomous volumetric soft tissue resection in surgical applications. RATS integrates macro-scale RGB-D imaging, micro-scale OCT, and a fiber-coupled surgical laser, calibrated through a novel multistage alignment pipeline that achieves OCT-to-laser calibration accuracy of 0.161+-0.031mm on tissue phantoms and ex vivo porcine tissue. A super-Gaussian laser-tissue interaction (LTI) model characterizes ablation crater morphology with an average RMSE of 0.231+-0.121mm, outperforming Gaussian baselines. A sampling-based model predictive control (MPC) framework operates directly on OCT voxel data to generate constraint-aware resection trajectories with closed-loop feedback, achieving 0.842mm RMSE and improving intersection-over-union agreement by 64.8% compared to feedforward execution. With OCT, RATS detects subsurface structures and modifies the planner's objective to preserve them, demonstrating clinical feasibility.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17860", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.17860", "abs": "https://arxiv.org/abs/2511.17860", "authors": ["Lukas Harris", "Micah Roschelle", "Jack Bartley", "Mekhail Anwar"], "title": "A Versatile Optical Frontend for Multicolor Fluorescence Imaging with Miniaturized Lensless Sensors", "comment": null, "summary": "Lensless imaging enables exceptionally compact fluorescence sensors, advancing applications in \\textit{in vivo} imaging and low-cost, point-of-care diagnostics. These sensors require a filter to block the excitation light while passing fluorescent emissions. However, conventional thin-film interference filters are sensitive to angle of incidence (AOI), complicating their use in lensless systems. Here we thoroughly analyze and optimize a technique using a fiber optic plate (FOP) to absorb off-axis light that would bleed through the interference filter while improving image resolution. Through simulations, we show that the numerical aperture (NA) of the FOP drives inherent design tradeoffs: collection efficiency improves rapidly with a higher NA, but at the cost of resolution, increased device thickness, and fluorescence excitation efficiency. To illustrate this, we optimize two optical frontends with full-width at half maximums (FWHMs) of 8.3° and 45.7°. Implementing these designs, we show that angle-insensitivity requires filters on both sides of the FOP, due to scattering. In imaging experiments, the 520-$μ$m-thick high-NA design is 59$\\times$ more sensitive to fluorescence while only degrading resolution by 3.2$\\times$. Alternatively, the low-NA design is capable of three-color fluorescence imaging with 110-$μ$m resolution at a 1-mm working distance. Overall, we demonstrate a versatile optical frontend that is adaptable to a range of applications using different fluorophores, illumination configurations, and lensless imaging techniques.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18015", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18015", "abs": "https://arxiv.org/abs/2511.18015", "authors": ["Luke Eilers", "Jonas Stapmanns", "Catarina Dias", "Jean-Pascal Pfister"], "title": "On the stability of event-based control with neuronal dynamics", "comment": "11 pages, 4 figures", "summary": "Event-based control, unlike analogue control, poses significant analytical challenges due to its hybrid dynamics. This work investigates the stability and inter-event time properties of a control-affine system under event-based impulsive control. The controller consists of multiple neuronal units with leaky integrate-and-fire dynamics acting on a time-invariant, multiple-input multiple-output plant in closed loop. Both the plant state and the neuronal units exhibit discontinuities that cancel if combined linearly, enabling a direct correspondence between the event-based impulsive controller and a corresponding analogue controller. Leveraging this observation, we prove global practical stability of the event-based impulsive control system. In the general nonlinear case, we show that the event-based impulsive controller ensures global practical asymptotic stability if the analogue system is input-to-state stable (ISS) with respect to specific disturbances. In the linear case, we further show global practical exponential stability if the analogue system is stable. We illustrate our results with numerical simulations. The findings reveal a fundamental link between analogue and event-based impulsive control, providing new insights for the design of neuromorphic controllers.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17781", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17781", "abs": "https://arxiv.org/abs/2511.17781", "authors": ["Kristy Sakano", "Jianyu An", "Dinesh Manocha", "Huan Xu"], "title": "SAFE-SMART: Safety Analysis and Formal Evaluation using STL Metrics for Autonomous RoboTs", "comment": null, "summary": "We present a novel, regulator-driven approach for post hoc safety evaluation of learning-based, black-box autonomous mobile robots, ensuring ongoing compliance with evolving, human-defined safety rules. In our iterative workflow, human safety requirements are translated by regulators into Signal Temporal Logic (STL) specifications. Rollout traces from the black-box model are externally verified for compliance, yielding quantitative safety metrics, Total Robustness Value (TRV) and Largest Robustness Value (LRV), which measure average and worst-case specification adherence. These metrics inform targeted retraining and iterative improvement by model designers. We apply our method across two different applications: a virtual driving scenario and an autonomous mobile robot navigating a complex environment, and observe statistically significant improvements across both scenarios. In the virtual driving scenario, we see a 177% increase in traces adhering to the simulation speed limit, a 1138% increase in traces minimizing off-road driving, and a 16% increase in traces successfully reaching the goal within the time limit. In the autonomous navigation scenario, there is a 300% increase in traces avoiding sharp turns, a 200% increase in traces reaching the goal within the time limit, and a 49% increase in traces minimizing time spent near obstacles. Finally, we validate our approach on a TurtleBot3 robot in the real world, and demonstrate improved obstacle navigation with safety buffers.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17609", "abs": "https://arxiv.org/abs/2511.17609", "authors": ["Linh Van Ma", "Unse Fatima", "Tepy Sokun Chriv", "Haroon Imran", "Moongu Jeon"], "title": "3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF", "comment": "International Conference on Control, Automation and Information Sciences (ICCAIS) 2025, October 27 - 29, 2025 | Jeju, Korea", "summary": "Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17562", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17562", "abs": "https://arxiv.org/abs/2511.17562", "authors": ["Wei Tian", "YuhaoZhou"], "title": "ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector", "comment": null, "summary": "This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17894", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.17894", "abs": "https://arxiv.org/abs/2511.17894", "authors": ["Yi Huang", "Feng Han", "Wenyi Liu", "Jingang Yi", "Yuebin Guo"], "title": "Machine Learning-based Online Stability Lobe Diagram Estimation and Chatter Suppression Control in Milling Process", "comment": null, "summary": "Chatter is a self-excited vibration in milling that degrades surface quality and accelerates tool wear. This paper presents an adaptive process controller that suppresses chatter by leveraging machine learning-based online estimation of the Stability Lobe Diagram (SLD) and surface roughness in the process. Stability analysis is conducted using the semi-discretization method for milling dynamics modeled by delay differential equations. An integrated machine learning framework estimates the SLD from sensor data and predicts surface roughness for chatter detection in real time. These estimates are integrated into an optimal controller that adaptively adjusts spindle speed to maintain process stability and improve surface finish. Simulations and experiments are performed to demonstrate the superior performance compared to the existing approaches.", "AI": {"tldr": "本文提出了一种自适应过程控制器，通过机器学习在线估计稳定性叶瓣图和表面粗糙度，并据此调整主轴转速，以抑制铣削颤振。", "motivation": "铣削加工中的颤振是一种自激振动，会降低表面质量并加速刀具磨损。", "method": "采用半离散化方法对由延迟微分方程建模的铣削动力学进行稳定性分析。集成机器学习框架用于从传感器数据在线估计稳定性叶瓣图（SLD）并预测表面粗糙度以进行颤振检测。这些估计结果被整合到一个优化控制器中，该控制器自适应地调整主轴转速以维持过程稳定性并改善表面光洁度。", "result": "仿真和实验结果表明，与现有方法相比，该方法具有更优异的性能。", "conclusion": "所提出的自适应过程控制器能够有效抑制颤振，并通过结合机器学习的在线估计和主轴转速的自适应调整来改善加工表面质量。"}}
{"id": "2511.18081", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18081", "abs": "https://arxiv.org/abs/2511.18081", "authors": ["Zijing Li"], "title": "Sparse Broad Learning System via Sequential Threshold Least-Squares for Nonlinear System Identification under Noise", "comment": null, "summary": "The Broad Learning System (BLS) has gained significant attention for its computational efficiency and less network parameters compared to deep learning structures. However, the standard BLS relies on the pseudoinverse solution, which minimizes the mean square error with $L_2$-norm but lacks robustness against sensor noise and outliers common in industrial environments. To address this limitation, this paper proposes a novel Sparse Broad Learning System (S-BLS) framework. Instead of the traditional ridge regression, we incorporate the Sequential Threshold Least-Squares (STLS) algorithm -- originally utilized in the sparse identification of nonlinear dynamics (SINDy) -- into the output weight learning process of BLS. By iteratively thresholding small coefficients, the proposed method promotes sparsity in the output weights, effectively filtering out noise components while maintaining modeling accuracy. This approach falls under the category of sparse regression and is particularly suitable for noisy environments. Experimental results on a numerical nonlinear system and a noisy Continuous Stirred Tank Reactor (CSTR) benchmark demonstrate that the proposed method is effective and achieves superior robustness compared to standard BLS.", "AI": {"tldr": "本文提出了一种稀疏广义学习系统（S-BLS），通过将序列阈值最小二乘（STLS）算法引入到输出权重学习中，以提高BLS在工业噪声环境下的鲁棒性。", "motivation": "标准广义学习系统（BLS）虽然计算效率高且参数少，但其基于L2范数的伪逆解对工业环境中常见的传感器噪声和异常值缺乏鲁棒性。", "method": "本文提出了稀疏广义学习系统（S-BLS）框架。它摒弃了传统的岭回归，将最初用于非线性动力学稀疏识别（SINDy）的序列阈值最小二乘（STLS）算法整合到BLS的输出权重学习过程中。通过迭代阈值处理小系数，该方法促进了输出权重的稀疏性，从而有效滤除噪声成分并保持建模精度。", "result": "在数值非线性系统和有噪声的连续搅拌釜反应器（CSTR）基准上的实验结果表明，所提出的方法是有效的，并且比标准BLS具有更优越的鲁棒性。", "conclusion": "通过引入STLS算法实现输出权重的稀疏性，S-BLS显著增强了广义学习系统在噪声环境下的鲁棒性，同时保持了模型精度，使其特别适用于工业噪声环境。"}}
{"id": "2511.18148", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18148", "abs": "https://arxiv.org/abs/2511.18148", "authors": ["Shixiao Liang", "Chengyuan Ma", "Pei Li", "Haotian Shi", "Jiaxi Liu", "Hang Zhou", "Keke Long", "Bofeng Cao", "Todd Szymkowski", "Xiaopeng Li"], "title": "Real-Time Lane-Level Crash Detection on Freeways Using Sparse Telematics Data", "comment": "15 pages,6 figures", "summary": "Real-time traffic crash detection is critical in intelligent transportation systems because traditional crash notifications often suffer delays and lack specific, lane-level location information, which can lead to safety risks and economic losses. This paper proposes a real-time, lane-level crash detection approach for freeways that only leverages sparse telematics trajectory data. In the offline stage, the historical trajectories are discretized into spatial cells using vector cross-product techniques, and then used to estimate a vehicle intention distribution and select an alert threshold by maximizing the F1-score based on official crash reports. In the online stage, incoming telematics records are mapped to these cells and scored for three modules: transition anomalies, speed deviations, and lateral maneuver risks, with scores accumulated into a cell-specific risk map. When any cell's risk exceeds the alert threshold, the system issues a prompt warning. Relying solely on telematics data, this real-time and low-cost solution is evaluated on a Wisconsin dataset and validated against official crash reports, achieving a 75% crash identification rate with accurate lane-level localization, an overall accuracy of 96%, an F1-score of 0.84, and a non-crash-to-crash misclassification rate of only 0.6%, while also detecting 13% of crashes more than 3 minutes before the recorded crash time.", "AI": {"tldr": "本文提出了一种利用稀疏远程信息处理轨迹数据，针对高速公路的实时、车道级交通事故检测方法。该方法通过离线阶段的空间离散化和意图分布估计，以及在线阶段的模块评分和风险积累，实现了高准确率的事故识别和预警。", "motivation": "传统的交通事故通知存在延迟且缺乏具体的车道级位置信息，这可能导致安全风险和经济损失。因此，需要一种实时、精确的交通事故检测系统。", "method": "该方法分为离线和在线两个阶段。离线阶段，利用向量叉积技术将历史轨迹离散化为空间单元，估算车辆意图分布，并根据官方事故报告通过最大化F1分数选择警报阈值。在线阶段，将传入的远程信息处理记录映射到这些单元，并针对三个模块（过渡异常、速度偏差、横向机动风险）进行评分，分数累积到单元特定的风险图中。当任何单元的风险超过警报阈值时，系统发出预警。", "result": "该系统在威斯康星州数据集上进行了评估，并与官方事故报告进行了验证，实现了75%的事故识别率和准确的车道级定位，总体准确率为96%，F1分数为0.84，非事故误报率仅为0.6%。此外，该系统还能在记录事故时间前3分钟以上检测到13%的事故。", "conclusion": "该研究提供了一种仅依靠远程信息处理数据，实时、低成本且有效的车道级交通事故检测解决方案，具有高准确率和预警能力，能够显著提升智能交通系统的安全性。"}}
{"id": "2511.17746", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17746", "abs": "https://arxiv.org/abs/2511.17746", "authors": ["Sharaj Kunjar", "Alyssa Hasegawa Smith", "Tyler R Mckenzie", "Rushali Mohbe", "Samuel V Scarpino", "Brooke Foucault Welles"], "title": "Computational frame analysis revisited: On LLMs for studying news coverage", "comment": null, "summary": "Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.", "AI": {"tldr": "本文系统评估了生成式LLM在媒体框架分析中的表现，并与传统人工编码和早期计算方法进行对比，发现人工编码仍更优，并提出了多方法融合的框架分析路线图。", "motivation": "计算方法在媒体框架识别方面表现出潜力和局限性，而生成式LLM正日益被用作内容分析工具，但它们在框架分析中的有效性尚未得到系统评估。", "method": "研究通过一个新颖的黄金标准数据集（包含2022年美国猴痘疫情六个月的新闻报道），系统评估了生成式LLM（如GPT和Claude），并将其与词袋模型、仅编码器Transformer模型以及传统人工编码程序进行对比。", "result": "生成式LLM展现出一些潜在应用，但始终不如人工编码者，在某些情况下甚至不如小型语言模型。模型选择始终需要某种形式的人工验证。不同方法的适用性取决于具体任务的性质，表明它们可以互补使用。", "conclusion": "研究支持采用方法论多元化方法，并为未来的计算框架分析研究者提供了如何利用互补方法并肩作战的见解和路线图。"}}
{"id": "2511.17873", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.17873", "abs": "https://arxiv.org/abs/2511.17873", "authors": ["Jin Yang", "Daniel S. Marcus", "Aristeidis Sotiras"], "title": "TransLK-Net: Entangling Transformer and Large Kernel for Progressive and Collaborative Feature Encoding and Decoding in Medical Image Segmentation", "comment": "7 figures", "summary": "Convolutional neural networks (CNNs) and vision transformers (ViTs) are widely employed for medical image segmentation, but they are still challenged by their intrinsic characteristics. CNNs are limited from capturing varying-scaled features and global contextual information due to the employment of fixed-sized kernels. In contrast, ViTs employ self-attention and MLP for global information modeling, but they lack mechanisms to learn spatial-wise local information. Additionally, self-attention leads the network to show high computational complexity. To tackle these limitations, we propose Progressively Entangled Transformer Large Kernel (PTLK) and Collaboratively Entangled Transformer Large Kernel (CTLK) modules to leverage the benefits of self-attention and large kernel convolutions and overcome shortcomings. Specifically, PTLK and CTLK modules employ the Multi-head Large Kernel to capture multi-scale local features and the Efficient Decomposed Self-attention to model global information efficiently. Subsequently, they employ the Attention Entanglement mechanism to enable local and global features to enhance and calibrate each other progressively and collaboratively. Additionally, an Attention-gated Channel MLP (AG-MLP) module is proposed to equip the standard MLP module with the capabilities of modeling spatial information. PTLK and CTLK modules are further incorporated as a Cross Entanglement Decoding (CED) block for efficient feature fusion and decoding. Finally, we propose a novel network for volumetric medical image segmentation that employs an encoder-decoder architecture, termed TransLK-Net. The encoder employs a hierarchical ViT architecture whose block is built by incorporating PTLK and CTLK with AG-MLP into a ViT block, and the decoder employs the CED block.", "AI": {"tldr": "该论文提出了一种名为TransLK-Net的新型网络，用于医学图像分割，通过结合自注意力机制和大核卷积的优势，解决了CNN和ViT在捕获多尺度局部特征和高效全局信息方面的局限性。", "motivation": "卷积神经网络（CNN）因其固定大小的卷积核，在捕获多尺度特征和全局上下文信息方面存在局限性。而视觉Transformer（ViT）虽然能建模全局信息，但缺乏学习空间局部信息的机制，并且自注意力机制导致计算复杂度高。研究旨在克服这些限制。", "method": "论文提出了两种模块：渐进式纠缠Transformer大核（PTLK）和协作式纠缠Transformer大核（CTLK）。它们结合多头大核卷积来捕获多尺度局部特征，以及高效分解自注意力机制来高效建模全局信息。此外，通过注意力纠缠机制使局部和全局特征相互增强和校准。还提出了一种注意力门控通道MLP（AG-MLP）模块来增强标准MLP的空间信息建模能力。PTLK和CTLK模块进一步整合为交叉纠缠解码（CED）块，用于高效特征融合和解码。最终，提出了一种名为TransLK-Net的编码器-解码器架构，其编码器采用分层ViT架构（包含PTLK、CTLK和AG-MLP），解码器采用CED块。", "result": "论文提出了一种用于体素医学图像分割的新型网络TransLK-Net，该网络能够结合自注意力机制和大核卷积的优点，高效地捕获多尺度局部特征和全局信息，并增强局部与全局特征的交互。", "conclusion": "通过结合大核卷积和高效自注意力机制，并引入特征纠缠和空间增强MLP，所提出的TransLK-Net能够有效解决现有CNN和ViT在医学图像分割中的固有局限性，有望实现更优异的分割性能。"}}
{"id": "2511.17615", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17615", "abs": "https://arxiv.org/abs/2511.17615", "authors": ["Young-Beom Woo"], "title": "Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis", "comment": "[Master's thesis, Korea University, 2025]", "summary": "Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.", "AI": {"tldr": "PnP-MIX是一种无需微调的方法，通过引导外观注意力、掩码引导噪声混合和背景稀释++策略，解决了多概念文本到图像生成中复杂的场景下概念篡改、语义不一致和概念泄露问题，实现了高保真度的多概念合成。", "motivation": "现有方法在复杂多对象场景中表现不佳，导致个性化和非个性化区域的意外改变，未能保留预期的提示结构，并扰乱区域间的交互，从而产生语义不一致。", "method": "PnP-MIX采用以下创新策略：1) 引导外观注意力，忠实反映每个个性化概念的预期外观；2) 掩码引导噪声混合策略，保留非个性化区域的完整性并实现个性化对象的精确集成；3) 背景稀释++策略，有效减少概念泄露并促进特征在个性化区域内的准确本地化。", "result": "广泛的实验结果表明，PnP-MIX在单概念和多概念个性化场景中始终超越现有方法，无需额外模型微调，展现出卓越的鲁棒性和性能。", "conclusion": "PnP-MIX是一种创新、无需微调的方法，能无缝地将多个个性化概念嵌入到单个生成图像中，有效解决了现有方法的局限性，实现了高保真度的文本到图像合成。"}}
{"id": "2511.17895", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17895", "abs": "https://arxiv.org/abs/2511.17895", "authors": ["Ziye Zhang", "Bin Pan", "Zhenwei Shi"], "title": "Spectral Super-Resolution Neural Operator with Atmospheric Radiative Transfer Prior", "comment": null, "summary": "Spectral super-resolution (SSR) aims to reconstruct hyperspectral images (HSIs) from multispectral observations, with broad applications in remote sensing. Data-driven methods are widely used, but they often overlook physical principles, leading to unrealistic spectra, particularly in atmosphere-affected bands. To address this challenge, we propose the Spectral Super-Resolution Neural Operator (SSRNO), which incorporates atmospheric radiative transfer (ART) prior into the data-driven procedure, yielding more physically consistent predictions. The proposed SSRNO framework consists of three stages: upsampling, reconstruction, and refinement. In the upsampling stage, we leverage prior information to expand the input multispectral image, producing a physically plausible hyperspectral estimate. Subsequently, we utilize a neural operator in the reconstruction stage to learn a continuous mapping across the spectral domain. Finally, the refinement stage imposes a hard constraint on the output HSI to eliminate color distortion. The upsampling and refinement stages are implemented via the proposed guidance matrix projection (GMP) method, and the reconstruction neural operator adopts U-shaped spectral-aware convolution (SAC) layers to capture multi-scale features. Moreover, we theoretically demonstrate the optimality of the GMP method. With the neural operator and ART priors, SSRNO also achieves continuous spectral reconstruction and zero-shot extrapolation. Various experiments validate the effectiveness and generalization ability of the proposed approach.", "AI": {"tldr": "该论文提出了一种名为SSRNO的谱超分辨率方法，通过将大气辐射传输（ART）先验知识整合到数据驱动流程中，结合神经算子，实现了更具物理一致性的高光谱图像重建。", "motivation": "现有数据驱动的谱超分辨率方法常忽略物理原理，导致重建光谱不真实，尤其是在受大气影响的波段。", "method": "SSRNO框架包含三个阶段：上采样、重建和细化。上采样阶段利用ART先验信息生成物理合理的高光谱估计。重建阶段使用神经算子学习跨光谱域的连续映射，并采用U形光谱感知卷积（SAC）层捕获多尺度特征。细化阶段对输出高光谱图像施加硬约束以消除颜色失真。上采样和细化阶段通过指导矩阵投影（GMP）方法实现，并从理论上证明了GMP的最优性。该方法还实现了连续光谱重建和零样本外推。", "result": "SSRNO能够产生更具物理一致性的预测，并实现了连续光谱重建和零样本外推能力。大量实验验证了所提方法的有效性和泛化能力。", "conclusion": "SSRNO通过将大气辐射传输先验与数据驱动的神经算子相结合，克服了传统数据驱动方法在物理一致性方面的不足，成功实现了更准确、连续且具有泛化能力的高光谱图像重建。"}}
{"id": "2511.17572", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.17572", "abs": "https://arxiv.org/abs/2511.17572", "authors": ["Patrick Gerard", "Aiden Chang", "Svitlana Volkova"], "title": "Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs", "comment": "37 pages, EurIPS 2025", "summary": "When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17798", "abs": "https://arxiv.org/abs/2511.17798", "authors": ["Francesco D'Orazio", "Sepehr Samavi", "Xintong Du", "Siqi Zhou", "Giuseppe Oriolo", "Angela P. Schoellig"], "title": "SM$^2$ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control", "comment": null, "summary": "Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17614", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17614", "abs": "https://arxiv.org/abs/2511.17614", "authors": ["Danyang Sun", "Fadi Dornaika", "Nagore Barrena"], "title": "HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation", "comment": null, "summary": "Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.", "AI": {"tldr": "本文提出HSMix，一种新颖的局部图像编辑数据增强方法，通过硬混合（超像素）和软混合（基于显著性的亮度调整）解决医学图像分割中的数据稀缺问题，该方法模型无关且即插即用。", "motivation": "医学图像分割常受数据稀缺和过拟合限制，原因在于标注成本高或疾病罕见。现有自监督和半监督学习方法复杂，需要手工预设或明确的伪标签。数据增强是一种更简单直接的解决方案，但在分割任务中，局部图像编辑增强的有效性尚未充分探索。", "method": "本文提出HSMix，一种用于医学语义分割的局部图像编辑数据增强方法。它通过以下步骤实现：1) 硬增强图像：结合两张源图像中的同质区域（超像素）创建。2) 软混合：基于局部聚合的像素级显著性系数，通过亮度混合进一步调整这些组合区域的亮度。3) 真值掩码：源图像的真值分割掩码也进行相同的混合操作以生成增强图像的对应掩码。该方法充分利用轮廓和显著性信息，在丰富增强空间多样性的同时保留了局部语义信息。它是一种模型无关的即插即用解决方案。", "result": "广泛的实验证据表明，HSMix在各种医学分割任务中均表现出有效性。", "conclusion": "HSMix是一种有效、简单且通用的数据增强方法，通过独特的硬软混合策略，在保留局部语义信息和增加多样性的同时，成功缓解了医学图像分割中的数据稀缺问题。"}}
{"id": "2511.17575", "categories": ["cs.CL", "stat.ME", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.17575", "abs": "https://arxiv.org/abs/2511.17575", "authors": ["Vladimir Berman"], "title": "Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models", "comment": null, "summary": "We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.\n  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.", "AI": {"tldr": "该研究提出了一个简单的非语言文本模型，通过随机字母和空格的组合，纯粹从组合学和分段角度推导出词长分布、词汇增长和齐普夫定律等统计规律，为自然语言和大型语言模型提供了一个结构化的零模型。", "motivation": "研究动机在于探究复杂的语言统计现象（如齐普夫定律）是否可以纯粹由简单的、非语言的机制产生，从而为理解自然语言和大型语言模型中的词汇统计提供一个基础的、结构化的零模型。", "method": "该研究采用了一个故意简化的、完全非语言的文本模型：文本被定义为从有限字母表和单个空格符号中独立抽取的序列。词被定义为非空格符号的最大块。在此符号级别框架内，研究者运用概率论和组合学（例如，优惠券收集器论证）推导了词长分布、词汇增长和词频排序等结构性结果，并将可能字符串数量的指数增长与每个字符串概率的指数衰减相结合，以导出齐普夫型排序-频率定律。", "result": "研究结果表明：1) 词长遵循由空格符号概率决定的几何分布；2) 给定长度的词的预期数量和不同词的预期数量有封闭形式的表达式，并由此得出一个临界词长k*；3) 结合可能字符串数量的指数增长和每个字符串概率的指数衰减，得到了一个齐普夫型排序-频率定律p(r)与r^{-alpha}成正比，其中指数alpha由字母表大小和空格概率明确决定。", "conclusion": "该研究在单个明确模型中统一推导了词长、词汇增长、临界长度和排序-频率结构，并认为这提供了一个结构化的零模型，适用于自然语言词汇统计和大型语言模型中的标记统计。结果表明，齐普夫式模式可以纯粹由组合学和分段产生，无需优化原理或语言组织，有助于澄清哪些现象需要超越随机文本结构的更深层解释。"}}
{"id": "2511.18051", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18051", "abs": "https://arxiv.org/abs/2511.18051", "authors": ["Jilan Mei", "Tengjie Zheng", "Lin Cheng", "Shengping Gong", "Xu Huang"], "title": "Sparse Kalman Identification for Partially Observable Systems via Adaptive Bayesian Learning", "comment": null, "summary": "Sparse dynamics identification is an essential tool for discovering interpretable physical models and enabling efficient control in engineering systems. However, existing methods rely on batch learning with full historical data, limiting their applicability to real-time scenarios involving sequential and partially observable data. To overcome this limitation, this paper proposes an online Sparse Kalman Identification (SKI) method by integrating the Augmented Kalman Filter (AKF) and Automatic Relevance Determination (ARD). The main contributions are: (1) a theoretically grounded Bayesian sparsification scheme that is seamlessly integrated into the AKF framework and adapted to sequentially collected data in online scenarios; (2) an update mechanism that adapts the Kalman posterior to reflect the updated selection of the basis functions that define the model structure; (3) an explicit gradient-descent formulation that enhances computational efficiency. Consequently, the SKI method achieves accurate model structure selection with millisecond-level efficiency and higher identification accuracy, as demonstrated by extensive simulations and real-world experiments (showing an 84.21\\% improvement in accuracy over the baseline AKF).", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17612", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17612", "abs": "https://arxiv.org/abs/2511.17612", "authors": ["Siddiqua Namrah"], "title": "Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression", "comment": "Master's thesis, Korea University, 2025", "summary": "Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17898", "abs": "https://arxiv.org/abs/2511.17898", "authors": ["Weixi Song", "Zhetao Chen", "Tao Xu", "Xianchao Zeng", "Xinyu Zhou", "Lixin Yang", "Donglin Wang", "Cewu Lu", "Yong-Lu Li"], "title": "L1 Sample Flow for Efficient Visuomotor Learning", "comment": null, "summary": "Denoising-based models, such as diffusion and flow matching, have been a critical component of robotic manipulation for their strong distribution-fitting and scaling capacity. Concurrently, several works have demonstrated that simple learning objectives, such as L1 regression, can achieve performance comparable to denoising-based methods on certain tasks, while offering faster convergence and inference. In this paper, we focus on how to combine the advantages of these two paradigms: retaining the ability of denoising models to capture multi-modal distributions and avoid mode collapse while achieving the efficiency of the L1 regression objective. To achieve this vision, we reformulate the original v-prediction flow matching and transform it into sample-prediction with the L1 training objective. We empirically show that the multi-modality can be expressed via a single ODE step. Thus, we propose \\textbf{L1 Flow}, a two-step sampling schedule that generates a suboptimal action sequence via a single integration step and then reconstructs the precise action sequence through a single prediction. The proposed method largely retains the advantages of flow matching while reducing the iterative neural function evaluations to merely two and mitigating the potential performance degradation associated with direct sample regression. We evaluate our method with varying baselines and benchmarks, including 8 tasks in MimicGen, 5 tasks in RoboMimic \\& PushT Bench, and one task in the real-world scenario. The results show the advantages of the proposed method with regard to training efficiency, inference speed, and overall performance. \\href{https://song-wx.github.io/l1flow.github.io/}{Project Website.}", "AI": {"tldr": "本文提出L1 Flow，一种结合去噪模型多模态能力和L1回归效率的方法，通过两步采样计划和L1目标，显著提升机器人操作任务的训练效率、推理速度和整体性能。", "motivation": "去噪模型（如扩散和流匹配）在机器人操作中因其强大的分布拟合能力而表现出色，但可能效率不高。L1回归等简单学习目标在某些任务上能达到相似性能，且收敛和推理更快。研究动机在于结合两者的优点：保留去噪模型捕获多模态分布和避免模式崩溃的能力，同时实现L1回归目标的效率。", "method": "将原始的v-prediction流匹配重新表述为L1训练目标下的样本预测。提出“L1 Flow”方法，采用两步采样计划：首先通过单次ODE积分生成次优动作序列，然后通过单次预测重建精确动作序列。这种方法将迭代的神经网络评估次数减少到两次，并缓解了直接样本回归可能带来的性能下降。", "result": "经验性地证明了多模态可以通过单次ODE步骤表达。L1 Flow在很大程度上保留了流匹配的优势，同时将迭代神经网络评估减少到两次，并减轻了直接样本回归相关的潜在性能下降。在MimicGen（8项任务）、RoboMimic & PushT Bench（5项任务）以及一项真实世界任务上进行了评估，结果显示该方法在训练效率、推理速度和整体性能方面具有显著优势。", "conclusion": "L1 Flow成功地将去噪模型捕获多模态分布的能力与L1回归的效率相结合。通过创新的两步采样计划，它在机器人操作任务中实现了卓越的训练效率、推理速度和整体性能，有效解决了现有方法的局限性。"}}
{"id": "2511.17889", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17889", "abs": "https://arxiv.org/abs/2511.17889", "authors": ["Ting Huang", "Dongjian Li", "Rui Yang", "Zeyu Zhang", "Zida Yang", "Hao Tang"], "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots", "comment": null, "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17619", "abs": "https://arxiv.org/abs/2511.17619", "authors": ["Qinghao Meng", "Junbo Yin", "Jianbing Shen", "Yunde Jia"], "title": "Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds", "comment": "8 pages, 5 figures, 2 tables", "summary": "Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.", "AI": {"tldr": "该研究提出了一种角点对齐回归方法，用于解决激光雷达3D目标检测中中心对齐回归的不稳定性问题，通过将预测目标从不稳定的中心转移到几何信息丰富的角点，显著提升了检测性能，并支持弱监督学习。", "motivation": "激光雷达3D目标检测中，中心对齐回归是主流方法，但由于激光雷达点云的前表面偏置特性，物体中心常落在鸟瞰图（BEV）的稀疏或空区域，导致边界框预测不稳定和不准确。", "method": "研究重新审视了边界框表示，提出角点对齐回归，将预测目标从不稳定的中心转移到位于密集可观测区域的几何信息丰富的角点。利用角点之间以及角点与图像2D框的固有几何约束，可以从角点标注中恢复部分3D边界框参数，从而实现无需完整3D标签的弱监督范式。设计了一个简单有效的角点感知检测头，可集成到现有检测器中。", "result": "在KITTI数据集上的实验表明，该方法比基于中心的基线检测器性能提高了3.5% AP。仅使用BEV角点点击，就能达到完全监督准确率的83%，证明了角点感知回归策略的有效性。", "conclusion": "角点对齐回归策略能有效解决激光雷达3D目标检测中中心对齐回归的不稳定性问题，通过将预测目标转移到几何信息更丰富的角点，显著提高了检测性能和准确性，并支持弱监督学习。"}}
{"id": "2511.17961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17961", "abs": "https://arxiv.org/abs/2511.17961", "authors": ["Hao Wang", "Xiaobao Wei", "Ying Li", "Qingpo Wuwu", "Dongli Wu", "Jiajun Cao", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "title": "RoboArmGS: High-Quality Robotic Arm Splatting via Bézier Curve Refinement", "comment": null, "summary": "Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable Bézier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable Bézier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.", "AI": {"tldr": "RoboArmGS提出一种混合表示，通过可学习的贝塞尔曲线细化URDF预设的机器人手臂运动，以更准确地建模真实世界运动，并贡献了RoboArm4D数据集，实现了领先的渲染质量和运动建模。", "motivation": "为Real2Sim2Real流程构建高质量的机器人手臂数字资产至关重要但充满挑战。现有方法简单地将静态3D高斯绑定到URDF连杆上，使其被动遵循URDF预设运动。然而，真实世界的机械臂运动是嘈杂的，理想化的URDF预设运动无法准确建模，导致3D高斯渲染中出现严重的伪影。", "method": "本文提出RoboArmGS，一种新颖的混合表示，通过可学习的贝塞尔曲线细化URDF预设运动，以实现更准确的真实世界运动建模。具体来说，引入一个可学习的贝塞尔曲线运动细化器，用于校正每个关节的残差，以解决真实世界运动与URDF预设运动之间的不匹配。RoboArmGS实现了更准确的真实世界运动学习，同时确保了3D高斯在手臂各部分之间的连贯绑定。为支持未来研究，贡献了一个名为RoboArm4D的数据集。", "result": "RoboArmGS在RoboArm4D数据集上进行了评估，在真实世界运动建模和渲染质量方面取得了最先进的性能。", "conclusion": "RoboArmGS通过可学习的贝塞尔曲线运动细化器，成功解决了真实世界机器人手臂运动的准确建模问题，实现了高质量的数字资产构建和卓越的渲染效果。"}}
{"id": "2511.17925", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17925", "abs": "https://arxiv.org/abs/2511.17925", "authors": ["Jeonghwan Kim", "Wontaek Kim", "Yidan Lu", "Jin Cheng", "Fatemeh Zargarbashi", "Zicheng Zeng", "Zekun Qi", "Zhiyang Dou", "Nitish Sontakke", "Donghoon Baek", "Sehoon Ha", "Tianyu Li"], "title": "Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game", "comment": null, "summary": "Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.", "AI": {"tldr": "本文提出Switch-JustDance，一个利用任天堂Switch上的Just Dance游戏来评估机器人全身控制的低成本、可复现的基准测试流水线，并验证了其评估能力。", "motivation": "当前机器人全身控制的评估缺乏标准化的、能在真实世界中与人类直接比较的基准。现有方法依赖预收集的人类动作数据集或模拟，限制了可复现性，忽视了硬件因素，并阻碍了公平的人机比较。", "method": "Switch-JustDance通过流媒体、动作重建和动作重定向模块，将Just Dance游戏中的编舞转换为机器人可执行的动作，并利用游戏的内置评分系统来评估控制器性能。研究首先验证了Just Dance平台的可靠性、有效性、敏感性及潜在偏差来源，然后用其对三种先进的人形机器人全身控制器进行了硬件基准测试。", "result": "结果表明，Just Dance平台能提供一致且可解释的性能测量，使其成为评估具身AI的合适工具。通过基准测试，研究获得了关于三种先进人形机器人全身控制器相对优势和局限性的见解。", "conclusion": "Switch-JustDance提供了一个有效、可复现且低成本的平台，用于评估机器人全身控制能力，填补了现有评估方法的空白，并为未来的人机比较和具身AI基准测试奠定了基础。"}}
{"id": "2511.18184", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18184", "abs": "https://arxiv.org/abs/2511.18184", "authors": ["Dileep Kumar", "Wajiha Shireen"], "title": "Energy Control Strategy to Enhance AC Fault Ride-Through in Offshore Wind MMC-HVDC Systems", "comment": null, "summary": "Modular Multilevel Converter-based High Voltage Direct Current (MMC-HVDC) system is a promising technology for integration of offshore wind farms (OWFs). However, onshore AC faults on MMC-HVDC reduce the power transfer capability of onshore converter station, leading to surplus power accumulation in HVDC link. This surplus power causes a rapid rise in DC-link voltage and may hinder safe operation of OWFs. To address such a situation, this paper presents an AC fault ride-through scheme that combines the storage of surplus power in MMC submodule (SM) capacitors and dissipation of residual power in an energy dissipation device (EDD). The proposed energy control facilitates use of half-bridge MMC SMs with low-capacitance, with their storage capacity leveraged to share the surplus power during faults, with a lower-rated EDD. The proposed scheme is tested on a 640kV/420MW MMC-HVDC system. The results show that proposed control scheme effectively maintains DC link voltages, ensuring connection of OWFs.", "AI": {"tldr": "本文提出了一种MMC-HVDC系统应对陆侧交流故障的穿越方案，通过结合子模块电容储能和能量耗散装置，有效维持直流链路电压，确保海上风电场的安全运行，并允许使用低容量半桥子模块和低额定能量耗散装置。", "motivation": "MMC-HVDC系统在陆侧交流故障时，陆侧换流站输电能力下降，导致直流链路功率过剩，直流电压快速上升，可能威胁海上风电场的安全运行。", "method": "提出一种交流故障穿越方案，该方案结合了两种机制：将过剩功率存储在MMC子模块（SM）电容器中，并将剩余功率耗散在能量耗散装置（EDD）中。此能量控制方法利用低容量半桥MMC SM的储能能力来分担过剩功率，并配合低额定功率的EDD。", "result": "在640kV/420MW MMC-HVDC系统上的测试结果表明，所提出的控制方案能有效维持直流链路电压，确保海上风电场的持续并网。", "conclusion": "该方案能有效应对陆侧交流故障，通过结合MMC子模块储能和EDD来维持直流链路电压，保障海上风电场运行，并优化了子模块电容和EDD的容量需求。"}}
{"id": "2511.18031", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.18031", "abs": "https://arxiv.org/abs/2511.18031", "authors": ["Yanxing Liu", "Jiancheng Pan", "Jianwei Yang", "Tiancheng Chen", "Peiling Zhou", "Bingchen Zhang"], "title": "Diverse Instance Generation via Diffusion Models for Enhanced Few-Shot Object Detection in Remote Sensing Images", "comment": "6 pages, 2 figures", "summary": "Few-shot object detection (FSOD) aims to detect novel instances with only a limited number of labeled training samples, presenting a challenge that is particularly prominent in numerous remote sensing applications such as endangered species monitoring and disaster assessment. Existing FSOD methods for remote sensing images (RSIs) have achieved promising progress but remain constrained by the limited diversity of instances. To address this issue, we propose a novel framework that can leverage a diffusion model pretrained on large-scale natural images to synthesize diverse remote sensing instances, thereby improving the performance of few-shot object detectors. Instead of directly synthesizing complete remote sensing images, we first generate instance-level slices via a specialized slice-to-slice module, and then embed these slices into full-scale imagery for enhanced data augmentation. To further adapt diffusion models for remote sensing scenarios, we develop a class-agnostic image inversion module that can invert remote sensing instance slices into semantic space. Additionally, we introduce contrastive loss to semantically align the synthesized images with their corresponding classes. Experimental results show that our method hasachieved an average performance improvement of 4.4% across multiple datasets and various approaches. Ablation experiments indicate that the elaborately designed inversion module can effectively enhance the performance of FSOD methods, and the semantic contrastive loss can further boost the performance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17854", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17854", "abs": "https://arxiv.org/abs/2511.17854", "authors": ["Allen Roush", "Devin Gonier", "John Hines", "Judah Goldfeder", "Philippe Martin Wyder", "Sanjay Basu", "Ravid Shwartz Ziv"], "title": "A superpersuasive autonomous policy debating system", "comment": "Accepted to CLIP workshop at AAAI 2026", "summary": "The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main", "AI": {"tldr": "DeepDebater是一个新颖的自主AI系统，能够参与并赢得完整的政策辩论，它采用多智能体工作流，结合大规模证据库进行迭代检索和自我修正，并在评估中表现出优于人类的论证能力。", "motivation": "现有AI在高度复杂、基于证据且具战略适应性的说服方面仍面临巨大挑战。之前的研究（如IBM Project Debater）仅限于简化和缩短的辩论形式，面向相对非专业的观众。", "method": "DeepDebater系统采用专业多智能体工作流的层次结构，其中由大型语言模型（LLM）驱动的智能体团队相互协作和批判，执行离散的论证任务。每个工作流利用迭代检索、合成和自我纠正，使用大规模政策辩论证据语料库（OpenDebateEvidence），生成完整的演讲稿、交叉询问和反驳。此外，系统还引入了实时、交互式的端到端演示管道，通过OpenAI TTS将演讲稿合成音频，并使用EchoMimic V1显示为说话人头像视频。系统支持完全自主（AI vs AI）和混合人机操作。", "result": "在初步评估中，DeepDebater对抗人类撰写的案例时，能生成定性上更优的论证组件，并在由独立自主评委裁决的模拟回合中持续获胜。专家人类辩论教练也更倾向于DeepDebater构建的论点、证据和案例。所有代码、生成的演讲稿、音频和说话人头像视频均已开源。", "conclusion": "DeepDebater展示了AI在复杂、证据驱动型政策辩论中参与并获胜的强大能力，通过其多智能体架构、大规模证据利用和迭代修正机制，显著提升了AI的说服能力，并为未来的AI辩论研究提供了坚实的基础。"}}
{"id": "2511.18154", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18154", "abs": "https://arxiv.org/abs/2511.18154", "authors": ["Le Wang", "Jessica Ye", "Michael Refors", "Oscar Flärdh", "Håkan Hjalmarsson"], "title": "Optimizing the Driving Profile for Vehicle Mass Estimation", "comment": null, "summary": "Accurate mass estimation is essential for the safe and efficient operation of autonomous heavy-duty vehicles, particularly during transportation missions in unstructured environments such as mining sites, where vehicle mass can vary significantly due to loading and unloading. While prior work has recognized the importance of acceleration profiles for estimation accuracy, the systematic design of driving profiles during transport has not been thoroughly investigated. This paper presents a framework for designing driving profiles to support accurate mass estimation. Based on application-oriented input design, it aims to meet a user-defined accuracy constraint under three optimization objectives: minimum-time, minimum-distance, and maximum accuracy (within a fixed time). It allows time- and distance-dependent bounds on acceleration and velocity, and is based on a Newtonian vehicle dynamics model with actuator dynamics. The optimal profiles are obtained by solving concave optimization problems using a branch-and-bound method, with alternative rank-constrained and semi-definite relaxations also discussed. Theoretical analysis provides insights into the optimal profiles, including feasibility conditions, key ratios between velocity and acceleration bounds, and trade-offs between time- and distance-optimal solutions. The framework is validated through simulations and real-world experiments on a Scania truck with different payloads. Results show that the designed profiles are feasible and effective, enabling accurate mass estimation as part of normal transportation operations without requiring dedicated calibration runs. An additional contribution is a non-causal Wiener filter, with parameters estimated via the Empirical Bayes method, used to filter the accelerometer signal with no phase-lag.", "AI": {"tldr": "本文提出了一种为重型自动驾驶车辆设计驾驶剖面的框架，以在运输任务中实现准确的质量估计，同时优化时间、距离或精度，并通过仿真和实车实验进行了验证。", "motivation": "对于自动驾驶重型车辆，尤其是在采矿场等非结构化环境中，准确的质量估算对于安全高效运行至关重要，因为车辆质量会因装卸而显著变化。尽管先前的工作已认识到加速度剖面对估计准确性的重要性，但尚未系统地研究运输过程中驾驶剖面的设计。", "method": "本文提出了一个基于应用导向输入设计的驾驶剖面设计框架，旨在满足用户定义的精度约束，并考虑最小时间、最小距离和最大精度（在固定时间内）三个优化目标。该框架允许对加速度和速度设置依赖于时间和距离的边界，并基于牛顿车辆动力学模型和执行器动力学。通过分支定界法解决凹优化问题以获得最优剖面，并讨论了替代的秩约束和半正定松弛。此外，还使用了一种参数通过经验贝叶斯方法估计的非因果维纳滤波器来过滤加速度计信号，以实现无相位滞后。", "result": "通过理论分析，本文深入探讨了最优剖面，包括可行性条件、速度和加速度边界之间的关键比率以及时间最优与距离最优解决方案之间的权衡。通过在斯堪尼亚卡车上进行不同载荷的仿真和实车实验，验证了所设计的剖面是可行且有效的，能够在正常运输操作中实现准确的质量估计，而无需专门的校准运行。", "conclusion": "所提出的驾驶剖面设计框架能够为重型自动驾驶车辆提供准确的质量估计，并将其集成到正常的运输操作中，从而避免了专门的校准运行。该方法在理论和实践中均表现出可行性和有效性。"}}
{"id": "2511.17813", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.17813", "abs": "https://arxiv.org/abs/2511.17813", "authors": ["Scott Merrill", "Shashank Srivastava"], "title": "Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation", "comment": "8 pages (29 pages including appendix), 18 figures. Code and datasets are available at https://github.com/smerrillunc/action-aware-llms. Submitted to ACL 2026", "summary": "Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this \"action-aware\" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17618", "abs": "https://arxiv.org/abs/2511.17618", "authors": ["Ju-Young Oh"], "title": "Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach", "comment": "[Master's thesis, Korea University, 2025]", "summary": "Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18197", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18197", "abs": "https://arxiv.org/abs/2511.18197", "authors": ["Jaeho Kim", "Daniel David", "Ana Vizitiv"], "title": "Linear Algebraic Approaches to Neuroimaging Data Compression: A Comparative Analysis of Matrix and Tensor Decomposition Methods for High-Dimensional Medical Images", "comment": null, "summary": "This paper evaluates Tucker decomposition and Singular Value Decomposition (SVD) for compressing neuroimaging data. Tucker decomposition preserves multi-dimensional relationships, achieving superior reconstruction fidelity and perceptual similarity. SVD excels in extreme compression but sacrifices fidelity. The results highlight Tucker decomposition's suitability for applications requiring the preservation of structural and temporal relationships.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18667", "categories": ["eess.IV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.18667", "abs": "https://arxiv.org/abs/2511.18667", "authors": ["Alexander Mehta", "Ruangrawee Kitichotkul", "Vivek K Goyal", "Julián Tachella"], "title": "Equivariant Deep Equilibrium Models for Imaging Inverse Problems", "comment": null, "summary": "Equivariant imaging (EI) enables training signal reconstruction models without requiring ground truth data by leveraging signal symmetries. Deep equilibrium models (DEQs) are a powerful class of neural networks where the output is a fixed point of a learned operator. However, training DEQs with complex EI losses requires implicit differentiation through fixed-point computations, whose implementation can be challenging. We show that backpropagation can be implemented modularly, simplifying training. Experiments demonstrate that DEQs trained with implicit differentiation outperform those trained with Jacobian-free backpropagation and other baseline methods. Additionally, we find evidence that EI-trained DEQs approximate the proximal map of an invariant prior.", "AI": {"tldr": "本文提出了一种模块化的反向传播方法，用于训练结合等变成像（EI）损失的深度平衡模型（DEQ），并证明隐式微分训练的DEQ在信号重建任务上优于其他方法。", "motivation": "等变成像（EI）允许在没有真实数据的情况下训练信号重建模型，利用信号对称性。深度平衡模型（DEQ）是一类强大的神经网络。然而，使用复杂的EI损失通过固定点计算进行隐式微分来训练DEQ在实现上具有挑战性。", "method": "研究采用等变成像（EI）进行无真实数据训练，使用深度平衡模型（DEQ）作为网络架构。核心方法是实现通过固定点计算的隐式微分进行反向传播，并提出一种模块化的实现方式以简化训练。实验将此方法与无雅可比矩阵的反向传播及其他基线方法进行比较。", "result": "研究表明，反向传播可以模块化地实现，从而简化了训练。实验证明，通过隐式微分训练的DEQ在性能上优于通过无雅可比矩阵反向传播训练的DEQ以及其他基线方法。此外，研究发现经过EI训练的DEQ近似于不变先验的近端映射。", "conclusion": "通过模块化实现隐式微分，可以有效且更优地训练结合等变成像损失的深度平衡模型。这些模型不仅在信号重建上表现出色，而且能够近似学习到不变先验的近端映射。"}}
{"id": "2511.17808", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17808", "abs": "https://arxiv.org/abs/2511.17808", "authors": ["Thales Sales Almeida", "Rodrigo Nogueira", "Hélio Pedrini"], "title": "PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese", "comment": null, "summary": "Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17992", "abs": "https://arxiv.org/abs/2511.17992", "authors": ["Chungeng Tian", "Fenghua He", "Ning Hao"], "title": "Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation", "comment": "20 pages, 16 figures", "summary": "The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.", "AI": {"tldr": "本文提出了一种名为“不可观子空间演化”（USE）的新型分析框架，用于系统地表征视觉惯性导航系统（VINS）中不可观子空间在整个估计管道中的动态演化，从而揭示了不一致性的产生机制。在此基础上，提出了一种名为“不可观子空间对齐”（USA）的解决方案范式，通过选择性地干预导致对齐偏差的估计步骤，有效消除了VINS的不一致性，并提供了精确且计算开销低的两种具体方法。", "motivation": "VINS中的不一致性是一个长期存在且基本性的挑战。现有研究主要将不一致性归因于可观测性失配，但这些分析通常基于简化的理论公式，未能涵盖MSCKF校正和延迟初始化等实际VINS估计器中的关键非标准估计步骤。此外，对不一致性如何在估计步骤中动态产生的全面理解的缺失，阻碍了精确高效解决方案的开发，导致当前方法在估计器精度、一致性和实现复杂性之间面临权衡。", "method": "本文提出了一种名为“不可观子空间演化”（USE）的新型分析框架，通过明确跟踪不可观子空间评估点的变化，系统地表征其在整个估计管道中的演化。在此洞察的指导下，提出了一种名为“不可观子空间对齐”（USA）的解决方案范式，通过选择性地干预那些导致对齐偏差的估计步骤来消除不一致性。具体设计了两种USA方法：基于变换的方法和基于重新评估的方法。", "result": "分析揭示，由某些步骤引起的可观测性对齐偏差是可观测性失配的前兆。提出的USA方法提供了精确且计算开销低的解决方案。广泛的仿真和真实世界实验验证了所提出方法的有效性。", "conclusion": "本文通过提出USE框架，深入理解了VINS不一致性的动态产生机制，并在此基础上开发了USA解决方案范式，能够有效且高效地消除VINS中的不一致性。这为解决VINS长期存在的不一致性问题提供了一个新的、实用的途径。"}}
{"id": "2511.17634", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17634", "abs": "https://arxiv.org/abs/2511.17634", "authors": ["Kaikwan Lau", "Andrew S. Na", "Justin W. L. Wan"], "title": "Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection", "comment": null, "summary": "This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed\" matrices to rapidly solve for subsequent ``target\" matrices. Our experiments show that this technique achieves a 15.8\\% to 43.7\\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.", "AI": {"tldr": "本文提出了一种新颖的跨矩阵Krylov投影方法，通过利用矩阵间的数学相似性来加速基于分数的扩散模型，显著降低了计算成本并提高了生成效率。", "motivation": "标准的稳定扩散模型在转换为Fokker-Planck公式后，需要为每张图像求解大型线性系统，这在处理大量图像时会导致高昂的计算成本。", "method": "该方法首先将标准稳定扩散模型转换为Fokker-Planck公式。其核心创新是一种跨矩阵Krylov投影方法，该方法利用矩阵间的数学相似性，构建一个由“种子”矩阵共享的子空间，从而快速求解后续的“目标”矩阵。", "result": "实验表明，与标准稀疏求解器相比，该技术实现了15.8%至43.7%的时间缩减。在去噪任务中，与DDPM基线相比，实现了高达115倍的加速。此外，在固定的计算预算下，该模型能够生成高质量图像，而DDPM则无法生成可识别的内容。", "conclusion": "该方法是一种在资源受限环境下实现高效生成图像的实用方法。"}}
{"id": "2511.18686", "categories": ["eess.IV", "cs.AR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.18686", "abs": "https://arxiv.org/abs/2511.18686", "authors": ["Kasidis Arunruangsirilert", "Jiro Katto"], "title": "Evaluation of Hardware-based Video Encoders on Modern GPUs for UHD Live-Streaming", "comment": "The 33rd International Conference on Computer Communications and Networks (ICCCN 2024), 29-31 July 2024, Big Island, Hawaii, USA", "summary": "Many GPUs have incorporated hardware-accelerated video encoders, which allow video encoding tasks to be offloaded from the main CPU and provide higher power efficiency. Over the years, many new video codecs such as H.265/HEVC, VP9, and AV1 were added to the latest GPU boards. Recently, the rise of live video content such as VTuber, game live-streaming, and live event broadcasts, drives the demand for high-efficiency hardware encoders in the GPUs to tackle these real-time video encoding tasks, especially at higher resolutions such as 4K/8K UHD. In this paper, RD performance, encoding speed, as well as power consumption of hardware encoders in several generations of NVIDIA, Intel GPUs as well as Qualcomm Snapdragon Mobile SoCs were evaluated and compared to the software counterparts, including the latest H.266/VVC codec, using several metrics including PSNR, SSIM, and machine-learning based VMAF. The results show that modern GPU hardware encoders can match the RD performance of software encoders in real-time encoding scenarios, and while encoding speed increased in newer hardware, there is mostly negligible RD performance improvement between hardware generations. Finally, the bitrate required for each hardware encoder to match YouTube transcoding quality was also calculated.", "AI": {"tldr": "本文评估了多代NVIDIA、Intel GPU以及Qualcomm Snapdragon移动SoC中的硬件视频编码器，与软件编码器（包括H.266/VVC）在实时视频编码场景下的RD性能、编码速度和功耗，并计算了匹配YouTube转码质量所需的比特率。", "motivation": "随着VTuber、游戏直播和现场活动广播等实时视频内容的兴起，对GPU中高效硬件编码器处理4K/8K UHD等高分辨率实时视频编码任务的需求日益增长。", "method": "研究评估了多代NVIDIA、Intel GPU和Qualcomm Snapdragon移动SoC中硬件编码器的RD性能、编码速度和功耗，并与软件编码器（包括最新的H.266/VVC）进行了比较。评估指标包括PSNR、SSIM和基于机器学习的VMAF。此外，还计算了每个硬件编码器匹配YouTube转码质量所需的比特率。", "result": "结果显示，现代GPU硬件编码器在实时编码场景中能够与软件编码器达到相当的RD性能。较新的硬件编码速度有所提高，但不同硬件世代之间的RD性能提升微乎其微。同时，计算了每个硬件编码器匹配YouTube转码质量所需的比特率。", "conclusion": "硬件编码器在实时视频编码任务中表现出色，能与软件编码器匹敌，且在速度和能效方面具有优势。尽管编码速度在不断提升，但硬件世代间的RD性能改进已趋于平缓。"}}
{"id": "2511.18493", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18493", "abs": "https://arxiv.org/abs/2511.18493", "authors": ["Gia Huy Thai", "Hoang-Nguyen Vu", "Anh-Minh Phan", "Quang-Thinh Ly", "Tram Dinh", "Thi-Ngoc-Truc Nguyen", "Nhat Ho"], "title": "Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion Segmentation", "comment": null, "summary": "The substantial diversity in cell scale and form remains a primary challenge in computer-aided cancer detection on gigapixel Whole Slide Images (WSIs), attributable to cellular heterogeneity. Existing CNN-Transformer hybrids rely on static computation graphs with fixed routing, which consequently causes redundant computation and limits their adaptability to input variability. We propose Shape-Adapting Gated Experts (SAGE), an input-adaptive framework that enables dynamic expert routing in heterogeneous visual networks. SAGE reconfigures static backbones into dynamically routed expert architectures. SAGE's dual-path design features a backbone stream that preserves representation and selectively activates an expert path through hierarchical gating. This gating mechanism operates at multiple hierarchical levels, performing a two-level, hierarchical selection between shared and specialized experts to modulate model logits for Top-K activation. Our Shape-Adapting Hub (SA-Hub) harmonizes structural and semantic representations across the CNN and the Transformer module, effectively bridging diverse modules. Embodied as SAGE-UNet, our model achieves superior segmentation on three medical benchmarks: EBHI, DigestPath, and GlaS, yielding state-of-the-art Dice Scores of 95.57%, 95.16%, and 94.17%, respectively, and robustly generalizes across domains by adaptively balancing local refinement and global context. SAGE provides a scalable foundation for dynamic expert routing, enabling flexible visual reasoning.", "AI": {"tldr": "SAGE提出了一种输入自适应框架，通过动态专家路由解决全玻片图像中细胞异质性导致的癌症检测挑战，实现了卓越的细胞分割性能。", "motivation": "细胞尺度和形态的多样性（细胞异质性）是全玻片图像(WSI)计算机辅助癌症检测的主要挑战。现有CNN-Transformer混合模型依赖静态计算图和固定路由，导致冗余计算并限制了对输入变异性的适应性。", "method": "本文提出了Shape-Adapting Gated Experts (SAGE)，一个输入自适应框架，它能在异构视觉网络中实现动态专家路由。SAGE将静态骨干网络重构为动态路由的专家架构。其双路径设计包括一个骨干流以保留表示，并通过分层门控选择性激活专家路径。该门控机制在多个层级操作，在共享专家和专业专家之间进行两级分层选择，以调节模型logits进行Top-K激活。Shape-Adapting Hub (SA-Hub)协调CNN和Transformer模块的结构和语义表示，有效连接不同模块。", "result": "作为SAGE-UNet实现，该模型在三个医学基准测试（EBHI、DigestPath和GlaS）上实现了卓越的分割性能，Dice分数分别达到95.57%、95.16%和94.17%，达到最先进水平。通过自适应地平衡局部细化和全局上下文，模型在不同领域展现出强大的泛化能力。", "conclusion": "SAGE为动态专家路由提供了一个可扩展的基础，实现了灵活的视觉推理。"}}
{"id": "2511.18267", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18267", "abs": "https://arxiv.org/abs/2511.18267", "authors": ["Aaron H. P. Farha", "Jonathan P. Ore", "Elias N. Pergantis", "Davide Ziviani", "Eckhard A. Groll", "Kevin J. Kircher"], "title": "Laboratory and field testing of a residential heat pump retrofit for a DC solar nanogrid", "comment": null, "summary": "Residential buildings are increasingly integrating large devices that run natively on direct current (DC), such as solar photovoltaics, electric vehicles, stationary batteries, and DC motors that drive heat pumps and other major appliances. Today, these natively-DC devices typically connect within buildings through alternating current (AC) distribution systems, entailing significant energy losses due to conversions between AC and DC. This paper investigates the alternative of connecting DC devices through DC distribution. Specifically, this paper shows through laboratory and field experiments that an off-the-shelf residential heat pump designed for conventional AC systems can be powered directly on DC with few hardware modifications and little change in performance. Supporting simulations of a DC nanogrid including historical heat pump and rest-of-house load measurements, a solar photovoltaic array, and a stationary battery suggest that connecting these devices through DC distribution could decrease annual electricity bills by 12.5% with an after-market AC-to-DC heat pump retrofit and by 16.7% with a heat pump designed to run on DC.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18428", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18428", "abs": "https://arxiv.org/abs/2511.18428", "authors": ["Xinyan Le", "Yao Zhu", "Yulin Hu", "Bin Han"], "title": "Joint Optimization for Security and Reliability in Round-Trip Transmissions for URLLC services", "comment": "6 pages,4 figures", "summary": "Physical layer security (PLS) is a potential solution for secure and reliable transmissions in future Ultra-Reliable and Low-Latency Communications (URLLC). This work jointly optimizes redundant bits and blocklength allocation in practical round-trip transmission scenarios. To minimize the leakage-failure probability, a metric that jointly characterizes security and reliability in PLS, we formulate an optimization problem for allocating both redundant bits and blocklength. By deriving the boundaries of the feasible set, we obtain the globally optimal solution for this integer optimization problem. To achieve more computationally efficient solutions, we propose a block coordinate descent (BCD) method that exploits the partial convexity of the objective function. Subsequently, we develop a majorization-minimization (MM) algorithm through convex approximation of the objective function, which further improves computational efficiency. Finally, we validate the performance of the three proposed approaches through simulations, demonstrating their practical applicability for future URLLC services.", "AI": {"tldr": "本文针对超可靠低延迟通信（URLLC）中的物理层安全（PLS），联合优化冗余比特和码块长度分配，以最小化泄露失败概率。提出了全局最优解、块坐标下降（BCD）和大化小（MM）三种算法，并通过仿真验证了其有效性。", "motivation": "为了在未来URLLC中实现安全可靠的传输，物理层安全（PLS）是一个潜在的解决方案。本文旨在解决往返传输场景中，如何通过联合优化冗余比特和码块长度来共同表征安全性和可靠性，并最小化泄露失败概率。", "method": "1. 建立一个整数优化问题，以最小化泄露失败概率，联合分配冗余比特和码块长度。2. 推导可行集的边界，获得该整数优化问题的全局最优解。3. 提出一种利用目标函数部分凸性的块坐标下降（BCD）方法，以提高计算效率。4. 通过目标函数的凸近似开发一种大化小（MM）算法，进一步提高计算效率。", "result": "1. 获得了整数优化问题的全局最优解。2. 提出的BCD和MM算法能够提供计算效率更高的解决方案。3. 通过仿真验证了三种所提方法的性能，展示了它们在未来URLLC服务中的实际适用性。", "conclusion": "通过联合优化冗余比特和码块长度，本文成功地解决了URLLC中物理层安全传输的泄露失败概率最小化问题。提出的全局最优解、BCD和MM算法在性能和计算效率方面均表现出色，具有实际应用价值。"}}
{"id": "2511.17633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17633", "abs": "https://arxiv.org/abs/2511.17633", "authors": ["DoYoung Kim", "Jin-Seop Lee", "Noo-ri Kim", "SungJoon Lee", "Jee-Hyong Lee"], "title": "BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?", "comment": "Paper accepted to AAAI 2026", "summary": "Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18445", "categories": ["eess.SY", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.18445", "abs": "https://arxiv.org/abs/2511.18445", "authors": ["Vishesh Vishal Ahire", "Yash Badrinarayan Amle", "Akshada Nanasaheb Waditke", "Ojas Nitin Ahire", "Amey Mahesh Warnekar", "Ayush Ganesh Ahire", "Prashant Anerao"], "title": "Speed Control Security System For safety of Driver and Surroundings", "comment": "9 Pages , 7 figures", "summary": "The speed control security system is best suited for the task of slowing the speed of a vehicle during rash driving as the Driver is over speeding the circuit captures the images of the lanes witch decides the speed of the road the car is currently on this input is further provided to the ESP-32 micro Prosser module in the car switch compiles this data with the data received for the RPM sensor of the car and decides whether the car is over speeding or not in case of over speeding a signal is send by the ESP to the Arduino witch actuates the dc motor used in the car to reduce the speed of the car by the use of a hydraulic brake system actuated by a DC motor.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17908", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.17908", "abs": "https://arxiv.org/abs/2511.17908", "authors": ["Debashish Chakraborty", "Eugene Yang", "Daniel Khashabi", "Dawn Lawrie", "Kevin Duh"], "title": "Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction", "comment": "Preprint", "summary": "Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18085", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18085", "abs": "https://arxiv.org/abs/2511.18085", "authors": ["Yuxuan Wu", "Guangming Wang", "Zhiheng Yang", "Maoqing Yao", "Brian Sheil", "Hesheng Wang"], "title": "Continually Evolving Skill Knowledge in Vision Language Action Model", "comment": null, "summary": "Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead.Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18668", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.18668", "abs": "https://arxiv.org/abs/2511.18668", "authors": ["Flora Lian", "Dinh Quang Huynh", "Hector Penades", "J. Stephany Berrio Perez", "Mao Shan", "Stewart Worrall"], "title": "Data Augmentation Strategies for Robust Lane Marking Detection", "comment": "8 figures, 2 tables, 10 pages, ACRA, Australasian conference on robotics and automation", "summary": "Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.\n  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.", "AI": {"tldr": "本文提出了一种基于生成式AI的数据增强管道，通过几何变换、AI修复和车辆覆盖来模拟部署特定视角，从而提高侧装摄像头车道线检测模型在不同条件下的泛化能力和鲁棒性。", "motivation": "现有车道线检测模型（如在CULane等公共数据集上训练的模型）在面对不同摄像头视角（特别是用于车轮监测的侧装摄像头）时，泛化能力差，存在域偏移问题。", "method": "该研究引入了一个基于生成式AI的数据增强管道，结合了几何透视变换、AI驱动的图像修复和车辆车身叠加，以模拟部署特定视角并保持车道线的连续性。该方法在SCNN和UFLDv2两种最先进的模型上进行了评估。", "result": "经过增强数据训练后，SCNN和UFLDv2模型在不同条件（包括阴影）下的鲁棒性均得到提升。实验结果显示，与预训练模型相比，模型的精度、召回率和F1分数均有所提高。", "conclusion": "该方法弥合了现有数据集与部署特定场景之间的差距，提供了一个可扩展且实用的框架，以提高试点部署场景中车道线检测的可靠性。"}}
{"id": "2511.17910", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17910", "abs": "https://arxiv.org/abs/2511.17910", "authors": ["Yuliang Zhan", "Xinyu Tang", "Han Wan", "Jian Li", "Ji-Rong Wen", "Hao Sun"], "title": "L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention", "comment": "AAAI 2026 oral", "summary": "Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.", "AI": {"tldr": "本文提出L2V-CoT，一种无需训练的潜在干预方法，通过利用LLM和VLM共享的低频潜在表示，将思维链（CoT）推理从LLM有效迁移到VLM，显著提升VLM的多步推理能力。", "motivation": "大型语言模型（LLM）在思维链（CoT）推理方面表现出色，但视觉语言模型（VLM）由于多模态推理数据有限，在多步推理任务中仍面临挑战。现有将CoT推理从LLM迁移到VLM的方法要么训练成本高昂，要么需要架构对齐。", "method": "1. 使用线性人工断层扫描（LAT）经验性地证明LLM和VLM尽管架构不同，但共享CoT推理的相似低频潜在表示。2. 基于此洞察，提出L2V-CoT，一种无需训练的潜在干预方法。3. 在频域从LLM中提取并重采样低频CoT表示。4. 进行维度匹配，并在推理时将这些潜在表示注入VLM，以增强其推理能力。", "result": "L2V-CoT方法在实验中始终优于无需训练的基线方法，甚至超越了监督学习方法，显著提升了VLM的推理能力。", "conclusion": "L2V-CoT通过利用LLM和VLM共享的低频潜在表示，成功实现了CoT推理从LLM到VLM的无需训练迁移，并在性能上超越了现有方法，为VLM的多步推理提供了有效解决方案。"}}
{"id": "2511.18086", "categories": ["cs.RO", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18086", "abs": "https://arxiv.org/abs/2511.18086", "authors": ["Miguel Lourenço", "António Grilo"], "title": "Anti-Jamming based on Null-Steering Antennas and Intelligent UAV Swarm Behavior", "comment": "10 pages", "summary": "Unmanned Aerial Vehicle (UAV) swarms represent a key advancement in autonomous systems, enabling coordinated missions through inter-UAV communication. However, their reliance on wireless links makes them vulnerable to jamming, which can disrupt coordination and mission success. This work investigates whether a UAV swarm can effectively overcome jamming while maintaining communication and mission efficiency.\n  To address this, a unified optimization framework combining Genetic Algorithms (GA), Supervised Learning (SL), and Reinforcement Learning (RL) is proposed. The mission model, structured into epochs and timeslots, allows dynamic path planning, antenna orientation, and swarm formation while progressively enforcing collision rules. Null-steering antennas enhance resilience by directing antenna nulls toward interference sources.\n  Results show that the GA achieved stable, collision-free trajectories but with high computational cost. SL models replicated GA-based configurations but struggled to generalize under dynamic or constrained settings. RL, trained via Proximal Policy Optimization (PPO), demonstrated adaptability and real-time decision-making with consistent communication and lower computational demand. Additionally, the Adaptive Movement Model generalized UAV motion to arbitrary directions through a rotation-based mechanism, validating the scalability of the proposed system.\n  Overall, UAV swarms equipped with null-steering antennas and guided by intelligent optimization algorithms effectively mitigate jamming while maintaining communication stability, formation cohesion, and collision safety. The proposed framework establishes a unified, flexible, and reproducible basis for future research on resilient swarm communication systems.", "AI": {"tldr": "本研究提出一个结合遗传算法（GA）、监督学习（SL）和强化学习（RL）的统一优化框架，并结合零点转向天线，使无人机蜂群在面对干扰时能有效维持通信、编队和任务效率。", "motivation": "无人机（UAV）蜂群依赖无线通信进行协调任务，但易受干扰攻击，这会破坏通信、任务协调和任务成功。因此，研究如何使无人机蜂群有效克服干扰并保持通信和任务效率至关重要。", "method": "本研究提出一个结合遗传算法（GA）、监督学习（SL）和强化学习（RL）的统一优化框架。任务模型被划分为时间段和时隙，支持动态路径规划、天线方向和蜂群编队，并逐步执行防碰撞规则。零点转向天线通过将天线零点指向干扰源来增强抗干扰能力。强化学习模型通过近端策略优化（PPO）进行训练，并引入自适应运动模型以实现无人机运动的通用性。", "result": "结果显示，GA实现了稳定、无碰撞的轨迹，但计算成本高。SL模型复制了基于GA的配置，但在动态或受限环境下泛化能力不足。通过PPO训练的RL模型展现了适应性和实时决策能力，同时保持了稳定的通信和较低的计算需求。此外，自适应运动模型通过基于旋转的机制将无人机运动推广到任意方向，验证了所提出系统的可扩展性。", "conclusion": "配备零点转向天线并由智能优化算法（尤其是强化学习）引导的无人机蜂群能够有效缓解干扰，同时保持通信稳定性、编队凝聚力和碰撞安全性。所提出的框架为未来弹性蜂群通信系统的研究奠定了统一、灵活和可复现的基础。"}}
{"id": "2511.17938", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17938", "abs": "https://arxiv.org/abs/2511.17938", "authors": ["Jianghao Wu", "Yasmeen George", "Jin Ye", "Yicheng Wu", "Daniel F. Schmidt", "Jianfei Cai"], "title": "SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization", "comment": null, "summary": "Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.", "AI": {"tldr": "SPINE是一种令牌选择性的测试时强化学习框架，通过仅更新高熵的分叉令牌并应用熵带正则化器，解决了现有测试时强化学习方法在LLM和MLLM推理中存在的分布偏移和响应长度缩短问题，从而提高了Pass@1并稳定了训练。", "motivation": "大型语言模型（LLM）和多模态LLM（MLLM）在思维链推理方面表现出色，但在测试时面临分布偏移和缺乏可验证监督的问题。现有的测试时强化学习（TTRL）方法通过对采样轨迹进行自洽投票来获得无标签的伪奖励，但常常出现崩溃：多数投票奖励占据主导，响应变短，Pass@1下降。研究发现这是由于统一的序列更新导致，而实际上只有少量高熵的令牌决定了推理分支。", "method": "SPINE是一种令牌选择性的测试时强化学习框架，它：(i) 仅更新分叉令牌（从前向传递统计中识别出的高熵分支点），以及(ii) 在这些令牌处应用熵带正则化器，以在熵过低时维持探索，并在熵过高时抑制噪声监督。SPINE可插入GRPO风格的目标函数，可选地带有KL锚点，并且无需标签或奖励模型。", "result": "在涵盖多模态VQA、通用和专业问答、数学推理以及医疗问答的十个基准测试中，SPINE始终优于TTRL的Pass@1，同时避免了响应长度的缩短，并在LLM和MLLM骨干网络上产生了更稳定的训练动态。", "conclusion": "将更新与思维链的分支点对齐是一种简单、无标签的机制，可以实现推理模型中稳定有效的测试时适应。"}}
{"id": "2511.17635", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17635", "abs": "https://arxiv.org/abs/2511.17635", "authors": ["Max A. Nelson", "Elif Keles", "Eminenur Sen Tasci", "Merve Yazol", "Halil Ertugrul Aktas", "Ziliang Hong", "Andrea Mia Bejar", "Gorkem Durak", "Oznur Leman Boyunaga", "Ulas Bagci"], "title": "Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification", "comment": "5 pages, 5 figures", "summary": "Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\\pm$ 0.072, a $\\sim$5% relative gain over a real-only baseline (AUC 0.864 $\\pm$ 0.061).", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18724", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.18724", "abs": "https://arxiv.org/abs/2511.18724", "authors": ["Sang NguyenQuang", "Xiem HoangVan", "Wen-Hsiao Peng"], "title": "Neural B-Frame Coding: Tackling Domain Shift Issues with Lightweight Online Motion Resolution Adaptation", "comment": "Accepted by TCAS-II: Express Briefs", "summary": "Learned B-frame codecs with hierarchical temporal prediction often encounter the domain-shift issue due to mismatches between the Group-of-Pictures (GOP) sizes for training and testing, leading to inaccurate motion estimates, particularly for large motion. A common solution is to turn large motion into small motion by downsampling video frames during motion estimation. However, determining the optimal downsampling factor typically requires costly rate-distortion optimization. This work introduces lightweight classifiers to predict downsampling factors. These classifiers leverage simple state signals from current and reference frames to balance rate-distortion performance with computational cost. Three variants are proposed: (1) a binary classifier (Bi-Class) trained with Focal Loss to choose between high and low resolutions, (2) a multi-class classifier (Mu-Class) trained with novel soft labels based on rate-distortion costs, and (3) a co-class approach (Co-Class) that combines the predictive capability of the multi-class classifier with the selective search of the binary classifier. All classifier methods can work seamlessly with existing B-frame codecs without requiring codec retraining. Experimental results show that they achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity. The code is available at: https://github.com/NYCU-MAPL/Fast-OMRA.git.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18112", "abs": "https://arxiv.org/abs/2511.18112", "authors": ["Min Lin", "Xiwen Liang", "Bingqian Lin", "Liu Jingzhi", "Zijian Jiao", "Kehan Li", "Yuhan Ma", "Yuecheng Liu", "Shen Zhao", "Yuzheng Zhuang", "Xiaodan Liang"], "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation", "comment": null, "summary": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.", "AI": {"tldr": "EchoVLA是一个记忆感知的视觉-语言-动作模型，专为长程移动操作任务设计，通过结合场景记忆和情景记忆来协调导航和操作。", "motivation": "现有VLA模型主要局限于短程桌面操作，缺乏处理长程移动操作所需的记忆和推理能力，尤其是在空间上下文不断变化的情况下协调导航和操作。", "method": "EchoVLA引入了受人脑启发的协同声明性记忆，包括维护空间语义地图的场景记忆和存储多模态上下文特征任务经验的情景记忆。训练和推理过程中，记忆根据观察、任务历史和指令进行存储、更新和检索，其表示通过粗粒度和细粒度注意力融合以指导移动臂扩散策略。此外，提出了MoMani基准，通过MLLM引导规划和反馈驱动细化生成专家级长程轨迹，并辅以真实机器人演示。", "result": "在模拟和真实世界环境中，EchoVLA显著提升了长程性能，在操作/导航上达到0.52 SR，在移动操作上达到0.31 SR，分别超过基线模型+0.08和+0.11。", "conclusion": "EchoVLA通过其记忆感知架构有效解决了长程移动操作的挑战，并在该领域取得了优异的性能。"}}
{"id": "2511.18551", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.18551", "abs": "https://arxiv.org/abs/2511.18551", "authors": ["Ingyu Jang", "Leila J. Bridgeman"], "title": "Dissipativity and L2 Stability of Large-Scale Networks with Changing Interconnections", "comment": "Under review for IFAC 2026. 6 pages, 2 figures, 1 table", "summary": "In this paper, the L2 stability of switched networks is studied based on the QSR-dissipativity of each agent. While the integration of dissipativity with switched systems has received considerable attention, most previous studies have focused on passivity, internal stability, or feedback networks involving only two agents. This work makes two contributions: first, the relationship between switched QSR-dissipativity and L2 stability is established based on the properties of dissipativity parameters of switched systems; and second, conditions for L2 stability of networks consisting of QSR-dissipative agents with switching interconnection topologies are derived. Crucially, this shows that a common storage function will exist across all modes, avoiding the need to find one, which becomes computationally taxing for large networks with many possible configurations. Numerical examples demonstrate how this can facilitate stability analysis for networked systems under arbitrary switching of swarm drones.", "AI": {"tldr": "本文基于每个智能体的QSR-耗散性，研究了切换网络的L2稳定性，并建立了切换QSR-耗散性与L2稳定性之间的关系，同时推导了具有切换互连拓扑的QSR-耗散智能体网络的L2稳定性条件，表明存在一个共同的存储函数。", "motivation": "以往关于耗散性与切换系统结合的研究主要集中在无源性、内部稳定性或仅涉及两个智能体的反馈网络。本文旨在将研究扩展到更一般的多智能体切换网络，并利用QSR-耗散性来分析其L2稳定性。", "method": "本文首先基于切换系统耗散性参数的特性，建立了切换QSR-耗散性与L2稳定性之间的关系。其次，推导了由QSR-耗散智能体组成的、具有切换互连拓扑的网络的L2稳定性条件。关键在于证明了所有模式下存在一个共同的存储函数，从而避免了为每个模式寻找存储函数的复杂计算。", "result": "研究建立了切换QSR-耗散性与L2稳定性之间的关系，并为具有切换互连拓扑的QSR-耗散智能体网络导出了L2稳定性条件。这些条件表明，一个共同的存储函数可以在所有模式下存在，极大地简化了稳定性分析。数值例子展示了该方法如何促进对任意切换无人机群等网络系统的稳定性分析。", "conclusion": "本文为基于QSR-耗散性的切换网络的L2稳定性分析提供了一个有效框架。通过证明共同存储函数的存在，显著简化了大规模网络的稳定性分析，并展示了其在实际应用中的潜力，如无人机群系统。"}}
{"id": "2511.17923", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17923", "abs": "https://arxiv.org/abs/2511.17923", "authors": ["Wenda Li", "Tongya Zheng", "Shunyu Liu", "Yu Wang", "Kaixuan Chen", "Hanyang Yuan", "Bingde Hu", "Zujie Ren", "Mingli Song", "Gang Chen"], "title": "Towards Efficient LLM-aware Heterogeneous Graph Learning", "comment": null, "summary": "Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17636", "abs": "https://arxiv.org/abs/2511.17636", "authors": ["Weijun Gao", "Rundong He", "Jinyang Dong", "Yongshun Gong"], "title": "TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection", "comment": null, "summary": "Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.", "AI": {"tldr": "本文提出了一种基于判别性和活性的典型集精炼方法，并结合偏度精炼来校正激活，从而显著提升了分布外检测的性能。", "motivation": "在开放世界环境中，机器学习模型安全部署的关键在于分布外(OOD)检测。现有基于激活的方法在校正激活时，常忽略通道的内在特性和分布偏度，导致典型集估计不准确，从而错误地包含异常激活，影响模型可靠性和性能。", "method": "本文提出了一种基于判别性(discriminability)和活性(activity)的典型集精炼方法，将激活校正到通道感知的典型集中。此外，引入了基于偏度(skewness)的精炼来缓解典型集估计中的分布偏差。最终，利用这些校正后的激活来计算能量分数(energy score)进行OOD检测。", "result": "在ImageNet-1K和CIFAR-100基准测试中，本文方法取得了最先进的性能，并能有效地泛化到不同的骨干网络和分数函数上。", "conclusion": "通过引入通道感知和偏度感知的激活校正机制，本文方法克服了现有激活基OOD检测方法的局限性，显著提高了OOD检测的准确性和鲁棒性，为机器学习模型的安全部署提供了更可靠的保障。"}}
{"id": "2511.18603", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18603", "abs": "https://arxiv.org/abs/2511.18603", "authors": ["Neon Srinivasu", "Amit Shivam", "Nobin Paul"], "title": "Bifurcation-Based Guidance Law for Powered Descent Landing", "comment": null, "summary": "This paper develops a new guidance law for powered descent landing of a rocket-powered vehicle. The proposed law derives the acceleration command for a point mass model of the vehicle by expressing velocity as a dynamical system undergoing supercritical transcritical bifurcation with three bifurcation parameters. The parameters are designed such that the stable equilibrium points of the velocity dynamics correspond to the guided targeting state, that is, the landing point. Numerical simulations are performed to demonstrate the working of the proposed guidance law.", "AI": {"tldr": "本文提出了一种基于超临界跨临界分岔的动力下降着陆制导律，通过将速度表示为动态系统，设计参数使其稳定平衡点对应目标着陆点。", "motivation": "开发一种新的火箭动力飞行器动力下降着陆制导律。", "method": "该方法为飞行器的质点模型推导了加速度指令，将速度表示为一个经历超临界跨临界分岔的动力系统，并使用三个分岔参数。这些参数经过设计，使得速度动力学的稳定平衡点对应于制导目标状态（即着陆点）。", "result": "数值模拟结果表明了所提出制导律的有效性。", "conclusion": "本文成功开发了一种基于分岔理论的动力下降着陆制导律，并通过仿真验证了其可行性。"}}
{"id": "2511.17655", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17655", "abs": "https://arxiv.org/abs/2511.17655", "authors": ["Md. Mohaiminul Islam", "Md. Mofazzal Hossen", "Maher Ali Rusho", "Nahiyan Nazah Ridita", "Zarin Tasnia Shanta", "Md. Simanto Haider", "Ahmed Faizul Haque Dhrubo", "Md. Khurshid Jahan", "Mohammad Abdul Qayum"], "title": "Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment", "comment": "This paper contains 17 pages, 4 tables, and 19 figures. This Paper is already accepted in IEEE Computational Intelligence Magazine (CIM)", "summary": "Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.", "AI": {"tldr": "本研究提供了一个完整的深度学习系统，用于从MRI图像中自动分类脑肿瘤，比较了六种架构（包括一个紧凑型CNN），并重点关注准确性、可解释性和可部署性，尤其适用于资源受限的环境。", "motivation": "开发一个能够自动化、准确、可解释且易于部署的脑肿瘤分类系统，以克服现有模型的黑箱问题，并适应不同医疗资源条件下的临床应用。", "method": "研究评估了六种深度学习架构（VGG-16、Inception V3、ResNet-50、Inception-ResNet V2、Xception以及一个自定义的紧凑型CNN）。方法包括对预处理、训练协议（使用AdamW优化器、CosineAnnealingLR、早停策略）和性能评估指标进行全面标准化。为提高可解释性，使用了Grad-CAM和GradientShap。性能评估超越了单一准确率，还包括了交并比(IoU)、豪斯多夫距离(Hausdorff distance)、精确召回曲线和混淆矩阵。", "result": "Inception-ResNet V2达到了最先进的性能，测试准确率高达99.53%，精确度、召回率和F1分数均达到至少99.50%。研究开发的紧凑型CNN（1.31M参数）实现了96.49%的测试准确率，体积比Inception-ResNet V2小100倍，并支持在边缘设备上进行实时推理（375毫秒）。这证明了轻量级模型在资源不足环境下部署的适用性。", "conclusion": "本研究提供了一个端到端的解决方案，构建了一个可信赖的人工智能框架，兼顾了准确性、可解释性和可部署性。该系统适用于先进和低资源医疗系统，能够在临床筛查和分诊层面发挥作用。"}}
{"id": "2511.18088", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18088", "abs": "https://arxiv.org/abs/2511.18088", "authors": ["Ibrahim Alsarraj", "Yuhao Wang", "Abdalla Swikir", "Cesare Stefanini", "Dezhen Song", "Zhanchi Wang", "Ke Wu"], "title": "A Unified Multi-Dynamics Framework for Perception-Oriented Modeling in Tendon-Driven Continuum Robots", "comment": null, "summary": "Tendon-driven continuum robots offer intrinsically safe and contact-rich interactions owing to their kinematic redundancy and structural compliance. However, their perception often depends on external sensors, which increase hardware complexity and limit scalability. This work introduces a unified multi-dynamics modeling framework for tendon-driven continuum robotic systems, exemplified by a spiral-inspired robot named Spirob. The framework integrates motor electrical dynamics, motor-winch dynamics, and continuum robot dynamics into a coherent system model. Within this framework, motor signals such as current and angular displacement are modeled to expose the electromechanical signatures of external interactions, enabling perception grounded in intrinsic dynamics. The model captures and validates key physical behaviors of the real system, including actuation hysteresis and self-contact at motion limits. Building on this foundation, the framework is applied to environmental interaction: first for passive contact detection, verified experimentally against simulation data; then for active contact sensing, where control and perception strategies from simulation are successfully applied to the real robot; and finally for object size estimation, where a policy learned in simulation is directly deployed on hardware. The results demonstrate that the proposed framework provides a physically grounded way to interpret interaction signatures from intrinsic motor signals in tendon-driven continuum robots.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18573", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18573", "abs": "https://arxiv.org/abs/2511.18573", "authors": ["Yiying He", "Zhiqiang Zuo", "Yianni Alissandratos", "Penny Willson", "Shameem Kazmi", "Alex P. S. Brogan", "Miao Guo"], "title": "Beyond the Expiry Date: Uncovering Hidden Value in Functional Drink Waste for a Circular Future", "comment": null, "summary": "Expired functional drinks have great valorisation potential due to the high concentration of organic molecules present. However, detailed information of the resources in these expired functional drinks is limited, hindering the rational design of a recovery system. To address this gap, we present here a study that comprehensively characterises the chemical composition of functional drinks and discus their potential use as feedstocks for biomethane production. The example functional drinks were abundant in sugars, organic acids, and amino acids, and were especially rich in glucose, fructose, and alanine. Our studies revealed that functional drinks with high COD values that corresponded to high proportions of sugar and organic acid and low proportions of sorbitol and amino acids could realise profitable recovery through anaerobic digestion, with a minimum biomethane yield of 11.72 mL CH4 / mL drink. To assess utility further we also examined the dynamic composition of functional drinks up to 16 weeks (at 4 °C) after expiration to capture the shift in resources during deterioration. In doing so, we identified 4 distinct periods of carbon resource variation: 1) chemically stable period, 2) sorbitol degradation period, 3) sugar degradation period, and 4) acidification period. Based on the time-course biomethane production experiments for expired functional drinks, the optimal operating time window for biomethane production from drinks without ascorbic acid would be after sorbitol degradation period in terms of its economic performance through convenient natural deterioration. Therefore, this comprehensive study on dynamic chemical composition in expired functional drinks and their biomethane production potential could facilitate a rational design of resource recovery system for soft drink field.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18054", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18054", "abs": "https://arxiv.org/abs/2511.18054", "authors": ["Gowtham", "Sai Rupesh", "Sanjay Kumar", "Saravanan", "Venkata Chaithanya"], "title": "Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets", "comment": null, "summary": "High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.", "AI": {"tldr": "本文提出了Blu-WERP，一种新颖的数据预处理流程，旨在优化Common Crawl数据质量以提升LLM性能，并在多模型规模和基准测试中显著优于现有基线。", "motivation": "高质量的训练数据对大型语言模型（LLM）的性能至关重要，但现有预处理流程在有效去除网络规模语料库中的噪声和非结构化内容方面存在不足。", "method": "Blu-WERP是一个新颖的数据预处理流程，专门设计用于优化Common Crawl WARC文件的质量。它通过先进的过滤和质量评估机制处理CC WARC转储。作者使用150M、400M、530M、750M和1B参数的模型，并在九个标准基准（涵盖世界知识与推理、语言理解和常识推理）上进行了综合评估，与DCLM等现有基线进行比较。", "result": "Blu-WERP在所有模型规模上始终表现出卓越性能。在1B参数规模下，Blu-WERP比DCLM和Fineweb分别取得了4.0%和9.5%的总体性能提升，并实现了更高的单位token质量效率。在类别分析中，它在世界知识与推理方面提升2.4%，在语言理解方面提升6.2%，在常识推理方面提升4.2%。", "conclusion": "Blu-WERP被确立为最先进的预处理流程，显著提高了LLM训练数据质量和下游模型性能，同时降低了计算成本。这表明预处理流程设计对LLM能力有重大影响，为研究人员和实践者提供了一个提高LLM训练效率和模型性能的有效解决方案。"}}
{"id": "2511.17649", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17649", "abs": "https://arxiv.org/abs/2511.17649", "authors": ["Jieru Lin", "Zhiwei Yu", "Börje F. Karlsson"], "title": "SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios", "comment": null, "summary": "Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18579", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18579", "abs": "https://arxiv.org/abs/2511.18579", "authors": ["Kooktae Lee", "Ethan Brook"], "title": "Connectivity-Preserving Multi-Agent Area Coverage via Optimal-Transport-Based Density-Driven Optimal Control (D2OC)", "comment": "Under review in IEEE Control Systems Letters (LCSS). 6 pages", "summary": "Multi-agent systems play a central role in area coverage tasks across search-and-rescue, environmental monitoring, and precision agriculture. Achieving non-uniform coverage, where spatial priorities vary across the domain, requires coordinating agents while respecting dynamic and communication constraints. Density-driven approaches can distribute agents according to a prescribed reference density, but existing methods do not ensure connectivity. This limitation often leads to communication loss, reduced coordination, and degraded coverage performance.\n  This letter introduces a connectivity-preserving extension of the Density-Driven Optimal Control (D2OC) framework. The coverage objective, defined using the Wasserstein distance between the agent distribution and the reference density, admits a convex quadratic program formulation. Communication constraints are incorporated through a smooth connectivity penalty, which maintains strict convexity, supports distributed implementation, and preserves inter-agent communication without imposing rigid formations.\n  Simulation studies show that the proposed method consistently maintains connectivity, improves convergence speed, and enhances non-uniform coverage quality compared with density-driven schemes that do not incorporate explicit connectivity considerations.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18170", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18170", "abs": "https://arxiv.org/abs/2511.18170", "authors": ["Kaier Liang", "Licheng Luo", "Yixuan Wang", "Mingyu Cai", "Cristian Ioan Vasile"], "title": "Time-aware Motion Planning in Dynamic Environments with Conformal Prediction", "comment": null, "summary": "Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io", "AI": {"tldr": "本文提出两个基于共形预测（CP）的运动规划框架，包括一个全局规划器和一个局部规划器，旨在解决动态环境中不确定障碍物行为带来的安全导航挑战，并通过自适应分位数机制增强轨迹可行性。", "motivation": "在动态环境中，由于障碍物行为的不确定性以及缺乏形式化的预测保证，实现安全导航仍然充满挑战。", "method": "本文提出两个运动规划框架：1. 一个全局规划器，结合安全区间路径规划（SIPP）进行不确定性感知的轨迹生成。2. 一个局部规划器，用于在线反应式规划，通过自适应CP缓解障碍物轨迹预测的不准确性。此外，引入了一种基于CP的自适应分位数机制，而非固定置信水平，自动调整分位数以保持轨迹可行性，从而在不确定性较高的区域自适应地收紧安全裕度。", "result": "所提出的全局规划器为长程导航提供了无分布安全保证；局部规划器通过自适应CP缓解了障碍物轨迹预测的不准确性，实现了在动态环境中的鲁棒和响应式运动；自适应分位数机制进一步增强了轨迹的可行性。通过在动态和杂乱环境中的数值实验验证了框架的有效性。", "conclusion": "所提出的基于共形预测的运动规划框架，结合自适应分位数机制，能够有效应对动态环境中的不确定性，提供安全、鲁棒且可行的导航解决方案。"}}
{"id": "2511.17946", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17946", "abs": "https://arxiv.org/abs/2511.17946", "authors": ["Shuo Zhang", "Fabrizio Gotti", "Fengran Mo", "Jian-Yun Nie"], "title": "Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models", "comment": null, "summary": "Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.", "AI": {"tldr": "LLMs幻觉是挑战。本文研究训练数据词汇覆盖率是否能作为幻觉检测信号。发现单独使用时效果不佳，但与对数概率结合时能提供补充信号，尤其在模型不确定性高的数据集上。", "motivation": "大型语言模型（LLMs）在开放域问答中面临幻觉这一根本性挑战。现有工作主要利用模型内部信号（如token级熵或生成一致性）来检测幻觉，但预训练数据暴露与幻觉之间的联系尚未被充分探索。尽管已知LLMs在长尾知识上表现不佳，但数据覆盖率本身作为检测信号的潜力被忽视。", "method": "通过在RedPajama的1.3万亿token预训练语料库上构建可扩展的后缀数组，检索问题和模型生成的n-gram统计数据。随后，在三个问答基准上评估这些词汇覆盖特征对幻觉检测的有效性。", "result": "实验结果表明，单独使用基于词频的特征作为预测因子时效果较弱。然而，当这些特征与对数概率结合时，尤其是在模型内在不确定性较高的数据集上，能带来适度的性能提升。", "conclusion": "词汇覆盖率特征为幻觉检测提供了一个补充信号。"}}
{"id": "2511.17955", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17955", "abs": "https://arxiv.org/abs/2511.17955", "authors": ["Dat Thanh Nguyen", "Nguyen Hung Lam", "Anh Hoang-Thi Nguyen", "Trong-Hop Do"], "title": "MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok", "comment": "Accepted at PACLIC39", "summary": "With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18140", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18140", "abs": "https://arxiv.org/abs/2511.18140", "authors": ["Yilong Wang", "Cheng Qian", "Ruomeng Fan", "Edward Johns"], "title": "Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting", "comment": "Videos are available on our project webpage at https://obact.github.io", "summary": "We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17668", "abs": "https://arxiv.org/abs/2511.17668", "authors": ["Ziyuan Gao"], "title": "MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation", "comment": "Accepted by WACV 2026 (round 2)", "summary": "Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17681", "abs": "https://arxiv.org/abs/2511.17681", "authors": ["Weiyi Lv", "Ning Zhang", "Hanyang Sun", "Haoran Jiang", "Kai Zhao", "Jing Xiao", "Dan Zeng"], "title": "Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models", "comment": null, "summary": "Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.", "AI": {"tldr": "RMOT框架VMRMOT通过整合运动模态和多模态大语言模型，解决了现有RMOT基准中静态语言描述与动态视觉模态不匹配的问题，提升了多模态跟踪性能。", "motivation": "现有Referring Multi-Object Tracking (RMOT) 基准的语言描述仅限于静态信息（外观、相对位置、初始运动状态），无法捕捉目标运动的动态变化。这导致静态参考与动态视觉模态之间存在时间差异，并限制了多模态跟踪性能。", "method": "本文提出VMRMOT框架，通过整合从目标动态中提取的运动模态。具体而言，它利用多模态大语言模型（MLLMs）提取运动特征并生成运动感知描述，以增强视觉模态与语言参考的对齐。此外，设计了一个Vision-Motion-Reference Alignment (VMRA) 模块，用于分层对齐视觉查询与运动和参考线索，增强跨模态一致性。还开发了一个Motion-Guided Prediction Head (MGPH)，利用运动模态来提升预测头的性能。VMRMOT是首个将MLLMs应用于RMOT任务中进行视觉-参考对齐的方法。", "result": "在多个RMOT基准上的广泛实验表明，VMRMOT超越了现有最先进的方法。", "conclusion": "VMRMOT通过引入运动模态和利用MLLMs，有效解决了RMOT中静态语言描述与动态视觉模态不匹配的挑战，显著提升了跟踪性能。它是首个将MLLMs应用于RMOT视觉-参考对齐的方法。"}}
{"id": "2511.18162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18162", "abs": "https://arxiv.org/abs/2511.18162", "authors": ["Sheridan Feucht", "Byron Wallace", "David Bau"], "title": "Vector Arithmetic in Concept and Token Subspaces", "comment": "9 pages, 6 figures. NeurIPS 2025 Mechanistic Interpretability Workshop", "summary": "In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that \"Athens\" - \"Greece\" + \"China\" = \"Beijing\". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like \"coding\" - \"code\" + \"dance\" = \"dancing\".", "AI": {"tldr": "研究表明，大型语言模型（LLMs）中的概念归纳头和词元归纳头能将语义和表层信息解耦。利用这些头的注意力权重转换隐藏状态，可以揭示连贯的语义和表层结构，显著提高语义算术和最近邻准确性。", "motivation": "LLMs在预测下一个词元时需要同时表示词的语义和表层信息。先前研究已识别出两种注意力头（概念归纳头和词元归纳头）可以解耦这些信息。本研究旨在探索这些头如何揭示模型激活中连贯的语义结构。", "method": "本研究在Llama-2-7b模型中，利用概念归纳头（复制词义）和词元归纳头（复制字面词元表示）的注意力权重来转换隐藏状态。然后，对转换后的隐藏状态执行平行四边形算术（如“雅典” - “希腊” + “中国” = “北京”）以评估语义连贯性，并测量最近邻准确率。同时，也对利用词元归纳头转换后的隐藏状态进行操作，以揭示表层词信息。", "result": "当使用概念头注意力权重转换隐藏状态时，能够更准确地执行平行四边形算术。这种转换使得最近邻准确率从原始隐藏状态的47%大幅提高到80%。类似地，词元头允许的转换能揭示隐藏状态中的表层词信息，支持诸如“coding” - “code” + “dance” = “dancing”等操作。这表明这些头能识别出具有连贯语义结构的激活子空间。", "conclusion": "概念归纳头和词元归纳头能够识别模型激活中分别展示连贯语义和表层词信息的子空间。通过利用这些头进行隐藏状态转换，可以有效地解耦并揭示LLMs内部对语言信息（语义和表层）的组织方式，显著提高相关操作的准确性。"}}
{"id": "2511.18153", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18153", "abs": "https://arxiv.org/abs/2511.18153", "authors": ["Shreyas Kumar", "Barat S", "Debojit Das", "Yug Desai", "Siddhi Jain", "Rajesh Kumar", "Harish J. Palanthandalam-Madapusi"], "title": "A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies", "comment": "10 pages, 9 figures", "summary": "Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18768", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18768", "abs": "https://arxiv.org/abs/2511.18768", "authors": ["Jiyu Lee", "Shenghui Cui"], "title": "Accelerated Transformer Energization Sequence for Inverter Based Resources in Black-Start Procedures with Active Flux Trajectory Manipulation in the Stationary Reference Frame", "comment": null, "summary": "This paper proposes advanced soft-magnetization techniques to enable ultra-fast and reliable black-start of grid-forming (GFM) converters. Conventional hard-magnetization with well-established three-phase voltages during transformer energization induces severe inrush currents due to flux offset, which can damage power semiconductor devices. To overcome this drawback, an ultra-fast soft-magnetization method is firstly introduced, leveraging the voltage programmability of the inverter to actively reshape the initial voltage profile and thereby eliminate flux offset of the transformer core. By suppressing the formation of flux offset itself, the proposed approach prevents magnetic saturation and achieves nominal terminal voltage within a few milliseconds while effectively suppressing inrush current. However, this method can still trigger surge currents to power semiconductor devices in the presence of an LC filter due to abrupt voltage magnitude and phase transitions. To address this issue, an enhanced Archimedean spiral soft-magnetization method is developed, where both voltage magnitude and phase evolve smoothly to simultaneously suppress inrush and surge currents. Furthermore, residual flux in the transformer core is considered, and a demagnetization sequence using the inverter is validated to ensure reliable start-up. Experimental results confirm that the proposed methods achieve rapid black-start performance within one fundamental cycle while ensuring safe and stable operation of GFM converters.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17674", "abs": "https://arxiv.org/abs/2511.17674", "authors": ["Kien Nguyen", "Feng Liu", "Clinton Fookes", "Sridha Sridharan", "Xiaoming Liu", "Arun Ross"], "title": "Person Recognition in Aerial Surveillance: A Decade Survey", "comment": "Accepted at T-BIOM", "summary": "The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18800", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18800", "abs": "https://arxiv.org/abs/2511.18800", "authors": ["Matthew Hampsey", "Pieter van Goor", "Ravi Banavar", "Robert Mahony"], "title": "Equivariant Tracking Control for Fully Actuated Mechanical Systems on Matrix Lie Groups", "comment": null, "summary": "Mechanical control systems such as aerial, marine, space, and terrestrial robots often naturally admit a state-space that has the structure of a Lie group. The kinetic energy of such systems is commonly invariant to the induced action by the Lie group, and the system dynamics can be written as a coupled ordinary differential equation on the group and the dual space of its Lie algebra, termed a Lie-Poisson system. In this paper, we show that Lie-Poisson systems can also be written as a left-invariant system on a semi-direct Lie group structure placed on the trivialised cotangent bundle of the symmetry group. The authors do not know of a prior reference for this observation and we are confident the insight has never been exploited in the context of tracking control. We use this representation to build a right-invariant tracking error for the full state of a Lie-Poisson mechanical system and show that the error dynamics for this error are themselves of Lie-Poisson structure, albeit with time-varying inertia. This allows us to tackle the general trajectory tracking problem using an energy shaping design metholodology. To demonstrate the approach, we apply the proposed design methodology to a simple attitude tracking control.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17722", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17722", "abs": "https://arxiv.org/abs/2511.17722", "authors": ["Saurav Sengupta", "Nazanin Moradinasab", "Jiebei Liu", "Donald E. Brown"], "title": "Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions", "comment": null, "summary": "Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.", "AI": {"tldr": "本研究开发了一个合成基准数据集和评估框架，以系统地分析视觉语言模型（VLMs）在不同图像和提示属性下计数性能的变化，并探讨注意力干预对计数性能的影响。", "motivation": "先前的研究表明VLMs在回答图像视觉属性查询时，尤其是在计数等需要高度特定化关注的任务中，会依赖训练中学到的固有偏见，且这种偏见在特定问题下会加剧。本研究旨在系统地理解和潜在地改进VLMs的计数能力。", "method": "1. 开发了一个合成基准数据集和评估框架，以系统地评估计数性能随图像（对象数量、颜色、背景颜色、纹理）和提示（特异性）属性变化的情况。2. 使用开源VLM分析注意力分配如何随输入参数变化。3. 实施基于注意力的干预措施，以调节不同层级对视觉token的关注，并评估其对计数性能的影响。", "result": "1. VLM的计数性能仍然具有挑战性，尤其是在高视觉或语言复杂性条件下。2. 某些注意力干预措施可以使计数性能获得适度提升。", "conclusion": "尽管VLM的计数能力仍面临挑战，特别是在复杂情境下，但通过实施特定的注意力干预，可以在一定程度上改善其计数表现。"}}
{"id": "2511.18177", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18177", "abs": "https://arxiv.org/abs/2511.18177", "authors": ["Elias Lumer", "Matt Melich", "Olivia Zino", "Elena Kim", "Sara Dieter", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah", "James A. Burke", "Roberto Hernandez"], "title": "Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models", "comment": "8 pages, 2 figures", "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.", "AI": {"tldr": "本文对金融文档领域的向量基和非向量基RAG架构进行了首次系统性比较，并评估了先进RAG技术（如交叉编码器重排和大小块检索）对检索准确性、答案质量、延迟和成本的影响，发现向量基RAG结合这些增强技术表现更优。", "motivation": "现有研究缺乏对金融文档中向量基和非向量基RAG架构的系统比较，且先进RAG技术对检索准确性、答案质量、延迟和成本的实际影响尚不明确。", "method": "研究采用系统性评估方法，比较了使用混合搜索和元数据过滤的向量基代理式RAG与遍历文档结构的非嵌入式分层节点系统。同时，对向量基架构应用了两种增强技术：交叉编码器重排（用于提高检索精度）和大小块检索（用于提高上下文完整性）。评估基于1,200份SEC文件和150个问题基准，测量了检索指标（MRR、Recall@5）、答案质量（通过LLM作为评判者的成对比较）、延迟和预处理成本。", "result": "向量基代理式RAG在与分层节点系统比较中取得了68%的胜率，且延迟相当（5.2秒对比5.98秒）。交叉编码器重排在最佳参数下使MRR@5绝对提高了59%。大小块检索相对于基线分块取得了65%的胜率，仅增加了0.2秒的延迟。", "conclusion": "将先进的RAG技术应用于金融问答系统可以提高检索准确性和答案质量，但在生产环境中需要权衡其成本与性能。"}}
{"id": "2511.18146", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18146", "abs": "https://arxiv.org/abs/2511.18146", "authors": ["Yomal De Mel", "Nisansa de Silva"], "title": "GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set", "comment": null, "summary": "This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19070", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19070", "abs": "https://arxiv.org/abs/2511.19070", "authors": ["Anis Ahmed", "Arefin Ahamed Shuvo", "Naruttam Kumar Roy", "Neloy Prosad Bishnu", "Ali Nasir"], "title": "Impact Analysis of COVID-19 in Bangladesh Power Sector and Recommendations based on Practical Data and Machine Learning Approach", "comment": null, "summary": "This paper investigates the impact of COVID-19 on the power sector in Bangladesh, how the country has dealt with it, and explores the path to stability. The study employs data visualisation and complex statistics to examine critical data about power systems in Bangladesh. This includes load patterns on a daily, monthly, annual, weekend, and weekday basis. Significant alterations in these patterns have been observed during our study e.g., in April and May of 2020, the power demand decreased by approximately 15.4% and 17.2%, respectively, compared to the corresponding period in 2019. We have used a Long-Short-Term Memory (LSTM) framework to predict the load profile of 2020 excluding COVID-19 effects. This model is compared with the actual load profile to determine the degree to which COVID-19 has impacted. The comparison indicates that the average power demand decreased by approximately 19.5% in April 2020 and 18.3% in May 2020, relative to its projected value. The study also investigates system stability by analyzing transmission loss and load factor, and the environmental effect by analyzing the Carbon Dioxide emission rate. Finally, the study provides recommendations for overcoming future disasters, such as developing more resilient power systems, investing in renewable energy, and improving energy efficiency.", "AI": {"tldr": "本研究分析了COVID-19对孟加拉国电力部门的影响，包括负荷模式变化、系统稳定性及环境效应，并提出了应对未来灾害的建议。", "motivation": "了解COVID-19疫情对孟加拉国电力部门的具体影响，探讨该国如何应对，并规划实现电力系统稳定的路径。", "method": "研究采用数据可视化和复杂统计方法分析孟加拉国电力系统的关键数据，包括日常、月度、年度、周末和工作日的负荷模式。使用长短期记忆（LSTM）框架预测了排除COVID-19影响的2020年负荷曲线，并与实际负荷曲线进行比较以量化疫情影响。此外，还通过分析输电损耗、负荷系数和二氧化碳排放率来评估系统稳定性和环境影响。", "result": "研究发现负荷模式出现显著变化，例如2020年4月和5月的电力需求分别比2019年同期下降约15.4%和17.2%。与预测值（无COVID-19影响）相比，2020年4月和5月的平均电力需求分别下降了约19.5%和18.3%。研究还分析了系统稳定性（输电损耗、负荷系数）和环境影响（二氧化碳排放率）。", "conclusion": "COVID-19对孟加拉国的电力需求产生了显著影响。为应对未来的灾害，研究建议发展更具弹性的电力系统、投资可再生能源并提高能源效率。"}}
{"id": "2511.19055", "categories": ["eess.SY", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.19055", "abs": "https://arxiv.org/abs/2511.19055", "authors": ["Xinda Zheng", "Canchen Jiang", "Hao Wang"], "title": "Large Language Model-Assisted Planning of Electric Vehicle Charging Infrastructure with Real-World Case Study", "comment": null, "summary": "The growing demand for electric vehicle (EV) charging infrastructure presents significant planning challenges, requiring efficient strategies for investment and operation to deliver cost-effective charging services. However, the potential benefits of EV charging assignment, particularly in response to varying spatial-temporal patterns of charging demand, remain under-explored in infrastructure planning. This paper proposes an integrated approach that jointly optimizes investment decisions and charging assignments while accounting for spatial-temporal demand dynamics and their interdependencies. To support efficient model development, we leverage a large language model (LLM) to assist in generating and refining the mathematical formulation from structured natural-language descriptions, significantly reducing the modeling burden. The resulting optimization model enables optimal joint decision-making for investment and operation. Additionally, we propose a distributed optimization algorithm based on the Alternating Direction Method of Multipliers (ADMM) to address computational complexity in high-dimensional scenarios, which can be executed on standard computing platforms. We validate our approach through a case study using 1.5 million real-world travel records from Chengdu, China, demonstrating a 30% reduction in total cost compared to a baseline without EV assignment.", "AI": {"tldr": "本文提出一种集成方法，联合优化电动汽车充电基础设施的投资和充电分配，考虑时空需求动态。该方法利用大型语言模型辅助建模，并采用基于ADMM的分布式优化算法处理复杂场景，在实际案例中将总成本降低了30%。", "motivation": "电动汽车充电基础设施需求增长带来规划挑战，需要高效的投资和运营策略以提供经济高效的充电服务。然而，考虑时空充电需求模式的电动汽车充电分配的潜在效益在基础设施规划中尚未得到充分探索。", "method": "提出一种集成方法，联合优化投资决策和充电分配，同时考虑时空需求动态及其相互依赖性。利用大型语言模型（LLM）辅助从结构化自然语言描述中生成和完善数学公式。提出一种基于交替方向乘子法（ADMM）的分布式优化算法，以解决高维场景下的计算复杂性。", "result": "该优化模型能够实现投资和运营的联合最优决策。在成都150万条真实出行记录的案例研究中，与没有电动汽车分配的基线相比，总成本降低了30%。", "conclusion": "该集成方法通过联合优化投资和充电分配，并考虑时空需求动态，能够显著降低电动汽车充电基础设施的总成本。所提出的分布式优化算法有效解决了高维场景的计算复杂性，证明了其在实际应用中的有效性。"}}
{"id": "2511.18183", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18183", "abs": "https://arxiv.org/abs/2511.18183", "authors": ["Yixuan Jia", "Qingyuan Li", "Jonathan P. How"], "title": "Off-Road Navigation via Implicit Neural Representation of Terrain Traversability", "comment": "9 pages", "summary": "Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19143", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19143", "abs": "https://arxiv.org/abs/2511.19143", "authors": ["Lisa Piccinin", "Valentina Breschi", "Chiara Ravazzi", "Fabrizio Dabbene", "Mara Tanelli"], "title": "Optimal policy design for innovation diffusion: shaping today's incentives for transforming the future", "comment": "Submitted to IFAC World Congress 2026 and Control Engineering Practice", "summary": "In this paper, we propose a new framework for the design of incentives aimed at promoting innovation diffusion in social influence networks. In particular, our framework relies on an extension of the Friedkin and Johnsen opinion dynamics model characterizing the effects of (i) short-memory incentives, which have an immediate yet transient impact, and (ii) long-term structural incentives, whose impact persists via an exponentially decaying memory. We propose to design these incentives via a model-predictive control (MPC) scheme over an augmented state that captures the memory in our opinion dynamics model, yielding a convex quadratic program with linear constraints. Our numerical simulations based on data on sustainable mobility habits show the effectiveness of the proposed approach, which balances large-scale adoption and resource allocation", "AI": {"tldr": "本文提出了一种新的框架，通过模型预测控制（MPC）设计激励措施，以促进社会影响力网络中的创新扩散，该框架基于扩展的Friedkin和Johnsen意见动力学模型，并考虑了短期和长期激励，在可持续出行习惯数据上显示出有效性。", "motivation": "研究旨在为社会影响力网络中促进创新扩散设计有效的激励机制。", "method": "该研究提出一个新框架，基于扩展的Friedkin和Johnsen意见动力学模型，该模型描述了短记忆激励（即时但短暂影响）和长期结构性激励（通过指数衰减记忆持续影响）的效果。通过在捕获记忆的增强状态上应用模型预测控制（MPC）方案来设计这些激励，从而产生一个具有线性约束的凸二次规划。", "result": "基于可持续出行习惯数据的数值模拟表明，所提出的方法是有效的，它平衡了大规模采纳和资源分配。", "conclusion": "所提出的基于MPC的激励设计方法能够有效促进创新在社会网络中的扩散，并能平衡大规模采纳与资源分配。"}}
{"id": "2511.17699", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17699", "abs": "https://arxiv.org/abs/2511.17699", "authors": ["Hosein Hasani", "Amirmohammad Izadi", "Fatemeh Askari", "Mobin Bagherian", "Sadegh Mohammadian", "Mohammad Izadi", "Mahdieh Soleymani Baghshah"], "title": "Understanding Counting Mechanisms in Large Language and Vision-Language Models", "comment": null, "summary": "This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18215", "abs": "https://arxiv.org/abs/2511.18215", "authors": ["Shangyuan Yuan", "Preston Fairchild", "Yu Mei", "Xinyu Zhou", "Xiaobo Tan"], "title": "AFT: Appearance-Based Feature Tracking for Markerless and Training-Free Shape Reconstruction of Soft Robots", "comment": null, "summary": "Accurate shape reconstruction is essential for precise control and reliable operation of soft robots. Compared to sensor-based approaches, vision-based methods offer advantages in cost, simplicity, and ease of deployment. However, existing vision-based methods often rely on complex camera setups, specific backgrounds, or large-scale training datasets, limiting their practicality in real-world scenarios. In this work, we propose a vision-based, markerless, and training-free framework for soft robot shape reconstruction that directly leverages the robot's natural surface appearance. These surface features act as implicit visual markers, enabling a hierarchical matching strategy that decouples local partition alignment from global kinematic optimization. Requiring only an initial 3D reconstruction and kinematic alignment, our method achieves real-time shape tracking across diverse environments while maintaining robustness to occlusions and variations in camera viewpoints. Experimental validation on a continuum soft robot demonstrates an average tip error of 2.6% during real-time operation, as well as stable performance in practical closed-loop control tasks. These results highlight the potential of the proposed approach for reliable, low-cost deployment in dynamic real-world settings.", "AI": {"tldr": "本文提出了一种基于视觉、无标记、免训练的软机器人形状重建框架，利用机器人自然表面特征进行分层匹配策略，实现了实时、鲁棒的形状跟踪，且误差较低，适用于实际控制任务。", "motivation": "现有的软机器人视觉形状重建方法通常依赖复杂的相机设置、特定背景或大量训练数据，限制了其在实际场景中的应用，因此需要一种更实用、低成本的解决方案。", "method": "该方法利用机器人自身的自然表面外观作为隐式视觉标记，采用分层匹配策略，将局部区域对齐与全局运动学优化解耦。它仅需初始的3D重建和运动学对齐，无需标记和训练数据。", "result": "实验证明，该方法能实现实时形状跟踪，对遮挡和相机视角变化具有鲁棒性。在连续软机器人上的平均尖端误差为2.6%，并在实际闭环控制任务中表现稳定。", "conclusion": "所提出的方法为软机器人形状重建提供了一种可靠、低成本的解决方案，在动态的实际环境中具有广泛的应用潜力。"}}
{"id": "2511.18243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18243", "abs": "https://arxiv.org/abs/2511.18243", "authors": ["Eashan Vytla", "Bhavanishankar Kalavakolanu", "Andrew Perrault", "Matthew McCrink"], "title": "Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters", "comment": null, "summary": "Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.", "AI": {"tldr": "本文探索了一种物理信息增强的世界模型，旨在改进Dreamer在空中机器人上的应用，以提高动态环境下的鲁棒性，但发现模型在泛化到新轨迹时存在困难，导致策略无法收敛。", "motivation": "当前的空中机器人控制算法在动态和恶劣环境下鲁棒性不足。模型基强化学习（RL）和Dreamer显示出潜力，但Dreamer应用于空中系统时面临样本效率低和动力学模型泛化能力差的挑战。", "method": "本文提出一种物理信息增强的世界模型，将四旋翼飞行器视为自由体系统，预测其受到的合力和力矩，然后通过6自由度Runge-Kutta积分器（RK4）预测未来状态。该方法与标准的基于RNN的世界模型进行了比较。", "result": "尽管两种模型在训练数据上表现良好，但它们都未能泛化到新轨迹，导致状态推演迅速发散，从而阻碍了策略的收敛。", "conclusion": "物理信息增强的世界模型和标准的RNN世界模型在训练数据上表现良好，但在泛化到新轨迹时均遇到严重问题，导致状态推演发散，未能实现策略收敛，表明空中机器人控制中世界模型的泛化能力仍是关键挑战。"}}
{"id": "2511.18236", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18236", "abs": "https://arxiv.org/abs/2511.18236", "authors": ["Nuno Soares", "António Grilo"], "title": "APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs", "comment": "9 pages", "summary": "The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.", "AI": {"tldr": "针对资源受限最短路径问题（RCSPP）在大型复杂图上现有算法可扩展性差的挑战，本文提出APULSE算法。APULSE是一种混合标签设置算法，结合A*启发式搜索、Pulse式剪枝和时间分桶策略，在大规模问题上比现有方法快数个数量级且更鲁棒，能找到接近最优解，特别适用于无人地面车辆（UGV）规划。", "motivation": "资源受限最短路径问题（RCSPP）是NP-hard优化难题，在网络路由、自主导航等领域应用广泛。然而，现有RCSPP求解器在应用于大型、密集图时面临严重的可扩展性限制，这使得它们在时间敏感的规划场景（如无人地面车辆任务规划）中不切实际。", "method": "本文提出了APULSE，一种混合标签设置算法，旨在高效解决复杂图上的RCSPP。APULSE集成了以下方法：1) 由A*启发式引导的最佳优先搜索；2) 激进的Pulse式剪枝机制；3) 用于有效减少状态空间的时间分桶策略。", "result": "通过大规模UGV规划场景的计算研究表明，APULSE始终能找到接近最优的解决方案，并且在速度和鲁棒性方面比现有最先进算法快数个数量级，尤其在竞争方法失效的大型问题实例上表现更优异。", "conclusion": "APULSE卓越的可扩展性使其成为复杂、大规模环境中RCSPP的有效解决方案，能够支持交互式决策支持和动态重新规划等功能。"}}
{"id": "2511.18203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18203", "abs": "https://arxiv.org/abs/2511.18203", "authors": ["Ziyi Yang", "Benned Hedegaard", "Ahmed Jaafar", "Yichen Wei", "Skye Thompson", "Shreyas S. Raman", "Haotian Fu", "Stefanie Tellex", "George Konidaris", "David Paulius", "Naman Shah"], "title": "SkillWrapper: Generative Predicate Invention for Skill Abstraction", "comment": null, "summary": "Generalizing from individual skill executions to solving long-horizon tasks remains a core challenge in building autonomous agents. A promising direction is learning high-level, symbolic abstractions of the low-level skills of the agents, enabling reasoning and planning independent of the low-level state space. Among possible high-level representations, object-centric skill abstraction with symbolic predicates has been proven to be efficient because of its compatibility with domain-independent planners. Recent advances in foundation models have made it possible to generate symbolic predicates that operate on raw sensory inputs, a process we call generative predicate invention, to facilitate downstream abstraction learning. However, it remains unclear which formal properties the learned representations must satisfy, and how they can be learned to guarantee these properties. In this paper, we address both questions by presenting a formal theory of generative predicate invention for skill abstraction, resulting in symbolic operators that can be used for provably sound and complete planning. Within this framework, we propose SkillWrapper, a method that leverages foundation models to actively collect robot data and learn human-interpretable, plannable representations of black-box skills, using only RGB image observations. Our extensive empirical evaluation in simulation and on real robots shows that SkillWrapper learns abstract representations that enable solving unseen, long-horizon tasks in the real world with black-box skills.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19084", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.19084", "abs": "https://arxiv.org/abs/2511.19084", "authors": ["Ruchuan Ou", "Learta Januzi", "Jonas Schießl", "Michael Heinrich Baumann", "Lars Grüne", "Timm Faulwasser"], "title": "PolyOCP.jl -- A Julia Package for Stochastic OCPs and MPC", "comment": null, "summary": "The consideration of stochastic uncertainty in optimal and predictive control is a well-explored topic. Recently Polynomial Chaos Expansions (PCE) have seen a lot of considerations for problems involving stochastically uncertain system parameters and also for problems with additive stochastic i.i.d. disturbances. While there exist a number of open-source PCE toolboxes, tailored open-source codes for the solution of OCPs involving additive stochastic i.i.d. disturbances in julia are not available. Hence, this paper introduces the toolbox PolyOCP.jl which enables to efficiently solve stochastic OCPs for a large class of disturbance distributions. We explain the main mathematical concepts between the PCE transcription of stochastic OCPs and how they are provided in the toolbox. We draw upon two examples to illustrate the functionalities of PolyOCP.jl.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18270", "abs": "https://arxiv.org/abs/2511.18270", "authors": ["Zhongkai Chen", "Yihao Sun", "Chao Yan", "Han Zhou", "Xiaojia Xiang", "Jie Jiang"], "title": "Skypilot: Fine-Tuning LLM with Physical Grounding for AAV Coverage Search", "comment": null, "summary": "Autonomous aerial vehicles (AAVs) have played a pivotal role in coverage operations and search missions. Recent advances in large language models (LLMs) offer promising opportunities to augment AAV intelligence. These advances help address complex challenges like area coverage optimization, dynamic path planning, and adaptive decision-making. However, the absence of physical grounding in LLMs leads to hallucination and reproducibility problems in spatial reasoning and decision-making. To tackle these issues, we present Skypilot, an LLM-enhanced two-stage framework that grounds language models in physical reality by integrating monte carlo tree search (MCTS). In the first stage, we introduce a diversified action space that encompasses generate, regenerate, fine-tune, and evaluate operations, coupled with physics-informed reward functions to ensure trajectory feasibility. In the second stage, we fine-tune Qwen3-4B on 23,000 MCTS-generated samples, achieving substantial inference acceleration while maintaining solution quality. Extensive numerical simulations and real-world flight experiments validate the efficiency and superiority of our proposed approach. Detailed information and experimental results are accessible at https://sky-pilot.top.", "AI": {"tldr": "本文提出Skypilot，一个LLM增强的两阶段框架，通过整合蒙特卡洛树搜索（MCTS）将语言模型与物理现实相结合，以解决自主飞行器（AAV）中LLM缺乏物理基础的问题，并通过微调实现推理加速，并通过仿真和真实飞行实验验证了其效率和优越性。", "motivation": "自主飞行器在覆盖操作和搜索任务中发挥关键作用。大型语言模型（LLM）的最新进展为增强AAV智能提供了机遇，以应对区域覆盖优化、动态路径规划和自适应决策等复杂挑战。然而，LLM缺乏物理基础导致空间推理和决策中出现幻觉和可重现性问题。", "method": "Skypilot是一个LLM增强的两阶段框架：\n1. **第一阶段**：通过整合蒙特卡洛树搜索（MCTS）将语言模型（LLM）与物理现实相结合。引入了多样化的动作空间（生成、重新生成、微调、评估）以及物理信息奖励函数，以确保轨迹的可行性。\n2. **第二阶段**：在23,000个MCTS生成的样本上微调Qwen3-4B模型，以在保持解决方案质量的同时实现显著的推理加速。", "result": "广泛的数值模拟和真实世界飞行实验验证了所提出方法的效率和优越性。在保持解决方案质量的同时，实现了显著的推理加速。", "conclusion": "Skypilot框架成功地解决了LLM在自主飞行器应用中缺乏物理基础的问题，为复杂的区域覆盖优化、动态路径规划和自适应决策提供了高效且卓越的解决方案。"}}
{"id": "2511.17724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17724", "abs": "https://arxiv.org/abs/2511.17724", "authors": ["Mohammad Atwany", "Mojtaba Lashgari", "Robin P. Choudhury", "Vicente Grau", "Abhirup Banerjee"], "title": "AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography", "comment": null, "summary": "Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG\", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18194", "abs": "https://arxiv.org/abs/2511.18194", "authors": ["Faheem Nizar", "Elias Lumer", "Anmol Gulati", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah"], "title": "Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems", "comment": null, "summary": "Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17727", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17727", "abs": "https://arxiv.org/abs/2511.17727", "authors": ["Victor Li", "Naveenraj Kamalakannan", "Avinash Parnandi", "Heidi Schambra", "Carlos Fernandez-Granda"], "title": "The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation", "comment": null, "summary": "Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.", "AI": {"tldr": "本文将视觉语言模型（VLMs）应用于中风康复视频分析，旨在量化康复剂量和损伤。结果显示，当前VLMs在精细运动理解方面存在局限性，但在高层活动分类和近似剂量计数方面展现出潜力，无需特定任务训练。", "motivation": "视觉语言模型（VLMs）在计算机视觉任务中表现出色，研究者希望探索其在数字健康领域的潜力，特别是解决数据驱动的中风康复中的两大基本挑战：从视频中自动量化康复剂量和损伤。", "method": "研究将康复剂量和损伤量化问题表述为运动识别任务，并利用VLMs来解决。该框架在一个包含29名健康对照者和51名中风幸存者的数据集上进行了评估。", "result": "结果表明，当前的VLMs缺乏精确量化所需的精细运动理解能力：剂量估算与不包含视觉信息的基线相当，损伤评分无法可靠预测。然而，通过优化提示和后处理，VLMs能够从少量帧中分类高层活动，以中等准确度检测运动和抓取，并对于轻度受损和健康参与者，能在地面真值25%的误差范围内近似剂量计数，所有这些均无需任务特定的训练或微调。", "conclusion": "研究强调了VLMs在数据驱动的中风康复和更广泛的临床视频分析中，当前在精细运动理解方面的局限性，以及在无需特定训练下处理高层任务方面的潜在机遇。"}}
{"id": "2511.18259", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.18259", "abs": "https://arxiv.org/abs/2511.18259", "authors": ["Xiaochen Zheng", "Alvaro Serra", "Ilya Schneider Chernov", "Maddalena Marchesi", "Eunice Musvasva", "Tatyana Y. Doktorova"], "title": "From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation", "comment": "22 pages, 4 figures, 3 tables", "summary": "Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17747", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17747", "abs": "https://arxiv.org/abs/2511.17747", "authors": ["Dawid Wolkiewicz", "Anastasiya Pechko", "Przemysław Spurek", "Piotr Syga"], "title": "AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations", "comment": null, "summary": "The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.", "AI": {"tldr": "AEGIS是一个针对3D高斯头像的隐私保护身份遮蔽框架，通过对高斯颜色系数施加对抗性扰动，在不影响感知真实性和功能完整性的前提下，实现完全去身份化并保持高视觉质量。", "motivation": "随着逼真3D面部头像（特别是使用3D高斯泼溅表示的头像）的日益普及，生物识别认证系统面临在线身份盗窃的新风险。现有的2D图像对抗性遮蔽方法不足以提供动态3D头像的鲁棒、视角一致的身份保护。", "method": "AEGIS通过预训练的面部验证网络指导，对高斯颜色系数应用对抗性扰动。这种方法确保了在不重新训练或修改头像几何形状的情况下，实现多视角下的一致保护，旨在隐藏身份相关面部特征同时保持头像的感知真实性和功能完整性。", "result": "AEGIS实现了完全去身份化，将面部检索和验证准确率降低到0%，同时保持了高感知质量（SSIM = 0.9555，PSNR = 35.52 dB）。它还保留了年龄、种族、性别和情感等关键面部属性。", "conclusion": "AEGIS为3D高斯头像提供了强大的隐私保护，有效解决了身份盗窃风险，并在最小的视觉失真下实现了身份隐藏，同时保留了重要的面部特征和感知质量。"}}
{"id": "2511.19231", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19231", "abs": "https://arxiv.org/abs/2511.19231", "authors": ["Amy K. Strong", "Ali Kashani", "Claus Danielson", "Leila J. Bridgeman"], "title": "Data-driven certificates of constraint enforcement and stability for unmodeled, discrete dynamical systems using tree data structures", "comment": null, "summary": "This paper addresses the critical challenge of developing data-driven certificates for the stability and safety of unmodeled dynamical systems by leveraging a tree data structure and an upper bound of the system's Lipschitz constant. Previously, an invariant set was synthesized by iteratively expanding an initial invariant set. In contrast, this work iteratively prunes the constraint set to synthesize an invariant set -- eliminating the need for a known, initial invariant set. Furthermore, we provide stability assurances by characterizing the asymptotic stability of the system relative to an invariant approximation of the minimal positive invariant set through synthesis of a discontinuous piecewise affine Lyapunov function over the computed invariant set. The proposed method takes inspiration from subdivision techniques and requires no prior system knowledge beyond Lipschitz continuity.", "AI": {"tldr": "本文提出一种数据驱动方法，利用树结构和Lipschitz常数上界，通过迭代剪枝而非扩展来合成非建模动态系统的不变集，并用分段仿射Lyapunov函数证明渐近稳定性，无需初始不变集和系统先验知识。", "motivation": "为非建模动态系统开发数据驱动的稳定性与安全性证明是一个关键挑战。", "method": "该研究利用树数据结构和系统Lipschitz常数的上界，通过迭代剪枝约束集来合成不变集（与以往的迭代扩展方法不同，无需已知初始不变集）。此外，该方法受细分技术启发，并在计算出的不变集上合成一个不连续分段仿射Lyapunov函数。", "result": "成功合成了不变集，且无需预先知道初始不变集。通过合成Lyapunov函数，提供了系统相对于最小正不变集近似的渐近稳定性保证。该方法仅需Lipschitz连续性，无需其他系统先验知识。", "conclusion": "该研究提供了一种有效的数据驱动方法，通过迭代剪枝和Lyapunov函数合成，为非建模动态系统提供了稳定性与安全性证明，且无需初始不变集和除Lipschitz连续性之外的系统先验知识。"}}
{"id": "2511.18301", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18301", "abs": "https://arxiv.org/abs/2511.18301", "authors": ["Harsh Rathva", "Pruthwik Mishra", "Shrikant Malviya"], "title": "\"AGI\" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa", "comment": "Accepted to the 1st Workshop on Confabulation, Hallucinations & Overgeneration in Multilingual and Practical Settings (CHOMPS) at AACL-IJCNLP 2025", "summary": "The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \\textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17731", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17731", "abs": "https://arxiv.org/abs/2511.17731", "authors": ["Lingxiao Li", "Yifan Wang", "Xinyan Gao", "Chen Tang", "Xiangyu Yue", "Chenyu You"], "title": "VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning", "comment": null, "summary": "Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18324", "abs": "https://arxiv.org/abs/2511.18324", "authors": ["Syed Mohaiminul Hoque", "Naimur Rahman", "Md Sakhawat Hossain"], "title": "Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection", "comment": "6 pages, 2 figures, 4 tables. Accepted at the Second International Workshop on Bangla Language Processing (BLP-2025) co-located with AACL-IJCNLP 2025. Ranked 6th (Subtask 1A, 73.23% micro F1) and 3rd (Subtask 1B, 73.28% micro F1) on the official leaderboard", "summary": "This paper introduces the approach of \"Gradient Masters\" for BLP-2025 Task 1: \"Bangla Multitask Hate Speech Identification Shared Task\". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.", "AI": {"tldr": "本文介绍了“Gradient Masters”团队在BLP-2025孟加拉语多任务仇恨言论识别共享任务中，提出了一种基于集成微调的混合方法，用于识别YouTube评论中的仇恨类型和目标群体，并取得了显著成果。", "motivation": "本研究的动机是参与BLP-2025任务1：“孟加拉语多任务仇恨言论识别共享任务”，旨在解决YouTube评论中仇恨类型分类（子任务1A）和目标群体分类（子任务1B）问题，特别是在孟加拉语这种低资源仇恨言论场景中评估模型的泛化能力和鲁棒性。", "method": "该论文采用“Gradient Masters”方法，提出了一种基于集成的微调策略，结合孟加拉语语言模型（Bangla Language Model）的混合方法。通过广泛的实验，评估了模型在开发和评估阶段的鲁棒性，并与其他语言模型变体进行了比较，以衡量其在低资源孟加拉语仇恨言论场景下的泛化能力和数据集覆盖范围。", "result": "所提出的方法超越了基线模型，在子任务1A中获得了微F1分数73.23%的第六名，在子任务1B中获得了73.28%的第三名。实验结果验证了模型在不同阶段的鲁棒性。此外，论文还对错误分类模式进行了详细分析。", "conclusion": "该论文的结论是，所提出的基于集成微调的混合方法在孟加拉语仇恨言论识别任务中表现出色，尤其在低资源场景下展现了良好的泛化能力和鲁棒性，并为理解仇恨言论检测中的错误分类模式提供了深入分析。"}}
{"id": "2511.18293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18293", "abs": "https://arxiv.org/abs/2511.18293", "authors": ["Shuai Zhang", "Jingsong Mu", "Cancan Zhao", "Leiqi Tian", "Zhijun Xing", "Bo Ouyang", "Xiang Li"], "title": "AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization", "comment": null, "summary": "Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis. However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging. Localization methods face challenges related to local minima due to the selection of initial poses. In this study, we design a robotic ultrasound system (RUSS) with an acoustic-impedance-aware ultrasound NeRF (AIA-UltraNeRF) to decouple the scanning and diagnostic processes. Specifically, AIA-UltraNeRF models a continuous function of hash-encoded spatial coordinates for the 3D ultrasound map, allowing for the storage of acoustic impedance without dense sampling. This approach accelerates both reconstruction and inference speeds. We then propose a dual-supervised network that leverages teacher and student models to hash-encode the rendered ultrasound images from the reconstructed map. AIA-UltraNeRF retrieves the most similar hash values without the need to render images again, providing an offline initial image position for localization. Moreover, we develop a RUSS with a spherical remote center of motion mechanism to hold the probe, implementing operator-independent scanning modes that separate image acquisition from diagnostic workflows. Experimental results on a phantom and human subjects demonstrate the effectiveness of acoustic impedance in implicitly characterizing the color of ultrasound images. AIAUltraNeRF achieves both reconstruction and localization with inference speeds that are 9.9 faster than those of vanilla NeRF.", "AI": {"tldr": "本研究提出了一种基于声阻抗感知的超声NeRF（AIA-UltraNeRF）和机器人超声系统（RUSS），以加速超声图像重建和定位，并实现扫描与诊断过程解耦。", "motivation": "以往基于NeRF的重建方法忽视了超声成像中声阻抗的关键作用；定位方法因初始姿态选择不当而面临局部最小值挑战。", "method": "本研究设计了一个机器人超声系统（RUSS），并开发了声阻抗感知的超声NeRF（AIA-UltraNeRF）。AIA-UltraNeRF通过哈希编码空间坐标建模3D超声图的连续函数，无需密集采样即可存储声阻抗，从而加速重建和推理。此外，提出了一种双监督网络，利用教师和学生模型对重建图渲染的超声图像进行哈希编码，通过检索相似哈希值提供离线初始图像位置。RUSS采用球形远程运动中心机制，实现操作员独立的扫描模式。", "result": "实验证明声阻抗能有效表征超声图像的颜色。AIA-UltraNeRF在重建和定位方面的推理速度比传统NeRF快9.9倍。该系统成功实现了扫描与诊断工作流程的分离。", "conclusion": "AIA-UltraNeRF结合RUSS有效解决了传统NeRF在超声重建和定位中的局限性，通过引入声阻抗概念和优化网络结构，显著提高了速度和效率，并支持操作员独立的扫描模式。"}}
{"id": "2511.19287", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19287", "abs": "https://arxiv.org/abs/2511.19287", "authors": ["Mamoon Aamir", "Mariyam Sattar", "Naveed Ur Rehman Junejo", "Aqsa Zafar Abbasi"], "title": "Innovative Modular Design and Kinematic Approach based on Screw Theory for Triple Scissors Links Deployable Space Antenna Mechanism", "comment": null, "summary": "This paper presents the geometry design and analysis of a novel triple scissors links deployable antenna mechanism (TSDAM) to deal with the problems of large aperture and high precision space antennas for deep space communication and Earth observation. This mechanism has only one degree of freedom (DoF) and thus makes for efficient and reliable deployment without loss of structural integrity. It employed a systematic design approach starting from a triple scissors links modular unit to a 25m aperture assembly. Different configurations constituting variable numbers of modular units were analyzed in SolidWorks to identify the deployable mechanism with lowest deformation. While the 24 units configuration offered superior stowage compactness, it exhibited higher deformation (0.01437mm), confirming the 12 units configuration as the optimal balance between structural stability and deployment efficiency. Screw theory was employed to analyze the kinematic properties, and numerical simulations were performed in MATLAB and SolidWorks. The deployable space antenna showed transition from stowed to fully deployed state in just 53 seconds with high stability throughout the deployment process. The TSDAM attained a storage ratio of up to 15.3 for height and volume with 0.01048mm of deformation for a 12 units configuration. Mesh convergence analysis proved the consistency of the simulation results for 415314 tetrahedral shaped elements. The virtual experiments in SolidWorks verified the analytical Screw theory based model and ensured that the design was smooth and flexible for deployment in operational conditions. The research establishes a robust design framework for future deployable antennas, offering enhanced performance, simplified structure, and improved reliability", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18313", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18313", "abs": "https://arxiv.org/abs/2511.18313", "authors": ["Joseph Oladokun"], "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search", "comment": "10 pages", "summary": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.", "AI": {"tldr": "本文提出路径约束检索（PCR）方法，通过结合图结构约束和语义搜索，确保大型语言模型（LLM）代理从知识图中检索到的信息在结构上保持一致，从而提高推理的连贯性。", "motivation": "LLM代理在从知识库中检索上下文时，常因检索到的信息与当前推理状态缺乏结构一致性而导致推理链不连贯。", "method": "路径约束检索（PCR）方法结合了结构图约束和语义搜索。它将搜索空间限制在可从锚点节点到达的节点，从而防止检索到结构上不相关的信息，确保检索到的信息在知识图内保持逻辑关系。", "result": "在PathRAG-6基准测试中，PCR实现了完全的结构一致性（基线方法为24-32%），并保持了强大的相关性分数。在技术领域，PCR在秩10时实现了完全相关性和完全结构一致性，显著优于向量搜索和混合检索。与基线相比，PCR将检索上下文的平均图距离减少了78%。", "conclusion": "研究结果表明，路径约束检索是一种有效的方法，可以提高LLM代理推理系统的可靠性和连贯性。"}}
{"id": "2511.17735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17735", "abs": "https://arxiv.org/abs/2511.17735", "authors": ["Samuel Stevens", "Jacob Beattie", "Tanya Berger-Wolf", "Yu Su"], "title": "Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders", "comment": null, "summary": "Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18306", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18306", "abs": "https://arxiv.org/abs/2511.18306", "authors": ["Mohammad Aqib", "Mohd Hamza", "Ying Hei Chui", "Qipei Mei"], "title": "Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning", "comment": null, "summary": "Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.", "AI": {"tldr": "本研究探讨了两种从建筑法规表格数据中提取信息的方法（直接图像输入和LaTeX转换），并发现直接输入法更准确。通过LoRA微调，VLMs在领域特定数据集上的性能显著提升，尤其Qwen2.5-VL-3B-Instruct的准确率提升超过100%。", "motivation": "建筑法规包含关键信息，自动化问答系统能提高效率并减少错误。RAG系统对该任务至关重要，但建筑法规中的表格数据因复杂布局、合并单元格和多行标题等问题，对传统NLP和VLM构成挑战。", "method": "本研究比较了两种使用预训练VLM从建筑法规表格数据中提取信息的方法：1) 直接输入法：将页面图像直接输入VLM以回答问题。2) 间接输入法：将包含表格的页面图像转换为LaTeX代码，然后基于LaTeX输入进行问答。为进一步提升性能，研究还使用低秩适应（LoRA）在领域特定表格数据集上对每个VLM进行了微调。", "result": "实验发现，直接输入法通常比间接输入法具有更高的准确性。通过LoRA微调后，模型的性能显著提升，其中Qwen2.5-VL-3B-Instruct的相对准确率增益超过100%。", "conclusion": "研究结果强调了参数高效微调方法（如LoRA）在使强大VLM适应理解专业领域（如建筑法规解释和合规性）中复杂结构化数据方面的潜力。"}}
{"id": "2511.19383", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19383", "abs": "https://arxiv.org/abs/2511.19383", "authors": ["Viet-Anh Le", "Mu Xie", "Rahul Mangharam"], "title": "A Hybrid Learning-to-Optimize Framework for Mixed-Integer Quadratic Programming", "comment": "submitted to L4DC 2026", "summary": "In this paper, we propose a learning-to-optimize (L2O) framework to accelerate solving parametric mixed-integer quadratic programming (MIQP) problems, with a particular focus on mixed-integer model predictive control (MI-MPC) applications. The framework learns to predict integer solutions with enhanced optimality and feasibility by integrating supervised learning (for optimality), self-supervised learning (for feasibility), and a differentiable quadratic programming (QP) layer, resulting in a hybrid L2O framework. Specifically, a neural network (NN) is used to learn the mapping from problem parameters to optimal integer solutions, while a differentiable QP layer is integrated to compute the corresponding continuous variables given the predicted integers and problem parameters. Moreover, a hybrid loss function is proposed, which combines a supervised loss with respect to the global optimal solution, and a self-supervised loss derived from the problem's objective and constraints. The effectiveness of the proposed framework is demonstrated on two benchmark MI-MPC problems, with comparative results against purely supervised and self-supervised learning models.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18299", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18299", "abs": "https://arxiv.org/abs/2511.18299", "authors": ["Steven Oh", "Tai Inui", "Magdeline Kuan", "Jia-Yeu Lin"], "title": "MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing", "comment": null, "summary": "Robotic manipulation tasks are contact-rich, yet most imitation learning (IL) approaches rely primarily on vision, which struggles to capture stiffness, roughness, slip, and other fine interaction cues. Tactile signals can address this gap, but existing sensors often require expensive, delicate, or integration-heavy hardware. In this work, we introduce MicCheck, a plug-and-play acoustic sensing approach that repurposes an off-the-shelf Bluetooth pin microphone as a low-cost contact sensor. The microphone clips into a 3D-printed gripper insert and streams audio via a standard USB receiver, requiring no custom electronics or drivers. Despite its simplicity, the microphone provides signals informative enough for both perception and control. In material classification, it achieves 92.9% accuracy on a 10-class benchmark across four interaction types (tap, knock, slow press, drag). For manipulation, integrating pin microphone into an IL pipeline with open source hardware improves the success rate on picking and pouring task from 0.40 to 0.80 and enables reliable execution of contact-rich skills such as unplugging and sound-based sorting. Compared with high-resolution tactile sensors, pin microphones trade spatial detail for cost and ease of integration, offering a practical pathway for deploying acoustic contact sensing in low-cost robot setups.", "AI": {"tldr": "该研究提出了一种名为MicCheck的即插即用声学传感方法，通过改装低成本蓝牙领夹式麦克风作为接触传感器，显著提升了机器人模仿学习在接触密集型操作任务中的表现，并在材料分类和复杂技能执行上取得了良好效果。", "motivation": "机器人操作任务通常涉及丰富的接触，但大多数模仿学习（IL）方法主要依赖视觉，难以捕捉刚度、粗糙度、滑动等精细的交互线索。现有触觉传感器虽然能弥补这一不足，但通常昂贵、脆弱或集成复杂。", "method": "该研究引入了MicCheck系统，将一个市售的蓝牙领夹式麦克风改装成低成本接触传感器。麦克风通过3D打印的夹持器插件固定，并通过标准USB接收器传输音频，无需定制电子元件或驱动。该方法将麦克风信号集成到模仿学习管道中。", "result": "MicCheck在材料分类任务中，对10种材料和四种交互类型（敲击、敲打、慢压、拖拽）达到了92.9%的准确率。在操作任务中，将其集成到模仿学习管道后，拾取和倾倒任务的成功率从0.40提高到0.80，并能可靠执行拔插和基于声音的分类等接触密集型技能。", "conclusion": "与高分辨率触觉传感器相比，领夹式麦克风牺牲了空间细节，但提供了低成本和易于集成的优势，为在低成本机器人设置中部署声学接触传感提供了一条实用的途径。"}}
{"id": "2511.17743", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.17743", "abs": "https://arxiv.org/abs/2511.17743", "authors": ["Haytham Younus", "Sohag Kabir", "Felician Campean", "Pascal Bonnaud", "David Delaux"], "title": "AI- and Ontology-Based Enhancements to FMEA for Advanced Systems Engineering: Current Developments and Future Directions", "comment": "This manuscript is based on research undertaken by our doctoral student at the University of Bradford. The associated PhD thesis has been formally submitted to the University and is currently awaiting final examination. The review article is being shared on arXiv to make the review accessible to the research community while the thesis examination process is ongoing", "summary": "This article presents a state-of-the-art review of recent advances aimed at transforming traditional Failure Mode and Effects Analysis (FMEA) into a more intelligent, data-driven, and semantically enriched process. As engineered systems grow in complexity, conventional FMEA methods, largely manual, document-centric, and expert-dependent, have become increasingly inadequate for addressing the demands of modern systems engineering. We examine how techniques from Artificial Intelligence (AI), including machine learning and natural language processing, can transform FMEA into a more dynamic, data-driven, intelligent, and model-integrated process by automating failure prediction, prioritisation, and knowledge extraction from operational data. In parallel, we explore the role of ontologies in formalising system knowledge, supporting semantic reasoning, improving traceability, and enabling cross-domain interoperability. The review also synthesises emerging hybrid approaches, such as ontology-informed learning and large language model integration, which further enhance explainability and automation. These developments are discussed within the broader context of Model-Based Systems Engineering (MBSE) and function modelling, showing how AI and ontologies can support more adaptive and resilient FMEA workflows. We critically analyse a range of tools, case studies, and integration strategies, while identifying key challenges related to data quality, explainability, standardisation, and interdisciplinary adoption. By leveraging AI, systems engineering, and knowledge representation using ontologies, this review offers a structured roadmap for embedding FMEA within intelligent, knowledge-rich engineering environments.", "AI": {"tldr": "本文综述了利用人工智能（AI）和本体（Ontologies）将传统失效模式与影响分析（FMEA）转型为更智能、数据驱动和语义丰富的过程的最新进展。", "motivation": "随着工程系统日益复杂，传统FMEA方法（手动、文档中心、依赖专家）已无法满足现代系统工程的需求，促使研究人员寻求更智能、数据驱动的解决方案。", "method": "本文审查了AI技术（如机器学习和自然语言处理）在自动化失效预测、优先级排序和知识提取方面的应用，以及本体论在形式化系统知识、语义推理和互操作性中的作用。同时，也探讨了本体论与学习、大型语言模型集成的混合方法，并将其置于基于模型的系统工程（MBSE）和功能建模的背景下进行讨论。", "result": "AI技术能够自动化失效预测、优先级排序和知识提取，使FMEA更加动态和数据驱动。本体论可以形式化系统知识，支持语义推理，提高可追溯性和跨域互操作性。新兴的混合方法进一步增强了可解释性和自动化。这些进展为FMEA提供了更具适应性和弹性的工作流程，并识别了数据质量、可解释性、标准化和跨学科采纳等关键挑战。", "conclusion": "通过整合AI、系统工程和本体论的知识表示，本文为将FMEA嵌入智能、知识丰富的工程环境提供了一个结构化的路线图，以应对现代复杂系统的挑战。"}}
{"id": "2511.18335", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18335", "abs": "https://arxiv.org/abs/2511.18335", "authors": ["James Y. Huang", "Wenxuan Zhou", "Nan Xu", "Fei Wang", "Qin Liu", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas", "comment": null, "summary": "The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.", "AI": {"tldr": "本文介绍了OmniStruct，一个评估大型语言模型(LLMs)在文本到结构任务（如信息提取、表格生成和函数调用）能力的综合基准。研究表明，通过合成数据微调的小型模型，其性能可以与GPT-4o匹敌。", "motivation": "大型语言模型生成遵循任意模式的结构化输出的能力对于信息提取、表格生成和函数调用等广泛下游任务至关重要。然而，目前尚不清楚LLMs在生成非结构化自然语言方面的显著进步是否能转化为文本到结构任务的强大性能。", "method": "1. 引入OmniStruct，一个用于评估LLMs在信息提取、表格生成和函数调用等多样化文本到结构任务能力的综合基准。2. 通过识别并改编现有数据集，将其统一到文本到结构问题设置下构建OmniStruct。3. 通过合成任务生成收集高质量训练数据，以促进高效文本到结构模型开发。4. 在不使用OmniStruct任务任何监督数据的情况下，用合成数据微调更小的模型。", "result": "实验证明，通过合成数据微调的小型模型可以成为通用的结构化生成模型，其性能能够与GPT-4o相媲美，而无需使用OmniStruct任务的任何监督数据。", "conclusion": "可以通过合成数据微调较小的模型，使其成为通用的结构化生成模型，并在文本到结构任务上达到与顶尖大型模型（如GPT-4o）相当的性能，从而为开发高效的文本到结构模型提供了可能性。"}}
{"id": "2511.17755", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17755", "abs": "https://arxiv.org/abs/2511.17755", "authors": ["Prantik Howlader", "Hoang Nguyen-Canh", "Srijan Das", "Jingyi Xu", "Hieu Le", "Dimitris Samaras"], "title": "CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation", "comment": "WACV 2026 accepted", "summary": "Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\\%$. Similarly, CORA improves performance by $+2.4\\%$ with only 180 labeled images on PanNuke, a histopathology dataset.", "AI": {"tldr": "本文提出CORA，一个半监督推理分割框架，通过条件视觉指令、噪声伪标签过滤器和token级对比对齐，解决了高昂标注成本问题，实现了在有限标注数据下鲁棒且高性能的推理分割。", "motivation": "推理分割需要像素级准确的掩码，且依赖复杂的上下文推理。尽管多模态语言模型在此领域有所进展，但其泛化能力受限于高昂的、多样化且高质量的像素级标注成本，导致在分布偏移下性能脆弱。", "method": "CORA是一个半监督推理分割框架，联合学习有限的标注数据和大量的未标注图像。它包含三个主要组件：1) 条件视觉指令，用于编码对象间的空间和上下文关系；2) 基于多模态LLM在语义等效查询中输出一致性的噪声伪标签过滤器；3) 标注样本和伪标注样本之间的token级对比对齐，以增强特征一致性。", "result": "CORA在受限标注设置下表现优于现有基线。在Cityscapes数据集上，仅需100张标注图像即可达到SOTA结果，超越基线2.3%；在PanNuke数据集上，仅需180张标注图像即可提升性能2.4%。", "conclusion": "CORA框架通过其创新的组件，能够在极少监督的情况下执行鲁棒的推理分割，显著优于现有方法，并达到了最先进的性能。"}}
{"id": "2511.19421", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19421", "abs": "https://arxiv.org/abs/2511.19421", "authors": ["Amy K. Strong", "Ali Kashani", "Claus Danielson", "Leila Bridgeman"], "title": "Data driven synthesis of provable invariant sets via stochastically sampled data", "comment": null, "summary": "Positive invariant (PI) sets are essential for ensuring safety, i.e. constraint adherence, of dynamical systems. With the increasing availability of sampled data from complex (and often unmodeled) systems, it is advantageous to leverage these data sets for PI set synthesis. This paper uses data driven geometric conditions of invariance to synthesize PI sets from data. Where previous data driven, set-based approaches to PI set synthesis used deterministic sampling schemes, this work instead synthesizes PI sets from any pre-collected data sets. Beyond a data set and Lipschitz continuity, no additional information about the system is needed. A tree data structure is used to partition the space and select samples used to construct the PI set, while Lipschitz continuity is used to provide deterministic guarantees of invariance. Finally, probabilistic bounds are given on the number of samples needed for the algorithm to determine of a certain volume.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18353", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18353", "abs": "https://arxiv.org/abs/2511.18353", "authors": ["Sigrid Helene Strand", "Thomas Wiedemann", "Bram Burczek", "Dmitriy Shutin"], "title": "Enhancing UAV Search under Occlusion using Next Best View Planning", "comment": "Submitted to IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "summary": "Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.", "AI": {"tldr": "本文提出了一种针对遮挡环境（如茂密森林）中无人机搜索的优化规划策略和高效的“下一个最佳视角”（NBV）算法，引入了几何启发式和可见性启发式。实验证明，可见性启发式在目标检测和覆盖率方面表现更优。", "motivation": "自然灾害或高风险环境后的搜救任务至关重要，但茂密森林等难以进入的遮挡地形使搜救极具挑战性。虽然无人机能提高效率，但在密林中，其有效性取决于能否捕获清晰的地面视图，因此需要一个强大的搜索策略来优化摄像机定位和视角。", "method": "本文提出了一种优化的规划策略和高效的“下一个最佳视角”算法，以解决遮挡环境中的问题。为此，提出了两种新颖的优化启发式方法：几何启发式和可见性启发式，用于选择最佳摄像机视角以增强搜索性能。研究在模拟和真实世界环境中进行了比较评估。", "result": "比较评估结果显示，可见性启发式表现更出色。在模拟森林中，它识别出超过90%的隐藏物体，并且比几何启发式提供了10%更好的检测率。此外，真实世界实验表明，可见性启发式在林冠下提供了更好的覆盖范围。", "conclusion": "可见性启发式显著提升了在茂密森林等遮挡环境中无人机搜救任务的性能，通过优化摄像机视角和提供更好的覆盖，具有改善搜救任务的巨大潜力。"}}
{"id": "2511.18486", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18486", "abs": "https://arxiv.org/abs/2511.18486", "authors": ["Jasan Zughaibi", "Denis von Arx", "Maurus Derungs", "Florian Heemeyer", "Luca A. Antonelli", "Quentin Boehler", "Michael Muehlebach", "Bradley J. Nelson"], "title": "Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control", "comment": null, "summary": "Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.", "AI": {"tldr": "本文提出通过系统级控制设计显著扩展电磁导航系统（eMNS）的工作空间，通过降低所需电流实现高效、可扩展的磁操纵。", "motivation": "磁操纵手术工具面临的挑战是，电磁导航系统（eMNS）的有效工作空间通常受到功率和热限制的严重制约。", "method": "研究确定了五种关键系统方法来扩展工作空间：以运动为中心的扭矩/力目标、能量最优电流分配、实时姿态估计、动态反馈和高带宽eMNS组件。通过将以磁场为中心的磁场对准策略替换为以运动为中心的基于扭矩/力的方法，在一个八线圈OctoMag eMNS上稳定了3D倒立摆，并推广到多智能体控制。通过结构化分析比较了不同范式的工作空间，并在临床导向的Navion eMNS上进行了跨平台评估。", "result": "在一个八线圈OctoMag eMNS上以显著降低的电流（0.1-0.2 A 对比 8-14 A）稳定了3D倒立摆。在共享工作空间内同时稳定了两个倒立摆。在Navion eMNS上实现了显著的工作空间扩展，在距离线圈50厘米处仍能保持稳定平衡。", "conclusion": "研究结果表明，反馈是实现可扩展、高效和临床相关的磁操纵的实用途径。"}}
{"id": "2511.18322", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18322", "abs": "https://arxiv.org/abs/2511.18322", "authors": ["Henrik Krauss", "Johann Licher", "Naoya Takeishi", "Annika Raatz", "Takehisa Yairi"], "title": "Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video", "comment": null, "summary": "Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18374", "categories": ["cs.RO", "eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.18374", "abs": "https://arxiv.org/abs/2511.18374", "authors": ["Jiaxun Sun"], "title": "Explicit Bounds on the Hausdorff Distance for Truncated mRPI Sets via Norm-Dependent Contraction Rates", "comment": null, "summary": "This paper establishes the first explicit and closed-form upper bound on the Hausdorff distance between the truncated minimal robust positively invariant (mRPI) set and its infinite-horizon limit. While existing mRPI approximations guarantee asymptotic convergence through geometric or norm-based arguments, none provides a computable expression that quantifies the truncation error for a given horizon. We show that the error satisfies \\( d_H(\\mathcal{E}_N,\\mathcal{E}_\\infty) \\le r_W\\,γ^{N+1}/(1-γ), \\) where $γ<1$ is the induced-norm contraction factor and $r_W$ depends only on the disturbance set. The bound is fully analytic, requires no iterative set computations, and directly characterizes the decay rate of the truncated Minkowski series. We further demonstrate that the choice of vector norm serves as a design parameter that accelerates convergence, enabling substantially tighter horizon selection for robust invariant-set computations and tube-based MPC. Numerical experiments validate the sharpness, scalability, and practical relevance of the proposed bound.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2511.17750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17750", "abs": "https://arxiv.org/abs/2511.17750", "authors": ["Zhimin Shao", "Abhay Yadav", "Rama Chellappa", "Cheng Peng"], "title": "SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration", "comment": null, "summary": "Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.", "AI": {"tldr": "图像匹配是视觉空间感知的基石，但跨域匹配因外观、尺度和视角变化而充满挑战。现有3D基础模型在精细几何细节上表现不足。本文提出SPIDER，一个结合2D和3D匹配的通用框架，并在新基准上显著超越现有最佳方法。", "motivation": "可靠的图像对应是视觉空间感知（如3D结构恢复和相机姿态估计）的基础。然而，在航空、室内和室外等不同场景下，由于外观、尺度和视角的巨大变化，无约束的特征匹配仍然极具挑战。传统的特征匹配多为2D-to-2D问题，而近期3D基础模型虽能提供基于双视图几何的空间特征匹配特性，但在大视角变化下，这些匹配往往集中于主导平面区域（如墙壁或地面），对精细几何细节的敏感度不足。研究旨在更好地理解这些权衡，并开发更通用的匹配方法。", "method": "首先，通过线性探测实验评估了各种视觉基础模型在图像匹配上的性能，以理解其权衡。在此基础上，引入了SPIDER，一个通用的特征匹配框架。SPIDER整合了一个共享的特征提取骨干网络，并配备了两个专门的网络头部，用于从粗到精地估计基于2D和基于3D的对应关系。最后，引入了一个新的图像匹配评估基准，专注于具有大基线的无约束场景。", "result": "SPIDER在所提出的新基准上显著优于现有最佳方法（SoTA）。实验结果证明了SPIDER作为一种通用图像匹配方法的强大能力。", "conclusion": "SPIDER是一个通用且高效的特征匹配框架，通过融合2D和3D匹配策略，有效解决了现有方法在处理跨域、大视角变化和精细几何细节方面的挑战。其在新基准上的卓越表现，验证了其作为通用图像匹配方法的强大潜力。"}}
{"id": "2511.18319", "categories": ["cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18319", "abs": "https://arxiv.org/abs/2511.18319", "authors": ["Xian Yeow Lee", "Lasitha Vidyaratne", "Gregory Sin", "Ahmed Farahat", "Chetan Gupta"], "title": "Weakly-supervised Latent Models for Task-specific Visual-Language Control", "comment": null, "summary": "Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18525", "abs": "https://arxiv.org/abs/2511.18525", "authors": ["Samarth Chopra", "Jing Liang", "Gershom Seneviratne", "Yonghan Lee", "Jaehoon Choi", "Jianyu An", "Stephen Cheng", "Dinesh Manocha"], "title": "Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation", "comment": "Submitted to ICRA 2026", "summary": "We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io", "AI": {"tldr": "Splatblox是一个实时户外导航系统，它利用高斯泼溅融合RGB图像和激光雷达点云，构建了一个可遍历性感知的ESDF，实现了在复杂地形和茂密植被环境中的高效导航。", "motivation": "现有户外导航系统在处理茂密植被、不规则障碍物和复杂地形时面临挑战，尤其难以区分可通行的植被（如高草）和刚性障碍物（如树木）。", "method": "该方法通过高斯泼溅（Gaussian Splatting）技术融合分割后的RGB图像和激光雷达点云，构建了一个同时编码几何和语义信息的欧几里得有符号距离场（ESDF）。该ESDF在线更新，支持语义推理，以区分可通行植被和刚性障碍物。激光雷达确保了360度几何覆盖，支持扩展规划视野。", "result": "Splatblox在四足机器人和轮式平台上进行了验证。在植被丰富的场景中，与现有先进方法相比，成功率提高了50%以上，卡顿事件减少了40%，路径缩短了5%，到达目标时间加快了13%。支持长达100米的长距离任务。", "conclusion": "Splatblox是一个在户外复杂环境（特别是茂密植被和不规则障碍物）中实现自主导航的有效实时系统，显著提高了导航的成功率、效率和鲁棒性。"}}
{"id": "2511.18369", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.18369", "abs": "https://arxiv.org/abs/2511.18369", "authors": ["Manon Berriche"], "title": "Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle", "comment": "in French language", "summary": "This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence énonciative) or interventions ('points d'arrêt') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17757", "abs": "https://arxiv.org/abs/2511.17757", "authors": ["Giancarlo Giannetti", "Faisal Z. Qureshi"], "title": "Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers", "comment": null, "summary": "Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17793", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17793", "abs": "https://arxiv.org/abs/2511.17793", "authors": ["Shweta Mahajan", "Hoang Le", "Hyojin Park", "Farzad Farhadzadeh", "Munawar Hayat", "Fatih Porikli"], "title": "Attention Guided Alignment in Efficient Vision-Language Models", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Efficient Reasoning", "summary": "Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability \"look\" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.", "AI": {"tldr": "本文分析了高效视觉-语言模型（VLMs）中的注意力模式，发现拼接式架构常难以区分语义匹配的图像-文本对，导致物体幻觉。为此，提出了AGE-VLM框架，通过交错的交叉注意力层和从SAM蒸馏的空间知识，增强视觉定位能力，显著减少幻觉。", "motivation": "现有的高效VLMs，特别是基于拼接的架构，在区分语义匹配与不匹配的图像-文本对时表现不佳，这是导致模型产生物体幻觉的关键因素。", "method": "引入了注意力引导高效视觉-语言模型（AGE-VLM）框架。该方法通过交错的交叉注意力层，在预训练的小型语言模型中注入视觉能力，并利用从Segment Anything Model (SAM) 中蒸馏的空间知识来引导模型“看向”正确的图像区域，以增强视觉定位。", "result": "AGE-VLM在不同的以视觉为中心的基准测试中，显著减少了幻觉现象，并且性能优于或与现有高效VLMs相当。", "conclusion": "研究结果为未来旨在增强VLM视觉和语言理解的研究提供了宝贵的见解，特别是在解决视觉定位和幻觉问题方面。"}}
{"id": "2511.19204", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19204", "abs": "https://arxiv.org/abs/2511.19204", "authors": ["Fabian Schramm", "Pierre Fabre", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "title": "Reference-Free Sampling-Based Model Predictive Control", "comment": null, "summary": "We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.", "AI": {"tldr": "本文提出了一种基于采样的模型预测控制（MPC）框架，通过双空间样条参数化，在不依赖预设步态或接触序列的情况下，实现了机器人多种新兴运动模式（如小跑、跳跃、倒立平衡），并能在标准CPU上实时运行。", "motivation": "传统机器人运动控制方法通常依赖手工设计的步态模式或预定义的接触序列，这限制了其适应性和多样性，且往往需要GPU加速或大量离线预训练。", "method": "该方法基于模型预测路径积分（MPPI），提出了一种作用于位置和速度控制点的双空间样条参数化。它通过优化高层目标来发现运动模式，并能自动适应接触建立和解除策略，仅需少量采样轨迹即可实现高效控制。", "result": "该方法能够自主发现多种运动模式，包括小跑、疾驰、稳定的站立策略、跳跃和倒立平衡。它在标准CPU硬件上实现了实时控制，无需GPU加速。在Go2四足机器人上验证了新兴步态和基本跳跃能力，并在模拟中展示了更复杂的行为，如后空翻、动态倒立平衡和类人机器人运动，所有这些都无需参考跟踪或离线预训练。", "conclusion": "所提出的基于采样的MPC框架，结合双空间样条参数化，能够使机器人自主学习和执行多样化且复杂的运动行为，且具有高样本效率和实时性，可在普通CPU上运行，显著降低了对预设知识和计算资源的需求。"}}
{"id": "2511.17766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17766", "abs": "https://arxiv.org/abs/2511.17766", "authors": ["Mansur Yerzhanuly"], "title": "Deepfake Geography: Detecting AI-Generated Satellite Images", "comment": "18 pages, 8 figures", "summary": "The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18604", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.18604", "abs": "https://arxiv.org/abs/2511.18604", "authors": ["Hannah Lee", "James D. Motes", "Marco Morales", "Nancy M. Amato"], "title": "An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms", "comment": null, "summary": "This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis", "AI": {"tldr": "本研究通过对约束进行分类，为基于约束的多智能体路径规划（MAPF）和多机器人运动规划（MRMP）算法设计提供指导，并提供选择建议。", "motivation": "旨在通过分析不同约束类型对基于约束的搜索算法的影响，为未来的MAPF和MRMP算法设计提供决策依据，以优化算法选择。", "method": "将约束分为保守型和激进型，并使用香草冲突优先搜索（CBS）和带优先级的冲突优先搜索（CBSw/P）在混合网格-路线图表示下进行实验，观察其搜索行为和性能。", "result": "激进型（优先级约束）方法在智能体数量或分辨率增加时能解决更多实例；保守型（运动约束）方法在两者都成功时能产生更好的解决方案质量。研究结果被整合为决策流程图，并扩展至MRMP，强调考虑拓扑特征的重要性。", "conclusion": "本研究为MAPF和MRMP中约束的选择提供了实用指导，揭示了不同约束类型在解决能力和解决方案质量之间的权衡，并强调了在规划中考虑问题、解决方案、表示和拓扑特征的重要性。"}}
{"id": "2511.18393", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18393", "abs": "https://arxiv.org/abs/2511.18393", "authors": ["Heejoon Koo"], "title": "Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models", "comment": "Accepted by the Association for the Advancement of Artificial Intelligence (AAAI) 2026 1st Workshop on Safe, Ethical, Certified, Uncertainty-aware, Robust, and Explainable AI for Health (SECURE-AI4H)", "summary": "A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18509", "abs": "https://arxiv.org/abs/2511.18509", "authors": ["Ziyu Meng", "Tengyu Liu", "Le Ma", "Yingying Wu", "Ran Song", "Wei Zhang", "Siyuan Huang"], "title": "SafeFall: Learning Protective Control for Humanoid Robots", "comment": null, "summary": "Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \\method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\\%, peak joint torques by 78.4\\%, and eliminated 99.3\\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18413", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18413", "abs": "https://arxiv.org/abs/2511.18413", "authors": ["Yu Xia", "Sungchul Kim", "Tong Yu", "Ryan A. Rossi", "Julian McAuely"], "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations", "comment": null, "summary": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.", "AI": {"tldr": "现有智能体推荐系统未能充分利用协同信号，导致推荐效果不佳。本文提出多智能体协同过滤（MACF）框架，将用户和物品实例化为LLM智能体，并通过中心协调器动态管理协作，有效提升了推荐性能。", "motivation": "大多数现有智能体推荐系统侧重于通用单智能体或多智能体任务分解流程，缺乏推荐导向的设计，未能充分利用用户-物品交互历史中的协同信号，导致推荐结果不尽如人意。", "method": "提出多智能体协同过滤（MACF）框架。将传统协同过滤算法与基于LLM的多智能体协作进行类比。给定目标用户和查询，将相似用户和相关物品实例化为具有独特配置文件的LLM智能体。每个智能体能够调用检索工具、推荐候选物品并与其他智能体交互。MACF采用一个中心协调器智能体，通过动态智能体招募和个性化协作指令，自适应地管理用户智能体和物品智能体之间的协作。", "result": "在来自三个不同领域的数据集上的实验结果表明，与强大的智能体推荐基线相比，MACF框架具有显著优势。", "conclusion": "MACF框架通过将用户和物品建模为LLM智能体并引入中心协调器进行动态协作管理，成功地将协同过滤原理融入智能体推荐中，有效解决了现有智能体推荐系统未能充分利用协同信号的问题，并显著提升了推荐效果。"}}
{"id": "2511.18411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18411", "abs": "https://arxiv.org/abs/2511.18411", "authors": ["Sultan Alrashed", "Chadi Helwe", "Francesco Orabona"], "title": "SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data", "comment": "Work in progress", "summary": "Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.", "AI": {"tldr": "本文介绍了SmolKalam，一个通过多模型集成翻译管道和质量过滤从Smoltalk2翻译而来的高质量、大规模、多轮阿拉伯语数据集，用于推理和工具调用，并探讨了有效的翻译技术。", "motivation": "尽管阿拉伯语预训练数据质量有所提高，但仍缺乏包含推理和工具调用的大规模、多轮阿拉伯语数据集。对于后训练，需要更高质量的数据集，而简单的翻译方法无法满足要求。", "method": "本文通过以下方法创建了SmolKalam数据集：1) 将Smoltalk2翻译成阿拉伯语。2) 采用多模型集成翻译管道。3) 应用质量过滤机制。4) 通过消融实验研究了对传统仅解码器模型有效的翻译技术。", "result": "研究成果是引入了SmolKalam，一个高质量的阿拉伯语多轮推理和工具调用数据集。同时，通过消融实验，探索并检验了对传统仅解码器模型有效的翻译技术。", "conclusion": "本文成功创建了一个急需的高质量、大规模阿拉伯语数据集SmolKalam，该数据集支持多轮推理和工具调用，并通过实践验证了有效的翻译策略，为阿拉伯语后训练数据匮乏问题提供了解决方案。"}}
{"id": "2511.18409", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18409", "abs": "https://arxiv.org/abs/2511.18409", "authors": ["Dana Arad", "Yonatan Belinkov", "Hanjie Chen", "Najoung Kim", "Hosein Mohebbi", "Aaron Mueller", "Gabriele Sarti", "Martin Tutek"], "title": "Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models", "comment": null, "summary": "Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17806", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.17806", "abs": "https://arxiv.org/abs/2511.17806", "authors": ["Ryoma Yataka", "Pu Perry Wang", "Petros Boufounos", "Ryuhei Takahashi"], "title": "REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion", "comment": "26 pages, Accepted to AAAI 2026; Code to be released", "summary": "Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \\textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.", "AI": {"tldr": "REXO提出了一种基于3D边界框扩散的多视图雷达目标检测方法，通过显式跨视图特征关联解决了现有方法的局限性，并在两个室内雷达数据集上显著超越了现有技术。", "motivation": "现有方法依赖于隐式跨视图雷达特征关联（如提议配对或查询到特征的交叉注意力），这在复杂的室内场景中会导致模糊的特征匹配和检测性能下降。", "method": "REXO（multi-view Radar object dEtection with 3D bounding boX diffusiOn）将DiffusionDet的2D边界框扩散过程提升到3D雷达空间。它利用噪声3D边界框来指导显式的跨视图雷达特征关联，增强了跨视图雷达条件去噪过程。通过考虑人物与地面接触的先验知识，REXO减少了扩散参数的数量。", "result": "在两个开放室内雷达数据集上进行评估，REXO在HIBER数据集上超越了现有最先进方法+4.22 AP，在MMVR数据集上超越了+11.02 AP。", "conclusion": "REXO通过将2D边界框扩散过程提升到3D雷达空间，并利用噪声3D边界框进行显式跨视图特征关联，有效解决了现有方法的局限性，显著提高了多视图室内雷达感知性能。"}}
{"id": "2511.17792", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17792", "abs": "https://arxiv.org/abs/2511.17792", "authors": ["Dingrui Wang", "Hongyuan Ye", "Zhihao Liang", "Zhexiao Sun", "Zhaowei Lu", "Yuchen Zhang", "Yuyu Zhao", "Yuan Gao", "Marvin Seegert", "Finn Schäfer", "Haotong Qin", "Wei Li", "Luigi Palmieri", "Felix Jahncke", "Mattia Piccinini", "Johannes Betz"], "title": "Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?", "comment": "10 pages", "summary": "While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18563", "abs": "https://arxiv.org/abs/2511.18563", "authors": ["Cem Bilaloglu", "Tobias Löw", "Sylvain Calinon"], "title": "Object-centric Task Representation and Transfer using Diffused Orientation Fields", "comment": null, "summary": "Curved objects pose a fundamental challenge for skill transfer in robotics: unlike planar surfaces, they do not admit a global reference frame. As a result, task-relevant directions such as \"toward\" or \"along\" the surface vary with position and geometry, making object-centric tasks difficult to transfer across shapes. To address this, we introduce an approach using Diffused Orientation Fields (DOF), a smooth representation of local reference frames, for transfer learning of tasks across curved objects. By expressing manipulation tasks in these smoothly varying local frames, we reduce the problem of transferring tasks across curved objects to establishing sparse keypoint correspondences. DOF is computed online from raw point cloud data using diffusion processes governed by partial differential equations, conditioned on keypoints. We evaluate DOF under geometric, topological, and localization perturbations, and demonstrate successful transfer of tasks requiring continuous physical interaction such as inspection, slicing, and peeling across varied objects. We provide our open-source codes at our website https://github.com/idiap/diffused_fields_robotics", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18683", "abs": "https://arxiv.org/abs/2511.18683", "authors": ["Yinan Dong", "Ziyu Xu", "Tsimafei Lazouski", "Sangli Teng", "Maani Ghaffari"], "title": "Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles", "comment": null, "summary": "Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.", "AI": {"tldr": "本文提出一种结合Lie群凸误差状态MPC和在线学习模块的高效控制器，用于自主水面车辆（ASV）在未知扰动下的轨迹跟踪，显著提高了跟踪精度。", "motivation": "自主水面车辆（ASV）易受风浪等环境扰动影响，导致在动态海洋条件下难以实现准确的轨迹跟踪。", "method": "该研究结合了Lie群上的凸误差状态模型预测控制器（MPC）与一个在线学习模块。MPC用于轨迹跟踪，而在线学习模块则实时补偿未知扰动，旨在实现自适应、鲁棒且计算高效的控制。", "result": "通过数值模拟、虚拟机器人X（VRX）模拟器和真实世界现场实验的广泛评估表明，该方法在各种扰动场景下均比现有方法取得了更卓越的跟踪精度。", "conclusion": "所提出的控制器设计能够实现自适应、鲁棒且计算高效的控制，显著提高了ASV在未知扰动下的轨迹跟踪精度。"}}
{"id": "2511.18606", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18606", "abs": "https://arxiv.org/abs/2511.18606", "authors": ["Kensuke Nakamura", "Arun L. Bishop", "Steven Man", "Aaron M. Johnson", "Zachary Manchester", "Andrea Bajcsy"], "title": "How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints", "comment": "3 figures, 10 tables, 22 pages", "summary": "Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement \"least-restrictive\" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a \"margin function\" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.", "AI": {"tldr": "现有潜空间安全滤波器在视觉运动控制中采用离散切换，损害任务性能，且其价值函数与平滑控制障碍函数不兼容。本文提出LatentCBF，通过梯度惩罚实现平滑边界函数并混合策略数据进行价值训练，实现平滑安全过滤，将任务完成率提高一倍。", "motivation": "现有的潜空间安全滤波器（基于Hamilton-Jacobi可达性）通过在名义策略和安全策略之间离散切换来实现“限制最少”的过滤，这可能会损害现代视觉运动策略的宝贵任务性能。此外，当前潜空间学习方法产生的价值函数与平滑优化控制障碍函数（CBFs）从根本上不兼容。", "method": "本文首先理论和经验上证明了现有潜空间学习方法产生的价值函数与CBFs不兼容的两个原因：1) 边界函数（用于编码失败）作为分类器表示时会饱和，导致价值函数不连续跳跃，并证明价值函数的Lipschitz常数与边界函数的Lipschitz常数线性相关，要求平滑的边界函数；2) 仅用安全策略数据训练的强化学习近似对名义策略动作的价值估计不准确。为解决这些问题，本文提出了LatentCBF：通过梯度惩罚实现平滑的边界函数，无需额外标注；并采用混合来自名义和安全策略分布数据进行价值训练的程序。", "result": "LatentCBF在模拟基准和带有视觉操纵策略的硬件实验中，实现了平滑的安全过滤，并将任务完成率比之前的切换方法提高了一倍。", "conclusion": "LatentCBF通过解决潜空间中不兼容的价值函数问题，成功地实现了平滑的安全过滤，同时显著提高了视觉运动控制系统的任务完成率，克服了现有离散切换方法的局限性。"}}
{"id": "2511.17803", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17803", "abs": "https://arxiv.org/abs/2511.17803", "authors": ["Kumar Krishna Agrawal", "Longchao Liu", "Long Lian", "Michael Nercessian", "Natalia Harguindeguy", "Yufu Wu", "Peter Mikhael", "Gigin Lin", "Lecia V. Sequist", "Florian Fintelmann", "Trevor Darrell", "Yutong Bai", "Maggie Chung", "Adam Yala"], "title": "Pillar-0: A New Frontier for Radiology Foundation Models", "comment": null, "summary": "Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18557", "abs": "https://arxiv.org/abs/2511.18557", "authors": ["Yacouba Diarra", "Nouhoum Souleymane Coulibaly", "Panga Azazia Kamaté", "Madani Amadou Tall", "Emmanuel Élisé Koné", "Aymane Dembélé", "Michael Leventhal"], "title": "Dealing with the Hard Facts of Low-Resource African NLP", "comment": "10 pages, 4 figures", "summary": "Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.", "AI": {"tldr": "本文报告了为低资源语言班巴拉语收集612小时语音数据、半自动标注、创建紧凑模型以及进行自动和人工评估的工作，并提供了实用建议。", "motivation": "为低资源语言创建语音数据集、模型和评估框架面临挑战，主要原因是缺乏相关经验和资源。", "method": "研究方法包括：实地收集612小时班巴拉语自发语音；对数据集进行半自动化转录标注；利用数据集创建多种单语超紧凑和小型模型；对模型输出进行自动和人工评估。", "result": "成功收集并标注了612小时的班巴拉语数据；创建了多个实用模型；提供了数据收集协议、标注和模型设计的实用建议；并强调了进行人工评估的重要性。此外，主要数据集、多个评估数据集、模型和代码均已公开。", "conclusion": "为低资源语言班巴拉语的语音技术发展提供了宝贵的数据、模型和实践经验。研究结果强调了在低资源语言项目中进行人工评估的必要性，并为未来的类似工作提供了实用的指导和公开资源。"}}
{"id": "2511.18694", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18694", "abs": "https://arxiv.org/abs/2511.18694", "authors": ["Shuo Wen", "Edwin Meriaux", "Mariana Sosa Guzmán", "Zhizun Wang", "Junming Shi", "Gregory Dudek"], "title": "Stable Multi-Drone GNSS Tracking System for Marine Robots", "comment": null, "summary": "Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.", "AI": {"tldr": "本文提出了一种可扩展的多无人机GNSS追踪系统，用于水面和近水面海洋机器人，通过结合视觉检测、多目标追踪、GNSS三角测量和扩展卡尔曼滤波器，实现稳定、实时的定位。", "motivation": "海洋机器人的精确 H定位至关重要，但全球导航卫星系统（GNSS）信号在水下不可靠或不可用。传统替代方案（如惯性导航、DVL、SLAM和声学方法）存在误差累积、计算需求高或依赖基础设施的问题。", "method": "该方法结合了高效视觉检测、轻量级多目标追踪、基于GNSS的三角测量以及置信度加权的扩展卡尔曼滤波器（EKF），以提供稳定的GNSS估计。此外，还引入了跨无人机追踪ID对齐算法，以实现视图间的全局一致性。", "result": "该系统能够提供稳定的实时GNSS估计，并实现具有冗余空中覆盖的鲁棒多机器人追踪。在多样化的复杂环境中验证了所提算法的可扩展性和鲁棒性。", "conclusion": "本文成功开发并验证了一个可扩展且鲁棒的多无人机GNSS追踪系统，为水面和近水面海洋机器人提供了可靠的定位解决方案。"}}
{"id": "2511.18423", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18423", "abs": "https://arxiv.org/abs/2511.18423", "authors": ["B. Y. Yan", "Chaofan Li", "Hongjin Qian", "Shuqi Lu", "Zheng Liu"], "title": "General Agentic Memory Via Deep Research", "comment": null, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17805", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17805", "abs": "https://arxiv.org/abs/2511.17805", "authors": ["Chengan Che", "Chao Wang", "Xinyue Chen", "Sophia Tsoka", "Luis C. Garcia-Peraza-Herrera"], "title": "A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking", "comment": "18 pages", "summary": "Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18617", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18617", "abs": "https://arxiv.org/abs/2511.18617", "authors": ["Litian Gong", "Fatemeh Bahrani", "Yutai Zhou", "Amin Banayeeanzade", "Jiachen Li", "Erdem Biyik"], "title": "AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations", "comment": "8 pages, 6 figures. Code and datasets available at http://autofocus-il.github.io/", "summary": "AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18491", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18491", "abs": "https://arxiv.org/abs/2511.18491", "authors": ["José Pombal", "Maya D'Eon", "Nuno M. Guerreiro", "Pedro Henrique Martins", "António Farinhas", "Ricardo Rei"], "title": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support", "comment": null, "summary": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18499", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18499", "abs": "https://arxiv.org/abs/2511.18499", "authors": ["Tyler Shoemaker"], "title": "For Those Who May Find Themselves on the Red Team", "comment": null, "summary": "This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.", "AI": {"tldr": "本文认为文学学者必须参与大型语言模型（LLM）的可解释性研究，以超越当前工具性解释标准，并建议将“红队”作为这种参与的潜在场域。", "motivation": "当前对LLM解释性的方法过于工具化，未能充分衡量LLM的解释能力。文学学者需要参与进来，拓宽解释的衡量标准。", "method": "本文是一篇立场论文，主要通过论证和提出建议来表达观点。它建议通过“红队”等方式，让文学学者参与到LLM可解释性研究中。", "result": "本文的主要成果是提出了一个论点：文学学者必须与LLM可解释性研究互动，即使这涉及意识形态斗争。它还建议“红队”是实现这种互动的可能途径。", "conclusion": "文学学者有必要且必须参与LLM的可解释性研究，以挑战并扩展现有的解释标准，而“红队”是一个可行的参与点。"}}
{"id": "2511.17812", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17812", "abs": "https://arxiv.org/abs/2511.17812", "authors": ["Xinshuang Liu", "Runfa Blark Li", "Shaoxiu Wei", "Truong Nguyen"], "title": "Importance-Weighted Non-IID Sampling for Flow Matching Models", "comment": null, "summary": "Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.", "AI": {"tldr": "本文提出一种重要性加权的非独立同分布（non-IID）采样框架，用于解决流匹配模型在有限采样预算下估计期望值时面临的挑战。该框架结合了基于分数的正则化和学习残差速度场的方法，以生成多样化、高质量的样本，并实现无偏估计和准确的期望值计算。", "motivation": "流匹配模型在表示复杂分布方面表现出色，但在有限采样预算下，估计其输出函数的期望值仍然具有挑战性。独立采样常常导致高方差估计，特别是在稀有但高影响的结果主导期望值时。", "method": "本文提出一个重要性加权的非独立同分布采样框架，通过联合抽取多个样本来覆盖流分布中多样化、显著的区域，并通过估计的重要性权重保持无偏估计。为平衡多样性和质量，引入了基于分数的正则化机制，利用对数概率的梯度（即分数函数）在数据流形的高密度区域内将样本推开，从而减轻脱离流形的漂移。此外，还开发了首个针对非独立同分布流样本的重要性加权方法，通过学习一个残差速度场来重现非独立同分布样本的边际分布。", "result": "实验结果表明，该方法能够生成多样化、高质量的样本，并准确估计重要性权重和期望值。", "conclusion": "本文提出的方法通过提高样本多样性、质量和估计准确性，推动了流匹配模型输出的可靠表征。代码将公开提供。"}}
{"id": "2511.17839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17839", "abs": "https://arxiv.org/abs/2511.17839", "authors": ["Yujiang Pu", "Zhanbo Huang", "Vishnu Boddeti", "Yu Kong"], "title": "Show Me: Unifying Instructional Image and Video Generation with Diffusion Models", "comment": "Accepted by WACV 2026", "summary": "Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.", "AI": {"tldr": "ShowMe是一个统一框架，通过选择性激活视频扩散模型的空间和时间组件，同时解决了视觉指令下的图像操作和视频预测问题，并引入了一致性奖励，在两项任务上均超越了现有专家模型。", "motivation": "现有研究将文本引导的图像操作和视频预测视为孤立任务。图像操作忽略了动作的时间演变，而视频预测模型则常忽视预期结果。这种分离促使研究者寻求一个能统一处理这两项任务的框架。", "method": "本文提出了ShowMe，一个统一框架，通过选择性激活视频扩散模型的空间和时间组件，同时支持图像操作和视频预测。此外，引入了结构和运动一致性奖励，以提高结构保真度和时间连贯性。", "result": "实验结果表明，ShowMe在指令性图像生成和视频生成两项任务上均优于现有专家模型。这种统一性带来了双重好处：视频预训练获得的空间知识增强了非刚性图像编辑的上下文一致性和真实感，而指令引导的操作阶段则赋予模型更强的目标导向推理能力，从而提升了视频预测性能。", "conclusion": "视频扩散模型可以作为一种强大的统一动作-对象状态转换器，有效处理视觉指令下的图像和视频生成任务。"}}
{"id": "2511.17828", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17828", "abs": "https://arxiv.org/abs/2511.17828", "authors": ["Guilherme J. Cavalcante", "José Gabriel A. Moreira", "Gabriel A. B. do Nascimento", "Vincent Dong", "Alex Nguyen", "Thaís G. do Rêgo", "Yuri Malheiros", "Telmo M. Silva Filho", "Carla R. Zeballos Torrez", "James C. Gee", "Anne Marie McCarthy", "Andrew D. A. Maidment", "Bruno Barufaldi"], "title": "Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations", "comment": "5 pages, 3 figures", "summary": "Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.", "AI": {"tldr": "本研究利用BiomedCLIP基础模型，通过多模态乳腺影像数据和加权对比学习，成功实现了BI-RADS乳腺密度分类，展示了其在乳腺影像应用中的强大泛化能力和潜力。", "motivation": "尽管基础模型在专业医学影像任务中具有前景，但其在乳腺影像领域的有效性尚未得到充分探索。本研究旨在利用基础模型解决模型泛化性挑战，并应用于乳腺BI-RADS密度分类。", "method": "研究将BiomedCLIP作为基础模型，针对BI-RADS乳腺密度分类任务进行适配。使用了多模态乳腺影像数据（合成2D图像、数字乳腺摄影和数字乳腺断层合成），并采用加权对比学习来解决类别不平衡问题。比较了单模态（仅s2D）和多模态训练方法，并使用GradCAM进行可视化解释。", "result": "单模态和多模态训练方法取得了相似的准确率（多模态：0.74，单模态：0.73）。多模态模型在不同影像模态上具有更广泛的适用性，并且在所有BI-RADS类别中AUC值始终高于0.84。在RSNA和EMBED数据集上的外部验证显示出强大的泛化能力（AUC范围：0.80-0.93）。GradCAM可视化证实了模型关注模式的一致性和临床相关性。", "conclusion": "本研究强调了基础模型在乳腺影像应用中的潜力，并为未来扩展到诊断任务铺平了道路，展示了其在解决模型泛化挑战方面的有效性和可解释性。"}}
{"id": "2511.17824", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17824", "abs": "https://arxiv.org/abs/2511.17824", "authors": ["Pranay Meshram", "Yash Turkar", "Kartikeya Singh", "Praveen Raj Masilamani", "Charuvahan Adhivarahan", "Karthik Dantu"], "title": "QAL: A Loss for Recall Precision Balance in 3D Reconstruction", "comment": "Accepted to WACV 2026. Camera-ready version to appear", "summary": "Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.\n  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.\n  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17844", "abs": "https://arxiv.org/abs/2511.17844", "authors": ["Shihan Cheng", "Nilesh Kulkarni", "David Hyde", "Dmitriy Smirnov"], "title": "Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation", "comment": null, "summary": "Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic \"real\" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.", "AI": {"tldr": "本文提出了一种数据高效的微调策略，利用稀疏、低质量的合成数据为大型文本到视频扩散模型添加新的生成控制（如相机参数），其效果优于使用真实数据进行微调。", "motivation": "为大型文本到视频扩散模型添加新的生成控制（如物理相机参数）通常需要大量高质量数据集，但这些数据集难以获取。", "method": "提出了一种数据高效的微调策略，通过使用稀疏、低质量的合成数据来学习新的生成控制。", "result": "在简单的合成数据上进行微调不仅实现了所需的控制，而且比在逼真的“真实”数据上微调的模型产生了更优异的结果。研究还提供了一个框架，从直观和定量上解释了这一现象。", "conclusion": "利用稀疏、低质量的合成数据进行数据高效微调，是为文本到视频扩散模型添加新控制的有效且更优越的方法，甚至超越了使用真实数据进行微调的效果。"}}
{"id": "2511.18703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18703", "abs": "https://arxiv.org/abs/2511.18703", "authors": ["Ardalan Tajbakhsh", "Augustinos Saravanos", "James Zhu", "Evangelos A. Theodorou", "Lorenz T. Biegler", "Aaron M. Johnson"], "title": "Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication", "comment": "9 pages, 5 figures", "summary": "This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.", "AI": {"tldr": "本文提出了一种延迟感知ADMM（DA-ADMM）变体，通过根据实时通信延迟统计数据调整惩罚参数，显著提高了多机器人系统在存在通信延迟情况下的协调性能、鲁棒性和成功率。", "motivation": "在多机器人系统中，使用分布式优化（如共识ADMM）进行无碰撞、动态可行的运动规划时，算法对通信延迟敏感。现有的惩罚参数调整或自适应方案（如残差平衡和自适应参数启发式）没有明确考虑延迟，导致性能下降。", "method": "本文引入了延迟感知ADMM（DA-ADMM）变体。该方法根据实时延迟统计数据调整ADMM的惩罚参数。它允许智能体在共识和对偶更新过程中降低过时信息的权重，并优先处理最新更新的信息。", "result": "在2D和3D环境下，使用双积分器、杜宾斯车和无人机动力学进行的广泛仿真表明，与固定参数、残差平衡和固定约束基线相比，DA-ADMM显著提高了鲁棒性、成功率和解决方案质量。结果还强调，性能下降不仅取决于延迟长度或频率，更取决于优化器对延迟信息进行情境推理的能力。DA-ADMM在各种延迟条件下均实现了持续更优的协调性能。", "conclusion": "所提出的DA-ADMM为在不完美通信条件下实现弹性多机器人运动规划提供了一种原则性且高效的机制，能够在广泛的延迟条件下持续获得更好的协调性能。"}}
{"id": "2511.18597", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18597", "abs": "https://arxiv.org/abs/2511.18597", "authors": ["H. M. Shadman Tabib", "Jaber Ahmed Deedar"], "title": "Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17881", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17881", "abs": "https://arxiv.org/abs/2511.17881", "authors": ["Ahmad Mohammadshirazi", "Pinaki Prasad Guha Neogi", "Dheeraj Kulshrestha", "Rajiv Ramnath"], "title": "MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use", "comment": null, "summary": "Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.", "AI": {"tldr": "本文提出MGA-VQA，一个多模态文档视觉问答框架，通过集成空间图推理、记忆增强推理和问题引导压缩，解决了现有模型在空间关系建模、高分辨率文档处理、多跳推理和可解释性方面的不足，并在多个基准测试中展现出卓越的准确性和效率。", "motivation": "现有文档视觉问答（DocVQA）方法在显式空间关系建模、高分辨率文档处理效率、多跳推理能力以及模型可解释性方面存在局限。", "method": "MGA-VQA是一个多模态框架，它整合了令牌级编码、空间图推理、记忆增强推理和问题引导压缩。该模型引入了可解释的基于图的决策路径和结构化记忆访问，以提高推理透明度。", "result": "MGA-VQA在六个基准数据集（FUNSD、CORD、SROIE、DocVQA、STE-VQA和RICO）上表现出卓越的准确性和效率，在答案预测和空间定位方面均有持续改进。", "conclusion": "MGA-VQA通过其新颖的多模态方法，有效解决了DocVQA的挑战，显著提升了准确性、效率和可解释性。"}}
{"id": "2511.18622", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18622", "abs": "https://arxiv.org/abs/2511.18622", "authors": ["Michael J. Bommarito"], "title": "OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph", "comment": "30 pages, 5 figures, 8 tables. Dataset available at https://huggingface.co/datasets/mjbommar/opengloss-dictionary", "summary": "We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.\n  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.\n  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.", "AI": {"tldr": "OpenGloss是一个合成的百科全书式英语词典和语义知识图谱，它将词典定义、百科上下文、词源历史和语义关系整合到一个统一资源中。该资源通过多智能体程序生成管道，在不到一周的时间内以低于1000美元的成本生成，包含53.7万个词义和15万个词位，并提供了丰富的语义边、用法示例、搭配和百科内容。", "motivation": "研究旨在解决现有词汇资源在整合词典定义、百科上下文、词源历史和语义关系方面的不足，以及手动策划综合词汇资源在成本和时间上的不切实际性。此外，它也旨在弥补教学应用中缺乏集成内容（定义、示例、搭配、百科、词源）的空白，以支持词汇学习和自然语言处理任务。", "method": "OpenGloss通过一个多智能体程序生成管道创建，该管道利用了经过模式验证的大型语言模型（LLM）输出和自动化质量保证机制。这种方法实现了在短时间内以低成本生成全面的词汇资源。", "result": "OpenGloss包含15万个词位下的53.7万个词义，与WordNet 3.1和Open English WordNet的规模相当，但提供了四倍多的词义定义。它还包含910万条语义边、100万个用法示例、300万个搭配和6000万字的百科内容。整个资源在不到一周的时间内以低于1000美元的成本生成。作为合成资源，它反映了当前基础模型的能力和局限性。数据集已在Hugging Face上公开可用。", "conclusion": "该研究证明了结构化生成方法能够以手动策划无法实现的成本和时间规模创建全面的词汇资源，从而支持基础模型改进后的快速迭代。OpenGloss通过提供集成内容，有效弥补了教学应用中的空白，支持词汇学习和自然语言处理任务。该资源作为LLM能力和局限性的体现，可供研究人员和教育工作者利用和改编。"}}
{"id": "2511.18702", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18702", "abs": "https://arxiv.org/abs/2511.18702", "authors": ["Xueyan Oh", "Leonard Loh", "Shaohui Foong", "Zhong Bao Andy Koh", "Kow Leong Ng", "Poh Kang Tan", "Pei Lin Pearlin Toh", "U-Xuan Tan"], "title": "CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection", "comment": "12 pages, 12 figures", "summary": "General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18616", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18616", "abs": "https://arxiv.org/abs/2511.18616", "authors": ["Joseph Malone", "Rachith Aiyappa", "Byunghwee Lee", "Haewoon Kwak", "Jisun An", "Yong-Yeol Ahn"], "title": "A Benchmark for Zero-Shot Belief Inference in Large Language Models", "comment": "28 pages, 5 figures", "summary": "Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18709", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18709", "abs": "https://arxiv.org/abs/2511.18709", "authors": ["Xueyan Oh", "Jonathan Her", "Zhixiang Ong", "Brandon Koh", "Yun Hann Tan", "U-Xuan Tan"], "title": "Autonomous Surface Selection For Manipulator-Based UV Disinfection In Hospitals Using Foundation Models", "comment": "7 pages, 7 figures; This paper has been accepted by IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "summary": "Ultraviolet (UV) germicidal radiation is an established non-contact method for surface disinfection in medical environments. Traditional approaches require substantial human intervention to define disinfection areas, complicating automation, while deep learning-based methods often need extensive fine-tuning and large datasets, which can be impractical for large-scale deployment. Additionally, these methods often do not address scene understanding for partial surface disinfection, which is crucial for avoiding unintended UV exposure. We propose a solution that leverages foundation models to simplify surface selection for manipulator-based UV disinfection, reducing human involvement and removing the need for model training. Additionally, we propose a VLM-assisted segmentation refinement to detect and exclude thin and small non-target objects, showing that this reduces mis-segmentation errors. Our approach achieves over 92\\% success rate in correctly segmenting target and non-target surfaces, and real-world experiments with a manipulator and simulated UV light demonstrate its practical potential for real-world applications.", "AI": {"tldr": "本文提出一种利用基础模型和视觉语言模型（VLM）辅助分割精炼的方法，以简化机械臂紫外线（UV）消毒的表面选择，减少人工干预和模型训练需求，并有效避免非目标物体的误照射。", "motivation": "传统的UV消毒方法需要大量人工干预定义消毒区域，难以自动化；基于深度学习的方法需要大量数据和微调，部署不便；现有方法通常不解决局部表面消毒的场景理解问题，无法避免意外的UV暴露。", "method": "该研究利用基础模型简化机械臂UV消毒的表面选择，从而减少人工参与并消除模型训练需求。此外，提出一种VLM辅助的分割精炼方法，用于检测和排除薄小非目标物体，以减少误分割错误。", "result": "该方法在正确分割目标和非目标表面方面取得了超过92%的成功率。通过机械臂和模拟UV光的真实世界实验，验证了其在实际应用中的巨大潜力。", "conclusion": "该方法通过利用基础模型和VLM辅助精炼，有效简化了机械臂UV消毒的表面选择过程，显著减少了人工干预和模型训练需求，并在实际应用中展现出强大的潜力和实用性。"}}
{"id": "2511.18708", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18708", "abs": "https://arxiv.org/abs/2511.18708", "authors": ["Yanbin Li", "Canran Xiao", "Shenghai Yuan", "Peilai Yu", "Ziruo Li", "Zhiguo Zhang", "Wenzheng Chi", "Wei Zhang"], "title": "GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration", "comment": "12 pages, 10 figures", "summary": "Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18712", "abs": "https://arxiv.org/abs/2511.18712", "authors": ["Tianyu Wang", "Chunxiang Yan", "Xuanhong Liao", "Tao Zhang", "Ping Wang", "Cong Wen", "Dingchuan Liu", "Haowen Yu", "Ximin Lyu"], "title": "Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control", "comment": null, "summary": "Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.", "AI": {"tldr": "本文提出一种针对轮式双足机器人头部不稳定性问题的方法，通过模型化地面力估计和导纳控制，提高了机器人在崎岖地形上的头部稳定性与整体适应性。", "motivation": "轮式双足机器人在崎岖地形上行驶时，头部不稳定性会降低机载传感器的精度或损坏脆弱的有效载荷。现有研究主要关注平台稳定性，而忽略了头部在世界坐标系中的主动稳定，导致垂直振荡，影响整体稳定性。", "method": "开发了一种针对6自由度轮式双足机器人的基于模型的地面力估计方法。利用这些力估计，实现了一种导纳控制算法以增强地形适应性。", "result": "仿真实验验证了力估计器的实时性能以及机器人在穿越不平坦地形时的鲁棒性。", "conclusion": "所提出的地面力估计方法与导纳控制算法能有效提高轮式双足机器人在崎岖地形上的地形适应性和鲁棒性，从而解决头部不稳定性问题。"}}
{"id": "2511.18618", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18618", "abs": "https://arxiv.org/abs/2511.18618", "authors": ["Mirza Raquib", "Munazer Montasir Akash", "Tawhid Ahmed", "Saydul Akbar Murad", "Farida Siddiqi Prity", "Mohammad Amzad Hossain", "Asif Pervez Polok", "Nick Rahimi"], "title": "A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News", "comment": null, "summary": "In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\\% and 73.43\\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\\% and 64.46\\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17843", "abs": "https://arxiv.org/abs/2511.17843", "authors": ["Chenyi Wang", "Zhaowei Li", "Ming F. Li", "Wujie Wen"], "title": "JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception", "comment": null, "summary": "Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17885", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17885", "abs": "https://arxiv.org/abs/2511.17885", "authors": ["Guoyang Xia", "Yifeng Ding", "Fengfa Li", "Lei Ren", "Wei Chen", "Fangxiang Feng", "Xiaojie Wang"], "title": "FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.", "AI": {"tldr": "为解决多模态大语言模型（MLLMs）高分辨率输入导致的推理延迟问题，本文提出了FastMMoE，一个针对MoE-MLLMs的免训练加速框架，通过减少专家激活和路由感知的视觉令牌剪枝，显著降低计算量，同时保持高性能。", "motivation": "多模态大语言模型（MLLMs）在高分辨率视觉输入下，会产生过长的视觉令牌序列和显著的推理延迟。减少冗余视觉令牌对于减轻计算/内存负担、保持性能至关重要，尤其是在资源受限或延迟敏感的场景中。现有视觉令牌剪枝方法主要依赖基于注意力的冗余分析，且专为密集架构设计，不适用于MoE模型。", "method": "本文提出了Fast Multimodal Mixture-of-Experts (FastMMoE)，一个基于路由分析的免训练加速框架，专为基于专家混合（MoE）的MLLMs设计。它结合了两种互补策略：(i) 减少视觉令牌的专家激活，以最小化不必要的专家计算；(ii) 路由感知的令牌剪枝，利用路由概率分布的相似性来识别和移除高度冗余的视觉令牌。", "result": "在DeepSeek-VL2和InternVL3.5等大规模MoE-MLLMs上的实验表明，FastMMoE可以将FLOPs减少高达55.0%，同时保持约95.5%的原始性能。在多个保留率下，其性能始终优于包括FastV和SparseVLM在内的密集模型剪枝基线。", "conclusion": "FastMMoE是一个有效的训练无关框架，能够显著加速基于MoE的多模态大语言模型，通过智能地减少视觉令牌冗余和专家计算，使其在计算效率和内存方面得到提升，从而更适用于实际部署。"}}
{"id": "2511.18635", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18635", "abs": "https://arxiv.org/abs/2511.18635", "authors": ["Shireen Chand", "Faith Baca", "Emilio Ferrara"], "title": "No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases", "comment": null, "summary": "Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.", "AI": {"tldr": "本研究发现，大型语言模型中针对性偏见缓解技术在目标维度上可能有效，但经常导致其他维度上意外的负面后果，例如增加偏见或降低模型连贯性。", "motivation": "大型语言模型（LLMs）从训练数据中继承了社会偏见，可能导致有害或不公平的输出。现有的偏见缓解技术通常只在其目标维度上进行评估，缺乏对跨类别影响的理解。", "method": "研究调查了四种偏见缓解技术，应用于来自七个模型家族的十个模型。探讨了种族、宗教、职业和性别相关的偏见。使用StereoSet基准测试来衡量去偏见对模型连贯性和刻板印象偏好的影响。", "result": "结果一致表明，虽然有针对性的缓解有时能减少目标维度的偏见，但它经常在其他维度上导致意想不到的负面后果，例如增加模型偏见和降低整体连贯性。", "conclusion": "研究强调了在检查和开发偏见缓解策略时，迫切需要强大的、多维度的评估工具，以避免无意中转移或加剧未受影响轴上的偏见。"}}
{"id": "2511.18659", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18659", "abs": "https://arxiv.org/abs/2511.18659", "authors": ["Jie He", "Richard He Bai", "Sinead Williamson", "Jeff Z. Pan", "Navdeep Jaitly", "Yizhe Zhang"], "title": "CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.", "AI": {"tldr": "CLaRa是一个统一的RAG框架，通过嵌入式压缩和在共享连续空间中的联合优化来解决长上下文和检索-生成分离优化的问题。它使用SCP进行数据合成，并通过可微分的top-k估计器端到端训练重排序器和生成器，实现了SOTA的压缩和重排序性能。", "motivation": "现有的检索增强生成（RAG）方法存在两个主要问题：处理长上下文的效率低下，以及检索和生成模块之间优化过程的分离。", "method": "本文提出了CLaRa（Continuous Latent Reasoning）框架，它在共享的连续空间中执行基于嵌入的压缩和联合优化。为了生成语义丰富且可检索的压缩向量，引入了SCP（key-preserving data synthesis）框架，该框架利用问答和释义监督进行数据合成。CLaRa通过单一的语言建模损失，并使用可微分的top-k估计器，实现重排序器和生成器的端到端训练，使梯度流经两个模块。理论上，这种统一优化能使检索相关性与答案质量对齐。", "result": "CLaRa在多个问答基准测试中实现了最先进的压缩和重排序性能，通常优于基于文本微调的基线方法。", "conclusion": "CLaRa通过其统一的嵌入式压缩和端到端联合优化方法，成功解决了RAG在长上下文处理和模块间优化分离的挑战，显著提升了性能。"}}
{"id": "2511.18619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18619", "abs": "https://arxiv.org/abs/2511.18619", "authors": ["Maanas Taneja"], "title": "Prompt Optimization as a State-Space Search Problem", "comment": null, "summary": "Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18743", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18743", "abs": "https://arxiv.org/abs/2511.18743", "authors": ["Yu Lei", "Shuzheng Si", "Wei Wang", "Yifei Wu", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context", "comment": null, "summary": "Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.", "AI": {"tldr": "RhinoInsight 是一个深度研究框架，通过引入可验证清单和证据审计两个控制机制，解决了现有大型语言模型研究代理在错误积累和上下文腐烂方面的问题，显著提升了深度研究任务的性能。", "motivation": "现有的大型语言模型研究代理采用线性的“规划-搜索-撰写报告”流程，但由于缺乏对模型行为和上下文的明确控制，导致错误累积和上下文腐烂。这促使研究者开发更鲁棒、可追溯且高质量的系统。", "method": "RhinoInsight 引入了两个控制机制：1. **可验证清单模块**：将用户需求转化为可追溯和可验证的子目标，通过人工或LLM评论员进行细化，并编译分层大纲以指导后续行动。2. **证据审计模块**：结构化搜索内容，迭代更新大纲，修剪噪音上下文，并使用评论员对高质量证据进行排名和绑定到草稿内容，以确保可验证性并减少幻觉。", "result": "实验表明，RhinoInsight 在深度研究任务上取得了最先进的性能，并在深度搜索任务上保持了竞争力。", "conclusion": "RhinoInsight 通过其独特的控制机制，无需参数更新，显著增强了大型语言模型作为研究代理的鲁棒性、可追溯性和整体质量，有效解决了现有系统面临的挑战。"}}
{"id": "2511.17888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17888", "abs": "https://arxiv.org/abs/2511.17888", "authors": ["Seulgi Jeong", "Jaeil Kim"], "title": "MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization", "comment": "Accepted at ICCV 2025 Personalization in Generative AI Workshop", "summary": "In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.", "AI": {"tldr": "MINDiff通过在推理时引入负注意力机制，有效缓解了文本到图像个性化模型中的过拟合问题，提升了文本对齐和用户控制，且无需重新训练。", "motivation": "大规模文本到图像模型在有限图像下学习特定主体时常出现过拟合。现有方法如DreamBooth通过类别特异性先验保持损失来缓解，但这增加了训练计算成本并限制了推理时的用户控制。", "method": "提出了掩码集成负注意力扩散（MINDiff）方法。该方法引入“负注意力”概念，通过在推理时修改交叉注意力机制，抑制主体在被遮蔽的无关区域的影响。用户还可在推理时调整一个尺度参数lambda来平衡主体保真度和文本对齐。", "result": "定性和定量实验表明，MINDiff比类别特异性先验保持损失更有效地缓解了过拟合。它增强了语义控制并改善了文本对齐。由于该方法完全在推理时操作且不改变模型架构，因此可以直接应用于现有DreamBooth模型而无需重新训练。", "conclusion": "MINDiff提供了一种在推理时操作的有效解决方案，能够更有效地缓解文本到图像个性化中的过拟合问题，提高了文本对齐和用户控制，且具有广泛的适用性，无需重新训练即可应用于现有模型。"}}
{"id": "2511.18649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18649", "abs": "https://arxiv.org/abs/2511.18649", "authors": ["Goun Pyeon", "Inbum Heo", "Jeesu Jung", "Taewook Hwang", "Hyuk Namgoong", "Hyein Seo", "Yerim Han", "Eunbin Kim", "Hyeonseok Kang", "Sangkeun Jung"], "title": "Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting", "comment": "52 pages, Korean", "summary": "This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).\n  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.\n  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17890", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.17890", "abs": "https://arxiv.org/abs/2511.17890", "authors": ["Wenyuan Li", "Guang Li", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Decoupled Audio-Visual Dataset Distillation", "comment": null, "summary": "Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.", "AI": {"tldr": "本文提出DAVDD框架，通过预训练和解耦表示学习，解决了音频-视觉数据集蒸馏中跨模态对齐不足和模态特有信息受损的问题，实现了最先进的蒸馏性能。", "motivation": "传统分布匹配（DM）方法难以捕捉内在跨模态对齐。现有跨模态匹配方法存在两个主要挑战：(i) 独立随机初始化的编码器导致模态映射空间不一致，增加训练难度；(ii) 模态间直接交互容易损害模态特有（私有）信息，降低蒸馏数据质量。", "method": "本文提出DAVDD，一个基于预训练的解耦音频-视觉蒸馏框架。它利用多样化的预训练库获取稳定的模态特征，并使用轻量级解耦库将特征解耦为通用和私有表示。为有效保留跨模态结构，引入通用跨模态匹配和样本-分布联合对齐策略，确保共享表示在样本和全局分布层面都对齐。同时，私有表示完全隔离于跨模态交互，保护了模态特有信息。", "result": "DAVDD在多个基准测试中，所有IPC设置下均取得了最先进的（SOTA）结果。", "conclusion": "解耦表示学习对于高质量的音频-视觉数据集蒸馏是有效的，DAVDD框架的实验结果验证了其有效性。"}}
{"id": "2511.18857", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18857", "abs": "https://arxiv.org/abs/2511.18857", "authors": ["Changsheng Luo", "Yushi Wang", "Wenhan Cai", "Mingguo Zhao"], "title": "AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion", "comment": null, "summary": "Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.", "AI": {"tldr": "本文提出AutoOdom，一种新型自回归本体感知里程计系统，通过创新的两阶段训练范式，解决了传统方法在仿真到现实迁移和数据需求方面的挑战，显著提升了足式机器人在复杂环境下的定位精度和鲁棒性。", "motivation": "在GPS受限和视觉退化环境中，传统视觉里程计系统失效，而足式机器人的本体感知里程计面临关键限制：分析滤波方法受模型不确定性和累积漂移影响；混合学习-滤波方法受分析组件限制；纯学习方法难以实现仿真到现实的迁移，并需要大量真实世界数据。", "method": "本文引入AutoOdom，一个新颖的自回归本体感知里程计系统。其采用两阶段训练范式：第一阶段利用大规模仿真数据学习足式运动固有的复杂非线性动力学和快速变化的接触状态；第二阶段引入自回归增强机制，使用有限的真实世界数据有效弥合仿真到现实的鸿沟。关键创新在于自回归训练方法，模型通过学习自身的预测来增强对传感器噪声的适应性和在高度动态环境中的鲁棒性。", "result": "在Booster T1人形机器人上的综合实验验证表明，AutoOdom在所有评估指标上均显著优于现有最先进方法。与Legolas基线相比，绝对轨迹误差提高了57.2%，Umeyama对齐误差提高了59.2%，相对位姿误差提高了36.2%。广泛的消融研究提供了关于传感器模态选择和时间建模的关键见解，揭示了关于IMU加速度数据的反直觉发现，并验证了其在挑战性运动场景中鲁棒本体感知里程计的系统设计选择。", "conclusion": "AutoOdom通过其创新的两阶段自回归训练范式，成功克服了足式机器人本体感知里程计的现有挑战，实现了在GPS受限和视觉退化环境中卓越的定位精度和鲁棒性，为足式机器人导航提供了重要进展。"}}
{"id": "2511.18718", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18718", "abs": "https://arxiv.org/abs/2511.18718", "authors": ["Omar Garib", "Jayaprakash D. Kambhampaty", "Olivia J. Pinon Fischer", "Dimitri N. Mavris"], "title": "AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation", "comment": "9 pages, 4 figures, 1 table, 1 algorithm", "summary": "We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17883", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17883", "abs": "https://arxiv.org/abs/2511.17883", "authors": ["Jiong Lin", "Jinchen Ruan", "Hod Lipson"], "title": "ArticFlow: Generative Simulation of Articulated Mechanisms", "comment": "8 pages, 8 figures", "summary": "Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18878", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18878", "abs": "https://arxiv.org/abs/2511.18878", "authors": ["Suzie Kim", "Hye-Bin Shin", "Hyo-Jeong Jang"], "title": "Accelerating Reinforcement Learning via Error-Related Human Brain Signals", "comment": null, "summary": "In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.", "AI": {"tldr": "本研究探究了隐式神经反馈（EEG）如何加速复杂机器人操作任务中的强化学习，发现其能有效提升高维操作任务的学习效率和成功率，并能适应个体差异。", "motivation": "先前的EEG引导强化学习研究主要集中在导航或低维运动任务。本研究旨在探索此类神经评估信号是否能改善涉及障碍物和精确末端执行器控制的高维操作任务中的策略学习。", "method": "将从离线训练的EEG分类器解码的错误相关电位整合到奖励塑造中，并系统地评估了人类反馈权重的影响。", "result": "在具有障碍物的7自由度机械臂抓取环境中进行的实验表明，神经反馈加速了强化学习，并且根据人类反馈的权重，有时能达到超过稀疏奖励基线的任务成功率。应用最佳反馈权重时，相对于稀疏奖励设置，强化学习的加速效果一致。此外，留一法交叉验证证实，尽管EEG可解码性存在固有的个体差异，所提出的框架仍保持鲁棒性。", "conclusion": "研究结果表明，基于EEG的强化学习可以扩展到运动任务之外，为人类对齐的操作技能习得提供了一条可行途径。"}}
{"id": "2511.18751", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18751", "abs": "https://arxiv.org/abs/2511.18751", "authors": ["Daiqing Wu", "Dongbao Yang", "Can Ma", "Yu Zhou"], "title": "Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion", "comment": "Accepted by ACM MM 2024", "summary": "As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.", "AI": {"tldr": "本文提出了一种基于分布的特征恢复与融合（DRF）方法，用于鲁棒的多模态情感分析，以有效处理图像-文本对中可能存在的低质量和缺失模态。", "motivation": "现有方法在处理图像-文本对情感分析时，未能充分考虑现实世界中常见的低质量和缺失模态问题，导致模型鲁棒性不足，因此需要开发能够鲁棒预测情感的模型。", "method": "DRF方法为每种模态维护一个特征队列以近似其特征分布。对于低质量模态，通过基于分布的模态质量估计来减少其融合贡献。对于缺失模态，则通过样本和分布监督建立模态间映射关系，从可用模态中恢复缺失模态。", "result": "在三种公开图像-文本数据集上，通过两种模拟低质量和缺失模态的干扰策略进行实验，结果表明DRF方法在两种策略下均普遍优于现有SOTA方法。", "conclusion": "DRF方法在处理低质量和缺失模态时表现出卓越的鲁棒性，有效验证了其在鲁棒多模态情感分析中的有效性。"}}
{"id": "2511.18696", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18696", "abs": "https://arxiv.org/abs/2511.18696", "authors": ["Wangjiaxuan Xin"], "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models", "comment": null, "summary": "This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18749", "categories": ["cs.CL", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18749", "abs": "https://arxiv.org/abs/2511.18749", "authors": ["Matthew R. DeVerna", "Kai-Cheng Yang", "Harry Yaojun Yan", "Filippo Menczer"], "title": "Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search", "comment": null, "summary": "Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.", "AI": {"tldr": "评估显示，大型语言模型在自动化事实核查方面表现不佳，推理和网络搜索提升有限，但使用高质量策展上下文的RAG系统能显著提高性能。", "motivation": "大型语言模型有望实现端到端自动化事实核查，但此前研究结果不一。鉴于主流聊天机器人已具备推理和网络搜索功能，且用户日益依赖它们进行验证，因此迫切需要进行严格评估。", "method": "研究评估了来自OpenAI、Google、Meta和DeepSeek的15个最新LLM，使用了超过6000个由PolitiFact核查的声明。比较了标准模型、推理增强模型和网络搜索增强模型的性能。此外，还测试了一个使用PolitiFact摘要的策展RAG系统。", "result": "标准模型表现不佳，推理能力提升微乎其微，网络搜索工具也仅带来中等收益，尽管事实核查结果在网络上可获取。相比之下，使用PolitiFact摘要的策展RAG系统平均将宏观F1分数提高了233%。", "conclusion": "研究结果表明，为模型提供高质量的策展上下文是实现自动化事实核查的一个有前景的方向。"}}
{"id": "2511.17886", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17886", "abs": "https://arxiv.org/abs/2511.17886", "authors": ["Pume Tuchinda", "Parinthapat Pengpun", "Romrawin Chumpu", "Sarana Nutanong", "Peerat Limkonchotiwat"], "title": "When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA", "comment": null, "summary": "Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17914", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17914", "abs": "https://arxiv.org/abs/2511.17914", "authors": ["Chenyang Jiang", "Hang Zhao", "Xinyu Zhang", "Zhengcen Li", "Qiben Shan", "Shaocong Wu", "Jingyong Su"], "title": "Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation", "comment": "10 pages, accepted by NeurIPS 2025", "summary": "Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.", "AI": {"tldr": "该研究针对长尾分布下数据集蒸馏的挑战，强调软标签的作用，并提出ADSA模块来校准蒸馏过程中产生的软标签偏差，显著提升了长尾数据集的蒸馏性能。", "motivation": "现有数据集蒸馏技术主要关注平衡数据集，在真实世界的长尾分布下性能不佳，导致存储和训练成本的降低效果受限。", "method": "本研究首先强调软标签在长尾数据集蒸馏中的关键作用，并揭示性能下降的潜在机制。具体地，推导了针对蒸馏数据集训练模型的失衡感知泛化界限。通过系统扰动数据失衡水平，识别出蒸馏模型和蒸馏图像引起的两种软标签偏差源。为解决此问题，提出了ADSA（自适应软标签对齐）模块，用于校准这些纠缠的偏差。该模块轻量级，可无缝集成到现有蒸馏流程中。", "result": "在ImageNet-1k-LT数据集上，结合EDC和IPC=50设置，ADSA将尾部类别准确率提高了11.8%，并将整体准确率提升至41.4%。广泛的实验证明，ADSA在有限标签预算和多种蒸馏技术下均提供了一个鲁棒且可推广的解决方案。", "conclusion": "ADSA成功解决了长尾数据集蒸馏中软标签偏差问题，显著提升了模型在长尾数据上的泛化能力和准确性，为该领域提供了一个有效且通用的解决方案。"}}
{"id": "2511.18810", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18810", "abs": "https://arxiv.org/abs/2511.18810", "authors": ["Yuxia Fu", "Zhizhen Zhang", "Yuqi Zhang", "Zijian Wang", "Zi Huang", "Yadan Luo"], "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18756", "abs": "https://arxiv.org/abs/2511.18756", "authors": ["Xueyu Du", "Lilian Zhang", "Fuan Duan", "Xincan Luo", "Maosong Wang", "Wenqi Wu", "JunMao"], "title": "SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map", "comment": null, "summary": "Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18774", "abs": "https://arxiv.org/abs/2511.18774", "authors": ["Bashar Talafha", "Amin Abu Alhassan", "Muhammad Abdul-Mageed"], "title": "Context-Aware Whisper for Arabic ASR Under Linguistic Varieties", "comment": null, "summary": "Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.", "AI": {"tldr": "本文提出上下文感知提示策略，无需重新训练即可将OpenAI Whisper模型应用于低资源阿拉伯语语音识别，显著降低了词错误率并减少了幻觉。", "motivation": "低资源自动语音识别（ASR）仍然是一个挑战，特别是对于像阿拉伯语这样方言差异大且标注数据有限的语言。", "method": "本文提出了上下文感知提示策略来适应Whisper模型，包括使用首次转录或检索到的语句进行解码器提示，以及使用目标说话者声音合成的语音进行编码器前缀。此外，还引入了提示重排序、说话者感知前缀合成以及模态特定检索（词汇、语义、声学）等技术。", "result": "在九种阿拉伯语语言条件下进行评估，该方法在现代标准阿拉伯语上将词错误率（WER）降低了高达22.3%，在方言语音上降低了9.2%，显著缓解了幻觉和说话者不匹配问题。", "conclusion": "上下文感知提示策略能有效改进OpenAI Whisper模型在真实世界、零样本设置下对低资源阿拉伯语语音识别的性能，尤其是在处理方言变体和减少错误方面。"}}
{"id": "2511.18832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18832", "abs": "https://arxiv.org/abs/2511.18832", "authors": ["Kaize Shi", "Xueyao Sun", "Xiaohui Tao", "Lin Li", "Qika Lin", "Guandong Xu"], "title": "Concept than Document: Context Compression via AMR-based Conceptual Entropy", "comment": null, "summary": "Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.", "AI": {"tldr": "本文提出了一种无监督的上下文压缩框架，利用抽象意义表示（AMR）图和节点级概念熵来过滤冗余信息，保留语义核心，从而解决大型语言模型在处理长上下文时的信息过载问题。", "motivation": "大型语言模型（LLMs）在处理长上下文时面临信息过载，特别是在检索增强生成（RAG）中，大量的支持文档常引入冗余内容。这不仅降低了推理准确性，还增加了计算开销。", "method": "该方法构建了一个无监督的上下文压缩框架。具体步骤包括：从原始上下文构建AMR图，量化AMR图中节点级熵以估计概念重要性，筛选出重要的信息节点，从而形成一个比原始文档更精炼、语义更集中的上下文。", "result": "在PopQA和EntityQuestions数据集上的实验表明，该方法优于普通基线和其他基线，在显著减少上下文长度的同时，实现了更高的准确性。", "conclusion": "据作者所知，这是首次引入基于AMR的概念熵进行上下文压缩的工作，展示了稳定语言特征在上下文工程中的潜力。"}}
{"id": "2511.17927", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17927", "abs": "https://arxiv.org/abs/2511.17927", "authors": ["Yingjie Ma", "Xun Lin", "Yong Xu", "Weicheng Xie", "Zitong Yu"], "title": "PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.", "AI": {"tldr": "该论文提出了PA-FAS框架，通过构建高质量的扩展推理序列和引入答案混洗机制，解决了多模态人脸反欺诈（FAS）中SFT+RL方法的推理路径受限和监督不匹配问题，显著提升了推理准确性和跨域泛化能力。", "motivation": "当前将强化学习（RL）应用于多模态人脸反欺诈（FAS）时，面临两个主要限制：1) 监督微调（SFT）后多模态推理路径有限，限制了互补模态的使用并缩小了探索空间，削弱了RL的效果；2) 单一任务监督与多样化推理路径不匹配，导致推理混淆和模型可能利用捷径学习。", "method": "该论文提出了PA-FAS框架，包含两个核心机制：1) 通过从有限标注中构建高质量的扩展推理序列，以丰富推理路径并放宽探索限制；2) 在SFT过程中引入答案混洗机制，强制进行全面的多模态分析，而非利用表面线索，从而鼓励深度推理并减轻捷径学习。", "result": "PA-FAS显著提高了多模态推理准确性和跨域泛化能力。它还更好地统一了多模态融合、泛化和可解释性，为可信赖的FAS提供了支持。", "conclusion": "PA-FAS通过解决SFT+RL在多模态FAS中的推理路径和监督问题，提供了一种更有效的方法，能够提升模型的推理能力、泛化性和可解释性，从而实现更可靠的人脸反欺诈系统。"}}
{"id": "2511.18950", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18950", "abs": "https://arxiv.org/abs/2511.18950", "authors": ["Juntao Gao", "Feiyang Ye", "Jing Zhang", "Wenjing Qian"], "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation", "comment": "11 pages, 5 figures", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.", "AI": {"tldr": "Compressor-VLA是一种混合指令条件下的视觉token压缩框架，旨在高效压缩VLA模型中的视觉信息，以减少计算开销并保留任务关键细节，从而实现更高效的机器人部署。", "motivation": "VLA模型在具身AI中表现强大，但处理冗余视觉token的巨大计算开销是实时机器人部署的关键瓶颈。标准的任务无关token剪枝技术难以保留任务关键视觉信息，无法同时兼顾整体上下文和精细细节。", "method": "本文提出了Compressor-VLA框架，包含两个token压缩模块：语义任务压缩器（STC）用于提取整体、任务相关的上下文，以及空间细化压缩器（SRC）用于保留精细的空间细节。这种压缩通过自然语言指令动态调节，实现对任务相关视觉信息的自适应凝练。", "result": "Compressor-VLA在LIBERO基准测试中取得了具有竞争力的成功率，同时与基线模型相比，FLOPs减少了59%，视觉token数量减少了3倍以上。在双臂机器人平台上的真实机器人部署验证了模型的sim-to-real可迁移性和实用性。定性分析表明，指令引导动态地将模型的感知焦点导向任务相关对象。", "conclusion": "Compressor-VLA通过结合语义和空间压缩，并利用指令进行动态调节，有效解决了VLA模型中视觉信息冗余的问题，显著提高了计算效率和实际部署能力，同时保持了任务性能和关键信息保留。"}}
{"id": "2511.17904", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17904", "abs": "https://arxiv.org/abs/2511.17904", "authors": ["Yuhang Ming", "Chenxin Fang", "Xingyuan Yu", "Fan Zhang", "Weichen Dai", "Wanzeng Kong", "Guofeng Zhang"], "title": "CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation", "comment": "15 pages, 8 figures, 4 tables", "summary": "Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18910", "abs": "https://arxiv.org/abs/2511.18910", "authors": ["Samuel Cerezo", "Seong Hun Lee", "Javier Civera"], "title": "An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization", "comment": "8 pages, 2 figures, 10 tables. Submitted to RA-L", "summary": "In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17918", "abs": "https://arxiv.org/abs/2511.17918", "authors": ["Youngsik Yun", "Dongjun Gu", "Youngjung Uh"], "title": "Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization", "comment": "Project page: https://bbangsik13.github.io/FASR", "summary": "Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17929", "abs": "https://arxiv.org/abs/2511.17929", "authors": ["Hui Lu", "Yi Yu", "Shijian Lu", "Deepu Rajan", "Boon Poh Ng", "Alex C. Kot", "Xudong Jiang"], "title": "MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection", "comment": null, "summary": "Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.", "AI": {"tldr": "MambaTAD是一个新型的状态空间模型，通过引入对角掩码双向状态空间模块和全局特征融合头，有效解决了长时动作检测中的上下文衰减和自元素冲突问题，实现了端到端的一阶段检测，并在多个基准测试中取得了优异性能。", "motivation": "时间动作检测（TAD）旨在定位视频中的动作，但现有的结构化状态空间模型（如Mamba）在处理长时动作时面临时间上下文衰减和自元素冲突的挑战。传统方法也因缺乏全局感知和检测头效率低下而难以检测长时动作。", "method": "本文提出了MambaTAD，一个用于TAD的新型状态空间模型，包含两项创新设计：1. 对角掩码双向状态空间（DMBSS）模块，用于促进全局特征融合和时间动作检测。2. 全局特征融合头，利用多粒度特征和全局感知能力逐步细化检测。此外，MambaTAD采用新的状态空间时间适配器（SSTA），以端到端一阶段的方式进行TAD，降低了网络参数和计算成本。", "result": "广泛的实验表明，MambaTAD在多个公共基准测试中持续取得了卓越的TAD性能。", "conclusion": "MambaTAD通过其新颖的DMBSS模块和全局特征融合头，成功解决了长时动作检测中的关键挑战，实现了高效准确的端到端TAD，并在各项评估中展现出优越的性能。"}}
{"id": "2511.17930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17930", "abs": "https://arxiv.org/abs/2511.17930", "authors": ["Yuan Qu", "Zhipeng Zhang", "Chaojun Xu", "Qiao Wan", "Mengying Xie", "Yuzeng Chen", "Zhenqi Liu", "Yanfei Zhong"], "title": "UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection", "comment": null, "summary": "In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.", "AI": {"tldr": "本文提出了一个统一的遥感变化检测框架UniRSCD，它基于状态空间模型和频率变化提示生成器作为统一编码器，并使用统一解码器和预测头来适应不同输出粒度的变化检测任务（如BCD、SCD、BDA），在多个数据集上取得了领先性能。", "motivation": "现有的遥感变化检测方法需要大量专家知识来设计专门的解码器以补偿编码过程中的信息损失，这导致了选择最优模型的不确定性，并限制了架构的通用性，尤其是在灾害突发等场景下。", "method": "本文提出了UniRSCD统一通用变化检测框架。它以状态空间模型为骨干，引入频率变化提示生成器作为统一编码器，该编码器动态扫描双时相全局上下文信息，并融合高频细节与低频整体信息，从而无需专门的解码器进行特征补偿。随后，统一解码器和预测头通过分层特征交互和任务自适应输出映射建立共享表示空间，将二元变化检测和语义变化检测等任务整合到一个统一架构中。", "result": "实验结果表明，所提出的架构能够适应多种变化检测任务，并在包括LEVIR-CD（二元变化数据集）、SECOND（语义变化数据集）和xBD（建筑损伤评估数据集）在内的五个数据集上取得了领先性能。", "conclusion": "UniRSCD框架成功解决了现有方法在不同粒度变化检测任务中对专门解码器的依赖和通用性限制问题，通过统一架构有效适应了不同任务的需求，并表现出卓越的性能。"}}
{"id": "2511.18808", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18808", "abs": "https://arxiv.org/abs/2511.18808", "authors": ["Cao Linxiao", "Wang Ruitao", "Li Jindong", "Zhou Zhipeng", "Yang Menglin"], "title": "HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations", "comment": "12 pages", "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17941", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17941", "abs": "https://arxiv.org/abs/2511.17941", "authors": ["Xiangyan Kong", "Xuecheng Wu", "Xiongwei Zhao", "Xiaodong Li", "Yunyun Shi", "Gang Wang", "Dingkang Yang", "Yang Liu", "Hong Chen", "Yulong Gao"], "title": "V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction", "comment": null, "summary": "V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.", "AI": {"tldr": "V2X-RECT是一个针对高密度交通环境的轨迹预测框架，通过解决频繁的身份切换、冗余交互和重复编码问题，实现了更高效、准确和鲁棒的预测。", "motivation": "在密集交通场景中，V2X预测面临多重挑战：目标频繁的身份切换阻碍跨视角关联和融合；多源信息在编码阶段容易产生冗余交互；传统的以车辆为中心的编码方式导致大量重复的历史轨迹特征编码，降低了实时推理性能。", "method": "本文提出了V2X-RECT框架。它包含三个核心模块：1) 多源身份匹配与校正模块，利用多视角时空关系实现稳定一致的目标关联，减少错配影响。2) 交通信号引导交互模块，编码红绿灯变化趋势并利用其对时空通行权的约束作用，筛选关键交互车辆，捕获信号变化对交互模式的动态影响。3) 局部时空坐标编码，实现历史轨迹和地图特征的可重用性，支持并行解码，显著提高推理效率。", "result": "在V2X-Seq和V2X-Traj数据集上的大量实验结果表明，V2X-RECT相较于SOTA方法取得了显著改进，并在不同交通密度下提升了鲁棒性和推理效率。", "conclusion": "V2X-RECT成功解决了高密度V2X场景中轨迹预测面临的数据关联不一致、冗余交互和历史信息重复编码等问题，通过创新性的模块设计，显著提升了预测的准确性、鲁棒性和实时推理效率。"}}
{"id": "2511.19201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19201", "abs": "https://arxiv.org/abs/2511.19201", "authors": ["Ann-Sophia Müller", "Moonkwang Jeong", "Jiyuan Tian", "Meng Zhang", "Tian Qiu"], "title": "Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap", "comment": "6 pages, 6 figures, IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Untethered magnetic manipulation of biomedical millirobots has a high potential for minimally invasive surgical applications. However, it is still challenging to exert high actuation forces on the small robots over a large distance. Permanent magnets offer stronger magnetic torques and forces than electromagnetic coils, however, feedback control is more difficult. As proven by Earnshaw's theorem, it is not possible to achieve a stable magnetic trap in 3D by static permanent magnets. Here, we report a stable 2D magnetic force trap by an array of permanent magnets to control a millirobot. The trap is located in an open space with a tunable distance to the magnet array in the range of 20 - 120mm, which is relevant to human anatomical scales. The design is achieved by a novel GPU-accelerated optimization algorithm that uses mean squared error (MSE) and Adam optimizer to efficiently compute the optimal angles for any number of magnets in the array. The algorithm is verified using numerical simulation and physical experiments with an array of two magnets. A millirobot is successfully trapped and controlled to follow a complex trajectory. The algorithm demonstrates high scalability by optimizing the angles for 100 magnets in under three seconds. Moreover, the optimization workflow can be adapted to optimize a permanent magnet array to achieve the desired force vector fields.", "AI": {"tldr": "该研究通过优化永磁体阵列设计，实现了稳定的二维磁力陷阱，利用GPU加速算法高效计算磁体角度，成功在相关距离内对毫机器人进行无束缚控制，为微创手术应用提供了可能。", "motivation": "无束缚磁操纵医用毫机器人在微创手术中潜力巨大，但面临两大挑战：难以在远距离上施加高驱动力，以及由于厄恩肖定理，静态永磁体无法实现三维稳定磁陷阱。永磁体虽能提供更强磁力，但反馈控制困难。", "method": "研究报告了一种通过永磁体阵列实现稳定二维磁力陷阱的方法。设计采用了新颖的GPU加速优化算法，该算法利用均方误差（MSE）和Adam优化器，高效计算阵列中任意数量磁体的最佳角度。通过数值模拟和双磁体阵列的物理实验验证了算法。", "result": "成功实现了一个稳定的二维磁力陷阱，其位于开放空间，与磁体阵列的距离可在20-120mm范围内调节，符合人体解剖学尺度。毫机器人被成功捕获并控制，跟随复杂轨迹移动。该算法展示了高可扩展性，能在三秒内优化100个磁体的角度。此外，优化流程可用于实现所需的力矢量场。", "conclusion": "该研究成功克服了无束缚磁操纵毫机器人面临的稳定性和作用距离挑战，通过优化的永磁体阵列和高效GPU加速算法，实现了稳定的二维磁力陷阱和对毫机器人的有效控制，为生物医学微创手术应用提供了有前景的解决方案。"}}
{"id": "2511.19031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19031", "abs": "https://arxiv.org/abs/2511.19031", "authors": ["Haihang Wu", "Yuchen Zhou"], "title": "Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors", "comment": null, "summary": "Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19011", "abs": "https://arxiv.org/abs/2511.19011", "authors": ["Jiale Zhang", "Yeqiang Qian", "Tong Qin", "Mingyang Jiang", "Siyuan Chen", "Ming Yang"], "title": "End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera", "comment": null, "summary": "The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19094", "abs": "https://arxiv.org/abs/2511.19094", "authors": ["David Bricher", "Andreas Mueller"], "title": "Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework", "comment": "MDPI Sensors, published 22 November 2025", "summary": "Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17943", "abs": "https://arxiv.org/abs/2511.17943", "authors": ["Zhiyu Xu", "Weilong Yan", "Yufei Shi", "Xin Meng", "Tao He", "Huiping Zhuang", "Ming Li", "Hehe Fan"], "title": "SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System", "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.", "AI": {"tldr": "SciEducator是一个迭代自进化的多智能体系统，用于科学视频理解和教育。它将戴明循环应用于科学活动解释和多模态教育内容生成，并在自建基准SciVBench上超越现有MLLM和视频智能体。", "motivation": "现有的多模态大语言模型（MLLMs）和视频智能体在需要整合外部专业知识和严谨逐步推理的科学视频理解和教育领域表现不佳。", "method": "提出SciEducator，首个迭代自进化的多智能体系统，用于科学视频理解和教育。该系统借鉴管理学中的戴明循环（PDSA），将其理念重构为自进化的推理和反馈机制，以解释复杂的科学活动。它能生成多模态教育内容，包括文本说明、视觉指南、音频旁白和交互式参考。为支持评估，构建了SciVBench基准，包含500个经过专家验证、基于文献的科学问答对，涵盖物理、化学和日常现象。", "result": "SciEducator在SciVBench基准测试中显著优于领先的闭源MLLM（如Gemini、GPT-4o）和最先进的视频智能体。", "conclusion": "SciEducator为科学视频理解和教育领域建立了一个新范式，通过迭代自进化的多智能体系统和多模态内容生成，有效解决了现有方法的不足。"}}
{"id": "2511.19135", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19135", "abs": "https://arxiv.org/abs/2511.19135", "authors": ["Pascal Goldschmid", "Aamir Ahmad"], "title": "Autonomous Docking of Multi-Rotor UAVs on Blimps under the Influence of Wind Gusts", "comment": "13 pages, 8 figures, 8 tables", "summary": "Multi-rotor UAVs face limited flight time due to battery constraints. Autonomous docking on blimps with onboard battery recharging and data offloading offers a promising solution for extended UAV missions. However, the vulnerability of blimps to wind gusts causes trajectory deviations, requiring precise, obstacle-aware docking strategies. To this end, this work introduces two key novelties: (i) a temporal convolutional network that predicts blimp responses to wind gusts, enabling rapid gust detection and estimation of points where the wind gust effect has subsided; (ii) a model predictive controller (MPC) that leverages these predictions to compute collision-free trajectories for docking, enabled by a novel obstacle avoidance method for close-range manoeuvres near the blimp. Simulation results show our method outperforms a baseline constant-velocity model of the blimp significantly across different scenarios. We further validate the approach in real-world experiments, demonstrating the first autonomous multi-rotor docking control strategy on blimps shown outside simulation. Source code is available here https://github.com/robot-perception-group/multi_rotor_airship_docking.", "AI": {"tldr": "本文提出了一种针对多旋翼无人机在气球飞艇上进行自主停靠的策略，通过时间卷积网络预测飞艇对阵风的响应，并结合模型预测控制器规划无碰撞轨迹，以克服电池限制并延长任务时间。", "motivation": "多旋翼无人机受电池限制导致飞行时间有限。在飞艇上自主停靠并进行电池充电和数据卸载是延长无人机任务时间的有效方案。然而，飞艇易受阵风影响导致轨迹偏差，因此需要精确且避障的停靠策略。", "method": "本研究引入了两项创新：(i) 使用时间卷积网络（TCN）预测飞艇对阵风的响应，实现快速阵风检测并估计阵风影响消退点；(ii) 采用模型预测控制器（MPC），利用TCN的预测信息计算无碰撞的停靠轨迹，并通过一种新颖的避障方法支持飞艇附近的近距离操作。", "result": "仿真结果表明，该方法在不同场景下显著优于基线恒速飞艇模型。此外，通过真实世界实验验证了该方法，展示了首次在模拟环境之外实现的多旋翼无人机在飞艇上的自主停靠控制策略。", "conclusion": "本研究提出的TCN结合MPC的策略有效解决了多旋翼无人机在受阵风影响的飞艇上进行自主、无碰撞停靠的挑战，成功延长了无人机任务能力，并在仿真和实际环境中均得到了验证。"}}
{"id": "2511.18843", "categories": ["cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18843", "abs": "https://arxiv.org/abs/2511.18843", "authors": ["Heger Arfaoui", "Mohammed Iheb Hergli", "Beya Benzina", "Slimane BenMiled"], "title": "A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis", "comment": null, "summary": "Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18860", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18860", "abs": "https://arxiv.org/abs/2511.18860", "authors": ["Xingyu Huang", "Fei Jiang", "Jianli Xiao"], "title": "Generating Reading Comprehension Exercises with Large Language Models for Educational Applications", "comment": null, "summary": "With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.", "AI": {"tldr": "本文提出了一种名为RCEG的新型大型语言模型（LLM）框架，能够自动生成高质量、个性化的英语阅读理解练习。", "motivation": "随着大型语言模型（LLMs）的快速发展，其在教育领域，特别是自动文本生成方面展现出巨大潜力，可用于创建智能和自适应的学习内容。因此，研究旨在开发一种能够自动生成高质量、个性化英语阅读理解练习的框架。", "method": "RCEG框架首先使用经过微调的LLM生成内容候选项，然后利用判别器选择最佳候选项，从而显著提高生成内容的质量。为评估其性能，本文构建了一个专门的英语阅读理解数据集进行实验，并采用内容多样性、事实准确性、语言毒性和教学对齐等综合评估指标分析实验结果。", "result": "实验结果表明，RCEG显著提高了所生成练习的相关性和认知适宜性。", "conclusion": "RCEG框架能够有效生成高质量、个性化的英语阅读理解练习，并在相关性和认知适宜性方面表现出色。"}}
{"id": "2511.19236", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19236", "abs": "https://arxiv.org/abs/2511.19236", "authors": ["Yuxuan Wang", "Haobin Jiang", "Shiqing Yao", "Ziluo Ding", "Zongqing Lu"], "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control", "comment": "23 pages, 8 figures, 11 tables", "summary": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.", "AI": {"tldr": "SENTINEL是一个端到端的人形机器人全身控制语言-动作模型，通过大规模数据集训练，直接将语言指令和本体感受输入映射到低级动作，实现强大的语义理解和稳定的真实世界执行。", "motivation": "现有的人形机器人控制系统依赖人工遥控或模块化流程，前者完全由人驱动，后者语言理解与物理执行分离，导致指令与物理行为对齐不佳。", "method": "提出SENTINEL模型，一个端到端的语言-动作模型，用于人形机器人全身控制。通过使用预训练全身控制器在仿真中跟踪人类动作并结合文本标注，构建大规模数据集。模型直接将语言指令和本体感受输入映射到低级动作，不使用中间表示。动作块通过流匹配生成，并通过残差动作头进行细化以适应真实世界部署。此外，支持将多模态输入转换为文本进行扩展。", "result": "该方法在仿真和真实世界部署中均表现出强大的语义理解能力和稳定的执行能力，并支持多模态输入扩展。", "conclusion": "SENTINEL通过其端到端的语言-动作模型，有效解决了现有方法的局限性，实现了人形机器人强大的语义理解和稳定的全身控制，适用于真实世界场景。"}}
{"id": "2511.19211", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19211", "abs": "https://arxiv.org/abs/2511.19211", "authors": ["Prabhat Kumar", "Chandra Prakash", "Josh Pinskier", "David Howard", "Matthijs Langelaar"], "title": "Soft pneumatic grippers: Topology optimization, 3D-printing and experimental validation", "comment": "9 Figures", "summary": "This paper presents a systematic topology optimization framework for designing a soft pneumatic gripper (SPG), explicitly considering the design-dependent nature of the actuating load. The load is modeled using Darcy's law with an added drainage term. A 2D soft arm unit is optimized by formulating it as a compliant mechanism design problem using the robust formulation. The problem is posed as a min-max optimization, where the output deformations of blueprint and eroded designs are considered. A volume constraint is imposed on the blueprint part, while a strain-energy constraint is enforced on the eroded part. The MMA is employed to solve the optimization problem and obtain the optimized soft unit. Finite element analysis with the Ogden material model confirms that the optimized 2D unit outperforms a conventional rectangular design under pneumatic loading. The optimized 2D unit is extruded to obtain a 3D module, and ten such units are assembled to create a soft arm. Deformation profiles of the optimized arm are analysed under different pressure loads. Four arms are 3D-printed and integrated with a supporting structure to realize the proposed SPG. The gripping performance of the SPG is demonstrated on objects with different weights, sizes, stiffness, and shapes.", "AI": {"tldr": "本文提出了一种系统性的拓扑优化框架，用于设计软气动夹持器（SPG），明确考虑了驱动载荷的设计依赖性，并通过有限元分析和物理夹持测试验证了其性能。", "motivation": "当前软气动夹持器的设计缺乏系统性，尤其是在处理驱动载荷的设计依赖性方面。研究旨在通过拓扑优化克服这一挑战，以实现更优的夹持器设计。", "method": "研究方法包括：使用达西定律（含排水项）建模驱动载荷；将2D软臂单元的优化表述为鲁棒（min-max）柔顺机构设计问题，考虑蓝图和侵蚀设计的输出变形；对蓝图部分施加体积约束，对侵蚀部分施加应变能约束；采用MMA算法求解优化问题；通过使用奥格登材料模型的有限元分析验证2D单元性能；将优化后的2D单元挤压成3D模块，组装成软臂；3D打印并集成四个软臂以实现SPG，并测试其夹持性能。", "result": "优化后的2D单元在气动载荷下表现优于传统的矩形设计（经FEA确认）。优化后的软臂在不同压力载荷下展现出预期的变形特性。所提出的SPG成功地夹持了不同重量、尺寸、刚度和形状的物体。", "conclusion": "该研究成功开发了一个系统的拓扑优化框架，用于设计高性能软气动夹持器。通过优化软臂单元，实现了卓越的夹持能力，并验证了该设计方法的有效性。"}}
{"id": "2511.17932", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.17932", "abs": "https://arxiv.org/abs/2511.17932", "authors": ["Yan Xu", "Yixing Wang", "Stella X. Yu"], "title": "Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion", "comment": "Accepted to NeurIPS 2025", "summary": "Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \\emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \\emph{completing a natural video} unfolding through space.\n  We recast the task as \\emph{test-time natural video completion}, using powerful priors from \\emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \\emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \\emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \\emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.\n  The result is coherent, high-fidelity renderings from sparse inputs \\emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18850", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18850", "abs": "https://arxiv.org/abs/2511.18850", "authors": ["Fengyuan Liu", "Huang Yi", "Sichun Luo", "Yuqi Wang", "Yazheng Yang", "Xinye Li", "Zefa Hu", "Junlan Feng", "Qi Liu"], "title": "Cognitive Alpha Mining via LLM-Driven Code-Based Evolution", "comment": null, "summary": "Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18848", "abs": "https://arxiv.org/abs/2511.18848", "authors": ["Václav Tran", "Jakub Šmíd", "Ladislav Lenc", "Jean-Pierre Salmon", "Pavel Král"], "title": "Large Language Models for the Summarization of Czech Documents: From History to the Present", "comment": null, "summary": "Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.\n  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od Čerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.\n  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18889", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18889", "abs": "https://arxiv.org/abs/2511.18889", "authors": ["Jingqian Zhao", "Bingbing Wang", "Geng Tu", "Yice Zhang", "Qianlong Wang", "Bin Liang", "Jing Li", "Ruifeng Xu"], "title": "CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation", "comment": "ACL'25", "summary": "Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \\textbf{CoreEval}, a \\textbf{Co}ntamination-\\textbf{re}silient \\textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.", "AI": {"tldr": "CoreEval是一种抗数据污染的评估策略，通过从GDELT数据库中提取并整合实时知识来自动更新评估数据集，有效缓解大型语言模型（LLM）评估中因数据污染导致的性能过高估计问题。", "motivation": "数据污染导致LLM评估不公平，模型在训练期间无意中接触到测试数据。现有方法（修改或生成新数据集）无法完全消除模型预存知识或保持原始数据集的语义复杂性，因此不能确保抗污染评估。", "method": "CoreEval首先从原始数据中提取实体关系，然后利用GDELT数据库检索相关且最新的知识。检索到的知识被重新语境化并与原始数据整合，经过提炼和重构以确保语义连贯性和任务相关性。最后，采用强大的数据反射机制迭代验证和完善标签，确保更新数据集与原始数据集的一致性。", "result": "在更新后的数据集上进行的大量实验验证了CoreEval的鲁棒性，证明其在缓解数据污染导致的性能过高估计方面是有效的。", "conclusion": "CoreEval提供了一种有效的抗污染评估策略，通过自动更新数据以整合实时知识，成功解决了LLM评估中的数据污染问题，提高了评估的公平性和准确性。"}}
{"id": "2511.19315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19315", "abs": "https://arxiv.org/abs/2511.19315", "authors": ["Weiliang Tang", "Jialin Gao", "Jia-Hui Pan", "Gang Wang", "Li Erran Li", "Yunhui Liu", "Mingyu Ding", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "Rethinking Intermediate Representation for VLM-based Robot Manipulation", "comment": null, "summary": "Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.", "AI": {"tldr": "本文提出了一种名为SEAM的语义组装表示，通过将机器人操作的中间表示分解为词汇和语法，解决了视觉语言模型（VLM）在机器人操作中可理解性和泛化性之间的权衡问题。SEAM结合了新的开放词汇分割范式，实现了卓越的性能、推理速度和泛化能力。", "motivation": "在机器人操作中，使用视觉语言模型（VLM）将人类指令转换为可解析的中间表示时，常常需要在VLM的可理解性（VLM-comprehensibility）和泛化性（generalizability）之间进行权衡。", "method": "1. 设计了受上下文无关文法启发的语义组装表示（SEAM），将中间表示分解为语义丰富的操作词汇和VLM友好的语法。2. 提出了一种新的开放词汇分割范式，结合检索增强的少样本学习策略，以最短的推理时间定位精细对象部件。3. 制定了新的动作泛化性和VLM可理解性指标。", "result": "1. SEAM实现了简洁的词汇和VLM友好的语法，能够处理多样化的未见任务。2. 新的开放词汇分割范式在所有现有并行工作中实现了最短的推理时间。3. 在动作泛化性和VLM可理解性两方面，SEAM展示了优于主流表示的性能。4. 广泛的真实世界实验表明，SEAM在各种设置和任务下都达到了SOTA性能。", "conclusion": "SEAM通过分解中间表示并引入新的分割范式，成功解决了VLM在机器人操作中可理解性和泛化性之间的权衡问题，显著提升了机器人在多样化任务中的操作能力和效率，实现了最先进的性能。"}}
{"id": "2511.18852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18852", "abs": "https://arxiv.org/abs/2511.18852", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "title": "FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models", "comment": null, "summary": "Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18864", "abs": "https://arxiv.org/abs/2511.18864", "authors": ["Yang Xiang", "Yixin Ji", "Juntao Li", "Min Zhang"], "title": "Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models", "comment": "Under Review", "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.", "AI": {"tldr": "本文首次研究大型推理模型（LRMs）的剪枝，发现现有方法效果不佳。通过使用自生成的具有挑战性和中等长度的推理数据进行校准，可以显著提高剪枝性能，并提出了一种选择性自生成推理（SSGR）数据构建策略。", "motivation": "大型推理模型（LRMs）在复杂推理任务上表现出色，但其长链式思维推理过程导致高昂的推理开销。剪枝是降低计算成本的有效方法，但目前主要集中在大型语言模型（LLMs）上，对LRMs的剪枝尚未探索，且直接应用现有技术效果不佳。", "method": "本文进行首次LRM剪枝的实证研究，并尝试使用自生成的推理数据进行校准。进一步探究了推理数据的难度和长度对剪枝结果的影响。基于这些发现，提出了一种选择性自生成推理（SSGR）数据构建策略，以提供有效的校准数据。", "result": "直接应用现有剪枝技术无法在LRMs上获得满意结果。使用自生成的推理数据进行校准可以显著提高剪枝性能。分析表明，具有挑战性且长度适中的自生成推理数据是理想的校准数据。实验结果验证，与通用剪枝方法相比，SSGR策略将剪枝后LRMs的推理能力提高了10%-13%。", "conclusion": "LRMs的剪枝需要专门的方法。利用自生成、具有挑战性且长度适中的推理数据作为校准数据，可以显著改善剪枝效果。所提出的SSGR策略能有效提升剪枝后LRMs的推理能力。"}}
{"id": "2511.17962", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17962", "abs": "https://arxiv.org/abs/2511.17962", "authors": ["Ziheng Jia", "Linhan Cao", "Jinliang Han", "Zicheng Zhang", "Jiaying Qian", "Jiarui Wang", "Zijian Chen", "Guangtao Zhai", "Xiongkuo Min"], "title": "VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment", "comment": null, "summary": "Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.\n  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.", "AI": {"tldr": "本文提出VITAL-Series大型多模态模型（LMM），通过以视觉编码器为中心的生成式预训练流程，构建了迄今为止最大的VQualA训练数据集，并采用多任务训练工作流，旨在提升视觉质量评估（VQualA）LMM的通用性、强大性和可迁移性。", "motivation": "现有的VQualA LMMs通常专注于单一任务并依赖全参数微调，这导致它们容易过拟合特定模态或任务类型，从而限制了其泛化能力和可迁移性，无法实现VQualA LMM所需的通用性、强大性和可迁移性。", "method": "本文提出一种以视觉编码器为中心的生成式预训练流程，开发了VITAL-Series LMMs。具体方法包括：1) 采用机器执行的标注审查范式，构建了超过4.5M的视觉-语言（VL）对，成为迄今最大的VQualA训练数据集。2) 采用多任务训练工作流，同时增强模型在图像和视频模态上的定量评分精度和质量解释能力。3) 基于视觉编码器，实现高效的模型动物园扩展。", "result": "本文构建了迄今为止最大的VQualA训练数据集（超过4.5M VL对）。模型动物园展现出强大的零样本性能，并且每个配对的解码器仅需使用不到预训练数据千分之一的少量数据进行快速预热，即可达到与完全训练模型相当的性能。", "conclusion": "本工作为推进VQualA基础LMMs奠定了基石，有效解决了现有VQualA LMMs在通用性、强大性和可迁移性方面的局限性。"}}
{"id": "2511.17945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17945", "abs": "https://arxiv.org/abs/2511.17945", "authors": ["Kaibin Wang", "Mingbao Lin"], "title": "Test-Time Temporal Sampling for Efficient MLLM Video Understanding", "comment": null, "summary": "Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\\sum_{i=1}^m α_i^2L^2)$, where $\\sum_{i=1}^m α_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18931", "abs": "https://arxiv.org/abs/2511.18931", "authors": ["Sahil Kale"], "title": "Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs", "comment": "10 pages, 8 figures", "summary": "Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.", "AI": {"tldr": "该研究评估了大型语言模型（LLM）集成网络搜索的效率和校准情况，发现搜索能提高准确性但模型仍存在过度自信和检索效率问题。", "motivation": "现代LLM集成了网络搜索以提供实时答案，但尚不清楚它们是否能有效校准，仅在真正需要时才使用搜索。", "method": "引入了一个基准测试，包含两个数据集：1）一个包含783个静态、时间锚定问题的数据集，用于测试模型是否基于内部低置信度调用搜索；2）一个包含288个动态、截止日期后查询的数据集，用于测试模型是否识别需要搜索并检索更新信息。评估了商业模型（如GPT-5-mini和Claude Haiku 4.5），且无法访问其内部状态或参数。", "result": "网络访问显著提高了GPT-5-mini和Claude Haiku 4.5在静态查询上的准确性，但置信度校准变差。在动态查询上，模型频繁调用搜索，但由于查询表述不佳，准确率仍低于70%。提高准确性的每次调用成本较低，但首次检索失败后收益递减。选择性调用有帮助，但模型在搜索后变得过度自信和不一致。", "conclusion": "内置网络搜索能显著提高事实准确性并可选择性调用，但模型仍然过度自信，在必要时跳过检索，并且在初始搜索查询表现不佳时会受挫。总的来说，内部网络搜索更适合作为低延迟验证层而非可靠的分析工具，仍有明显的改进空间。"}}
{"id": "2511.18082", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18082", "abs": "https://arxiv.org/abs/2511.18082", "authors": ["Wencheng Ye", "Tianshi Wang", "Lei Zhu", "Fengling Li", "Guoli Yang"], "title": "ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.", "AI": {"tldr": "ActDistill是一个通用的动作引导自蒸馏框架，旨在将现有视觉-语言-动作（VLA）模型的动作预测能力转移到轻量级模型，显著降低计算开销和推理延迟，从而实现高效的具身智能。", "motivation": "尽管最近的VLA模型在灵活性和泛化性方面表现出色，但其高计算开销和推理延迟限制了它们在机器人操作中的部署。", "method": "ActDistill框架利用动作先验来指导知识转移和模型压缩，实现面向动作的VLA模型效率。它使用一个训练好的VLA模型作为教师，并引入图结构封装策略来显式建模动作预测的层次演化。学生模型从图封装的教师模型中派生，配备动态路由器，根据动作预测需求自适应选择计算路径，并通过分层图信息监督进行指导。推理时，辅助组件被移除，学生模型仅执行动态路由的层。", "result": "实验表明，ActDistill在具身基准测试中实现了与全尺寸VLA模型相当或更优的性能，同时将计算量减少了50%以上，并带来了高达1.67倍的加速。", "conclusion": "ActDistill为高效的具身智能建立了一个通用范式，解决了VLA模型在机器人部署中的计算效率问题。"}}
{"id": "2511.17952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17952", "abs": "https://arxiv.org/abs/2511.17952", "authors": ["Liangyang Ouyang", "Yifei Huang", "Mingfang Zhang", "Caixin Kang", "Ryosuke Furuta", "Yoichi Sato"], "title": "Multi-speaker Attention Alignment for Multimodal Social Interaction", "comment": null, "summary": "Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19377", "abs": "https://arxiv.org/abs/2511.19377", "authors": ["Mamoon Aamir", "Mariyam Sattar", "Naveed Ur Rehman Junejo", "Aqsa Zafar Abbasi"], "title": "Deployment Dynamics and Optimization of Novel Space Antenna Deployable Mechanism", "comment": null, "summary": "Given the increasing need for large aperture antennas in space missions, the difficulty of fitting such structures into small launch vehicles has prompted the design of deployable antenna systems. The thesis introduces a new Triple Scissors Deployable Truss Mechanism (TSDTM) for space antenna missions. The new mechanism is to be stowed during launch and efficiently deploy in orbit, offering maximum aperture size while taking up minimal launch volume. The thesis covers the entire design process from geometric modeling, kinematic analysis with screw theory and Newtonian approaches, dynamic analysis by eigenvalue and simulation methods, and verification with SolidWorks. In addition, optimization routines were coded based on Support Vector Machines for material choice in LEO environments and machine learning method for geometric setup. The TSDTM presented has enhanced structural dynamics with good comparison between simulation and analytical predictions. The structure optimized proved highly accurate, with a deviation of just 1.94% between machine learning-predicted and simulated natural frequencies, demonstrating the potential of incorporating AI-based methods in space structural design.", "AI": {"tldr": "本论文提出了一种新型三剪式可展开桁架机构（TSDTM），用于空间天线任务，旨在解决大型天线在小型运载火箭中的部署问题，并结合AI方法进行设计与优化。", "motivation": "空间任务对大孔径天线需求增加，但将此类结构装入小型运载火箭存在困难，因此需要设计可展开天线系统以最小化发射体积并最大化在轨孔径。", "method": "引入了三剪式可展开桁架机构（TSDTM），并涵盖了从几何建模、基于螺旋理论和牛顿方法的运动学分析、通过特征值和仿真方法的动力学分析，到使用SolidWorks进行验证的完整设计过程。此外，还基于支持向量机为LEO环境下的材料选择编码了优化程序，并利用机器学习方法进行了几何设置。", "result": "所提出的TSDTM具有增强的结构动力学性能，仿真与分析预测之间具有良好的一致性。优化后的结构高度精确，机器学习预测的固有频率与仿真结果之间的偏差仅为1.94%。", "conclusion": "该研究展示了将AI方法（如机器学习）融入空间结构设计的巨大潜力，能够实现高精度的结构优化和性能预测。"}}
{"id": "2511.19433", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19433", "abs": "https://arxiv.org/abs/2511.19433", "authors": ["Dong Jing", "Gang Wang", "Jiaqi Liu", "Weiliang Tang", "Zelong Sun", "Yunchao Yao", "Zhenyu Wei", "Yunhui Liu", "Zhiwu Lu", "Mingyu Ding"], "title": "Mixture of Horizons in Action Chunking", "comment": "15 pages, 14 figures", "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons", "AI": {"tldr": "视觉-语言-动作（VLA）模型在机器人操作中对动作块长度（horizon）敏感，存在长短期权衡。本文提出“多视野混合”（MoH）策略，通过并行处理不同horizon的动作段并融合输出来结合长短期优势，显著提高了性能、泛化能力和推理效率，并在LIBERO上达到新的SOTA。", "motivation": "VLA模型在机器人操作中的性能对训练时使用的动作块长度（horizon）非常敏感。研究发现，长horizon能提供更强的全局预见性但降低了精细准确性，而短horizon能提高局部控制精度但难以应对长期任务。因此，固定选择单一horizon是次优的，存在固有的性能权衡。", "method": "本文提出一种“多视野混合”（MoH）策略来缓解这种权衡。MoH将动作块重新排列成几个具有不同horizon的片段，然后使用共享的动作Transformer并行处理这些片段，并通过一个轻量级的线性门融合它们的输出。", "result": "MoH策略具有三大优势：1) 它能在一个模型中共同利用长期预见性和短期精度，从而提高性能和对复杂任务的泛化能力。2) MoH是全注意力动作模块的即插即用方案，训练和推理开销极小。3) MoH支持动态推理，通过跨视野共识选择稳定的动作，在保持优越性能的同时，吞吐量比基线提高了2.5倍。广泛实验表明，MoH在模拟和真实世界任务中，对基于流的策略$π_0, π_{0.5}$和一步回归策略$π_{\text{reg}}$均带来了持续且显著的性能提升。值得注意的是，在混合任务设置下，结合MoH的$π_{0.5}$在仅3万次训练迭代后，在LIBERO上实现了99%的平均成功率，达到了新的SOTA。", "conclusion": "MoH策略成功解决了VLA模型中动作块长度选择的固有权衡问题，通过有效结合长短期优势，显著提升了机器人操作任务的性能、泛化能力和推理效率，并在多个基准测试中取得了领先成果。"}}
{"id": "2511.17964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17964", "abs": "https://arxiv.org/abs/2511.17964", "authors": ["Chenyang Yu", "Xuehu Liu", "Pingping Zhang", "Huchuan Lu"], "title": "X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification", "comment": "Accepted by AAAI2026. More modifications may be performed", "summary": "Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.", "AI": {"tldr": "本文提出了一种名为X-ReID的跨模态特征学习框架，用于视频可见光-红外行人重识别（VVI-ReID），旨在缩小模态差距并有效利用时空信息。", "motivation": "大规模视觉-语言模型（如CLIP）在检索任务中表现出色，但在VVI-ReID领域的潜力尚未充分挖掘。主要挑战在于缩小模态差距和有效利用视频序列中的时空信息。", "method": "本文提出了X-ReID框架。具体包括：1) 跨模态原型协作（CPC），用于对齐和整合不同模态的特征，以减少模态差异。2) 多粒度信息交互（MII），结合相邻帧的短期交互、跨帧的长期信息融合以及跨模态特征对齐，以增强时间建模并进一步缩小模态差距。最终，通过整合多粒度信息，获得鲁棒的序列级表示。", "result": "在两个大型VVI-ReID基准数据集（HITSZ-VCM和BUPTCampus）上的广泛实验表明，本文方法优于现有最先进的方法。", "conclusion": "X-ReID框架通过引入跨模态原型协作和多粒度信息交互，有效解决了VVI-ReID中的模态差距和时空信息利用问题，取得了显著的性能提升。"}}
{"id": "2511.18685", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18685", "abs": "https://arxiv.org/abs/2511.18685", "authors": ["Dayong Liu", "Chao Xu", "Weihong Chen", "Suyu Zhang", "Juncheng Wang", "Jiankang Deng", "Baigui Sun", "Yang Liu"], "title": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents.", "AI": {"tldr": "该研究引入CFG-Bench基准，旨在系统评估多模态大语言模型（MLLMs）在具身智能体中进行精细物理交互的能力。结果表明当前MLLMs在精细动作指令和高阶推理方面存在显著局限性，但通过在其数据上进行监督微调可显著提升具身任务性能。", "motivation": "现有具身智能体基准主要侧重于高层规划或空间推理，未能充分探索物理交互所需的精细动作智能，导致评估存在空白。", "method": "引入CFG-Bench基准，包含1,368个精选视频和19,562个三模态问答对。该基准旨在评估模型在四个认知能力上的表现：1) 物理交互，2) 时序因果关系，3) 意图理解，以及4) 评估判断。", "result": "领先的MLLMs在生成精细物理交互指令方面表现不佳，并且在高阶推理（意图和评估）方面存在深刻局限性。然而，在CFG-Bench数据上进行的监督微调（SFT）能显著提高MLLMs在现有具身基准上的性能。", "conclusion": "MLLMs在具身智能体所需的精细动作智能和高阶推理方面存在明显不足。CFG-Bench揭示了这些局限性，并提供了开发更强大、更具基础性具身智能体的方向，通过特定数据微调可有效提升其能力。"}}
{"id": "2511.17958", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17958", "abs": "https://arxiv.org/abs/2511.17958", "authors": ["Yulong Shi", "Jiapeng Li", "Lin Qi"], "title": "HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation", "comment": "Accepted by The 36th British Machine Vision Conference (BMVC 2025)", "summary": "Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17855", "abs": "https://arxiv.org/abs/2511.17855", "authors": ["Jordan Abi Nader", "David Lee", "Nathaniel Dennler", "Andreea Bobu"], "title": "QuickLAP: Quick Language-Action Preference Learning for Autonomous Driving Agents", "comment": null, "summary": "Robots must learn from both what people do and what they say, but either modality alone is often incomplete: physical corrections are grounded but ambiguous in intent, while language expresses high-level goals but lacks physical grounding. We introduce QuickLAP: Quick Language-Action Preference learning, a Bayesian framework that fuses physical and language feedback to infer reward functions in real time. Our key insight is to treat language as a probabilistic observation over the user's latent preferences, clarifying which reward features matter and how physical corrections should be interpreted. QuickLAP uses Large Language Models (LLMs) to extract reward feature attention masks and preference shifts from free-form utterances, which it integrates with physical feedback in a closed-form update rule. This enables fast, real-time, and robust reward learning that handles ambiguous feedback. In a semi-autonomous driving simulator, QuickLAP reduces reward learning error by over 70% compared to physical-only and heuristic multimodal baselines. A 15-participant user study further validates our approach: participants found QuickLAP significantly more understandable and collaborative, and preferred its learned behavior over baselines. Code is available at https://github.com/MIT-CLEAR-Lab/QuickLAP.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17965", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.17965", "abs": "https://arxiv.org/abs/2511.17965", "authors": ["Yangyang Liu", "Yuhao Wang", "Pingping Zhang"], "title": "Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification", "comment": "Accepted by AAAI2026. More modifications may be performed", "summary": "Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.", "AI": {"tldr": "本文提出了一种名为Signal的新型选择性交互和全局-局部对齐框架，用于多模态目标重识别，旨在解决背景干扰和多模态一致性对齐问题，通过选择性交互和多尺度对齐提取更具判别力的特征。", "motivation": "现有多模态目标重识别方法主要关注多模态特征融合，但忽略了背景干扰。此外，当前的多模态融合方法通常侧重于模态对齐，但在多模态一致性对齐方面存在不足。", "method": "本文提出了一个名为Signal的框架。具体而言，它包含：1) 选择性交互模块（SIM），用于选择具有模态内和模态间信息的重要块级token，并使其与类别token交互，生成更具判别力的特征。2) 全局对齐模块（GAM），通过最小化Gramian空间中3D多面体的体积，同时对齐多模态特征。3) 局部对齐模块（LAM），以位移感知的方式对齐局部特征。", "result": "在RGBNT201、RGBNT100、MSVR310三个多模态目标重识别基准上的广泛实验验证了所提方法的有效性。", "conclusion": "所提出的Signal框架能够提取更具判别力的特征，有效解决了多模态目标重识别中的背景干扰和多模态一致性对齐问题，从而提升了重识别性能。"}}
{"id": "2511.18891", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18891", "abs": "https://arxiv.org/abs/2511.18891", "authors": ["Adam Rychert", "Gasper Spagnolo", "Evgenii Posashkov"], "title": "Reproducibility Study of Large Language Model Bayesian Optimization", "comment": "7 pages, 8 figures. Reproducibility study of the LLAMBO framework (ICLR 2024). Code: https://github.com/spagnoloG/llambo-reproducibility", "summary": "In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.\n  Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.\n  Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.", "AI": {"tldr": "本研究是一项关于LLAMBO框架的复现性研究，使用Llama 3.1 70B替代GPT-3.5。结果确认了LLAMBO的主要主张，并证明其架构在更换大型语言模型骨干时仍保持鲁棒性和有效性。", "motivation": "重新审视并复现Daxberger等人(2024)提出的LLAMBO框架，使用开源的Llama 3.1 70B模型替代原有的GPT-3.5，以验证其核心主张并评估其在不同大型语言模型骨干下的鲁棒性。", "method": "复制了LLAMBO在Bayesmark和HPOBench上的核心实验，遵循原始评估协议。将所有文本编码组件中的GPT-3.5替换为开源的Llama 3.1 70B模型。进行了上下文热启动、判别性代理性能、文本上下文消融以及不同大小骨干模型（Gemma 27B, Llama 3.1 8B）的实验。", "result": "研究结果广泛证实了LLAMBO的主要主张。通过文本问题和超参数描述进行的上下文热启动显著改善了早期遗憾行为并降低了运行间的方差。LLAMBO的判别性代理作为纯粹的单任务回归器弱于GP或SMAC，但受益于语言模型诱导的跨任务语义先验。移除文本上下文会显著降低预测准确性和校准。LLAMBO的候选采样器比TPE或随机采样生成更高质量和更多样化的提案。较小的骨干模型（Gemma 27B, Llama 3.1 8B）导致不稳定或无效的预测，表明其容量不足以实现可靠的代理行为。", "conclusion": "本研究表明，LLAMBO架构对于更换大型语言模型骨干具有鲁棒性，并且在使用Llama 3.1 70B实例化时仍然有效。"}}
{"id": "2511.17979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17979", "abs": "https://arxiv.org/abs/2511.17979", "authors": ["Bo Yin", "Xiaobin Hu", "Xingyu Zhou", "Peng-Tao Jiang", "Yue Liao", "Junwei Zhu", "Jiangning Zhang", "Ying Tai", "Chengjie Wang", "Shuicheng Yan"], "title": "FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning", "comment": null, "summary": "Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.", "AI": {"tldr": "本文提出了FeRA，一个基于频率能量机制的扩散模型微调框架，通过频率能量指标、软频率路由器和频率能量一致性正则化，实现高效、稳定且兼容的模型适应。", "motivation": "扩散模型在生成建模中取得了显著成功，但如何有效地将大型预训练模型适应新任务仍然是一个挑战。", "method": "FeRA是一个频率驱动的微调框架，它将参数更新与扩散模型固有的频率能量演进对齐。该框架包含三个协同组件：(i) 紧凑的频率能量指标，用于表征潜在带状能量分布；(ii) 软频率路由器，自适应地融合多个频率特定的适配器专家，其路由在训练和推理时均进行，推理时由潜在频率能量动态决定；(iii) 频率能量一致性正则化，稳定扩散优化并确保各频带间适应性的一致性。该方法与基于适配器的微调方案无缝集成，并适用于不同的扩散骨干网络和分辨率。", "result": "通过将适应过程与频率能量机制对齐，FeRA提供了一个简单、稳定且兼容的范式，用于有效和鲁棒的扩散模型适应。", "conclusion": "FeRA框架通过利用扩散模型去噪过程中的频率能量机制，成功解决了大型预训练扩散模型适应新任务的挑战，实现了高效、稳定且兼容的模型适应。"}}
{"id": "2511.18937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18937", "abs": "https://arxiv.org/abs/2511.18937", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials", "comment": "13 pages, 3 tables, 5 figures", "summary": "We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.", "AI": {"tldr": "本文提出了一种图形化、基于知识的方法（Safeterm），通过增强MedDRA来审查临床试验中的治疗相关不良事件（AE），以提高信号检测的清晰度、效率和准确性。", "motivation": "当前临床试验中对不良事件的解释需要更高的清晰度、效率和准确性。研究旨在通过引入医学知识层来改进MedDRA，从而更好地识别和分析不良事件信号。", "method": "该方法通过向MedDRA添加一个隐藏的医学知识层（Safeterm），以2D图谱形式捕获术语间的语义关系。利用此层，AE优选术语可以自动重组为相似性聚类，并量化其与试验疾病的关联。Safeterm图谱在线可用，并与ClinicalTrials.gov的AE发生率汇总表连接。通过收缩发生率比计算治疗特异性不成比例指标，并通过精确加权聚合得出聚类级别的EBGM值。提供两种可视化输出：显示AE发生率的语义图和用于快速信号检测的预期性与不成比例图。", "result": "将该自动化方法应用于三个历史试验，结果清晰地恢复了所有预期的安全信号。", "conclusion": "通过医学知识层（Safeterm）增强MedDRA，显著提高了临床试验中不良事件解释的清晰度、效率和准确性。"}}
{"id": "2511.18934", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.18934", "abs": "https://arxiv.org/abs/2511.18934", "authors": ["Yuchen Ji", "Bo Xu", "Jie Shi", "Jiaqing Liang", "Deqing Yang", "Yu Mao", "Hai Chen", "Yanghua Xiao"], "title": "Skeletons Matter: Dynamic Data Augmentation for Text-to-Query", "comment": "Accepted at EMNLP 2025", "summary": "The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17973", "abs": "https://arxiv.org/abs/2511.17973", "authors": ["Hiroto Honda"], "title": "Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning", "comment": "Accepted to WACV 2026", "summary": "Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.", "AI": {"tldr": "本文提出了一种名为对抗性伪重放（APR）的方法，通过对新任务图像进行对抗性攻击来在线合成伪重放图像，以解决无样本类别增量学习（EFCIL）中的灾难性遗忘和语义漂移问题，在EFCIL基准测试中取得了最先进的性能。", "motivation": "无样本类别增量学习（EFCIL）面临在学习新类别的同时保留旧知识的挑战，由于存储限制或隐私问题，无法存储旧任务图像。这导致了显著的塑性-稳定性困境（学习新任务与灾难性遗忘），尤其是在没有旧任务图像的情况下。", "method": "本文引入了对抗性伪重放（APR）方法。该方法通过对新任务图像进行对抗性攻击来在线合成伪重放图像，攻击目标是增强的旧类别均值原型。生成的图像用于知识蒸馏以防止语义漂移。此外，APR通过在伪重放样本上学习一个转换矩阵，校准协方差矩阵以补偿每个任务后的语义漂移。", "result": "APR方法成功地协调了稳定性和可塑性，在标准EFCIL基准测试的挑战性冷启动设置中取得了最先进的性能。", "conclusion": "所提出的对抗性伪重放（APR）方法通过在线合成伪重放图像和校准协方差矩阵，有效地解决了EFCIL中的灾难性遗忘和语义漂移问题，实现了稳定性与可塑性的平衡，并在相关基准上表现出色。"}}
{"id": "2511.19097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19097", "abs": "https://arxiv.org/abs/2511.19097", "authors": ["Ziyuan Gao", "Di Liang", "Xianjie Wu", "Philippe Morel", "Minlong Peng"], "title": "DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF", "comment": "Accepted by AAAI 2026", "summary": "Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding has O(n) time complexity. This makes real-time deployment impractical for complex reasoning tasks. We present DeCoRL (Decoupled Reasoning Chains via Coordinated Reinforcement Learning), a novel framework that transforms reasoning from sequential processing into collaborative modular orchestration. DeCoRL trains lightweight specialized models to generate reasoning sub-steps concurrently, eliminating sequential bottlenecks through parallel processing. To enable precise error attribution, the framework designs modular reward functions that score each sub-step independently. Cascaded DRPO optimization then coordinates these rewards while preserving inter-step dependencies. Comprehensive evaluation demonstrates state-of-the-art results across RM-Bench, RMB, and RewardBench, outperforming existing methods including large-scale models. DeCoRL delivers 3.8 times faster inference while maintaining superior solution quality and offers a 22.7\\% improvement in interpretability through explicit reward attribution. These advancements, combined with a 72.4\\% reduction in energy consumption and a 68\\% increase in throughput, make real-time deployment of complex reasoning systems a reality.", "AI": {"tldr": "DeCoRL是一个新型强化学习框架，通过并行模块化处理和解耦奖励解决了现有CoT推理方法中奖励信号不分化和顺序解码效率低的问题，显著提升了推理速度、可解释性和性能。", "motivation": "现有用于思维链推理的强化学习方法存在两个主要限制：一是它们作为整体黑盒运作，提供不分化的奖励信号，掩盖了每个步骤的贡献并阻碍了错误诊断；二是顺序解码的时间复杂度为O(n)，使得复杂推理任务的实时部署不切实际。", "method": "DeCoRL（通过协调强化学习解耦推理链）将推理从顺序处理转变为协作模块化编排。它训练轻量级专业模型并行生成推理子步骤，通过并行处理消除顺序瓶颈。为实现精确的错误归因，该框架设计了模块化奖励函数，独立评估每个子步骤。随后，通过Cascaded DRPO优化协调这些奖励，同时保留步骤间的依赖关系。", "result": "DeCoRL在RM-Bench、RMB和RewardBench上取得了最先进的结果，优于包括大型模型在内的现有方法。它实现了3.8倍的推理速度提升，同时保持了卓越的解决方案质量，并通过明确的奖励归因将可解释性提高了22.7%。此外，它还减少了72.4%的能耗并增加了68%的吞吐量。", "conclusion": "DeCoRL的这些进步，结合其卓越的性能、速度、可解释性、能耗降低和吞吐量提升，使得复杂推理系统的实时部署成为可能。"}}
{"id": "2511.18570", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18570", "abs": "https://arxiv.org/abs/2511.18570", "authors": ["Samarth Chopra", "Jing Liang", "Gershom Seneviratne", "Dinesh Manocha"], "title": "PhysGS: Bayesian-Inferred Gaussian Splatting for Physical Property Estimation", "comment": "Submitted to CVPR 2026", "summary": "Understanding physical properties such as friction, stiffness, hardness, and material composition is essential for enabling robots to interact safely and effectively with their surroundings. However, existing 3D reconstruction methods focus on geometry and appearance and cannot infer these underlying physical properties. We present PhysGS, a Bayesian-inferred extension of 3D Gaussian Splatting that estimates dense, per-point physical properties from visual cues and vision--language priors. We formulate property estimation as Bayesian inference over Gaussian splats, where material and property beliefs are iteratively refined as new observations arrive. PhysGS also models aleatoric and epistemic uncertainties, enabling uncertainty-aware object and scene interpretation. Across object-scale (ABO-500), indoor, and outdoor real-world datasets, PhysGS improves accuracy of the mass estimation by up to 22.8%, reduces Shore hardness error by up to 61.2%, and lowers kinetic friction error by up to 18.1% compared to deterministic baselines. Our results demonstrate that PhysGS unifies 3D reconstruction, uncertainty modeling, and physical reasoning in a single, spatially continuous framework for dense physical property estimation. Additional results are available at https://samchopra2003.github.io/physgs.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17967", "abs": "https://arxiv.org/abs/2511.17967", "authors": ["Hao Li", "Yuhao Wang", "Xiantao Hu", "Wenning Hao", "Pingping Zhang", "Dong Wang", "Huchuan Lu"], "title": "CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking", "comment": "Accepted by AAAI2026. More modifications may be performed", "summary": "RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.17988", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.17988", "abs": "https://arxiv.org/abs/2511.17988", "authors": ["Haodong Chen", "Xianfei Han", "Qwen"], "title": "HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation", "comment": null, "summary": "Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.", "AI": {"tldr": "本文提出HyM-UNet，一种结合CNN和Mamba的混合架构，用于医学图像分割，旨在融合局部特征提取和高效全局建模能力，并在ISIC 2018数据集上取得了SOTA性能。", "motivation": "卷积神经网络（CNN）受限于局部感受野，难以捕捉复杂的全局解剖结构，而准确的器官和病灶分割是计算机辅助诊断的关键前提。因此，需要一种方法来有效结合局部细节和全局上下文信息。", "method": "本文提出了HyM-UNet混合架构。具体方法包括：1) 设计了一个分层编码器，在浅层使用卷积模块保留高频纹理细节，在深层引入Visual Mamba模块以线性复杂度捕获长距离语义依赖。2) 提出Mamba引导融合跳跃连接（MGF-Skip），利用深层语义特征作为门控信号，动态抑制浅层特征中的背景噪声，从而增强对模糊边界的感知。", "result": "在公共基准数据集ISIC 2018上进行的广泛实验表明，HyM-UNet在Dice系数和IoU方面显著优于现有的最先进方法，同时保持了更低的参数量和推理延迟。", "conclusion": "所提出的HyM-UNet方法在处理具有复杂形状和尺度变化的医学分割任务中表现出有效性和鲁棒性，验证了其在结合局部与全局信息方面的优势。"}}
{"id": "2511.17993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17993", "abs": "https://arxiv.org/abs/2511.17993", "authors": ["Jiayu Wang", "Haoyu Bian", "Haoran Sun", "Shaoning Zeng"], "title": "SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining", "comment": "12 pages, 7 figures, Published in AAAI 2026", "summary": "Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.", "AI": {"tldr": "SD-PSFNet是一种新颖的图像去雨方法，它借鉴多阶段图像恢复理念，结合点扩散函数（PSF）机制和动态物理建模，通过顺序特征融合实现从粗到细的去雨和细节恢复，并在多个数据集上取得了最先进的性能。", "motivation": "图像去雨对视觉应用至关重要，但由于雨滴复杂的多尺度物理特性及其与场景的耦合，去雨任务面临巨大挑战。", "method": "提出SD-PSFNet，采用三级级联的顺序恢复架构，允许对降解过程进行多次动态评估和优化。网络利用学习到的PSF机制动态模拟雨纹光学特性，实现雨景分离，并通过每个阶段的新型PSF组件逐步增强输出。此外，SD-PSFNet还整合了自适应门控融合，以实现最佳的跨阶段特征整合，从而实现从粗略去雨到精细细节恢复的顺序优化。", "result": "SD-PSFNet在Rain100H (33.12dB/0.9371)、RealRain-1k-L (42.28dB/0.9872) 和 RealRain-1k-H (41.08dB/0.9838) 数据集上取得了最先进的PSNR/SSIM指标。", "conclusion": "SD-PSFNet在复杂场景和密集降雨条件下展现出卓越的能力，为图像去雨提供了一种新的物理感知方法。"}}
{"id": "2511.19078", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19078", "abs": "https://arxiv.org/abs/2511.19078", "authors": ["Yutong Li", "Yitian Zhou", "Xudong Wang", "GuoChen", "Caiyan Qin"], "title": "GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.", "AI": {"tldr": "GraphMind是一个动态图基框架，它将图神经网络(GNN)与大型语言模型(LLM)结合，通过构建异构演化图来动态表示推理状态，从而在多步推理中实现上下文感知的定理选择和迭代结论生成。", "motivation": "现有的大型语言模型在多步推理（如数学证明）中，缺乏明确且动态的机制来结构化地表示和演化中间推理状态。这限制了它们进行上下文感知定理选择和迭代结论生成的能力。", "method": "GraphMind将推理过程建模为一个异构演化图，其中节点代表条件、定理和结论，边捕捉节点间的逻辑依赖关系。它通过GNN编码当前推理状态，并利用语义匹配进行定理选择。该框架以闭环方式实现上下文感知、可解释和结构化的推理。", "result": "在各种问答(QA)数据集上的实验表明，GraphMind方法在多步推理中取得了持续的性能提升，并显著优于现有基线。", "conclusion": "GraphMind提供了一种有效且泛化性强的方法，通过动态图结构整合GNN和LLM，显著提高了大型语言模型在多步推理中的上下文感知、可解释和结构化推理能力。"}}
{"id": "2511.18005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18005", "abs": "https://arxiv.org/abs/2511.18005", "authors": ["Shengyuan Wang", "Zhiheng Zheng", "Yu Shang", "Lixuan He", "Yangcheng Yu", "Fan Hangyu", "Jie Feng", "Qingmin Liao", "Yong Li"], "title": "RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale", "comment": "The code will be made publicly available soon at: https://github.com/tsinghua-fib-lab/RAISECity", "summary": "City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \\textbf{R}eality-\\textbf{A}ligned \\textbf{I}ntelligent \\textbf{S}ynthesis \\textbf{E}ngine that creates detailed, \\textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.", "AI": {"tldr": "RAISECity是一个现实对齐的智能合成引擎，利用代理框架和多模态基础工具，克服现有挑战，生成高质量、高保真、可扩展的城市级3D世界。", "motivation": "现有城市级3D生成方法在质量、保真度和可扩展性方面面临重大挑战，这阻碍了具身智能和世界模型的发展。", "method": "本文提出了RAISECity，一个代理框架。它利用多样化的多模态基础工具获取真实世界知识，维护鲁棒的中间表示，并构建复杂的3D场景。该代理设计包含动态数据处理、迭代自我反思和细化，以及高级多模态工具调用，以最小化累积误差并提升性能。", "result": "RAISECity在真实世界对齐、形状精度、纹理保真度和美学水平方面表现出卓越性能，在整体感知质量上对现有基线方法取得了超过90%的胜率。", "conclusion": "RAISECity结合了3D质量、现实对齐、可扩展性以及与计算机图形管线的无缝兼容性，使其成为沉浸式媒体、具身智能和世界模型应用的一个有前景的基础。"}}
{"id": "2511.19063", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19063", "abs": "https://arxiv.org/abs/2511.19063", "authors": ["Hayami Takahashi", "Kensuke Takahashi"], "title": "Logic of Montage", "comment": null, "summary": "In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form \"Effect of Contradictory Structure.\" \"Effect of Contradictory Structure\" is not static but dynamic. Effect in \"Effect of Contradictory Structure\" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, \"Effect of Contradictory Structure\" can be overlapped with each other. This overlapping operation is called \"montage.\" A broader \"Structure\" that includes related \"Effect of Contradictory Structure\" and \"Effect of Structure\" are set up. Montage produces \"Effect of Structure\". In montage, it is necessary to set something like \"strength,\" so we adopted Deleuze and Deleuze/Guattari's word \"intensity\" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of \"intensity\" through Austin's use of the word \"force.\" \"Effect of Structure\" process is demonstrated using the example of proceeding to the next level of education.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19198", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19198", "abs": "https://arxiv.org/abs/2511.19198", "authors": ["Ann-Sophia Müller", "Moonkwang Jeong", "Meng Zhang", "Jiyuan Tian", "Arkadiusz Miernik", "Stefanie Speidel", "Tian Qiu"], "title": "Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks", "comment": "6 pages, 4 figures, 1 table, IEEE International Conference on Intelligent Robots and Systems (IROS)", "summary": "Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18874", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.RO", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18874", "abs": "https://arxiv.org/abs/2511.18874", "authors": ["Yuzhi Chen", "Yuanchang Xie", "Lei Zhao", "Pan Liu", "Yajie Zou", "Chen Wang"], "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction", "comment": null, "summary": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19083", "abs": "https://arxiv.org/abs/2511.19083", "authors": ["Wenxuan Mu", "Jinzhong Ning", "Di Zhao", "Yijia Zhang"], "title": "A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis", "comment": "This paper has been accepted by AAAI 2026 (Main Technical Track)", "summary": "In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.", "AI": {"tldr": "KDR-Agent是一个多智能体框架，通过整合知识检索、消歧和反思分析，解决了低资源多领域情境下上下文学习（ICL）命名实体识别（NER）的现有局限性，显著优于基线模型。", "motivation": "现有的基于ICL的NER方法存在三个主要局限性：1) 依赖动态检索标注示例，但在标注数据稀缺时存在问题；2) 由于LLM内部领域知识不足，泛化到未见领域的能力有限；3) 未能整合外部知识或解决实体歧义。", "method": "提出了KDR-Agent，一个多智能体框架，用于多领域低资源上下文NER。它利用自然语言类型定义和静态的实体级对比示例，减少对大量标注语料的依赖。一个中央规划器协调专门的智能体来：i) 从维基百科检索领域特定提及的事实知识；ii) 通过上下文推理解决歧义实体；iii) 通过结构化自我评估反思并纠正模型预测。", "result": "在来自五个领域的十个数据集上进行的实验表明，KDR-Agent在多个LLM骨干模型上，显著优于现有的零样本和少样本ICL基线方法。", "conclusion": "KDR-Agent通过结合知识检索、消歧和反思分析，有效解决了低资源多领域ICL-NER的挑战，并在性能上取得了显著提升。"}}
{"id": "2511.17986", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17986", "abs": "https://arxiv.org/abs/2511.17986", "authors": ["Lun Huang", "You Xie", "Hongyi Xu", "Tianpei Gu", "Chenxu Zhang", "Guoxian Song", "Zenan Li", "Xiaochen Zhao", "Linjie Luo", "Guillermo Sapiro"], "title": "Plan-X: Instruct Video Generation via Semantic Planning", "comment": "The project page is at https://byteaigc.github.io/Plan-X", "summary": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19120", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19120", "abs": "https://arxiv.org/abs/2511.19120", "authors": ["Phong Le", "Mees Lindeman", "Raquel G. Alhama"], "title": "On the Optimality of Discrete Object Naming: a Kinship Case Study", "comment": null, "summary": "The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.", "AI": {"tldr": "本文提出一个信息论框架，证明了自然语言命名系统中信息量与复杂性之间的最佳权衡，当且仅当听者的解码器与说话者的贝叶斯解码器一致时才能实现，并通过经验验证了这一点。", "motivation": "以往关于自然语言命名系统平衡信息量与复杂性的研究存在局限性：一是假设听者是最佳的，二是假设语言间存在普遍的交流需求。本文旨在解决这些简化问题。", "method": "引入了一个针对离散对象命名系统的信息论框架；通过理论证明确定最佳权衡的条件；采用新兴通信中的指称博弈设置，并以亲属关系语义领域为例，进行实证验证。", "result": "研究证明，当且仅当听者的解码器等同于说话者的贝叶斯解码器时，才能实现命名系统中的最佳权衡。此外，这种最优性不仅在理论上可实现，而且在学习到的通信系统中也得到了经验性的体现。", "conclusion": "自然语言命名系统中信息量与复杂性之间的最佳权衡是可实现且可观察的，其关键在于听者解码器与说话者贝叶斯解码器的一致性。"}}
{"id": "2511.19118", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19118", "abs": "https://arxiv.org/abs/2511.19118", "authors": ["Juan-José Guzmán-Landa", "Jesús Vázquez-Osorio", "Juan-Manuel Torres-Moreno", "Ligia Quintana Torres", "Miguel Figueroa-Saavedra", "Martha-Lorena Avendaño-Garrido", "Graham Ranger", "Patricia Velázquez-Morales", "Gerardo Eugenio Sierra Martínez"], "title": "A symbolic Perl algorithm for the unification of Nahuatl word spellings", "comment": "MICAI 2025, LNAI 16221, pp. 141-154, 2026. 10 pages, 4 Figures, 8 Tables", "summary": "In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $π$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18007", "abs": "https://arxiv.org/abs/2511.18007", "authors": ["Siteng Ma", "Honghui Du", "Prateek Mathur", "Brendan S. Kelly", "Ronan P. Killeen", "Aonghus Lawlor", "Ruihai Dong"], "title": "Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging", "comment": "This paper has been accepted at International Joint Conference on Neural Networks (IJCNN) 2025", "summary": "Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.", "AI": {"tldr": "本研究提出了一种名为LMI-AL的新型深度主动学习框架，专门用于纵向医学图像中的变化检测，通过选择最具信息量的样本进行标注，显著减少了标注成本。", "motivation": "纵向医学图像的变化检测需要大量准确标注的数据，但其标注成本高昂且耗时，因为需要跨多个时间点进行标注，且细微病变和变化容易被遗漏。现有深度主动学习（DAL）主要关注分类和分割等静态任务，无法直接应用于涉及识别多图像间细微差异的变化检测任务。", "method": "本研究提出了纵向医学图像主动学习（LMI-AL）框架。它通过配对并差异化基线和随访3D图像中的所有2D切片，然后迭代地选择最具信息量的图像对进行标注。利用深度主动学习（DAL）训练深度学习模型，以最少的 K人工标注实现变化检测。", "result": "实验结果表明，LMI-AL在标注数据少于8%的情况下，可以达到与在完全标注数据集上训练的模型相当的性能。", "conclusion": "LMI-AL框架能够有效降低纵向医学图像变化检测任务的标注成本，同时保持高检测性能。研究还提供了详细的性能分析，为未来研究提供指导。"}}
{"id": "2511.19122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19122", "abs": "https://arxiv.org/abs/2511.19122", "authors": ["Yaping Chai", "Haoran Xie", "Joe S. Qin"], "title": "Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis", "comment": "8 pages, 4 figures", "summary": "Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.", "AI": {"tldr": "本文提出了一种情感增强的多任务方面类别情感分析（ACSA）框架，利用大型语言模型（LLM）共同学习情感极性和基于Ekman基本情绪的类别特定情绪，并通过VAD维度框架进行情绪细化，显著提高了ACSA性能。", "motivation": "现有ACSA方法主要关注情感极性，忽略了构成情感表达的潜在情绪维度。这种局限性阻碍了模型捕获针对特定方面类别的细粒度情感信号的能力。", "method": "引入了一种新颖的情感增强多任务ACSA框架，共同学习情感极性和基于Ekman六种基本情绪的类别特定情绪。该方法利用LLM的生成能力，为每个方面类别生成情感描述。此外，为确保生成情感的准确性和一致性，引入了基于效价-唤醒-支配（VAD）维度框架的情绪细化机制，对与VAD坐标不一致的情绪进行结构化的LLM细化策略重新标注。", "result": "实验结果表明，该方法在所有基准数据集上均显著优于强大的基线模型。", "conclusion": "研究强调了将情感维度整合到ACSA中的有效性。"}}
{"id": "2511.19232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19232", "abs": "https://arxiv.org/abs/2511.19232", "authors": ["Christos-Nikolaos Zacharopoulos", "Revekka Kyriakoglou"], "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations", "comment": "Accepted at AICS2025", "summary": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.", "AI": {"tldr": "本研究评估了phi-2模型如何以及在何处检测到语义异常。结果显示，模型在中间层开始有效检测语义异常，其表示空间经历了一个先扩张后收缩的过程，这与人类阅读中语义异常检测的时序相似。", "motivation": "研究旨在探究Transformer模型（具体为phi-2）如何以及在模型的哪个层次（“何处”）识别出句子中出现的语义不一致或“脱轨”现象。", "method": "研究评估了因果语言模型phi-2，使用了一个精心策划的语料库，其中包含语义合理和不合理的句子结尾。分析侧重于模型每个隐藏层的状态。采用了两种互补的探测方法：1. 使用线性探针进行逐层检测，以区分合理与不合理的结尾。2. 检查编码语义异常的有效维度。", "result": "1. 线性探针检测显示，在模型底层约三分之一的层中，线性解码器难以区分合理与不合理的结尾。然而，在中层块中，其准确性急剧提高，并在顶层之前达到峰值。2. 有效维度分析表明，语义异常最初会拓宽表示子空间，随后在中层瓶颈之后发生收缩。这可能表明一个探索阶段过渡到快速整合阶段。", "conclusion": "这些结果表明，Transformer模型在其中间层检测到语义异常，这与经典心理语言学中人类阅读的发现相符。在人类阅读中，语义异常通常在句法解析之后，在在线处理序列的后期才被检测到。"}}
{"id": "2511.18011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18011", "abs": "https://arxiv.org/abs/2511.18011", "authors": ["Jun Zhang", "Jie Feng", "Long Chen", "Junhui Wang", "Zhicheng Liu", "Depeng Jin", "Yong Li"], "title": "RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios", "comment": "The code and data are publicly available at: https://github.com/tsinghua-fib-lab/RoadBench", "summary": "Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19131", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19131", "abs": "https://arxiv.org/abs/2511.19131", "authors": ["Zijian Wang", "Yanxiang Ma", "Chang Xu"], "title": "Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization", "comment": "AAAI2026", "summary": "Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19399", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19399", "abs": "https://arxiv.org/abs/2511.19399", "authors": ["Rulin Shao", "Akari Asai", "Shannon Zejiang Shen", "Hamish Ivison", "Varsha Kishore", "Jingming Zhuo", "Xinran Zhao", "Molly Park", "Samuel G. Finlayson", "David Sontag", "Tyler Murray", "Sewon Min", "Pradeep Dasigi", "Luca Soldaini", "Faeze Brahman", "Wen-tau Yih", "Tongshuang Wu", "Luke Zettlemoyer", "Yoon Kim", "Hannaneh Hajishirzi", "Pang Wei Koh"], "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research", "comment": null, "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.", "AI": {"tldr": "本文提出了一种名为RLER的新型强化学习方法，用于训练长篇深度研究模型。基于RLER，开发了首个直接针对开放式、长篇深度研究进行训练的开源模型DR Tulu-8B，该模型在多个基准测试中显著优于现有开源模型，并与专有系统相当或超越。", "motivation": "大多数现有的开源深度研究模型是通过可验证奖励强化学习（RLVR）在易于验证的短篇问答任务上训练的，这种方法无法扩展到现实的长篇研究任务。因此，需要一种能够直接训练开放式、长篇深度研究模型的有效方法。", "method": "引入了“随评估标准演进的强化学习”（RLER）方法。RLER在训练过程中构建并维护与策略模型共同演进的评估标准（rubrics），这些标准能够纳入模型新探索到的信息，并提供有区分度的、基于策略的反馈。利用RLER，开发了Deep Research Tulu (DR Tulu-8B)。此外，还发布了新的基于MCP的深度研究系统代理基础设施。", "result": "DR Tulu-8B在科学、医疗和通用领域的四个长篇深度研究基准测试中，显著优于现有的开源深度研究模型，并与专有深度研究系统相当或超越，同时其模型更小，每次查询的成本也更低。", "conclusion": "RLER成功解决了训练长篇深度研究模型的挑战，并使得DR Tulu-8B成为首个直接为该任务训练的开源模型。DR Tulu-8B在性能上表现出色，与专有系统竞争，并为未来的深度研究提供了开放的数据、模型和代码，极大地促进了该领域的发展。"}}
{"id": "2511.18028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18028", "abs": "https://arxiv.org/abs/2511.18028", "authors": ["Chenyu Li", "Danfeng Hong", "Bing Zhang", "Zhaojie Pan", "Naoto Yokoya", "Jocelyn Chanussot"], "title": "MambaX: Image Super-Resolution with State Predictive Control", "comment": null, "summary": "Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \\textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.", "AI": {"tldr": "针对图像超分辨率（SR）中Mamba模型固定线性映射器的局限性，本文提出了MambaX，一个非线性状态预测控制模型，通过动态学习、状态交叉控制和渐进式转换学习，有效解决了误差传播和多模态融合问题，并在单图像和多模态SR任务中表现出卓越性能。", "motivation": "现有的图像超分辨率方法主要关注最终分辨率提升，忽视了中间阶段的误差传播和累积控制。虽然Mamba模型通过将重建过程表示为状态序列并允许中间干预，但其固定的线性映射器在感受野和灵活性上受限，尤其在处理精细图像时效果不佳。", "method": "本文创建了MambaX模型，该模型：1) 将连续光谱带映射到潜在状态空间，并通过动态学习控制方程的非线性状态参数来泛化SR任务；2) 采用动态状态预测控制学习来近似状态空间模型的非线性微分系数；3) 引入新颖的状态交叉控制范式用于多模态SR融合；4) 利用渐进式转换学习来缓解由域和模态偏移引起的异质性。", "result": "MambaX动态光谱-状态表示模型在单图像SR和基于多模态融合的SR任务中均展现出卓越的性能，突显了其在跨任意维度和模态进行光谱泛化建模方面的巨大潜力。", "conclusion": "MambaX通过其非线性状态预测控制、动态学习机制和多模态融合策略，显著提升了图像超分辨率技术，尤其在处理复杂场景和多模态数据方面，为光谱泛化建模开辟了新途径。"}}
{"id": "2511.19166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19166", "abs": "https://arxiv.org/abs/2511.19166", "authors": ["Samantha Dies", "Courtney Maynard", "Germans Savcisens", "Tina Eliassi-Rad"], "title": "Representational Stability of Truth in Large Language Models", "comment": "25 pages, 24 figures", "summary": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\\leq 8.2\\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18012", "abs": "https://arxiv.org/abs/2511.18012", "authors": ["Jiaying Zhou", "Qingchao Chen"], "title": "State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection", "comment": null, "summary": "Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., \"a sleeping cat\") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., \"cat lying on sofa\") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18050", "abs": "https://arxiv.org/abs/2511.18050", "authors": ["Tian Ye", "Song Fei", "Lei Zhu"], "title": "UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios", "comment": "Project Page: https://w2genai-lab.github.io/UltraFlux/", "summary": "Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.", "AI": {"tldr": "本文提出UltraFlux，一个基于Flux的扩散Transformer模型，通过数据模型协同设计，解决了现有扩散Transformer在原生4K、多长宽比图像生成中的失败模式，实现了高质量、细节丰富的4K图像生成。", "motivation": "扩散Transformer在1K分辨率下表现良好，但扩展到原生4K分辨率和多样化的长宽比时，会暴露出位置编码、VAE压缩和优化等紧密耦合的失败模式，单独解决其中任何一个因素都无法显著提升质量。", "method": "研究采用数据-模型协同设计方法：\n1.  **数据侧**：构建MultiAspect-4K-1M，一个包含100万张4K图像的数据集，具有受控的多长宽比覆盖、双语字幕和丰富的VLM/IQA元数据。\n2.  **模型侧 (UltraFlux)**：\n    *   结合Resonance 2D RoPE与YaRN，实现4K训练窗口、频率和长宽比感知的位置编码。\n    *   采用简单、非对抗性的VAE后训练方案，提高4K重建保真度。\n    *   引入SNR-Aware Huber Wavelet目标函数，重新平衡不同时间步和频段的梯度。\n    *   实施阶段性美学课程学习策略，将高美学监督集中在由模型先验控制的高噪声步骤上。", "result": "UltraFlux在Aesthetic-Eval at 4096基准测试和多长宽比4K设置下，在保真度、美学和对齐指标上持续优于强大的开源基线。结合LLM提示词优化器，UltraFlux能达到或超越专有模型Seedream 4.0的性能。", "conclusion": "UltraFlux通过其创新的数据和模型组件协同设计，成功克服了原生4K图像生成中的挑战，提供了一个稳定、细节保留的4K扩散Transformer，能够泛化到宽、方、高多种长宽比，并在多个评估指标上取得了最先进的性能。"}}
{"id": "2511.19317", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19317", "abs": "https://arxiv.org/abs/2511.19317", "authors": ["Md. Tanzim Ferdous", "Naeem Ahsan Chowdhury", "Prithwiraj Bhattacharjee"], "title": "MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset", "comment": null, "summary": "This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18014", "abs": "https://arxiv.org/abs/2511.18014", "authors": ["Kacper Dobek", "Daniel Jankowski", "Krzysztof Krawiec"], "title": "Modeling Retinal Ganglion Cells with Neural Differential Equations", "comment": "Accepted to the AAAI-26 Student Abstract and Poster Program, with supplementary material", "summary": "This work explores Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) for modeling retinal ganglion cell activity in tiger salamanders across three datasets. Compared to a convolutional baseline and an LSTM, both architectures achieved lower MAE, faster convergence, smaller model sizes, and favorable query times, though with slightly lower Pearson correlation. Their efficiency and adaptability make them well suited for scenarios with limited data and frequent retraining, such as edge deployments in vision prosthetics.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18037", "abs": "https://arxiv.org/abs/2511.18037", "authors": ["Yunfan Lu", "Nico Messikommer", "Xiaogang Xu", "Liming Chen", "Yuhan Chen", "Nikola Zubic", "Davide Scaramuzza", "Hui Xiong"], "title": "Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation", "comment": null, "summary": "Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.", "AI": {"tldr": "本文提出了首个统一的、基于统计的混合传感器（APS和EVS）成像噪声模型，并开发了相应的校准流程和模拟器（HESIM），以解决混合传感器中复杂且未被充分理解的噪声问题。", "motivation": "事件帧混合传感器结合了EVS的高动态范围和低延迟以及APS丰富的空间强度信息，但其复杂的电路架构引入了难以理解和建模的非平凡噪声模式。", "method": "研究方法包括：1) 提出一个统一的、基于统计的成像噪声模型，联合描述APS和EVS像素的噪声行为，显式包含光子散粒噪声、暗电流噪声、固定模式噪声和量化噪声，并将EVS噪声与光照水平和暗电流关联起来。2) 基于该模型开发了一个校准流程，用于从真实数据估计噪声参数。3) 提出了HESIM，一个基于统计的模拟器，用于在真实、联合校准的噪声统计下生成原始帧和事件。", "result": "研究成果包括：1) 建立了第一个统一的、基于统计的混合传感器噪声模型。2) 开发了用于噪声参数估计的校准流程。3) 创建了HESIM模拟器，能够生成具有真实噪声统计的原始数据。4) 在多个成像任务（如视频帧插值和去模糊）上对两种混合传感器进行了实验验证，结果表明模型和模拟器在模拟到真实数据之间具有很强的迁移性。", "conclusion": "该研究成功开发并验证了一个针对事件帧混合传感器的统一噪声模型、校准流程和模拟器（HESIM），有效解决了混合传感器噪声建模的难题，并证明了其在模拟真实数据方面的有效性，为未来的成像任务提供了支持。"}}
{"id": "2511.17673", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17673", "abs": "https://arxiv.org/abs/2511.17673", "authors": ["Myung Ho Kim"], "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop", "comment": "27 pages", "summary": "Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: https://github.com/enkiluv/scl-core-experiment Demo: https://scl-travel-planner.streamlit.app/", "AI": {"tldr": "本文提出结构化认知循环（SCL）架构，通过R-CCAM模块化认知和软符号控制，解决了大型语言模型代理的推理执行耦合、记忆易失性和行动失控问题，旨在构建可靠、可解释和可控的AI代理。", "motivation": "大型语言模型代理面临推理与执行纠缠、记忆易失性以及行动序列失控等根本性架构问题。", "method": "引入结构化认知循环（SCL），一个模块化架构，将代理认知明确分为检索、认知、控制、行动和记忆（R-CCAM）五个阶段。其核心是软符号控制，一种自适应治理机制，将符号约束应用于概率推理，以平衡神经灵活性与经典符号系统的可解释性和可控性。", "result": "在多步条件推理任务中，SCL实现了零策略违规，消除了冗余工具调用，并保持了完整的决策可追溯性。这些结果有效解决了现有框架（如ReAct、AutoGPT和记忆增强方法）中的关键空白。", "conclusion": "SCL为可信赖代理提供了模块化分解、自适应符号治理和透明状态管理三项设计原则。通过连接专家系统原理与现代LLM能力，为构建可靠、可解释和可治理的AI代理提供了实用且有理论基础的途径。"}}
{"id": "2511.19333", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19333", "abs": "https://arxiv.org/abs/2511.19333", "authors": ["Shaltiel Shmidman", "Asher Fredman", "Oleg Sudakov", "Meriem Bendris"], "title": "Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces", "comment": null, "summary": "Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18036", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18036", "abs": "https://arxiv.org/abs/2511.18036", "authors": ["Ziyi Guo", "Zhou Liu", "Wentao Zhang"], "title": "Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers", "comment": null, "summary": "The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.", "AI": {"tldr": "本文提出了一个用于从科学论文自动生成系统架构图的综合基准，并引入了一个基于多智能体协作的端到端系统Paper2SysArch作为基线。", "motivation": "手动创建系统架构图耗时且主观，现有生成模型缺乏结构控制和语义理解。该领域研究的关键障碍是缺乏用于定量评估自动生成图表的标准化基准。", "method": "引入了一个包含3,000篇论文及其高质量真实图表的综合基准，并设计了一个评估语义准确性、布局连贯性和视觉质量的三层评估指标。此外，提出了Paper2SysArch系统，一个利用多智能体协作将论文转换为结构化、可编辑图表的端到端系统，并在一个手动精选的挑战性子集上进行了评估。", "result": "该基准为可复现研究和公平比较奠定了基础。Paper2SysArch系统在复杂案例（挑战性子集）上取得了69.0的综合评分。", "conclusion": "本文的主要贡献是建立了一个大规模的基础基准，以推动自动化科学可视化的研究进展。同时，所提出的系统作为概念验证，为解决这一复杂任务展示了一条有前景的道路。"}}
{"id": "2511.18089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18089", "abs": "https://arxiv.org/abs/2511.18089", "authors": ["Wenjing Liu", "Qin Ren", "Wen Zhang", "Yuewei Lin", "Chenyu You"], "title": "Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective", "comment": null, "summary": "Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.", "AI": {"tldr": "本文提出了一种名为Together-Then-Apart (TTA) 的多模态生存分析框架，通过最小-最大优化同时建模共享和模态特异性表示，旨在平衡模态间的对齐和模态自身的独特性。", "motivation": "现有的多模态生存分析方法过度强调跨模态对齐，往往忽视模态特异性特征，导致表示塌缩和多样性降低。研究者认为保留模态特异性结构与实现语义一致性同样重要。", "method": "Together-Then-Apart (TTA) 框架采用统一的最小-最大优化方法。在“Together”阶段，通过共享原型和非平衡最优传输目标来最小化语义差异，自适应地突出信息丰富的token。在“Apart”阶段，通过模态锚点和对比正则化器最大化表示多样性，以保留独特的模态信息并防止特征塌缩。", "result": "在五个TCGA基准测试中，TTA始终优于现有的最先进方法，取得了显著的经验性提升。", "conclusion": "TTA框架不仅在实践中表现出色，还提供了一个新的理论视角，阐述了如何在鲁棒、可解释和具有生物学意义的多模态生存分析中，共同实现对齐和独特性。"}}
{"id": "2511.19149", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19149", "abs": "https://arxiv.org/abs/2511.19149", "authors": ["Moazzam Umer Gondal", "Hamad Ul Qudous", "Daniya Siddiqui", "Asma Ahmad Farhan"], "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation", "comment": "Submitted to Expert Systems with Applications", "summary": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.", "AI": {"tldr": "本文提出了一种检索增强框架（RAG），结合多服装检测、属性推理和大型语言模型（LLM）提示，用于自动生成时尚图像的描述和标签，解决了传统端到端模型在属性准确性和领域泛化方面的局限性。", "motivation": "现有的端到端图像描述生成器在时尚领域面临属性忠实度和领域泛化能力差的问题。研究旨在生成视觉接地、描述性强且风格有趣的时尚图像文本，克服这些限制。", "method": "该系统采用以下方法：1. 基于YOLO的检测器进行多服装定位；2. K-means聚类提取主导颜色；3. 基于结构化产品索引的CLIP-FAISS检索模块推断面料和性别属性。这些属性与检索到的风格示例共同构成“事实证据包”，用于引导LLM生成类人描述和丰富的标签。此外，使用微调的BLIP模型作为有监督的基线进行比较。", "result": "实验结果显示，YOLO检测器在九种服装类别上获得了0.71的mAP@0.5。RAG-LLM管道生成了富有表现力且属性对齐的描述，标签生成平均属性覆盖率为0.80，在50%阈值下达到完全覆盖。相比之下，BLIP模型在词汇重叠度上更高，但泛化能力较差。检索增强方法表现出更好的事实依据、更少的幻觉，并具有在各种服装领域可扩展部署的巨大潜力。", "conclusion": "研究表明，检索增强生成（RAG）是一种有效且可解释的范式，可用于自动化和视觉接地的时尚内容生成，具有广阔的应用前景。"}}
{"id": "2511.18055", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18055", "abs": "https://arxiv.org/abs/2511.18055", "authors": ["Bowen Qu", "Shangkun Sun", "Xiaoyu Liang", "Wei Gao"], "title": "IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment", "comment": "18 pages, 10 figures, 8 tables", "summary": "Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18090", "abs": "https://arxiv.org/abs/2511.18090", "authors": ["Mingwei He", "Tongda Xu", "Xingtong Ge", "Ming Sun", "Chao Zhou", "Yan Wang"], "title": "Versatile Recompression-Aware Perceptual Image Super-Resolution", "comment": null, "summary": "Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.", "AI": {"tldr": "本文提出VRPSR，一种通用的重压缩感知感知超分辨率方法。它利用扩散模型模拟编解码器，并采用专门的训练技术，在保持感知质量的同时，显著降低了图像重压缩后的比特率。", "motivation": "现有的感知超分辨率方法通常不考虑后续的图像重压缩，这可能导致额外的伪影并影响效率。然而，由于编解码器不可微分且配置多样，联合优化超分辨率和重压缩极具挑战性。", "method": "VRPSR首先将压缩建模为条件文本到图像生成任务，并利用预训练的扩散模型构建一个通用的编解码器模拟器。其次，它提出了一系列针对感知超分辨率的训练技术，包括使用感知目标优化模拟器，以及采用轻微压缩图像作为训练目标。", "result": "在H.264/H.265/H.266压缩标准下，VRPSR在基于Real-ESRGAN和S3Diff的方法上实现了超过10%的比特率节省。此外，VRPSR还促进了超分辨率模型与重压缩后处理模型的联合优化。", "conclusion": "VRPSR成功地使现有的感知超分辨率方法具备了通用的重压缩感知能力，显著降低了比特率，并为超分辨率与后处理模型的联合优化提供了可能。"}}
{"id": "2511.19417", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19417", "abs": "https://arxiv.org/abs/2511.19417", "authors": ["James Y. Huang", "Sheng Zhang", "Qianchu Liu", "Guanghui Qin", "Tinghui Zhu", "Tristan Naumann", "Muhao Chen", "Hoifung Poon"], "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.", "AI": {"tldr": "BeMyEyes是一个模块化、多智能体框架，通过协调高效的VLM（感知器）和强大的LLM（推理器）之间的对话，使LLM具备多模态推理能力，避免了大型多模态模型的训练，且在知识密集型多模态任务上超越了大型专有VLM。", "motivation": "大型语言模型（LLM）在知识密集型推理任务中表现出色，但缺乏多模态感知能力。将LLM扩展到新模态通常需要昂贵的、大规模视觉语言模型（VLM）开发。小型VLM虽然高效灵活，但缺乏前沿LLM的广泛知识和推理能力。研究旨在找到一种方法，结合两者的优势，以更高效、灵活的方式实现多模态推理。", "method": "本文提出了BeMyEyes框架，一个模块化的多智能体系统。它通过对话协调高效、适应性强的VLM作为感知器，以及强大的LLM作为推理器。此外，引入了一个数据合成和监督微调（SFT）流程，用于训练感知器智能体，使其能与推理器智能体有效协作。", "result": "实验表明，BeMyEyes框架成功解锁了LLM的多模态推理能力。一个轻量级且完全开源的解决方案（将纯文本的DeepSeek-R1与Qwen2.5-VL-7B感知器结合）在广泛的知识密集型多模态任务上超越了GPT-4o等大型专有VLM。", "conclusion": "该研究证明了BeMyEyes多智能体方法在构建未来多模态推理系统方面的有效性、模块化和可扩展性。它提供了一个轻量级、完全开源的解决方案，避免了训练大型多模态模型的需要，同时保留了LLM的泛化和推理能力，并能灵活扩展到新的领域和模态。"}}
{"id": "2511.18058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18058", "abs": "https://arxiv.org/abs/2511.18058", "authors": ["Wei Huang", "Zhitong Xiong", "Chenying Liu", "Xiao Xiang Zhu"], "title": "Hierarchical Semi-Supervised Active Learning for Remote Sensing", "comment": "Under review", "summary": "The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18063", "categories": ["cs.CV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2511.18063", "abs": "https://arxiv.org/abs/2511.18063", "authors": ["Gabriela Fernandes"], "title": "A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)", "comment": null, "summary": "Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.", "AI": {"tldr": "本研究开发了一种基于深度学习的虚拟病理助手，能有效区分宫颈腺癌原位病变（AIS）和正常宫颈腺体，准确率为0.7323，F1分数为0.75（异常类）。", "motivation": "宫颈腺癌原位病变（AIS）是一种重要的癌前病变，其准确的组织病理学诊断具有挑战性。早期检测对于预防其进展为浸润性宫颈腺癌至关重要。", "method": "研究使用了包含2240张专家标注H&E图像（1010张正常，1230张AIS）的CAISHI数据集。所有图像经过Macenko染色标准化和基于图像块的预处理。训练了一个EfficientNet-B3卷积神经网络，采用类别平衡采样和焦点损失（focal loss）来解决数据不平衡并强调难例。模型部署在基于Gradio的虚拟诊断助手中。", "result": "最终模型实现了0.7323的总体准确率，异常类别的F1分数为0.75，正常类别的F1分数为0.71。Grad-CAM热力图显示出生物学上可解释的激活模式，突出了与AIS形态一致的核异型性和腺体拥挤。", "conclusion": "这些发现证明了轻量级、可解释的AI系统在宫颈腺体病理学中的可行性，在筛查工作流程、教育和资源匮乏地区具有潜在应用。"}}
{"id": "2511.19314", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19314", "abs": "https://arxiv.org/abs/2511.19314", "authors": ["Jaewoo Lee", "Archiki Prasad", "Justin Chih-Yao Chen", "Zaid Khan", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking", "comment": "18 pages, code: https://github.com/G-JWLee/PRInTS", "summary": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.", "AI": {"tldr": "本文提出PRInTS，一个生成式过程奖励模型（PRM），通过多维度评分和轨迹总结来增强AI代理的信息搜寻能力，在多项基准测试中超越了现有方法和前沿模型。", "motivation": "AI代理在多步骤信息搜寻任务中面临挑战，现有过程奖励模型（PRMs）无法捕捉信息搜寻步骤的丰富维度（如工具交互、工具输出推理），也难以处理长周期任务中快速增长的上下文。", "method": "引入PRInTS，一个具有双重能力的生成式PRM：1) 基于PRM对多个步骤质量维度（如工具输出解释、工具调用信息量）进行密集评分；2) 轨迹总结，压缩不断增长的上下文同时保留评估步骤所需的基本信息。通过best-of-n采样进行训练和评估。", "result": "在FRAMES、GAIA（1-3级）和WebWalkerQA（易-难）等基准测试中，PRInTS通过best-of-n采样显著提升了开源模型和专用代理的信息搜寻能力。它以更小的骨干代理匹配或超越了前沿模型的性能，并优于其他强大的奖励建模基线。", "conclusion": "PRInTS成功解决了现有PRMs在多步骤信息搜寻任务中的局限性，通过多维度评分和上下文总结显著提升了AI代理的性能，使其能够更好地进行信息获取和推理。"}}
{"id": "2511.18123", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18123", "abs": "https://arxiv.org/abs/2511.18123", "authors": ["Dachuan Zhao", "Weiyue Li", "Zhenda Shen", "Yushu Qiu", "Bowen Xu", "Haoyu Chen", "Yongchao Chen"], "title": "Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\\textbf{S}$ubspace $\\textbf{P}$rojection $\\textbf{D}$ebiasing ($\\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18075", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18075", "abs": "https://arxiv.org/abs/2511.18075", "authors": ["Jianhang Yao", "Yongbin Zheng", "Siqi Lu", "Wanying Xu", "Peng Sun"], "title": "VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection", "comment": "15 pages, 8 figures, accepted by AAAI 2026", "summary": "To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\\textbf{VK-Det}$, a $\\textbf{V}$isual $\\textbf{K}$nowledge-guided open-vocabulary object $\\textbf{Det}$ection framework $\\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\\mathrm{mAP}^{N}$ on DIOR and 23.3 $\\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18120", "abs": "https://arxiv.org/abs/2511.18120", "authors": ["Hannuo Zhang", "Zhixiang Chi", "Yang Wang", "Xinxin Zuo"], "title": "MVS-TTA: Test-Time Adaptation for Multi-View Stereo via Meta-Auxiliary Learning", "comment": "8 pages, 7 figures", "summary": "Recent learning-based multi-view stereo (MVS) methods are data-driven and have achieved remarkable progress due to large-scale training data and advanced architectures. However, their generalization remains sub-optimal due to fixed model parameters trained on limited training data distributions. In contrast, optimization-based methods enable scene-specific adaptation but lack scalability and require costly per-scene optimization. In this paper, we propose MVS-TTA, an efficient test-time adaptation (TTA) framework that enhances the adaptability of learning-based MVS methods by bridging these two paradigms. Specifically, MVS-TTA employs a self-supervised, cross-view consistency loss as an auxiliary task to guide inference-time adaptation. We introduce a meta-auxiliary learning strategy to train the model to benefit from auxiliary-task-based updates explicitly. Our framework is model-agnostic and can be applied to a wide range of MVS methods with minimal architectural changes. Extensive experiments on standard datasets (DTU, BlendedMVS) and a challenging cross-dataset generalization setting demonstrate that MVS-TTA consistently improves performance, even when applied to state-of-the-art MVS models. To our knowledge, this is the first attempt to integrate optimization-based test-time adaptation into learning-based MVS using meta-learning. The code will be available at https://github.com/mart87987-svg/MVS-TTA.", "AI": {"tldr": "本文提出了MVS-TTA框架，通过结合自监督的跨视图一致性损失和元辅助学习策略，为基于学习的多视图立体视觉（MVS）方法提供高效的测试时间适应能力，以解决其泛化性问题。", "motivation": "尽管基于学习的MVS方法取得了显著进展，但由于训练数据分布有限，其泛化能力仍不理想。而基于优化的方法虽然能实现场景特定适应，但缺乏可扩展性且成本高昂。因此，需要一种方法来结合两者的优势，提高学习型MVS方法的适应性。", "method": "MVS-TTA框架通过弥合学习型和优化型范式之间的差距来增强学习型MVS方法的适应性。它采用自监督的跨视图一致性损失作为辅助任务，指导推理时间的适应。此外，引入了元辅助学习策略来训练模型，使其能够从基于辅助任务的更新中明确受益。该框架与模型无关，可应用于多种MVS方法，且只需最少的架构改动。", "result": "在标准数据集（DTU、BlendedMVS）和具有挑战性的跨数据集泛化设置上进行的广泛实验表明，MVS-TTA能够持续提升性能，即使应用于最先进的MVS模型也是如此。", "conclusion": "MVS-TTA首次尝试将基于优化的测试时间适应与元学习相结合，应用于基于学习的MVS方法，显著提高了模型的泛化能力和适应性。"}}
{"id": "2511.18824", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18824", "abs": "https://arxiv.org/abs/2511.18824", "authors": ["Alvin Wei Ming Tan", "Jane Yang", "Tarun Sepuri", "Khai Loong Aw", "Robert Z. Sparks", "Zi Yin", "Virginia A. Marchman", "Michael C. Frank", "Bria Long"], "title": "Assessing the alignment between infants' visual and linguistic experience using multimodal language models", "comment": null, "summary": "Figuring out which objects or concepts words refer to is a central language learning challenge for young children. Most models of this process posit that children learn early object labels from co-occurrences of words and their referents that occur when someone around them talks about an object in the immediate physical environment. But how aligned in time are children's visual and linguistic experiences during everyday learning? To date, answers to this question have been limited by the need for labor-intensive manual annotations of vision-language co-occurrences. Here, we evaluate the use of contrastive language-image pretraining (CLIP) models to automatically characterize vision-language alignment in egocentric videos taken from the infant perspective in home environments. After validating CLIP alignment scores using human alignment judgments, we apply this metric to a large corpus of infant-perspective videos. We show that idealized aligned moments for learning (e.g., \"look at the ball\" with a ball present in the child's view) are relatively rare in children's everyday experiences compared to modern machine learning datasets, and highlight variability in alignment both within and across children. These findings suggest that infrequent alignment is a constraint for models describing early word learning and offer a new method for investigating children's multimodal environment.", "AI": {"tldr": "本研究利用CLIP模型自动量化了儿童日常生活中视觉与语言的对齐程度，发现理想的视觉-语言对齐时刻在儿童日常经验中相对稀少，且存在显著的个体差异。", "motivation": "理解儿童如何学习词语指代是语言学习的核心挑战。现有模型假设儿童从词语与其指代物在物理环境中的共现中学习。然而，儿童日常视觉和语言体验在时间上对齐的程度如何？此前，由于需要大量人工标注，这一问题难以回答。", "method": "研究评估了使用对比语言-图像预训练（CLIP）模型自动量化从婴儿视角拍摄的家庭环境中视觉-语言对齐的有效性。通过与人类判断进行比对，验证了CLIP对齐分数的准确性，随后将此方法应用于大量婴儿视角视频语料库。", "result": "结果显示，与现代机器学习数据集相比，儿童日常经验中理想的、用于学习的对齐时刻（例如，“看球”时球出现在儿童视野中）相对稀少。此外，对齐程度在儿童内部和儿童之间都存在显著变异性。", "conclusion": "这些发现表明，不频繁的对齐是描述早期词语学习模型的一个限制，并提供了一种调查儿童多模态环境的新方法。"}}
{"id": "2511.18083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18083", "abs": "https://arxiv.org/abs/2511.18083", "authors": ["Md Abdullah Al Kafi", "Raka Moni", "Sumit Kumar Banshal"], "title": "Less Is More: An Explainable AI Framework for Lightweight Malaria Classification", "comment": null, "summary": "Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.\n  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.\n  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).\n  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18127", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18127", "abs": "https://arxiv.org/abs/2511.18127", "authors": ["Ruicong Liu", "Yifei Huang", "Liangyang Ouyang", "Caixin Kang", "Yoichi Sato"], "title": "SFHand: A Streaming Framework for Language-guided 3D Hand Forecasting and Embodied Manipulation", "comment": null, "summary": "Real-time 3D hand forecasting is a critical component for fluid human-computer interaction in applications like AR and assistive robotics. However, existing methods are ill-suited for these scenarios, as they typically require offline access to accumulated video sequences and cannot incorporate language guidance that conveys task intent. To overcome these limitations, we introduce SFHand, the first streaming framework for language-guided 3D hand forecasting. SFHand autoregressively predicts a comprehensive set of future 3D hand states, including hand type, 2D bounding box, 3D pose, and trajectory, from a continuous stream of video and language instructions. Our framework combines a streaming autoregressive architecture with an ROI-enhanced memory layer, capturing temporal context while focusing on salient hand-centric regions. To enable this research, we also introduce EgoHaFL, the first large-scale dataset featuring synchronized 3D hand poses and language instructions. We demonstrate that SFHand achieves new state-of-the-art results in 3D hand forecasting, outperforming prior work by a significant margin of up to 35.8%. Furthermore, we show the practical utility of our learned representations by transferring them to downstream embodied manipulation tasks, improving task success rates by up to 13.4% on multiple benchmarks. Dataset page: https://huggingface.co/datasets/ut-vision/EgoHaFL, project page: https://github.com/ut-vision/SFHand.", "AI": {"tldr": "SFHand是一个流式框架，首次实现了语言引导的实时3D手部预测，并引入了大规模数据集EgoHaFL，显著提升了预测精度和下游任务表现。", "motivation": "现有的3D手部预测方法通常需要离线访问累积的视频序列，且无法整合传达任务意图的语言指导，这使其不适用于AR和辅助机器人等需要流畅人机交互的实时应用场景。", "method": "本文提出了SFHand，一个流式自回归架构，结合了ROI增强的记忆层，能够从连续的视频流和语言指令中自回归地预测未来3D手部状态（包括手部类型、2D边界框、3D姿态和轨迹）。为支持此研究，还引入了EgoHaFL，这是首个包含同步3D手部姿态和语言指令的大规模数据集。", "result": "SFHand在3D手部预测任务上取得了新的最先进结果，比现有方法提高了高达35.8%。此外，所学习的表示在下游具身操作任务中也展现了实用性，在多个基准测试中将任务成功率提高了高达13.4%。", "conclusion": "SFHand成功解决了实时、语言引导3D手部预测的挑战，通过创新的流式框架和大规模数据集EgoHaFL，显著提升了预测性能，并证明了其在实际具身操作任务中的有效性。"}}
{"id": "2511.18121", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18121", "abs": "https://arxiv.org/abs/2511.18121", "authors": ["Ming Zhong", "Yuanlei Wang", "Liuzhou Zhang", "Arctanx An", "Renrui Zhang", "Hao Liang", "Ming Lu", "Ying Shen", "Wentao Zhang"], "title": "VCU-Bridge: Hierarchical Visual Connotation Understanding via Semantic Bridging", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel on benchmarks, their processing paradigm differs from the human ability to integrate visual information. Unlike humans who naturally bridge details and high-level concepts, models tend to treat these elements in isolation. Prevailing evaluation protocols often decouple low-level perception from high-level reasoning, overlooking their semantic and causal dependencies, which yields non-diagnostic results and obscures performance bottlenecks. We present VCU-Bridge, a framework that operationalizes a human-like hierarchy of visual connotation understanding: multi-level reasoning that advances from foundational perception through semantic bridging to abstract connotation, with an explicit evidence-to-inference trace from concrete cues to abstract conclusions. Building on this framework, we construct HVCU-Bench, a benchmark for hierarchical visual connotation understanding with explicit, level-wise diagnostics. Comprehensive experiments demonstrate a consistent decline in performance as reasoning progresses to higher levels. We further develop a data generation pipeline for instruction tuning guided by Monte Carlo Tree Search (MCTS) and show that strengthening low-level capabilities yields measurable gains at higher levels. Interestingly, it not only improves on HVCU-Bench but also brings benefits on general benchmarks (average +2.53%), especially with substantial gains on MMStar (+7.26%), demonstrating the significance of the hierarchical thinking pattern and its effectiveness in enhancing MLLM capabilities. The project page is at https://vcu-bridge.github.io .", "AI": {"tldr": "多模态大语言模型（MLLMs）在视觉信息整合方面与人类存在差异，缺乏层次化的视觉内涵理解。本文提出了VCU-Bridge框架和HVCU-Bench基准来评估这一能力，发现模型在更高层推理时性能下降。通过蒙特卡洛树搜索（MCTS）指导的指令微调，强化低层能力，不仅提升了层次化理解，也显著改善了通用基准的性能。", "motivation": "现有MLLMs在处理视觉信息时，倾向于孤立地处理细节和高层概念，而非像人类那样自然地整合。目前的评估协议常将低层感知与高层推理分离，忽视了它们之间的语义和因果依赖性，导致诊断结果不准确，并掩盖了性能瓶颈。", "method": "本文提出了VCU-Bridge框架，用于操作类人层次化的视觉内涵理解，包括从基础感知到语义桥接再到抽象内涵的多级推理，并具有从具体线索到抽象结论的显式证据到推断的追溯。在此框架基础上，构建了HVCU-Bench基准，用于分层视觉内涵理解的显式、分级诊断。此外，开发了一个由蒙特卡洛树搜索（MCTS）指导的指令微调数据生成流水线，以增强模型的低层能力。", "result": "综合实验表明，随着推理进展到更高层次，模型的性能持续下降。通过MCTS指导的指令微调，强化低层能力，在高层推理中取得了可衡量的收益。有趣的是，这不仅改善了HVCU-Bench上的表现，还在通用基准上带来了益处（平均+2.53%），尤其在MMStar上取得了显著提升（+7.26%）。", "conclusion": "层次化思维模式对于增强多模态大语言模型的能力至关重要且有效，能够提升模型在层次化视觉理解方面的表现，并带来通用基准的性能提升。"}}
{"id": "2511.19304", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19304", "abs": "https://arxiv.org/abs/2511.19304", "authors": ["Jiayi Zhang", "Yiran Peng", "Fanqi Kong", "Yang Cheng", "Yifan Wu", "Zhaoyang Yu", "Jinyu Xiang", "Jianhao Ruan", "Jinlin Wang", "Maojia Song", "HongZhang Liu", "Xiangru Tang", "Bang Liu", "Chenglin Wu", "Yuyu Luo"], "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning", "comment": null, "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18136", "abs": "https://arxiv.org/abs/2511.18136", "authors": ["Chunming He", "Rihan Zhang", "Longxiang Tang", "Ziyun Yang", "Kai Li", "Deng-Ping Fan", "Sina Farsiu"], "title": "SCALER: SAM-Enhanced Collaborative Learning for Label-Deficient Concealed Object Segmentation", "comment": "4 figures, 6 tables", "summary": "Existing methods for label-deficient concealed object segmentation (LDCOS) either rely on consistency constraints or Segment Anything Model (SAM)-based pseudo-labeling. However, their performance remains limited due to the intrinsic concealment of targets and the scarcity of annotations. This study investigates two key questions: (1) Can consistency constraints and SAM-based supervision be jointly integrated to better exploit complementary information and enhance the segmenter? and (2) beyond that, can the segmenter in turn guide SAM through reciprocal supervision, enabling mutual improvement? To answer these questions, we present SCALER, a unified collaborative framework toward LDCOS that jointly optimizes a mean-teacher segmenter and a learnable SAM. SCALER operates in two alternating phases. In \\textbf{Phase \\uppercase\\expandafter{\\romannumeral1}}, the segmenter is optimized under fixed SAM supervision using entropy-based image-level and uncertainty-based pixel-level weighting to select reliable pseudo-label regions and emphasize harder examples. In \\textbf{Phase \\uppercase\\expandafter{\\romannumeral2}}, SAM is updated via augmentation invariance and noise resistance losses, leveraging its inherent robustness to perturbations. Experiments demonstrate that SCALER yields consistent performance gains across eight semi- and weakly-supervised COS tasks. The results further suggest that SCALER can serve as a general training paradigm to enhance both lightweight segmenters and large foundation models under label-scarce conditions. Code will be released.", "AI": {"tldr": "SCALER是一个统一的协作框架，通过交替优化平均教师分割器和可学习的SAM，解决了标签稀缺隐匿物体分割（LDCOS）的挑战，实现了分割器和SAM的相互提升。", "motivation": "现有的标签稀缺隐匿物体分割（LDCOS）方法（基于一致性约束或SAM的伪标签）性能有限，原因在于目标固有的隐匿性和标注稀缺。研究旨在探索如何共同整合一致性约束和SAM监督以增强分割器，并进一步实现分割器反过来指导SAM的相互监督，从而实现双向改进。", "method": "本文提出了SCALER框架，联合优化一个平均教师分割器和一个可学习的SAM。该框架分两个交替阶段：阶段一，在固定SAM监督下优化分割器，采用基于熵的图像级和基于不确定性的像素级加权来选择可靠的伪标签区域并强调难例；阶段二，利用SAM固有的扰动鲁棒性，通过增强不变性和抗噪声损失来更新SAM。", "result": "实验证明，SCALER在八项半监督和弱监督隐匿物体分割任务中均取得了显著的性能提升。结果还表明，SCALER可以作为一种通用的训练范式，在标签稀缺条件下同时增强轻量级分割器和大型基础模型。", "conclusion": "SCALER通过其独特的协作和相互监督机制，有效解决了标签稀缺隐匿物体分割的挑战，能够一致性地提升分割性能，并为在有限标注下训练各种模型提供了一种通用的解决方案。"}}
{"id": "2511.18152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18152", "abs": "https://arxiv.org/abs/2511.18152", "authors": ["Chunming He", "Rihan Zhang", "Zheng Chen", "Bowen Yang", "CHengyu Fang", "Yunlong Lin", "Fengyang Xiao", "Sina Farsiu"], "title": "UnfoldLDM: Deep Unfolding-based Blind Image Restoration with Latent Diffusion Priors", "comment": "6 figures, 11 tables", "summary": "Deep unfolding networks (DUNs) combine the interpretability of model-based methods with the learning ability of deep networks, yet remain limited for blind image restoration (BIR). Existing DUNs suffer from: (1) \\textbf{Degradation-specific dependency}, as their optimization frameworks are tied to a known degradation model, making them unsuitable for BIR tasks; and (2) \\textbf{Over-smoothing bias}, resulting from the direct feeding of gradient descent outputs, dominated by low-frequency content, into the proximal term, suppressing fine textures. To overcome these issues, we propose UnfoldLDM to integrate DUNs with latent diffusion model (LDM) for BIR. In each stage, UnfoldLDM employs a multi-granularity degradation-aware (MGDA) module as the gradient descent step. MGDA models BIR as an unknown degradation estimation problem and estimates both the holistic degradation matrix and its decomposed forms, enabling robust degradation removal. For the proximal step, we design a degradation-resistant LDM (DR-LDM) to extract compact degradation-invariant priors from the MGDA output. Guided by this prior, an over-smoothing correction transformer (OCFormer) explicitly recovers high-frequency components and enhances texture details. This unique combination ensures the final result is degradation-free and visually rich. Experiments show that our UnfoldLDM achieves a leading place on various BIR tasks and benefits downstream tasks. Moreover, our design is compatible with existing DUN-based methods, serving as a plug-and-play framework. Code will be released.", "AI": {"tldr": "本文提出UnfoldLDM，将深度展开网络（DUNs）与潜在扩散模型（LDM）结合，以解决盲图像复原（BIR）中DUNs的降解特异性依赖和过平滑偏差问题，实现了领先的复原性能。", "motivation": "现有深度展开网络（DUNs）在盲图像复原（BIR）任务中存在局限性：1) 依赖于已知的降解模型，导致“降解特异性依赖”，不适用于BIR；2) 梯度下降输出直接输入近端项，导致“过平滑偏差”，抑制了精细纹理。", "method": "本文提出UnfoldLDM，将DUNs与潜在扩散模型（LDM）集成以进行BIR。在每个阶段：1) 梯度下降步骤采用多粒度降解感知（MGDA）模块，将BIR建模为未知降解估计问题，并估计整体和分解的降解矩阵，以实现鲁棒的降解去除。2) 近端步骤设计了抗降解LDM（DR-LDM）来从MGDA输出中提取紧凑的降解不变先验。3) 在此先验的指导下，过平滑校正Transformer（OCFormer）显式恢复高频分量并增强纹理细节。", "result": "实验结果表明，UnfoldLDM在各种BIR任务上取得了领先的性能，并对下游任务有益。此外，所提出的设计与现有基于DUN的方法兼容，可作为即插即用的框架。", "conclusion": "UnfoldLDM通过创新性地结合DUNs和LDM，有效克服了现有DUNs在盲图像复原中的降解特异性依赖和过平滑偏差问题，恢复了无降解且视觉丰富的图像，并具有良好的通用性和兼容性。"}}
{"id": "2511.18102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18102", "abs": "https://arxiv.org/abs/2511.18102", "authors": ["Aditya Chinchure", "Sahithya Ravi", "Pushkar Shukla", "Vered Shwartz", "Leonid Sigal"], "title": "Spotlight: Identifying and Localizing Video Generation Errors Using VLMs", "comment": null, "summary": "Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18104", "abs": "https://arxiv.org/abs/2511.18104", "authors": ["Xiaohong Liu", "Xiufeng Song", "Huayu Zheng", "Lei Bai", "Xiaoming Liu", "Guangtao Zhai"], "title": "Consolidating Diffusion-Generated Video Detection with Unified Multimodal Forgery Learning", "comment": "Code and dataset are available at https://github.com/SparkleXFantasy/MM-Det-Plus", "summary": "The proliferation of videos generated by diffusion models has raised increasing concerns about information security, highlighting the urgent need for reliable detection of synthetic media. Existing methods primarily focus on image-level forgery detection, leaving generic video-level forgery detection largely underexplored. To advance video forensics, we propose a consolidated multimodal detection algorithm, named MM-Det++, specifically designed for detecting diffusion-generated videos. Our approach consists of two innovative branches and a Unified Multimodal Learning (UML) module. Specifically, the Spatio-Temporal (ST) branch employs a novel Frame-Centric Vision Transformer (FC-ViT) to aggregate spatio-temporal information for detecting diffusion-generated videos, where the FC-tokens enable the capture of holistic forgery traces from each video frame. In parallel, the Multimodal (MM) branch adopts a learnable reasoning paradigm to acquire Multimodal Forgery Representation (MFR) by harnessing the powerful comprehension and reasoning capabilities of Multimodal Large Language Models (MLLMs), which discerns the forgery traces from a flexible semantic perspective. To integrate multimodal representations into a coherent space, a UML module is introduced to consolidate the generalization ability of MM-Det++. In addition, we also establish a large-scale and comprehensive Diffusion Video Forensics (DVF) dataset to advance research in video forgery detection. Extensive experiments demonstrate the superiority of MM-Det++ and highlight the effectiveness of unified multimodal forgery learning in detecting diffusion-generated videos.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18173", "abs": "https://arxiv.org/abs/2511.18173", "authors": ["Enrico Pallotta", "Sina Mokhtarzadeh Azar", "Lars Doorenbos", "Serdar Ozsoy", "Umar Iqbal", "Juergen Gall"], "title": "EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses", "comment": null, "summary": "Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.", "AI": {"tldr": "本文提出了EgoControl，一个基于扩散模型的、可由3D身体姿态精细控制的第一人称视角视频生成模型，用于实现可控的具身视频模拟。", "motivation": "具身AI代理需要能够模拟、预测和规划动作，这要求第一人称视角视频生成能够通过身体运动实现精细控制。", "method": "EgoControl是一个姿态可控的视频扩散模型，在第一人称视角数据上进行训练。它通过显式的3D身体姿态序列来条件化未来帧的生成。为实现精确运动控制，模型引入了一种新颖的姿态表示（捕捉全局摄像机动态和关节身体运动），并通过扩散过程中的专用控制机制进行整合。", "result": "给定一小段观察帧序列和目标姿态序列，EgoControl能生成与所提供姿态控制对齐的、时间连贯且视觉真实的未来帧。实验证明，EgoControl能生成高质量、姿态一致的第一人称视角视频。", "conclusion": "EgoControl为可控的具身视频模拟和理解铺平了道路。"}}
{"id": "2511.18105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18105", "abs": "https://arxiv.org/abs/2511.18105", "authors": ["Purvish Jajal", "Nick John Eliopoulos", "Benjamin Shiue-Hal Chou", "George K. Thiruvathukal", "Yung-Hsiang Lu", "James C. Davis"], "title": "AdaPerceiver: Transformers with Adaptive Width, Depth, and Tokens", "comment": null, "summary": "Modern transformer architectures achieve remarkable performance across tasks and domains but remain rigid in how they allocate computation at inference time. Real-world deployment often requires models to adapt to diverse hardware and latency constraints, yet most approaches to dynamic computation focus on a single axis -- such as reducing the number of tokens. We present a novel capability: AdaPerceiver, the first transformer architecture with unified adaptivity across depth, width, and tokens within a single model. We propose an architecture that supports adaptivity along these axes. We couple this with an efficient joint training regime that ensures the model maintains performance across its various configurations. We evaluate AdaPerceiver on image classification, semantic segmentation, and depth estimation tasks. On image classification, AdaPerceiver expands the accuracy-throughput Pareto front. It achieves 85.4% accuracy while yielding 36% higher throughput than FlexiViT-L. On dense prediction, AdaPerceiver matches ViT-H/14 while having $\\sim$26x fewer encoder FLOPs (floating-point operations) on semantic segmentation and depth estimation. Finally, we show how AdaPerceiver equipped with a policy can maintain ImageNet1K accuracy ($\\pm0.1$ percentage points) while reducing FLOPs by $24-33$%.", "AI": {"tldr": "AdaPerceiver是一种新型Transformer架构，首次在深度、宽度和token维度上实现统一自适应计算，显著提升了模型在不同硬件和延迟约束下的效率和性能。", "motivation": "现代Transformer模型在推理时计算分配僵化，难以适应多样化的硬件和延迟需求。现有动态计算方法通常只关注单一维度（如减少token数量），缺乏跨维度（深度、宽度、token）的统一自适应能力。", "method": "本文提出AdaPerceiver架构，支持模型在深度、宽度和token维度上的自适应。同时，设计了一种高效的联合训练机制，以确保模型在各种配置下都能保持良好的性能。", "result": "在图像分类任务上，AdaPerceiver扩展了准确率-吞吐量帕累托前沿，实现了85.4%的准确率，吞吐量比FlexiViT-L高36%。在密集预测任务（语义分割和深度估计）上，AdaPerceiver在编码器FLOPs减少约26倍的情况下，性能与ViT-H/14相当。此外，配备策略的AdaPerceiver在ImageNet1K上保持准确率（±0.1个百分点）的同时，FLOPs减少了24-33%。", "conclusion": "AdaPerceiver成功实现了Transformer模型在深度、宽度和token维度上的统一自适应，显著提升了计算效率，并在多种任务上展现出更优的准确率-吞吐量权衡，为模型部署提供了更大的灵活性和更强的适应性。"}}
{"id": "2511.18163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18163", "abs": "https://arxiv.org/abs/2511.18163", "authors": ["Pasquale De Marinis", "Uzay Kaymak", "Rogier Brussee", "Gennaro Vessio", "Giovanna Castellano"], "title": "Matching-Based Few-Shot Semantic Segmentation Models Are Interpretable by Design", "comment": null, "summary": "Few-Shot Semantic Segmentation (FSS) models achieve strong performance in segmenting novel classes with minimal labeled examples, yet their decision-making processes remain largely opaque. While explainable AI has advanced significantly in standard computer vision tasks, interpretability in FSS remains virtually unexplored despite its critical importance for understanding model behavior and guiding support set selection in data-scarce scenarios. This paper introduces the first dedicated method for interpreting matching-based FSS models by leveraging their inherent structural properties. Our Affinity Explainer approach extracts attribution maps that highlight which pixels in support images contribute most to query segmentation predictions, using matching scores computed between support and query features at multiple feature levels. We extend standard interpretability evaluation metrics to the FSS domain and propose additional metrics to better capture the practical utility of explanations in few-shot scenarios. Comprehensive experiments on FSS benchmark datasets, using different models, demonstrate that our Affinity Explainer significantly outperforms adapted standard attribution methods. Qualitative analysis reveals that our explanations provide structured, coherent attention patterns that align with model architectures and and enable effective model diagnosis. This work establishes the foundation for interpretable FSS research, enabling better model understanding and diagnostic for more reliable few-shot segmentation systems. The source code is publicly available at https://github.com/pasqualedem/AffinityExplainer.", "AI": {"tldr": "该论文提出了首个专门用于解释基于匹配的少样本语义分割（FSS）模型的方法——Affinity Explainer，通过利用多层特征匹配分数生成归因图，揭示支持图像像素对查询预测的贡献。该方法在FSS基准测试中表现优异，并为可解释FSS研究奠定了基础。", "motivation": "尽管少样本语义分割（FSS）模型在处理新类别方面表现出色，但其决策过程不透明。在数据稀缺的FSS场景中，理解模型行为和指导支持集选择至关重要，但FSS领域的可解释性研究几乎空白。", "method": "本文引入了Affinity Explainer方法，专门用于解释基于匹配的FSS模型。该方法利用模型固有的结构特性，通过计算支持图像和查询图像在多个特征层级之间的匹配分数，提取归因图，以突出支持图像中对查询分割预测贡献最大的像素。同时，还扩展并提出了适用于FSS领域的可解释性评估指标。", "result": "在FSS基准数据集上使用不同模型进行的综合实验表明，Affinity Explainer显著优于经过适配的标准归因方法。定性分析显示，其解释提供了结构化、连贯的注意力模式，与模型架构一致，并能实现有效的模型诊断。", "conclusion": "这项工作为可解释的FSS研究奠定了基础，有助于更好地理解和诊断模型，从而构建更可靠的少样本分割系统。"}}
{"id": "2511.18174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18174", "abs": "https://arxiv.org/abs/2511.18174", "authors": ["Mukai Yu", "Mosam Dabhi", "Liuyue Xie", "Sebastian Scherer", "László A. Jeni"], "title": "Unified Spherical Frontend: Learning Rotation-Equivariant Representations of Spherical Images from Any Camera", "comment": null, "summary": "Modern perception increasingly relies on fisheye, panoramic, and other wide field-of-view (FoV) cameras, yet most pipelines still apply planar CNNs designed for pinhole imagery on 2D grids, where image-space neighborhoods misrepresent physical adjacency and models are sensitive to global rotations. Frequency-domain spherical CNNs partially address this mismatch but require costly spherical harmonic transforms that constrain resolution and efficiency. We introduce the Unified Spherical Frontend (USF), a lens-agnostic framework that transforms images from any calibrated camera into a unit-sphere representation via ray-direction correspondences, and performs spherical resampling, convolution, and pooling directly in the spatial domain. USF is modular: projection, location sampling, interpolation, and resolution control are fully decoupled. Its distance-only spherical kernels offer configurable rotation-equivariance (mirroring translation-equivariance in planar CNNs) while avoiding harmonic transforms entirely. We compare standard planar backbones with their spherical counterparts across classification, detection, and segmentation tasks on synthetic (Spherical MNIST) and real-world datasets (PANDORA, Stanford 2D-3D-S), and stress-test robustness to extreme lens distortions, varying FoV, and arbitrary rotations. USF processes high-resolution spherical imagery efficiently and maintains less than 1% performance drop under random test-time rotations, even without rotational augmentation, and even enables zero-shot generalization from one lens type to unseen wide-FoV lenses with minimal performance degradation.", "AI": {"tldr": "本文提出了一种名为USF的镜头无关框架，用于在空间域直接处理单位球体上的宽视场相机图像的球形CNN，实现了旋转等变性、高效率且无需球谐变换，并在各种任务和镜头类型上展现出鲁棒性和泛化能力。", "motivation": "现代感知系统越来越多地依赖广角相机，但大多数现有管道仍使用为针孔图像设计的平面CNN，导致图像空间邻域无法准确表示物理邻接，且模型对全局旋转敏感。现有的频域球形CNN虽然部分解决了这个问题，但需要昂贵的球谐变换，限制了分辨率和效率。", "method": "USF是一个镜头无关的框架，通过光线方向对应将任何校准相机图像转换为单位球体表示。它直接在空间域进行球形重采样、卷积和池化。USF是模块化的，投影、位置采样、插值和分辨率控制完全解耦。其仅基于距离的球形核提供了可配置的旋转等变性，并完全避免了球谐变换。", "result": "USF能高效处理高分辨率球形图像，在随机测试时旋转下性能下降不到1%，即使没有旋转增强。它还能实现从一种镜头类型到未见过宽视场镜头的零样本泛化，性能下降极小。该方法在分类、检测和分割任务上，于合成和真实世界数据集上进行了评估，并展示了对极端镜头畸变、不同视场和任意旋转的鲁棒性。", "conclusion": "USF为宽视场相机提供了一个高效、鲁棒且可泛化的球形CNN解决方案，通过在单位球体上的空间域操作，克服了平面CNN和频域球形CNN的局限性。"}}
{"id": "2511.18116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18116", "abs": "https://arxiv.org/abs/2511.18116", "authors": ["Yuheng Shao", "Lizhang Wang", "Changhao Li", "Peixian Chen", "Qinyuan Liu"], "title": "PromptMoE: Generalizable Zero-Shot Anomaly Detection via Visually-Guided Prompt Mixtures", "comment": "14 pages, 8 figures. Accepted to AAAI 2026", "summary": "Zero-Shot Anomaly Detection (ZSAD) aims to identify and localize anomalous regions in images of unseen object classes. While recent methods based on vision-language models like CLIP show promise, their performance is constrained by existing prompt engineering strategies. Current approaches, whether relying on single fixed, learnable, or dense dynamic prompts, suffer from a representational bottleneck and are prone to overfitting on auxiliary data, failing to generalize to the complexity and diversity of unseen anomalies. To overcome these limitations, we propose $\\mathtt{PromptMoE}$. Our core insight is that robust ZSAD requires a compositional approach to prompt learning. Instead of learning monolithic prompts, $\\mathtt{PromptMoE}$ learns a pool of expert prompts, which serve as a basis set of composable semantic primitives, and a visually-guided Mixture-of-Experts (MoE) mechanism to dynamically combine them for each instance. Our framework materializes this concept through a Visually-Guided Mixture of Prompt (VGMoP) that employs an image-gated sparse MoE to aggregate diverse normal and abnormal expert state prompts, generating semantically rich textual representations with strong generalization. Extensive experiments across 15 datasets in industrial and medical domains demonstrate the effectiveness and state-of-the-art performance of $\\mathtt{PromptMoE}$.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18131", "abs": "https://arxiv.org/abs/2511.18131", "authors": ["Xiaofan Li", "Yanpeng Sun", "Chenming Wu", "Fan Duan", "YuAn Wang", "Weihao Bo", "Yumeng Zhang", "Dingkang Liang"], "title": "Video4Edit: Viewing Image Editing as a Degenerate Temporal Process", "comment": "10 pages, 5 figures", "summary": "We observe that recent advances in multimodal foundation models have propelled instruction-driven image generation and editing into a genuinely cross-modal, cooperative regime. Nevertheless, state-of-the-art editing pipelines remain costly: beyond training large diffusion/flow models, they require curating massive high-quality triplets of \\{instruction, source image, edited image\\} to cover diverse user intents. Moreover, the fidelity of visual replacements hinges on how precisely the instruction references the target semantics. We revisit this challenge through the lens of temporal modeling: if video can be regarded as a full temporal process, then image editing can be seen as a degenerate temporal process. This perspective allows us to transfer single-frame evolution priors from video pre-training, enabling a highly data-efficient fine-tuning regime. Empirically, our approach matches the performance of leading open-source baselines while using only about one percent of the supervision demanded by mainstream editing models.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18115", "abs": "https://arxiv.org/abs/2511.18115", "authors": ["Wenyu Li", "Sidun Liu", "Peng Qiao", "Yong Dou", "Tongrui Hu"], "title": "Muskie: Multi-view Masked Image Modeling for 3D Vision Pre-training", "comment": null, "summary": "We present Muskie, a native multi-view vision backbone designed for 3D vision tasks. Unlike existing models, which are frame-wise and exhibit limited multi-view consistency, Muskie is designed to process multiple views simultaneously and introduce multi-view consistency in pre-training stage. Muskie is trained to reconstruct heavily masked content in one view by finding and utilizing geometric correspondences from other views. Through this pretext task and our proposed aggressive masking strategy, the model implicitly to learn view-invariant features and develop strong geometric understanding without any 3D supervision. Compared with state-of-the-art frame-wise backbones such as DINO, Muskie achieves higher multi-view correspondence accuracy. Furthermore, we demonstrate that using Muskie as a backbone consistently enhances performance on downstream 3D tasks, including camera pose estimation and pointmap reconstruction. Codes are publicly available at https://leo-frank.github.io/Muskie/", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18232", "abs": "https://arxiv.org/abs/2511.18232", "authors": ["Mingi Kang"], "title": "Parallel qMRI Reconstruction from 4x Accelerated Acquisitions", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) acquisitions require extensive scan times, limiting patient throughput and increasing susceptibility to motion artifacts. Accelerated parallel MRI techniques reduce acquisition time by undersampling k-space data, but require robust reconstruction methods to recover high-quality images. Traditional approaches like SENSE require both undersampled k-space data and pre-computed coil sensitivity maps. We propose an end-to-end deep learning framework that jointly estimates coil sensitivity maps and reconstructs images from only undersampled k-space measurements at 4x acceleration. Our two-module architecture consists of a Coil Sensitivity Map (CSM) estimation module and a U-Net-based MRI reconstruction module. We evaluate our method on multi-coil brain MRI data from 10 subjects with 8 echoes each, using 2x SENSE reconstructions as ground truth. Our approach produces visually smoother reconstructions compared to conventional SENSE output, achieving comparable visual quality despite lower PSNR/SSIM metrics. We identify key challenges including spatial misalignment between different acceleration factors and propose future directions for improved reconstruction quality.", "AI": {"tldr": "该研究提出一个端到端深度学习框架，用于从欠采样k空间数据中联合估计线圈敏感度图并重建MRI图像，无需预先计算敏感度图。", "motivation": "MRI扫描时间过长，限制了患者吞吐量并增加了运动伪影的风险。加速并行MRI技术通过欠采样缩短时间，但需要稳健的重建方法。传统方法（如SENSE）需要预先计算线圈敏感度图。", "method": "该方法是一个端到端深度学习框架，包含两个模块：线圈敏感度图（CSM）估计模块和一个基于U-Net的MRI重建模块。它仅从4倍加速的欠采样k空间测量数据中同时估计CSM并重建图像。在10名受试者的多线圈脑部MRI数据（每个受试者8个回波）上进行评估，以2倍SENSE重建结果作为真值。", "result": "与传统SENSE输出相比，该方法产生了视觉上更平滑的重建图像，尽管PSNR/SSIM指标较低，但视觉质量相当。研究还指出了不同加速因子之间空间错位等关键挑战。", "conclusion": "该端到端深度学习框架能有效从欠采样k空间数据中联合估计线圈敏感度图并重建MRI图像，提供与传统SENSE可比的视觉质量。未来工作将致力于解决空间错位等问题以进一步提高重建质量。"}}
{"id": "2511.18242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18242", "abs": "https://arxiv.org/abs/2511.18242", "authors": ["Yogesh Kulkarni", "Pooyan Fazli"], "title": "EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning", "comment": null, "summary": "Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion. We introduce $\\textbf{EgoVITA}$, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an $\\textbf{egocentric planning phase}$, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an $\\textbf{exocentric verification phase}$, where it switches to a third-person perspective to check the visual and logical consistency of that plan. Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by $\\mathbf{+7.7}$ on EgoBlind and $\\mathbf{+4.4}$ on EgoOrient, while maintaining strong generalization on exocentric video tasks.", "AI": {"tldr": "EgoVITA是一个强化学习框架，通过结合第一人称规划和第三人称验证，使多模态大语言模型（MLLMs）能更好地进行第一人称意图和动作推理。", "motivation": "多模态大语言模型（MLLMs）在第一人称（自我中心）视角下的意图和动作推理面临巨大挑战。与第三人称视频不同，自我中心视频反映的是行动者不断变化的视角，具有部分可观察性、有限视野和自我参照运动等特点。", "method": "本文提出了EgoVITA，一个基于Group Relative Policy Optimization (GRPO) 的强化学习框架。EgoVITA包含两个交替阶段：1) 自我中心规划阶段，模型从第一人称视角预测未来的逐步行动计划；2) 异中心验证阶段，模型切换到第三人称视角检查计划的视觉和逻辑一致性。通过GRPO，模型学习制定能够因果预测未来视觉观察的计划。", "result": "EgoVITA在自我中心推理任务上取得了显著提升，在EgoBlind上超越基线Qwen2.5-VL-7B +7.7，在EgoOrient上超越 +4.4。同时，它在异中心视频任务上保持了强大的泛化能力。", "conclusion": "EgoVITA通过其独特的自我中心规划和异中心验证机制，有效解决了MLLMs在第一人称推理中的挑战，显著提高了性能，并展现了良好的跨任务泛化能力。"}}
{"id": "2511.18200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18200", "abs": "https://arxiv.org/abs/2511.18200", "authors": ["Haoming Wang", "Qiyao Xue", "Wei Gao"], "title": "InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity", "comment": null, "summary": "Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.", "AI": {"tldr": "本文提出了 InfiniBench，一个全自动、可定制且用户友好的基准生成器，能够合成理论上无限多样的3D场景，以评估视觉-语言模型（VLMs）的空间推理能力。", "motivation": "现有基准在场景复杂性方面定制性有限，无法隔离和分析VLM在不同空间条件下的特定故障模式。缺乏多样化、可扩展且完全可定制的基准来评估VLM的空间推理能力。", "method": "InfiniBench 将自然语言场景描述转化为逼真的视频。其核心创新包括：1) 基于LLM的代理框架，迭代优化场景描述中的程序性约束；2) 灵活的基于聚类的布局优化器，生成以前程序方法难以处理的密集和杂乱场景；3) 任务感知型摄像机轨迹优化方法，渲染场景为具有完整物体覆盖的视频作为VLM输入。", "result": "实验表明，InfiniBench 在提示忠实度和物理合理性方面优于最先进的程序性和基于LLM的3D生成方法，尤其是在高复杂性场景中表现突出。", "conclusion": "InfiniBench 成功地为测量、视角采纳和时空跟踪等代表性空间推理任务生成了基准，展示了其在评估VLM空间推理能力方面的实用性。"}}
{"id": "2511.18185", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18185", "abs": "https://arxiv.org/abs/2511.18185", "authors": ["Yutong Wu", "Yifan Wang", "Qining Zhang", "Chuan Zhou", "Lei Ying"], "title": "Early Lung Cancer Diagnosis from Virtual Follow-up LDCT Generation via Correlational Autoencoder and Latent Flow Matching", "comment": "10 pages, 3 figures", "summary": "Lung cancer is one of the most commonly diagnosed cancers, and early diagnosis is critical because the survival rate declines sharply once the disease progresses to advanced stages. However, achieving an early diagnosis remains challenging, particularly in distinguishing subtle early signals of malignancy from those of benign conditions. In clinical practice, a patient with a high risk may need to undergo an initial baseline and several annual follow-up examinations (e.g., CT scans) before receiving a definitive diagnosis, which can result in missing the optimal treatment. Recently, Artificial Intelligence (AI) methods have been increasingly used for early diagnosis of lung cancer, but most existing algorithms focus on radiomic features extraction from single early-stage CT scans. Inspired by recent advances in diffusion models for image generation, this paper proposes a generative method, named CorrFlowNet, which creates a virtual, one-year follow-up CT scan after the initial baseline scan. This virtual follow-up would allow for an early detection of malignant/benign nodules, reducing the need to wait for clinical follow-ups. During training, our approach employs a correlational autoencoder to encode both early baseline and follow-up CT images into a latent space that captures the dynamics of nodule progression as well as the correlations between them, followed by a flow matching algorithm on the latent space with a neural ordinary differential equation. An auxiliary classifier is used to further enhance the diagnostic accuracy. Evaluations on a real clinical dataset show our method can significantly improve downstream lung nodule risk assessment compared with existing baseline models. Moreover, its diagnostic accuracy is comparable with real clinical CT follow-ups, highlighting its potential to improve cancer diagnosis.", "AI": {"tldr": "本文提出CorrFlowNet，一种生成式AI方法，通过从初始CT扫描生成虚拟的一年期随访CT扫描，以实现肺癌结节的早期诊断，减少患者等待时间并提高诊断准确性。", "motivation": "肺癌早期诊断至关重要但极具挑战性，难以区分早期恶性与良性信号。临床实践中，患者常需多次年度随访检查才能确诊，可能错过最佳治疗时机。现有AI算法多关注单次早期CT扫描的影像组学特征，未能有效利用时间维度信息。", "method": "CorrFlowNet受扩散模型启发，采用生成式方法。它使用一个相关自编码器将早期基线和真实随访CT图像编码到潜在空间，捕获结节进展动态及图像间相关性。随后，在潜在空间应用流匹配算法（结合神经常微分方程）生成虚拟的一年期随访CT扫描。此外，引入辅助分类器以进一步提高诊断准确性。", "result": "在真实临床数据集上的评估表明，CorrFlowNet相较于现有基线模型能显著改善下游肺结节风险评估。其诊断准确性与真实的临床CT随访结果相当。", "conclusion": "CorrFlowNet通过生成虚拟随访CT扫描，有望显著改善肺癌的早期诊断，减少患者等待确诊的时间，并达到与真实临床随访相媲美的诊断准确性，展现出巨大的临床应用潜力。"}}
{"id": "2511.18139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18139", "abs": "https://arxiv.org/abs/2511.18139", "authors": ["Shuhuan Wang", "Yuzhen Xie", "Jiayi Li"], "title": "Compact neural networks for astronomy with optimal transport bias correction", "comment": "18 pages, 5 figures, 3 tables. Research article", "summary": "Astronomical imaging confronts an efficiency-resolution tradeoff that limits large-scale morphological classification and redshift prediction. We introduce WaveletMamba, a theory-driven framework integrating wavelet decomposition with state-space modeling, mathematical regularization, and multi-level bias correction. WaveletMamba achieves 81.72% +/- 0.53% classification accuracy at 64x64 resolution with only 3.54M parameters, delivering high-resolution performance (80.93% +/- 0.27% at 244x244) at low-resolution inputs with 9.7x computational efficiency gains. The framework exhibits Resolution Multistability, where models trained on low-resolution data achieve consistent accuracy across different input scales despite divergent internal representations. The framework's multi-level bias correction synergizes HK distance (distribution-level optimal transport) with Color-Aware Weighting (sample-level fine-tuning), achieving 22.96% Log-MSE improvement and 26.10% outlier reduction without explicit selection function modeling. Here, we show that mathematical rigor enables unprecedented efficiency and comprehensive bias correction in scientific AI, bridging computer vision and astrophysics to revolutionize interdisciplinary scientific discovery.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18164", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18164", "abs": "https://arxiv.org/abs/2511.18164", "authors": ["Chunming He", "Rihan Zhang", "Dingming Zhang", "Fengyang Xiao", "Deng-Ping Fan", "Sina Farsiu"], "title": "Nested Unfolding Network for Real-World Concealed Object Segmentation", "comment": "6 figures, 14 tables", "summary": "Deep unfolding networks (DUNs) have recently advanced concealed object segmentation (COS) by modeling segmentation as iterative foreground-background separation. However, existing DUN-based methods (RUN) inherently couple background estimation with image restoration, leading to conflicting objectives and requiring pre-defined degradation types, which are unrealistic in real-world scenarios. To address this, we propose the nested unfolding network (NUN), a unified framework for real-world COS. NUN adopts a DUN-in-DUN design, embedding a degradation-resistant unfolding network (DeRUN) within each stage of a segmentation-oriented unfolding network (SODUN). This design decouples restoration from segmentation while allowing mutual refinement. Guided by a vision-language model (VLM), DeRUN dynamically infers degradation semantics and restores high-quality images without explicit priors, whereas SODUN performs reversible estimation to refine foreground and background. Leveraging the multi-stage nature of unfolding, NUN employs image-quality assessment to select the best DeRUN outputs for subsequent stages, naturally introducing a self-consistency loss that enhances robustness. Extensive experiments show that NUN achieves a leading place on both clean and degraded benchmarks. Code will be released.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18262", "abs": "https://arxiv.org/abs/2511.18262", "authors": ["Tao Shen", "Xin Wan", "Taicai Chen", "Rui Zhang", "Junwen Pan", "Dawei Lu", "Fanding Lei", "Zhilin Lu", "Yunfei Yang", "Chen Cheng", "Qi She", "Chang Liu", "Zhenbang Sun"], "title": "MammothModa2: A Unified AR-Diffusion Framework for Multimodal Understanding and Generation", "comment": null, "summary": "Unified multimodal models aim to integrate understanding and generation within a single framework, yet bridging the gap between discrete semantic reasoning and high-fidelity visual synthesis remains challenging. We present MammothModa2 (Mammoth2), a unified autoregressive-diffusion (AR-Diffusion) framework designed to effectively couple autoregressive semantic planning with diffusion-based generation. Mammoth2 adopts a serial design: an AR path equipped with generation experts performs global semantic modeling over discrete tokens, while a single-stream Diffusion Transformer (DiT) decoder handles high-fidelity image synthesis. A carefully designed AR-Diffusion feature alignment module combines multi-layer feature aggregation, unified condition encoding, and in-context conditioning to stably align AR's representations with the diffusion decoder's continuous latents. Mammoth2 is trained end-to-end with joint Next-Token Prediction and Flow Matching objectives, followed by supervised fine-tuning and reinforcement learning over both generation and editing. With roughly 60M supervised generation samples and no reliance on pre-trained generators, Mammoth2 delivers strong text-to-image and instruction-based editing performance on public benchmarks, achieving 0.87 on GenEval, 87.2 on DPGBench, and 4.06 on ImgEdit, while remaining competitive with understanding-only backbones (e.g., Qwen3-VL-8B) on multimodal understanding tasks. These results suggest that a carefully coupled AR-Diffusion architecture can provide high-fidelity generation and editing while maintaining strong multimodal comprehension within a single, parameter- and data-efficient model.", "AI": {"tldr": "MammothModa2 (Mammoth2) 是一个统一的自回归-扩散 (AR-Diffusion) 框架，旨在将离散语义规划与高保真扩散生成有效结合，并在生成、编辑和多模态理解任务上表现出色。", "motivation": "统一的多模态模型在整合理解和生成方面面临挑战，尤其是在弥合离散语义推理与高保真视觉合成之间的鸿沟。", "method": "Mammoth2 采用串行设计：一个配备生成专家的 AR 路径负责离散令牌的全局语义建模，而一个单流扩散 Transformer (DiT) 解码器处理高保真图像合成。一个精心设计的 AR-Diffusion 特征对齐模块，结合多层特征聚合、统一条件编码和上下文条件化，以稳定对齐 AR 表示与扩散解码器的连续潜在空间。该模型通过联合 Next-Token Prediction 和 Flow Matching 目标进行端到端训练，随后进行生成和编辑的监督微调和强化学习。", "result": "Mammoth2 在未依赖预训练生成器的情况下，仅用约 60M 监督生成样本，在文本到图像和基于指令的编辑任务上表现出色，GenEval 达到 0.87，DPGBench 达到 87.2，ImgEdit 达到 4.06。同时，在多模态理解任务上与仅理解骨干网络（如 Qwen3-VL-8B）保持竞争力。", "conclusion": "研究结果表明，精心耦合的 AR-Diffusion 架构可以在单个参数和数据高效的模型中，提供高保真生成和编辑能力，同时保持强大的多模态理解能力。"}}
{"id": "2511.18192", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18192", "abs": "https://arxiv.org/abs/2511.18192", "authors": ["Ahmad Mohammadshirazi", "Pinaki Prasad Guha Neogi", "Dheeraj Kulshrestha", "Rajiv Ramnath"], "title": "ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization", "comment": null, "summary": "Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.", "AI": {"tldr": "ARIAL是一个模块化框架，通过LLM代理协调专业工具，在文档视觉问答（VQA）中实现了文本答案的高精度提取和可靠的空间定位，同时提高了可解释性。", "motivation": "现有文档VQA系统在实现高文本准确性的同时，空间定位不可靠，或者为了可解释性牺牲了性能。在高风险应用中，同时需要准确的文本答案和精确的空间定位，以确保可解释性。", "method": "ARIAL（Agentic Reasoning for Interpretable Answer Localization）是一个模块化框架，它利用基于LLM的规划代理来协调专用工具。它将文档VQA分解为结构化子任务：基于OCR的文本提取（使用TrOCR）、检索增强的上下文选择（使用语义搜索）、答案生成（通过微调的Gemma 3-27B模型）以及通过文本到区域对齐进行的显式边界框定位。这种模块化架构提供了透明的推理轨迹，支持工具级别的可审计性和独立组件优化。", "result": "ARIAL在四个基准测试（DocVQA、FUNSD、CORD和SROIE）上均取得了最先进的结果，涵盖了文本准确性（ANLS）和空间精度（mAP at IoU 0.50 to 0.95）。具体而言，在DocVQA上，ARIAL实现了88.7 ANLS和50.1 mAP，在FUNSD上为90.0 ANLS和50.3 mAP，在CORD上为85.5 ANLS和60.2 mAP，在SROIE上为93.1 ANLS，在DocVQA上比之前的最佳方法（DLaVA）分别提高了+2.8 ANLS和+3.9 mAP。", "conclusion": "该研究表明，通过代理协调专业工具可以同时提高文档VQA的性能和可解释性，为构建值得信赖、可解释的文档AI系统提供了途径。"}}
{"id": "2511.18204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18204", "abs": "https://arxiv.org/abs/2511.18204", "authors": ["Pavan Narahari", "Suraj Rajendran", "Lorena Bori", "Jonas E. Malmsten", "Qiansheng Zhan", "Zev Rosenwaks", "Nikica Zaninovic", "Iman Hajirasouliha"], "title": "Generating Synthetic Human Blastocyst Images for In-Vitro Fertilization Blastocyst Grading", "comment": "The manuscript is 23 pages, with five main figures and one table. The supplemental material includes 23 pages with fourteen figures and four tables", "summary": "The success of in vitro fertilization (IVF) at many clinics relies on the accurate morphological assessment of day 5 blastocysts, a process that is often subjective and inconsistent. While artificial intelligence can help standardize this evaluation, models require large, diverse, and balanced datasets, which are often unavailable due to data scarcity, natural class imbalance, and privacy constraints. Existing generative embryo models can mitigate these issues but face several limitations, such as poor image quality, small training datasets, non-robust evaluation, and lack of clinically relevant image generation for effective data augmentation. Here, we present the Diffusion Based Imaging Model for Artificial Blastocysts (DIA) framework, a set of latent diffusion models trained to generate high-fidelity, novel day 5 blastocyst images. Our models provide granular control by conditioning on Gardner-based morphological categories and z-axis focal depth. We rigorously evaluated the models using FID, a memorization metric, an embryologist Turing test, and three downstream classification tasks. Our results show that DIA models generate realistic images that embryologists could not reliably distinguish from real images. Most importantly, we demonstrated clear clinical value. Augmenting an imbalanced dataset with synthetic images significantly improved classification accuracy (p < 0.05). Also, adding synthetic images to an already large, balanced dataset yielded statistically significant performance gains, and synthetic data could replace up to 40% of real data in some cases without a statistically significant loss in accuracy. DIA provides a robust solution for mitigating data scarcity and class imbalance in embryo datasets. By generating novel, high-fidelity, and controllable synthetic images, our models can improve the performance, fairness, and standardization of AI embryo assessment tools.", "AI": {"tldr": "该研究提出了DIA框架，一个基于扩散模型的工具，用于生成高保真、可控的囊胚图像，以解决体外受精（IVF）中胚胎评估AI模型的数据稀缺和类别不平衡问题，并显著提高了分类准确性。", "motivation": "体外受精（IVF）中囊胚的形态评估通常主观且不一致。尽管人工智能可以实现标准化，但AI模型需要大量、多样化且平衡的数据集，而这些数据因稀缺性、自然类别不平衡和隐私限制而难以获取。现有的胚胎生成模型存在图像质量差、训练数据集小、评估不严谨以及缺乏临床相关图像生成等局限性。", "method": "研究提出了“基于扩散的成像模型用于人工囊胚”（DIA）框架，该框架是一组经过训练的潜在扩散模型，旨在生成高保真、新颖的第5天囊胚图像。模型通过以Gardner形态学类别和Z轴焦深为条件，提供精细的控制。研究通过FID、记忆化指标、胚胎学家图灵测试和三个下游分类任务对模型进行了严格评估。", "result": "DIA模型生成的图像非常逼真，胚胎学家无法可靠地将其与真实图像区分开来。最重要的是，研究证明了其明确的临床价值：用合成图像增强不平衡数据集显著提高了分类准确性（p < 0.05）。即使在已经很大且平衡的数据集中添加合成图像，也获得了统计学上显著的性能提升，并且在某些情况下，合成数据可以替代高达40%的真实数据，而准确性没有统计学上的显著损失。", "conclusion": "DIA为缓解胚胎数据集中的数据稀缺和类别不平衡问题提供了一个强大的解决方案。通过生成新颖、高保真和可控的合成图像，DIA模型可以提高AI胚胎评估工具的性能、公平性和标准化水平。"}}
{"id": "2511.18254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18254", "abs": "https://arxiv.org/abs/2511.18254", "authors": ["Siyi Li", "Qingwen Zhang", "Ishan Khatri", "Kyle Vedder", "Deva Ramanan", "Neehar Peri"], "title": "UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization", "comment": "Project Page: https://lisiyi777.github.io/UniFlow/", "summary": "LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.", "AI": {"tldr": "本文提出UniFlow模型，通过跨数据集训练，显著提升了激光雷达场景流估计的泛化能力和精度，在多个数据集上达到最先进水平，包括对未见传感器的适应。", "motivation": "现有激光雷达场景流方法通常只在一个传感器上训练和评估，难以泛化到多样且未见的传感器。以往研究认为，在多个数据集上进行训练会降低低级任务的性能。本文旨在挑战这一传统观念，探索学习能泛化到不同激光雷达传感器的通用运动先验。", "method": "本文发现，与语义分割和3D目标检测不同，场景流估计这类低级任务从跨数据集训练中受益匪浅。基于此发现，作者提出了UniFlow，一个前馈模型家族，它统一并在多个具有不同传感器布置和点云密度的大规模激光雷达场景流数据集上进行训练。", "result": "UniFlow在Waymo和nuScenes数据集上分别比现有方法提升了5.1%和35.2%，达到了新的最先进水平。此外，UniFlow在TruckScenes等未见数据集上表现出色，比特定于TruckScenes的模型提高了30.1%。研究还表明，针对快速移动物体训练的模型在不同数据集上对快速移动物体依然表现良好。", "conclusion": "跨数据集训练对于激光雷达场景流估计非常有效，它能学习到更具泛化性的运动先验，从而在多样化和未见的传感器上实现卓越的性能。这一发现颠覆了以往关于多数据集训练对低级任务性能影响的传统认知。"}}
{"id": "2511.18222", "categories": ["cs.CV", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.18222", "abs": "https://arxiv.org/abs/2511.18222", "authors": ["Victor Ferrari", "Marcio Pereira", "Lucas Alvarenga", "Gustavo Leite", "Guido Araujo"], "title": "Using MLIR Transform to Design Sliced Convolution Algorithm", "comment": null, "summary": "This paper proposes SConvTransform, a Transform dialect extension that provides operations for optimizing 2D convolutions in MLIR. Its main operation, SConvOp, lowers Linalg convolutions into tiled and packed generic operations through a fully declarative transformation pipeline. The process is guided by a Convolution Slicing Analysis that determines tile sizes and data layout strategies based on input and filter shapes, as well as target architecture parameters. SConvOp handles edge cases by splitting irregular regions and adjusting affine maps where needed. All packing and tiling operations are derived from a parametric set of affine equations, enabling reusable and analyzable transformations. Although functional correctness was the primary goal of this work, the experimental evaluation demonstrates the effectiveness of SConvTransform, achieving good enough performance across different target architectures. Future work will focus on optimizing performance and porting to other target devices. When applied to standard convolution configurations, the generated code achieves up to 60% of peak performance on ARM SME and 67% on Intel AVX512. These results validate the benefit of combining static shape analysis with structured tiling and packing strategies within the MLIR Transform dialect. Furthermore, the modular design of SConvTransform facilitates integration with future extensions, enabling continued optimization of convolution workloads through MLIR's extensible compilation infrastructure.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18271", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18271", "abs": "https://arxiv.org/abs/2511.18271", "authors": ["Tianyang Han", "Junhao Su", "Junjie Hu", "Peizhen Yang", "Hengyu Shi", "Junfeng Luo", "Jialin Gao"], "title": "Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models", "comment": null, "summary": "Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems.", "AI": {"tldr": "当前文生图模型在隐式世界知识和物理因果推理方面存在局限。本文引入了PicWorld基准和PW-Agent评估器，对主流模型进行了评估，发现它们普遍存在这些能力不足，呼吁开发更具推理和知识整合能力的模型。", "motivation": "尽管文生图模型能生成逼真且符合指令的图像，但在需要隐式世界知识的提示上频繁失败。现有评估协议侧重于组合对齐或单轮VQA，未能充分测试知识基础、多物理交互和可审计证据等关键维度。", "method": "本文提出了PicWorld，一个包含1,100个提示的综合基准，用于评估文生图模型对隐式世界知识和物理因果推理的掌握。为实现细粒度评估，本文还提出了PW-Agent，一个基于证据的多智能体评估器，通过将提示分解为可验证的视觉证据，分层评估图像的物理真实性和逻辑一致性。", "result": "对17个主流文生图模型在PicWorld上的分析表明，它们普遍在隐式世界知识和物理因果推理能力上表现出不同程度的根本性局限。", "conclusion": "研究结果强调了未来文生图系统需要开发具备推理感知和知识整合能力的架构。"}}
{"id": "2511.18272", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18272", "abs": "https://arxiv.org/abs/2511.18272", "authors": ["Richard J. Young"], "title": "Vision Token Masking Alone Cannot Prevent PHI Leakage in Medical Document OCR: A Systematic Evaluation", "comment": "24 pages, 11 figures, 2 tables", "summary": "Large vision-language models (VLMs) are increasingly deployed for optical character recognition (OCR) in healthcare settings, raising critical concerns about protected health information (PHI) exposure during document processing. This work presents the first systematic evaluation of inference-time vision token masking as a privacy-preserving mechanism for medical document OCR using DeepSeek-OCR. We introduce seven masking strategies (V3-V9) targeting different architectural layers (SAM encoder blocks, compression layers, dual vision encoders, projector fusion) and evaluate PHI reduction across HIPAA-defined categories using 100 synthetic medical billing statements (drawn from a corpus of 38,517 annotated documents) with perfect ground-truth annotations. All masking strategies converge to 42.9% PHI reduction, successfully suppressing long-form spatially-distributed identifiers (patient names, dates of birth, physical addresses at 100% effectiveness) while failing to prevent short structured identifiers (medical record numbers, social security numbers, email addresses, account numbers at 0% effectiveness). Ablation studies varying mask expansion radius (r=1,2,3) demonstrate that increased spatial coverage does not improve reduction beyond this ceiling, indicating that language model contextual inference - not insufficient visual masking - drives structured identifier leakage. A simulated hybrid architecture combining vision masking with NLP post-processing achieves 88.6% total PHI reduction (assuming 80% NLP accuracy on remaining identifiers). This negative result establishes boundaries for vision-only privacy interventions in VLMs, provides guidance distinguishing PHI types amenable to vision-level versus language-level redaction, and redirects future research toward decoder-level fine-tuning and hybrid defense-in-depth architectures for HIPAA-compliant medical document processing.", "AI": {"tldr": "本研究首次系统评估了推理时视觉 token 遮罩作为医学文档 OCR 中保护隐私的机制，发现其对长文本 PHI 有效，但对短结构化 PHI 无效，并提出需要混合架构进行全面保护。", "motivation": "大型视觉-语言模型（VLMs）在医疗保健环境中越来越多地用于光学字符识别（OCR），这引发了在文档处理过程中受保护健康信息（PHI）暴露的关键担忧。", "method": "本研究使用 DeepSeek-OCR，引入了七种针对不同架构层（SAM 编码器块、压缩层、双视觉编码器、投影仪融合）的遮罩策略（V3-V9）。通过 100 份合成医疗账单（来自 38,517 份标注文档）评估了 HIPAA 定义类别下的 PHI 减少情况，并进行了遮罩扩展半径（r=1,2,3）的消融研究。此外，还模拟了一个结合视觉遮罩和 NLP 后处理的混合架构。", "result": "所有遮罩策略均收敛到 42.9% 的 PHI 减少率，成功抑制了长文本空间分布式标识符（患者姓名、出生日期、物理地址，有效性达 100%），但未能阻止短结构化标识符（医疗记录号、社会安全号、电子邮件地址、账号，有效性为 0%）。增加空间覆盖范围并不能提高减少率。结果表明，语言模型的上下文推理而非视觉遮罩不足导致了结构化标识符的泄露。模拟的混合架构实现了 88.6% 的总 PHI 减少率（假设 NLP 对剩余标识符的准确率为 80%）。", "conclusion": "本研究为 VLM 中纯视觉隐私干预设定了界限，为区分适合视觉层面和语言层面修订的 PHI 类型提供了指导，并将未来的研究方向引向解码器级别的微调和混合深度防御架构，以实现符合 HIPAA 规范的医疗文档处理。"}}
{"id": "2511.18208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18208", "abs": "https://arxiv.org/abs/2511.18208", "authors": ["Ahmed Gomaa", "Annette Schwarz", "Ludwig Singer", "Arnd Dörfler", "Matthias Stefan May", "Pluvio Stephan", "Ishita Sheth", "Juliane Szkitsak", "Katharina Breininger", "Yixing Huang", "Benjamin Frey", "Oliver Schnell", "Daniel Delev", "Roland Coras", "Daniel Höfler", "Philipp Schubert", "Jenny Stritzelberger", "Sabine Semrau", "Andreas Maier", "Dieter H Heiland", "Udo S. Gaipl", "Andrea Wittig", "Rainer Fietkau", "Christoph Bert", "Stefanie Corradini", "Florian Putz"], "title": "Large-Scale Pre-training Enables Multimodal AI Differentiation of Radiation Necrosis from Brain Metastasis Progression on Routine MRI", "comment": null, "summary": "Background: Differentiating radiation necrosis (RN) from tumor progression after stereotactic radiosurgery (SRS) remains a critical challenge in brain metastases. While histopathology represents the gold standard, its invasiveness limits feasibility. Conventional supervised deep learning approaches are constrained by scarce biopsy-confirmed training data. Self-supervised learning (SSL) overcomes this by leveraging the growing availability of large-scale unlabeled brain metastases imaging datasets. Methods: In a two-phase deep learning strategy inspired by the foundation model paradigm, a Vision Transformer (ViT) was pre-trained via SSL on 10,167 unlabeled multi-source T1CE MRI sub-volumes. The pre-trained ViT was then fine-tuned for RN classification using a two-channel input (T1CE MRI and segmentation masks) on the public MOLAB dataset (n=109) using 20% of datasets as same-center held-out test set. External validation was performed on a second-center test cohort (n=28). Results: The self-supervised model achieved an AUC of 0.916 on the same-center test set and 0.764 on the second center test set, surpassing the fully supervised ViT (AUC 0.624/0.496; p=0.001/0.008) and radiomics (AUC 0.807/0.691; p=0.005/0.014). Multimodal integration further improved performance (AUC 0.947/0.821; p=0.073/0.001). Attention map visualizations enabled interpretability showing the model focused on clinically relevant lesion subregions. Conclusion: Large-scale pre-training on increasingly available unlabeled brain metastases datasets substantially improves AI model performance. A two-phase multimodal deep learning strategy achieved high accuracy in differentiating radiation necrosis from tumor progression using only routine T1CE MRI and standard clinical data, providing an interpretable, clinically accessible solution that warrants further validation.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18255", "abs": "https://arxiv.org/abs/2511.18255", "authors": ["Sina Mokhtarzadeh Azar", "Emad Bahrami", "Enrico Pallotta", "Gianpiero Francesca", "Radu Timofte", "Juergen Gall"], "title": "Sequence-Adaptive Video Prediction in Continuous Streams using Diffusion Noise Optimization", "comment": null, "summary": "In this work, we investigate diffusion-based video prediction models, which forecast future video frames, for continuous video streams. In this context, the models observe continuously new training samples, and we aim to leverage this to improve their predictions. We thus propose an approach that continuously adapts a pre-trained diffusion model to a video stream. Since fine-tuning the parameters of a large diffusion model is too expensive, we refine the diffusion noise during inference while keeping the model parameters frozen, allowing the model to adaptively determine suitable sampling noise. We term the approach Sequence Adaptive Video Prediction with Diffusion Noise Optimization (SAVi-DNO). To validate our approach, we introduce a new evaluation setting on the Ego4D dataset, focusing on simultaneous adaptation and evaluation on long continuous videos. Empirical results demonstrate improved performance based on FVD, SSIM, and PSNR metrics on long videos of Ego4D and OpenDV-YouTube, as well as videos of UCF-101 and SkyTimelapse, showcasing SAVi-DNO's effectiveness.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18305", "abs": "https://arxiv.org/abs/2511.18305", "authors": ["Raja Kumar", "Arka Sadhu", "Ram Nevatia"], "title": "DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition", "comment": null, "summary": "Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\\textbf{DiVE-k}$, $\\textbf{Di}$fferential $\\textbf{V}$isual r$\\textbf{E}$asoning using top-$\\textbf{k}$ generations, framework that leverages model's own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model's top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios.", "AI": {"tldr": "大型视觉语言模型（LVLMs）在细粒度图像识别方面表现不佳，本文提出DiVE-k框架，利用模型自身的top-k预测作为强化学习信号，通过多项选择题训练模型进行差异化推理，显著提高了泛化能力。", "motivation": "大型视觉语言模型（LVLMs）在细粒度图像识别上难以区分视觉相似的类别。现有基于强化学习的微调方法奖励信号脆弱，易导致记忆化，且未能促进泛化所需的差异化推理。", "method": "提出DiVE-k框架。对于每张训练图像，DiVE-k利用模型自身的top-k预测生成多项选择题，并使用强化学习训练模型选择正确答案。这种方法促使模型在多个合理选项中进行细粒度差异化推理，并提供简单可验证的奖励信号，从而减轻记忆化并改善泛化。", "result": "DiVE-k在五个标准细粒度数据集上显著优于现有方法。在标准基类到新类泛化设置中，DiVE-k在调和平均指标上分别超过QWEN2.5-VL-7B和ViRFT 10.04%和6.16%。在混合领域和少样本场景中也观察到类似的提升。", "conclusion": "DiVE-k通过利用模型自身的top-k预测进行差异化视觉推理，有效解决了LVLMs在细粒度图像识别中的挑战，显著提高了模型在各种泛化设置下的性能。"}}
{"id": "2511.18281", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18281", "abs": "https://arxiv.org/abs/2511.18281", "authors": ["Yara Bahram", "Melodie Desbos", "Mohammadhadi Shateri", "Eric Granger"], "title": "Uni-DAD: Unified Distillation and Adaptation of Diffusion Models for Few-step Few-shot Image Generation", "comment": "Under review paper at CVPR 2026", "summary": "Diffusion models (DMs) produce high-quality images, yet their sampling remains costly when adapted to new domains. Distilled DMs are faster but typically remain confined within their teacher's domain. Thus, fast and high-quality generation for novel domains relies on two-stage training pipelines: Adapt-then-Distill or Distill-then-Adapt. However, both add design complexity and suffer from degraded quality or diversity. We introduce Uni-DAD, a single-stage pipeline that unifies distillation and adaptation of DMs. It couples two signals during training: (i) a dual-domain distribution-matching distillation objective that guides the student toward the distributions of the source teacher and a target teacher, and (ii) a multi-head generative adversarial network (GAN) loss that encourages target realism across multiple feature scales. The source domain distillation preserves diverse source knowledge, while the multi-head GAN stabilizes training and reduces overfitting, especially in few-shot regimes. The inclusion of a target teacher facilitates adaptation to more structurally distant domains. We perform evaluations on a variety of datasets for few-shot image generation (FSIG) and subject-driven personalization (SDP). Uni-DAD delivers higher quality than state-of-the-art (SoTA) adaptation methods even with less than 4 sampling steps, and outperforms two-stage training pipelines in both quality and diversity.", "AI": {"tldr": "扩散模型在新域适应时采样成本高，现有蒸馏方法受限于教师域，两阶段训练管线复杂且存在质量或多样性问题。Uni-DAD提出单阶段管线，统一蒸馏与适应，通过双域分布匹配蒸馏和多头GAN损失，实现新域快速、高质量、多样性生成，优于现有方法。", "motivation": "扩散模型生成图像质量高，但在新域适应时采样成本高昂。已有的蒸馏扩散模型虽然快速，但通常局限于其教师模型的领域。目前针对新域的快速高质量生成依赖于两阶段训练管线（先适应后蒸馏或先蒸馏后适应），但这些方法增加了设计复杂性，并导致质量或多样性下降。因此，需要一种单阶段、高效且高质量的解决方案来解决新域生成问题。", "method": "Uni-DAD是一种单阶段管线，统一了扩散模型的蒸馏和适应过程。它在训练过程中耦合了两个信号：(i) 一个双域分布匹配蒸馏目标，引导学生模型趋向源教师模型和目标教师模型的分布，以保留源知识并促进对结构上更远域的适应；(ii) 一个多头生成对抗网络（GAN）损失，鼓励跨多个特征尺度的目标真实感，从而稳定训练并减少过拟合，尤其是在少样本情况下。", "result": "Uni-DAD在各种数据集上（包括少样本图像生成FSIG和主题驱动个性化SDP）进行了评估。结果表明，即使在少于4个采样步骤的情况下，Uni-DAD也能比现有最先进的适应方法提供更高的质量。在质量和多样性方面，Uni-DAD均优于两阶段训练管线。", "conclusion": "Uni-DAD成功地将扩散模型的蒸馏和适应统一到一个单阶段管线中，解决了现有两阶段方法在设计复杂性、质量和多样性方面的不足。它在新域生成任务中实现了更高的图像质量和多样性，且采样速度更快，甚至在少样本和结构差异较大的域适应场景下表现出色，超越了当前最先进的适应方法和两阶段训练管线。"}}
{"id": "2511.18333", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18333", "abs": "https://arxiv.org/abs/2511.18333", "authors": ["Xuanke Shi", "Boxuan Li", "Xiaoyang Han", "Zhongang Cai", "Lei Yang", "Dahua Lin", "Quan Wang"], "title": "ConsistCompose: Unified Multimodal Layout Control for Image Composition", "comment": "22 pages, 17 figures", "summary": "Unified multimodal models that couple visual understanding with image generation have advanced rapidly, yet most systems still focus on visual grounding-aligning language with image regions-while their generative counterpart, linguistic-embedded layout-grounded generation (LELG) for layout-controllable multi-instance generation, remains underexplored and limits precise compositional control. We present ConsistCompose, a unified multimodal framework that embeds layout coordinates directly into language prompts, enabling layout-controlled multi-instance image generation from Interleaved Image-Text within a single generative interface. We further construct ConsistCompose3M, a 3.4M multi-instance generation dataset with layout and identity annotations (2.6M text-guided and 0.8M image-guided data pairs) that provides large-scale supervision for layout-conditioned generation. Within this framework, LELG is instantiated through instance-coordinate binding prompts and coordinate-aware classifier-free guidance, which translate linguistic layout cues into precise spatial control without task-specific branches. Experiments on COCO-Position and MS-Bench show that ConsistCompose substantially improves spatial accuracy over layout-controlled baselines while preserving identity fidelity and competitive general multimodal understanding, establishing a unified paradigm for layout-controllable multimodal image generation.", "AI": {"tldr": "现有统一多模态模型在布局控制的多实例图像生成方面存在不足，限制了精确的组合控制。ConsistCompose通过将布局坐标直接嵌入语言提示，实现了布局可控的多实例图像生成，并构建了ConsistCompose3M数据集。", "motivation": "尽管统一多模态模型在视觉理解和图像生成方面取得了进展，但大多数系统仍侧重于视觉定位（语言与图像区域对齐），而其生成对应物——用于布局可控多实例生成的语言嵌入布局接地生成（LELG）——仍未得到充分探索，限制了精确的组合控制。", "method": "本文提出了ConsistCompose，一个统一的多模态框架，它将布局坐标直接嵌入到语言提示中，从而在单一生成界面内实现从交错图像-文本进行布局控制的多实例图像生成。此外，构建了ConsistCompose3M数据集（340万多实例生成数据，包含布局和身份标注），为布局条件生成提供大规模监督。在该框架内，LELG通过实例-坐标绑定提示和坐标感知无分类器指导实现，将语言布局线索转化为精确的空间控制，无需特定任务分支。", "result": "在COCO-Position和MS-Bench上的实验表明，ConsistCompose在保持身份保真度和具有竞争力的通用多模态理解的同时，显著提高了空间精度，优于现有的布局控制基线。", "conclusion": "ConsistCompose为布局可控的多模态图像生成建立了一个统一的范式，通过有效整合语言布局线索与精确的空间控制。"}}
{"id": "2511.18264", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18264", "abs": "https://arxiv.org/abs/2511.18264", "authors": ["Ruijie Fan", "Junyan Ye", "Huan Chen", "Zilong Huang", "Xiaolei Wang", "Weijia Li"], "title": "SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors", "comment": null, "summary": "Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.", "AI": {"tldr": "SatSAM2是一种基于SAM2的零样本卫星视频追踪器，通过引入卡尔曼滤波约束运动模块和运动约束状态机，解决了现有方法泛化性差和遮挡问题，并在新提出的MVOT基准和现有数据集上表现优越。", "motivation": "现有卫星视频追踪方法泛化能力差，需要针对特定场景训练，并且在存在遮挡时容易丢失目标。", "method": "本文提出了SatSAM2，一个基于SAM2的零样本卫星视频追踪器，旨在将基础模型应用于遥感领域。SatSAM2包含两个核心模块：基于卡尔曼滤波的约束运动模块（KFCMM），用于利用时间运动线索并抑制漂移；以及运动约束状态机（MCSM），用于根据运动动态和可靠性调节追踪状态。此外，还提出了MatrixCity视频目标追踪（MVOT）合成基准，包含1500+序列和15.7万帧标注，以支持大规模评估。", "result": "SatSAM2在两个卫星追踪基准和MVOT上，均优于传统和基于基础模型的追踪器（包括SAM2及其变体）。特别是在OOTB数据集上，SatSAM2比最先进的方法实现了5.84%的AUC提升。", "conclusion": "SatSAM2为卫星视频追踪提供了一个零样本、高性能的解决方案，有效解决了泛化性和遮挡挑战。本文提出的代码和数据集将公开发布以促进进一步研究。"}}
{"id": "2511.18307", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18307", "abs": "https://arxiv.org/abs/2511.18307", "authors": ["Sajjan Acharya", "Rajendra Baskota"], "title": "ScriptViT: Vision Transformer-Based Personalized Handwriting Generation", "comment": null, "summary": "Styled handwriting generation aims to synthesize handwritten text that looks both realistic and aligned with a specific writer's style. While recent approaches involving GAN, transformer and diffusion-based models have made progress, they often struggle to capture the full spectrum of writer-specific attributes, particularly global stylistic patterns that span long-range spatial dependencies. As a result, capturing subtle writer-specific traits such as consistent slant, curvature or stroke pressure, while keeping the generated text accurate is still an open problem. In this work, we present a unified framework designed to address these limitations. We introduce a Vision Transformer-based style encoder that learns global stylistic patterns from multiple reference images, allowing the model to better represent long-range structural characteristics of handwriting. We then integrate these style cues with the target text using a cross-attention mechanism, enabling the system to produce handwritten images that more faithfully reflect the intended style. To make the process more interpretable, we utilize Salient Stroke Attention Analysis (SSAA), which reveals the stroke-level features the model focuses on during style transfer. Together, these components lead to handwriting synthesis that is not only more stylistically coherent, but also easier to understand and analyze.", "AI": {"tldr": "本文提出一个统一框架，利用基于Vision Transformer的风格编码器和交叉注意力机制，解决手写生成中难以捕捉全局风格模式的问题，并通过显著笔画注意力分析提高可解释性，实现风格更连贯、更逼真的手写合成。", "motivation": "现有基于GAN、Transformer和扩散模型的方法在手写生成中，难以捕捉作者特定的全部属性，特别是跨越长距离空间依赖的全局风格模式（如一致的倾斜、曲率或笔压），同时保持生成文本的准确性，这是一个开放问题。", "method": "该研究提出了一个统一框架：1) 引入基于Vision Transformer的风格编码器，从多张参考图像中学习全局风格模式，以更好地表示手写的长程结构特征。2) 使用交叉注意力机制将这些风格线索与目标文本整合。3) 利用显著笔画注意力分析（SSAA）揭示模型在风格迁移过程中关注的笔画级特征，提高过程的可解释性。", "result": "所提出的系统能够生成更忠实反映预期风格的手写图像，实现风格上更连贯的手写合成。此外，通过SSAA，整个生成过程也变得更容易理解和分析。", "conclusion": "该框架通过有效捕捉全局风格模式并将其与目标文本融合，实现了风格更连贯且更易于理解和分析的手写合成，并提供了对风格迁移过程的洞察。"}}
{"id": "2511.18277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18277", "abs": "https://arxiv.org/abs/2511.18277", "authors": ["Yeji Song", "Jaehyun Lee", "Mijin Koo", "JunHoo Lee", "Nojun Kwak"], "title": "Point-to-Point: Sparse Motion Guidance for Controllable Video Editing", "comment": null, "summary": "Accurately preserving motion while editing a subject remains a core challenge in video editing tasks. Existing methods often face a trade-off between edit and motion fidelity, as they rely on motion representations that are either overfitted to the layout or only implicitly defined. To overcome this limitation, we revisit point-based motion representation. However, identifying meaningful points remains challenging without human input, especially across diverse video scenarios. To address this, we propose a novel motion representation, anchor tokens, that capture the most essential motion patterns by leveraging the rich prior of a video diffusion model. Anchor tokens encode video dynamics compactly through a small number of informative point trajectories and can be flexibly relocated to align with new subjects. This allows our method, Point-to-Point, to generalize across diverse scenarios. Extensive experiments demonstrate that anchor tokens lead to more controllable and semantically aligned video edits, achieving superior performance in terms of edit and motion fidelity.", "AI": {"tldr": "本文提出了一种名为“锚点令牌”（anchor tokens）的新型基于点的运动表示，它利用视频扩散模型的先验知识捕捉核心运动模式。该方法通过少量信息点轨迹紧凑编码视频动态，并能灵活重新定位以适应新主体，从而在视频编辑中实现更可控、语义对齐的编辑，同时保持卓越的编辑和运动保真度。", "motivation": "在视频编辑任务中，精确保留运动同时编辑主体是一个核心挑战。现有方法常面临编辑和运动保真度之间的权衡，因为它们的运动表示要么过拟合布局，要么是隐式定义的。此外，在没有人工输入的情况下，识别有意义的运动点在多样化的视频场景中仍然具有挑战性。", "method": "该研究重新审视了基于点的运动表示，并提出了一种新颖的运动表示——“锚点令牌”。这些锚点令牌通过利用视频扩散模型的丰富先验知识，捕捉最基本的运动模式。它们通过少量信息丰富的点轨迹紧凑编码视频动态，并能灵活重新定位以与新主体对齐。该方法被称为“点对点”（Point-to-Point）。", "result": "实验结果表明，锚点令牌带来了更可控、语义更对齐的视频编辑。在编辑保真度和运动保真度方面，该方法均表现出卓越的性能，并能泛化到多样化的场景。", "conclusion": "通过引入锚点令牌这一新颖的运动表示，本文提出的方法成功克服了现有视频编辑方法在运动保真度方面的局限性。锚点令牌使得视频编辑在保持运动的同时更具可控性和语义一致性，显著提升了编辑和运动保真度。"}}
{"id": "2511.18286", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18286", "abs": "https://arxiv.org/abs/2511.18286", "authors": ["Runwei Guan", "Rongsheng Hu", "Shangshu Chen", "Ningyuan Xiao", "Xue Xia", "Jiayang Liu", "Beibei Chen", "Ziren Tang", "Ningwei Ouyang", "Shaofeng Liang", "Yuxuan Fan", "Wanjie Sun", "Yutao Yue"], "title": "RoadSceneVQA: Benchmarking Visual Question Answering in Roadside Perception Systems for Intelligent Transportation System", "comment": "9 pages, 6 figures, accepted by AAAI 2026. The model is also called Dream, to the other me in the world forever", "summary": "Current roadside perception systems mainly focus on instance-level perception, which fall short in enabling interaction via natural language and reasoning about traffic behaviors in context. To bridge this gap, we introduce RoadSceneVQA, a large-scale and richly annotated visual question answering (VQA) dataset specifically tailored for roadside scenarios. The dataset comprises 34,736 diverse QA pairs collected under varying weather, illumination, and traffic conditions, targeting not only object attributes but also the intent, legality, and interaction patterns of traffic participants. RoadSceneVQA challenges models to perform both explicit recognition and implicit commonsense reasoning, grounded in real-world traffic rules and contextual dependencies. To fully exploit the reasoning potential of Multi-modal Large Language Models (MLLMs), we further propose CogniAnchor Fusion (CAF), a vision-language fusion module inspired by human-like scene anchoring mechanisms. Moreover, we propose the Assisted Decoupled Chain-of-Thought (AD-CoT) to enhance the reasoned thinking via CoT prompting and multi-task learning. Based on the above, we propose the baseline model RoadMind. Experiments on RoadSceneVQA and CODA-LM benchmark show that the pipeline consistently improves both reasoning accuracy and computational efficiency, allowing the MLLM to achieve state-of-the-art performance in structural traffic perception and reasoning tasks.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18290", "abs": "https://arxiv.org/abs/2511.18290", "authors": ["Jungho Lee", "Minhyeok Lee", "Sunghun Yang", "Minseok Kang", "Sangyoun Lee"], "title": "SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes", "comment": "Project Page: https://Jho-Yonsei.github.io/SwiftVGGT/", "summary": "3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.", "AI": {"tldr": "SwiftVGGT是一种免训练方法，通过优化回环检测和块对齐，显著减少了大规模场景3D重建的推理时间，同时保持了高重建质量。", "motivation": "大规模场景的3D重建面临准确性和计算效率之间的固有矛盾，现有方法要么牺牲质量追求速度，要么以慢速推理时间换取高质量重建。", "method": "本文提出了SwiftVGGT，一种免训练方法。它在不依赖外部视觉地点识别（VPR）模型的情况下执行回环检测，以保持全局一致性并消除冗余计算。此外，提出了一种简单有效的点采样方法，通过单一的Sim(3)SVD步骤对相邻块进行对齐，从而避免了迭代重加权最小二乘（IRLS）优化。", "result": "SwiftVGGT在多个数据集上进行了评估，结果表明它实现了最先进的重建质量，同时推理时间仅为近期基于VGGT的大规模重建方法的33%。", "conclusion": "SwiftVGGT成功解决了大规模3D重建中准确性和计算效率之间的权衡问题，显著减少了推理时间，同时保持了高质量的稠密3D重建，使其适用于千米级环境。"}}
{"id": "2511.18359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18359", "abs": "https://arxiv.org/abs/2511.18359", "authors": ["Alexandros Stergiou"], "title": "TRANSPORTER: Transferring Visual Semantics from VLM Manifolds", "comment": "Project page: https://alexandrosstergiou.github.io/TRANSPORTER", "summary": "How do video understanding models acquire their answers? Although current Vision Language Models (VLMs) reason over complex scenes with diverse objects, action performances, and scene dynamics, understanding and controlling their internal processes remains an open challenge. Motivated by recent advancements in text-to-video (T2V) generative models, this paper introduces a logits-to-video (L2V) task alongside a model-independent approach, TRANSPORTER, to generate videos that capture the underlying rules behind VLMs' predictions. Given the high-visual-fidelity produced by T2V models, TRANSPORTER learns an optimal transport coupling to VLM's high-semantic embedding spaces. In turn, logit scores define embedding directions for conditional video generation. TRANSPORTER generates videos that reflect caption changes over diverse object attributes, action adverbs, and scene context. Quantitative and qualitative evaluations across VLMs demonstrate that L2V can provide a fidelity-rich, novel direction for model interpretability that has not been previously explored.", "AI": {"tldr": "本文提出L2V（logits-to-video）任务和模型无关方法TRANSPORTER，通过生成视频来解释视觉语言模型（VLM）的预测机制，利用T2V模型的高视觉保真度，将VLM的logit分数转化为视频生成方向。", "motivation": "尽管当前的视觉语言模型（VLM）能对复杂场景进行推理，但其内部处理过程难以理解和控制，这促使研究者探索新的模型可解释性方法。近期文本到视频（T2V）生成模型的进展为这一探索提供了新的可能性。", "method": "引入了L2V（logits-to-video）任务和一种模型无关的方法TRANSPORTER。TRANSPORTER学习一种最优传输耦合，将VLM的高语义嵌入空间与T2V模型连接起来。VLM的logit分数被用来定义嵌入方向，从而进行条件视频生成，以捕捉VLM预测背后的潜在规则。", "result": "TRANSPORTER能够生成反映字幕变化的视频，这些变化包括不同的物体属性、动作副词和场景上下文。定量和定性评估表明，L2V为模型可解释性提供了一个高保真度且前所未有的新方向。", "conclusion": "L2V任务和TRANSPORTER方法为理解视觉语言模型如何获取答案提供了一种新颖且有效的方法，通过可视化其预测规则，极大地增强了模型的可解释性。"}}
{"id": "2511.18316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18316", "abs": "https://arxiv.org/abs/2511.18316", "authors": ["Subhajeet Das", "Pritam Paul", "Rohit Bahadur", "Sohan Das"], "title": "Stro-VIGRU: Defining the Vision Recurrent-Based Baseline Model for Brain Stroke Classification", "comment": "Presented at the International Conference on Computational Intelligence and Data Communication, Accepted for publication in the Taylor and Francis Conference Proceedings", "summary": "Stroke majorly causes death and disability worldwide, and early recognition is one of the key elements of successful treatment of the same. It is common to diagnose strokes using CT scanning, which is fast and readily available, however, manual analysis may take time and may result in mistakes. In this work, a pre-trained Vision Transformer-based transfer learning framework is proposed for the early identification of brain stroke. A few of the encoder blocks of the ViT model are frozen, and the rest are allowed to be fine-tuned in order to learn brain stroke-specific features. The features that have been extracted are given as input to a single-layer Bi-GRU to perform classification. Class imbalance is handled by data augmentation. The model has achieved 94.06% accuracy in classifying brain stroke from the Stroke Dataset.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18370", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.18370", "abs": "https://arxiv.org/abs/2511.18370", "authors": ["Zenghao Chai", "Chen Tang", "Yongkang Wong", "Xulei Yang", "Mohan Kankanhalli"], "title": "MimiCAT: Mimic with Correspondence-Aware Cascade-Transformer for Category-Free 3D Pose Transfer", "comment": "tech report", "summary": "3D pose transfer aims to transfer the pose-style of a source mesh to a target character while preserving both the target's geometry and the source's pose characteristic. Existing methods are largely restricted to characters with similar structures and fail to generalize to category-free settings (e.g., transferring a humanoid's pose to a quadruped). The key challenge lies in the structural and transformation diversity inherent in distinct character types, which often leads to mismatched regions and poor transfer quality. To address these issues, we first construct a million-scale pose dataset across hundreds of distinct characters. We further propose MimiCAT, a cascade-transformer model designed for category-free 3D pose transfer. Instead of relying on strict one-to-one correspondence mappings, MimiCAT leverages semantic keypoint labels to learn a novel soft correspondence that enables flexible many-to-many matching across characters. The pose transfer is then formulated as a conditional generation process, in which the source transformations are first projected onto the target through soft correspondence matching and subsequently refined using shape-conditioned representations. Extensive qualitative and quantitative experiments demonstrate that MimiCAT transfers plausible poses across different characters, significantly outperforming prior methods that are limited to narrow category transfer (e.g., humanoid-to-humanoid).", "AI": {"tldr": "本文提出MimiCAT，一个级联Transformer模型，通过学习软对应关系实现跨类别3D姿态迁移，解决了现有方法在不同结构角色间姿态迁移的局限性。", "motivation": "现有3D姿态迁移方法大多局限于结构相似的角色，无法泛化到无类别限制的场景（例如，将人形角色的姿态迁移到四足动物），主要挑战在于不同角色类型固有的结构和变换多样性。", "method": "研究者首先构建了一个包含数百万姿态数据的跨数百种不同角色的数据集。在此基础上，提出了MimiCAT，一个级联Transformer模型，通过语义关键点标签学习新颖的软对应关系，实现角色间的灵活多对多匹配。姿态迁移被公式化为一个条件生成过程，其中源变换首先通过软对应匹配投射到目标上，然后使用形状条件表示进行细化。", "result": "广泛的定性和定量实验表明，MimiCAT能够跨不同角色迁移合理的姿态，显著优于仅限于狭窄类别（例如，人对人）迁移的现有方法。", "conclusion": "MimiCAT通过其新颖的软对应机制和级联Transformer架构，成功解决了跨类别3D姿态迁移的挑战，实现了在多样化角色间的高质量姿态转移。"}}
{"id": "2511.18344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18344", "abs": "https://arxiv.org/abs/2511.18344", "authors": ["Tianyang Xu", "Jinjie Gu", "Xuefeng Zhu", "XiaoJun Wu", "Josef Kittler"], "title": "A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles", "comment": null, "summary": "With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.", "AI": {"tldr": "本文发布了MM-UAV数据集，这是首个大规模多模态无人机跟踪基准，整合了RGB、红外和事件信号。同时，提出了一种新颖的多模态多无人机跟踪框架，包含偏移引导自适应对齐、自适应动态融合和事件增强关联机制，在复杂场景下表现优于现有方法。", "motivation": "低空无人机普及使得视觉多目标跟踪成为关键安全技术，但单一视觉模态在低光照、杂乱背景和快速运动等挑战性场景中表现不佳。多模态跟踪更具弹性，但缺乏专用公共数据集阻碍了有效解决方案的开发。", "method": "1. 发布MM-UAV数据集：首个大规模多模态无人机跟踪基准，包含RGB、红外和事件信号三种关键感知模态，涵盖30多个挑战性场景，共1,321个同步多模态序列和超过280万帧标注数据。2. 提出新型多模态多无人机跟踪框架：专为无人机跟踪应用设计。3. 技术创新：a) 偏移引导自适应对齐模块，解决传感器间的空间不匹配问题。b) 自适应动态融合模块，平衡不同模态的互补信息。c) 事件增强关联机制，利用事件模态的运动线索进行更可靠的身份维护，克服传统外观建模的局限性。", "result": "综合实验表明，所提出的框架性能持续优于现有最先进的方法。", "conclusion": "MM-UAV数据集和提出的框架弥补了多模态无人机跟踪领域的数据和方法空白，为未来研究提供了坚实的基础和高性能基线。数据集和源代码将公开，以促进该领域的研究发展。"}}
{"id": "2511.18317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18317", "abs": "https://arxiv.org/abs/2511.18317", "authors": ["Dongcai Tan", "Shunkun Liang", "Bin Li", "Banglei Guan", "Ang Su", "Yuan Lin", "Dapeng Zhang", "Minggang Wan", "Zibin Liu", "Chenglong Wang", "Jiajian Zhu", "Zhang Li", "Yang Shang", "Qifeng Yu"], "title": "Optimal Pose Guidance for Stereo Calibration in 3D Deformation Measurement", "comment": null, "summary": "Stereo optical measurement techniques, such as digital image correlation (DIC), are widely used in 3D deformation measurement as non-contact, full-field measurement methods, in which stereo calibration is a crucial step. However, current stereo calibration methods lack intuitive optimal pose guidance, leading to inefficiency and suboptimal accuracy in deformation measurements. The aim of this study is to develop an interactive calibration framework that automatically generates the next optimal pose, enabling high-accuracy stereo calibration for 3D deformation measurement. We propose a pose optimization method that introduces joint optimization of relative and absolute extrinsic parameters, with the minimization of the covariance matrix trace adopted as the loss function to solve for the next optimal pose. Integrated with this method is a user-friendly graphical interface, which guides even non-expert users to capture qualified calibration images. Our proposed method demonstrates superior efficiency (requiring fewer images) and accuracy (demonstrating lower measurement errors) compared to random pose, while maintaining robustness across varying FOVs. In the thermal deformation measurement tests on an S-shaped specimen, the results exhibit high agreement with finite element analysis (FEA) simulations in both deformation magnitude and evolutionary trends. We present a pose guidance method for high-precision stereo calibration in 3D deformation measurement. The simulation experiments, real-world experiments, and thermal deformation measurement applications all demonstrate the significant application potential of our proposed method in the field of 3D deformation measurement.\n  Keywords: Stereo calibration, Optimal pose guidance, 3D deformation measurement, Digital image correlation", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18326", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18326", "abs": "https://arxiv.org/abs/2511.18326", "authors": ["Helia Abedini", "Saba Rahimi", "Reza Vaziri"], "title": "General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification", "comment": null, "summary": "Brain tumor detection from MRI scans plays a crucial role in early diagnosis and treatment planning. Deep convolutional neural networks (CNNs) have demonstrated strong performance in medical imaging tasks, particularly when pretrained on large datasets. However, it remains unclear which type of pretrained model performs better when only a small dataset is available: those trained on domain-specific medical data or those pretrained on large general datasets. In this study, we systematically evaluate three pretrained CNN architectures for brain tumor classification: RadImageNet DenseNet121 with medical-domain pretraining, EfficientNetV2S, and ConvNeXt-Tiny, which are modern general-purpose CNNs. All models were trained and fine-tuned under identical conditions using a limited-size brain MRI dataset to ensure a fair comparison. Our results reveal that ConvNeXt-Tiny achieved the highest accuracy, followed by EfficientNetV2S, while RadImageNet DenseNet121, despite being pretrained on domain-specific medical data, exhibited poor generalization with lower accuracy and higher loss. These findings suggest that domain-specific pretraining may not generalize well under small-data conditions. In contrast, modern, deeper general-purpose CNNs pretrained on large-scale datasets can offer superior transfer learning performance in specialized medical imaging tasks.", "AI": {"tldr": "本研究比较了在小规模数据集上，医学领域预训练模型与通用预训练模型在脑肿瘤检测中的表现。结果显示，现代通用CNN（如ConvNeXt-Tiny）优于医学领域预训练模型。", "motivation": "在只有小规模数据集可用时，尚不清楚哪种类型的预训练模型（领域特定医学数据预训练或大型通用数据集预训练）在医学图像任务中表现更好。", "method": "研究系统地评估了三种预训练CNN架构（RadImageNet DenseNet121、EfficientNetV2S和ConvNeXt-Tiny），在相同条件下使用有限大小的脑部MRI数据集进行训练和微调，用于脑肿瘤分类。", "result": "ConvNeXt-Tiny取得了最高的准确率，其次是EfficientNetV2S。尽管RadImageNet DenseNet121经过医学领域预训练，但其泛化能力差，准确率较低且损失较高。", "conclusion": "研究表明，在小数据条件下，领域特定预训练可能无法很好地泛化。相反，在大型数据集上预训练的现代、更深层的通用CNN在专业医学成像任务中可以提供卓越的迁移学习性能。"}}
{"id": "2511.18373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18373", "abs": "https://arxiv.org/abs/2511.18373", "authors": ["Xiyang Wu", "Zongxia Li", "Jihui Jin", "Guangyao Shi", "Gouthaman KV", "Vishnu Raj", "Nilotpal Sinha", "Jingxi Chen", "Fan Du", "Dinesh Manocha"], "title": "MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.", "AI": {"tldr": "现有视觉语言模型(VLM)在视频物理推理方面表现不佳。本文提出了MASS-Bench基准测试和MASS方法，通过将物理世界线索转化为VLM可理解的表示，显著提升了VLM在物理推理和理解任务上的性能，超越了现有SOTA模型。", "motivation": "视觉语言模型(VLM)在标准视频任务上表现良好，但在涉及运动动力学和空间交互的物理驱动推理方面存在不足。这限制了它们解释真实或AI生成内容(AIGC)视频以及生成物理一致内容的能力。", "method": "本文提出：1) **MASS-Bench**，一个包含4,350个真实和AIGC视频、8,361个专注于物理相关理解任务的自由形式视频问答对的综合基准，并提供详细标注，包括视觉检测、子片段定位和实体全序列3D运动跟踪。2) **MASS**，一种模型无关的方法，通过基于深度的3D编码和视觉定位，结合用于对象动力学的运动跟踪器，将时空信号注入VLM的语言空间。此外，通过强化微调来加强跨模态对齐和推理。", "result": "实验和消融研究表明，经过改进的VLM在物理推理和理解方面，比同类和更大的基线模型以及现有SOTA模型分别高出8.7%和6.0%，性能与闭源SOTA VLM（如Gemini-2.5-Flash）相当。", "conclusion": "本文提出的方法有效解决了VLM在视频物理推理方面的不足，显著提升了其性能，验证了该方法的有效性。"}}
{"id": "2511.18329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18329", "abs": "https://arxiv.org/abs/2511.18329", "authors": ["Shohei Tanaka", "Atsushi Hashimoto", "Yoshitaka Ushiku"], "title": "SciPostLayoutTree: A Dataset for Structural Analysis of Scientific Posters", "comment": null, "summary": "Scientific posters play a vital role in academic communication by presenting ideas through visual summaries. Analyzing reading order and parent-child relations of posters is essential for building structure-aware interfaces that facilitate clear and accurate understanding of research content. Despite their prevalence in academic communication, posters remain underexplored in structural analysis research, which has primarily focused on papers. To address this gap, we constructed SciPostLayoutTree, a dataset of approximately 8,000 posters annotated with reading order and parent-child relations. Compared to an existing structural analysis dataset, SciPostLayoutTree contains more instances of spatially challenging relations, including upward, horizontal, and long-distance relations. As a solution to these challenges, we develop Layout Tree Decoder, which incorporates visual features as well as bounding box features including position and category information. The model also uses beam search to predict relations while capturing sequence-level plausibility. Experimental results demonstrate that our model improves the prediction accuracy for spatially challenging relations and establishes a solid baseline for poster structure analysis. The dataset is publicly available at https://huggingface.co/datasets/omron-sinicx/scipostlayouttree. The code is also publicly available at https://github.com/omron-sinicx/scipostlayouttree.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18346", "abs": "https://arxiv.org/abs/2511.18346", "authors": ["Wenshuo Gao", "Junyi Fan", "Jiangyue Zeng", "Shuai Yang"], "title": "FlowPortal: Residual-Corrected Flow for Training-Free Video Relighting and Background Replacement", "comment": "Project Page: https://gaowenshuo.github.io/FlowPortalProject/", "summary": "Video relighting with background replacement is a challenging task critical for applications in film production and creative media. Existing methods struggle to balance temporal consistency, spatial fidelity, and illumination naturalness. To address these issues, we introduce FlowPortal, a novel training-free flow-based video relighting framework. Our core innovation is a Residual-Corrected Flow mechanism that transforms a standard flow-based model into an editing model, guaranteeing perfect reconstruction when input conditions are identical and enabling faithful relighting when they differ, resulting in high structural consistency. This is further enhanced by a Decoupled Condition Design for precise lighting control and a High-Frequency Transfer mechanism for detail preservation. Additionally, a masking strategy isolates foreground relighting from background pure generation process. Experiments demonstrate that FlowPortal achieves superior performance in temporal coherence, structural preservation, and lighting realism, while maintaining high efficiency. Project Page: https://gaowenshuo.github.io/FlowPortalProject/.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18396", "abs": "https://arxiv.org/abs/2511.18396", "authors": ["Jinhao Li", "Sarah M. Erfani", "Lei Feng", "James Bailey", "Feng Liu"], "title": "Exploring Weak-to-Strong Generalization for CLIP-based Classification", "comment": "TMLR", "summary": "Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, class prototype learning (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67% improvement over strong baseline methods.", "AI": {"tldr": "本研究探索了在视觉-语言模型（CLIP）中应用弱监督到强泛化，并提出类别原型学习（CPL）方法，通过学习更具代表性的类别原型，在有限预训练场景下显著提升了CLIP模型的分类性能。", "motivation": "大规模商业模型与用户意图对齐是防止有害输出的关键，但随着模型复杂性增加，人类监督变得不切实际且低效。当模型超越人类知识时，提供准确反馈更具挑战性。近期提出的弱模型监督强模型方案具有潜力，但此前主要应用于纯语言模型，将其扩展到多模态视觉-语言模型（如CLIP）以利用其优势是重要的。", "method": "本研究在CLIP分类任务中探索了弱监督到强泛化。提出了一种名为“类别原型学习”（CPL）的方法，旨在通过学习每个类别更具代表性的原型来增强CLIP模型的分类能力。该方法在弱监督下使用简单的损失函数。", "result": "研究结果表明，尽管在弱监督下使用了简单的损失函数，CPL在特定场景（特别是预训练有限的情况下）仍能带来显著且稳健的改进。广泛的实验证明，在该设置下，CPL方法有效，比现有强基线方法提高了3.67%。", "conclusion": "弱监督到强泛化概念可有效应用于CLIP-based分类任务。所提出的类别原型学习（CPL）方法能够通过学习更具代表性的类别原型，在有限预训练的场景下显著提升CLIP模型的分类能力。"}}
{"id": "2511.18380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18380", "abs": "https://arxiv.org/abs/2511.18380", "authors": ["Timing Yang", "Guoyizhe Wei", "Alan Yuille", "Feng Wang"], "title": "RNN as Linear Transformer: A Closer Investigation into Representational Potentials of Visual Mamba Models", "comment": null, "summary": "Mamba has recently garnered attention as an effective backbone for vision tasks. However, its underlying mechanism in visual domains remains poorly understood. In this work, we systematically investigate Mamba's representational properties and make three primary contributions. First, we theoretically analyze Mamba's relationship to Softmax and Linear Attention, confirming that it can be viewed as a low-rank approximation of Softmax Attention and thereby bridging the representational gap between Softmax and Linear forms. Second, we introduce a novel binary segmentation metric for activation map evaluation, extending qualitative assessments to a quantitative measure that demonstrates Mamba's capacity to model long-range dependencies. Third, by leveraging DINO for self-supervised pretraining, we obtain clearer activation maps than those produced by standard supervised approaches, highlighting Mamba's potential for interpretability. Notably, our model also achieves a 78.5 percent linear probing accuracy on ImageNet, underscoring its strong performance. We hope this work can provide valuable insights for future investigations of Mamba-based vision architectures.", "AI": {"tldr": "本文系统性地研究了Mamba在视觉领域的表示特性，理论上将其与Softmax注意力联系起来，引入了一种新的评估指标来量化其长程依赖建模能力，并通过自监督预训练提高了可解释性，同时取得了良好的性能。", "motivation": "Mamba作为视觉任务的有效骨干网络受到了关注，但其在视觉领域的基本机制和表示特性仍不清楚。", "method": "1. 理论分析Mamba与Softmax及线性注意力的关系。2. 引入一种新颖的二元分割度量用于激活图评估。3. 利用DINO进行自监督预训练以获得更清晰的激活图。", "result": "1. Mamba可被视为Softmax注意力的低秩近似，弥合了Softmax和线性形式之间的表示差距。2. 新度量量化证实了Mamba建模长程依赖的能力。3. 自监督预训练（DINO）产生了更清晰的激活图，突显了Mamba的潜力。4. 模型在ImageNet上实现了78.5%的线性探测准确率。", "conclusion": "本研究为未来基于Mamba的视觉架构的探索提供了有价值的见解，揭示了其表示特性和可解释性。"}}
{"id": "2511.18352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18352", "abs": "https://arxiv.org/abs/2511.18352", "authors": ["Zitong Xu", "Dake Shen", "Yaosong Du", "Kexiang Hao", "Jinghan Huang", "Xiande Huang"], "title": "MagicWand: A Universal Agent for Generation and Evaluation Aligned with User Preference", "comment": null, "summary": "Recent advances in AIGC (Artificial Intelligence Generated Content) models have enabled significant progress in image and video generation. However, users still struggle to obtain content that aligns with their preferences due to the difficulty of crafting detailed prompts and the lack of mechanisms to retain their preferences. To address these challenges, we construct \\textbf{UniPrefer-100K}, a large-scale dataset comprising images, videos, and associated text that describes the styles users tend to prefer. Based on UniPrefer-100K, we propose \\textbf{MagicWand}, a universal generation and evaluation agent that enhances prompts based on user preferences, leverages advanced generation models for high-quality content, and applies preference-aligned evaluation and refinement. In addition, we introduce \\textbf{UniPreferBench}, the first large-scale benchmark with over 120K annotations for assessing user preference alignment across diverse AIGC tasks. Experiments on UniPreferBench demonstrate that MagicWand consistently generates content and evaluations that are well aligned with user preferences across a wide range of scenarios.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18378", "abs": "https://arxiv.org/abs/2511.18378", "authors": ["Shijian Wang", "Runhao Fu", "Siyi Zhao", "Qingqin Zhan", "Xingjian Wang", "Jiarui Jin", "Yuan Lu", "Hanqian Wu", "Cunjian Chen"], "title": "Synthetic Curriculum Reinforces Compositional Text-to-Image Generation", "comment": null, "summary": "Text-to-Image (T2I) generation has long been an open problem, with compositional synthesis remaining particularly challenging. This task requires accurate rendering of complex scenes containing multiple objects that exhibit diverse attributes as well as intricate spatial and semantic relationships, demanding both precise object placement and coherent inter-object interactions. In this paper, we propose a novel compositional curriculum reinforcement learning framework named CompGen that addresses compositional weakness in existing T2I models. Specifically, we leverage scene graphs to establish a novel difficulty criterion for compositional ability and develop a corresponding adaptive Markov Chain Monte Carlo graph sampling algorithm. This difficulty-aware approach enables the synthesis of training curriculum data that progressively optimize T2I models through reinforcement learning. We integrate our curriculum learning approach into Group Relative Policy Optimization (GRPO) and investigate different curriculum scheduling strategies. Our experiments reveal that CompGen exhibits distinct scaling curves under different curriculum scheduling strategies, with easy-to-hard and Gaussian sampling strategies yielding superior scaling performance compared to random sampling. Extensive experiments demonstrate that CompGen significantly enhances compositional generation capabilities for both diffusion-based and auto-regressive T2I models, highlighting its effectiveness in improving the compositional T2I generation systems.", "AI": {"tldr": "本文提出CompGen，一个新颖的组合式课程强化学习框架，通过利用场景图和自适应MCMC图采样算法，生成渐进式训练数据，显著提升了文本到图像（T2I）模型的组合生成能力。", "motivation": "文本到图像（T2I）生成长期以来是一个开放性问题，其中组合合成尤其具有挑战性。这需要准确渲染包含多个对象、多样属性以及复杂空间和语义关系的复杂场景，要求精确的对象放置和连贯的对象间交互。", "method": "本文提出了一个名为CompGen的组合式课程强化学习框架，以解决现有T2I模型在组合能力方面的弱点。具体来说，利用场景图建立新的组合能力难度标准，并开发了相应的自适应马尔可夫链蒙特卡洛（MCMC）图采样算法。这种难度感知方法能够合成渐进优化T2I模型的课程训练数据，并通过强化学习进行训练。该课程学习方法被整合到组相对策略优化（GRPO）中，并研究了不同的课程调度策略（如从易到难、高斯采样和随机采样）。", "result": "实验表明，CompGen在不同课程调度策略下表现出不同的扩展曲线，其中从易到难和高斯采样策略相比随机采样，产生了更优越的扩展性能。广泛的实验证明，CompGen显著增强了基于扩散和自回归T2I模型的组合生成能力。", "conclusion": "CompGen框架有效提高了T2I生成系统的组合能力，突出了其在改进组合式T2I生成系统方面的有效性。"}}
{"id": "2511.18424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18424", "abs": "https://arxiv.org/abs/2511.18424", "authors": ["Avishka Perera", "Kumal Hewagamage", "Saeedha Nazar", "Kavishka Abeywardana", "Hasitha Gallella", "Ranga Rodrigo", "Mohamed Afham"], "title": "CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images", "comment": "24 pages, 10 figures", "summary": "Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.", "AI": {"tldr": "CrossJEPA是一种简单高效的跨模态联合嵌入预测架构（JEPA），它利用图像基础模型，通过预测器从3D点云推断特定2D视图的嵌入，实现了3D表示学习的新SOTA，同时显著降低了计算成本和训练时间。", "motivation": "当前的图像到点云跨模态学习方法通常导致模型庞大、训练缓慢且计算成本高昂，难以在资源受限环境中部署。尽管JEPA在自监督学习中以其简洁高效而广受欢迎，但由于对JEPA中掩蔽作用的误解，其在跨模态设置中尚未得到充分探索。", "method": "本文提出了CrossJEPA，一种跨模态联合嵌入预测架构。它利用图像基础模型的知识，训练一个预测器来从对应的3D点云推断特定渲染2D视图的嵌入，从而引入了一种超越掩蔽的JEPA式预训练策略。通过在跨域投影信息上条件化预测器，CrossJEPA提纯了目标领域独有的语义监督信号。此外，它采用冻结教师设计和一次性目标嵌入缓存机制，实现了均摊效率。", "result": "CrossJEPA在合成ModelNet40数据集上（94.2%）和真实世界ScanObjectNN数据集上（88.3%）的线性探测任务中均达到了新的SOTA。它仅使用14.1M的预训练参数（其中点编码器为8.5M），在标准单GPU上预训练约6小时。这些结果表明CrossJEPA是一个高性能、内存高效且训练快速的3D表示学习框架。", "conclusion": "CrossJEPA通过知识蒸馏为3D表示学习提供了一个高性能、内存高效且训练快速的框架，有效解决了现有跨模态方法的计算成本高昂问题，并展示了JEPA在跨模态设置中的巨大潜力，超越了对掩蔽的传统认知。"}}
{"id": "2511.18367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18367", "abs": "https://arxiv.org/abs/2511.18367", "authors": ["Zilong Chen", "Huan-ang Gao", "Delin Qu", "Haohan Chi", "Hao Tang", "Kai Zhang", "Hao Zhao"], "title": "Alias-free 4D Gaussian Splatting", "comment": "Project page: https://4d-alias-free.github.io/4D-Alias-free/", "summary": "Existing dynamic scene reconstruction methods based on Gaussian Splatting enable real-time rendering and generate realistic images. However, adjusting the camera's focal length or the distance between Gaussian primitives and the camera to modify rendering resolution often introduces strong artifacts, stemming from the frequency constraints of 4D Gaussians and Gaussian scale mismatch induced by the 2D dilated filter. To address this, we derive a maximum sampling frequency formulation for 4D Gaussian Splatting and introduce a 4D scale-adaptive filter and scale loss, which flexibly regulates the sampling frequency of 4D Gaussian Splatting. Our approach eliminates high-frequency artifacts under increased rendering frequencies while effectively reducing redundant Gaussians in multi-view video reconstruction. We validate the proposed method through monocular and multi-view video reconstruction experiments.Ours project page: https://4d-alias-free.github.io/4D-Alias-free/", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18434", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18434", "abs": "https://arxiv.org/abs/2511.18434", "authors": ["Yongkun Du", "Pinxuan Chen", "Xuye Ying", "Zhineng Chen"], "title": "DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation", "comment": null, "summary": "The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.", "AI": {"tldr": "现有MLLM文档解析与翻译基准缺乏真实世界拍摄文档的挑战。本文引入DocPTBench，一个针对拍摄文档的综合基准，揭示了现有模型在此类文档上性能显著下降。", "motivation": "多模态大语言模型（MLLMs）在文档解析和翻译方面潜力巨大，但现有基准（如OmniDocBench和DITrans）主要基于清晰的扫描或数字原生文档，未能充分反映真实世界拍摄条件下（如几何畸变和光度变化）文档的复杂挑战。", "method": "本文提出了DocPTBench，一个专为拍摄文档解析和翻译设计的综合基准。它包含1,300多份来自多个领域的高分辨率拍摄文档，涵盖八种翻译场景，并为解析和翻译提供了精心的人工验证注释。", "result": "实验表明，从数字原生文档转向拍摄文档会导致性能大幅下降：流行的MLLMs在端到端解析中平均准确率下降18%，翻译中下降12%；而专门的文档解析模型平均下降25%。", "conclusion": "巨大的性能差距凸显了真实世界拍摄文档带来的独特挑战，并揭示了现有模型鲁棒性的局限性。"}}
{"id": "2511.18382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18382", "abs": "https://arxiv.org/abs/2511.18382", "authors": ["Timing Yang", "Sucheng Ren", "Alan Yuille", "Feng Wang"], "title": "ViMix-14M: A Curated Multi-Source Video-Text Dataset with Long-Form, High-Quality Captions and Crawl-Free Access", "comment": null, "summary": "Text-to-video generation has surged in interest since Sora, yet open-source models still face a data bottleneck: there is no large, high-quality, easily obtainable video-text corpus. Existing public datasets typically require manual YouTube crawling, which yields low usable volume due to link rot and access limits, and raises licensing uncertainty. This work addresses this challenge by introducing ViMix-14M, a curated multi-source video-text dataset of around 14 million pairs that provides crawl-free, download-ready access and long-form, high-quality captions tightly aligned to video. ViMix-14M is built by merging diverse open video sources, followed by unified de-duplication and quality filtering, and a multi-granularity, ground-truth-guided re-captioning pipeline that refines descriptions to better match actions, scenes, and temporal structure. We evaluate the dataset by multimodal retrieval, text-to-video generation, and video question answering tasks, observing consistent improvements over counterpart datasets. We hope this work can help removing the key barrier to training and fine-tuning open-source video foundation models, and provide insights of building high-quality and generalizable video-text datasets.", "AI": {"tldr": "本文介绍了ViMix-14M，一个包含约1400万对视频-文本的精选多源数据集，旨在解决开源文本到视频生成模型面临的数据瓶颈，提供易于获取、高质量且对齐紧密的视频-文本数据。", "motivation": "自Sora发布以来，文本到视频生成领域兴趣激增，但开源模型受限于数据瓶颈：缺乏大型、高质量、易于获取的视频-文本语料库。现有公开数据集通常需要手动抓取YouTube，导致可用量低、链接失效、访问受限以及许可不确定性。", "method": "ViMix-14M通过合并多种开放视频源构建，随后进行统一去重和质量过滤。接着，采用多粒度、以真实数据为指导的重新标注流程，优化描述以更好地匹配动作、场景和时间结构，确保字幕与视频紧密对齐。", "result": "ViMix-14M包含约1400万对视频-文本数据，提供免抓取、可直接下载的访问方式，并拥有长篇、高质量且与视频紧密对齐的字幕。在多模态检索、文本到视频生成和视频问答任务中进行评估，ViMix-14M表现出优于现有同类数据集的一致性改进。", "conclusion": "这项工作旨在移除训练和微调开源视频基础模型的关键障碍，并为构建高质量、通用性强的视频-文本数据集提供见解。"}}
{"id": "2511.18416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18416", "abs": "https://arxiv.org/abs/2511.18416", "authors": ["Haonan Wang", "Hanyu Zhou", "Haoyue Liu", "Luxin Yan"], "title": "4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation", "comment": null, "summary": "We investigate a challenging task of dynamic scene geometry estimation, which requires representing both spatial and temporal features. Typically, existing methods align the two features into a unified latent space to model scene geometry. However, this unified paradigm suffers from potential mismatched representation due to the heterogeneous nature between spatial and temporal features. In this work, we propose 4D-VGGT, a general foundation model with divide-and-conquer spatiotemporal representation for dynamic scene geometry. Our model is divided into three aspects: 1) Multi-setting input. We design an adaptive visual grid that supports input sequences with arbitrary numbers of views and time steps. 2) Multi-level representation. We propose a cross-view global fusion for spatial representation and a cross-time local fusion for temporal representation. 3) Multi-task prediction. We append multiple task-specific heads to spatiotemporal representations, enabling a comprehensive visual geometry estimation for dynamic scenes. Under this unified framework, these components enhance the feature discriminability and application universality of our model for dynamic scenes. In addition, we integrate multiple geometry datasets to train our model and conduct extensive experiments to verify the effectiveness of our method across various tasks on multiple dynamic scene geometry benchmarks.", "AI": {"tldr": "本文提出4D-VGGT，一个用于动态场景几何估计的通用基础模型。它采用分而治之的时空表示方法，通过多设置输入、多级表示和多任务预测来解决现有方法中空间和时间特征异构性导致的表示不匹配问题，从而提升特征判别力和应用普适性。", "motivation": "现有的动态场景几何估计方法通常将空间和时间特征对齐到统一的潜在空间中，但由于这两种特征的异构性，可能导致表示不匹配的问题。", "method": "本文提出了4D-VGGT模型，其核心是分而治之的时空表示策略。具体包括：1) 多设置输入：设计自适应视觉网格以支持任意数量的视图和时间步输入序列。2) 多级表示：提出用于空间表示的跨视图全局融合和用于时间表示的跨时间局部融合。3) 多任务预测：为时空表示附加多个任务特定的头部，以实现动态场景的全面视觉几何估计。此外，模型通过整合多个几何数据集进行训练。", "result": "在统一框架下，4D-VGGT的各个组件增强了模型的特征判别力和在动态场景中的应用普适性。通过在多个动态场景几何基准上对各种任务进行广泛实验，验证了该方法的有效性。", "conclusion": "4D-VGGT模型通过其创新的分而治之的时空表示方法，成功解决了动态场景几何估计中空间和时间特征异构性带来的挑战，显著提升了特征的判别能力和模型的泛用性，为动态场景几何估计提供了一个有效且全面的解决方案。"}}
{"id": "2511.18385", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18385", "abs": "https://arxiv.org/abs/2511.18385", "authors": ["Chuang Peng", "Renshuai Tao", "Zhongwei Ren", "Xianglong Liu", "Yunchao Wei"], "title": "Can a Second-View Image Be a Language? Geometric and Semantic Cross-Modal Reasoning for X-ray Prohibited Item Detection", "comment": "10 pages, 4 figures", "summary": "Automatic X-ray prohibited items detection is vital for security inspection and has been widely studied. Traditional methods rely on visual modality, often struggling with complex threats. While recent studies incorporate language to guide single-view images, human inspectors typically use dual-view images in practice. This raises the question: can the second view provide constraints similar to a language modality? In this work, we introduce DualXrayBench, the first comprehensive benchmark for X-ray inspection that includes multiple views and modalities. It supports eight tasks designed to test cross-view reasoning. In DualXrayBench, we introduce a caption corpus consisting of 45,613 dual-view image pairs across 12 categories with corresponding captions. Building upon these data, we propose the Geometric (cross-view)-Semantic (cross-modality) Reasoner (GSR), a multimodal model that jointly learns correspondences between cross-view geometry and cross-modal semantics, treating the second-view images as a \"language-like modality\". To enable this, we construct the GSXray dataset, with structured Chain-of-Thought sequences: <top>, <side>, <conclusion>. Comprehensive evaluations on DualXrayBench demonstrate that GSR achieves significant improvements across all X-ray tasks, offering a new perspective for real-world X-ray inspection.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18399", "abs": "https://arxiv.org/abs/2511.18399", "authors": ["Yuxiang Nie", "Han Wang", "Yongjie Ye", "Haiyang Yu", "Weitao Jia", "Tao Zeng", "Hao Feng", "Xiang Fei", "Yang Li", "Xiaohui Lv", "Guozhi Tang", "Jingqun Tang", "Jinghui Lu", "Zehui Dai", "Jiacong Wang", "Dingkang Yang", "An-Lan Wang", "Can Huang"], "title": "ChineseVideoBench: Benchmarking Multi-modal Large Models for Chinese Video Question Answering", "comment": null, "summary": "This paper introduces ChineseVideoBench, a pioneering benchmark specifically designed for evaluating Multimodal Large Language Models (MLLMs) in Chinese Video Question Answering. The growing demand for sophisticated video analysis capabilities highlights the critical need for comprehensive, culturally-aware evaluation frameworks. ChineseVideoBench addresses this gap by providing a robust dataset and tailored evaluation metrics, enabling rigorous assessment of state-of-the-art MLLMs on complex Chinese video content. Specifically, ChineseVideoBench comprises 8 main classes and 12 sub-classes, encompassing tasks that demand both deep video understanding and nuanced Chinese linguistic and cultural awareness. Our empirical evaluations reveal that ChineseVideoBench presents a significant challenge to current MLLMs. Among the models assessed, Gemini 2.5 Pro achieves the highest performance with an overall score of 77.9%, while InternVL-38B emerges as the most competitive open-source model.", "AI": {"tldr": "本文介绍了ChineseVideoBench，一个专门用于评估多模态大语言模型（MLLMs）在中文视频问答任务上的基准。", "motivation": "日益增长的复杂视频分析需求，以及对全面、文化感知评估框架的迫切需求，尤其是在中文视频内容方面。", "method": "引入了ChineseVideoBench，包含8个主类和12个子类的数据集，以及定制的评估指标。它旨在测试MLLMs在深度视频理解和细致的中文语言文化意识方面的能力。", "result": "实证评估表明，ChineseVideoBench对当前的MLLMs构成了重大挑战。Gemini 2.5 Pro表现最佳，总分77.9%，而InternVL-38B是表现最好的开源模型。", "conclusion": "ChineseVideoBench为当前的MLLMs带来了显著挑战，并为评估其在复杂中文视频内容上的性能提供了严格的工具。"}}
{"id": "2511.18386", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18386", "abs": "https://arxiv.org/abs/2511.18386", "authors": ["Peter Siegel", "Federico Tombari", "Marc Pollefeys", "Daniel Barath"], "title": "SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation", "comment": null, "summary": "We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \\textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18437", "abs": "https://arxiv.org/abs/2511.18437", "authors": ["Chi Zhang", "Haibo Qiu", "Qiming Zhang", "Yufei Xu", "Zhixiong Zeng", "Siqi Yang", "Peng Shi", "Lin Ma", "Jing Zhang"], "title": "Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.", "AI": {"tldr": "针对VLM的RLVR忽视视觉感知导致幻觉，PEARL通过引入感知清单和感知奖励，将推理锚定到经过验证的视觉证据，显著提升了多模态推理能力。", "motivation": "现有的VLM强化学习（RLVR）仅验证最终文本输出，忽略了视觉感知这一基础步骤，导致视觉幻觉和奖励欺骗，使得基于有缺陷感知的推理不可靠。", "method": "PEARL（Perceptual-Evidence Anchored Reinforced Learning）是一个双分支、感知-推理协同系统。它为每个问答实例生成一个“感知清单”（一组可验证的感知子问题），以探测模型对关键视觉证据的理解。通过对清单进行辅助推理，产生感知奖励，直接强化模型的感知能力，并作为推理的保真门。如果模型通过感知检查，其策略更新会偏向于锚定证据的推理；否则，过程停止以防止基于错误前提的推理。PEARL可与GRPO和DAPO等流行RL方法无缝集成。", "result": "PEARL在多模态推理基准测试中取得了显著提升，例如在MathVerse上比基线提高了9.7%，比GRPO提高了6.6%。", "conclusion": "PEARL通过明确将多模态推理锚定到经过验证的视觉证据，有效解决了VLM中RLVR的局限性，显著增强了模型的推理能力。"}}
{"id": "2511.18425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18425", "abs": "https://arxiv.org/abs/2511.18425", "authors": ["Mansur Yerzhanuly"], "title": "LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection", "comment": "13 pages, 3 figures, 1 table", "summary": "Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.", "AI": {"tldr": "本文介绍LungX，一种结合EfficientNet、CBAM和Vision Transformer的混合架构，用于肺炎检测。它在20,000张胸部X光片上实现了最先进的性能，并显示出卓越的病灶定位能力。", "motivation": "肺炎仍然是全球主要的死亡原因之一，因此及时诊断至关重要。", "method": "研究引入了LungX，这是一种新颖的混合架构。它结合了EfficientNet的多尺度特征、CBAM注意力机制以及Vision Transformer的全局上下文建模能力，以增强肺炎检测。该模型在来自RSNA和CheXpert的20,000张精选胸部X光片上进行了评估。", "result": "LungX取得了最先进的性能，准确率达86.5%，AUC为0.943，比EfficientNet-B0基线提高了6.7%的AUC。视觉分析通过可解释的注意力图显示出卓越的病灶定位能力。", "conclusion": "LungX作为一种AI诊断辅助工具，在肺炎检测方面表现出色。未来的方向包括多中心验证和架构优化，目标是达到88%的准确率以进行临床部署。"}}
{"id": "2511.18470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18470", "abs": "https://arxiv.org/abs/2511.18470", "authors": ["Heeseung Yun", "Joonil Na", "Jaeyeon Kim", "Calvin Murdock", "Gunhee Kim"], "title": "Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span", "comment": "NeurIPS 2025 Spotlight", "summary": "People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.", "AI": {"tldr": "该研究提出EgoSpanLift方法，将以自我为中心的视觉感知预测从2D图像平面提升至3D场景，并构建了一个新的3D视觉范围预测基准，在预测未来3D视觉焦点方面表现优异。", "motivation": "虽然以自我为中心的用户和场景理解研究多集中于运动和接触式交互，但预测人类视觉感知本身（其在指导人类行为中的基础作用及其对AR/VR和辅助技术的影响）仍未得到充分探索。现有研究缺乏对人在3D环境中视觉焦点（视觉范围）的预测。", "method": "该论文提出了EgoSpanLift方法，将以自我为中心的视觉范围预测从2D图像平面转换到3D场景。具体而言，它将SLAM导出的关键点转换为与凝视兼容的几何结构，并提取体素化的视觉范围区域。EgoSpanLift还结合了3D U-Net和单向Transformer，实现时空融合以高效预测3D网格中的未来视觉范围。此外，研究团队还从原始以自我为中心的多传感器数据中整理了一个包含364.6K样本的综合基准数据集用于3D视觉范围预测。", "result": "该方法在以自我为中心的2D凝视预测和3D定位方面均优于现有基线。即使在不进行额外2D特定训练的情况下，将其结果投影回2D图像平面时，也能达到可与2D特定方法媲美的性能。", "conclusion": "该研究成功解决了以自我为中心的3D视觉范围预测的挑战，通过提出的EgoSpanLift方法和新的3D数据集，实现了对未来3D视觉焦点的有效预测，并在多项任务中展现出卓越性能，为AR/VR和辅助技术提供了新的可能性。"}}
{"id": "2511.18422", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18422", "abs": "https://arxiv.org/abs/2511.18422", "authors": ["Mohammad Jafari Vayeghan", "Niloufar Delfan", "Mehdi Tale Masouleh", "Mansour Parvaresh Rizi", "Behzad Moshiri"], "title": "NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI", "comment": null, "summary": "Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18436", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18436", "abs": "https://arxiv.org/abs/2511.18436", "authors": ["Hao Shen", "Jikang Cheng", "Renye Yan", "Zhongyuan Wang", "Wei Peng", "Baojin Huang"], "title": "When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection", "comment": null, "summary": "The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.", "AI": {"tldr": "本文提出了一种名为DARW的域感知相对加权策略，以有效利用生成式回放来改进增量式伪造检测，解决了现有方法在处理域重叠时的局限性。", "motivation": "人脸生成技术快速发展，导致伪造方法多样化。现有基于样本回放的增量式伪造检测方法存在多样性低和隐私问题。生成式回放是潜在解决方案，但其在伪造检测中的可行性及如何处理生成样本与新伪造模型之间的域边界模糊问题尚不明确。", "method": "本文系统研究了生成式回放，并识别出两种场景：当回放生成器与新伪造模型相似时产生“域风险”样本；当回放生成器差异大时产生“域安全”样本。为此，提出域感知相对加权（DARW）策略：直接监督域安全样本；对域风险样本应用相对分离损失以平衡监督和潜在混淆；通过域混淆分数动态调整这种权衡。", "result": "大量实验表明，DARW在不同生成式回放设置下，持续提升了伪造检测的增量学习性能，并减轻了域重叠带来的不利影响。", "conclusion": "DARW是一种有效利用生成式回放进行增量式伪造检测的策略，它通过区分域安全和域风险样本，并动态调整监督方式，成功解决了域重叠带来的挑战。"}}
{"id": "2511.18452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18452", "abs": "https://arxiv.org/abs/2511.18452", "authors": ["Loick Chambon", "Paul Couairon", "Eloi Zablocki", "Alexandre Boulch", "Nicolas Thome", "Matthieu Cord"], "title": "NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering", "comment": "Code: https://github.com/valeoai/NAF", "summary": "Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.", "AI": {"tldr": "本文提出了一种名为邻域注意力滤波（NAF）的新型上采样器，它通过学习自适应空间和内容权重，实现了零样本、与视觉基础模型（VFM）无关的特征上采样，超越了现有的VFM特定上采样器，并在像素级任务上达到了最先进的性能，同时保持了高效率。", "motivation": "视觉基础模型（VFMs）提取的空间下采样表示给像素级任务带来了挑战。现有的上采样方法存在根本性权衡：经典滤波器速度快、适用性广但形式固定；现代上采样器精度高但需要针对每个VFM重新训练。研究动机是弥合这一差距，开发一种既能保持高精度又无需为每个VFM重新训练的通用上采样方法。", "method": "NAF通过交叉尺度邻域注意力（Cross-Scale Neighborhood Attention）和旋转位置嵌入（Rotary Position Embeddings, RoPE）学习自适应的空间和内容权重。它仅由高分辨率输入图像引导，实现零样本操作，即无需重新训练即可上采样任何VFM的特征。", "result": "NAF是第一个VFM无关的架构，它在零样本设置下超越了VFM特定的上采样器，并在多个下游任务中取得了最先进的性能。它保持了高效率，能够处理2K特征图并以18 FPS的速度重建中间分辨率图。除了特征上采样，NAF在图像修复方面也表现出色，突显了其多功能性。", "conclusion": "NAF成功地弥合了现有上采样方法在通用性与精度之间的差距，提供了一种高效、精确且与VFM无关的特征上采样解决方案。其零样本能力和在多任务上的优异表现，使其成为像素级任务和图像修复领域的强大工具。"}}
{"id": "2511.18441", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.18441", "abs": "https://arxiv.org/abs/2511.18441", "authors": ["Lorenzo Rutayisire", "Nicola Capodieci", "Fabio Pellacini"], "title": "ReCoGS: Real-time ReColoring for Gaussian Splatting scenes", "comment": "Project page is available at https://github.com/loryruta/recogs", "summary": "Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18444", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18444", "abs": "https://arxiv.org/abs/2511.18444", "authors": ["Arpit Garg", "Hemanth Saratchandran", "Simon Lucey"], "title": "SineProject: Machine Unlearning for Stable Vision Language Alignment", "comment": "In Submission", "summary": "Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.", "AI": {"tldr": "多模态大语言模型（MLLMs）需要遗忘特定知识，但现有方法常破坏视觉语言对齐。本文提出SineProject，通过增强投影器网络来稳定对齐，在有效遗忘目标信息的同时显著减少了良性查询的误拒。", "motivation": "MLLMs需要不经完整重训练地遗忘特定知识（如不安全或隐私信息），但现有遗忘方法往往会破坏视觉语言对齐，导致模型同时拒绝有害和良性查询。", "method": "研究发现现有方法失败的原因在于遗忘过程中投影器网络的雅可比矩阵严重病态，导致优化不稳定和跨模态嵌入漂移。本文引入SineProject，通过正弦调制的可训练参数增强冻结的投影器，改善雅可比矩阵的谱条件，从而在遗忘过程中稳定对齐。", "result": "在LLaVA v1.5 7B和13B模型的标准安全和隐私遗忘基准测试中，SineProject在实现目标信息完全遗忘的同时，减少了良性查询的拒绝，以可忽略的计算开销实现了最先进的遗忘-保留权衡。", "conclusion": "SineProject通过稳定投影器网络在遗忘过程中的视觉语言对齐，有效解决了MLLMs的知识遗忘问题，实现了更优的遗忘效果和更低的良性查询误拒率，且计算开销极小。"}}
{"id": "2511.18448", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18448", "abs": "https://arxiv.org/abs/2511.18448", "authors": ["Shaoyu Liu", "Jianing Li", "Guanghui Zhao", "Yunjian Zhang", "Xiangyang Ji"], "title": "EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18514", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18514", "abs": "https://arxiv.org/abs/2511.18514", "authors": ["Abishek Karthik", "Sreya Mynampati", "Pandiyaraju V"], "title": "Unified Deep Learning Platform for Dust and Fault Diagnosis in Solar Panels Using Thermal and Visual Imaging", "comment": null, "summary": "Solar energy is one of the most abundant and tapped sources of renewable energies with enormous future potential. Solar panel output can vary widely with factors like intensity, temperature, dirt, debris and so on affecting it. We have implemented a model on detecting dust and fault on solar panels. These two applications are centralized as a single-platform and can be utilized for routine-maintenance and any other checks. These are checked against various parameters such as power output, sinusoidal wave (I-V component of solar cell), voltage across each solar cell and others. Firstly, we filter and preprocess the obtained images using gamma removal and Gaussian filtering methods alongside some predefined processes like normalization. The first application is to detect whether a solar cell is dusty or not based on various pre-determined metrics like shadowing, leaf, droppings, air pollution and from other human activities to extent of fine-granular solar modules. The other one is detecting faults and other such occurrences on solar panels like faults, cracks, cell malfunction using thermal imaging application. This centralized platform can be vital since solar panels have different efficiency across different geography (air and heat affect) and can also be utilized for small-scale house requirements to large-scale solar farm sustentation effectively. It incorporates CNN, ResNet models that with self-attention mechanisms-KerNet model which are used for classification and results in a fine-tuned system that detects dust or any fault occurring. Thus, this multi-application model proves to be efficient and optimized in detecting dust and faults on solar panels. We have performed various comparisons and findings that demonstrates that our model has better efficiency and accuracy results overall than existing models.", "AI": {"tldr": "该论文提出一个基于图像处理和深度学习（CNN、ResNet、KerNet）的集中式平台，用于高效检测太阳能电池板上的灰尘和故障，旨在提高维护效率和准确性。", "motivation": "太阳能电池板的输出受多种因素（如灰尘、温度、碎片）影响，效率会大幅波动。需要一个可靠的系统来检测灰尘和故障，以进行日常维护并保持其性能，这对于从小规模住宅到大型太阳能农场都至关重要。", "method": "该研究采用多应用模型，首先对图像进行伽马去除、高斯滤波和归一化等预处理。灰尘检测基于阴影、树叶、鸟粪、空气污染等预设指标。故障检测（如裂缝、电池故障）则利用热成像技术。模型核心集成CNN、ResNet以及带有自注意力机制的KerNet模型进行分类，形成一个可同时检测灰尘和故障的集中式平台。", "result": "所提出的多应用模型在检测太阳能电池板上的灰尘和故障方面表现出高效和优化。与现有模型相比，该模型在整体效率和准确性方面均取得了更好的结果。", "conclusion": "该多应用模型被证明在检测太阳能电池板上的灰尘和故障方面是有效和优化的，其性能优于现有模型，可为太阳能电池板的日常维护提供重要支持。"}}
{"id": "2511.18516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18516", "abs": "https://arxiv.org/abs/2511.18516", "authors": ["Haidong Kang", "Ketong Qian", "Yi Lu"], "title": "Breaking Forgetting: Training-Free Few-Shot Class-Incremental Learning via Conditional Diffusion", "comment": null, "summary": "Efforts to overcome catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) have primarily focused on developing more effective gradient-based optimization strategies. In contrast, little attention has been paid to the training cost explosion that inevitably arises as the number of novel classes increases, a consequence of relying on gradient learning even under extreme data scarcity. More critically, since FSCIL typically provides only a few samples for each new class, gradient-based updates not only induce severe catastrophic forgetting on base classes but also hinder adaptation to novel ones. This paper seeks to break this long-standing limitation by asking: Can we design a training-free FSCIL paradigm that entirely removes gradient optimization? We provide an affirmative answer by uncovering an intriguing connection between gradient-based optimization and the Conditional Diffusion process. Building on this observation, we propose a Conditional Diffusion-driven FSCIL (CD-FSCIL) framework that substitutes the conventional gradient update process with a diffusion-based generative transition, enabling training-free incremental adaptation while effectively mitigating forgetting. Furthermore, to enhance representation under few-shot constraints, we introduce a multimodal learning strategy that integrates visual features with natural language descriptions automatically generated by Large Language Models (LLMs). This synergy substantially alleviates the sample scarcity issue and improves generalization across novel classes. Extensive experiments on mainstream FSCIL benchmarks demonstrate that our method not only achieves state-of-the-art performance but also drastically reduces computational and memory overhead, marking a paradigm shift toward training-free continual adaptation.", "AI": {"tldr": "本文提出了一种无训练的FSCIL范式（CD-FSCIL），通过条件扩散过程替代梯度优化，并结合LLM生成的多模态学习，有效解决了灾难性遗忘和训练成本爆炸问题，实现了最先进的性能和显著的资源节省。", "motivation": "现有的FSCIL方法主要依赖梯度优化，导致灾难性遗忘、随着新类别增加训练成本急剧上升，并且在少量样本下难以适应新类别。研究旨在探索能否设计一种完全移除梯度优化的无训练FSCIL范式。", "method": "本文通过发现梯度优化与条件扩散过程之间的联系，提出了一种条件扩散驱动的FSCIL（CD-FSCIL）框架。该框架用基于扩散的生成式转换取代了传统的梯度更新过程，实现了无训练的增量适应。此外，为增强少量样本约束下的表示能力，引入了一种多模态学习策略，将视觉特征与大型语言模型（LLMs）自动生成的自然语言描述相结合。", "result": "在主流FSCIL基准测试上，所提出的方法不仅实现了最先进的性能，而且显著降低了计算和内存开销。这标志着向无训练持续适应的范式转变。", "conclusion": "本文成功地设计了一种无训练的FSCIL范式，通过条件扩散和多模态学习有效缓解了灾难性遗忘，提升了对新类别的泛化能力，并大幅降低了资源消耗，为持续学习领域开辟了新方向。"}}
{"id": "2511.18454", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18454", "abs": "https://arxiv.org/abs/2511.18454", "authors": ["Ming-Jhe Lee"], "title": "RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading", "comment": "7 pages, 5 figures", "summary": "The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of \"Gradient Conflict\" and \"Negative Transfer\" in multi-task training, we propose a \"Two-Stage Decoupled Training Strategy.\" Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed \"Feature Injection\" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a \"Range Loss\" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18463", "abs": "https://arxiv.org/abs/2511.18463", "authors": ["Bowei Pu", "Chuanbin Liu", "Yifan Ge", "Peichen Zhou", "Yiwei Sun", "Zhiyin Lu", "Jiankang Wang", "Hongtao Xie"], "title": "Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding", "comment": "32 pages, 36 figures", "summary": "Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18471", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18471", "abs": "https://arxiv.org/abs/2511.18471", "authors": ["Liav Hen", "Tom Tirer", "Raja Giryes", "Shady Abu-Hussein"], "title": "Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale", "comment": null, "summary": "Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.", "AI": {"tldr": "本文提出了一种名为AdaPS的自适应似然步长策略，用于指导扩散模型解决逆问题。该方法通过观察依赖的加权方案平衡先验和数据保真度，无需超参数，显著提升了图像重建质量，超越现有基线。", "motivation": "扩散模型在逆问题中表现强大，但其核心挑战在于平衡先验贡献与数据保真项。过于激进的似然更新可能引入伪影，而过于保守的更新会减慢收敛或导致次优重建。", "method": "研究者开发了一种自适应似然步长策略，通过观察依赖的加权方案来指导扩散过程。该方案基于两种对难以处理的中间似然梯度进行近似的差异，自然地适应扩散调度、时间重采样和注入的随机性。最终方法被称为自适应后验扩散采样（AdaPS），它无需超参数。", "result": "AdaPS在CelebA-HQ和ImageNet-256验证集上，显著提升了超分辨率、高斯去模糊和运动去模糊等多种图像任务的重建质量。它在感知质量上持续超越现有的基于扩散的基线，且失真损失极小或没有，无需任务特定调整。广泛的消融研究进一步证明了其对扩散步数、观测噪声水平和不同随机性的鲁棒性。", "conclusion": "AdaPS提供了一种无超参数的自适应似然步长策略，用于解决扩散模型在逆问题中的挑战。该方法通过智能地平衡先验和数据保真度，在多种图像重建任务中实现了卓越且鲁棒的性能，显著提高了重建质量。"}}
{"id": "2511.18534", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18534", "abs": "https://arxiv.org/abs/2511.18534", "authors": ["Pengcheng Fang", "Hongli Chen", "Guangzhen Yao", "Jian Shi", "Fangfang Tang", "Xiaohao Cai", "Shanshan Shan", "Feng Liu"], "title": "HiFi-MambaV2: Hierarchical Shared-Routed MoE for High-Fidelity MRI Reconstruction", "comment": null, "summary": "Reconstructing high-fidelity MR images from undersampled k-space data requires recovering high-frequency details while maintaining anatomical coherence. We present HiFi-MambaV2, a hierarchical shared-routed Mixture-of-Experts (MoE) Mamba architecture that couples frequency decomposition with content-adaptive computation. The model comprises two core components: (i) a separable frequency-consistent Laplacian pyramid (SF-Lap) that delivers alias-resistant, stable low- and high-frequency streams; and (ii) a hierarchical shared-routed MoE that performs per-pixel top-1 sparse dispatch to shared experts and local routers, enabling effective specialization with stable cross-depth behavior. A lightweight global context path is fused into an unrolled, data-consistency-regularized backbone to reinforce long-range reasoning and preserve anatomical coherence. Evaluated on fastMRI, CC359, ACDC, M4Raw, and Prostate158, HiFi-MambaV2 consistently outperforms CNN-, Transformer-, and prior Mamba-based baselines in PSNR, SSIM, and NMSE across single- and multi-coil settings and multiple acceleration factors, consistently surpassing consistent improvements in high-frequency detail and overall structural fidelity. These results demonstrate that HiFi-MambaV2 enables reliable and robust MRI reconstruction.", "AI": {"tldr": "HiFi-MambaV2是一种分层共享路由的MoE Mamba架构，通过频率分解和内容自适应计算，实现了高保真MR图像重建，在多个数据集上显著优于现有基线。", "motivation": "从欠采样k空间数据重建高保真MR图像，需要恢复高频细节同时保持解剖学一致性。", "method": "本文提出了HiFi-MambaV2模型，一个分层共享路由的Mixture-of-Experts (MoE) Mamba架构，其核心组件包括：(i) 可分离频率一致的拉普拉斯金字塔(SF-Lap)，用于提供抗混叠、稳定的低频和高频流；(ii) 分层共享路由的MoE，执行逐像素的top-1稀疏分派到共享专家和局部路由器，实现有效的专业化和稳定的跨深度行为。此外，一个轻量级的全局上下文路径被融合到一个展开的、数据一致性正则化的骨干网络中，以增强长程推理并保持解剖学一致性。", "result": "HiFi-MambaV2在fastMRI、CC359、ACDC、M4Raw和Prostate158等数据集上，在单线圈和多线圈设置以及多种加速因子下，PSNR、SSIM和NMSE指标始终优于基于CNN、Transformer和现有Mamba的基线模型。它在高频细节和整体结构保真度方面均实现了持续改进。", "conclusion": "这些结果表明HiFi-MambaV2能够实现可靠且鲁棒的MRI重建。"}}
{"id": "2511.18473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18473", "abs": "https://arxiv.org/abs/2511.18473", "authors": ["Juan Romero", "Qiang Fu", "Matteo Ravasi", "Wolfgang Heidrich"], "title": "Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements", "comment": null, "summary": "Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18507", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18507", "abs": "https://arxiv.org/abs/2511.18507", "authors": ["Kai Jiang", "Siqi Huang", "Xiangyu Chen", "Jiawei Shao", "Hongyuan Zhang", "Xuelong Li"], "title": "Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives", "comment": "18 pages, 16 figures. This is a preprint version of a paper submitted to CVPR 2026", "summary": "Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.", "AI": {"tldr": "本文旨在解决多模态大语言模型（MLLMs）在视觉理解中面对动态场景变化时的灾难性遗忘问题，构建了多场景视觉问答数据集MSVQA，并提出了UNIFIER持续学习方法以缓解跨场景遗忘并实现知识积累。", "motivation": "部署在设备上的多模态大语言模型（MLLMs）需要持续适应下游任务中动态变化的场景（如背景和视角），以有效执行复杂的视觉任务。然而，MLLMs在此过程中面临灾难性遗忘问题。", "method": "1. 构建了一个多模态视觉理解数据集（MSVQA），包含高空、水下、低空和室内四种不同场景和视角，用于研究MLLMs在真实世界数据流场景变化下的灾难性遗忘。\n2. 提出了UNIFIER（多场景视角下的多模态持续学习）方法，通过在每个视觉块内将来自不同场景的视觉信息解耦到不同的分支，并将其投影到相同的特征空间。\n3. 对每个分支的特征施加一致性约束，以保持跨场景视觉表示的稳定性。", "result": "在MSVQA数据集上的大量实验表明，UNIFIER有效缓解了跨场景任务的遗忘，并实现了同一场景内的知识积累。", "conclusion": "UNIFIER方法能够有效解决多模态大语言模型在学习不同视觉场景时面临的视觉差异和灾难性遗忘问题，实现了跨场景任务的持续学习和知识积累。"}}
{"id": "2511.18533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18533", "abs": "https://arxiv.org/abs/2511.18533", "authors": ["Md Mizanur Rahman Mustakim", "Jianwu Li", "Sumya Bhuiyan", "Mohammad Mehedi Hasan", "Bing Han"], "title": "DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation", "comment": null, "summary": "Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.", "AI": {"tldr": "本文提出DE-KAN，一种双编码器Kolmogorov Arnold网络，用于全景X光片中牙齿的精确分割，通过融合全局和局部特征并利用KAN层显著提高了分割性能。", "motivation": "由于解剖变异、牙齿形状不规则和结构重叠，从全景X光片中准确分割单个牙齿仍然是一个挑战性任务，这限制了传统深度学习模型的性能。", "method": "本文提出了DE-KAN（Dual Encoder Kolmogorov Arnold Network）。该框架采用ResNet-18编码器处理增强输入以提取全局特征，并使用定制的CNN编码器处理原始输入以提取局部空间特征。这些特征通过基于KAN的瓶颈层进行融合，KAN层引入了基于Kolmogorov Arnold表示定理的非线性可学习激活函数，以提高学习能力和可解释性。", "result": "DE-KAN在两个基准牙科X光数据集上进行了广泛实验，结果表明其优于现有最先进的分割模型。它达到了94.5%的mIoU、97.1%的Dice系数、98.91%的准确率和97.36%的召回率，相较于现有方法，Dice系数提升高达4.7%。", "conclusion": "DE-KAN通过其独特的双编码器和KAN融合机制，有效增强了特征表示和分割精度，成功解决了牙齿分割的挑战，并超越了现有技术水平。"}}
{"id": "2511.18504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18504", "abs": "https://arxiv.org/abs/2511.18504", "authors": ["Md Tasnin Tanvir", "Soumitra Das", "Sk Md Abidar Rahaman", "Ali Shiri Sichani"], "title": "Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression", "comment": "9 pages, 6 figures", "summary": "The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18513", "abs": "https://arxiv.org/abs/2511.18513", "authors": ["He Huang", "Yujun Guo", "Wei He"], "title": "LRDUN: A Low-Rank Deep Unfolding Network for Efficient Spectral Compressive Imaging", "comment": "17 pages, 16 figures,", "summary": "Deep unfolding networks (DUNs) have achieved remarkable success and become the mainstream paradigm for spectral compressive imaging (SCI) reconstruction. Existing DUNs are derived from full-HSI imaging models, where each stage operates directly on the high-dimensional HSI, refining the entire data cube based on the single 2D coded measurement. However, this paradigm leads to computational redundancy and suffers from the ill-posed nature of mapping 2D residuals back to 3D space of HSI. In this paper, we propose two novel imaging models corresponding to the spectral basis and subspace image by explicitly integrating low-rank (LR) decomposition with the sensing model. Compared to recovering the full HSI, estimating these compact low-dimensional components significantly mitigates the ill-posedness. Building upon these novel models, we develop the Low-Rank Deep Unfolding Network (LRDUN), which jointly solves the two subproblems within an unfolded proximal gradient descent (PGD) framework. Furthermore, we introduce a Generalized Feature Unfolding Mechanism (GFUM) that decouples the physical rank in the data-fidelity term from the feature dimensionality in the prior module, enhancing the representational capacity and flexibility of the network. Extensive experiments on simulated and real datasets demonstrate that the proposed LRDUN achieves state-of-the-art (SOTA) reconstruction quality with significantly reduced computational cost.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18654", "abs": "https://arxiv.org/abs/2511.18654", "authors": ["Nayu Dong", "Townim Chowdhury", "Hieu Phan", "Mark Jenkinson", "Johan Verjans", "Zhibin Liao"], "title": "From Healthy Scans to Annotated Tumors: A Tumor Fabrication Framework for 3D Brain MRI Synthesis", "comment": null, "summary": "The scarcity of annotated Magnetic Resonance Imaging (MRI) tumor data presents a major obstacle to accurate and automated tumor segmentation. While existing data synthesis methods offer promising solutions, they often suffer from key limitations: manual modeling is labor intensive and requires expert knowledge. Deep generative models may be used to augment data and annotation, but they typically demand large amounts of training pairs in the first place, which is impractical in data limited clinical settings. In this work, we propose Tumor Fabrication (TF), a novel two-stage framework for unpaired 3D brain tumor synthesis. The framework comprises a coarse tumor synthesis process followed by a refinement process powered by a generative model. TF is fully automated and leverages only healthy image scans along with a limited amount of real annotated data to synthesize large volumes of paired synthetic data for enriching downstream supervised segmentation training. We demonstrate that our synthetic image-label pairs used as data enrichment can significantly improve performance on downstream tumor segmentation tasks in low-data regimes, offering a scalable and reliable solution for medical image enrichment and addressing critical challenges in data scarcity for clinical AI applications.", "AI": {"tldr": "本文提出了一种名为Tumor Fabrication (TF)的两阶段框架，用于非配对3D脑肿瘤合成。该方法利用健康图像和少量真实注释数据，自动生成大量配对的合成数据，以解决MRI肿瘤数据稀缺问题，显著提升低数据量下的肿瘤分割性能。", "motivation": "准确和自动化的肿瘤分割面临主要障碍，即带注释的磁共振成像（MRI）肿瘤数据稀缺。现有数据合成方法存在局限性：手动建模费时且需要专业知识；深度生成模型通常需要大量训练对，这在临床数据受限的环境中不切实际。", "method": "本文提出了一种新颖的两阶段框架，名为Tumor Fabrication (TF)，用于非配对3D脑肿瘤合成。该框架包括一个粗略的肿瘤合成过程，随后是一个由生成模型驱动的精炼过程。TF是全自动的，仅利用健康图像扫描和少量真实的带注释数据来合成大量配对的合成数据。", "result": "实验证明，使用TF生成的合成图像-标签对作为数据增强，可以在低数据量情况下显著提高下游肿瘤分割任务的性能。", "conclusion": "TF为医学图像增强提供了一个可扩展且可靠的解决方案，有效解决了临床AI应用中数据稀缺的关键挑战。"}}
{"id": "2511.18600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18600", "abs": "https://arxiv.org/abs/2511.18600", "authors": ["Hong Li", "Chongjie Ye", "Houyuan Chen", "Weiqing Xiao", "Ziyang Yan", "Lixing Xiao", "Zhaoxi Chen", "Jianfeng Xiang", "Shaocong Xu", "Xuhui Liu", "Yikai Wang", "Baochang Zhang", "Xiaoguang Han", "Jiaolong Yang", "Hao Zhao"], "title": "NeAR: Coupled Neural Asset-Renderer Stack", "comment": "20 pages, 16 figures", "summary": "Neural asset authoring and neural rendering have emerged as fundamentally disjoint threads: one generates digital assets using neural networks for traditional graphics pipelines, while the other develops neural renderers that map conventional assets to images. However, the potential of jointly designing the asset representation and renderer remains largely unexplored. We argue that coupling them can unlock an end-to-end learnable graphics stack with benefits in fidelity, consistency, and efficiency. In this paper, we explore this possibility with NeAR: a Coupled Neural Asset-Renderer Stack. On the asset side, we build on Trellis-style Structured 3D Latents and introduce a lighting-homogenized neural asset: from a casually lit input, a rectified-flow backbone predicts a Lighting-Homogenized SLAT that encodes geometry and intrinsic material cues in a compact, view-agnostic latent. On the renderer side, we design a lighting-aware neural renderer that uses this neural asset, along with explicit view embeddings and HDR environment maps, to achieve real-time, relightable rendering. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit single-image reconstruction, (3) unknown-lit single-image relighting, and (4) novel-view relighting. Our coupled stack surpasses state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.", "AI": {"tldr": "本文提出了NeAR，一个耦合的神经资产-渲染器堆栈，旨在通过联合设计资产表示和渲染器，实现端到端可学习的图形栈，从而提升真实感、一致性和效率。", "motivation": "神经资产创作和神经渲染目前是独立的领域，分别专注于生成数字资产和将传统资产渲染成图像。研究人员认为，将两者耦合起来可以解锁一个端到端可学习的图形栈，从而在保真度、一致性和效率方面带来显著优势。", "method": "NeAR堆栈在资产侧基于Trellis风格的结构化3D潜在表示，并引入了一个光照均匀化的神经资产（SLAT）。SLAT通过一个整流流骨干网络从随意光照的输入中预测，编码了几何和固有材质线索，形成紧凑、与视角无关的潜在表示。在渲染侧，设计了一个光照感知神经渲染器，利用此神经资产、显式视角嵌入和HDR环境图，实现实时、可重打光渲染。该方法在G-buffer前向渲染、随机光照单图像重建、未知光照单图像重打光和新视角重打光四项任务上进行了验证。", "result": "NeAR耦合堆栈在定量指标和感知质量方面均超越了最先进的基线方法。", "conclusion": "耦合神经资产和渲染器的方法能够带来显著优势，并有望启发未来将神经资产和渲染器视为协同设计组件而非独立实体的图形栈设计。"}}
{"id": "2511.18537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18537", "abs": "https://arxiv.org/abs/2511.18537", "authors": ["Tuomas Varanka", "Juan Luis Gonzalez", "Hyeongwoo Kim", "Pablo Garrido", "Xu Yao"], "title": "Zero-Shot Video Deraining with Video Diffusion Models", "comment": "WACV 2026", "summary": "Existing video deraining methods are often trained on paired datasets, either synthetic, which limits their ability to generalize to real-world rain, or captured by static cameras, which restricts their effectiveness in dynamic scenes with background and camera motion. Furthermore, recent works in fine-tuning diffusion models have shown promising results, but the fine-tuning tends to weaken the generative prior, limiting generalization to unseen cases. In this paper, we introduce the first zero-shot video deraining method for complex dynamic scenes that does not require synthetic data nor model fine-tuning, by leveraging a pretrained text-to-video diffusion model that demonstrates strong generalization capabilities. By inverting an input video into the latent space of diffusion models, its reconstruction process can be intervened and pushed away from the model's concept of rain using negative prompting. At the core of our approach is an attention switching mechanism that we found is crucial for maintaining dynamic backgrounds as well as structural consistency between the input and the derained video, mitigating artifacts introduced by naive negative prompting. Our approach is validated through extensive experiments on real-world rain datasets, demonstrating substantial improvements over prior methods and showcasing robust generalization without the need for supervised training.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18656", "abs": "https://arxiv.org/abs/2511.18656", "authors": ["Harrison Bagley", "Will Meakin", "Simon Lucey", "Yee Wei Law", "Tat-Jun Chin"], "title": "Robust Physical Adversarial Patches Using Dynamically Optimized Clusters", "comment": "Supplementary material available at: https://drive.google.com/drive/folders/1Yntcc9CARdbvoJJ51cyUm1DWGSvU9X4V?usp=drive_link", "summary": "Physical adversarial attacks on deep learning systems is concerning due to the ease of deploying such attacks, usually by placing an adversarial patch in a scene to manipulate the outcomes of a deep learning model. Training such patches typically requires regularization that improves physical realizability (e.g., printability, smoothness) and/or robustness to real-world variability (e.g. deformations, viewing angle, noise). One type of variability that has received little attention is scale variability. When a patch is rescaled, either digitally through downsampling/upsampling or physically through changing imaging distances, interpolation-induced color mixing occurs. This smooths out pixel values, resulting in a loss of high-frequency patterns and degrading the adversarial signal. To address this, we present a novel superpixel-based regularization method that guides patch optimization to scale-resilient structures. Our ap proach employs the Simple Linear Iterative Clustering (SLIC) algorithm to dynamically cluster pixels in an adversarial patch during optimization. The Implicit Function Theorem is used to backpropagate gradients through SLIC to update the superpixel boundaries and color. This produces patches that maintain their structure over scale and are less susceptible to interpolation losses. Our method achieves greater performance in the digital domain, and when realized physically, these performance gains are preserved, leading to improved physical performance. Real-world performance was objectively assessed using a novel physical evaluation protocol that utilizes screens and cardboard cut-outs to systematically vary real-world conditions.", "AI": {"tldr": "该研究提出了一种新颖的基于超像素的正则化方法，通过结合SLIC和隐函数定理，优化生成对尺度变化具有鲁棒性的对抗性补丁，从而提高物理对抗攻击的性能。", "motivation": "物理对抗攻击中，对抗性补丁的尺度变化（数字重采样或物理距离变化）会导致插值引起的颜色混合、高频模式丢失，进而削弱对抗信号。现有研究对此类尺度可变性关注不足。", "method": "提出了一种基于超像素的正则化方法。在优化过程中，利用简单线性迭代聚类（SLIC）算法动态聚类对抗性补丁中的像素。通过隐函数定理，将梯度反向传播通过SLIC，以更新超像素边界和颜色。这使得补丁结构在尺度变化下仍能保持，减少插值损失。同时，采用了一种新的物理评估协议来客观评估真实世界性能。", "result": "该方法在数字域中实现了更高的性能。当物理实现时，这些性能提升得以保留，从而改善了物理攻击性能。通过使用屏幕和纸板模型的新型物理评估协议，客观评估了真实世界性能。", "conclusion": "所提出的基于超像素的正则化方法能够生成在尺度变化下保持结构、不易受插值损失影响的对抗性补丁，显著提升了物理对抗攻击的性能。"}}
{"id": "2511.18640", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18640", "abs": "https://arxiv.org/abs/2511.18640", "authors": ["Akhil Kondepudi", "Akshay Rao", "Chenhui Zhao", "Yiwei Lyu", "Samir Harake", "Soumyanil Banerjee", "Rushikesh Joshi", "Anna-Katharina Meissner", "Renly Hou", "Cheng Jiang", "Asadur Chowdury", "Ashok Srinivasan", "Brian Athey", "Vikas Gulani", "Aditya Pandey", "Honglak Lee", "Todd Hollon"], "title": "Health system learning achieves generalist neuroimaging models", "comment": "53 pages, 4 main figures, 10 extended data figures", "summary": "Frontier artificial intelligence (AI) models, such as OpenAI's GPT-5 and Meta's DINOv3, have advanced rapidly through training on internet-scale public data, yet such systems lack access to private clinical data. Neuroimaging, in particular, is underrepresented in the public domain due to identifiable facial features within MRI and CT scans, fundamentally restricting model performance in clinical medicine. Here, we show that frontier models underperform on neuroimaging tasks and that learning directly from uncurated data generated during routine clinical care at health systems, a paradigm we call health system learning, yields high-performance, generalist neuroimaging models. We introduce NeuroVFM, a visual foundation model trained on 5.24 million clinical MRI and CT volumes using a scalable volumetric joint-embedding predictive architecture. NeuroVFM learns comprehensive representations of brain anatomy and pathology, achieving state-of-the-art performance across multiple clinical tasks, including radiologic diagnosis and report generation. The model exhibits emergent neuroanatomic understanding and interpretable visual grounding of diagnostic findings. When paired with open-source language models through lightweight visual instruction tuning, NeuroVFM generates radiology reports that surpass frontier models in accuracy, clinical triage, and expert preference. Through clinically grounded visual understanding, NeuroVFM reduces hallucinated findings and critical errors, offering safer clinical decision support. These results establish health system learning as a paradigm for building generalist medical AI and provide a scalable framework for clinical foundation models.", "AI": {"tldr": "本文提出了一种名为“健康系统学习”的新范式，通过在524万份临床MRI和CT影像上训练NeuroVFM模型，解决了前沿AI模型在神经影像任务中表现不佳的问题。NeuroVFM在多项临床任务中实现了最先进的性能，并能生成更准确、更安全的放射学报告。", "motivation": "前沿AI模型（如GPT-5、DINOv3）主要依赖于互联网公共数据训练，但缺乏私有临床数据，尤其神经影像数据因隐私问题在公共领域代表性不足。这限制了这些模型在临床医学中的性能，导致它们在神经影像任务中表现不佳。", "method": "研究引入了“健康系统学习”范式，直接从卫生系统日常临床护理中产生的未整理数据中学习。他们开发了NeuroVFM，一个视觉基础模型，使用可扩展的体积联合嵌入预测架构，在524万份临床MRI和CT影像上进行训练。此外，通过轻量级视觉指令微调，将NeuroVFM与开源语言模型结合。", "result": "NeuroVFM学习了大脑解剖和病理学的全面表征，在放射诊断和报告生成等多项临床任务中实现了最先进的性能。该模型展现出新兴的神经解剖学理解和可解释的诊断发现视觉基础。与开源语言模型结合后，NeuroVFM生成的放射学报告在准确性、临床分诊和专家偏好方面超越了前沿模型，减少了幻觉性发现和关键错误。", "conclusion": "这些结果确立了“健康系统学习”作为构建通用医疗AI的范式，并为临床基础模型提供了一个可扩展的框架。NeuroVFM通过临床接地的视觉理解，提供了更安全的临床决策支持。"}}
{"id": "2511.18559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18559", "abs": "https://arxiv.org/abs/2511.18559", "authors": ["Kuan Wei Huang", "Brandon Li", "Bharath Hariharan", "Noah Snavely"], "title": "C3Po: Cross-View Cross-Modality Correspondence by Pointmap Prediction", "comment": "NeurIPS 2025", "summary": "Geometric models like DUSt3R have shown great advances in understanding the geometry of a scene from pairs of photos. However, they fail when the inputs are from vastly different viewpoints (e.g., aerial vs. ground) or modalities (e.g., photos vs. abstract drawings) compared to what was observed during training. This paper addresses a challenging version of this problem: predicting correspondences between ground-level photos and floor plans. Current datasets for joint photo--floor plan reasoning are limited, either lacking in varying modalities (VIGOR) or lacking in correspondences (WAFFLE). To address these limitations, we introduce a new dataset, C3, created by first reconstructing a number of scenes in 3D from Internet photo collections via structure-from-motion, then manually registering the reconstructions to floor plans gathered from the Internet, from which we can derive correspondence between images and floor plans. C3 contains 90K paired floor plans and photos across 597 scenes with 153M pixel-level correspondences and 85K camera poses. We find that state-of-the-art correspondence models struggle on this task. By training on our new data, we can improve on the best performing method by 34% in RMSE. We also identify open challenges in cross-modal geometric reasoning that our dataset aims to help address.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18601", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18601", "abs": "https://arxiv.org/abs/2511.18601", "authors": ["Wenchao Ma", "Dario Kneubuehler", "Maurice Chu", "Ian Sachs", "Haomiao Jiang", "Sharon Xiaolei Huang"], "title": "RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data", "comment": "Accepted by NeurIPS 2025", "summary": "In this paper, we present RigAnyFace (RAF), a scalable neural auto-rigging framework for facial meshes of diverse topologies, including those with multiple disconnected components. RAF deforms a static neutral facial mesh into industry-standard FACS poses to form an expressive blendshape rig. Deformations are predicted by a triangulation-agnostic surface learning network augmented with our tailored architecture design to condition on FACS parameters and efficiently process disconnected components. For training, we curated a dataset of facial meshes, with a subset meticulously rigged by professional artists to serve as accurate 3D ground truth for deformation supervision. Due to the high cost of manual rigging, this subset is limited in size, constraining the generalization ability of models trained exclusively on it. To address this, we design a 2D supervision strategy for unlabeled neutral meshes without rigs. This strategy increases data diversity and allows for scaled training, thereby enhancing the generalization ability of models trained on this augmented data. Extensive experiments demonstrate that RAF is able to rig meshes of diverse topologies on not only our artist-crafted assets but also in-the-wild samples, outperforming previous works in accuracy and generalizability. Moreover, our method advances beyond prior work by supporting multiple disconnected components, such as eyeballs, for more detailed expression animation. Project page: https://wenchao-m.github.io/RigAnyFace.github.io", "AI": {"tldr": "RigAnyFace (RAF) 是一种可扩展的神经自动绑定框架，能为各种拓扑结构的面部网格（包括具有多个不连通组件的网格）生成行业标准的FACS表情混合形状绑定，并通过结合3D和2D监督策略实现高泛化能力。", "motivation": "传统手动绑定成本高昂且泛化能力有限。现有方法难以处理多样化的面部网格拓扑结构，尤其是不支持具有多个不连通组件（如眼球）的面部模型，限制了表情动画的细节。", "method": "本文提出了RigAnyFace (RAF) 框架。它使用一个与三角剖分无关的表面学习网络，该网络经过专门设计以适应FACS参数并有效处理不连通组件，从而将静态中性面部网格变形为FACS姿态。训练数据包括由专业艺术家精心绑定的少量3D地面真实数据，以及大量未绑定的中性网格。为了解决小规模3D监督数据导致的泛化问题，提出了一种针对无绑定中性网格的2D监督策略，以增加数据多样性并进行大规模训练。", "result": "RAF能够为各种拓扑结构的面部网格（包括艺术家制作的资产和野外样本）生成绑定。它在准确性和泛化能力方面均优于现有工作。此外，该方法支持多个不连通组件（如眼球），实现了更详细的表情动画，超越了以往的工作。", "conclusion": "RigAnyFace (RAF) 提供了一个可扩展、准确且泛化能力强的神经自动绑定框架，能够处理具有多样拓扑结构和不连通组件的面部网格，显著提升了面部表情动画的自动化水平和细节表现力。"}}
{"id": "2511.18595", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18595", "abs": "https://arxiv.org/abs/2511.18595", "authors": ["Wenhao Guo", "Golrokh Mirzaei"], "title": "Stage-Specific Benchmarking of Deep Learning Models for Glioblastoma Follow-Up MRI", "comment": "17 pages, 11 figures", "summary": "Differentiating true tumor progression (TP) from treatment-related pseudoprogression (PsP) in glioblastoma remains challenging, especially at early follow-up. We present the first stage-specific, cross-sectional benchmarking of deep learning models for follow-up MRI using the Burdenko GBM Progression cohort (n = 180). We analyze different post-RT scans independently to test whether architecture performance depends on time-point. Eleven representative DL families (CNNs, LSTMs, hybrids, transformers, and selective state-space models) were trained under a unified, QC-driven pipeline with patient-level cross-validation. Across both stages, accuracies were comparable (~0.70-0.74), but discrimination improved at the second follow-up, with F1 and AUC increasing for several models, indicating richer separability later in the care pathway. A Mamba+CNN hybrid consistently offered the best accuracy-efficiency trade-off, while transformer variants delivered competitive AUCs at substantially higher computational cost and lightweight CNNs were efficient but less reliable. Performance also showed sensitivity to batch size, underscoring the need for standardized training protocols. Notably, absolute discrimination remained modest overall, reflecting the intrinsic difficulty of TP vs. PsP and the dataset's size imbalance. These results establish a stage-aware benchmark and motivate future work incorporating longitudinal modeling, multi-sequence MRI, and larger multi-center cohorts.", "AI": {"tldr": "该研究首次对胶质母细胞瘤复发性MRI中区分真性肿瘤进展（TP）和假性进展（PsP）的深度学习模型进行了阶段性基准测试，发现不同模型在准确性上相似但鉴别能力在后期随访中有所提高，Mamba+CNN混合模型表现出最佳的准确性-效率平衡，但总体鉴别能力仍有待提升。", "motivation": "在胶质母细胞瘤的早期随访中，区分真性肿瘤进展（TP）和治疗相关的假性进展（PsP）仍然是一个挑战。", "method": "研究对Burdenko GBM Progression队列（n=180）的随访MRI进行了首次阶段性、横断面深度学习模型基准测试。独立分析了不同放疗后扫描，以测试模型性能是否依赖于时间点。在统一的、质量控制驱动的流程下，使用患者级别的交叉验证训练了11种代表性深度学习模型（包括CNNs、LSTMs、混合模型、Transformers和选择性状态空间模型）。", "result": "在两个阶段中，模型的准确性相当（约0.70-0.74），但在第二次随访时鉴别能力有所提高，F1和AUC值对于几种模型有所增加，表明在治疗后期可分离性更强。Mamba+CNN混合模型持续提供了最佳的准确性-效率权衡。Transformer变体在计算成本显著更高的情况下提供了有竞争力的AUC，而轻量级CNNs效率高但可靠性较低。性能也对批量大小敏感。总体而言，绝对鉴别能力仍然一般，反映了TP与PsP区分的内在难度和数据集大小不平衡。", "conclusion": "这些结果建立了区分TP与PsP的阶段感知基准，并启发了未来在整合纵向建模、多序列MRI和更大规模多中心队列方面的工作。"}}
{"id": "2511.18591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18591", "abs": "https://arxiv.org/abs/2511.18591", "authors": ["Wei Dong", "Han Zhou", "Junwei Lin", "Jun Chen"], "title": "Zero-Reference Joint Low-Light Enhancement and Deblurring via Visual Autoregressive Modeling with VLM-Derived Modulation", "comment": "Accepted by AAAI 2026; First Var-based method for joint LLIE and deblurring", "summary": "Real-world dark images commonly exhibit not only low visibility and contrast but also complex noise and blur, posing significant restoration challenges. Existing methods often rely on paired data or fail to model dynamic illumination and blur characteristics, leading to poor generalization. To tackle this, we propose a generative framework based on visual autoregressive (VAR) modeling, guided by perceptual priors from the vision-language model (VLM). Specifically, to supply informative conditioning cues for VAR models, we deploy an adaptive curve estimation scheme to modulate the diverse illumination based on VLM-derived visibility scores. In addition, we integrate dynamic and spatial-frequency-aware Rotary Positional Encodings (SF-RoPE) into VAR to enhance its ability to model structures degraded by blur. Furthermore, we propose a recursive phase-domain modulation strategy that mitigates blur-induced artifacts in the phase domain via bounded iterative refinement guided by VLM-assessed blur scores. Our framework is fully unsupervised and achieves state-of-the-art performance on benchmark datasets.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18627", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18627", "abs": "https://arxiv.org/abs/2511.18627", "authors": ["Jan Benedikt Ruhland", "Thorsten Papenbrock", "Jan-Peter Sowa", "Ali Canbay", "Nicole Eter", "Bernd Freisleben", "Dominik Heider"], "title": "Functional Localization Enforced Deep Anomaly Detection Using Fundus Images", "comment": null, "summary": "Reliable detection of retinal diseases from fundus images is challenged by the variability in imaging quality, subtle early-stage manifestations, and domain shift across datasets. In this study, we systematically evaluated a Vision Transformer (ViT) classifier under multiple augmentation and enhancement strategies across several heterogeneous public datasets, as well as the AEyeDB dataset, a high-quality fundus dataset created in-house and made available for the research community. The ViT demonstrated consistently strong performance, with accuracies ranging from 0.789 to 0.843 across datasets and diseases. Diabetic retinopathy and age-related macular degeneration were detected reliably, whereas glaucoma remained the most frequently misclassified disease. Geometric and color augmentations provided the most stable improvements, while histogram equalization benefited datasets dominated by structural subtlety. Laplacian enhancement reduced performance across different settings.\n  On the Papila dataset, the ViT with geometric augmentation achieved an AUC of 0.91, outperforming previously reported convolutional ensemble baselines (AUC of 0.87), underscoring the advantages of transformer architectures and multi-dataset training. To complement the classifier, we developed a GANomaly-based anomaly detector, achieving an AUC of 0.76 while providing inherent reconstruction-based explainability and robust generalization to unseen data. Probabilistic calibration using GUESS enabled threshold-independent decision support for future clinical implementation.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18676", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18676", "abs": "https://arxiv.org/abs/2511.18676", "authors": ["Yongcheng Yao", "Yongshuo Zong", "Raman Dutt", "Yongxin Yang", "Sotirios A Tsaftaris", "Timothy Hospedales"], "title": "MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis", "comment": "8 pages, 8 figures, 4 tables", "summary": "Current vision-language models (VLMs) in medicine are primarily designed for categorical question answering (e.g., \"Is this normal or abnormal?\") or qualitative descriptive tasks. However, clinical decision-making often relies on quantitative assessments, such as measuring the size of a tumor or the angle of a joint, from which physicians draw their own diagnostic conclusions. This quantitative reasoning capability remains underexplored and poorly supported in existing VLMs. In this work, we introduce MedVision, a large-scale dataset and benchmark specifically designed to evaluate and improve VLMs on quantitative medical image analysis. MedVision spans 22 public datasets covering diverse anatomies and modalities, with 30.8 million image-annotation pairs. We focus on three representative quantitative tasks: (1) detection of anatomical structures and abnormalities, (2) tumor/lesion (T/L) size estimation, and (3) angle/distance (A/D) measurement. Our benchmarks show that current off-the-shelf VLMs perform poorly on these tasks. However, with supervised fine-tuning on MedVision, we significantly enhance their performance across detection, T/L estimation, and A/D measurement, demonstrating reduced error rates and improved precision. This work provides a foundation for developing VLMs with robust quantitative reasoning capabilities in medical imaging. Code and data are available at https://medvision-vlm.github.io.", "AI": {"tldr": "本文介绍了MedVision，一个大规模数据集和基准，旨在评估和改进医学视觉语言模型(VLMs)在定量医学图像分析方面的能力，并展示了监督微调的有效性。", "motivation": "当前的医学VLM主要处理分类或定性描述任务，而临床决策严重依赖定量评估（如肿瘤大小、关节角度）。现有VLM缺乏对这种定量推理能力的支持，这促使了对专门数据集和评估方法的需求。", "method": "研究引入了MedVision数据集和基准，该数据集整合了22个公共数据集，包含3080万张图像-标注对，涵盖多种解剖结构和模态。它专注于三个代表性定量任务：解剖结构和异常检测、肿瘤/病变大小估计以及角度/距离测量。研究通过在MedVision上对现有VLM进行监督微调来评估和提升其性能。", "result": "基准测试显示，现有的VLM在这些定量任务上表现不佳。然而，通过在MedVision上进行监督微调，这些模型的性能在检测、肿瘤/病变估计和角度/距离测量方面显著提升，错误率降低，精度提高。", "conclusion": "MedVision为开发具有强大医学影像定量推理能力的VLM奠定了基础，并证明了通过专门数据集进行微调可以显著提升VLM在临床定量分析任务中的表现。"}}
{"id": "2511.18679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18679", "abs": "https://arxiv.org/abs/2511.18679", "authors": ["Xiang Gao", "Yuanpeng Liu", "Xinmu Wang", "Jiazhi Li", "Minghao Guo", "Yu Guo", "Xiyun Song", "Heather Yu", "Zhiqiang Lao", "Xianfeng David Gu"], "title": "Neural Geometry Image-Based Representations with Optimal Transport (OT)", "comment": "WACV2026 Rround 2 Accepted", "summary": "Neural representations for 3D meshes are emerging as an effective solution for compact storage and efficient processing. Existing methods often rely on neural overfitting, where a coarse mesh is stored and progressively refined through multiple decoder networks. While this can restore high-quality surfaces, it is computationally expensive due to successive decoding passes and the irregular structure of mesh data. In contrast, images have a regular structure that enables powerful super-resolution and restoration frameworks, but applying these advantages to meshes is difficult because their irregular connectivity demands complex encoder-decoder architectures. Our key insight is that a geometry image-based representation transforms irregular meshes into a regular image grid, making efficient image-based neural processing directly applicable. Building on this idea, we introduce our neural geometry image-based representation, which is decoder-free, storage-efficient, and naturally suited for neural processing. It stores a low-resolution geometry-image mipmap of the surface, from which high-quality meshes are restored in a single forward pass. To construct geometry images, we leverage Optimal Transport (OT), which resolves oversampling in flat regions and undersampling in feature-rich regions, and enables continuous levels of detail (LoD) through geometry-image mipmapping. Experimental results demonstrate state-of-the-art storage efficiency and restoration accuracy, measured by compression ratio (CR), Chamfer distance (CD), and Hausdorff distance (HD).", "AI": {"tldr": "本文提出了一种基于神经几何图像的3D网格表示方法，它无需解码器、存储高效，并通过单次前向传播恢复高质量网格。该方法利用最优传输构建几何图像，并支持连续细节级别。", "motivation": "现有神经网格表示方法计算成本高昂（需要多次解码），并且难以处理网格的不规则结构。虽然图像具有规则结构，易于高效处理，但将其优势应用于不规则网格却很困难。", "method": "核心思想是将不规则网格转换为规则的几何图像网格，从而直接应用高效的图像处理技术。具体方法包括：存储表面低分辨率的几何图像mipmap；利用最优传输（OT）构建几何图像，解决平坦区域过采样和特征丰富区域欠采样问题，并通过几何图像mipmap实现连续细节级别（LoD）。该方法是无解码器的，并通过单次前向传播恢复高质量网格。", "result": "实验结果表明，该方法在存储效率和恢复精度方面达到了最先进水平，并通过压缩比（CR）、倒角距离（CD）和豪斯多夫距离（HD）进行衡量。", "conclusion": "所提出的基于神经几何图像的表示方法，通过将不规则网格转换为规则的几何图像，并结合最优传输和mipmap技术，提供了一种高效、无解码器、存储高效的3D网格表示和恢复解决方案，实现了卓越的性能。"}}
{"id": "2511.18672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18672", "abs": "https://arxiv.org/abs/2511.18672", "authors": ["Yuchen Xia", "Souvik Kundu", "Mosharaf Chowdhury", "Nishil Talati"], "title": "Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement", "comment": null, "summary": "Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18691", "abs": "https://arxiv.org/abs/2511.18691", "authors": ["Kazi Reyazul Hasan", "Md Nafiu Rahman", "Wasif Jalal", "Sadif Ahmed", "Shahriar Raj", "Mubasshira Musarrat", "Muhammad Abdullah Adnan"], "title": "EVCC: Enhanced Vision Transformer-ConvNeXt-CoAtNet Fusion for Classification", "comment": null, "summary": "Hybrid vision architectures combining Transformers and CNNs have significantly advanced image classification, but they usually do so at significant computational cost. We introduce EVCC (Enhanced Vision Transformer-ConvNeXt-CoAtNet), a novel multi-branch architecture integrating the Vision Transformer, lightweight ConvNeXt, and CoAtNet through key innovations: (1) adaptive token pruning with information preservation, (2) gated bidirectional cross-attention for enhanced feature refinement, (3) auxiliary classification heads for multi-task learning, and (4) a dynamic router gate employing context-aware confidence-driven weighting. Experiments across the CIFAR-100, Tobacco3482, CelebA, and Brain Cancer datasets demonstrate EVCC's superiority over powerful models like DeiT-Base, MaxViT-Base, and CrossViT-Base by consistently achieving state-of-the-art accuracy with improvements of up to 2 percentage points, while reducing FLOPs by 25 to 35%. Our adaptive architecture adjusts computational demands to deployment needs by dynamically reducing token count, efficiently balancing the accuracy-efficiency trade-off while combining global context, local details, and hierarchical features for real-world applications. The source code of our implementation is available at https://anonymous.4open.science/r/EVCC.", "AI": {"tldr": "EVCC是一种新型多分支混合视觉架构，结合了Vision Transformer、ConvNeXt和CoAtNet，通过自适应令牌剪枝、门控双向交叉注意力等创新，在提高图像分类准确率（高达2%）的同时，显著降低了计算成本（FLOPs减少25-35%）。", "motivation": "混合视觉架构（结合Transformer和CNN）在图像分类方面取得了显著进展，但通常伴随着高昂的计算成本。本研究旨在解决这一效率瓶颈。", "method": "本研究引入了EVCC（Enhanced Vision Transformer-ConvNeXt-CoAtNet），这是一种新颖的多分支架构，整合了Vision Transformer、轻量级ConvNeXt和CoAtNet，并通过以下关键创新实现：(1) 具有信息保留的自适应令牌剪枝；(2) 用于增强特征细化的门控双向交叉注意力；(3) 用于多任务学习的辅助分类头；(4) 采用上下文感知置信度驱动加权的动态路由器门。", "result": "在CIFAR-100、Tobacco3482、CelebA和Brain Cancer数据集上的实验表明，EVCC在保持SOTA准确率（最高提升2个百分点）的同时，FLOPs减少了25%至35%，性能优于DeiT-Base、MaxViT-Base和CrossViT-Base等强大模型。其自适应架构能够根据部署需求动态调整计算量。", "conclusion": "EVCC通过动态减少令牌数量，有效地平衡了准确性和效率之间的权衡，结合了全局上下文、局部细节和分层特征，使其能够适应计算需求，适用于真实世界的应用。"}}
{"id": "2511.18682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18682", "abs": "https://arxiv.org/abs/2511.18682", "authors": ["Xiang Gao", "Xinmu Wang", "Zhou Zhao", "Junqi Huang", "Xianfeng David Gu"], "title": "Hierarchical GraphCut Phase Unwrapping based on Invariance of Diffeomorphisms Framework", "comment": "Open Journal of Signal Processing (OJSP) as journal paper for ICIP2025 Accepted", "summary": "Recent years have witnessed rapid advancements in 3D scanning technologies, with applications spanning VR/AR, digital human creation, and medical imaging. Structured-light scanning with phase-shifting techniques is preferred for its use of low-intensity visible light and high accuracy, making it well suited for capturing 4D facial dynamics. A key step is phase unwrapping, which recovers continuous phase values from measurements wrapped modulo 2pi. The goal is to estimate the unwrapped phase count k in the equation Phi = phi + 2pi k, where phi is the wrapped phase and Phi is the true phase. Noise, occlusions, and complex 3D geometry make recovering the true phase challenging because phase unwrapping is ill-posed: measurements only provide modulo 2pi values, and estimating k requires assumptions about surface continuity. Existing methods trade speed for accuracy: fast approaches lack precision, while accurate algorithms are too slow for real-time use. To overcome these limitations, this work proposes a phase unwrapping framework that reformulates GraphCut-based unwrapping as a pixel-labeling problem. This framework improves the estimation of the unwrapped phase count k through the invariance property of diffeomorphisms applied in image space via conformal and optimal transport (OT) maps. An odd number of diffeomorphisms are precomputed from the input phase data, and a hierarchical GraphCut algorithm is applied in each domain. The resulting label maps are fused via majority voting to robustly estimate k at each pixel. Experimental results demonstrate a 45.5x speedup and lower L2 error in real experiments and simulations, showing potential for real-time applications.", "AI": {"tldr": "本文提出了一种新的相位展开框架，通过将GraphCut相位展开重构为像素标记问题，并利用微分同胚（共形和最优传输映射）的不变性来提高无包裹相位计数k的估计，实现了显著的速度提升和更高的精度，有望应用于实时场景。", "motivation": "3D扫描技术在VR/AR、数字人、医学成像等领域快速发展，其中基于相移技术的结构光扫描因其高精度和低强度可见光特性，特别适用于捕捉4D面部动态。然而，关键步骤——相位展开（从包裹相位中恢复连续相位值）是一个病态问题，受噪声、遮挡和复杂3D几何形状影响。现有方法在速度和精度之间存在权衡，快速方法缺乏精度，而精确算法又太慢，无法满足实时应用需求。", "method": "本研究提出了一种相位展开框架，将基于GraphCut的展开重构为像素标记问题。该框架通过共形和最优传输（OT）映射在图像空间中应用微分同胚的不变性，改进了无包裹相位计数k的估计。具体方法是：从输入相位数据中预计算奇数个微分同胚，然后在每个域中应用分层GraphCut算法。最后，通过多数投票融合生成的标签图，以鲁棒地估计每个像素的k值。", "result": "实验结果表明，在真实实验和模拟中，该方法实现了45.5倍的速度提升，并降低了L2误差。", "conclusion": "该方法在速度和精度上的显著改进，展示了其在实时应用中的巨大潜力。"}}
{"id": "2511.18673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18673", "abs": "https://arxiv.org/abs/2511.18673", "authors": ["Yiqing Shi", "Yiren Song", "Mike Zheng Shou"], "title": "Edit2Perceive: Image Editing Diffusion Models Are Strong Dense Perceivers", "comment": null, "summary": "Recent advances in diffusion transformers have shown remarkable generalization in visual synthesis, yet most dense perception methods still rely on text-to-image (T2I) generators designed for stochastic generation. We revisit this paradigm and show that image editing diffusion models are inherently image-to-image consistent, providing a more suitable foundation for dense perception task. We introduce Edit2Perceive, a unified diffusion framework that adapts editing models for depth, normal, and matting. Built upon the FLUX.1 Kontext architecture, our approach employs full-parameter fine-tuning and a pixel-space consistency loss to enforce structure-preserving refinement across intermediate denoising states. Moreover, our single-step deterministic inference yields up to faster runtime while training on relatively small datasets. Extensive experiments demonstrate comprehensive state-of-the-art results across all three tasks, revealing the strong potential of editing-oriented diffusion transformers for geometry-aware perception.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18711", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18711", "abs": "https://arxiv.org/abs/2511.18711", "authors": ["Yuyang Wanyan", "Xiaoshan Yang", "Weiming Dong", "Changsheng Xu"], "title": "Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation", "comment": null, "summary": "In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative LowRank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and subrouters, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.", "AI": {"tldr": "本文针对小样本视频域适应（FSVDA）任务，提出了一种新颖的模态协作低秩分解器（MC-LRD）框架，通过分解模态特有和模态共享特征来更好地应对域偏移和模态协作挑战，并在多个基准测试中取得了显著改进。", "motivation": "小样本视频域适应任务中，视频的多模态特性带来了独特挑战，要求同时考虑域对齐和模态协作，而现有方法忽略了这一点。研究发现，在域偏移影响下，各模态以及融合的多模态特征在目标域上的泛化性能受限，因为每个模态由具有不同域偏移程度的耦合特征组成，这增加了域适应的复杂性并降低了多模态特征融合的有效性。", "method": "本文引入了模态协作低秩分解器（MC-LRD）框架，用于从每个模态中分解出具有不同域偏移水平的模态特有和模态共享特征。MC-LRD包含每个模态的多个分解器和多模态分解路由器（MDR）。每个分解器在不同模态之间逐步共享参数。MDR用于选择性地激活分解器以生成模态特有和模态共享特征。为确保高效分解，对分解器和子路由器分别应用正交去相关约束以增强其多样性。此外，提出了一种跨域激活一致性损失，以确保相同类别的目标和源样本表现出分解器的一致激活偏好，从而促进域对齐。", "result": "在三个公开基准测试上进行的广泛实验结果表明，所提出的模型比现有方法取得了显著改进。", "conclusion": "MC-LRD框架通过有效地分解模态特有和模态共享特征，并结合跨域激活一致性损失，成功解决了小样本视频域适应中域对齐和模态协作的挑战，显著提升了模型性能。"}}
{"id": "2511.18713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18713", "abs": "https://arxiv.org/abs/2511.18713", "authors": ["Hongbin Lin", "Yiming Yang", "Chaoda Zheng", "Yifan Zhang", "Shuaicheng Niu", "Zilu Guo", "Yafeng Li", "Gui Gui", "Shuguang Cui", "Zhen Li"], "title": "DriveFlow: Rectified Flow Adaptation for Robust 3D Object Detection in Autonomous Driving", "comment": "Accepted by AAAI 2026", "summary": "In autonomous driving, vision-centric 3D object detection recognizes and localizes 3D objects from RGB images. However, due to high annotation costs and diverse outdoor scenes, training data often fails to cover all possible test scenarios, known as the out-of-distribution (OOD) issue. Training-free image editing offers a promising solution for improving model robustness by training data enhancement without any modifications to pre-trained diffusion models. Nevertheless, inversion-based methods often suffer from limited effectiveness and inherent inaccuracies, while recent rectified-flow-based approaches struggle to preserve objects with accurate 3D geometry. In this paper, we propose DriveFlow, a Rectified Flow Adaptation method for training data enhancement in autonomous driving based on pre-trained Text-to-Image flow models. Based on frequency decomposition, DriveFlow introduces two strategies to adapt noise-free editing paths derived from text-conditioned velocities. 1) High-Frequency Foreground Preservation: DriveFlow incorporates a high-frequency alignment loss for foreground to maintain precise 3D object geometry. 2) Dual-Frequency Background Optimization: DriveFlow also conducts dual-frequency optimization for background, balancing editing flexibility and semantic consistency. Comprehensive experiments validate the effectiveness and efficiency of DriveFlow, demonstrating comprehensive performance improvements on all categories across OOD scenarios. Code is available at https://github.com/Hongbin98/DriveFlow.", "AI": {"tldr": "针对自动驾驶中视觉中心3D目标检测的域外(OOD)问题，本文提出DriveFlow方法。该方法基于预训练的文本到图像流模型，通过频率分解和双策略（高频前景保留与双频背景优化），实现免训练的数据增强，有效保持3D物体几何并优化背景，从而提升模型在OOD场景下的鲁棒性。", "motivation": "自动驾驶中的视觉中心3D目标检测面临高昂的标注成本和多样的户外场景，导致训练数据无法覆盖所有可能的测试场景，即存在域外(OOD)问题。现有的免训练图像编辑方法（如基于反演或整流流的方法）在有效性、准确性或保持3D几何方面存在局限性。", "method": "本文提出了DriveFlow，一种基于预训练文本到图像流模型的整流流适应方法，用于自动驾驶训练数据增强。该方法基于频率分解，引入了两种策略来适应从文本条件速度导出的无噪声编辑路径：1) 高频前景保留：通过高频对齐损失来保持前景精确的3D物体几何。2) 双频背景优化：对背景进行双频优化，以平衡编辑灵活性和语义一致性。", "result": "全面的实验验证了DriveFlow的有效性和效率。它在OOD场景下，对所有类别的性能都展现出全面的提升。", "conclusion": "DriveFlow通过免训练的数据增强方法，有效解决了自动驾驶中视觉中心3D目标检测的OOD问题，提升了模型的鲁棒性，并成功保持了关键的3D物体几何，提供了一个有前景的解决方案。"}}
{"id": "2511.18706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18706", "abs": "https://arxiv.org/abs/2511.18706", "authors": ["Zhaoyang Jia", "Zihan Zheng", "Naifu Xue", "Jiahao Li", "Bin Li", "Zongyu Guo", "Xiaoyi Zhang", "Houqiang Li", "Yan Lu"], "title": "CoD: A Diffusion Foundation Model for Image Compression", "comment": null, "summary": "Existing diffusion codecs typically build on text-to-image diffusion foundation models like Stable Diffusion. However, text conditioning is suboptimal from a compression perspective, hindering the potential of downstream diffusion codecs, particularly at ultra-low bitrates. To address it, we introduce \\textbf{CoD}, the first \\textbf{Co}mpression-oriented \\textbf{D}iffusion foundation model, trained from scratch to enable end-to-end optimization of both compression and generation. CoD is not a fixed codec but a general foundation model designed for various diffusion-based codecs. It offers several advantages: \\textbf{High compression efficiency}, replacing Stable Diffusion with CoD in downstream codecs like DiffC achieves SOTA results, especially at ultra-low bitrates (e.g., 0.0039 bpp); \\textbf{Low-cost and reproducible training}, 300$\\times$ faster training than Stable Diffusion ($\\sim$ 20 vs. $\\sim$ 6,250 A100 GPU days) on entirely open image-only datasets; \\textbf{Providing new insights}, e.g., We find pixel-space diffusion can achieve VTM-level PSNR with high perceptual quality and can outperform GAN-based codecs using fewer parameters. We hope CoD lays the foundation for future diffusion codec research. Codes will be released.", "AI": {"tldr": "CoD是一个面向压缩的扩散基础模型，从头开始训练，旨在解决现有扩散编解码器中文本条件不足的问题，并在超低比特率下实现最先进的压缩效率。", "motivation": "现有扩散编解码器依赖于如Stable Diffusion等文本到图像扩散模型，但文本条件对于压缩而言并非最优，尤其是在超低比特率下，这限制了下游扩散编解码器的潜力。", "method": "引入CoD，这是首个面向压缩的扩散基础模型，从零开始训练，旨在实现压缩和生成的端到端优化。CoD使用完全开放的纯图像数据集进行训练，训练速度比Stable Diffusion快300倍。", "result": "CoD在下游编解码器（如DiffC）中替换Stable Diffusion后，实现了最先进的压缩效率，尤其是在超低比特率（如0.0039 bpp）下。它训练成本低且可复现（约20个A100 GPU天）。研究发现像素空间扩散可以达到VTM级别的PSNR并具有高感知质量，并且可以使用更少的参数超越基于GAN的编解码器。", "conclusion": "CoD为未来的扩散编解码器研究奠定了基础，通过其高压缩效率、低成本训练和提供的全新见解，有望推动该领域的发展。"}}
{"id": "2511.18677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18677", "abs": "https://arxiv.org/abs/2511.18677", "authors": ["Yunpeng Gong", "Yongjie Hou", "Jiangming Shi", "Kim Long Diep", "Min Jiang"], "title": "A Theory-Inspired Framework for Few-Shot Cross-Modal Sketch Person Re-Identification", "comment": "Accepted by AAAI2026", "summary": "Sketch based person re-identification aims to match hand-drawn sketches with RGB surveillance images, but remains challenging due to significant modality gaps and limited annotated data. To address this, we introduce KTCAA, a theoretically grounded framework for few-shot cross-modal generalization. Motivated by generalization theory, we identify two key factors influencing target domain risk: (1) domain discrepancy, which quantifies the alignment difficulty between source and target distributions; and (2) perturbation invariance, which evaluates the model's robustness to modality shifts. Based on these insights, we propose two components: (1) Alignment Augmentation (AA), which applies localized sketch-style transformations to simulate target distributions and facilitate progressive alignment; and (2) Knowledge Transfer Catalyst (KTC), which enhances invariance by introducing worst-case perturbations and enforcing consistency. These modules are jointly optimized under a meta-learning paradigm that transfers alignment knowledge from data-rich RGB domains to sketch-based scenarios. Experiments on multiple benchmarks demonstrate that KTCAA achieves state-of-the-art performance, particularly in data-scarce conditions.", "AI": {"tldr": "本文提出KTCAA框架，用于解决草图行人重识别中模态差距大和数据稀缺导致的少样本跨模态泛化挑战。该框架通过对齐增强（AA）和知识迁移催化剂（KTC）模块，在元学习范式下优化，实现了最先进的性能。", "motivation": "草图行人重识别任务面临两大挑战：手绘草图与RGB监控图像之间存在显著的模态差异，以及标注数据，特别是少样本场景下的数据非常有限。", "method": "KTCAA是一个基于泛化理论的少样本跨模态泛化框架，旨在解决域差异和扰动不变性问题。它包含两个核心组件：1) 对齐增强（AA），通过局部草图风格转换模拟目标分布，促进渐进式对齐；2) 知识迁移催化剂（KTC），通过引入最坏情况扰动并强制一致性来增强模型的不变性。这两个模块在元学习范式下联合优化，将对齐知识从数据丰富的RGB域迁移到基于草图的场景。", "result": "KTCAA在多个基准测试上取得了最先进的性能，尤其在数据稀缺（少样本）条件下表现突出。", "conclusion": "KTCAA是一个理论基础扎实的有效框架，成功解决了草图行人重识别中模态差距和数据稀缺的挑战，特别是在少样本跨模态泛化方面表现出色，提升了模型的对齐能力和鲁棒性。"}}
{"id": "2511.18684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18684", "abs": "https://arxiv.org/abs/2511.18684", "authors": ["Shristi Das Biswas", "Arani Roy", "Kaushik Roy"], "title": "Now You See It, Now You Don't - Instant Concept Erasure for Safe Text-to-Image and Video Generation", "comment": null, "summary": "Robust concept removal for text-to-image (T2I) and text-to-video (T2V) models is essential for their safe deployment. Existing methods, however, suffer from costly retraining, inference overhead, or vulnerability to adversarial attacks. Crucially, they rarely model the latent semantic overlap between the target erase concept and surrounding content -- causing collateral damage post-erasure -- and even fewer methods work reliably across both T2I and T2V domains. We introduce Instant Concept Erasure (ICE), a training-free, modality-agnostic, one-shot weight modification approach that achieves precise, persistent unlearning with zero overhead. ICE defines erase and preserve subspaces using anisotropic energy-weighted scaling, then explicitly regularises against their intersection using a unique, closed-form overlap projector. We pose a convex and Lipschitz-bounded Spectral Unlearning Objective, balancing erasure fidelity and intersection preservation, that admits a stable and unique analytical solution. This solution defines a dissociation operator that is translated to the model's text-conditioning layers, making the edit permanent and runtime-free. Across targeted removals of artistic styles, objects, identities, and explicit content, ICE efficiently achieves strong erasure with improved robustness to red-teaming, all while causing only minimal degradation of original generative abilities in both T2I and T2V models.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18729", "abs": "https://arxiv.org/abs/2511.18729", "authors": ["Lin Liu", "Caiyan Jia", "Guanyi Yu", "Ziying Song", "JunQiao Li", "Feiyang Jia", "Peiliang Wu", "Xiaoshuai Hao", "Yandan Luo"], "title": "GuideFlow: Constraint-Guided Flow Matching for Planning in End-to-End Autonomous Driving", "comment": null, "summary": "Driving planning is a critical component of end-to-end (E2E) autonomous driving. However, prevailing Imitative E2E Planners often suffer from multimodal trajectory mode collapse, failing to produce diverse trajectory proposals. Meanwhile, Generative E2E Planners struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. In this paper, we propose \\textit{\\textbf{GuideFlow}}, a novel planning framework that leverages Constrained Flow Matching. Concretely, \\textit{\\textbf{GuideFlow}} explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our core contribution lies in directly enforcing explicit constraints within the flow matching generation process, rather than relying on implicit constraint encoding. Crucially, \\textit{\\textbf{GuideFlow}} unifies the training of the flow matching with the Energy-Based Model (EBM) to enhance the model's autonomous optimization capability to robustly satisfy physical constraints. Secondly, \\textit{\\textbf{GuideFlow}} parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Extensive evaluations on major driving benchmarks (Bench2Drive, NuScenes, NavSim and ADV-NuScenes) validate the effectiveness of \\textit{\\textbf{GuideFlow}}. Notably, on the NavSim test hard split (Navhard), \\textit{\\textbf{GuideFlow}} achieved SOTA with an EPDMS score of 43.0. The code will be released.", "AI": {"tldr": "GuideFlow是一种新颖的自动驾驶规划框架，它利用约束流匹配直接在生成过程中强制执行安全和物理约束，并通过与EBM统一训练来增强优化能力，解决了现有端到端规划器多模态轨迹模式崩溃和难以整合约束的问题，实现了多样化且安全的轨迹生成，并在多个基准测试中达到了SOTA性能。", "motivation": "现有的端到端模仿式规划器常遭遇多模态轨迹模式崩溃，无法生成多样化的轨迹；而生成式规划器难以直接在生成过程中整合关键的安全和物理约束，需要额外的优化阶段来精炼输出。", "method": "GuideFlow提出了一种基于约束流匹配的规划框架。它显式建模流匹配过程以缓解模式崩溃，并直接在流匹配生成过程中强制执行显式约束，而非依赖隐式编码。核心贡献在于将流匹配训练与基于能量的模型（EBM）统一，以增强模型自主优化能力来满足物理约束。此外，GuideFlow将驾驶激进性参数化为生成过程中的控制信号，实现轨迹风格的精确操控。", "result": "GuideFlow在主要的驾驶基准测试（Bench2Drive、NuScenes、NavSim和ADV-NuScenes）上进行了广泛评估，验证了其有效性。特别是在NavSim的Navhard测试集上，GuideFlow以43.0的EPDMS分数达到了SOTA。", "conclusion": "GuideFlow通过引入约束流匹配，有效解决了端到端自动驾驶规划中多模态轨迹模式崩溃和物理约束难以整合的问题。它通过直接强制执行显式约束并与EBM统一训练，实现了多样化、安全且可控的轨迹生成，并在多个重要基准测试中取得了领先性能。"}}
{"id": "2511.18742", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18742", "abs": "https://arxiv.org/abs/2511.18742", "authors": ["Zhenghan Fang", "Jian Zheng", "Qiaozi Gao", "Xiaofeng Gao", "Jeremias Sulam"], "title": "ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion", "comment": null, "summary": "Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.", "AI": {"tldr": "本文提出ProxT2I，一种基于后向离散化和学习条件近端算子的文本到图像（T2I）扩散模型，通过强化学习优化采样器，并发布了新的LAION-Face-T2I-15M数据集。该模型在采样效率和人类偏好对齐方面优于现有方法，且计算量和模型尺寸更小。", "motivation": "目前大多数扩散模型采样器依赖于前向离散化和学习到的分数函数，导致采样过程缓慢且不稳定，需要大量采样步骤才能生成高质量样本。", "method": "本文开发了ProxT2I，一个基于后向离散化的T2I扩散模型，它使用学习到的条件近端算子代替分数函数。此外，研究利用强化学习和策略优化来优化采样器以获得特定任务的奖励。同时，还构建了一个包含1500万高质量人类图像和精细化描述的大规模开源数据集LAION-Face-T2I-15M用于训练和评估。", "result": "与基于分数函数的基线相比，ProxT2I显著提高了采样效率和人类偏好对齐。它在保持与现有最先进开源T2I模型相当的生成质量的同时，所需计算资源更少，模型尺寸更小，提供了一个轻量级但高性能的人类文本到图像生成解决方案。", "conclusion": "ProxT2I通过引入后向离散化和近端算子，结合强化学习优化，并在新数据集上训练，成功提供了一个高效、高质量且资源需求更低的人类文本到图像生成方案，性能与现有顶尖模型相当。"}}
{"id": "2511.18695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18695", "abs": "https://arxiv.org/abs/2511.18695", "authors": ["Changcai Li", "Wenwei Lin", "Zuoxun Hou", "Gang Chen", "Wei Zhang", "Huihui Zhou", "Weishi Zheng"], "title": "Exploring Surround-View Fisheye Camera 3D Object Detection", "comment": "9 pages,6 figures, accepted at AAAI 2026", "summary": "In this work, we explore the technical feasibility of implementing end-to-end 3D object detection (3DOD) with surround-view fisheye camera system. Specifically, we first investigate the performance drop incurred when transferring classic pinhole-based 3D object detectors to fisheye imagery. To mitigate this, we then develop two methods that incorporate the unique geometry of fisheye images into mainstream detection frameworks: one based on the bird's-eye-view (BEV) paradigm, named FisheyeBEVDet, and the other on the query-based paradigm, named FisheyePETR. Both methods adopt spherical spatial representations to effectively capture fisheye geometry. In light of the lack of dedicated evaluation benchmarks, we release Fisheye3DOD, a new open dataset synthesized using CARLA and featuring both standard pinhole and fisheye camera arrays. Experiments on Fisheye3DOD show that our fisheye-compatible modeling improves accuracy by up to 6.2% over baseline methods.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18699", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18699", "abs": "https://arxiv.org/abs/2511.18699", "authors": ["Jiarui Xue", "Dongjian Yang", "Ye Sun", "Gang Liu"], "title": "Dendritic Convolution for Noise Image Recognition", "comment": "11 pages, 8 figures", "summary": "In real-world scenarios of image recognition, there exists substantial noise interference. Existing works primarily focus on methods such as adjusting networks or training strategies to address noisy image recognition, and the anti-noise performance has reached a bottleneck. However, little is known about the exploration of anti-interference solutions from a neuronal perspective.This paper proposes an anti-noise neuronal convolution. This convolution mimics the dendritic structure of neurons, integrates the neighborhood interaction computation logic of dendrites into the underlying design of convolutional operations, and simulates the XOR logic preprocessing function of biological dendrites through nonlinear interactions between input features, thereby fundamentally reconstructing the mathematical paradigm of feature extraction. Unlike traditional convolution where noise directly interferes with feature extraction and exerts a significant impact, DDC mitigates the influence of noise by focusing on the interaction of neighborhood information. Experimental results demonstrate that in image classification tasks (using YOLOv11-cls, VGG16, and EfficientNet-B0) and object detection tasks (using YOLOv11, YOLOv8, and YOLOv5), after replacing traditional convolution with the dendritic convolution, the accuracy of the EfficientNet-B0 model on noisy datasets is relatively improved by 11.23%, and the mean Average Precision (mAP) of YOLOv8 is increased by 19.80%. The consistency between the computation method of this convolution and the dendrites of biological neurons enables it to perform significantly better than traditional convolution in complex noisy environments.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18746", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18746", "abs": "https://arxiv.org/abs/2511.18746", "authors": ["Hao Li", "Qiao Sun"], "title": "Any4D: Open-Prompt 4D Generation from Natural Language and Images", "comment": null, "summary": "While video-generation-based embodied world models have gained increasing attention, their reliance on large-scale embodied interaction data remains a key bottleneck. The scarcity, difficulty of collection, and high dimensionality of embodied data fundamentally limit the alignment granularity between language and actions and exacerbate the challenge of long-horizon video generation--hindering generative models from achieving a \\textit{\"GPT moment\"} in the embodied domain. There is a naive observation: \\textit{the diversity of embodied data far exceeds the relatively small space of possible primitive motions}. Based on this insight, we propose \\textbf{Primitive Embodied World Models} (PEWM), which restricts video generation to fixed shorter horizons, our approach \\textit{1) enables} fine-grained alignment between linguistic concepts and visual representations of robotic actions, \\textit{2) reduces} learning complexity, \\textit{3) improves} data efficiency in embodied data collection, and \\textit{4) decreases} inference latency. By equipping with a modular Vision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further enables flexible closed-loop control and supports compositional generalization of primitive-level policies over extended, complex tasks. Our framework leverages the spatiotemporal vision priors in video models and the semantic awareness of VLMs to bridge the gap between fine-grained physical interaction and high-level reasoning, paving the way toward scalable, interpretable, and general-purpose embodied intelligence.", "AI": {"tldr": "本文提出了原始具身世界模型（PEWM），通过将视频生成限制在较短的时间范围内，解决了现有具身世界模型对大规模数据依赖的瓶颈，实现了语言与动作的细粒度对齐、提高了数据效率，并支持复杂任务的组合泛化。", "motivation": "视频生成式具身世界模型依赖大规模具身交互数据，但具身数据稀缺、收集困难且维度高，限制了语言与动作的对齐粒度，加剧了长时程视频生成的挑战，阻碍了具身领域“GPT时刻”的到来。研究者观察到具身数据的多样性远超原始动作的有限空间。", "method": "基于原始动作空间有限的洞察，本文提出了原始具身世界模型（PEWM）。PEWM将视频生成限制在固定的较短时间范围内，并配备了模块化的视觉-语言模型（VLM）规划器和起点-目标热图引导（SGG）机制。", "result": "PEWM实现了语言概念与机器人动作视觉表征之间的细粒度对齐，降低了学习复杂性，提高了具身数据收集效率，并减少了推理延迟。通过VLM规划器和SGG机制，PEWM进一步实现了灵活的闭环控制，并支持原始级别策略在扩展复杂任务上的组合泛化。", "conclusion": "PEWM利用视频模型的时空视觉先验和VLM的语义感知能力，弥合了细粒度物理交互与高级推理之间的差距，为实现可扩展、可解释和通用目的的具身智能铺平了道路。"}}
{"id": "2511.18701", "categories": ["cs.CV", "cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18701", "abs": "https://arxiv.org/abs/2511.18701", "authors": ["Mustafa Munir", "Harsh Goel", "Xiwen Wei", "Minkyu Choi", "Sahil Shah", "Kartikeya Bhardwaj", "Paul Whatmough", "Sandeep Chinchali", "Radu Marculescu"], "title": "ObjectAlign: Neuro-Symbolic Object Consistency Verification and Correction", "comment": null, "summary": "Video editing and synthesis often introduce object inconsistencies, such as frame flicker and identity drift that degrade perceptual quality. To address these issues, we introduce ObjectAlign, a novel framework that seamlessly blends perceptual metrics with symbolic reasoning to detect, verify, and correct object-level and temporal inconsistencies in edited video sequences. The novel contributions of ObjectAlign are as follows: First, we propose learnable thresholds for metrics characterizing object consistency (i.e. CLIP-based semantic similarity, LPIPS perceptual distance, histogram correlation, and SAM-derived object-mask IoU). Second, we introduce a neuro-symbolic verifier that combines two components: (a) a formal, SMT-based check that operates on masked object embeddings to provably guarantee that object identity does not drift, and (b) a temporal fidelity check that uses a probabilistic model checker to verify the video's formal representation against a temporal logic specification. A frame transition is subsequently deemed \"consistent\" based on a single logical assertion that requires satisfying both the learned metric thresholds and this unified neuro-symbolic constraint, ensuring both low-level stability and high-level temporal correctness. Finally, for each contiguous block of flagged frames, we propose a neural network based interpolation for adaptive frame repair, dynamically choosing the interpolation depth based on the number of frames to be corrected. This enables reconstruction of the corrupted frames from the last valid and next valid keyframes. Our results show up to 1.4 point improvement in CLIP Score and up to 6.1 point improvement in warp error compared to SOTA baselines on the DAVIS and Pexels video datasets.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18719", "abs": "https://arxiv.org/abs/2511.18719", "authors": ["Ziqi Ni", "Yuanzhi Liang", "Rui Li", "Yi Zhou", "Haibing Huang", "Chi Zhang", "Xuelong Li"], "title": "Seeing What Matters: Visual Preference Policy Optimization for Visual Generation", "comment": null, "summary": "Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.", "AI": {"tldr": "ViPO是一种GRPO变体，通过将标量奖励提升为像素级优势，利用视觉骨干网络构建感知结构模块，为视觉生成模型提供更精细的优化信号，从而优于传统GRPO。", "motivation": "现有的GRPO在训练视觉生成模型时，仅使用每个样本的单一标量奖励，忽略了视觉内容的丰富时空结构。这种粗糙的监督阻碍了局部伪影的修正和细粒度感知线索的建模。", "method": "本文提出了Visual Preference Policy Optimization (ViPO)，一种GRPO的变体，将标量反馈提升为结构化的像素级优势。ViPO采用感知结构模块（Perceptual Structuring Module），利用预训练的视觉骨干网络构建空间和时间感知的优势图，将优化压力重新分配到感知上重要的区域，同时保持标准GRPO的稳定性。", "result": "在图像和视频基准测试中，ViPO始终优于传统的GRPO，提高了与人类偏好奖励的域内对齐，并增强了域外评估的泛化能力。该方法与架构无关、轻量级，并与现有GRPO训练流程完全兼容。", "conclusion": "ViPO通过提供更具表现力和信息量的学习信号，即结构化的像素级反馈，显著改善了视觉生成模型的对齐和泛化能力，是现有GRPO训练管道的有效增强。"}}
{"id": "2511.18780", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18780", "abs": "https://arxiv.org/abs/2511.18780", "authors": ["Ruize Ma", "Minghong Cai", "Yilei Jiang", "Jiaming Han", "Yi Feng", "Yingshui Tan", "Xiaoyong Zhu", "Bo Zhang", "Bo Zheng", "Xiangyu Yue"], "title": "ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection", "comment": null, "summary": "Recent progress in video generative models has enabled the creation of high-quality videos from multimodal prompts that combine text and images. While these systems offer enhanced controllability, they also introduce new safety risks, as harmful content can emerge from individual modalities or their interaction. Existing safety methods are often text-only, require prior knowledge of the risk category, or operate as post-generation auditors, struggling to proactively mitigate such compositional, multimodal risks. To address this challenge, we present ConceptGuard, a unified safeguard framework for proactively detecting and mitigating unsafe semantics in multimodal video generation. ConceptGuard operates in two stages: First, a contrastive detection module identifies latent safety risks by projecting fused image-text inputs into a structured concept space; Second, a semantic suppression mechanism steers the generative process away from unsafe concepts by intervening in the prompt's multimodal conditioning. To support the development and rigorous evaluation of this framework, we introduce two novel benchmarks: ConceptRisk, a large-scale dataset for training on multimodal risks, and T2VSafetyBench-TI2V, the first benchmark adapted from T2VSafetyBench for the Text-and-Image-to-Video (TI2V) safety setting. Comprehensive experiments on both benchmarks show that ConceptGuard consistently outperforms existing baselines, achieving state-of-the-art results in both risk detection and safe video generation.", "AI": {"tldr": "ConceptGuard是一个统一的视频生成安全框架，能主动检测并缓解多模态提示（文本和图像）中潜在的不安全语义，通过两阶段方法实现，并引入了新的风险检测和评估基准。", "motivation": "当前的视频生成模型能从多模态提示创建高质量视频，但也带来了新的安全风险，有害内容可能源于单一模态或其交互。现有安全方法多为纯文本、需预知风险类别或作为后生成审计工具，难以主动应对这种组合性的多模态风险。", "method": "ConceptGuard框架分两阶段运行：1. 对比检测模块：将融合的图像-文本输入映射到结构化概念空间，以识别潜在安全风险。2. 语义抑制机制：通过干预提示的多模态条件作用，引导生成过程远离不安全概念。为支持开发和评估，还引入了两个新基准：ConceptRisk（多模态风险训练数据集）和T2VSafetyBench-TI2V（首个针对文本和图像到视频（TI2V）安全设置的基准）。", "result": "在ConceptRisk和T2VSafetyBench-TI2V这两个基准上进行的综合实验表明，ConceptGuard始终优于现有基线，在风险检测和安全视频生成方面均取得了最先进的成果。", "conclusion": "ConceptGuard提供了一个统一且主动的安全保障框架，有效解决了多模态视频生成中的不安全语义检测和缓解问题，显著提升了视频生成的安全性。"}}
{"id": "2511.18734", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18734", "abs": "https://arxiv.org/abs/2511.18734", "authors": ["Keyang Lu", "Sifan Zhou", "Hongbin Xu", "Gang Xu", "Zhifei Yang", "Yikai Wang", "Zhen Xiao", "Jieyi Long", "Ming Li"], "title": "Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion", "comment": "22 pages, 16 figures", "summary": "Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.", "AI": {"tldr": "Yo'City是一个新颖的智能体框架，利用大型模型的推理能力，实现了用户定制化和无限扩展的3D城市生成，并通过分层规划、迭代生成和关系引导扩展机制，显著优于现有方法。", "motivation": "现有3D城市生成方法多依赖单一扩散模型训练，限制了其生成个性化和无边界城市级场景的能力。", "method": "Yo'City采用智能体框架，利用大型模型的推理和组合能力。具体方法包括：1) 顶层规划：定义“城市-区域-网格”分层结构，通过全局规划器确定布局，本地设计器细化网格级描述。2) 网格级3D生成：采用“生产-优化-评估”的等距图像合成循环，再进行图像到3D的生成。3) 连续城市演化：引入用户交互、关系引导的扩展机制，通过基于场景图的距离和语义感知布局优化，确保空间连贯的城市增长。同时构建了基准数据集和六个多维度评估指标。", "result": "Yo'City在语义、几何、纹理和布局等所有评估方面，始终优于现有的最先进方法。", "conclusion": "Yo'City提供了一个卓越的框架，能够实现用户定制化和无限扩展的3D城市生成，解决了现有方法的局限性。"}}
{"id": "2511.18763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18763", "abs": "https://arxiv.org/abs/2511.18763", "authors": ["Xuanzhao Dong", "Wenhui Zhu", "Yujian Xiong", "Xiwen Chen", "Hao Wang", "Xin Li", "Jiajun Cheng", "Zhipeng Wang", "Shao Tang", "Oana Dumitrascu", "Yalin Wang"], "title": "VAOT: Vessel-Aware Optimal Transport for Retinal Fundus Enhancement", "comment": null, "summary": "Color fundus photography (CFP) is central to diagnosing and monitoring retinal disease, yet its acquisition variability (e.g., illumination changes) often degrades image quality, which motivates robust enhancement methods. Unpaired enhancement pipelines are typically GAN-based, however, they can distort clinically critical vasculature, altering vessel topology and endpoint integrity. Motivated by these structural alterations, we propose Vessel-Aware Optimal Transport (\\textbf{VAOT}), a framework that combines an optimal-transport objective with two structure-preserving regularizers: (i) a skeleton-based loss to maintain global vascular connectivity and (ii) an endpoint-aware loss to stabilize local termini. These constraints guide learning in the unpaired setting, reducing noise while preserving vessel structure. Experimental results on synthetic degradation benchmark and downstream evaluations in vessel and lesion segmentation demonstrate the superiority of the proposed methods against several state-of-the art baselines. The code is available at https://github.com/Retinal-Research/VAOT", "AI": {"tldr": "本文提出了一种名为VAOT的血管感知最优传输框架，用于在无配对设置下增强眼底图像，通过结合最优传输目标和两个结构保留正则化器，有效提高图像质量同时保护血管结构。", "motivation": "眼底彩照（CFP）在诊断和监测视网膜疾病中至关重要，但其采集变异性（如光照变化）常导致图像质量下降。现有的无配对增强方法（通常基于GAN）可能扭曲临床关键的血管结构，改变血管拓扑和端点完整性。", "method": "本文提出了血管感知最优传输（VAOT）框架，该框架将最优传输目标与两个结构保留正则化器结合：(i) 基于骨架的损失，以维持全局血管连通性；(ii) 端点感知损失，以稳定局部末梢。这些约束在无配对设置下指导学习，从而在减少噪声的同时保留血管结构。", "result": "在合成降级基准测试和血管与病灶分割的下游评估中，所提出的方法优于几种最先进的基线方法。", "conclusion": "VAOT框架能够有效地增强眼底图像，同时在无配对设置下显著保留关键的血管结构，解决了现有方法可能导致的结构扭曲问题。"}}
{"id": "2511.18735", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18735", "abs": "https://arxiv.org/abs/2511.18735", "authors": ["Zhantao Gong", "Liaoyuan Fan", "Qing Guo", "Xun Xu", "Xulei Yang", "Shijie Li"], "title": "Thinking Ahead: Foresight Intelligence in MLLMs and World Models", "comment": "25 pages, 27 figures, submitted to CVPR 2026", "summary": "In this work, we define Foresight Intelligence as the capability to anticipate and interpret future events-an ability essential for applications such as autonomous driving, yet largely overlooked by existing research. To bridge this gap, we introduce FSU-QA, a new Visual Question-Answering (VQA) dataset specifically designed to elicit and evaluate Foresight Intelligence. Using FSU-QA, we conduct the first comprehensive study of state-of-the-art Vision-Language Models (VLMs) under foresight-oriented tasks, revealing that current models still struggle to reason about future situations. Beyond serving as a benchmark, FSU-QA also enables the assessment of world models by measuring the semantic coherence of their generated predictions, quantified through performance gains when VLMs are augmented with such outputs. Our experiments further demonstrate that FSU-QA can effectively enhance foresight reasoning: even small VLMs fine-tuned on FSU-QA surpass much larger, advanced models by a substantial margin. Together, these findings position FSU-QA as a principled foundation for developing next-generation models capable of truly anticipating and understanding future events.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18757", "abs": "https://arxiv.org/abs/2511.18757", "authors": ["Yongqi Zhu", "Morui Zhu", "Qi Chen", "Deyuan Qu", "Song Fu", "Qing Yang"], "title": "From Features to Reference Points: Lightweight and Adaptive Fusion for Cooperative Autonomous Driving", "comment": "10 pages, 4 figures", "summary": "We present RefPtsFusion, a lightweight and interpretable framework for cooperative autonomous driving. Instead of sharing large feature maps or query embeddings, vehicles exchange compact reference points, e.g., objects' positions, velocities, and size information. This approach shifts the focus from \"what is seen\" to \"where to see\", creating a sensor- and model-independent interface that works well across vehicles with heterogeneous perception models while greatly reducing communication bandwidth. To enhance the richness of shared information, we further develop a selective Top-K query fusion that selectively adds high-confidence queries from the sender. It thus achieves a strong balance between accuracy and communication cost. Experiments on the M3CAD dataset show that RefPtsFusion maintains stable perception performance while reducing communication overhead by five orders of magnitude, dropping from hundreds of MB/s to only a few KB/s at 5 FPS (frame per second), compared to traditional feature-level fusion methods. Extensive experiments also demonstrate RefPtsFusion's strong robustness and consistent transmission behavior, highlighting its potential for scalable, real-time cooperative driving systems.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18801", "abs": "https://arxiv.org/abs/2511.18801", "authors": ["Yichen Yang", "Hong Li", "Haodong Zhu", "Linin Yang", "Guojun Lei", "Sheng Xu", "Baochang Zhang"], "title": "PartDiffuser: Part-wise 3D Mesh Generation via Discrete Diffusion", "comment": null, "summary": "Existing autoregressive (AR) methods for generating artist-designed meshes struggle to balance global structural consistency with high-fidelity local details, and are susceptible to error accumulation. To address this, we propose PartDiffuser, a novel semi-autoregressive diffusion framework for point-cloud-to-mesh generation. The method first performs semantic segmentation on the mesh and then operates in a \"part-wise\" manner: it employs autoregression between parts to ensure global topology, while utilizing a parallel discrete diffusion process within each semantic part to precisely reconstruct high-frequency geometric features. PartDiffuser is based on the DiT architecture and introduces a part-aware cross-attention mechanism, using point clouds as hierarchical geometric conditioning to dynamically control the generation process, thereby effectively decoupling the global and local generation tasks. Experiments demonstrate that this method significantly outperforms state-of-the-art (SOTA) models in generating 3D meshes with rich detail, exhibiting exceptional detail representation suitable for real-world applications.", "AI": {"tldr": "PartDiffuser是一种新颖的半自回归扩散框架，用于点云到网格的生成。它通过在部件间进行自回归以确保全局拓扑，同时在部件内部并行进行离散扩散以重建局部细节，从而有效平衡了全局结构一致性和高保真局部细节。", "motivation": "现有的自回归（AR）方法在生成艺术家设计的网格时，难以平衡全局结构一致性和高保真局部细节，并且容易出现错误累积。", "method": "PartDiffuser首先对网格进行语义分割，然后采用“部件式”操作：部件间采用自回归方法确保全局拓扑，同时在每个语义部件内部利用并行离散扩散过程精确重建高频几何特征。该方法基于DiT架构，引入了部件感知交叉注意力机制，并使用点云作为分层几何条件来动态控制生成过程，从而有效解耦了全局和局部生成任务。", "result": "实验表明，PartDiffuser在生成具有丰富细节的3D网格方面显著优于最先进（SOTA）模型，展现出卓越的细节表示能力，适用于实际应用。", "conclusion": "PartDiffuser成功解决了现有方法在全局结构与局部细节之间平衡的难题，通过其独特的半自回归扩散框架和部件感知机制，能够生成高质量、细节丰富的3D网格，具有广泛的实际应用潜力。"}}
{"id": "2511.18781", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18781", "abs": "https://arxiv.org/abs/2511.18781", "authors": ["Haotian Yan", "Bocheng Guo", "Jianzhong He", "Nir A. Sochen", "Ofer Pasternak", "Lauren J O'Donnell", "Fan Zhang"], "title": "A Novel Dual-Stream Framework for dMRI Tractography Streamline Classification with Joint dMRI and fMRI Data", "comment": "Submitted to ISBI 2026, 7 pages, 2 figures", "summary": "Streamline classification is essential to identify anatomically meaningful white matter tracts from diffusion MRI (dMRI) tractography. However, current streamline classification methods rely primarily on the geometric features of the streamline trajectory, failing to distinguish between functionally distinct fiber tracts with similar pathways. To address this, we introduce a novel dual-stream streamline classification framework that jointly analyzes dMRI and functional MRI (fMRI) data to enhance the functional coherence of tract parcellation. We design a novel network that performs streamline classification using a pretrained backbone model for full streamline trajectories, while augmenting with an auxiliary network that processes fMRI signals from fiber endpoint regions. We demonstrate our method by parcellating the corticospinal tract (CST) into its four somatotopic subdivisions. Experimental results from ablation studies and comparisons with state-of-the-art methods demonstrate our approach's superior performance.", "AI": {"tldr": "本文提出了一种新颖的双流线分类框架，结合扩散MRI (dMRI) 和功能MRI (fMRI) 数据，以提高白质束分割的功能一致性，并成功地将皮质脊髓束 (CST) 分割成其体感拓扑亚区。", "motivation": "当前的流线分类方法主要依赖于流线轨迹的几何特征，无法区分路径相似但功能不同的纤维束。为了解决这一问题，研究旨在通过整合功能信息来增强白质束分割的功能相干性。", "method": "研究设计了一个新颖的双流线分类网络。该网络使用一个预训练的骨干模型处理完整的流线轨迹（来自dMRI），同时辅以一个辅助网络，处理来自纤维端点区域的功能MRI信号，从而联合分析dMRI和fMRI数据。", "result": "实验结果表明，该方法在消融研究和与现有最先进方法的比较中表现出卓越的性能。它成功地将皮质脊髓束 (CST) 分割成其四个体感拓扑亚区。", "conclusion": "该双流线分类方法通过整合dMRI和fMRI数据，有效增强了白质束分割的功能一致性，并在区分功能不同的纤维束方面展现出优越性。"}}
{"id": "2511.18788", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18788", "abs": "https://arxiv.org/abs/2511.18788", "authors": ["Shiyi Mu", "Zichong Gu", "Zhiqi Ai", "Anqi Liu", "Yilin Gao", "Shugong Xu"], "title": "StereoDETR: Stereo-based Transformer for 3D Object Detection", "comment": "Accepted by IEEE TCSVT, 2025", "summary": "Compared to monocular 3D object detection, stereo-based 3D methods offer significantly higher accuracy but still suffer from high computational overhead and latency. The state-of-the-art stereo 3D detection method achieves twice the accuracy of monocular approaches, yet its inference speed is only half as fast. In this paper, we propose StereoDETR, an efficient stereo 3D object detection framework based on DETR. StereoDETR consists of two branches: a monocular DETR branch and a stereo branch. The DETR branch is built upon 2D DETR with additional channels for predicting object scale, orientation, and sampling points. The stereo branch leverages low-cost multi-scale disparity features to predict object-level depth maps. These two branches are coupled solely through a differentiable depth sampling strategy. To handle occlusion, we introduce a constrained supervision strategy for sampling points without requiring extra annotations. StereoDETR achieves real-time inference and is the first stereo-based method to surpass monocular approaches in speed. It also achieves competitive accuracy on the public KITTI benchmark, setting new state-of-the-art results on pedestrian and cyclist subsets. The code is available at https://github.com/shiyi-mu/StereoDETR-OPEN.", "AI": {"tldr": "StereoDETR是一个基于DETR的高效立体3D目标检测框架，它结合了单目DETR和立体视差分支，通过可微分深度采样策略实现实时推理，并在KITTI基准测试中达到了领先的速度和有竞争力的精度。", "motivation": "虽然立体3D目标检测比单目方法精度更高，但其计算开销和延迟也更大，推理速度仅为单目方法的一半。研究旨在开发一种既能保持高精度又能实现实时推理的立体3D检测方法。", "method": "本文提出了StereoDETR，包含两个分支：一个基于2D DETR的单目DETR分支（额外通道预测物体尺度、方向和采样点）和一个利用低成本多尺度视差特征预测物体级深度图的立体分支。两个分支通过可微分深度采样策略耦合。为处理遮挡，引入了无需额外标注的约束监督策略用于采样点。", "result": "StereoDETR实现了实时推理，是首个在速度上超越单目方法的立体3D检测方法。它在公共KITTI基准测试中也取得了有竞争力的精度，并在行人和骑行者子集上刷新了最先进的结果。", "conclusion": "StereoDETR成功克服了立体3D目标检测在速度上的局限性，首次实现了比单目方法更快的立体检测，同时保持了高精度，为高效的立体3D目标检测提供了一个有前景的解决方案。"}}
{"id": "2511.18775", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18775", "abs": "https://arxiv.org/abs/2511.18775", "authors": ["Kihyun Na", "Jinyoung Choi", "Injung Kim"], "title": "Rethinking Garment Conditioning in Diffusion-based Virtual Try-On", "comment": "15 pages (including references and supplementary material), 10 figures, 7 tables. Code and pretrained models will be released", "summary": "Virtual Try-On (VTON) is the task of synthesizing an image of a person wearing a target garment, conditioned on a person image and a garment image. While diffusion-based VTON models featuring a Dual UNet architecture demonstrate superior fidelity compared to single UNet models, they incur substantial computational and memory overhead due to their heavy structure. In this study, through visualization analysis and theoretical analysis, we derived three hypotheses regarding the learning of context features to condition the denoising process. Based on these hypotheses, we developed Re-CatVTON, an efficient single UNet model that achieves high performance. We further enhance the model by introducing a modified classifier-free guidance strategy tailored for VTON's spatial concatenation conditioning, and by directly injecting the ground-truth garment latent derived from the clean garment latent to prevent the accumulation of prediction error. The proposed Re-CatVTON significantly improves performance compared to its predecessor (CatVTON) and requires less computation and memory than the high-performance Dual UNet model, Leffa. Our results demonstrate improved FID, KID, and LPIPS scores, with only a marginal decrease in SSIM, establishing a new efficiency-performance trade-off for single UNet VTON models.", "AI": {"tldr": "本文提出了一种名为Re-CatVTON的高效单UNet虚拟试穿（VTON）模型。通过可视化和理论分析，该模型在保持高保真度的同时，显著降低了计算和内存开销，并引入了改进的指导策略和潜变量注入方法。", "motivation": "虽然基于扩散的双UNet VTON模型在图像保真度方面表现出色，但其结构复杂，导致巨大的计算和内存开销。研究动机是开发一种高效的单UNet模型，以在降低资源消耗的同时实现高性能。", "method": "研究人员通过可视化和理论分析，推导了关于上下文特征学习的三个假设。基于这些假设，他们开发了Re-CatVTON，一个高效的单UNet模型。此外，模型通过引入针对VTON空间拼接条件定制的改进分类器无关指导策略，以及直接注入来自干净服装潜变量的真实服装潜变量来防止预测误差累积，从而得到进一步增强。", "result": "Re-CatVTON模型与其前身（CatVTON）相比，性能显著提升，并且比高性能双UNet模型（如Leffa）需要更少的计算和内存。实验结果表明，FID、KID和LPIPS分数有所改善，而SSIM仅略有下降，为单UNet VTON模型建立了新的效率-性能权衡。", "conclusion": "Re-CatVTON是一个高效且高性能的单UNet VTON模型，它在计算效率和图像质量之间取得了新的平衡。通过创新的结构和增强策略，Re-CatVTON在降低资源消耗的同时，提供了卓越的虚拟试穿体验。"}}
{"id": "2511.18765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18765", "abs": "https://arxiv.org/abs/2511.18765", "authors": ["Hui Shan", "Ming Li", "Haitao Yang", "Kai Zheng", "Sizhe Zheng", "Yanwei Fu", "Xiangru Huang"], "title": "NI-Tex: Non-isometric Image-based Garment Texture Generation", "comment": null, "summary": "Existing industrial 3D garment meshes already cover most real-world clothing geometries, yet their texture diversity remains limited. To acquire more realistic textures, generative methods are often used to extract Physically-based Rendering (PBR) textures and materials from large collections of wild images and project them back onto garment meshes. However, most image-conditioned texture generation approaches require strict topological consistency between the input image and the input 3D mesh, or rely on accurate mesh deformation to match to the image poses, which significantly constrains the texture generation quality and flexibility. To address the challenging problem of non-isometric image-based garment texture generation, we construct 3D Garment Videos, a physically simulated, garment-centric dataset that provides consistent geometry and material supervision across diverse deformations, enabling robust cross-pose texture learning. We further employ Nano Banana for high-quality non-isometric image editing, achieving reliable cross-topology texture generation between non-isometric image-geometry pairs. Finally, we propose an iterative baking method via uncertainty-guided view selection and reweighting that fuses multi-view predictions into seamless, production-ready PBR textures. Through extensive experiments, we demonstrate that our feedforward dual-branch architecture generates versatile and spatially aligned PBR materials suitable for industry-level 3D garment design.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18766", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18766", "abs": "https://arxiv.org/abs/2511.18766", "authors": ["Xintao Chen", "Xiaohao Xu", "Bozhong Zheng", "Yun Liu", "Yingna Wu"], "title": "Unsupervised Multi-View Visual Anomaly Detection via Progressive Homography-Guided Alignment", "comment": null, "summary": "Unsupervised visual anomaly detection from multi-view images presents a significant challenge: distinguishing genuine defects from benign appearance variations caused by viewpoint changes. Existing methods, often designed for single-view inputs, treat multiple views as a disconnected set of images, leading to inconsistent feature representations and a high false-positive rate. To address this, we introduce ViewSense-AD (VSAD), a novel framework that learns viewpoint-invariant representations by explicitly modeling geometric consistency across views. At its core is our Multi-View Alignment Module (MVAM), which leverages homography to project and align corresponding feature regions between neighboring views. We integrate MVAM into a View-Align Latent Diffusion Model (VALDM), enabling progressive and multi-stage alignment during the denoising process. This allows the model to build a coherent and holistic understanding of the object's surface from coarse to fine scales. Furthermore, a lightweight Fusion Refiner Module (FRM) enhances the global consistency of the aligned features, suppressing noise and improving discriminative power. Anomaly detection is performed by comparing multi-level features from the diffusion model against a learned memory bank of normal prototypes. Extensive experiments on the challenging RealIAD and MANTA datasets demonstrate that VSAD sets a new state-of-the-art, significantly outperforming existing methods in pixel, view, and sample-level visual anomaly proving its robustness to large viewpoint shifts and complex textures.", "AI": {"tldr": "本文提出ViewSense-AD (VSAD) 框架，通过显式建模视角间的几何一致性，学习视角不变的特征表示，从而解决多视角图像无监督视觉异常检测中区分真实缺陷与视角变化带来的外观差异的挑战。", "motivation": "现有多视角异常检测方法将多视角图像视为不连贯的集合，导致特征表示不一致和高误报率，难以有效区分真实缺陷与视角变化引起的外观差异。", "method": "VSAD框架的核心是多视角对齐模块 (MVAM)，它利用单应性矩阵对相邻视角间的对应特征区域进行投影和对齐。MVAM被整合到一个视角对齐潜在扩散模型 (VALDM) 中，在去噪过程中实现渐进式多阶段对齐。此外，一个轻量级融合细化模块 (FRM) 用于增强对齐特征的全局一致性。异常检测通过比较扩散模型的多级特征与学习到的正常原型记忆库来完成。", "result": "在RealIAD和MANTA数据集上的广泛实验表明，VSAD在像素、视角和样本级别的视觉异常检测方面均达到了新的最先进水平，显著优于现有方法，并证明了其对大视角变化和复杂纹理的鲁棒性。", "conclusion": "VSAD通过学习视角不变的表示并建模多视角间的几何一致性，有效解决了多视角无监督视觉异常检测的难题，并在挑战性数据集上表现出卓越的性能和鲁棒性。"}}
{"id": "2511.18786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18786", "abs": "https://arxiv.org/abs/2511.18786", "authors": ["Junyang Chen", "Jiangxin Dong", "Long Sun", "Yixin Yang", "Jinshan Pan"], "title": "STCDiT: Spatio-Temporally Consistent Diffusion Transformer for High-Quality Video Super-Resolution", "comment": "Project page: https://jychen9811.github.io/STCDiT_page", "summary": "We present STCDiT, a video super-resolution framework built upon a pre-trained video diffusion model, aiming to restore structurally faithful and temporally stable videos from degraded inputs, even under complex camera motions. The main challenges lie in maintaining temporal stability during reconstruction and preserving structural fidelity during generation. To address these challenges, we first develop a motion-aware VAE reconstruction method that performs segment-wise reconstruction, with each segment clip exhibiting uniform motion characteristic, thereby effectively handling videos with complex camera motions. Moreover, we observe that the first-frame latent extracted by the VAE encoder in each clip, termed the anchor-frame latent, remains unaffected by temporal compression and retains richer spatial structural information than subsequent frame latents. We further develop an anchor-frame guidance approach that leverages structural information from anchor frames to constrain the generation process and improve structural fidelity of video features. Coupling these two designs enables the video diffusion model to achieve high-quality video super-resolution. Extensive experiments show that STCDiT outperforms state-of-the-art methods in terms of structural fidelity and temporal consistency.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18814", "abs": "https://arxiv.org/abs/2511.18814", "authors": ["Jiawei Hou", "Shenghao Zhang", "Can Wang", "Zheng Gu", "Yonggen Ling", "Taiping Zeng", "Xiangyang Xue", "Jingbo Zhang"], "title": "DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video", "comment": null, "summary": "Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.", "AI": {"tldr": "该论文引入了DA4D，一个大规模4D检测数据集，并提出了DetAny4D，一个端到端的开放集4D目标检测框架，用于从序列输入中直接预测3D边界框，显著提高了检测精度和时间稳定性。", "motivation": "现有的开放集4D目标检测方法通常是逐帧预测，缺乏时间一致性建模，或者依赖复杂的多阶段管道，容易导致错误传播。此外，缺乏捕获连续可靠3D边界框标注的大规模数据集也阻碍了该领域的发展。", "method": "1. 引入DA4D数据集：一个包含28万多序列的大规模4D检测数据集，具有高质量的边界框标注。 2. 提出DetAny4D框架：一个端到端的开放集框架，直接从序列输入预测3D边界框。 3. 特征融合：融合来自预训练基础模型的多模态特征。 4. 几何感知时空解码器：设计一个几何感知的时空解码器来有效捕获空间和时间动态。 5. 多任务学习：采用多任务学习架构，并结合专用训练策略，以保持不同长度序列的全局一致性。", "result": "DetAny4D在广泛的实验中实现了具有竞争力的检测精度，并显著提高了时间稳定性，有效解决了4D目标检测中长期存在的抖动和不一致问题。", "conclusion": "该研究通过引入大规模数据集DA4D和提出端到端框架DetAny4D，成功克服了现有4D目标检测方法的局限性，显著提升了检测性能和时间稳定性，为4D目标检测领域带来了实质性进展。"}}
{"id": "2511.18787", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18787", "abs": "https://arxiv.org/abs/2511.18787", "authors": ["Bhuvan Sachdeva", "Karan Uppal", "Abhinav Java", "Vineeth N. Balasubramanian"], "title": "Understanding Task Transfer in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) perform well on multimodal benchmarks but lag behind humans and specialized models on visual perception tasks like depth estimation or object counting. Finetuning on one task can unpredictably affect performance on others, making task-specific finetuning challenging. In this paper, we address this challenge through a systematic study of task transferability. We examine how finetuning a VLM on one perception task affects its zero-shot performance on others. To quantify these effects, we introduce Perfection Gap Factor (PGF), a metric that captures both the breadth and magnitude of transfer. Using three open-weight VLMs evaluated across 13 perception tasks, we construct a task-transfer graph that reveals previously unobserved relationships among perception tasks. Our analysis uncovers patterns of positive and negative transfer, identifies groups of tasks that mutually influence each other, organizes tasks into personas based on their transfer behavior and demonstrates how PGF can guide data selection for more efficient training. These findings highlight both opportunities for positive transfer and risks of negative interference, offering actionable guidance for advancing VLMs.", "AI": {"tldr": "本文系统研究了视觉-语言模型（VLMs）在不同感知任务上进行微调时的任务迁移性。通过引入“完美差距因子”（PGF）并构建任务迁移图，揭示了感知任务间的相互影响模式，并为更高效的VLM训练提供了指导。", "motivation": "视觉-语言模型（VLMs）在多模态基准上表现出色，但在深度估计或物体计数等视觉感知任务上仍落后于人类和专用模型。此外，对VLM进行任务特定微调时，一个任务上的微调可能不可预测地影响其他任务的性能，这使得微调过程充满挑战。", "method": "研究通过系统性地研究任务可迁移性来解决挑战。具体方法包括：检查在一个感知任务上微调VLM如何影响其在其他任务上的零样本性能；引入“完美差距因子”（PGF）这一新度量来量化迁移的广度和幅度；使用三个开源VLM，在13个感知任务上进行评估；构建一个任务迁移图以揭示任务间的关系。", "result": "分析揭示了感知任务之间以前未被观察到的关系。发现了正向和负向迁移的模式。识别出相互影响的任务组。根据迁移行为将任务组织成不同的“角色”（personas）。展示了PGF如何指导数据选择，从而实现更高效的训练。", "conclusion": "研究结果突出了正向迁移的机遇和负向干扰的风险。这些发现为推进视觉-语言模型（VLMs）提供了可操作的指导。"}}
{"id": "2511.18792", "categories": ["cs.CV", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18792", "abs": "https://arxiv.org/abs/2511.18792", "authors": ["Cheng Jiang", "Yihe Yan", "Yanxiang Wang", "Chun Tung Chou", "Wen Hu"], "title": "Scale What Counts, Mask What Matters: Evaluating Foundation Models for Zero-Shot Cross-Domain Wi-Fi Sensing", "comment": null, "summary": "While Wi-Fi sensing offers a compelling, privacy-preserving alternative to cameras, its practical utility has been fundamentally undermined by a lack of robustness across domains. Models trained in one setup fail to generalize to new environments, hardware, or users, a critical \"domain shift\" problem exacerbated by modest, fragmented public datasets. We shift from this limited paradigm and apply a foundation model approach, leveraging Masked Autoencoding (MAE) style pretraining on the largest and most heterogeneous Wi-Fi CSI datasets collection assembled to date. Our study pretrains and evaluates models on over 1.3 million samples extracted from 14 datasets, collected using 4 distinct devices across the 2.4/5/6 GHz bands and bandwidths from 20 to 160 MHz. Our large-scale evaluation is the first to systematically disentangle the impacts of data diversity versus model capacity on cross-domain performance. The results establish scaling trends on Wi-Fi CSI sensing. First, our experiments show log-linear improvements in unseen domain performance as the amount of pretraining data increases, suggesting that data scale and diversity are key to domain generalization. Second, based on the current data volume, larger model can only provide marginal gains for cross-domain performance, indicating that data, rather than model capacity, is the current bottleneck for Wi-Fi sensing generalization. Finally, we conduct a series of cross-domain evaluations on human activity recognition, human gesture recognition and user identification tasks. The results show that the large-scale pretraining improves cross-domain accuracy ranging from 2.2% to 15.7%, compared to the supervised learning baseline. Overall, our findings provide insightful direction for designing future Wi-Fi sensing systems that can eventually be robust enough for real-world deployment.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18822", "abs": "https://arxiv.org/abs/2511.18822", "authors": ["Zhennan Chen", "Junwei Zhu", "Xu Chen", "Jiangning Zhang", "Xiaobin Hu", "Hanzhen Zhao", "Chengjie Wang", "Jian Yang", "Ying Tai"], "title": "DiP: Taming Diffusion Models in Pixel Space", "comment": null, "summary": "Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\\times$256.", "AI": {"tldr": "扩散模型面临生成质量与计算效率的权衡。DiP提出一种高效的像素空间扩散框架，通过解耦全局结构和局部细节生成，实现了与LDM相当的效率和高生成质量，且无需依赖VAE。", "motivation": "扩散模型在生成质量和计算效率之间存在根本性权衡。现有潜在扩散模型（LDMs）效率高但可能丢失信息且非端到端训练；现有像素空间模型无需VAE但计算成本高昂，不适用于高分辨率合成。因此，需要解决这一困境。", "method": "DiP（高效像素空间扩散框架）将生成过程解耦为全局和局部两个阶段：一个Diffusion Transformer (DiT) 主干网络在大块图像上操作以构建高效的全局结构；一个协同训练的轻量级Patch Detailer Head利用上下文特征恢复细粒度的局部细节。这种协同设计实现了与LDM相当的计算效率，且无需依赖VAE。", "result": "DiP实现了与LDM相当的计算效率，推理速度比现有方法快10倍，总参数量仅增加0.3%，并在ImageNet 256x256上取得了1.90的FID分数。", "conclusion": "DiP通过解耦全局和局部生成，成功解决了像素空间扩散模型在生成质量和计算效率之间的困境，在不依赖VAE的情况下，提供了快速推理和高质量的图像合成能力。"}}
{"id": "2511.18817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18817", "abs": "https://arxiv.org/abs/2511.18817", "authors": ["Siyuan Wei", "Chunjie Wang", "Xiao Liu", "Xiaosheng Yan", "Zhishan Zhou", "Rui Huang"], "title": "Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring", "comment": "8 pages", "summary": "3D Multi-modal Large Language Models (MLLMs) still lag behind their 2D peers, largely because large-scale, high-quality 3D scene-dialogue datasets remain scarce. Prior efforts hinge on expensive human annotation and leave two key ambiguities unresolved: viewpoint ambiguity, where spatial language presumes unknown camera poses, and object referring ambiguity, where non-exclusive descriptions blur the line between targets and distractors. We therefore present a fully automated pipeline that converts raw 3D scans into unambiguous, high-quality dialogue data at a fraction of the previous cost. By synergizing rule-based constraints with 2D MLLMs and LLMs, the pipeline enables controllable, scalable generation without human intervention. The pipeline comprises four stages: (1) meta-annotation collection harvesting object-, frame-, and scene-level captions, (2) scene graph construction with relation correction to capture proximal object relations, (3) discriminative object referring that generates exclusive and compact descriptions, and (4) multi-task data generation synthesizing diverse dialogues. Our pipeline systematically mitigates inherent flaws in source datasets and produces the final Disc3D dataset, over 2 million samples in 25K hybrid 3D scenes, spanning scene, view, and object captioning, visual grounding, and five object-centric QA tasks. Extensive experiments demonstrate that training with Disc3D yields consistent, significant improvements on both public benchmarks and our multifaceted Disc3D-QA tasks. Code, data, and models will be publicly available.", "AI": {"tldr": "本文提出了一种全自动管道，通过结合规则约束、2D多模态大语言模型（MLLMs）和大语言模型（LLMs），解决了3D场景对话数据稀缺和现有数据中视角及物体指代模糊问题，生成了大规模、高质量的Disc3D数据集，显著提升了3D MLLMs的性能。", "motivation": "3D多模态大语言模型（MLLMs）因缺乏大规模、高质量的3D场景对话数据集而落后于2D同行。现有方法成本高昂，且存在视角模糊（空间语言假定未知相机姿态）和物体指代模糊（非排他性描述）两大关键问题。", "method": "开发了一个全自动管道，将原始3D扫描转换为高质量、无歧义的对话数据。该管道结合了基于规则的约束、2D MLLMs和LLMs，实现了可控、可扩展的生成，无需人工干预。它包括四个阶段：1) 元标注收集（物体、帧、场景级别标注），2) 场景图构建与关系校正，3) 判别性物体指代（生成排他性、简洁的描述），4) 多任务数据生成（合成多样化对话）。", "result": "该管道系统地解决了源数据集的固有缺陷，生成了最终的Disc3D数据集，包含超过200万个样本，涵盖2.5万个混合3D场景，任务类型包括场景、视图、物体描述，视觉定位以及五个以物体为中心的问答任务。大量实验表明，使用Disc3D进行训练在公共基准和我们多方面的Disc3D-QA任务上均取得了持续、显著的改进。", "conclusion": "所提出的全自动数据生成管道成功解决了3D MLLMs数据稀缺和数据质量问题，通过生成大规模、无歧义的Disc3D数据集，显著提升了3D MLLMs在多项任务上的表现，为3D视觉语言理解领域的发展提供了重要支持。"}}
{"id": "2511.18806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18806", "abs": "https://arxiv.org/abs/2511.18806", "authors": ["Qinglei Cao", "Ziyao Tang", "Xiaoqin Tang"], "title": "TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging", "comment": "Please consider this version as the latest camera-ready version", "summary": "X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18811", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18811", "abs": "https://arxiv.org/abs/2511.18811", "authors": ["Yuqiu Jiang", "Xiaozhen Qiao", "Tianyu Mei", "Haojian Huang", "Yifan Chen", "Ye Zheng", "Zhe Sun"], "title": "Mitigating Long-Tail Bias in HOI Detection via Adaptive Diversity Cache", "comment": null, "summary": "Human-Object Interaction (HOI) detection is a fundamental task in computer vision, empowering machines to comprehend human-object relationships in diverse real-world scenarios. Recent advances in VLMs have significantly improved HOI detection by leveraging rich cross-modal representations. However, most existing VLM-based approaches rely heavily on additional training or prompt tuning, resulting in substantial computational overhead and limited scalability, particularly in long-tailed scenarios where rare interactions are severely underrepresented. In this paper, we propose the Adaptive Diversity Cache (ADC) module, a novel training-free and plug-and-play mechanism designed to mitigate long-tail bias in HOI detection. ADC constructs class-specific caches that accumulate high-confidence and diverse feature representations during inference. The method incorporates frequency-aware cache adaptation that favors rare categories and is designed to enable robust prediction calibration without requiring additional training or fine-tuning. Extensive experiments on HICO-DET and V-COCO datasets show that ADC consistently improves existing HOI detectors, achieving up to +8.57\\% mAP gain on rare categories and +4.39\\% on the full dataset, demonstrating its effectiveness in mitigating long-tail bias while preserving overall performance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18825", "abs": "https://arxiv.org/abs/2511.18825", "authors": ["Xiele Wu", "Zicheng Zhang", "Mingtao Chen", "Yixian Liu", "Yiming Liu", "Shushi Wang", "Zhichao Hu", "Yuhong Liu", "Guangtao Zhai", "Xiaohong Liu"], "title": "Q-Save: Towards Scoring and Attribution for Generated Video Evaluation", "comment": "20 pages, 11 figures", "summary": "We present Q-Save, a new benchmark dataset and model for holistic and explainable evaluation of AI-generated video (AIGV) quality. The dataset contains near 10000 videos, each annotated with a scalar mean opinion score (MOS) and fine-grained attribution labels along three core dimensions: visual quality, dynamic quality, and text-video alignment. These multi-aspect annotations enable both accurate quality assessment and interpretable reasoning behind the scores. To leverage this data, we propose a unified evaluation model that jointly performs quality scoring and attribution-based explanation. The model adopts the SlowFast framework to distinguish between fast frames and slow frames - slow frames are processed with high resolution while fast frames use low resolution, balancing evaluation accuracy and computational efficiency. For training, we use data formatted in Chain-of-Thought (COT) style and employ a multi-stage strategy: we first conduct Supervised Fine-Tuning (SFT), then further enhance the model with Grouped Relative Policy Optimization (GRPO), and finally perform SFT again to improve model stability. Experimental results demonstrate that our model achieves state-of-the-art performance in video quality prediction while also providing human-aligned, interpretable justifications. Our dataset and model establish a strong foundation for explainable evaluation in generative video research, contributing to the development of multimodal generation and trustworthy AI. Code and dataset will be released upon publication.", "AI": {"tldr": "Q-Save是一个新的基准数据集和模型，用于对AI生成视频(AIGV)进行全面且可解释的质量评估。它包含近万个视频，并提供多维度标注，其模型在视频质量预测和解释性方面达到了SOTA。", "motivation": "现有AIGV评估方法缺乏全面性和可解释性，无法提供细粒度的归因解释。", "method": "提出了Q-Save数据集，包含近万个视频，每个视频都标注了平均意见得分(MOS)和视觉质量、动态质量、文本-视频对齐三个核心维度的细粒度归因标签。在此基础上，提出了一个统一的评估模型，采用SlowFast框架处理视频帧以平衡准确性和效率。模型训练采用多阶段策略：首先进行监督微调(SFT)，然后通过分组相对策略优化(GRPO)增强，最后再次进行SFT以提高模型稳定性，训练数据采用思维链(COT)格式。", "result": "实验结果表明，该模型在视频质量预测方面达到了最先进的性能，同时能提供与人类对齐且可解释的理由。", "conclusion": "Q-Save数据集和模型为生成式视频研究中的可解释评估奠定了坚实基础，有助于多模态生成和可信AI的发展。"}}
{"id": "2511.18838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18838", "abs": "https://arxiv.org/abs/2511.18838", "authors": ["Xiaofan Li", "Chenming Wu", "Yanpeng Sun", "Jiaming Zhou", "Delin Qu", "Yansong Qu", "Weihao Bo", "Haibao Yu", "Dingkang Liang"], "title": "FVAR: Visual Autoregressive Modeling via Next Focus Prediction", "comment": "10 pages, 4 figures", "summary": "Visual autoregressive models achieve remarkable generation quality through next-scale predictions across multi-scale token pyramids. However, the conventional method uses uniform scale downsampling to build these pyramids, leading to aliasing artifacts that compromise fine details and introduce unwanted jaggies and moiré patterns. To tackle this issue, we present \\textbf{FVAR}, which reframes the paradigm from \\emph{next-scale prediction} to \\emph{next-focus prediction}, mimicking the natural process of camera focusing from blur to clarity. Our approach introduces three key innovations: \\textbf{1) Next-Focus Prediction Paradigm} that transforms multi-scale autoregression by progressively reducing blur rather than simply downsampling; \\textbf{2) Progressive Refocusing Pyramid Construction} that uses physics-consistent defocus kernels to build clean, alias-free multi-scale representations; and \\textbf{3) High-Frequency Residual Learning} that employs a specialized residual teacher network to effectively incorporate alias information during training while maintaining deployment simplicity. Specifically, we construct optical low-pass views using defocus point spread function (PSF) kernels with decreasing radius, creating smooth blur-to-clarity transitions that eliminate aliasing at its source. To further enhance detail generation, we introduce a High-Frequency Residual Teacher that learns from both clean structure and alias residuals, distilling this knowledge to a vanilla VAR deployment network for seamless inference. Extensive experiments on ImageNet demonstrate that FVAR substantially reduces aliasing artifacts, improves fine detail preservation, and enhances text readability, achieving superior performance with perfect compatibility to existing VAR frameworks.", "AI": {"tldr": "FVAR通过引入“下一焦点预测”范式，利用物理一致的散焦核构建渐进重聚焦金字塔，并结合高频残差学习，有效解决了视觉自回归模型中由传统下采样引起的混叠伪影，显著提升了细节保留和生成质量。", "motivation": "传统的视觉自回归模型采用统一的尺度下采样来构建多尺度金字塔，这会导致混叠伪影，损害精细细节，并引入锯齿和莫尔条纹等不良效应。", "method": "FVAR提出了三项创新：1) **下一焦点预测范式**，将多尺度自回归转换为逐步减少模糊而非简单下采样；2) **渐进重聚焦金字塔构建**，使用物理一致的散焦点扩散函数（PSF）核来构建干净、无混叠的多尺度表示，实现从模糊到清晰的平滑过渡；3) **高频残差学习**，引入一个专门的残差教师网络，在训练期间有效地整合混叠信息，并将其知识蒸馏到部署的VAR网络中。", "result": "FVAR在ImageNet上的实验表明，它显著减少了混叠伪影，改善了精细细节的保留，并增强了文本的可读性。它在与现有VAR框架完全兼容的情况下，取得了卓越的性能。", "conclusion": "FVAR成功地通过改变多尺度预测范式，从源头上消除了视觉自回归模型中的混叠问题，提升了生成质量和细节表现，并能与现有框架无缝集成。"}}
{"id": "2511.18851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18851", "abs": "https://arxiv.org/abs/2511.18851", "authors": ["Yilin Wen", "Kechuan Dong", "Yusuke Sugano"], "title": "Robust Long-term Test-Time Adaptation for 3D Human Pose Estimation through Motion Discretization", "comment": "Accepted by AAAI 2026, main track", "summary": "Online test-time adaptation addresses the train-test domain gap by adapting the model on unlabeled streaming test inputs before making the final prediction. However, online adaptation for 3D human pose estimation suffers from error accumulation when relying on self-supervision with imperfect predictions, leading to degraded performance over time. To mitigate this fundamental challenge, we propose a novel solution that highlights the use of motion discretization. Specifically, we employ unsupervised clustering in the latent motion representation space to derive a set of anchor motions, whose regularity aids in supervising the human pose estimator and enables efficient self-replay. Additionally, we introduce an effective and efficient soft-reset mechanism by reverting the pose estimator to its exponential moving average during continuous adaptation. We examine long-term online adaptation by continuously adapting to out-of-domain streaming test videos of the same individual, which allows for the capture of consistent personal shape and motion traits throughout the streaming observation. By mitigating error accumulation, our solution enables robust exploitation of these personal traits for enhanced accuracy. Experiments demonstrate that our solution outperforms previous online test-time adaptation methods and validate our design choices.", "AI": {"tldr": "针对3D人体姿态估计的在线测试时自适应方法，由于自我监督中的不完美预测导致误差累积。本文提出通过运动离散化（利用无监督聚类生成锚点运动）和软重置机制来缓解误差累积，从而更鲁棒地利用个体特征以提高精度。", "motivation": "3D人体姿态估计的在线测试时自适应方法在依赖不完美预测进行自我监督时，会遭受误差累积问题，导致性能随时间下降。", "method": "本文提出一种新颖的解决方案，核心是运动离散化。具体来说，它在潜在运动表示空间中采用无监督聚类来导出锚点运动，这些锚点运动的规律性有助于监督人体姿态估计器并实现高效的自我回放。此外，引入了一种有效且高效的软重置机制，通过在持续自适应过程中将姿态估计器恢复到其指数移动平均状态。该方法通过对同一人的域外流媒体测试视频进行长期在线自适应，以捕获一致的个人形状和运动特征。", "result": "实验表明，该解决方案优于以前的在线测试时自适应方法，并验证了其设计选择。通过缓解误差累积，该解决方案能够鲁棒地利用这些个人特征来提高准确性。", "conclusion": "所提出的解决方案通过运动离散化和软重置机制，有效缓解了3D人体姿态估计在线测试时自适应中的误差累积问题，从而能够鲁棒地利用个体特征并提高准确性，超越了现有方法。"}}
{"id": "2511.18816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18816", "abs": "https://arxiv.org/abs/2511.18816", "authors": ["Nimeshika Udayangani", "Sarah Erfani", "Christopher Leckie"], "title": "SupLID: Geometrical Guidance for Out-of-Distribution Detection in Semantic Segmentation", "comment": "10 pages, CIKM 2025", "summary": "Out-of-Distribution (OOD) detection in semantic segmentation aims to localize anomalous regions at the pixel level, advancing beyond traditional image-level OOD techniques to better suit real-world applications such as autonomous driving. Recent literature has successfully explored the adaptation of commonly used image-level OOD methods--primarily based on classifier-derived confidence scores (e.g., energy or entropy)--for this pixel-precise task. However, these methods inherit a set of limitations, including vulnerability to overconfidence. In this work, we introduce SupLID, a novel framework that effectively guides classifier-derived OOD scores by exploiting the geometrical structure of the underlying semantic space, particularly using Linear Intrinsic Dimensionality (LID). While LID effectively characterizes the local structure of high-dimensional data by analyzing distance distributions, its direct application at the pixel level remains challenging. To overcome this, SupLID constructs a geometrical coreset that captures the intrinsic structure of the in-distribution (ID) subspace. It then computes OOD scores at the superpixel level, enabling both efficient real-time inference and improved spatial smoothness. We demonstrate that geometrical cues derived from SupLID serve as a complementary signal to traditional classifier confidence, enhancing the model's ability to detect diverse OOD scenarios. Designed as a post-hoc scoring method, SupLID can be seamlessly integrated with any semantic segmentation classifier at deployment time. Our results demonstrate that SupLID significantly enhances existing classifier-based OOD scores, achieving state-of-the-art performance across key evaluation metrics, including AUR, FPR, and AUP. Code is available at https://github.com/hdnugit/SupLID.", "AI": {"tldr": "SupLID是一种新颖的像素级OOD检测框架，通过利用语义空间的几何结构（LID）并结合超像素来指导分类器置信度分数，显著提升了语义分割中的OOD检测性能。", "motivation": "语义分割中的像素级OOD检测对于自动驾驶等实际应用至关重要，但传统的图像级OOD方法不适用。现有像素级OOD方法（源自图像级方法）继承了过置信等局限性。", "method": "本文提出了SupLID框架，它通过利用语义空间的几何结构，特别是线性内在维度（LID），来引导分类器衍生的OOD分数。为克服LID在像素级直接应用的挑战，SupLID构建了一个几何核心集来捕捉ID子空间的内在结构，并在超像素级别计算OOD分数，以实现高效实时推理和改善空间平滑性。SupLID作为一种后处理评分方法，可无缝集成到任何语义分割分类器中。", "result": "SupLID的几何线索与传统分类器置信度互补，增强了模型检测多样OOD场景的能力。实验结果表明，SupLID显著提升了现有基于分类器的OOD分数，并在AUR、FPR和AUP等关键评估指标上达到了最先进的性能。", "conclusion": "SupLID通过结合几何结构信息（LID）和超像素处理，有效地解决了语义分割中像素级OOD检测的挑战，提供了一种互补且高性能的OOD评分方法，能够无缝集成并显著提升现有模型的检测能力。"}}
{"id": "2511.18823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18823", "abs": "https://arxiv.org/abs/2511.18823", "authors": ["Fufangchen Zhao", "Liao Zhang", "Daiqi Shi", "Yuanjun Gao", "Chen Ye", "Yang Cai", "Jian Gao", "Danfeng Yan"], "title": "VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models", "comment": null, "summary": "We propose VideoPerceiver, a novel video multimodal large language model (VMLLM) that enhances fine-grained perception in video understanding, addressing VMLLMs' limited ability to reason about brief actions in short clips or rare transient events in long videos. VideoPerceiver adopts a two-stage training framework. During supervised fine-tuning (SFT), we construct \"key-information-missing\" videos by extracting event-action keywords from captions, identifying corresponding key frames, and replacing them with adjacent frames. We jointly encode original and modified video tokens with text tokens, aligning intermediate visual representations with keywords via an auxiliary contrastive loss to enhance sensitivity to fine-grained motion cues. In reinforcement learning (RL), both video variants are fed into the model to generate descriptions, and a novel relative reward ensures responses from complete videos outperform those from degraded inputs, explicitly training the model to recover temporally precise action details. We also curate a dataset of 80,000 videos with fine-grained actions and transient events. Experiments show VideoPerceiver substantially outperforms state-of-the-art VMLLMs on fine-grained action understanding and rare event captioning benchmarks, while maintaining strong performance on standard tasks. By prioritizing task-relevant visual features, our work redefines video-language model training for fine-grained perception.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18834", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18834", "abs": "https://arxiv.org/abs/2511.18834", "authors": ["Lei Ke", "Hubery Yin", "Gongye Liu", "Zhengyao Lv", "Jingcai Guo", "Chen Li", "Wenhan Luo", "Yujiu Yang", "Jing Lyu"], "title": "FlowSteer: Guiding Few-Step Image Synthesis with Authentic Trajectories", "comment": "Few-Step Image Synthesis", "summary": "With the success of flow matching in visual generation, sampling efficiency remains a critical bottleneck for its practical application. Among flow models' accelerating methods, ReFlow has been somehow overlooked although it has theoretical consistency with flow matching. This is primarily due to its suboptimal performance in practical scenarios compared to consistency distillation and score distillation. In this work, we investigate this issue within the ReFlow framework and propose FlowSteer, a method unlocks the potential of ReFlow-based distillation by guiding the student along teacher's authentic generation trajectories. We first identify that Piecewised ReFlow's performance is hampered by a critical distribution mismatch during the training and propose Online Trajectory Alignment(OTA) to resolve it. Then, we introduce a adversarial distillation objective applied directly on the ODE trajectory, improving the student's adherence to the teacher's generation trajectory. Furthermore, we find and fix a previously undiscovered flaw in the widely-used FlowMatchEulerDiscreteScheduler that largely degrades few-step inference quality. Our experiment result on SD3 demonstrates our method's efficacy.", "AI": {"tldr": "FlowSteer通过解决ReFlow训练中的分布不匹配问题、引入对抗性蒸馏以及修复调度器缺陷，显著提升了ReFlow在视觉生成中的采样效率和性能。", "motivation": "流匹配（flow matching）在视觉生成中取得成功，但采样效率仍是实际应用的瓶颈。ReFlow虽然与流匹配具有理论一致性，但在实际应用中性能不如一致性蒸馏和分数蒸馏，因此被忽视。", "method": "本研究在ReFlow框架内调查了其性能问题，并提出了FlowSteer。具体方法包括：1) 识别并解决分段式ReFlow训练中关键的分布不匹配问题，提出了在线轨迹对齐（Online Trajectory Alignment, OTA）。2) 引入直接作用于ODE轨迹的对抗性蒸馏目标，以提高学生模型对教师模型生成轨迹的依从性。3) 发现并修复了广泛使用的FlowMatchEulerDiscreteScheduler中一个先前未被发现的缺陷，该缺陷会严重降低少步推理质量。", "result": "在SD3上的实验结果表明，所提出的方法（FlowSteer）有效提升了性能。", "conclusion": "FlowSteer通过指导学生模型沿着教师模型的真实生成轨迹进行学习，成功释放了基于ReFlow蒸馏的潜力，解决了其在实际应用中的性能不足问题，并提高了采样效率。"}}
{"id": "2511.18858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18858", "abs": "https://arxiv.org/abs/2511.18858", "authors": ["Xiao Cui", "Yulei Qin", "Xinyue Li", "Wengang Zhou", "Hongsheng Li", "Houqiang Li"], "title": "Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling", "comment": "AAAI 2026 (Oral)", "summary": "Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance.Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10.", "AI": {"tldr": "本文提出了一种针对长尾数据集蒸馏的新方法，通过统计对齐来解决模型偏差和校正监督，显著提升了在不平衡数据集上的性能。", "motivation": "现有数据集蒸馏方法在长尾分布下表现不佳，因为不平衡的类别频率会导致模型表示偏差和批量归一化 (BN) 统计数据受损。", "method": "该研究重新审视了基于轨迹方法的局限性，转而采用统计对齐视角。具体包括：1) 增强专家模型（用于恢复的观察模型和用于重新标记的教师模型），以实现可靠的统计估计和软标签生成；2) 通过动态调整动量的完整前向传递来重新校准BN统计数据，以减少表示偏差；3) 通过多轮机制逐步选择高置信度和多样化的增强，初始化合成图像，以促进覆盖率和多样性。", "result": "在四个长尾基准测试中，该方法在不同类别不平衡程度下均优于现有最先进方法。特别是在IPC=10和IF=10的条件下，在CIFAR-100-LT上将Top-1准确率提高了15.6%，在Tiny-ImageNet-LT上提高了11.8%。", "conclusion": "该方法通过有效缓解模型偏差和恢复公平监督，成功解决了长尾数据集蒸馏的挑战，取得了显著的性能提升。"}}
{"id": "2511.18826", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18826", "abs": "https://arxiv.org/abs/2511.18826", "authors": ["Aakash Gore", "Anoushka Dey", "Aryan Mishra"], "title": "Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification", "comment": null, "summary": "Knowledge distillation has emerged as a powerful technique for model compression, enabling the transfer of knowledge from large teacher networks to compact student models. However, traditional knowledge distillation methods treat all teacher predictions equally, regardless of the teacher's confidence in those predictions. This paper proposes an uncertainty-aware dual-student knowledge distillation framework that leverages teacher prediction uncertainty to selectively guide student learning. We introduce a peer-learning mechanism where two heterogeneous student architectures, specifically ResNet-18 and MobileNetV2, learn collaboratively from both the teacher network and each other. Experimental results on ImageNet-100 demonstrate that our approach achieves superior performance compared to baseline knowledge distillation methods, with ResNet-18 achieving 83.84\\% top-1 accuracy and MobileNetV2 achieving 81.46\\% top-1 accuracy, representing improvements of 2.04\\% and 0.92\\% respectively over traditional single-student distillation approaches.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18847", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18847", "abs": "https://arxiv.org/abs/2511.18847", "authors": ["Ishmam Tashdeed", "Md. Atiqur Rahman", "Sabrina Islam", "Md. Azam Hossain"], "title": "Personalized Federated Segmentation with Shared Feature Aggregation and Boundary-Focused Calibration", "comment": null, "summary": "Personalized federated learning (PFL) possesses the unique capability of preserving data confidentiality among clients while tackling the data heterogeneity problem of non-independent and identically distributed (Non-IID) data. Its advantages have led to widespread adoption in domains such as medical image segmentation. However, the existing approaches mostly overlook the potential benefits of leveraging shared features across clients, where each client contains segmentation data of different organs. In this work, we introduce a novel personalized federated approach for organ agnostic tumor segmentation (FedOAP), that utilizes cross-attention to model long-range dependencies among the shared features of different clients and a boundary-aware loss to improve segmentation consistency. FedOAP employs a decoupled cross-attention (DCA), which enables each client to retain local queries while attending to globally shared key-value pairs aggregated from all clients, thereby capturing long-range inter-organ feature dependencies. Additionally, we introduce perturbed boundary loss (PBL) which focuses on the inconsistencies of the predicted mask's boundary for each client, forcing the model to localize the margins more precisely. We evaluate FedOAP on diverse tumor segmentation tasks spanning different organs. Extensive experiments demonstrate that FedOAP consistently outperforms existing state-of-the-art federated and personalized segmentation methods.", "AI": {"tldr": "本文提出了一种名为FedOAP的个性化联邦学习方法，用于器官无关的肿瘤分割，通过解耦交叉注意力捕捉跨客户端共享特征的长期依赖，并利用扰动边界损失提高分割一致性。", "motivation": "现有的个性化联邦学习（PFL）方法在医学图像分割中，忽视了利用不同客户端（包含不同器官分割数据）之间共享特征的潜在益处，尽管PFL在保护数据隐私和处理非独立同分布数据方面具有优势。", "method": "本文引入了一种新颖的个性化联邦方法FedOAP：\n1. 采用**解耦交叉注意力（DCA）**来建模不同客户端共享特征之间的长程依赖关系。每个客户端保留本地查询，同时关注从所有客户端聚合的全局共享键值对。\n2. 引入**扰动边界损失（PBL）**，专注于每个客户端预测掩膜边界的不一致性，以提高分割一致性和精确地定位边缘。", "result": "FedOAP在涵盖不同器官的各种肿瘤分割任务上进行了评估，实验结果表明，FedOAP持续优于现有的最先进的联邦和个性化分割方法。", "conclusion": "FedOAP通过有效利用跨客户端共享特征和精确处理边界，成功地解决了器官无关肿瘤分割的挑战，并在个性化联邦学习环境中取得了卓越的性能。"}}
{"id": "2511.18873", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.18873", "abs": "https://arxiv.org/abs/2511.18873", "authors": ["Yiming Wang", "Shaofei Wang", "Marko Mihajlovic", "Siyu Tang"], "title": "Neural Texture Splatting: Expressive 3D Gaussian Splatting for View Synthesis, Geometry, and Dynamic Reconstruction", "comment": "SIGGRAPH Asia 2025 (conference track), Project page: https://19reborn.github.io/nts/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading approach for high-quality novel view synthesis, with numerous variants extending its applicability to a broad spectrum of 3D and 4D scene reconstruction tasks. Despite its success, the representational capacity of 3DGS remains limited by the use of 3D Gaussian kernels to model local variations. Recent works have proposed to augment 3DGS with additional per-primitive capacity, such as per-splat textures, to enhance its expressiveness. However, these per-splat texture approaches primarily target dense novel view synthesis with a reduced number of Gaussian primitives, and their effectiveness tends to diminish when applied to more general reconstruction scenarios. In this paper, we aim to achieve concrete performance improvement over state-of-the-art 3DGS variants across a wide range of reconstruction tasks, including novel view synthesis, geometry and dynamic reconstruction, under both sparse and dense input settings. To this end, we introduce Neural Texture Splatting (NTS). At the core of our approach is a global neural field (represented as a hybrid of a tri-plane and a neural decoder) that predicts local appearance and geometric fields for each primitive. By leveraging this shared global representation that models local texture fields across primitives, we significantly reduce model size and facilitate efficient global information exchange, demonstrating strong generalization across tasks. Furthermore, our neural modeling of local texture fields introduces expressive view- and time-dependent effects, a critical aspect that existing methods fail to account for. Extensive experiments show that Neural Texture Splatting consistently improves models and achieves state-of-the-art results across multiple benchmarks.", "AI": {"tldr": "本文提出神经纹理溅射（NTS），通过引入全局神经场来预测每个高斯基元的局部外观和几何场，显著提升了3D高斯溅射在多种三维和四维重建任务中的性能，并解决了现有方法对视图和时间依赖性建模不足的问题。", "motivation": "尽管3D高斯溅射（3DGS）在高质量新视角合成方面表现出色，但其表示能力受限于使用3D高斯核建模局部变化。现有通过增加每个基元容量（如每个溅射纹理）的方法，主要针对密集新视角合成并减少高斯基元数量，在更通用的重建场景中效果不佳。因此，需要一种能在广泛重建任务中实现显著性能提升的3DGS变体。", "method": "本文引入了神经纹理溅射（NTS）。其核心是一个全局神经场（由三平面和神经解码器混合表示），用于预测每个高斯基元的局部外观和几何场。通过利用这个共享的全局表示来建模跨基元的局部纹理场，NTS显著减小了模型尺寸，促进了高效的全局信息交换，并引入了富有表现力的视图和时间依赖性效果。", "result": "NTS在包括新视角合成、几何和动态重建在内的多种任务中，无论是在稀疏还是密集输入设置下，都持续改进了现有模型，并在多个基准测试中取得了最先进的结果。它有效解决了现有方法未能考虑的视图和时间依赖性问题。", "conclusion": "神经纹理溅射（NTS）通过引入一个全局神经场来预测局部外观和几何场，成功地提升了3DGS在广泛重建任务中的性能。该方法不仅减小了模型尺寸、实现了高效信息交换，还引入了关键的视图和时间依赖性建模能力，从而在多个基准测试中达到了最先进的水平。"}}
{"id": "2511.18827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18827", "abs": "https://arxiv.org/abs/2511.18827", "authors": ["Mohammadreza Amiri", "Monireh Hosseini"], "title": "Leveraging Metaheuristic Approaches to Improve Deep Learning Systems for Anxiety Disorder Detection", "comment": "12 pages", "summary": "Despite being among the most common psychological disorders, anxiety-related conditions are still primarily identified through subjective assessments, such as clinical interviews and self-evaluation questionnaires. These conventional methods often require significant time and may vary depending on the evaluator. However, the emergence of advanced artificial intelligence techniques has created new opportunities for detecting anxiety in a more consistent and automated manner. To address the limitations of traditional approaches, this study introduces a comprehensive model that integrates deep learning architectures with optimization strategies inspired by swarm intelligence. Using multimodal and wearable-sensor datasets, the framework analyzes physiological, emotional, and behavioral signals. Swarm intelligence techniques including genetic algorithms and particle swarm optimization are incorporated to refine the feature space and optimize hyperparameters. Meanwhile, deep learning components are tasked with deriving layered and discriminative representations from sequential, multi-source inputs. Our evaluation shows that the fusion of these two computational paradigms significantly enhances detection performance compared with using deep networks alone. The hybrid model achieves notable improvements in accuracy and demonstrates stronger generalization across various individuals. Overall, the results highlight the potential of combining metaheuristic optimization with deep learning to develop scalable, objective, and clinically meaningful solutions for assessing anxiety disorders", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18831", "abs": "https://arxiv.org/abs/2511.18831", "authors": ["Shaobo Wang", "Tianle Niu", "Runkang Yang", "Deshan Liu", "Xu He", "Zichen Wen", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "title": "VideoCompressa: Data-Efficient Video Understanding via Joint Temporal Compression and Spatial Reconstruction", "comment": "15 pages, 6 tables, 8 figures", "summary": "The scalability of video understanding models is increasingly limited by the prohibitive storage and computational costs of large-scale video datasets. While data synthesis has improved data efficiency in the image domain, its extension to video remains challenging due to pervasive temporal redundancy and complex spatiotemporal dynamics. In this work, we uncover a critical insight: the primary source of inefficiency in video datasets is not inter-sample redundancy, but intra-sample frame-level redundancy. To leverage this insight, we introduce VideoCompressa, a novel framework for video data synthesis that reframes the problem as dynamic latent compression. Specifically, VideoCompressa jointly optimizes a differentiable keyframe selector-implemented as a lightweight ConvNet with Gumbel-Softmax sampling-to identify the most informative frames, and a pretrained, frozen Variational Autoencoder (VAE) to compress these frames into compact, semantically rich latent codes. These latent representations are then fed into a compression network, enabling end-to-end backpropagation. Crucially, the keyframe selector and synthetic latent codes are co-optimized to maximize retention of task-relevant information. Experiments show that our method achieves unprecedented data efficiency: on UCF101 with ConvNets, VideoCompressa surpasses full-data training by 2.34\\% points using only 0.13\\% of the original data, with over 5800x speedup compared to traditional synthesis method. Moreover, when fine-tuning Qwen2.5-7B-VL on HMDB51, VideoCompressa matches full-data performance using just 0.41\\% of the training data-outperforming zero-shot baseline by 10.61\\%.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18856", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18856", "abs": "https://arxiv.org/abs/2511.18856", "authors": ["Sana Alamgeer"], "title": "Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos", "comment": null, "summary": "The main goal of the project is to design a new model that predicts regions of interest in 360$^{\\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.", "AI": {"tldr": "设计并评估一种混合显著性模型，用于预测360度视频中的感兴趣区域（ROI），以优化流媒体传输和用户体验。", "motivation": "在360度视频流中，ROI对于预测视口和智能剪辑至关重要，有助于减少带宽消耗、降低用户头部移动，并提高流媒体效率和观看体验。", "method": "方法包括：预处理视频以获取帧、开发训练并测试一个混合显著性模型来预测ROI（即显著性区域），最后对模型输出进行后处理以获得每帧的最终ROI。随后，将该方法的性能与360RAT数据集的主观标注进行比较。", "result": "该研究设计、训练并测试了一个混合显著性模型，用于预测360度视频中的感兴趣区域，并将其性能与360RAT数据集的主观标注进行了比较。", "conclusion": "该项目成功开发了一个混合显著性模型，用于识别360度视频中的感兴趣区域，并通过与主观标注的比较来评估其性能，旨在提升360度视频流的效率和观看质量。"}}
{"id": "2511.18919", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18919", "abs": "https://arxiv.org/abs/2511.18919", "authors": ["Ruiying Liu", "Yuanzhi Liang", "Haibin Huang", "Tianshu Yu", "Chi Zhang"], "title": "Learning What to Trust: Bayesian Prior-Guided Optimization for Visual Generation", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has emerged as an effective and lightweight framework for post-training visual generative models. However, its performance is fundamentally limited by the ambiguity of textual visual correspondence: a single prompt may validly describe diverse visual outputs, and a single image or video may support multiple equally correct interpretations. This many to many relationship leads reward models to generate uncertain and weakly discriminative signals, causing GRPO to underutilize reliable feedback and overfit noisy ones. We introduce Bayesian Prior-Guided Optimization (BPGO), a novel extension of GRPO that explicitly models reward uncertainty through a semantic prior anchor. BPGO adaptively modulates optimization trust at two levels: inter-group Bayesian trust allocation emphasizes updates from groups consistent with the prior while down-weighting ambiguous ones, and intra-group prior-anchored renormalization sharpens sample distinctions by expanding confident deviations and compressing uncertain scores. Across both image and video generation tasks, BPGO delivers consistently stronger semantic alignment, enhanced perceptual fidelity, and faster convergence than standard GRPO and recent variants.", "AI": {"tldr": "针对GRPO在视觉生成模型后训练中受文本-视觉对应模糊性限制的问题，BPGO通过语义先验锚点显式建模奖励不确定性，自适应调整优化信任，显著提升了语义对齐、感知保真度和收敛速度。", "motivation": "现有GRPO框架在视觉生成模型后训练中，因文本-视觉对应关系的模糊性（多对多关系），导致奖励模型产生不确定且区分度弱的信号。这使得GRPO未能有效利用可靠反馈并过度拟合噪声，从而限制了其性能。", "method": "本文提出了贝叶斯先验引导优化 (BPGO)，作为GRPO的扩展。BPGO通过语义先验锚点显式建模奖励不确定性，并在两个层面自适应地调制优化信任：1) 组间贝叶斯信任分配，强调与先验一致的组的更新，并降低模糊组的权重；2) 组内先验锚定重新归一化，通过扩大置信偏差和压缩不确定分数来锐化样本区分度。", "result": "在图像和视频生成任务中，BPGO相较于标准GRPO及其最新变体，实现了更强的语义对齐、增强的感知保真度以及更快的收敛速度。", "conclusion": "BPGO通过有效解决文本-视觉对应模糊性导致的奖励不确定性问题，显著提升了后训练视觉生成模型的性能，带来了更优的语义对齐和感知质量。"}}
{"id": "2511.18894", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18894", "abs": "https://arxiv.org/abs/2511.18894", "authors": ["Chenyu Mu", "Guihai Chen", "Xun Yang", "Erkun Yang", "Cheng Deng"], "title": "MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting", "comment": null, "summary": "Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.", "AI": {"tldr": "针对医学图像分割中噪声标注和模糊边界导致的训练不稳定问题，本文提出MetaDCSeg框架，通过动态学习像素级权重和显式建模边界不确定性，有效抑制噪声影响并提升分割性能。", "motivation": "医学图像分割常受到噪声标注和模糊解剖边界的干扰，导致模型训练不稳定。现有方法依赖全局噪声假设或基于置信度的样本选择，不足以有效缓解噪声导致的性能下降，尤其在挑战性的边界区域。", "method": "本文提出MetaDCSeg，一个鲁棒框架，它动态学习最优的像素级权重以抑制噪声标签的影响，同时保留可靠标注。通过动态中心距离（DCD）机制显式建模边界不确定性，该方法利用加权特征距离处理前景、背景和边界中心，将模型注意力引向模糊边界附近难以分割的像素。", "result": "在四个不同噪声水平的基准数据集上进行的大量实验表明，MetaDCSeg持续优于现有最先进的方法。", "conclusion": "MetaDCSeg通过精确处理结构边界（现有方法常忽略），显著增强了分割性能，为医学图像分割提供了一个鲁棒的解决方案。"}}
{"id": "2511.18839", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18839", "abs": "https://arxiv.org/abs/2511.18839", "authors": ["Yasiru Laksara", "Uthayasanker Thayasivam"], "title": "Enhancing Multi-Label Thoracic Disease Diagnosis with Deep Ensemble-Based Uncertainty Quantification", "comment": null, "summary": "The utility of deep learning models, such as CheXNet, in high stakes clinical settings is fundamentally constrained by their purely deterministic nature, failing to provide reliable measures of predictive confidence. This project addresses this critical gap by integrating robust Uncertainty Quantification (UQ) into a high performance diagnostic platform for 14 common thoracic diseases on the NIH ChestX-ray14 dataset. Initial architectural development failed to stabilize performance and calibration using Monte Carlo Dropout (MCD), yielding an unacceptable Expected Calibration Error (ECE) of 0.7588. This technical failure necessitated a rigorous architectural pivot to a high diversity, 9-member Deep Ensemble (DE). This resulting DE successfully stabilized performance and delivered superior reliability, achieving a State-of-the-Art (SOTA) average Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.8559 and an average F1 Score of 0.3857. Crucially, the DE demonstrated superior calibration (Mean ECE of 0.0728 and Negative Log-Likelihood (NLL) of 0.1916) and enabled the reliable decomposition of total uncertainty into its Aleatoric (irreducible data noise) and Epistemic (reducible model knowledge) components, with a mean Epistemic Uncertainty (EU) of 0.0240. These results establish the Deep Ensemble as a trustworthy and explainable platform, transforming the model from a probabilistic tool into a reliable clinical decision support system.", "AI": {"tldr": "该项目通过深度集成（Deep Ensemble）方法，为NIH ChestX-ray14数据集上的14种胸部疾病诊断提供了一种高性能且具有可靠不确定性量化（UQ）的平台，实现了最先进的性能和卓越的校准。", "motivation": "现有深度学习模型（如CheXNet）的确定性本质限制了它们在高风险临床环境中的应用，因为它们无法提供可靠的预测置信度度量。", "method": "项目最初尝试使用蒙特卡洛Dropout (MCD) 但未能稳定性能和校准。随后，研究团队转向了一个高多样性的9成员深度集成（Deep Ensemble, DE）架构。", "result": "深度集成平台实现了最先进的平均AUROC为0.8559和平均F1分数为0.3857。它还表现出卓越的校准性能（平均ECE为0.0728，NLL为0.1916），并能可靠地将总不确定性分解为偶然不确定性（Aleatoric）和认知不确定性（Epistemic），平均认知不确定性为0.0240。", "conclusion": "研究结果表明，深度集成是一个值得信赖且可解释的平台，能够将模型从一个概率工具转变为一个可靠的临床决策支持系统。"}}
{"id": "2511.18865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18865", "abs": "https://arxiv.org/abs/2511.18865", "authors": ["Yu Zhang", "Haoan Ping", "Yuchen Li", "Zhenshan Bing", "Fuchun Sun", "Alois Knoll"], "title": "DualGazeNet: A Biologically Inspired Dual-Gaze Query Network for Salient Object Detection", "comment": null, "summary": "Recent salient object detection (SOD) methods aim to improve performance in four key directions: semantic enhancement, boundary refinement, auxiliary task supervision, and multi-modal fusion. In pursuit of continuous gains, these approaches have evolved toward increasingly sophisticated architectures with multi-stage pipelines, specialized fusion modules, edge-guided learning, and elaborate attention mechanisms. However, this complexity paradoxically introduces feature redundancy and cross-component interference that obscure salient cues, ultimately reaching performance bottlenecks. In contrast, human vision achieves efficient salient object identification without such architectural complexity. This contrast raises a fundamental question: can we design a biologically grounded yet architecturally simple SOD framework that dispenses with most of this engineering complexity, while achieving state-of-the-art accuracy, computational efficiency, and interpretability? In this work, we answer this question affirmatively by introducing DualGazeNet, a biologically inspired pure Transformer framework that models the dual biological principles of robust representation learning and magnocellular-parvocellular dual-pathway processing with cortical attention modulation in the human visual system. Extensive experiments on five RGB SOD benchmarks show that DualGazeNet consistently surpasses 25 state-of-the-art CNN- and Transformer-based methods. On average, DualGazeNet achieves about 60\\% higher inference speed and 53.4\\% fewer FLOPs than four Transformer-based baselines of similar capacity (VST++, MDSAM, Sam2unet, and BiRefNet). Moreover, DualGazeNet exhibits strong cross-domain generalization, achieving leading or highly competitive performance on camouflaged and underwater SOD benchmarks without relying on additional modalities.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18886", "abs": "https://arxiv.org/abs/2511.18886", "authors": ["Guangyuan Li", "Siming Zheng", "Shuolin Xu", "Jinwei Chen", "Bo Li", "Xiaobin Hu", "Lei Zhao", "Peng-Tao Jiang"], "title": "MagicWorld: Interactive Geometry-driven Video World Exploration", "comment": null, "summary": "Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.", "AI": {"tldr": "MagicWorld是一种交互式视频世界模型，通过整合3D几何先验和历史信息检索，解决了现有方法在视角变化下结构不稳定和多步交互中遗忘历史信息导致误差累积的问题，显著提升了场景的稳定性和连续性。", "motivation": "现有交互式视频世界模型存在两个主要局限：1) 未充分利用指令驱动的场景运动与底层3D几何之间的对应关系，导致视角变化时结构不稳定。2) 在多步交互中容易遗忘历史信息，导致误差累积和场景语义与结构的逐渐漂移。", "method": "本文提出了MagicWorld模型，它从单一场景图像开始，利用用户动作驱动动态场景演化并自回归合成连续场景。具体方法包括：1) 引入动作引导3D几何模块（AG3D），从每次交互的第一帧和相应动作构建点云，为视角转换提供明确的几何约束，从而提高结构一致性。2) 提出历史缓存检索（HCR）机制，在生成过程中检索相关的历史帧并将其作为条件信号注入，帮助模型利用过去的场景信息并减轻误差累积。", "result": "实验结果表明，MagicWorld在交互迭代过程中显著改善了场景的稳定性和连续性。", "conclusion": "MagicWorld通过集成3D几何先验和历史检索机制，有效解决了现有交互式视频世界模型在结构稳定性差和误差累积方面的挑战，实现了更稳定和连续的场景生成。"}}
{"id": "2511.18922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18922", "abs": "https://arxiv.org/abs/2511.18922", "authors": ["Zhenxing Mi", "Yuxin Wang", "Dan Xu"], "title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control", "comment": "Project page: https://mizhenxing.github.io/One4D", "summary": "We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D", "AI": {"tldr": "One4D是一个统一的4D生成和重建框架，能根据不同稀疏度的输入（单图到视频）生成同步的RGB帧和点云图。它通过统一掩码条件（UMC）和解耦LoRA控制（DLC）机制，实现高质量的动态4D内容生成。", "motivation": "研究动机在于创建一个统一的框架，能够处理从单张图像到完整视频等不同稀疏度的条件帧，进行4D内容的生成和重建，并输出同步的RGB帧和几何（点云图）。此外，现有的深度图或点云图重建的扩散模型微调策略常常会降低基础视频模型的性能。", "method": "该研究提出了One4D框架，通过统一掩码条件（UMC）机制处理不同稀疏度的条件帧。它将强大的视频生成模型应用于RGB和点云图的联合生成，并设计了特定的网络架构。为解决现有微调策略导致模型退化的问题，引入了解耦LoRA控制（DLC），该机制使用两个特定模态的LoRA适配器（分别用于RGB帧和点云图），形成解耦的计算分支，并通过轻量级、零初始化的控制链接逐步学习像素级的一致性。", "result": "One4D在适度的计算预算下，通过混合合成和真实4D数据集的训练，在生成和重建任务中均能产生高质量的RGB帧和准确的点云图。", "conclusion": "这项工作代表了利用视频扩散模型实现通用、高质量、基于几何的4D世界建模的重要一步。"}}
{"id": "2511.18875", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.18875", "abs": "https://arxiv.org/abs/2511.18875", "authors": ["Wengyi Zhan", "Mingbao Lin", "Zhihang Lin", "Rongrong Ji"], "title": "Parallel Vision Token Scheduling for Fast and Accurate Multimodal LMMs Inference", "comment": null, "summary": "Multimodal large language models (MLLMs) deliver impressive vision-language reasoning but suffer steep inference latency because self-attention scales quadratically with sequence length and thousands of visual tokens contributed by high-resolution images. Naively pruning less-informative visual tokens reduces this burden, yet indiscriminate removal can strip away contextual cues essential for background or fine-grained questions, undermining accuracy. In this paper, we present ParVTS (Parallel Vision Token Scheduling), a training-free scheduling framework that partitions visual tokens into subject and non-subject groups, processes them in parallel to transfer their semantics into question tokens, and discards the non-subject path mid-inference to reduce computation. This scheduling reduces computational complexity, requires no heuristics or additional modules, and is compatible with diverse existing MLLM architectures. Experiments across multiple MLLM backbones show that ParVTS prunes up to 88.9% of visual tokens with minimal performance drop, achieving 1.77x speedup and 70% FLOPs reduction.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18870", "abs": "https://arxiv.org/abs/2511.18870", "authors": ["Bing Wu", "Chang Zou", "Changlin Li", "Duojun Huang", "Fang Yang", "Hao Tan", "Jack Peng", "Jianbing Wu", "Jiangfeng Xiong", "Jie Jiang", "Linus", "Patrol", "Peizhen Zhang", "Peng Chen", "Penghao Zhao", "Qi Tian", "Songtao Liu", "Weijie Kong", "Weiyan Wang", "Xiao He", "Xin Li", "Xinchi Deng", "Xuefei Zhe", "Yang Li", "Yanxin Long", "Yuanbo Peng", "Yue Wu", "Yuhong Liu", "Zhenyu Wang", "Zuozhuo Dai", "Bo Peng", "Coopers Li", "Gu Gong", "Guojian Xiao", "Jiahe Tian", "Jiaxin Lin", "Jie Liu", "Jihong Zhang", "Jiesong Lian", "Kaihang Pan", "Lei Wang", "Lin Niu", "Mingtao Chen", "Mingyang Chen", "Mingzhe Zheng", "Miles Yang", "Qiangqiang Hu", "Qi Yang", "Qiuyong Xiao", "Runzhou Wu", "Ryan Xu", "Rui Yuan", "Shanshan Sang", "Shisheng Huang", "Siruis Gong", "Shuo Huang", "Weiting Guo", "Xiang Yuan", "Xiaojia Chen", "Xiawei Hu", "Wenzhi Sun", "Xiele Wu", "Xianshun Ren", "Xiaoyan Yuan", "Xiaoyue Mi", "Yepeng Zhang", "Yifu Sun", "Yiting Lu", "Yitong Li", "You Huang", "Yu Tang", "Yixuan Li", "Yuhang Deng", "Yuan Zhou", "Zhichao Hu", "Zhiguang Liu", "Zhihe Yang", "Zilin Yang", "Zhenzhi Lu", "Zixiang Zhou", "Zhao Zhong"], "title": "HunyuanVideo 1.5 Technical Report", "comment": null, "summary": "We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18925", "abs": "https://arxiv.org/abs/2511.18925", "authors": ["Yash Mali"], "title": "AttenDence: Maximizing Attention Confidence for Test Time Adaptation", "comment": "Initial submission. 5 pages, 4 figures", "summary": "Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.", "AI": {"tldr": "本文提出了一种针对Transformer的测试时间自适应（TTA）新方法，通过最小化CLS token到图像块的注意力熵来提高模型在分布偏移下的鲁棒性。", "motivation": "现有的TTA方法通常通过最小化输出分布的熵来适应分布偏移。然而，Transformer模型具有注意力机制，这提供了一个额外的无监督学习信号。研究动机是利用这一信号，通过鼓励模型在分布偏移下更自信地关注相关图像区域，从而提升适应性。", "method": "该方法将CLS token到图像块的注意力分布的熵最小化作为一种新颖的TTA目标。这种方法旨在促使模型在面对分布偏移时，能够更确定地将注意力集中在图像的相关区域。", "result": "实验结果表明，注意力熵最小化在多种腐败类型下提高了模型的鲁棒性，同时在干净数据上的性能没有受到损害。即使在测试时只有单张图像流的情况下，该方法也表现出有效性。", "conclusion": "通过最小化注意力熵，可以有效地使Transformer模型在测试时间适应分布偏移，从而提高其在各种腐败情况下的鲁棒性，并且即使在单样本流的极端条件下也表现良好。"}}
{"id": "2511.18927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18927", "abs": "https://arxiv.org/abs/2511.18927", "authors": ["Keming Shen", "Bizhu Wu", "Junliang Chen", "Xiaoqin Wang", "Linlin Shen"], "title": "FineXtrol: Controllable Motion Generation via Fine-Grained Text", "comment": "20 pages, 14 figures, AAAI 2026", "summary": "Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.", "AI": {"tldr": "本文提出了 FineXtrol，一个用于高效、可控运动生成的框架，它利用时间感知、精确且细粒度的文本控制信号来描述特定身体部位的运动，并通过分层对比学习模块增强文本编码器的判别能力。", "motivation": "现有方法在文本驱动的运动生成中存在问题：大型语言模型（LLMs）生成的细节常有偏差且缺乏明确的时间线索；使用全局3D坐标序列作为控制信号则计算成本高昂。", "method": "我们提出了 FineXtrol 框架，该框架通过时间感知、精确、用户友好且细粒度的文本控制信号指导运动生成，这些信号描述了特定身体部位随时间的运动。此外，我们设计了一个分层对比学习模块，以促使文本编码器为这些新型控制信号生成更具判别性的嵌入，从而提高运动的可控性。", "result": "定量结果表明 FineXtrol 在可控运动生成方面表现出色，而定性分析则展示了其在指导特定身体部位运动方面的灵活性。", "conclusion": "FineXtrol 提供了一种有效且高效的解决方案，通过其新颖的控制信号和学习模块，显著增强了文本驱动运动生成的精度和可控性，特别是在细粒度身体部位运动方面。"}}
{"id": "2511.18882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18882", "abs": "https://arxiv.org/abs/2511.18882", "authors": ["Ayca Duran", "Christoph Waibel", "Bernd Bickel", "Iro Armeni", "Arno Schlueter"], "title": "Facade Segmentation for Solar Photovoltaic Suitability", "comment": "NeurIPS 2025 Tackling Climate Change with Machine Learning Workshop version. Non-archival", "summary": "Building integrated photovoltaic (BIPV) facades represent a promising pathway towards urban decarbonization, especially where roof areas are insufficient and ground-mounted arrays are infeasible. Although machine learning-based approaches to support photovoltaic (PV) planning on rooftops are well researched, automated approaches for facades still remain scarce and oversimplified. This paper therefore presents a pipeline that integrates detailed information on the architectural composition of the facade to automatically identify suitable surfaces for PV application and estimate the solar energy potential. The pipeline fine-tunes SegFormer-B5 on the CMP Facades dataset and converts semantic predictions into facade-level PV suitability masks and PV panel layouts considering module sizes and clearances. Applied to a dataset of 373 facades with known dimensions from ten cities, the results show that installable BIPV potential is significantly lower than theoretical potential, thus providing valuable insights for reliable urban energy planning. With the growing availability of facade imagery, the proposed pipeline can be scaled to support BIPV planning in cities worldwide.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18888", "abs": "https://arxiv.org/abs/2511.18888", "authors": ["Qian Jiang", "Qianqian Wang", "Xin Jin", "Michal Wozniak", "Shaowen Yao", "Wei Zhou"], "title": "MFmamba: A Multi-function Network for Panchromatic Image Resolution Restoration Based on State-Space Model", "comment": "9 pages, 9 figures. This paper has been accepted for publication in AAAI-2026", "summary": "Remote sensing images are becoming increasingly widespread in military, earth resource exploration. Because of the limitation of a single sensor, we can obtain high spatial resolution grayscale panchromatic (PAN) images and low spatial resolution color multispectral (MS) images. Therefore, an important issue is to obtain a color image with high spatial resolution when there is only a PAN image at the input. The existing methods improve spatial resolution using super-resolution (SR) technology and spectral recovery using colorization technology. However, the SR technique cannot improve the spectral resolution, and the colorization technique cannot improve the spatial resolution. Moreover, the pansharpening method needs two registered inputs and can not achieve SR. As a result, an integrated approach is expected. To solve the above problems, we designed a novel multi-function model (MFmamba) to realize the tasks of SR, spectral recovery, joint SR and spectral recovery through three different inputs. Firstly, MFmamba utilizes UNet++ as the backbone, and a Mamba Upsample Block (MUB) is combined with UNet++. Secondly, a Dual Pool Attention (DPA) is designed to replace the skip connection in UNet++. Finally, a Multi-scale Hybrid Cross Block (MHCB) is proposed for initial feature extraction. Many experiments show that MFmamba is competitive in evaluation metrics and visual results and performs well in the three tasks when only the input PAN image is used.", "AI": {"tldr": "本文提出了一种名为MFmamba的新型多功能模型，旨在仅使用全色（PAN）图像作为输入，实现遥感图像的超分辨率（SR）、光谱恢复以及二者联合的任务，解决了现有方法无法同时提升空间和光谱分辨率的问题。", "motivation": "由于单一传感器（如高空间分辨率PAN和低空间分辨率MS）的局限性，在仅有PAN图像输入时，获取高空间分辨率的彩色图像成为一个重要问题。现有超分辨率技术无法提高光谱分辨率，而着色技术无法提高空间分辨率。此外，全色锐化方法需要两个已配准的输入且无法实现超分辨率。因此，需要一种集成的方法来解决这些问题。", "method": "本文设计了一种新颖的多功能模型MFmamba，通过三种不同的输入实现SR、光谱恢复以及SR与光谱恢复的联合任务。MFmamba以UNet++为骨干网络，并结合了Mamba上采样块（MUB）。此外，设计了双池化注意力（DPA）模块以替代UNet++中的跳跃连接。最后，提出了多尺度混合交叉块（MHCB）用于初始特征提取。", "result": "大量实验表明，MFmamba在评估指标和视觉结果方面具有竞争力，并且在仅使用PAN图像作为输入的情况下，在三项任务中均表现出色。", "conclusion": "MFmamba模型成功地通过单一PAN图像输入，实现了遥感图像的超分辨率、光谱恢复以及两者的联合，有效解决了现有技术在同时提升空间和光谱分辨率方面的不足。"}}
{"id": "2511.18957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18957", "abs": "https://arxiv.org/abs/2511.18957", "authors": ["Jianhao Zeng", "Yancheng Bai", "Ruidong Chen", "Xuanpu Zhang", "Lei Sun", "Dongyang Jin", "Ryan Xu", "Nannan Zhang", "Dan Song", "Xiangxiang Chu"], "title": "Eevee: Towards Close-up High-resolution Video-based Virtual Try-on", "comment": null, "summary": "Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.", "AI": {"tldr": "本文针对视频虚拟试穿技术中纹理细节不足和缺乏特写视频的问题，提出了一个包含详细服装信息和全身/特写试穿视频的高分辨率数据集，并引入了一种新的服装一致性度量VGID。实验证明该数据集能显著提升试穿结果的真实感和细节保真度，并揭示了现有方法的局限性。", "motivation": "当前视频虚拟试穿技术存在两大局限：1) 依赖单一服装图像输入，限制了对真实纹理细节的捕捉；2) 大多数方法只生成全身试穿视频，忽略了商业上对提供细节特写视频的需求。", "method": "1. 引入了一个高分辨率的视频虚拟试穿数据集，其特点包括：提供包含特写镜头和文本描述的高保真服装图像，以及包含真人模特全身和特写试穿视频。2. 提出了一种新的服装一致性度量VGID（Video Garment Inception Distance），用于量化纹理和结构细节的保留情况，特别适用于特写视频的精细评估。3. 通过实验验证了数据集的有效性，并对现有模型进行了基准测试。", "result": "1. 利用所提出数据集中详细的服装图像，现有视频生成模型能够提取并融合纹理特征，显著提升了虚拟试穿结果的真实感和细节保真度。2. 全面的基准测试有效揭示了当前方法在纹理和结构保留方面存在的问题。", "conclusion": "本文通过引入高分辨率数据集和新的评估指标，成功解决了视频虚拟试穿技术在纹理细节捕捉和特写视频生成方面的关键挑战，提升了虚拟试穿的真实感，并为未来方法的研究和评估提供了有效工具。"}}
{"id": "2511.18920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18920", "abs": "https://arxiv.org/abs/2511.18920", "authors": ["Wenhao Xu", "Xin Dong", "Yue Li", "Haoyuan Shi", "Zhiwei Xiong"], "title": "EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models", "comment": "8 pages, 7 figures", "summary": "Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18921", "abs": "https://arxiv.org/abs/2511.18921", "authors": ["Juncheng Li", "Yige Li", "Hanxun Huang", "Yunhao Chen", "Xin Wang", "Yixu Wang", "Xingjun Ma", "Yu-Gang Jiang"], "title": "BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models", "comment": null, "summary": "Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \\textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\\% yielding over 90\\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18946", "abs": "https://arxiv.org/abs/2511.18946", "authors": ["José Teixeira", "Pascal Klöckner", "Diana Montezuma", "Melis Erdal Cesur", "João Fraga", "Hugo M. Horlings", "Jaime S. Cardoso", "Sara P. Oliveira"], "title": "Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining", "comment": null, "summary": "In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.", "AI": {"tldr": "该研究开发了一种名为CSSP2P GAN的虚拟染色模型，通过盲法病理专家评估证明其具有更高的病理学保真度，并探讨了对抗性损失的关键作用以及当前评估指标的局限性。", "motivation": "免疫组织化学(IHC)成本高且劳动密集，虚拟染色作为一种图像到图像的转换任务提供了一种有前景的替代方案。然而，现有的大多数虚拟染色模型（基于条件生成对抗网络）忽略了对抗性损失对染色质量的影响，并且依赖于SSIM和PSNR等不够稳健的评估指标来衡量性能。", "method": "本文开发了CSSP2P GAN模型。在迭代开发过程中，研究了对抗性损失对虚拟染色图像质量的影响。通过盲法病理专家评估来衡量模型的病理学保真度。最后，将CSSP2P GAN与该领域的参考工作进行比较，并强调了当前评估指标的局限性。", "result": "CSSP2P GAN通过盲法病理专家评估，实现了更高的病理学保真度。研究证明了对抗性损失在虚拟染色图像质量中的关键作用。同时，研究强调了当前广泛使用的评估指标（如SSIM和PSNR）的局限性，并展示了CSSP2P GAN的卓越性能。", "conclusion": "CSSP2P GAN在虚拟染色领域取得了显著进展，通过解决对抗性损失的影响和改进评估方法，提供了具有更高病理学保真度的虚拟染色图像，并超越了现有模型。"}}
{"id": "2511.18929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18929", "abs": "https://arxiv.org/abs/2511.18929", "authors": ["Zijian Song", "Xiaoxin Lin", "Tao Pu", "Zhenlong Yuan", "Guangrun Wang", "Liang Lin"], "title": "Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search", "comment": "10 pages, 9 figures", "summary": "Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.", "AI": {"tldr": "本文提出了以人为中心的开放未来任务发现 (HOTD) 问题，旨在帮助LMMs在动态场景中发现能减少人类努力的任务。为此，我们引入了HOTD-Bench基准和协同多智能体搜索树 (CMAST) 框架，该框架在实验中显著优于现有LMMs。", "motivation": "尽管大型多模态模型 (LMMs) 在机器人和具身AI领域取得了进展，但它们在开放未来场景中发现直接协助人类任务的能力仍未得到充分探索，尤其是在人类意图高度并发和动态的情况下，如何识别能够减少人类努力的任务是一个关键挑战。", "method": "本文首先形式化了以人为中心的开放未来任务发现 (HOTD) 问题。为促进研究，提出了HOTD-Bench基准，包含2000多个真实世界视频、半自动化标注流程和针对开放集未来评估的模拟协议。此外，提出协同多智能体搜索树 (CMAST) 框架，通过多智能体系统分解复杂推理，并通过可扩展的搜索树模块构建推理过程。", "result": "实验结果表明，CMAST在HOTD-Bench上取得了最佳性能，显著超越了现有的LMMs。CMAST还能与现有LMMs良好集成，持续提升其性能。", "conclusion": "本文成功形式化并解决了以人为中心的开放未来任务发现问题，提出了HOTD-Bench基准和创新的CMAST框架。CMAST展现出卓越的性能，能够有效发现并协助人类在动态开放未来场景中的任务，显著优于现有LMMs。"}}
{"id": "2511.18991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18991", "abs": "https://arxiv.org/abs/2511.18991", "authors": ["Duolikun Danier", "Ge Gao", "Steven McDonagh", "Changjian Li", "Hakan Bilen", "Oisin Mac Aodha"], "title": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation", "comment": null, "summary": "Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.", "AI": {"tldr": "视频生成模型存在3D不一致性问题，本研究提出ViCoDR方法，通过学习多视角一致的扩散表示来显著提高生成视频的3D一致性。", "motivation": "当前视频生成模型生成的视频存在3D不一致性（例如，物体在摄像机姿态变化时变形），这会损害用户体验和模拟真实感。", "method": "我们首先分析了现有的相机控制视频扩散模型，揭示了3D一致性表示与视频之间的强相关性。然后，我们提出了ViCoDR，一种通过学习多视角一致的扩散表示来提高视频模型3D一致性的新方法。", "result": "ViCoDR在相机控制的图像到视频、文本到视频和多视角生成模型上进行了评估，结果表明其显著提高了生成视频的3D一致性。", "conclusion": "通过学习多视角一致的扩散表示，可以有效改善视频生成模型的3D一致性，从而提高生成内容的真实感和用户体验。"}}
{"id": "2511.18993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18993", "abs": "https://arxiv.org/abs/2511.18993", "authors": ["Christos Koutlis", "Symeon Papadopoulos"], "title": "AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization", "comment": "WACV 2026", "summary": "With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.", "AI": {"tldr": "本文提出了一种名为AuViRe（音视频语音表示重建）的新方法，通过利用跨模态语音表示重建中的差异，实现对深度伪造的精准时间定位。", "motivation": "随着复杂的合成音视频内容（如恶意操纵）的快速发展，确保数字媒体的完整性变得至关重要。", "method": "AuViRe方法从一种模态（例如唇部运动）重建基于另一种模态（例如音频波形）的语音表示。在被篡改的视频片段中，跨模态重建的难度显著增加，导致差异被放大，从而提供了强大的判别线索用于精确的时间伪造定位。", "result": "AuViRe在LAV-DF数据集上超越现有技术+8.9 AP@0.95，在AV-Deepfake1M数据集上超越+9.6 AP@0.5，并在真实世界实验中超越+5.1 AUC。", "conclusion": "AuViRe通过利用跨模态重建的差异，为深度伪造的时间定位提供了鲁棒的判别线索，并显著提升了性能。"}}
{"id": "2511.18983", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18983", "abs": "https://arxiv.org/abs/2511.18983", "authors": ["Ching-Yi Lai", "Chih-Yu Jian", "Pei-Cheng Chuang", "Chia-Ming Lee", "Chih-Chung Hsu", "Chiou-Ting Hsu", "Chia-Wen Lin"], "title": "UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection", "comment": "24-page manuscript accepted to IJCV", "summary": "In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.", "AI": {"tldr": "针对社交媒体压缩导致的深度伪造检测挑战，本文提出一种单模态生成多模态对比学习（UMCL）框架，通过将单一视觉模态转换为三种互补特征并进行显式对齐和跨质量相似性学习，实现了对不同压缩率下深度伪造的鲁棒检测。", "motivation": "现有深度伪造检测方法在社交媒体平台不同程度的压缩下，泛化性和可靠性面临挑战。单模态方法在数据压缩下特征退化，而多模态方法则面临数据收集、标注成本高昂以及实际场景中模态质量或可访问性不一致的问题。", "method": "本文提出UMCL框架。在训练阶段，将单一视觉模态转换为三种互补特征：抗压缩的rPPG信号、时间地标动态和预训练视觉-语言模型的语义嵌入。通过亲和力驱动的语义对齐（ASA）策略，利用亲和力矩阵建模模态间关系并通过对比学习优化其一致性。随后，采用跨质量相似性学习（CQSL）策略增强特征在不同压缩率下的鲁棒性。", "result": "实验结果表明，该方法在不同压缩率和操纵类型下均表现出卓越的性能，为鲁棒深度伪造检测树立了新基准。即使单个特征退化，该方法也能保持高检测精度，并通过显式对齐提供对特征关系的可解释性洞察。", "conclusion": "UMCL框架通过单模态生成多模态特征并进行显式对齐和跨质量学习，有效解决了社交媒体压缩对深度伪造检测的挑战，实现了在不同压缩率下高精度且可解释的鲁棒检测。"}}
{"id": "2511.18942", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18942", "abs": "https://arxiv.org/abs/2511.18942", "authors": ["Zong-Wei Hong", "Jing-lun Li", "Lin-Ze Li", "Shen Zhang", "Yao Tang"], "title": "VeCoR - Velocity Contrastive Regularization for Flow Matching", "comment": null, "summary": "Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.\n  To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both \"where to go\" and \"where not to go.\" To be formal, we propose \\textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.\n  On ImageNet-1K 256$\\times$256, VeCoR yields 22\\% and 35\\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/", "AI": {"tldr": "该论文提出了速度对比正则化（VeCoR），通过引入对比性的双向监督来增强流匹配（FM）模型，解决了标准FM在轨迹上累积误差和偏离数据流形的问题，从而提高了生成图像的稳定性和质量。", "motivation": "标准流匹配（FM）模型在学习速度场时，可能沿轨迹累积误差，导致生成样本偏离数据流形，特别是在轻量级或低步数配置下，这会引起感知质量下降。", "method": "研究者提出了速度对比正则化（VeCoR），这是一种补充训练方案，通过对比性的双向监督来增强标准的FM目标函数。VeCoR不仅使预测速度与稳定的参考方向对齐（正向监督），还将其推离不一致的、偏离流形的方向（负向监督）。这种对比性公式将FM从纯粹的吸引式单向目标转变为双向训练信号。", "result": "在ImageNet-1K 256×256数据集上，VeCoR使SiT-XL/2和REPA-SiT-XL/2骨干网络的FID分别相对降低了22%和35%。在MS-COCO文本到图像生成任务中，FID进一步相对提高了32%。这些结果表明，VeCoR在稳定性、收敛性和图像质量方面，特别是在低步数和轻量级设置下，均有显著且一致的改进。", "conclusion": "VeCoR通过引入对比性的双向监督，有效地解决了流匹配模型中轨迹误差累积和样本偏离数据流形的问题。它将FM从单向吸引目标转变为更稳健的双向训练信号，显著提升了跨数据集和骨干网络的感知保真度和生成性能，尤其适用于低步数和轻量级应用。"}}
{"id": "2511.18976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18976", "abs": "https://arxiv.org/abs/2511.18976", "authors": ["Huaming Ling", "Ying Wang", "Si Chen", "Junfeng Fan"], "title": "Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs", "comment": null, "summary": "We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18968", "abs": "https://arxiv.org/abs/2511.18968", "authors": ["Bhuvan Sachdeva", "Sneha Kumari", "Rudransh Agarwal", "Shalaka Kumaraswamy", "Niharika Singri Prasad", "Simon Mueller", "Raphael Lechtenboehmer", "Maximilian W. M. Wintergerst", "Thomas Schultz", "Kaushik Murali", "Mohit Jain"], "title": "CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery", "comment": null, "summary": "Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19035", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19035", "abs": "https://arxiv.org/abs/2511.19035", "authors": ["Kai Zhenga", "Zhenkai Wu", "Fupeng Wei", "Miaolan Zhou", "Kai Lie", "Haitao Guo", "Lei Ding", "Wei Zhang", "Hang-Cheng Dong"], "title": "CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones", "comment": null, "summary": "Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.", "AI": {"tldr": "针对冲突区域的损害评估挑战，本文提出了一种新的任务——变化语义检测（CSD），并引入了基于DINOv3骨干和多尺度交叉注意力差异孪生网络（MC-DiSNet）的方法。同时发布了Gaza-change数据集，实验证明该方法在CSD任务上表现出色。", "motivation": "冲突区域的损害评估对于人道主义援助和区域稳定至关重要。然而，由于建筑风格相似、损害区域小、边界模糊、数据有限、标注困难以及类内相似度高和语义变化模糊等问题，准确快速地评估损害面临巨大挑战。", "method": "本文引入了一个预训练的DINOv3模型作为骨干网络，以实现鲁棒的特征提取。在此基础上，提出了一种多尺度交叉注意力差异孪生网络（MC-DiSNet）。此外，定义了一项新任务——变化语义检测（CSD），该任务直接关注变化区域的语义像素，而非传统语义变化检测（SCD）所需的双时相图像大规模语义标注。同时，发布了一个新的Gaza-change数据集，包含2023-2024年的高分辨率卫星图像对及像素级语义变化标注。", "result": "实验结果表明，本文提出的方法能够有效解决CSD任务，并在Gaza-Change和SECOND数据集上的CSD框架下取得了出色的性能。", "conclusion": "该方法在CSD任务上的有效性和卓越性能为冲突区域的快速损害评估实际应用铺平了道路。"}}
{"id": "2511.19021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19021", "abs": "https://arxiv.org/abs/2511.19021", "authors": ["Qiyang Yu", "Yu Fang", "Tianrui Li", "Xuemei Cao", "Yan Chen", "Jianghao Li", "Fan Min"], "title": "Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting", "comment": "10 pages, 7 figures", "summary": "Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \\b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.", "AI": {"tldr": "Grc-ViT是一种动态的粗粒度到细粒度的视觉Transformer框架，它根据图像复杂度自适应地调整视觉粒度，以解决传统ViT在处理局部细节上的不足，同时提高计算效率和准确性。", "motivation": "Vision Transformers (ViTs) 擅长捕获全局依赖性，但在有效表示细粒度局部细节方面存在不足。现有的多尺度方法通过集成层次或混合特征来缓解此问题，但它们依赖于固定的补丁大小并引入了冗余计算。", "method": "本文提出了粒度驱动的Vision Transformer (Grc-ViT)。它是一个动态的粗粒度到细粒度框架，包含两个关键阶段：1) 粗粒度评估模块，利用边缘密度、熵和频域线索评估视觉复杂度，以估计合适的补丁和窗口大小；2) 细粒度细化模块，根据所选粒度优化注意力计算。该方法通过端到端优化两个可学习参数（α和β）来平衡全局推理和局部感知。", "result": "Grc-ViT增强了细粒度辨别能力，并在准确性和计算效率之间实现了卓越的权衡。", "conclusion": "Grc-ViT通过动态调整视觉粒度，有效解决了ViT在表示细粒度局部细节方面的局限性，从而在保持高准确性的同时提高了计算效率。"}}
{"id": "2511.18978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18978", "abs": "https://arxiv.org/abs/2511.18978", "authors": ["Santiago Moreno", "Pablo Meseguer", "Rocío del Amor", "Valery Naranjo"], "title": "Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models", "comment": "Conference manuscript accepted for oral presentation at CASEIB 2025", "summary": "Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.18989", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18989", "abs": "https://arxiv.org/abs/2511.18989", "authors": ["Wassim Benabbas", "Mohammed Brahimi", "Samir Akhrouf", "Bilal Fortas"], "title": "Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning", "comment": null, "summary": "Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19046", "abs": "https://arxiv.org/abs/2511.19046", "authors": ["Anglin Liu", "Rundong Xue", "Xu R. Cao", "Yifan Shen", "Yi Lu", "Xiang Li", "Qianqian Chen", "Jintai Chen"], "title": "MedSAM3: Delving into Segment Anything with Medical Concepts", "comment": null, "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.", "AI": {"tldr": "MedSAM-3是一个可文本提示的医学图像和视频分割模型，通过微调SAM-3并引入多模态大语言模型代理，实现了开放词汇的概念分割，显著优于现有模型，解决了泛化性和手动标注的痛点。", "motivation": "现有的医学图像分割方法缺乏泛化性，并且在新的临床应用中需要大量耗时的人工标注。", "method": "该研究提出了MedSAM-3模型，通过在配对语义概念标签的医学图像上微调Segment Anything Model (SAM) 3架构。这使得模型能够通过开放词汇文本描述进行可提示概念分割（PCS）。此外，还引入了MedSAM-3 Agent框架，该框架集成了多模态大语言模型（MLLMs），以在代理循环工作流中执行复杂的推理和迭代细化。", "result": "在X射线、MRI、超声、CT和视频等多种医学成像模态的综合实验中，该方法显著优于现有的专业模型和基础模型。", "conclusion": "MedSAM-3提供了一个通用、可文本提示的医学图像和视频分割解决方案，能够精确识别解剖结构，减少了对大量手动标注的需求，并显著提高了性能。"}}
{"id": "2511.19065", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19065", "abs": "https://arxiv.org/abs/2511.19065", "authors": ["Jin-Young Kim", "Hyojun Go", "Lea Bogensperger", "Julius Erbach", "Nikolai Kalischek", "Federico Tombari", "Konrad Schindler", "Dominik Narnhofer"], "title": "Understanding, Accelerating, and Improving MeanFlow Training", "comment": null, "summary": "MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.", "AI": {"tldr": "本文分析了MeanFlow模型中瞬时速度场和平均速度场之间的训练动态，揭示了它们之间的复杂相互作用，并基于这些发现提出了一种改进的训练方案，显著提升了MeanFlow在少量步数生成任务上的性能和训练效率。", "motivation": "MeanFlow承诺通过联合学习瞬时和平均速度场实现高质量的少量步数生成，但其底层的训练动态尚不明确。理解这两种速度场的相互作用对于优化模型性能至关重要。", "method": "研究通过分析瞬时速度和平均速度之间的相互作用来理解训练动态，并进行了任务亲和性分析。基于观察结果，设计了一种有效的训练方案：首先加速瞬时速度的形成，然后将重点从短时间间隔的平均速度转移到长时间间隔的平均速度。", "result": "研究发现：(i) 建立良好的瞬时速度是学习平均速度的前提；(ii) 瞬时速度的学习在时间间隔较小时受益于平均速度，但随着间隔增大而退化；(iii) 平滑地学习大间隔平均速度（对一步生成至关重要）依赖于先形成准确的瞬时速度和小间隔平均速度。改进后的训练方案实现了更快的收敛速度和显著更好的少量步数生成效果：在1-NFE ImageNet 256x256上，FID达到2.87（基线为3.43），或以2.5倍更短的训练时间或更小的骨干网络达到基线性能。", "conclusion": "通过深入分析MeanFlow的训练动态并设计出相应的优化训练方案，可以显著提高模型的训练效率和在少量步数生成任务上的表现，尤其是在单步生成方面表现出色。"}}
{"id": "2511.19004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19004", "abs": "https://arxiv.org/abs/2511.19004", "authors": ["Wentao Qu", "Guofeng Mei", "Yang Wu", "Yongshun Gong", "Xiaoshui Huang", "Liang Xiao"], "title": "A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation", "comment": null, "summary": "Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19062", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19062", "abs": "https://arxiv.org/abs/2511.19062", "authors": ["Qiyang Yu", "Yu Fang", "Tianrui Li", "Xuemei Cao", "Yan Chen", "Jianghao Li", "Fan Min", "Yi Zhang"], "title": "Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation", "comment": "19 pages, 7 figures", "summary": "Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.", "AI": {"tldr": "本文提出Grc-SAM，一个受粒度计算启发的粗到细框架，旨在解决无提示图像分割中现有模型（如SAM）在区域定位和高分辨率细粒度建模方面的局限性。", "motivation": "现有无提示图像分割模型（特别是SAM）存在两个主要限制：1) 缺乏自主区域定位机制（局部性问题）；2) 在高分辨率下细粒度建模能力有限（可伸缩性问题），因为它们通常在单一粒度级别生成提示。", "method": "Grc-SAM是一个粗到细的框架：1) 粗粒度阶段：自适应地从特征中提取高响应区域，实现精确的前景定位并减少对外部提示的依赖。2) 细粒度阶段：采用更精细的块划分和稀疏局部Swin风格注意力机制，以增强细节建模并实现高分辨率分割。3) 改进后的掩码被编码为潜在提示嵌入，用于SAM解码器，用自动化推理过程取代了手工制作的提示。该方法集成了多粒度注意力，将粒度计算与视觉Transformer相结合。", "result": "广泛的实验结果表明，Grc-SAM在准确性和可伸缩性方面均优于基线方法。", "conclusion": "Grc-SAM为无提示分割提供了一个独特的粒度计算视角，有效解决了现有模型的局限性。"}}
{"id": "2511.19033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19033", "abs": "https://arxiv.org/abs/2511.19033", "authors": ["Gengyuan Zhang", "Mingcong Ding", "Jingpei Wu", "Ruotong Liao", "Volker Tresp"], "title": "ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay", "comment": "8 main pages plus 13 pages Appendix", "summary": "Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.", "AI": {"tldr": "ReEXplore是一种无需训练的框架，通过追溯经验回放和分层前沿选择，显著提升了多模态大模型（MLLM）在具身探索任务中的性能，使其更加鲁棒、可追溯和高效。", "motivation": "现有的基于MLLM的具身探索方法存在不足：1) 过度依赖预训练的陈旧知识；2) 模仿学习或强化学习等训练方法对于稀疏奖励的长周期任务成本高昂；3) 基于前沿的探索产生的大规模、视觉细微的动作空间难以让MLLM做出可靠决策。", "method": "本文提出了ReEXplore，一个无需训练的框架。它采用：1) 追溯经验回放，在推理时注入提炼的抽象经验；2) 分层前沿选择，将前沿排序分解为从粗到细的决策。", "result": "ReEXplore实现了鲁棒、可追溯和高效的探索。在多个具身探索基准测试中，它比强大的MLLM基线表现出显著提升，在使用开源骨干网络时，成功率和导航效率都提高了三倍。", "conclusion": "ReEXplore通过其无需训练的方法，有效解决了MLLM在具身探索中的挑战，通过经验回放和分层前沿选择，大幅提升了探索性能和效率。"}}
{"id": "2511.19024", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19024", "abs": "https://arxiv.org/abs/2511.19024", "authors": ["Long Tang", "Guoquan Zhen", "Jie Hao", "Jianbo Zhang", "Huiyu Duan", "Liang Yuan", "Guangtao Zhai"], "title": "Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling", "comment": null, "summary": "Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \\underline{l}ayer\\underline{i}nteraction and MoE-based \\underline{f}eature d\\underline{e}coupling, termed \\textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \\href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\\texttt{Life-IQA}}.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19049", "abs": "https://arxiv.org/abs/2511.19049", "authors": ["Ruojun Xu", "Yu Kai", "Xuhua Ren", "Jiaxiang Cheng", "Bing Ma", "Tianxiang Zheng", "Qinhlin Lu"], "title": "Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation", "comment": null, "summary": "Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.", "AI": {"tldr": "本文分析了DPO在扩散模型中存在的似然位移问题，尤其是在视频生成任务中，并提出了Policy-Guided DPO (PG-DPO) 方法，通过结合自适应拒绝缩放和隐式偏好正则化来有效缓解该问题，显著提升了视频生成的偏好对齐效果。", "motivation": "DPO在对齐生成输出与人类偏好方面表现出色，但存在似然位移的局限性，即训练过程中选中样本的概率会反常下降，损害生成质量。尽管该问题已在自回归模型中研究，但在扩散模型中（特别是视频生成任务）其影响仍未被充分探索，导致次优性能。", "method": "研究人员对扩散框架内的DPO损失进行了形式分析，描述了训练样本更新如何影响模型对其他样本的预测。通过此分析，识别出两种主要失败模式：由小奖励差引起的“优化冲突”和由大奖励差引起的“次优最大化”。基于这些洞察，提出了Policy-Guided DPO (PG-DPO)，该方法结合了自适应拒绝缩放 (ARS) 和隐式偏好正则化 (IPR) 以有效缓解似然位移。", "result": "实验结果表明，PG-DPO在定量指标和定性评估方面均优于现有方法，为提高视频生成任务中的偏好对齐提供了一个鲁棒的解决方案。", "conclusion": "PG-DPO通过解决DPO在扩散模型中的似然位移问题，特别是针对视频生成任务，提供了一种结合ARS和IPR的新颖且有效的解决方案，显著提升了模型与人类偏好的对齐能力。"}}
{"id": "2511.19032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19032", "abs": "https://arxiv.org/abs/2511.19032", "authors": ["Xiangjie Sui", "Songyang Li", "Hanwei Zhu", "Baoliang Chen", "Yuming Fang", "Xin Sun"], "title": "Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric", "comment": "15 pages", "summary": "Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.", "AI": {"tldr": "本文提出Bench-C基准和RAS指标，用于更准确地评估大型视觉语言模型在视觉损坏下的鲁棒性，揭示了模型在损坏下的独特行为和预测结构退化。", "motivation": "现有评估范式在研究大型视觉语言模型（LVLMs）的视觉损坏鲁棒性方面存在局限性：1）当前数据集中的低区分度样本掩盖了模型间真实的鲁棒性差距；2）传统的基于准确率的指标未能捕捉底层预测结构的退化。", "method": "本文引入了Bench-C，一个侧重于区分度样本的综合基准，通过考虑损坏下的预测不一致性和语义多样性来选择样本。此外，提出了鲁棒性对齐分数（RAS），一个统一的指标，通过考量预测不确定性偏移和校准对齐来衡量logit级别预测结构的退化。", "result": "综合实验和分析揭示了几个有趣的发现：1）模型在损坏下表现出独特的行为模式，例如错误的置信度和犹豫；2）尽管细微损坏可能导致准确率略微提高，但整体预测结构仍在退化；3）通过将损坏鲁棒性分解为破坏性和纠正性组件，可以揭示模型之间不同的失败和恢复模式。", "conclusion": "本文通过引入新的基准和指标，为LVLMs在视觉损坏下的鲁棒性评估提供了更全面和深入的方法，揭示了现有评估的不足以及模型在复杂损坏条件下的真实行为和预测结构变化。"}}
{"id": "2511.19119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19119", "abs": "https://arxiv.org/abs/2511.19119", "authors": ["Qirui Wang", "Jingyi He", "Yining Pan", "Si Yong Yeo", "Xulei Yang", "Shijie Li"], "title": "MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images", "comment": null, "summary": "Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.", "AI": {"tldr": "本文提出了MonoSR，一个大规模单目空间推理数据集，涵盖室内、室外和以物体为中心的场景，并支持多种问题类型，旨在推动开放世界单目空间推理。同时，评估了现有视觉语言模型并分析了辅助信息的重要性。", "motivation": "空间推理（SR）对于具身AI和自动驾驶等实际应用至关重要。然而，现有研究主要集中于室内环境且依赖多视角观测，这限制了其在室外场景的泛化能力和对单目图像（最常见的实际设置）的适用性。", "method": "本文提出并构建了MonoSR数据集，该数据集涵盖室内、室外和以物体为中心的多种场景，并支持多种问题类型。在此数据集上，评估了先进的视觉语言模型以揭示其在该挑战性任务上的局限性。此外，还分析了辅助信息对于单目空间推理是否关键，并为未来模型设计提供了实用指导。", "result": "MonoSR数据集为开放世界单目空间推理提供了途径。评估结果显示，先进的视觉语言模型在单目空间推理任务上存在局限性。对辅助信息的分析为设计未来的模型提供了指导。", "conclusion": "MonoSR数据集和相关分析共同为在真实世界、开放世界环境中推进单目空间推理奠定了基础。"}}
{"id": "2511.19067", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19067", "abs": "https://arxiv.org/abs/2511.19067", "authors": ["Timur Mamedov", "Anton Konushin", "Vadim Konushin"], "title": "DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling", "comment": null, "summary": "Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.", "AI": {"tldr": "DynaMix是一种新型的行人再识别（Re-ID）方法，它通过动态结合手工标注的多摄像头数据和大规模伪标注的单摄像头数据，显著提升了在未见摄像头和环境下的泛化性能。", "motivation": "现有的行人再识别方法严重依赖有限的标注多摄像头数据，难以在未见摄像头和环境中有效识别个体。研究者旨在开发一种能利用更广泛数据（包括大规模伪标注单摄像头数据）并有效泛化的方法。", "method": "本文提出了DynaMix，它动态适应训练数据的结构和噪声。核心组件包括：1) 重新标注模块，实时优化单摄像头身份的伪标签；2) 高效质心模块，在大规模身份空间中保持鲁棒的身份表示；3) 数据采样模块，精心构建混合数据小批量以平衡学习复杂性和批内多样性。所有组件都设计为可高效扩展，支持在数百万图像和数十万身份上进行有效训练。", "result": "DynaMix在泛化行人再识别任务中持续超越了现有最先进的方法。它能有效处理数百万图像和数十万身份，展示了卓越的性能。", "conclusion": "DynaMix通过动态结合多源数据并设计高效的模块，为泛化行人再识别提供了一种新颖且有效的方法，成功解决了现有方法对有限标注数据的依赖问题，并实现了领先的性能。"}}
{"id": "2511.19057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19057", "abs": "https://arxiv.org/abs/2511.19057", "authors": ["Hai Wu", "Shuai Tang", "Jiale Wang", "Longkun Zou", "Mingyue Guo", "Rongqin Liang", "Ke Chen", "Yaowei Wang"], "title": "LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space", "comment": "25 pages", "summary": "Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19105", "abs": "https://arxiv.org/abs/2511.19105", "authors": ["Jichao Chen", "YangYang Qu", "Ruibo Tang", "Dirk Slock"], "title": "Graph-based 3D Human Pose Estimation using WiFi Signals", "comment": null, "summary": "WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.", "AI": {"tldr": "本文提出GraphPose-Fi，一个基于图的框架，用于WiFi基的3D人体姿态估计，通过显式建模骨骼拓扑结构来提高性能。", "motivation": "现有的WiFi基人体姿态估计方法通常直接将WiFi CSI映射到3D关节坐标，忽略了人体关节固有的拓扑关系，导致精度不足。", "method": "GraphPose-Fi框架包括：1) 一个跨天线共享的CNN编码器，用于子载波-时间特征提取；2) 一个轻量级注意力模块，自适应地重新加权时间和跨天线的特征；3) 一个基于图的回归头，结合GCN层和自注意力机制，以捕获局部拓扑和全局依赖关系。", "result": "该方法在MM-Fi数据集的各种设置下，显著优于现有方法。", "conclusion": "GraphPose-Fi通过显式建模骨骼拓扑结构，有效提升了WiFi基3D人体姿态估计的性能。"}}
{"id": "2511.19145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19145", "abs": "https://arxiv.org/abs/2511.19145", "authors": ["Dongha Lee", "Jinhee Park", "Minjun Kim", "Junseok Kwon"], "title": "ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation", "comment": "16 pages, 5 figures, under review", "summary": "We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.", "AI": {"tldr": "ABM-LoRA是一种新的初始化策略，通过对齐激活边界来加速低秩适配器（LoRA）的收敛，显著减少信息损失并提高性能。", "motivation": "LoRA的随机初始化导致梯度更新在不匹配的切线空间中进行，造成显著信息损失并阻碍早期收敛。", "method": "ABM-LoRA通过在下游训练前，将适配器的激活边界与预训练模型的激活边界对齐，从而最大化全参数梯度在适配器子空间中的投影。", "result": "ABM-LoRA在语言理解（T5-Base on GLUE）、对话生成（LLaMA2-7B on WizardLM）和视觉识别（ViT-B/16 on VTAB-1K）等多样架构和任务上均显示出有效性。在VTAB-1K上，它取得了所有方法中的最高准确率，尤其在需要几何理解的结构化推理任务上增益显著。", "conclusion": "ABM-LoRA通过在初始化时减少信息损失、降低起始损失并加速收敛，显著提升了低秩适配器的训练效率和性能，尤其在复杂任务上表现突出。"}}
{"id": "2511.19137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19137", "abs": "https://arxiv.org/abs/2511.19137", "authors": ["Zhifeng Xie", "Keyi Zhang", "Yiye Yan", "Yuling Guo", "Fan Yang", "Jiting Zhou", "Mengtian Li"], "title": "FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation", "comment": null, "summary": "Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.", "AI": {"tldr": "FilmSceneDesigner是一个自动化电影场景生成系统，能根据自然语言描述，通过智能体链式框架和程序化生成流程，从零开始构建具有电影真实感的完整场景。", "motivation": "传统的电影场景设计依赖专家手动建模，耗时费力，效率低下，限制了电影制作的灵活性和速度。", "method": "该研究引入了FilmSceneDesigner系统。首先，它设计了一个基于智能体的链式框架，通过提示策略从自然语言描述（包括场景类型、历史时期和风格）生成结构化参数。其次，提出了一个程序化生成管道，利用这些参数执行一系列专用功能，包括平面图和结构生成、材质分配、门窗放置以及物体检索和布局。此外，为了增强电影真实感和资产多样性，构建了一个包含6,862个电影专用3D资产和733种材质的SetDepot-Pro数据集。", "result": "实验结果和人工评估表明，该系统能够生成结构合理、具有高电影保真度的场景。生成的场景可支持虚拟预演、施工图制作和情绪板创建等下游任务。", "conclusion": "FilmSceneDesigner系统成功地自动化了电影场景设计流程，通过结合自然语言处理和程序化生成技术，显著提高了效率和场景质量，为电影制作提供了有力的支持。"}}
{"id": "2511.19071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19071", "abs": "https://arxiv.org/abs/2511.19071", "authors": ["Fangda Chen", "Jintao Tang", "Pancheng Wang", "Ting Wang", "Shasha Li", "Ting Deng"], "title": "DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation", "comment": "Accepted by BIBM 2024", "summary": "The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.", "AI": {"tldr": "针对SAM在3D医学图像分割中存在的空间特征丢失和手动提示依赖问题，本文提出了DEAP-3DSAM模型，通过特征增强解码器和双注意力提示器实现了空间特征增强和自动提示，并在多个数据集上取得了最先进的性能。", "motivation": "尽管Segment Anything Model (SAM) 在2D图像分割中表现出色，但其在3D医学图像分割中的应用存在局限性：1) 伪3D处理导致空间特征丢失，限制了性能；2) 大多数基于SAM的方法仍依赖手动提示，这在实际应用中难以实施且需要大量专家知识。", "method": "本文提出了Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) 模型。具体方法包括：1) 设计了一个特征增强解码器（Feature Enhanced Decoder），将原始图像特征与丰富的空间细节信息融合，以增强空间特征；2) 设计了一个双注意力提示器（Dual Attention Prompter），通过空间注意力和通道注意力自动获取提示信息。", "result": "DEAP-3DSAM在四个公共腹部肿瘤分割数据集上进行了综合实验，结果表明该模型在3D图像分割中达到了最先进的性能，超越或匹配了现有手动提示方法。此外，定量和定性消融研究均证实了所提出模块的有效性。", "conclusion": "DEAP-3DSAM通过解决SAM在3D医学图像分割中的空间特征丢失和手动提示依赖问题，显著提升了其在该领域的性能，并实现了最先进的结果，为实际应用提供了更有效和自动化的解决方案。"}}
{"id": "2511.19111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19111", "abs": "https://arxiv.org/abs/2511.19111", "authors": ["Hai Ci", "Ziheng Peng", "Pei Yang", "Yingxin Xuan", "Mike Zheng Shou"], "title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection", "comment": "16 pages, 10 figures", "summary": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19117", "categories": ["cs.CV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.19117", "abs": "https://arxiv.org/abs/2511.19117", "authors": ["Minchong Chen", "Xiaoyun Yuan", "Junzhe Wan", "Jianing Zhang", "Jun Zhang"], "title": "3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion", "comment": "11 pages, 7 figures", "summary": "The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.", "AI": {"tldr": "本文提出3M-TI，一种免校准的多相机跨模态扩散框架，用于移动热成像超分辨率，通过集成跨模态自注意力模块，利用RGB信息提升热图像质量，并显著改善下游任务表现。", "motivation": "移动平台热传感器的微型化限制了其空间分辨率和纹理保真度，导致图像模糊且信息量少。现有热超分辨率方法（单图像和RGB引导）分别存在难以恢复精细结构或需要繁琐且不鲁棒的跨相机校准的问题。", "method": "本文提出3M-TI，一个免校准的多相机跨模态扩散框架。其核心在于将跨模态自注意力模块（CSM）集成到扩散UNet中，取代原有的自注意力层，以在去噪过程中自适应地对齐热图像和RGB特征，无需显式相机校准。此设计使扩散网络能利用其生成先验来增强超分辨率热图像的空间分辨率、结构保真度和纹理细节。", "result": "在真实移动热像仪和公共基准上的广泛评估表明，3M-TI在视觉质量和定量指标上均达到了最先进的性能。更重要的是，经3M-TI增强的热图像在目标检测和分割等关键下游任务中取得了显著提升。", "conclusion": "3M-TI为移动热成像提供了一种实用且鲁棒的解决方案，通过提高热图像质量，显著提升了移动热感知系统的性能和实用价值。"}}
{"id": "2511.19126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19126", "abs": "https://arxiv.org/abs/2511.19126", "authors": ["Beilin Chu", "Weike You", "Mengtao Li", "Tingting Zheng", "Kehan Zhao", "Xuan Xu", "Zhigao Lu", "Jia Song", "Moxuan Xu", "Linna Zhou"], "title": "When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP", "comment": "14 pages, 7 figures and 7 tables", "summary": "The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.", "AI": {"tldr": "该研究通过引入Patch Shuffle来抑制CLIP模型中的语义偏差，并提出SemAnti微调范式，以提高AI生成图像检测器在跨领域泛化方面的鲁棒性。", "motivation": "GAN和扩散模型的快速发展使得AI生成图像的检测面临新挑战。虽然基于CLIP的检测器具有良好的泛化潜力，但它们通常依赖语义线索而非生成器伪影，导致在分布偏移下性能脆弱。研究旨在解决CLIP检测器中的语义偏差问题。", "method": "研究重新审视了语义偏差的性质，发现Patch Shuffle通过破坏全局语义连续性同时保留局部伪影线索，能有效降低语义熵并同质化自然图像和合成图像的特征分布。通过分层分析，发现CLIP的深层语义结构在语义偏差被抑制后能稳定跨域表示。基于此，提出SemAnti（语义对抗性）微调范式，冻结语义子空间，仅在打乱语义的情况下调整对伪影敏感的层。", "result": "尽管方法简单，SemAnti在AIGCDetectBenchmark和GenImage上实现了最先进的跨域泛化性能。", "conclusion": "研究表明，调节语义是释放CLIP在鲁棒AI生成图像检测方面全部潜力的关键。"}}
{"id": "2511.19109", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19109", "abs": "https://arxiv.org/abs/2511.19109", "authors": ["Mohan Ramesh", "Mark Azer", "Fabian B. Flohr"], "title": "HABIT: Human Action Benchmark for Interactive Traffic in CARLA", "comment": "Accepted to WACV 2026. This is the pre-camera-ready version", "summary": "Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19199", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19199", "abs": "https://arxiv.org/abs/2511.19199", "authors": ["Teodora Popordanoska", "Jiameng Li", "Matthew B. Blaschko"], "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection", "comment": "First two authors contributed equally", "summary": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.", "AI": {"tldr": "本文介绍了CLASH，一个用于多模态矛盾检测的新基准，旨在评估模型识别图像与文本之间矛盾的能力，并发现现有模型在该任务上的显著局限性。", "motivation": "现实世界中常出现矛盾的多模态输入，但现有基准通常假设输入一致性，未能评估跨模态矛盾检测——这是防止幻觉和确保可靠性的基本能力。", "method": "研究引入了CLASH基准，它将COCO图像与包含受控的物体级或属性级矛盾的描述配对。样本包括多项选择和开放式格式的目标问题。基准包含通过自动化质量检查过滤的大规模微调集和较小的人工验证诊断集。", "result": "对现有最先进模型的分析揭示了它们在识别跨模态冲突方面的显著局限性，暴露了系统性的模态偏见和特定类别的弱点。此外，实证表明，在CLASH上进行有针对性的微调可以显著增强冲突检测能力。", "conclusion": "CLASH基准揭示了当前多模态模型在识别跨模态矛盾方面的不足，并证明了通过专门的微调可以有效提升模型的矛盾检测能力，这对于提高模型可靠性至关重要。"}}
{"id": "2511.19134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19134", "abs": "https://arxiv.org/abs/2511.19134", "authors": ["Shuyu Cao", "Minxin Chen", "Yucheng Song", "Zhaozhong Chen", "Xinyou Zhang"], "title": "MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery", "comment": "Submitted to IEEE Geoscience and Remote Sensing Letters", "summary": "Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19200", "abs": "https://arxiv.org/abs/2511.19200", "authors": ["Itay Cohen", "Ethan Fetaya", "Amir Rosenfeld"], "title": "Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?", "comment": null, "summary": "Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired \"real\"/\"lookalike\" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.", "AI": {"tldr": "研究者探讨了视觉语言模型（如CLIP）区分真实物体和“看起来像”物体的能力，并发现通过在嵌入空间中学习一个方向可以提高这种辨别能力，从而缩小与人类感知的差距。", "motivation": "尽管计算机视觉模型在识别基准上表现出色，但在与人类感知相比仍存在显著差距。一个微妙的能力是判断图像是否“看起来像”某个物体，而非其真实实例。本研究旨在探讨视觉语言模型是否能捕捉到这种区别。", "method": "研究者创建了一个名为RoLA（Real or Lookalike）的数据集，包含真实和“看起来像”的示例。首先，他们使用带有“真实”/“看起来像”提示的基线进行评估。随后，他们估算了CLIP嵌入空间中一个能够区分真实和“看起来像”物体表示的方向。最后，将此方向应用于图像和文本嵌入。", "result": "将估算出的方向应用于图像和文本嵌入后，在Conceptual12M数据集上的跨模态检索性能得到改善，并且由CLIP前缀字幕器生成的图像描述也得到了增强。", "conclusion": "研究表明，视觉语言模型（如CLIP）可以被训练以捕捉真实物体和“看起来像”物体之间的区别。通过在嵌入空间中学习特定方向，可以提升模型的辨别能力，使其在理解这种微妙的人类感知能力上更进一步。"}}
{"id": "2511.19169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19169", "abs": "https://arxiv.org/abs/2511.19169", "authors": ["Bingchen Li", "Xin Li", "Jiaqi Xu", "Jiaming Guo", "Wenbo Li", "Renjing Pei", "Zhibo Chen"], "title": "Test-Time Preference Optimization for Image Restoration", "comment": "Accepted by AAAI26", "summary": "Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.", "AI": {"tldr": "本文提出了首个图像恢复的测试时偏好优化（TTPO）范式，旨在无需模型重训练和人工偏好数据收集的情况下，提升恢复图像的感知质量并使其更符合人类偏好，同时兼容任何图像恢复模型。", "motivation": "现有的图像恢复（IR）模型，无论是基于L1/LPIPS损失训练的还是零样本方法，往往无法与人类偏好对齐，导致恢复图像不尽如人意。因此，迫切需要一种方法来提升恢复质量，并能灵活适应各种IR任务和骨干模型，而无需重训练或大量偏好数据收集。", "method": "本文提出了训练无关的三阶段TTPO管道：(i) 基于初始恢复图像，利用扩散反演和去噪在线生成候选偏好图像；(ii) 使用自动化偏好对齐度量或人类反馈选择偏好和非偏好图像；(iii) 将选定的偏好图像作为奖励信号，指导扩散去噪过程，优化恢复图像以更好地与人类偏好对齐。", "result": "在各种图像恢复任务和模型上的广泛实验证明了所提出管道的有效性和灵活性。", "conclusion": "TTPO是首个用于图像恢复的测试时偏好优化范式，它显著提升了感知质量，能够在线生成偏好数据，并与任何IR模型骨干兼容，解决了现有方法与人类偏好不符的痛点。"}}
{"id": "2511.19220", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19220", "abs": "https://arxiv.org/abs/2511.19220", "authors": ["Federico Felizzi", "Olivia Riccomi", "Michele Ferramola", "Francesco Andrea Causio", "Manuel Del Medico", "Vittorio De Vita", "Lorenzo De Mori", "Alessandra Piscitelli Pietro Eric Risuleo", "Bianca Destro Castaniti", "Antonio Cristiano Alessia Longo", "Luigi De Angelis", "Mariapia Vassalli", "Marcello Di Pumpo"], "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering", "comment": "Accepted at the Workshop on Multimodal Representation Learning for Healthcare (MMRL4H), EurIPS 2025", "summary": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.", "AI": {"tldr": "本研究调查了大型视觉语言模型（VLMs）在意大利医学问答中对视觉信息的真实依赖程度，发现不同模型在视觉基础能力上存在显著差异，并揭示了模型可能依赖文本捷径而非真实图像分析。", "motivation": "尽管大型视觉语言模型在医学视觉问答基准测试中表现出色，但它们对视觉信息的实际依赖程度尚不明确。研究旨在探究前沿VLM是否在回答意大利医学问题时展现出真正的视觉基础。", "method": "研究测试了四种最先进的VLM（Claude Sonnet 4.5、GPT-4o、GPT-5-mini和Gemini 2.0 flash exp）。使用来自EuropeMedQA意大利数据集的60个明确需要图像解释的问题，将正确的医学图像替换为空白占位符，以测试模型是否真正整合了视觉和文本信息。", "result": "结果显示视觉依赖性存在显著差异：GPT-4o表现出最强的视觉基础，准确率下降了27.9个百分点（从83.2%降至55.3%）。而GPT-5-mini、Gemini和Claude则保持了较高的准确率，下降幅度分别为8.5、2.4和5.6个百分点。所有模型在解释中都自信地编造了视觉解释，表明它们对文本捷径的依赖程度不同于真实的视觉分析。", "conclusion": "这些发现突出了模型鲁棒性的关键差异，并强调了在临床部署前进行严格评估的必要性。"}}
{"id": "2511.19147", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19147", "abs": "https://arxiv.org/abs/2511.19147", "authors": ["Huisoo Lee", "Jisu Han", "Hyunsouk Cho", "Wonjun Hwang"], "title": "Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation", "comment": "15 pages, 8 figures", "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19172", "abs": "https://arxiv.org/abs/2511.19172", "authors": ["Kehua Chen", "Tianlu Mao", "Zhuxin Ma", "Hao Jiang", "Zehao Li", "Zihan Liu", "Shuqi Gao", "Honglong Zhao", "Feng Dai", "Yucheng Zhang", "Zhaoqi Wang"], "title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes", "comment": "Project page: https://m3phist0.github.io/MetroGS", "summary": "Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19229", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19229", "abs": "https://arxiv.org/abs/2511.19229", "authors": ["Selena Song", "Ziming Xu", "Zijun Zhang", "Kun Zhou", "Jiaxian Guo", "Lianhui Qin", "Biwei Huang"], "title": "Learning Plug-and-play Memory for Guiding Video Diffusion Models", "comment": null, "summary": "Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.", "AI": {"tldr": "本文提出DiT-Mem，一个即插即用的记忆模块，通过注入世界知识来增强基于扩散Transformer的视频生成模型，使其更好地遵循物理规律并提高视频保真度。", "motivation": "基于扩散Transformer（DiT）的视频生成模型在视觉质量和时间一致性方面表现出色，但它们经常违反基本的物理定律和常识性动态，这表明它们缺乏明确的世界知识。", "method": "研究人员通过实证研究发现DiT可以通过干预其隐藏状态进行引导，并且嵌入空间中的低通和高通滤波器能够自然地解耦低级外观和高级物理/语义线索。在此基础上，他们提出了DiT-Mem，一个可学习的记忆编码器，由堆叠的3D CNN、低/高通滤波器和自注意力层组成。该编码器将参考视频映射为一组紧凑的记忆令牌，这些令牌作为记忆连接到DiT的自注意力层中。训练时，扩散主干保持冻结，仅优化DiT-Mem编码器，实现了高效的训练。", "result": "该方法在少量训练参数（1.5亿）和1万个数据样本上实现了高效的训练过程，并在推理时支持即插即用。在最先进的模型上进行的广泛实验证明了该方法在改善物理规则遵循和视频保真度方面的有效性。", "conclusion": "DiT-Mem成功地为扩散Transformer视频生成模型配备了一个即插即用的记忆模块，有效地注入了有用的世界知识，从而显著提高了模型对物理规则的遵循能力和视频的保真度。"}}
{"id": "2511.19221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19221", "abs": "https://arxiv.org/abs/2511.19221", "authors": ["Jianhua Han", "Meng Tian", "Jiangtong Zhu", "Fan He", "Huixin Zhang", "Sitong Guo", "Dechang Zhu", "Hao Tang", "Pei Xu", "Yuze Guo", "Minzhe Niu", "Haojie Zhu", "Qichao Dong", "Xuechao Yan", "Siyuan Dong", "Lu Hou", "Qingqiu Huang", "Xiaosong Jia", "Hang Xu"], "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving", "comment": null, "summary": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.", "AI": {"tldr": "本文提出Percept-WAM，一个感知增强的世界感知-动作模型，首次在一个视觉-语言模型(VLM)中隐式整合2D/3D场景理解能力，通过统一的感知令牌和网格条件预测机制，显著提升了自动驾驶中的感知和规划性能，尤其是在长尾和复杂场景下。", "motivation": "自动驾驶严重依赖准确鲁棒的空间感知，但现有视觉-语言模型在空间定位和理解方面表现薄弱，导致VLA系统在长尾场景和复杂交互中感知和定位能力有限，易出现故障。", "method": "Percept-WAM通过以下方法解决问题：1) 首次在一个VLM中隐式整合2D/3D场景理解能力。2) 将2D/3D感知任务统一为World-PV和World-BEV令牌，编码空间坐标和置信度。3) 提出一种用于密集目标感知的网格条件预测机制，结合IoU感知评分和并行自回归解码，提高长尾、远距离和小目标场景的稳定性。4) 利用预训练VLM参数保持通用智能。5) 直接输出感知结果和轨迹控制。", "result": "实验结果显示，Percept-WAM在下游感知基准上匹配或超越了传统检测器和分割器，在COCO 2D检测和nuScenes BEV 3D检测上分别达到51.7/58.9 mAP。结合轨迹解码器后，进一步提升了nuScenes和NAVSIM上的规划性能，例如在NAVSIM上PMDS指标超越DiffusionDrive 2.1。定性结果也突显了其强大的开放词汇和长尾泛化能力。", "conclusion": "Percept-WAM通过在一个统一的视觉-语言模型中创新性地整合2D/3D场景理解能力，并采用专门的感知令牌和预测机制，有效解决了自动驾驶中空间感知的挑战，在感知和规划任务上均取得了显著提升，尤其在复杂和长尾场景中表现出强大的泛化能力。"}}
{"id": "2511.19180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19180", "abs": "https://arxiv.org/abs/2511.19180", "authors": ["Mansur Ozaman"], "title": "Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification", "comment": "4 figures", "summary": "One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19183", "abs": "https://arxiv.org/abs/2511.19183", "authors": ["Carsten T. Lüth", "Jeremias Traub", "Kim-Celine Kahl", "Till J. Bungert", "Lukas Klein", "Lars Krämer", "Paul F. Jaeger", "Fabian Isensee", "Klaus Maier-Hein"], "title": "nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation", "comment": "Accepted at TMLR", "summary": "Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19187", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19187", "abs": "https://arxiv.org/abs/2511.19187", "authors": ["Nithira Jayarathne", "Naveen Basnayake", "Keshawa Jayasundara", "Pasindu Dodampegama", "Praveen Wijesinghe", "Hirushika Pelagewatta", "Kavishka Abeywardana", "Sandushan Ranaweera", "Chamira Edussooriya"], "title": "SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection", "comment": "4 pages, 3 figures", "summary": "Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19202", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.19202", "abs": "https://arxiv.org/abs/2511.19202", "authors": ["Brent Zoomers", "Florian Hahlbohm", "Joni Vanherck", "Lode Jorissen", "Marcus Magnor", "Nick Michiels"], "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting", "comment": "15 pages, 13 figures", "summary": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.", "AI": {"tldr": "本文提出一种新颖的方法，通过学习高斯体的视点依赖可见性函数，为3D高斯溅射引入遮挡剔除，显著提高了大型复杂场景的渲染性能和图像质量。", "motivation": "3D高斯溅射（3D Gaussian Splatting）虽能利用视锥剔除和LOD策略加速渲染，但其半透明特性阻碍了高效的遮挡剔除技术的应用，限制了其在大规模场景中的渲染效率。", "method": "该方法通过一个小型、共享的多层感知器（MLP）学习场景中所有高斯体的视点依赖可见性函数。在光栅化之前，针对视锥内的高斯体查询该MLP以剔除被遮挡的图元。这些神经查询被直接集成到一个利用Tensor Cores进行高效计算的新型实例化软件光栅器中。", "result": "该方法在组合场景中，结合了实例化光栅器和遮挡剔除MLP，在显存使用量和图像质量方面超越了当前最先进的技术，并与现有的LOD技术具有互补特性。", "conclusion": "通过引入基于学习的遮挡剔除机制和定制的实例化光栅器，该研究成功克服了3D高斯溅射在复杂场景中缺乏有效遮挡剔除的局限性，显著提升了渲染效率和质量。"}}
{"id": "2511.19301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19301", "abs": "https://arxiv.org/abs/2511.19301", "authors": ["Johannes Meier", "Florian Günther", "Riccardo Marin", "Oussema Dhaouadi", "Jacques Kaiser", "Daniel Cremers"], "title": "IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection", "comment": null, "summary": "Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.\n  To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.", "AI": {"tldr": "本文提出IDEAL-M3D，首个针对单目3D检测的实例级主动学习方法，通过多样性驱动的集成学习，解决了传统方法中图像级选择的低效性和深度模糊导致的偏向性问题，显著节省了标注资源并提升了性能。", "motivation": "单目3D检测部署简便但需要大量昂贵的3D标注。现有主动学习方法存在两个局限：1) 选择整个图像进行标注效率低下，因为其中包含非信息性实例；2) 基于不确定性的选择会偏向深度模糊的远距离物体，而忽略近距离物体。", "method": "本文提出了IDEAL-M3D，首个用于单目3D检测的实例级主动学习管线。首次证明了显式多样化、快速训练的集成模型能改进单目3D的多样性驱动主动学习。通过异构骨干网络和与任务无关的特征、损失权重扰动以及时间依赖的套袋法来引入多样性。", "result": "IDEAL-M3D展示了卓越的性能和显著的资源节省：仅使用60%的标注，就在KITTI验证集和测试集上取得了与使用整个数据集训练相同检测器相当或更好的AP3D性能。", "conclusion": "IDEAL-M3D通过实例级选择和多样性驱动的集成学习，有效解决了单目3D检测中主动学习的局限性，实现了在有限标注预算下最大化性能的目标，是资源高效的单目3D检测的有效方案。"}}
{"id": "2511.19294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19294", "abs": "https://arxiv.org/abs/2511.19294", "authors": ["Phurtivilai Patt", "Leyang Huang", "Yinqiang Zhang", "Yang Lei"], "title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting", "comment": null, "summary": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.", "AI": {"tldr": "本文提出了一种名为“预先密集化”的新方法，通过结合激光雷达和单目深度估计来初始化3D高斯飞溅（3DGS）场景，以避免自适应密度控制带来的浮动伪影和资源浪费，从而提高视觉保真度、计算效率并显著降低资源消耗和训练时间。", "motivation": "现有3D高斯飞溅（3DGS）方法依赖自适应密度控制，这会导致浮动伪影和低效的资源使用。", "method": "提出了一种“预先密集化”方法。它结合稀疏激光雷达数据和单目深度估计（来自对应的RGB图像）来增强3D场景的初始化。通过ROI感知的采样方案，优先处理语义和几何上重要的区域，生成一个密集的点云。这种方法绕过了原始管线中可能引入冗余高斯的自适应密度控制，使优化能更专注于3D高斯基元的其他属性，减少重叠并提升视觉质量。", "result": "该方法在保持与最先进技术相当的结果的同时，显著降低了资源消耗和训练时间。在四个新收集的数据集上进行了广泛比较和消融研究，证明了其在复杂场景中保留感兴趣区域的有效性。", "conclusion": "“预先密集化”方法有效解决了3DGS的局限性，通过改进初始化过程，减少了伪影和资源消耗，同时保持了高视觉质量，并能有效保留复杂场景中的感兴趣区域。"}}
{"id": "2511.19217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19217", "abs": "https://arxiv.org/abs/2511.19217", "authors": ["Wanjiang Weng", "Xiaofeng Tan", "Junbo Wang", "Guo-Sen Xie", "Pan Zhou", "Hongsong Wang"], "title": "ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment", "comment": "Accepted by AAAI 2026", "summary": "Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.", "AI": {"tldr": "为解决扩散模型中文本到动作生成存在的文本-动作错位问题，本文提出了ReAlign方法。该方法包含一个步长感知奖励模型和奖励引导采样策略，显著提升了文本-动作对齐和动作质量。", "motivation": "文本到动作生成在游戏、电影和机器人等领域潜力巨大。尽管扩散模型能生成多样且真实的动作，但其文本与动作分布之间存在错位，导致生成的动作语义不一致或质量低下。", "method": "本文提出了ReAlign（Reward-guided sampling Alignment）方法，包含：1) 一个步长感知奖励模型，用于在去噪采样过程中评估对齐质量；2) 一个奖励引导策略，将扩散过程导向最优对齐分布。该奖励模型整合了步长感知令牌，并结合了文本对齐模块（用于语义一致性）和动作对齐模块（用于真实感），在每个时间步精炼噪声动作，以平衡概率密度和对齐。", "result": "在动作生成和检索任务上的大量实验表明，与现有最先进的方法相比，ReAlign显著改善了文本-动作对齐和动作质量。", "conclusion": "ReAlign方法通过引入步长感知奖励模型和奖励引导策略，有效解决了扩散模型在文本到动作生成中的文本-动作错位问题，从而生成了更佳对齐和更高质量的动作。"}}
{"id": "2511.19261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19261", "abs": "https://arxiv.org/abs/2511.19261", "authors": ["Shuai Wang", "Daoan Zhang", "Tianyi Bai", "Shitong Shao", "Jiebo Luo", "Jiaheng Wei"], "title": "LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models", "comment": null, "summary": "Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19254", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19254", "abs": "https://arxiv.org/abs/2511.19254", "authors": ["Mohamed Rissal Hedna", "Sesugh Samuel Nder"], "title": "Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation", "comment": "9 pages, 5 figures, 1 algorithm", "summary": "Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19306", "abs": "https://arxiv.org/abs/2511.19306", "authors": ["Zixuan Wang", "Haoran Sun", "Jiaming Lu", "Wenxuan Wang", "Zhongling Huang", "Dingwen Zhang", "Xuelin Qian", "Junwei Han"], "title": "Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection", "comment": "10 pages, 2 figures", "summary": "Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.", "AI": {"tldr": "本文提出DGSPNet，一个端到端的语言提示驱动框架，用于红外小目标检测。它通过双粒度语义提示（粗粒度文本先验和细粒度视觉到文本映射）以及文本引导的通道和空间注意力机制，显著提高了检测精度，并减少了对手动标注的依赖。", "motivation": "红外小目标检测面临特征表示有限和背景干扰严重导致性能不佳的挑战。现有受CLIP启发的方法虽然尝试利用文本指导，但受限于不准确的文本描述和对手动标注的依赖。", "method": "提出DGSPNet框架，整合双粒度语义提示：粗粒度文本先验（如“红外图像”、“小目标”）和通过图像空间内视觉到文本映射获得的细粒度个性化语义描述。该方法在推理时无需标注即可利用语言提示。此外，引入文本引导通道注意力（TGCA）和文本引导空间注意力（TGSA）机制，以增强模型对潜在目标的敏感性。", "result": "广泛的实验表明，该方法显著提高了检测精度，并在三个基准数据集上实现了最先进的性能。", "conclusion": "DGSPNet通过充分利用文本描述的精确性和简洁性，有效克服了红外小目标检测中的挑战，提高了检测准确性，并减少了对人工标注的需求。"}}
{"id": "2511.19235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19235", "abs": "https://arxiv.org/abs/2511.19235", "authors": ["Carl Lindström", "Mahan Rafidashti", "Maryam Fatemi", "Lars Hammarstrand", "Martin R. Oswald", "Lennart Svensson"], "title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes", "comment": null, "summary": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.", "AI": {"tldr": "IDSplat是一个自监督的3D高斯泼溅框架，无需人工标注即可重建具有显式实例分解和可学习运动轨迹的动态驾驶场景。", "motivation": "现有动态场景重建方法要么依赖昂贵的人工标注来获取物体轨迹，要么使用缺乏明确物体级分解的时变表示，导致静态和动态元素交织，难以进行场景分离。", "method": "IDSplat将动态物体建模为经历刚性变换的连贯实例。它通过零样本、基于语言的视频跟踪（利用激光雷达锚定到3D）实现实例分解，并通过特征对应估计一致的姿态。引入了一种协调转弯平滑方案，以获得时间上和物理上一致的运动轨迹，减轻姿态错位和跟踪失败。最后，联合优化物体姿态和高斯参数。", "result": "在Waymo开放数据集上的实验表明，IDSplat在保持实例级分解的同时，实现了具有竞争力的重建质量，并且无需重新训练即可泛化到不同的序列和视角密度，使其适用于大规模自动驾驶应用。", "conclusion": "IDSplat提供了一种实用且自监督的方法，用于高保真动态场景重建，具有显式实例分解和可学习轨迹，适用于自动驾驶系统开发中的传感器仿真。"}}
{"id": "2511.19343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19343", "abs": "https://arxiv.org/abs/2511.19343", "authors": ["Qihan Huang", "Haofei Zhang", "Rong Wei", "Yi Wang", "Rui Tang", "Mingli Song", "Jie Song"], "title": "Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning", "comment": null, "summary": "RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.", "AI": {"tldr": "现有用于多模态大语言模型（MLLM）感知的强化学习（RL）方法面临数据质量低的问题，限制了探索。本文提出了Syn-GRPO，通过在线数据生成器合成高质量、多样化响应的训练数据，显著提升了MLLM的感知性能。", "motivation": "RL方法在MLLM感知中表现出良好的泛化能力，但现有方法存在数据质量低的问题，即数据样本无法引出MLLM的多样化响应，从而限制了强化学习的探索范围。虽然一些方法尝试通过熵约束来缓解，但未能从根本上解决问题。", "method": "本文提出了Syn-GRPO（Synthesis-GRPO），它在GRPO训练中利用在线数据生成器合成具有多样化响应的高质量训练数据。Syn-GRPO包含两个核心组件：1) 数据服务器，使用图像生成模型从现有样本中合成新样本，采用解耦和异步方案实现高生成效率；2) GRPO工作流，为数据服务器提供新的图像描述，并利用多样性奖励监督MLLM预测图像描述，以合成具有多样化响应的样本。", "result": "在三个视觉感知任务上的实验结果表明，Syn-GRPO大幅提高了数据质量，实现了显著优于现有MLLM感知方法的性能。Syn-GRPO展示了在扩展长期自演化RL方面的巨大潜力。", "conclusion": "Syn-GRPO通过引入在线数据生成器有效解决了MLLM强化学习中数据质量低的问题，显著提升了MLLM的感知能力，并为长期自演化强化学习提供了有前景的方向。"}}
{"id": "2511.19274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19274", "abs": "https://arxiv.org/abs/2511.19274", "authors": ["Mingyang Chen", "Jiawei Du", "Bo Huang", "Yi Wang", "Xiaobo Zhang", "Wei Wang"], "title": "Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection", "comment": "Accepted by AAAI 2026", "summary": "Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.", "AI": {"tldr": "本文提出了一种新颖的、基于扩散模型的数据核心集选择方法，通过部分逆向去噪引起的重建偏差来估计数据似然，并在ImageNet上显著优于现有基线。", "motivation": "现有的核心集选择方法主要依赖启发式信号（如训练动态或模型不确定性），缺乏对数据似然的明确建模，这可能导致选出的子集未能有效捕捉数据分布的关键结构，从而影响模型训练效果。", "method": "本文提出了一种理论基础扎实的方法：利用扩散模型通过部分逆向去噪引起的重建偏差来估计数据似然。具体而言，它建立了重建误差与数据似然之间的正式联系，该联系基于马尔可夫扩散过程的证据下界（ELBO）。此外，还引入了一种高效的信息论方法来确定最佳重建时间步，以确保偏差信号能可靠地指示潜在数据似然。", "result": "在ImageNet上的大量实验表明，重建偏差是一种有效的评分标准，在不同选择比例下始终优于现有基线，并且仅使用50%的数据就能达到与全数据训练相近的性能。进一步分析显示，这种似然感知的分数揭示了数据选择中的有益见解，阐明了数据分布特征与模型学习偏好之间的相互作用。", "conclusion": "通过扩散模型利用重建偏差估计数据似然，本文提出了一种有原则的、分布感知的数据选择评分标准。该方法在性能上显著超越了现有技术，并为理解数据分布特性与模型学习之间的关系提供了新的视角。"}}
{"id": "2511.19268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19268", "abs": "https://arxiv.org/abs/2511.19268", "authors": ["Dewei Zhou", "Mingwei Li", "Zongxin Yang", "Yu Lu", "Yunqiu Xu", "Zhizhong Wang", "Zeyi Huang", "Yi Yang"], "title": "BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment", "comment": "29 pages", "summary": "Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19356", "abs": "https://arxiv.org/abs/2511.19356", "authors": ["Rui Li", "Yuanzhi Liang", "Ziqi Ni", "Haibing Huang", "Chi Zhang", "Xuelong Li"], "title": "Growing with the Generator: Self-paced GRPO for Video Generation", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.", "AI": {"tldr": "现有GRPO视频生成模型面临静态奖励模型的局限，本研究提出自步GRPO，通过奖励机制与生成器共同演进，动态调整奖励焦点，从而提升生成质量和稳定性。", "motivation": "现有的GRPO（Group Relative Policy Optimization）在视频生成后训练中，依赖静态、固定容量的奖励模型。这种僵化的奖励机制导致分布偏差，随着生成器改进快速饱和，并最终限制了基于强化学习对齐的稳定性和有效性。", "method": "本研究提出了自步GRPO（Self-Paced GRPO），一个能力感知型GRPO框架，其中奖励反馈与生成器共同演进。该方法引入了一种渐进式奖励机制，随着生成质量的提高，奖励的侧重点会自动从粗略的视觉保真度转向时间连贯性和细粒度的文本-视频语义对齐。", "result": "在VBench上，跨多个视频生成骨干网络的实验表明，与使用静态奖励的GRPO基线相比，自步GRPO在视觉质量和语义对齐方面均取得了持续改进，验证了其有效性和通用性。", "conclusion": "自步GRPO通过缓解奖励-策略不匹配、减轻奖励利用问题并实现更稳定的优化，有效提高了视频生成的视觉质量和语义对齐，证明了其在强化学习对齐视频生成中的有效性和普适性。"}}
{"id": "2511.19319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19319", "abs": "https://arxiv.org/abs/2511.19319", "authors": ["Lingwei Dang", "Zonghan Li", "Juntong Li", "Hongwen Zhang", "Liang An", "Yebin Liu", "Qingyao Wu"], "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis", "comment": "Project Page: https://droliven.github.io/SyncMV4D", "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.", "AI": {"tldr": "SyncMV4D是首个联合生成同步多视角手物交互（HOI）视频和4D运动的模型，通过融合视觉先验、运动动态和多视角几何，解决了现有方法的局限性。", "motivation": "现有基于视频的单视角方法缺乏全面的3D几何感知，导致几何失真或不真实的运动模式。而3D HOI方法依赖高质量的实验室3D数据，限制了其在真实世界场景中的泛化能力。", "method": "本文提出了SyncMV4D框架，包含两项核心创新：1) 多视角联合扩散（MJD）模型，协同生成HOI视频和中间运动；2) 扩散点对齐器（DPA），将粗糙的中间运动细化为全局对齐的4D度量点轨迹。该框架通过一个闭环、相互增强的循环紧密耦合2D外观与4D动态，即生成视频调节4D运动的细化，而对齐的4D点轨迹被重新投影以指导下一步的联合生成。", "result": "实验结果表明，SyncMV4D在视觉真实感、运动合理性和多视角一致性方面优于现有最先进的方法。", "conclusion": "SyncMV4D通过联合生成同步多视角HOI视频和4D运动，并统一视觉、运动和几何信息，有效克服了现有方法的局限性，在真实感和一致性方面表现出色。"}}
{"id": "2511.19278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19278", "abs": "https://arxiv.org/abs/2511.19278", "authors": ["Qianying Liu", "Xiao Liang", "Zhiqiang Zhang", "Yibo Chen", "Xu Tang", "Zhongfei Qing", "Fengfan Zhou", "Yao Hu", "Paul Henderson"], "title": "ReMatch: Boosting Representation through Matching for Multimodal Retrieval", "comment": null, "summary": "We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19401", "abs": "https://arxiv.org/abs/2511.19401", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "In-Video Instructions: Visual Signals as Generative Control", "comment": null, "summary": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.", "AI": {"tldr": "本文提出“视频内指令”范式，通过将视觉信号（如叠加文本、箭头、轨迹）直接嵌入帧中作为指令，实现可控的图像到视频生成，优于传统文本提示。", "motivation": "大规模视频生成模型已展现出强大的视觉能力，能预测符合逻辑和物理线索的未来帧。研究旨在探索如何利用这些能力，通过解释帧内嵌入的视觉信号作为指令，实现可控的图像到视频生成，以克服基于文本提示的全局性和粗糙性限制。", "method": "提出“视频内指令”范式，将用户引导直接编码到视觉域中，通过在帧内添加叠加文本、箭头或轨迹等视觉元素作为指令。这种方法为视觉主体及其预期动作之间提供了明确、空间感知且无歧义的对应关系。该方法在Veo 3.1、Kling 2.5和Wan 2.2等三个最先进的生成器上进行了广泛实验。", "result": "实验结果表明，视频模型能够可靠地解释和执行这些视觉嵌入的指令，尤其在复杂的多对象场景中表现出色。", "conclusion": "视频生成模型能够有效解释和执行视觉嵌入的指令，证明“视频内指令”是一种实现可控图像到视频生成的有效范式，尤其适用于需要精确、空间感知控制的复杂场景。"}}
{"id": "2511.19316", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19316", "abs": "https://arxiv.org/abs/2511.19316", "authors": ["Xincheng Wang", "Hanchi Sun", "Wenjun Sun", "Kejun Xue", "Wangqiu Zhou", "Jianbo Zhang", "Wei Sun", "Dandan Zhu", "Xiongkuo Min", "Jun Jia", "Zhijun Fang"], "title": "Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach", "comment": null, "summary": "Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19365", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19365", "abs": "https://arxiv.org/abs/2511.19365", "authors": ["Zehong Ma", "Longhui Wei", "Shuai Wang", "Shiliang Zhang", "Qi Tian"], "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation", "comment": "Project Page: https://zehong-ma.github.io/DeCo. Code Repository: https://github.com/Zehong-Ma/DeCo", "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.", "AI": {"tldr": "本文提出了一种名为频率解耦像素扩散（DeCo）的新框架，通过将高频细节和低频语义的生成解耦，并引入频率感知流匹配损失，显著提高了像素扩散模型的训练和推理效率及性能，缩小了与潜在扩散方法的差距。", "motivation": "现有的像素扩散模型通过单一的Diffusion Transformer（DiT）同时建模高频信号和低频语义，导致训练和推理速度慢，限制了其在端到端、高模型容量生成方面的潜力。", "method": "本文提出了频率解耦像素扩散（DeCo）框架。它利用一个轻量级像素解码器，根据DiT提供的语义指导生成高频细节，从而使DiT专注于建模低频语义。此外，引入了一种频率感知流匹配损失，以强调视觉上显著的频率并抑制不重要的频率。", "result": "DeCo在像素扩散模型中取得了卓越的性能，在ImageNet上达到了1.62（256x256）和2.22（512x512）的FID分数，缩小了与潜在扩散方法的差距。此外，其预训练的文本到图像模型在GenEval系统级比较中获得了0.86的领先总分。", "conclusion": "通过频率解耦生成和引入频率感知损失，DeCo提供了一种更高效的像素扩散范式，在性能上超越了现有像素扩散模型，并有效地缩小了与潜在扩散方法之间的差距。"}}
{"id": "2511.19320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19320", "abs": "https://arxiv.org/abs/2511.19320", "authors": ["Jiaming Zhang", "Shengming Cao", "Rui Li", "Xiaotong Zhao", "Yutao Cui", "Xinglin Hou", "Gangshan Wu", "Haolan Chen", "Yu Xu", "Limin Wang", "Kai Ma"], "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation", "comment": "10 pages, with supp", "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19425", "abs": "https://arxiv.org/abs/2511.19425", "authors": ["Tianrun Chen", "Runlong Cao", "Xinda Yu", "Lanyun Zhu", "Chaotao Ding", "Deyi Ji", "Cheng Chen", "Qi Zhu", "Chunyan Xu", "Papa Mao", "Ying Zang"], "title": "SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation", "comment": null, "summary": "The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.", "AI": {"tldr": "SAM3-Adapter是首个为Segment Anything 3 (SAM3)设计的适配器框架，旨在解决SAM和SAM2在精细、低级分割任务（如伪装物体检测、医学图像分割、阴影检测）上的不足，显著提升了分割精度和效率，并在多项任务上取得了最先进的成果。", "motivation": "尽管大型基础模型如Segment Anything在图像分割中展现出前所未有的通用性，但包括SAM及其后续版本在内的前几代模型，在伪装物体检测、医学图像分割、细胞图像分割和阴影检测等精细、低级分割任务上仍存在局限。随着更高效、更高性能的SAM3的出现，研究者希望利用其优势来解决这些长期存在的挑战。", "method": "本文提出了SAM3-Adapter，这是第一个专门为SAM3量身定制的适配器框架。该框架基于原始SAM-Adapter的模块化和可组合设计理念，旨在解锁SAM3的全部分割能力，同时减少计算开销。", "result": "SAM3-Adapter不仅降低了计算开销，而且持续超越了基于SAM和SAM2的解决方案，在医学成像、伪装物体分割和阴影检测等多个下游任务中建立了新的最先进成果。实验证明，将SAM3与SAM3-Adapter集成后，相比所有先前的基于SAM的适配方案，能提供卓越的准确性、鲁棒性和效率。", "conclusion": "SAM3-Adapter成功解决了SAM和SAM2在精细分割任务上的局限，通过适配SAM3，显著提升了分割性能、通用性和任务适应性。该工作为未来的研究和实际分割应用奠定了坚实的基础，并有望推动相关领域的发展。"}}
{"id": "2511.19339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19339", "abs": "https://arxiv.org/abs/2511.19339", "authors": ["Anjie Le", "Can Peng", "Yuyuan Liu", "J. Alison Noble"], "title": "POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse", "comment": null, "summary": "In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.", "AI": {"tldr": "本文提出了一种在表示层面进行机器遗忘的新方法POUR，通过几何投影实现对特定视觉概念或训练图像的有效遗忘，同时保留其他知识，并优于现有技术。", "motivation": "现有机器遗忘方法通常只修改分类器，而内部表示保持不变，导致遗忘不彻底。这促使研究人员探索如何在表示层面实现更彻底的遗忘。", "method": "研究将遗忘概念扩展到表示层面，推导了遗忘效率、保留保真度和类别分离之间的三项相互作用。基于神经坍缩理论，证明了单形等角紧框架（ETF）的正交投影在低维空间中仍是ETF，从而得到一个可证明最优的遗忘算子。引入了表示遗忘分数（RUS）来量化表示层面的遗忘和保留保真度。在此基础上，提出了POUR（表示的可证明最优遗忘），包括闭式几何投影方法（POUR-P）和基于蒸馏的特征级遗忘变体（POUR-D）。", "result": "在CIFAR-10/100和PathMNIST数据集上的实验表明，POUR在有效遗忘的同时能保留现有知识，并且在分类级别和表示级别的指标上均优于最先进的遗忘方法。", "conclusion": "POUR是一种可证明最优的几何投影方法，能够有效且彻底地在表示层面实现机器遗忘，同时保持对保留知识的忠实性，解决了现有方法遗忘不彻底的问题。"}}
{"id": "2511.19434", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19434", "abs": "https://arxiv.org/abs/2511.19434", "authors": ["Yasin Esfandiari", "Stefan Bauer", "Sebastian U. Stich", "Andrea Dittadi"], "title": "Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts", "comment": "ICLR 2025 DeLTa workshop", "summary": "Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.", "AI": {"tldr": "扩散模型在图像质量和数据似然之间存在权衡。本文提出一种即插即用的采样方法，通过在去噪轨迹上切换两个预训练专家模型（高噪声时使用图像质量专家，低噪声时使用似然专家），从而在不重新训练的情况下，同时提升或保持图像质量和数据似然。", "motivation": "图像生成扩散模型在感知样本质量和数据似然之间存在固有的权衡：侧重高噪声去噪的训练目标能生成逼真图像但似然性差，而侧重似然的训练则会过分强调低噪声步骤并损害视觉保真度。", "method": "引入一种简单的即插即用采样方法，通过在去噪轨迹上切换两个预训练的扩散专家来组合它们。具体来说，在高噪声水平下应用图像质量专家以塑造全局结构，然后在低噪声水平下切换到似然专家以细化像素统计。该方法无需重新训练或微调，只需选择一个中间切换步骤。", "result": "在CIFAR-10和ImageNet32数据集上，合并模型始终匹配或优于其基础组件，相对于单独的每个专家，它在似然和样本质量方面都有所改进或保持。这些结果表明，跨噪声水平的专家切换是打破图像扩散模型中似然-质量权衡的有效方法。", "conclusion": "跨噪声水平的专家切换是打破图像扩散模型中似然-质量权衡的有效方法。"}}
{"id": "2511.19326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19326", "abs": "https://arxiv.org/abs/2511.19326", "authors": ["Farnoosh Koleini", "Hongfei Xue", "Ahmed Helmy", "Pu Wang"], "title": "MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation", "comment": null, "summary": "Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.", "AI": {"tldr": "MonoMSK是一个混合框架，结合数据驱动学习和物理仿真，从单目视频中重建生物力学真实的3D人体运动（运动学和动力学），并首次实现精确的单目动力学估计。", "motivation": "当前方法在重建生物力学真实的3D人体运动方面存在局限性：基于标记的系统受限于实验室且效率低下；流行的单目方法使用过于简化的模型（如SMPL）并忽略物理原理，导致生物力学保真度不足。", "method": "MonoMSK提出一个混合框架，利用解剖学上精确的肌肉骨骼模型，通过整合基于Transformer的逆动力学与可微分的正向运动学和动力学层（由ODE仿真控制），建立一个物理调节的逆-正向循环。该框架还引入了一种新颖的正向-逆向一致性损失，以确保运动重建与底层动力学推理保持一致。", "result": "在BML-MoVi、BEDLAM和OpenCap数据集上的实验表明，MonoMSK在运动学精度上显著优于现有最先进的方法，并且首次实现了精确的单目动力学估计。", "conclusion": "MonoMSK通过结合数据驱动学习和物理仿真，为从单目视频中估计生物力学真实的3D人体运动和动力学提供了一个创新且有效的方法，克服了现有技术的局限性。"}}
{"id": "2511.19351", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19351", "abs": "https://arxiv.org/abs/2511.19351", "authors": ["Abdurahman Ali Mohammed", "Catherine Fonder", "Ying Wei", "Wallapak Tavanapong", "Donald S Sakaguchi", "Qi Li", "Surya K. Mallapragada"], "title": "CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting", "comment": "The IEEE International Conference on Data Mining (ICDM) 2025", "summary": "Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19367", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19367", "abs": "https://arxiv.org/abs/2511.19367", "authors": ["Saniah Kayenat Chowdhury", "Rusab Sarmun", "Muhammad E. H. Chowdhury", "Sohaib Bassam Zoghoul", "Israa Al-Hashimi", "Adam Mushtak", "Amith Khandakar"], "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification", "comment": null, "summary": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19435", "abs": "https://arxiv.org/abs/2511.19435", "authors": ["Zechuan Zhang", "Zhenyuan Chen", "Zongxin Yang", "Yi Yang"], "title": "Are Image-to-Video Models Good Zero-Shot Image Editors?", "comment": "technical report", "summary": "Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.", "AI": {"tldr": "IF-Edit是一个免调优框架，它将预训练的图像到视频扩散模型重新用于指令驱动的图像编辑，解决了提示错位、冗余时间潜在和后期帧模糊等挑战，并在推理任务上表现出色。", "motivation": "大规模视频扩散模型展示了强大的世界模拟和时间推理能力，但它们作为零样本图像编辑器的潜力尚未得到充分探索。", "method": "IF-Edit框架包含三个核心组件：1) 一个思维链提示增强模块，将静态编辑指令转换为时间基础的推理提示；2) 一个时间潜在丢弃策略，在专家切换点后压缩帧潜在，加速去噪同时保持语义和时间连贯性；3) 一个自洽的后精修步骤，使用短静止视频轨迹锐化后期帧。", "result": "在四个公共基准测试（涵盖非刚性编辑、物理和时间推理以及通用指令编辑）上的实验表明，IF-Edit在以推理为中心的任务上表现强劲，同时在通用编辑方面也具有竞争力。", "conclusion": "本研究提供了一个将视频扩散模型视为图像编辑器的系统视角，并提出了一个用于统一视频-图像生成推理的简单方法。"}}
{"id": "2511.19436", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.19436", "abs": "https://arxiv.org/abs/2511.19436", "authors": ["Qiang Wang", "Xinyuan Gao", "SongLin Dong", "Jizhou Han", "Jiangyang Li", "Yuhang He", "Yihong Gong"], "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection", "comment": null, "summary": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.", "AI": {"tldr": "VDC-Agent是一个用于视频详细字幕的自演化框架，无需人工标注或大型教师模型，通过生成、评分和细化闭环，自动构建数据集VDC-Agent-19K，并在此基础上微调多模态大语言模型，在VDC基准测试中达到了最先进的性能。", "motivation": "研究动机是开发一种视频详细字幕方法，该方法不需要人工标注或依赖更大的教师模型，以降低成本和复杂性。", "method": "本文提出了VDC-Agent，一个自演化框架，其核心是一个生成、原则指导评分（提供分数和文本建议）和提示词细化的闭环。当字幕质量下降时，通过自我反思路径修正更新。该过程在未标注视频上生成(字幕, 分数)轨迹，并将其转换为偏好元组，过滤后形成VDC-Agent-19K数据集（18,886对）。最后，使用由易到难的课程直接偏好优化方法，基于Qwen2.5-VL-7B-Instruct微调基础多模态大语言模型。", "result": "VDC-Agent-7B在VDC基准测试中取得了最先进的性能，平均准确率为49.08%，分数为2.50。它超越了专业的视频字幕模型，并且相对于基础模型，在相似的推理成本下，准确率提高了5.13%，分数提高了0.27。", "conclusion": "VDC-Agent是一个高效的自演化框架，能够在不依赖人工标注或大型教师模型的情况下，实现视频详细字幕的SOTA性能。其自动构建数据集和偏好优化微调策略被证明是成功的。"}}
{"id": "2511.19394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19394", "abs": "https://arxiv.org/abs/2511.19394", "authors": ["Rachit Saluja", "Asli Cihangir", "Ruining Deng", "Johannes C. Paetzold", "Fengbei Liu", "Mert R. Sabuncu"], "title": "BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation", "comment": null, "summary": "Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single \"background\" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.\n  In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19418", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19418", "abs": "https://arxiv.org/abs/2511.19418", "authors": ["Yiming Qin", "Bomin Wei", "Jiaxin Ge", "Konstantinos Kallidromitis", "Stephanie Fu", "Trevor Darrell", "Xudong Wang"], "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens", "comment": "Project page: https://wakalsprojectpage.github.io/comt-website/", "summary": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.", "AI": {"tldr": "COVT框架通过引入连续视觉令牌，增强了视觉-语言模型（VLM）的密集视觉感知能力，从而提升了空间和几何推理性能。", "motivation": "当前的视觉-语言模型在语言空间推理方面表现出色，但在需要密集视觉感知的任务（如空间推理和几何感知）上表现不佳，原因是它们捕获空间维度上密集视觉信息的机制有限。", "method": "本文提出了视觉思维链（COVT）框架，使VLM不仅能通过文字，还能通过连续视觉令牌（编码丰富感知线索的紧凑潜在表示，约20个令牌）进行推理。COVT从轻量级视觉专家中提取知识，捕捉2D外观、3D几何、空间布局和边缘结构等互补属性。训练时，VLM自回归预测这些视觉令牌以重建密集监督信号（如深度、分割、边缘和DINO特征）。推理时，模型直接在连续视觉令牌空间中进行推理，同时保持效率，并可选择解码密集预测以提高可解释性。", "result": "在CV-Bench、MMVP、RealWorldQA、MMStar、WorldMedQA和HRBench等十多个多样化感知基准上进行评估，将COVT集成到Qwen2.5-VL和LLaVA等强大的VLM中，性能持续提升3%至16%。", "conclusion": "紧凑的连续视觉思维使VLM能够实现更精确、更具基础性和更可解释的多模态智能。"}}
{"id": "2511.19380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19380", "abs": "https://arxiv.org/abs/2511.19380", "authors": ["Maroun Ayli", "Youssef Bakouny", "Tushar Sharma", "Nader Jalloul", "Hani Seifeddine", "Rima Kilany"], "title": "UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval", "comment": "12 pages, 2 figures, 3 algorithms, 4 tables", "summary": "Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.", "AI": {"tldr": "本文提出了一种新颖的基于图的UI表示方法，将UI截图转换为属性图，并通过对比图自编码器学习结构嵌入。该方法在UI搜索框架UISearch中实现了高精度和低延迟，解决了企业软件UI设计一致性和模式发现的挑战。", "motivation": "企业软件公司面临UI屏幕数量庞大、版本众多带来的设计一致性、模式发现和合规性检查等严峻挑战。现有方法依赖视觉相似性或文本语义，缺乏对UI构成中结构属性的明确建模。", "method": "研究者提出了一种将UI截图转换为属性图的新型图表示方法，编码了层级关系和空间布局。利用对比图自编码器学习嵌入，以保留视觉、结构和语义属性的多层次相似性。并将此表示实现于UISearch，一个结合结构嵌入和语义搜索的多模态搜索框架。", "result": "结构嵌入比最先进的视觉编码器具有更好的判别能力，显著提升了UI表示的表达力。UISearch在20,396个金融软件UI上实现了0.92的Top-5准确率，中位延迟为47.5毫秒，并能扩展到20,000多个屏幕。混合索引架构支持复杂查询和细粒度UI区分，这是纯视觉方法无法实现的。", "conclusion": "所提出的图基表示和结构嵌入在UI表示的表达能力方面取得了根本性进展，显著提升了UI分析和搜索的效率与精度。UISearch框架通过结合结构和语义信息，为企业软件UI的管理和分析提供了一个强大且可扩展的解决方案。"}}
{"id": "2511.19426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19426", "abs": "https://arxiv.org/abs/2511.19426", "authors": ["Yun Zhou", "Yaoting Wang", "Guangquan Jie", "Jinyu Liu", "Henghui Ding"], "title": "Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction", "comment": "Code: https://github.com/FudanCVL/Ref-SAM3D", "summary": "SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19431", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.19431", "abs": "https://arxiv.org/abs/2511.19431", "authors": ["Jacob Lin", "Edward Gryspeerdt", "Ronald Clark"], "title": "Cloud4D", "comment": "NeurIPS 2025 Spotlight, project page: https://cloud4d.jacob-lin.com/", "summary": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19430", "abs": "https://arxiv.org/abs/2511.19430", "authors": ["Dingkang Liang", "Cheng Zhang", "Xiaopeng Xu", "Jianzhong Ju", "Zhenbo Luo", "Xiang Bai"], "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution", "comment": "Accepted to AAAI 2026 (Oral). The code is available at \\url{https://github.com/H-EmbodVis/GRANT}", "summary": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT", "AI": {"tldr": "处理失败", "motivation": "处理失败", "method": "处理失败", "result": "处理失败", "conclusion": "处理失败"}}
{"id": "2511.19437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19437", "abs": "https://arxiv.org/abs/2511.19437", "authors": ["Jingzhi Bao", "Hongze Chen", "Lingting Zhu", "Chenyu Liu", "Runze Zhang", "Keyang Luo", "Zeyu Hu", "Weikai Chen", "Yingda Yin", "Xin Wang", "Zehong Lin", "Jun Zhang", "Xiaoguang Han"], "title": "LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context", "comment": "Project page: https://lumitex.vercel.app", "summary": "Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.", "AI": {"tldr": "LumiTex是一个端到端的框架，用于从图像提示生成PBR纹理，解决了在有限光照下进行材质分解以及无缝、视图一致纹理补全的挑战，并实现了最先进的性能。", "motivation": "现有方法在以下两个基本挑战上表现不足：1) 在有限光照线索下从图像提示中进行材质分解；2) 实现无缝且视图一致的纹理补全。", "method": "LumiTex包含三个关键组件：1) 一个多分支生成方案，在共享光照先验下解耦反照率和金属度-粗糙度，以实现鲁棒的材质理解；2) 一个光照感知材质注意力机制，将光照上下文注入解码过程，以物理基础生成反照率、金属度和粗糙度贴图；3) 一个基于大型视图合成模型的几何引导修复模块，用于丰富纹理覆盖并确保无缝、视图一致的UV补全。", "result": "LumiTex在纹理质量方面达到了最先进的性能，超越了现有开源和商业方法。", "conclusion": "LumiTex成功解决了PBR纹理生成中的核心挑战，通过其创新的多组件框架，在纹理质量和一致性上取得了卓越的成果。"}}
{"id": "2511.17643", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17643", "abs": "https://arxiv.org/abs/2511.17643", "authors": ["Yayan Qiu", "Sean Hanna"], "title": "Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?", "comment": null, "summary": "Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics.", "AI": {"tldr": "本研究证明了pix2pix GAN能够自动学习空间拓扑关系，并提出了一种基于Grasshopper的快速检测方法来评估其性能，填补了图像生成GAN拓扑性能检测的空白。", "motivation": "在建筑设计和城市更新中，考虑空间内在和外在属性的区域特征至关重要，但现有基于图像和图的GAN方法在模型嵌套和数据转换过程中可能导致信息丢失。因此，需要简化工具并使GAN能够自主识别拓扑关系，以方便建筑师和用户参与设计。", "method": "本研究提出了一种快速检测pix2pix学习拓扑关系能力的方法。具体通过在GAN前后添加两个基于Grasshopper的检测模块来实现。同时，提供了定量数据，可视化了学习过程，并分析了灰度图和RGB等不同输入模式对其学习效率的影响。", "result": "研究结果证明了pix2pix能够自动学习空间拓扑关系并将其应用于建筑设计。此外，该研究提出的检测方法耗时短、操作简单，其两个检测模块可广泛用于定制具有相同拓扑结构的图像数据集和批量检测图像的拓扑关系。", "conclusion": "本研究证明了pix2pix在学习空间拓扑关系方面的潜力，并提供了一种高效的检测方法。未来，这项工作有望为利用GAN在建筑设计和城市更新中保留空间拓扑特征提供理论基础和数据支持。"}}
