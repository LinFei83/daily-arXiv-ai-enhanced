{"id": "2508.10318", "categories": ["eess.SY", "cs.SY", "90B25, 90C35, 62P30, 93C95"], "pdf": "https://arxiv.org/pdf/2508.10318", "abs": "https://arxiv.org/abs/2508.10318", "authors": ["Huangbin Liang", "Beatriz Moya", "Francisco Chinesta", "Eleni Chatzi"], "title": "Quantifying the Value of Seismic Structural Health Monitoring for post-earthquake recovery of electric power system in terms of resilience enhancement", "comment": "21 pages. 14 figures", "summary": "Post-earthquake recovery of electric power networks (EPNs) is critical to\ncommunity resilience. Traditional recovery processes often rely on prolonged\nand imprecise manual inspections for damage diagnosis, leading to suboptimal\nrepair prioritization and extended service disruptions. Seismic Structural\nHealth Monitoring (SSHM) offers the potential to expedite recovery by enabling\nmore accurate and timely damage assessment. However, SSHM deployment incurs\ncosts, and its system-level resilience benefit remains underexplored. This\nstudy proposes a probabilistic simulation framework to quantify the value of\nSSHM for enhancing EPN resilience. The framework includes seismic damage\nmodeling based on network configuration, hazard intensity, fragility functions,\nand damage-functionality mappings, combined with recovery simulations\nincorporating resource constraints, repair and transfer durations. System\nfunctionality is evaluated using graph-based island detection and optimal power\nflow analysis. Resilience is quantified via the Lack of Resilience (LoR) metric\nderived from the functionality restoration curve. SSHM is incorporated by\naltering the quality of damage information used in repair scheduling. Different\nmonitoring scenarios (e.g., no-SSHM baseline, partial SSHM, full SSHM with\nvarious accuracies) are modeled using confusion matrices to simulate damage\nmisclassification. Results show that improved damage awareness via SSHM\nsignificantly accelerates recovery and reduces LoR by up to 21%. This work\nsupports evidence-based decisions for SSHM deployment in critical\ninfrastructure.", "AI": {"tldr": "本研究提出一个概率模拟框架，量化地震结构健康监测（SSHM）在加速电网（EPN）震后恢复和增强韧性方面的价值，结果显示SSHM可显著提高恢复速度并降低韧性缺失。", "motivation": "传统的电网震后恢复依赖耗时且不精确的人工检查，导致修复优先级 suboptimal 和服务中断延长。地震结构健康监测（SSHM）有望加速恢复，但其部署成本高昂，且系统级韧性效益尚未得到充分探索。", "method": "本研究提出了一个概率模拟框架，包括基于网络配置、灾害强度、易损性函数和损伤-功能映射的地震损伤建模，以及考虑资源限制、修复和传输持续时间的恢复模拟。系统功能通过基于图的孤岛检测和最优潮流分析进行评估。韧性通过功能恢复曲线导出的“韧性缺失”（LoR）指标量化。SSHM通过改变修复调度中损伤信息的质量（使用混淆矩阵模拟损伤误分类）来纳入模型，并比较了不同监测场景。", "result": "通过SSHM提高损伤感知能力可显著加速电网恢复，并将韧性缺失（LoR）降低高达21%。", "conclusion": "本研究为关键基础设施中SSHM的部署提供了循证决策支持，证明其能有效提升电网的震后恢复能力和韧性。"}}
{"id": "2508.10446", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.10446", "abs": "https://arxiv.org/abs/2508.10446", "authors": ["Halima El Badaoui"], "title": "A Structured Framework for Prioritizing Unsafe Control Actions in STPA: Case Study on eVTOL Operations", "comment": null, "summary": "Systems Theoretic Process Analysis (STPA) is a widely recommended method for\nanalysing complex system safety. STPA can identify numerous Unsafe Control\nActions (UCAs) and requirements depending on the level of granularity of the\nanalysis and the complexity of the system being analysed. Managing numerous\nresults is challenging, especially during a fast-paced development lifecycle.\nExtensive research has been done to optimize the efficiency of managing and\nprioritising the STPA results. However, maintaining the objectivity of\nprioritisation and communicating the prioritised results have become common\nchallenges. In this paper, the authors present a complementary approach that\nincorporates inputs from both the safety analysts and domain experts to more\nobjectively prioritise UCAs. This is done by evaluating the severity of each\nUCA, the impact factor of each controller or decision maker that issues the\nUCA, and the ranking provided by the subject matter experts who assess the UCA\ncriticalities based on different factors. In addition, a Monte Carlo simulation\nis introduced to reduce subjectivity and relativity, thus enabling more\nobjective prioritisation of the UCAs. As part of the approach to better\ncommunicate the prioritisation results and plan the next steps of system\ndevelopment, a dynamic-scaling prioritisation matrix was developed to capture\ndifferent sets of prioritised UCAs. The approach was applied to a real project\nto improve the safe operations of Electric Vertical Take-off and Landing\n(eVTOL). The results highlighted critical UCAs that need to be prioritised for\nsafer eVTOL operation. 318 UCAs were identified in total. Based on the\napplication of the prioritisation methodology, 110 were recognized as\nhigh-priority UCAs to strengthen the system design.", "AI": {"tldr": "本研究提出了一种结合专家输入和蒙特卡洛模拟的方法，以更客观地优先排序系统理论过程分析（STPA）识别出的不安全控制行为（UCA），并利用动态矩阵改进结果沟通。", "motivation": "STPA在复杂系统安全分析中能识别大量UCA，但管理和优先排序这些结果（尤其是在快速开发周期中）具有挑战性，且难以保持优先排序的客观性和有效沟通。", "method": "提出了一种补充方法，结合安全分析师和领域专家的输入，通过评估UCA的严重性、控制器或决策者的影响因子以及主题专家基于不同因素的批判性排名来客观化UCA的优先级。引入蒙特卡洛模拟以减少主观性和相对性。开发了动态缩放优先级矩阵以更好地沟通优先级结果。", "result": "该方法应用于一个真实的电动垂直起降（eVTOL）项目，共识别出318个UCA。通过应用所提出的优先级方法，其中110个被识别为高优先级UCA，需优先处理以加强系统设计和实现更安全的eVTOL操作。", "conclusion": "该方法能够更客观地优先排序UCA，并有效沟通优先级结果，从而识别出关键UCA，有助于改进系统设计和操作安全。"}}
{"id": "2508.10601", "categories": ["eess.SY", "cs.SY", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10601", "abs": "https://arxiv.org/abs/2508.10601", "authors": ["Vojtěch Mlynář", "Salambô Dago", "Jakob Rieser", "Mario A. Ciampini", "Markus Aspelmeyer", "Nikolai Kiesel", "Andreas Kugi", "Andreas Deutschmann-Olek"], "title": "Feedback stabilization of a nanoparticle at the intensity minimum of an optical double-well potential", "comment": null, "summary": "In this work, we develop and analyze adaptive feedback control strategies to\nstabilize and confine a nanoparticle at the unstable intensity minimum of an\noptical double-well potential. The resulting stochastic optimal control problem\nfor a noise-driven mechanical particle in a nonlinear optical potential must\naccount for unavoidable experimental imperfections such as measurement\nnonlinearities and slow drifts of the optical setup. To address these issues,\nwe simplify the model in the vicinity of the unstable equilibrium and employ\nindirect adaptive control techniques to dynamically follow changes in the\npotential landscape. Our approach leads to a simple and efficient Linear\nQuadratic Gaussian (LQG) controller that can be implemented on fast and\ncost-effective FPGAs, ensuring accessibility and reproducibility. We\ndemonstrate that this strategy successfully tracks the intensity minimum and\nsignificantly reduces the nanoparticle's residual state variance, effectively\nlowering its center-of-mass temperature. While conventional optical traps rely\non confining optical forces in the light field at the intensity maxima,\ntrapping at intensity minima mitigates absorption heating, which is crucial for\nadvanced quantum experiments. Since LQG control naturally extends into the\nquantum regime, our results provide a promising pathway for future experiments\non quantum state preparation beyond the current absorption heating limitation,\nlike matter-wave interference and tests of the quantum-gravity interface.", "AI": {"tldr": "本文开发并分析了一种自适应反馈控制策略，用于在光学双势阱的不稳定强度最小值处稳定和限制纳米粒子，以克服传统光阱的吸收加热问题。", "motivation": "传统的纳米粒子光阱依赖于强度最大值处的束缚力，但这会导致吸收加热，限制了量子实验的进展。在强度最小值处捕获可减轻吸收加热。此外，需要解决实验中不可避免的测量非线性、光学设置慢漂移等问题。", "method": "研究人员在不稳定平衡点附近简化了模型，并采用了间接自适应控制技术来动态跟踪势能景观的变化。具体实现为一个简单高效的线性二次高斯（LQG）控制器，可部署在快速且经济的FPGA上。", "result": "该策略成功跟踪了强度最小值，显著降低了纳米粒子的残余态方差，有效降低了其质心温度。", "conclusion": "由于LQG控制可以自然地扩展到量子领域，该成果为未来超越当前吸收加热限制的量子态制备实验（如物质波干涉和量子引力界面测试）提供了一条有前景的途径。"}}
{"id": "2508.10679", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.10679", "abs": "https://arxiv.org/abs/2508.10679", "authors": ["Jinhua He", "Tingzhe Pan", "Chao Li", "Xin Jin", "Zijie Meng", "Wei Zhou"], "title": "A Robust Optimization Approach for Demand Response Participation of Fixed-Frequency Air Conditioners", "comment": null, "summary": "With the continuous increase in the penetration of renewable energy in the\nemerging power systems, the pressure on system peak regulation has been\nsignificantly intensified. Against this backdrop, demand side resources\nparticularly air conditioning loads have garnered considerable attention for\ntheir substantial regulation potential and fast response capabilities, making\nthem promising candidates for providing auxiliary peak shaving services. This\nstudy focuses on fixed frequency air conditioners (FFACs) and proposes an\noptimization model and solution method for their participation in demand\nresponse (DR) programs. First, a probabilistic response model for FFACs is\ndeveloped based on the Markov assumption. Second, by sampling this\nprobabilistic model, the aggregate power consumption of an FFAC cluster under\ndecentralized control is obtained. Subsequently, a robust optimization model is\nformulated to maximize the profit of an aggregator managing the FFAC cluster\nduring DR events, taking into account the aggregated response power. The model\nexplicitly considers temperature uncertainty to ensure user comfort in a robust\nsense. Finally, leveraging the structure of the proposed model, it is\nreformulated as a mixed-integer linear programming (MILP) problem and solved\nusing a commercial optimization solver. Simulation results validate the\neffectiveness of the proposed model and solution approach.", "AI": {"tldr": "本研究提出了一种针对定频空调（FFACs）参与需求响应（DR）的优化模型和求解方法，旨在最大化聚合商利润，同时考虑温度不确定性和用户舒适度。", "motivation": "随着可再生能源渗透率的提高，电力系统调峰压力显著增大。需求侧资源，特别是空调负荷，因其巨大的调节潜力和快速响应能力，被视为提供辅助调峰服务的潜在选择。", "method": "1. 基于马尔可夫假设，建立了FFACs的概率响应模型。2. 通过对概率模型采样，获得了分散控制下FFACs集群的总功耗。3. 构建了一个鲁棒优化模型，以在DR事件中最大化管理FFACs集群的聚合商的利润，并考虑了聚合响应功率和温度不确定性以确保用户舒适度。4. 将模型重新表述为混合整数线性规划（MILP）问题，并使用商业优化求解器进行求解。", "result": "仿真结果验证了所提出的模型和求解方法的有效性。", "conclusion": "所提出的模型和求解方法能够有效地优化定频空调参与需求响应，在最大化聚合商利润的同时，鲁棒地考虑了温度不确定性并保障了用户舒适度。"}}
{"id": "2508.10144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10144", "abs": "https://arxiv.org/abs/2508.10144", "authors": ["Xu Ma", "Jiajie Zhang", "Fujing Xie", "Sören Schwertfeger"], "title": "WiFi-based Global Localization in Large-Scale Environments Leveraging Structural Priors from osmAG", "comment": null, "summary": "Global localization is essential for autonomous robotics, especially in\nindoor environments where the GPS signal is denied. We propose a novel\nWiFi-based localization framework that leverages ubiquitous wireless\ninfrastructure and the OpenStreetMap Area Graph (osmAG) for large-scale indoor\nenvironments. Our approach integrates signal propagation modeling with osmAG's\ngeometric and topological priors. In the offline phase, an iterative\noptimization algorithm localizes WiFi Access Points (APs) by modeling wall\nattenuation, achieving a mean localization error of 3.79 m (35.3\\% improvement\nover trilateration). In the online phase, real-time robot localization uses the\naugmented osmAG map, yielding a mean error of 3.12 m in fingerprinted areas\n(8.77\\% improvement over KNN fingerprinting) and 3.83 m in non-fingerprinted\nareas (81.05\\% improvement). Comparison with a fingerprint-based method shows\nthat our approach is much more space efficient and achieves superior\nlocalization accuracy, especially for positions where no fingerprint data are\navailable. Validated across a complex 11,025 &m^2& multi-floor environment,\nthis framework offers a scalable, cost-effective solution for indoor robotic\nlocalization, solving the kidnapped robot problem. The code and dataset are\navailable at https://github.com/XuMa369/osmag-wifi-localization.", "AI": {"tldr": "该论文提出了一种基于WiFi和OpenStreetMap区域图（osmAG）的室内机器人全局定位框架，通过建模信号衰减和优化AP定位，实现了高精度、可扩展且经济高效的室内定位，尤其在无指纹数据区域表现优异。", "motivation": "在GPS信号受限的室内环境中，机器人全局定位至关重要。传统的定位方法可能存在精度、可扩展性或成本问题，尤其是在大型复杂环境中。", "method": "该方法结合了信号传播模型和osmAG的几何拓扑先验知识。离线阶段，通过迭代优化算法，建模墙体衰减来定位WiFi接入点（AP）。在线阶段，利用增强的osmAG地图进行实时机器人定位。", "result": "离线AP定位平均误差为3.79米（比三边测量法提升35.3%）。在线机器人定位在有指纹区域平均误差为3.12米（比KNN指纹法提升8.77%），在无指纹区域平均误差为3.83米（提升81.05%）。该方法比基于指纹的方法更节省空间，尤其在无指纹数据区域定位精度更高。在11,025平方米的多楼层环境中进行了验证。", "conclusion": "该框架为室内机器人定位提供了一个可扩展、经济高效的解决方案，能够解决“被绑架机器人”问题，特别适用于大型复杂室内环境，并在无指纹数据区域展现出卓越的定位能力。"}}
{"id": "2508.09991", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09991", "abs": "https://arxiv.org/abs/2508.09991", "authors": ["Lovedeep Gondara", "Gregory Arbour", "Raymond Ng", "Jonathan Simkin", "Shebnum Devji"], "title": "Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry", "comment": null, "summary": "Automating data extraction from clinical documents offers significant\npotential to improve efficiency in healthcare settings, yet deploying Natural\nLanguage Processing (NLP) solutions presents practical challenges. Drawing upon\nour experience implementing various NLP models for information extraction and\nclassification tasks at the British Columbia Cancer Registry (BCCR), this paper\nshares key lessons learned throughout the project lifecycle. We emphasize the\ncritical importance of defining problems based on clear business objectives\nrather than solely technical accuracy, adopting an iterative approach to\ndevelopment, and fostering deep interdisciplinary collaboration and co-design\ninvolving domain experts, end-users, and ML specialists from inception. Further\ninsights highlight the need for pragmatic model selection (including hybrid\napproaches and simpler methods where appropriate), rigorous attention to data\nquality (representativeness, drift, annotation), robust error mitigation\nstrategies involving human-in-the-loop validation and ongoing audits, and\nbuilding organizational AI literacy. These practical considerations,\ngeneralizable beyond cancer registries, provide guidance for healthcare\norganizations seeking to successfully implement AI/NLP solutions to enhance\ndata management processes and ultimately improve patient care and public health\noutcomes.", "AI": {"tldr": "本文分享了在不列颠哥伦比亚癌症登记处（BCCR）部署NLP解决方案进行临床文档数据提取的实践经验和关键教训，强调了以业务目标为导向、迭代开发、跨学科协作等成功实施AI/NLP的关键因素。", "motivation": "尽管自动化从临床文档中提取数据能显著提高医疗效率，但在医疗环境中部署自然语言处理（NLP）解决方案面临实际挑战。本文旨在分享作者在实施过程中积累的经验，以帮助其他医疗机构成功部署AI/NLP。", "method": "基于在不列颠哥伦比亚癌症登记处（BCCR）实施各种NLP模型进行信息提取和分类任务的经验，本文通过总结项目生命周期中的关键教训来提供指导。强调的方法包括：以清晰的业务目标定义问题、采用迭代开发方法、促进跨学科（领域专家、最终用户、机器学习专家）深度协作和共同设计、务实的模型选择（包括混合和更简单方法）、严格关注数据质量（代表性、漂移、标注）、鲁棒的错误缓解策略（人工验证、持续审计）以及培养组织内的AI素养。", "result": "关键教训包括：必须基于清晰的业务目标而非单纯的技术准确性来定义问题；采用迭代式开发方法；从项目伊始就促进领域专家、最终用户和机器学习专家之间的深度跨学科协作和共同设计。此外，还需注重务实的模型选择、严格的数据质量控制（包括代表性、漂移和标注）、包含人工验证和持续审计的鲁棒错误缓解策略，以及提升组织的AI素养。", "conclusion": "这些实践考量具有普适性，可为寻求成功实施AI/NLP解决方案以增强数据管理流程并最终改善患者护理和公共健康结果的医疗机构提供指导。"}}
{"id": "2508.10066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10066", "abs": "https://arxiv.org/abs/2508.10066", "authors": ["Javier Rodenas", "Eduardo Aguilar", "Petia Radeva"], "title": "Stochastic-based Patch Filtering for Few-Shot Learning", "comment": "CVPR Workshop MetaFood 2025", "summary": "Food images present unique challenges for few-shot learning models due to\ntheir visual complexity and variability. For instance, a pasta dish might\nappear with various garnishes on different plates and in diverse lighting\nconditions and camera perspectives. This problem leads to losing focus on the\nmost important elements when comparing the query with support images, resulting\nin misclassification. To address this issue, we propose Stochastic-based Patch\nFiltering for Few-Shot Learning (SPFF) to attend to the patch embeddings that\nshow greater correlation with the class representation. The key concept of SPFF\ninvolves the stochastic filtering of patch embeddings, where patches less\nsimilar to the class-aware embedding are more likely to be discarded. With\npatch embedding filtered according to the probability of appearance, we use a\nsimilarity matrix that quantifies the relationship between the query image and\nits respective support images. Through a qualitative analysis, we demonstrate\nthat SPFF effectively focuses on patches where class-specific food features are\nmost prominent while successfully filtering out non-relevant patches. We\nvalidate our approach through extensive experiments on few-shot classification\nbenchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing\nSoA methods.", "AI": {"tldr": "针对食物图像的视觉复杂性和可变性导致的少样本学习挑战，本文提出了基于随机的补丁过滤方法（SPFF），通过过滤与类别表示相关性较低的补丁，有效聚焦于关键特征，提升分类性能。", "motivation": "食物图像（如意大利面）在不同配菜、盘子、光照和视角下具有高度的视觉复杂性和可变性，导致少样本学习模型在比较查询图像和支持图像时，难以聚焦于最重要的元素，从而引发错误分类。", "method": "本文提出SPFF方法，其核心是随机过滤补丁嵌入。具体来说，与类别感知嵌入相似度较低的补丁被丢弃的可能性更大。在补丁嵌入经过概率性过滤后，使用一个相似度矩阵来量化查询图像与其支持图像之间的关系。", "result": "定性分析表明SPFF能有效聚焦于类别特有的食物特征最突出的补丁，同时成功过滤掉不相关的补丁。在Food-101、VireoFood-172和UECFood-256等少样本分类基准上的大量实验验证了该方法的有效性，并超越了现有的最先进方法。", "conclusion": "SPFF通过随机过滤不相关的补丁嵌入，使模型在少样本学习中更专注于食物图像的关键类别特征，从而解决了食物图像的复杂性挑战，显著提升了分类性能，达到了最先进的水平。"}}
{"id": "2508.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10047", "abs": "https://arxiv.org/abs/2508.10047", "authors": ["Ziyang Xiao", "Jingrong Xie", "Lilin Xu", "Shisi Guan", "Jingyan Zhu", "Xiongwei Han", "Xiaojin Fu", "WingYin Yu", "Han Wu", "Wei Shi", "Qingcan Kang", "Jiahui Duan", "Tao Zhong", "Mingxuan Yuan", "Jia Zeng", "Yuan Wang", "Gang Chen", "Dongxiang Zhang"], "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions", "comment": null, "summary": "By virtue of its great utility in solving real-world problems, optimization\nmodeling has been widely employed for optimal decision-making across various\nsectors, but it requires substantial expertise from operations research\nprofessionals. With the advent of large language models (LLMs), new\nopportunities have emerged to automate the procedure of mathematical modeling.\nThis survey presents a comprehensive and timely review of recent advancements\nthat cover the entire technical stack, including data synthesis and fine-tuning\nfor the base model, inference frameworks, benchmark datasets, and performance\nevaluation. In addition, we conducted an in-depth analysis on the quality of\nbenchmark datasets, which was found to have a surprisingly high error rate. We\ncleaned the datasets and constructed a new leaderboard with fair performance\nevaluation in terms of base LLM model and datasets. We also build an online\nportal that integrates resources of cleaned datasets, code and paper repository\nto benefit the community. Finally, we identify limitations in current\nmethodologies and outline future research opportunities.", "AI": {"tldr": "该综述全面回顾了利用大型语言模型（LLMs）自动化优化建模的最新进展，分析并清理了现有基准数据集，构建了新的排行榜，并指出了未来的研究方向。", "motivation": "优化建模在解决实际问题中非常有用，但需要专业的运筹学知识。大型语言模型的出现为自动化数学建模过程带来了新机遇，因此需要对相关进展进行系统性回顾和评估。", "method": "本文综述了LLMs在优化建模中涵盖整个技术栈的最新进展，包括基础模型的数据合成和微调、推理框架、基准数据集和性能评估。此外，对基准数据集的质量进行了深入分析并进行了清理，构建了新的排行榜，并创建了一个集成清理数据集、代码和论文资源的在线门户。", "result": "发现现有基准数据集存在惊人的高错误率。通过清理数据集，构建了一个新的、公平的性能评估排行榜。同时，建立了一个在线门户，为社区提供了整合的资源。", "conclusion": "识别了当前方法的局限性，并概述了未来的研究机会，以推动LLMs在自动化优化建模领域的发展。"}}
{"id": "2508.10196", "categories": ["eess.IV", "cs.CV", "68T07"], "pdf": "https://arxiv.org/pdf/2508.10196", "abs": "https://arxiv.org/abs/2508.10196", "authors": ["Nishan Rai", "Sujan Khatri", "Devendra Risal"], "title": "Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks", "comment": "11 pages, 9 figures, 4 tables. Undergraduate research project report", "summary": "Early detection of lung cancer is critical to improving survival outcomes. We\npresent a deep learning framework for automated lung cancer screening from\nchest computed tomography (CT) images with integrated explainability. Using the\nIQ-OTH/NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes),\nwe evaluate a custom convolutional neural network (CNN) and three fine-tuned\ntransfer learning backbones: DenseNet121, ResNet152, and VGG19. Models are\ntrained with cost-sensitive learning to mitigate class imbalance and evaluated\nvia accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152\nachieved the highest accuracy (97.3%), DenseNet121 provided the best overall\nbalance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). We\nfurther apply Shapley Additive Explanations (SHAP) to visualize evidence\ncontributing to predictions, improving clinical transparency. Results indicate\nthat CNN-based approaches augmented with explainability can provide fast,\naccurate, and interpretable support for lung cancer screening, particularly in\nresource-limited settings.", "AI": {"tldr": "该研究提出了一个深度学习框架，用于从胸部CT图像自动筛查肺癌，并集成了可解释性，以提高诊断准确性和临床透明度。", "motivation": "早期发现肺癌对于提高患者生存率至关重要。研究旨在开发一个自动化、准确且可解释的工具来辅助肺癌筛查。", "method": "研究使用IQ-OTH/NCCD数据集（包含正常、良性和恶性三类共1197张扫描图像），评估了一个自定义卷积神经网络（CNN）以及三个经过微调的迁移学习骨干网络：DenseNet121、ResNet152和VGG19。模型采用成本敏感学习来缓解类别不平衡问题，并通过准确率、精确率、召回率、F1分数和ROC-AUC进行评估。此外，应用Shapley Additive Explanations (SHAP) 来可视化预测证据，增强临床透明度。", "result": "ResNet152模型实现了最高的准确率（97.3%）。DenseNet121模型在精确率、召回率和F1分数方面提供了最佳的整体平衡（分别高达92%、90%和91%）。SHAP的应用提升了预测结果的临床可解释性。", "conclusion": "结合可解释性的CNN方法能够为肺癌筛查提供快速、准确和可解释的支持，尤其在资源有限的环境中具有重要应用潜力。"}}
{"id": "2508.10705", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.10705", "abs": "https://arxiv.org/abs/2508.10705", "authors": ["Jinhua He", "Zechun Hu"], "title": "Probabilistic Forecasting Method for Offshore Wind Farm Cluster under Typhoon Conditions: a Score-Based Conditional Diffusion Model", "comment": null, "summary": "Offshore wind power (OWP) exhibits significant fluctuations under typhoon\nconditions, posing substantial challenges to the secure operation of power\nsystems. Accurate forecasting of OWP is therefore essential. However, the\ninherent scarcity of historical typhoon data and stochasticity of OWP render\ntraditional point forecasting methods particularly difficult and inadequate. To\naddress this challenge and provide grid operators with the comprehensive\ninformation necessary for decision-making, this study proposes a score-based\nconditional diffusion model (SCDM) for probabilistic forecasting of OWP during\ntyphoon events. First, a knowledge graph algorithm is employed to embed\nhistorical typhoon paths as vectors. Then, a deterministic network is\nconstructed to predict the wind power under typhoon conditions based on these\nvector embeddings. Finally, to better characterize prediction errors, a\ndenoising network is developed. At the core of this approach is a\nmean-reverting stochastic differential equation (SDE), which transforms complex\nerror distributions into a standard Gaussian, enabling the sampling of\nforecasting errors using a reverse-time SDE. The probabilistic forecasting\nresults are reconstructed by combining deterministic forecasts with sampled\nerrors. The proposed method is evaluated using real-world data from a cluster\nof 9 offshore wind farms. Results demonstrate that under typhoon conditions,\nour approach outperforms baseline models for both deterministic and\nprobabilistic metrics, verifying the effectiveness of the approach.", "AI": {"tldr": "本文提出了一种基于分数的条件扩散模型（SCDM），用于台风条件下海上风电的概率预测，以应对传统方法的不足和数据稀缺性。", "motivation": "台风期间海上风电（OWP）波动剧烈，对电力系统安全运行构成挑战。由于历史台风数据稀缺和OWP的随机性，传统点预测方法难以准确预测，因此需要一种更全面、准确的预测方法来为电网运营商提供决策信息。", "method": "该研究首先使用知识图谱算法将历史台风路径嵌入为向量。然后，构建一个确定性网络，利用这些向量嵌入预测台风条件下的风电。最后，开发一个去噪网络来表征预测误差，其核心是一个均值回归随机微分方程（SDE），将复杂的误差分布转换为标准高斯分布，并通过逆时SDE采样预测误差。最终的概率预测结果通过结合确定性预测和采样的误差重建。", "result": "该方法在9个海上风电场的真实数据上进行了评估。结果表明，在台风条件下，该方法在确定性预测和概率性预测指标上均优于基线模型，验证了其有效性。", "conclusion": "所提出的基于分数的条件扩散模型（SCDM）能够有效应对台风条件下海上风电预测的挑战，提供准确的概率预测，优于现有基线模型。"}}
{"id": "2508.10203", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.10203", "abs": "https://arxiv.org/abs/2508.10203", "authors": ["Matthew D. Osburn", "Cameron K. Peterson", "John L. Salmon"], "title": "Systematic Constraint Formulation and Collision-Free Trajectory Planning Using Space-Time Graphs of Convex Sets", "comment": "21 pages with references, 20 figures", "summary": "In this paper, we create optimal, collision-free, time-dependent trajectories\nthrough cluttered dynamic environments. The many spatial and temporal\nconstraints make finding an initial guess for a numerical solver difficult.\nGraphs of Convex Sets (GCS) and the recently developed Space-Time Graphs of\nConvex Sets formulation (ST-GCS) enable us to generate optimal minimum distance\ncollision-free trajectories without providing an initial guess to the solver.\nWe also explore the derivation of general GCS-compatible constraints and\ndocument an intuitive strategy for adapting general constraints to the\nframework. We show that ST-GCS produces equivalent trajectories to the standard\nGCS formulation when the environment is static. We then show ST-GCS operating\nin dynamic environments to find minimum distance collision-free trajectories.", "AI": {"tldr": "本文提出并应用ST-GCS方法，在动态杂乱环境中生成最优、无碰撞、随时间变化的轨迹，无需初始猜测。", "motivation": "在具有大量时空约束的动态杂乱环境中，为数值求解器找到初始猜测非常困难。", "method": "采用凸集图（GCS）和新开发的空时凸集图（ST-GCS）公式，无需提供初始猜测即可生成最优最小距离无碰撞轨迹。同时，探讨了通用GCS兼容约束的推导和适应策略。", "result": "ST-GCS在静态环境下能生成与标准GCS公式等效的轨迹。此外，ST-GCS在动态环境中也能成功运行，找到最小距离的无碰撞轨迹。", "conclusion": "ST-GCS是一种有效的方法，能够在动态环境中生成最优、无碰撞的轨迹，解决了传统方法对初始猜测的需求。"}}
{"id": "2508.09993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09993", "abs": "https://arxiv.org/abs/2508.09993", "authors": ["Hugo Massaroli", "Leonardo Iara", "Emmanuel Iarussi", "Viviana Siless"], "title": "A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in realworld\napplications, yet concerns about their fairness persist especially in\nhighstakes domains like criminal justice, education, healthcare, and finance.\nThis paper introduces transparent evaluation protocol for benchmarking the\nfairness of opensource LLMs using smart contracts on the Internet Computer\nProtocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,\nimmutable, and reproducible evaluations by executing onchain HTTP requests to\nhosted Hugging Face endpoints and storing datasets, prompts, and metrics\ndirectly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the\nPISA dataset for academic performance prediction (OECD, 2018), a dataset\nsuitable for fairness evaluation using statistical parity and equal opportunity\nmetrics (Hardt et al., 2016). We also evaluate structured Context Association\nMetrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure\nsocial bias in contextual associations. We further extend our analysis with a\nmultilingual evaluation across English, Spanish, and Portuguese using the\nKaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic\ndisparities. All code and results are open source, enabling community audits\nand longitudinal fairness tracking across model versions.", "AI": {"tldr": "该论文提出了一种基于区块链的透明评估协议，用于在互联网计算机协议（ICP）上对开源大型语言模型（LLM）的公平性进行基准测试，确保评估结果可验证、不可变和可复现。", "motivation": "鉴于大型语言模型（LLM）在现实世界应用中日益普及，尤其是在刑事司法、教育、医疗保健和金融等高风险领域，对LLM公平性的担忧持续存在。因此，需要一种透明、可验证和可复现的评估方法来解决这些问题。", "method": "引入了一个透明的评估协议，利用ICP区块链上的智能合约进行LLM公平性基准测试。该方法通过执行链上HTTP请求到Hugging Face端点，并将数据集、提示和度量直接存储在链上，确保评估的可验证性、不变性和可复现性。研究人员在PISA数据集上对Llama、DeepSeek和Mistral模型进行了学术表现预测的基准测试，并使用统计均等和机会均等指标进行公平性评估。同时，利用StereoSet数据集导出的结构化上下文关联度量来衡量上下文关联中的社会偏见。此外，还使用Kaleidoscope基准对英语、西班牙语和葡萄牙语进行了多语言评估。", "result": "成功在PISA数据集上对Llama、DeepSeek和Mistral模型进行了公平性基准测试，并评估了StereoSet数据集的结构化上下文关联度量。多语言评估揭示了跨语言的差异。所有代码和结果均已开源，便于社区审计和模型版本间的公平性长期追踪。", "conclusion": "该研究提供了一种创新且透明的LLM公平性评估方法，通过区块链技术确保评估的可验证性、不变性和可复现性，从而促进了社区审计和公平性的长期追踪。研究发现存在跨语言的公平性差异。"}}
{"id": "2508.10104", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10104", "abs": "https://arxiv.org/abs/2508.10104", "authors": ["Oriane Siméoni", "Huy V. Vo", "Maximilian Seitzer", "Federico Baldassarre", "Maxime Oquab", "Cijo Jose", "Vasil Khalidov", "Marc Szafraniec", "Seungeun Yi", "Michaël Ramamonjisoa", "Francisco Massa", "Daniel Haziza", "Luca Wehrstedt", "Jianyuan Wang", "Timothée Darcet", "Théo Moutakanni", "Leonel Sentana", "Claire Roberts", "Andrea Vedaldi", "Jamie Tolan", "John Brandt", "Camille Couprie", "Julien Mairal", "Hervé Jégou", "Patrick Labatut", "Piotr Bojanowski"], "title": "DINOv3", "comment": null, "summary": "Self-supervised learning holds the promise of eliminating the need for manual\ndata annotation, enabling models to scale effortlessly to massive datasets and\nlarger architectures. By not being tailored to specific tasks or domains, this\ntraining paradigm has the potential to learn visual representations from\ndiverse sources, ranging from natural to aerial images -- using a single\nalgorithm. This technical report introduces DINOv3, a major milestone toward\nrealizing this vision by leveraging simple yet effective strategies. First, we\nleverage the benefit of scaling both dataset and model size by careful data\npreparation, design, and optimization. Second, we introduce a new method called\nGram anchoring, which effectively addresses the known yet unsolved issue of\ndense feature maps degrading during long training schedules. Finally, we apply\npost-hoc strategies that further enhance our models' flexibility with respect\nto resolution, model size, and alignment with text. As a result, we present a\nversatile vision foundation model that outperforms the specialized state of the\nart across a broad range of settings, without fine-tuning. DINOv3 produces\nhigh-quality dense features that achieve outstanding performance on various\nvision tasks, significantly surpassing previous self- and weakly-supervised\nfoundation models. We also share the DINOv3 suite of vision models, designed to\nadvance the state of the art on a wide spectrum of tasks and data by providing\nscalable solutions for diverse resource constraints and deployment scenarios.", "AI": {"tldr": "DINOv3是一个自监督视觉基础模型，通过扩展数据和模型规模、引入Gram anchoring以及应用后处理策略，实现了在多种视觉任务上超越现有专业模型的性能，无需微调，并能生成高质量的密集特征。", "motivation": "自监督学习有望消除手动数据标注的需求，使模型能轻松扩展到大规模数据集和更大架构。它旨在通过单一算法从自然图像到航空图像等多样化来源学习视觉表示，且不针对特定任务或领域。", "method": "1. 通过精心的准备、设计和优化，充分利用数据集和模型规模扩展的优势。2. 引入名为“Gram anchoring”的新方法，有效解决训练时间过长导致密集特征图退化的问题。3. 应用后处理策略，进一步增强模型在分辨率、模型大小以及与文本对齐方面的灵活性。", "result": "DINOv3作为一个多功能的视觉基础模型，在广泛的设置下，无需微调即可超越专业领域的最新技术。它能生成高质量的密集特征，在各种视觉任务上表现出色，显著超越了之前的自监督和弱监督基础模型。", "conclusion": "DINOv3是一个实现自监督学习愿景的重要里程碑，它提供了一套可扩展的视觉模型，旨在通过提供适用于各种资源限制和部署场景的解决方案，推进广泛任务和数据上的最新技术。"}}
{"id": "2508.10108", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; E.0"], "pdf": "https://arxiv.org/pdf/2508.10108", "abs": "https://arxiv.org/abs/2508.10108", "authors": ["Sattvik Sahai", "Prasoon Goyal", "Michael Johnston", "Anna Gottardi", "Yao Lu", "Lucy Hu", "Luke Dai", "Shaohua Liu", "Samyuth Sagi", "Hangjie Shi", "Desheng Zhang", "Lavina Vaz", "Leslie Ball", "Maureen Murray", "Rahul Gupta", "Shankar Ananthakrishna"], "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development", "comment": "18 pages, 1st Proceedings of Amazon Nova AI Challenge (Trusted AI\n  2025)", "summary": "AI systems for software development are rapidly gaining prominence, yet\nsignificant challenges remain in ensuring their safety. To address this, Amazon\nlaunched the Trusted AI track of the Amazon Nova AI Challenge, a global\ncompetition among 10 university teams to drive advances in secure AI. In the\nchallenge, five teams focus on developing automated red teaming bots, while the\nother five create safe AI assistants. This challenge provides teams with a\nunique platform to evaluate automated red-teaming and safety alignment methods\nthrough head-to-head adversarial tournaments where red teams have multi-turn\nconversations with the competing AI coding assistants to test their safety\nalignment. Along with this, the challenge provides teams with a feed of high\nquality annotated data to fuel iterative improvement. Throughout the challenge,\nteams developed state-of-the-art techniques, introducing novel approaches in\nreasoning-based safety alignment, robust model guardrails, multi-turn\njail-breaking, and efficient probing of large language models (LLMs). To\nsupport these efforts, the Amazon Nova AI Challenge team made substantial\nscientific and engineering investments, including building a custom baseline\ncoding specialist model for the challenge from scratch, developing a tournament\norchestration service, and creating an evaluation harness. This paper outlines\nthe advancements made by university teams and the Amazon Nova AI Challenge team\nin addressing the safety challenges of AI for software development,\nhighlighting this collaborative effort to raise the bar for AI safety.", "AI": {"tldr": "亚马逊Nova AI挑战赛的信任AI赛道旨在通过全球大学团队竞赛推动软件开发AI的安全进展，本文概述了挑战赛中取得的进步。", "motivation": "软件开发AI系统快速发展，但其安全性仍面临巨大挑战，因此亚马逊发起了这项挑战赛来解决AI安全问题。", "method": "挑战赛分为红队（开发自动化红队机器人）和蓝队（开发安全AI助手），通过对抗性锦标赛进行评估，红队与AI编码助手进行多轮对话以测试其安全性。挑战赛提供高质量标注数据促进迭代改进。亚马逊团队构建了定制基线编码模型、开发了锦标赛编排服务和评估工具。", "result": "参赛团队开发了最先进的技术，包括基于推理的安全对齐、鲁棒模型护栏、多轮越狱和LLM高效探测等新方法。", "conclusion": "本文总结了大学团队和亚马逊Nova AI挑战赛团队在解决软件开发AI安全挑战方面取得的进展，强调了这项合作努力提升了AI安全标准。"}}
{"id": "2508.10215", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10215", "abs": "https://arxiv.org/abs/2508.10215", "authors": ["Sahar Nasirihaghighi"], "title": "Data-Efficient Learning for Generalizable Surgical Video Understanding", "comment": null, "summary": "Advances in surgical video analysis are transforming operating rooms into\nintelligent, data-driven environments. Computer-assisted systems support full\nsurgical workflow, from preoperative planning to intraoperative guidance and\npostoperative assessment. However, developing robust and generalizable models\nfor surgical video understanding remains challenging due to (I) annotation\nscarcity, (II) spatiotemporal complexity, and (III) domain gap across\nprocedures and institutions. This doctoral research aims to bridge the gap\nbetween deep learning-based surgical video analysis in research and its\nreal-world clinical deployment. To address the core challenge of recognizing\nsurgical phases, actions, and events, critical for analysis, I benchmarked\nstate-of-the-art neural network architectures to identify the most effective\ndesigns for each task. I further improved performance by proposing novel\narchitectures and integrating advanced modules. Given the high cost of expert\nannotations and the domain gap across surgical video sources, I focused on\nreducing reliance on labeled data. We developed semi-supervised frameworks that\nimprove model performance across tasks by leveraging large amounts of unlabeled\nsurgical video. We introduced novel semi-supervised frameworks, including DIST,\nSemiVT-Surge, and ENCORE, that achieved state-of-the-art results on challenging\nsurgical datasets by leveraging minimal labeled data and enhancing model\ntraining through dynamic pseudo-labeling. To support reproducibility and\nadvance the field, we released two multi-task datasets: GynSurg, the largest\ngynecologic laparoscopy dataset, and Cataract-1K, the largest cataract surgery\nvideo dataset. Together, this work contributes to robust, data-efficient, and\nclinically scalable solutions for surgical video analysis, laying the\nfoundation for generalizable AI systems that can meaningfully impact surgical\ncare and training.", "AI": {"tldr": "该博士研究旨在通过开发半监督学习框架和发布大规模数据集，解决手术视频分析中标签稀缺和领域差异等挑战，从而推动深度学习在真实临床场景中的应用。", "motivation": "手术视频分析正在将手术室转变为智能、数据驱动的环境，支持从术前规划到术后评估的完整工作流程。然而，由于（I）标注稀缺性、（II）时空复杂性和（III）不同手术和机构间的领域差异，开发鲁棒且泛化性强的模型仍然面临挑战。本研究旨在弥合深度学习手术视频分析在研究与实际临床部署之间的差距。", "method": "研究方法包括：1) 对最先进的神经网络架构进行基准测试，以识别用于识别手术阶段、动作和事件的最有效设计；2) 提出新的架构并集成先进模块以提高性能；3) 开发半监督框架（如DIST、SemiVT-Surge和ENCORE），利用大量未标注手术视频，减少对标注数据的依赖，并通过动态伪标签增强模型训练；4) 发布了两个多任务数据集：GynSurg（最大的妇科腹腔镜数据集）和Cataract-1K（最大的白内障手术视频数据集）。", "result": "研究结果包括：1) 识别了针对不同手术识别任务的有效模型设计；2) 通过新颖的架构和先进模块提高了性能；3) 开发的半监督框架（DIST、SemiVT-Surge和ENCORE）在具有挑战性的手术数据集上，利用最少量的标注数据实现了最先进的成果；4) 成功发布了两个大型多任务数据集GynSurg和Cataract-1K，以支持领域内的可复现性和进展。", "conclusion": "这项工作为手术视频分析提供了鲁棒、数据高效且临床可扩展的解决方案，为可泛化的人工智能系统奠定了基础，这些系统有望对手术护理和培训产生有意义的影响。"}}
{"id": "2508.10730", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.10730", "abs": "https://arxiv.org/abs/2508.10730", "authors": ["Giacomo Oliveri", "Francesco Zardi", "Aaron Angel Salas Sanchez", "Andrea Massa"], "title": "Multi-Functional Polarization-Based Coverage Control through Static Passive EMSs", "comment": null, "summary": "An innovative multi-functional static-passive electromagnetic skin (SP-EMS)\nsolution is proposed to simultaneously support, in reflection, two independent\nwave-manipulation functionalities with a single meta-atoms arrangement on the\nEMS aperture when illuminated by two EM sources operating at the same\nfrequency, but working in different polarization states. Towards this end, a\nsimple reference meta-atom is designed first to enable an accurate and\nindependent control of each polarization component of the local reflection\ntensor. Successively, the macro-scale synthesis of multi-polarization (MP)\nSP-EMSs (MP-SP-EMSs) is carried out by solving a global optimization problem\nwhere a cost function, which mathematically codes separate requirements for\neach polarization, is minimized with a customized version of the\nsystem-by-design (SbD) technique. Representative results from a set of\nnumerical and experimental tests are reported to assess the feasibility of a\nmulti-function EMS based on polarization diversity as well as the effectiveness\nand the robustness of the proposed method for the synthesis of MP-SP-EMSs.", "AI": {"tldr": "提出了一种创新的多功能静被动电磁皮肤（SP-EMS）解决方案，该方案通过单一超原子阵列，在相同频率但不同极化状态的两个电磁源照射下，同时支持两种独立的波操纵功能。", "motivation": "现有技术可能难以在单一超原子阵列上同时实现针对不同极化状态的多个独立波操纵功能，本研究旨在解决这一挑战，以实现更高效、多功能的电磁皮肤。", "method": "首先设计一个简单的参考超原子，以精确独立控制局部反射张量的每个极化分量。随后，通过解决一个全局优化问题进行多极化SP-EMS（MP-SP-EMS）的宏观合成，该优化问题使用定制的系统设计（SbD）技术最小化一个数学编码了每个极化独立需求的成本函数。", "result": "数值和实验测试结果表明，基于极化分集的多功能EMS是可行的，并且所提出的MP-SP-EMS合成方法有效且鲁棒。", "conclusion": "所提出的基于极化分集的多功能SP-EMS解决方案及其合成方法，能够有效实现单一电磁皮肤孔径上同时支持两种独立波操纵功能，具有良好的可行性、有效性和鲁棒性。"}}
{"id": "2508.10269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10269", "abs": "https://arxiv.org/abs/2508.10269", "authors": ["Kejun Li", "Jeeseop Kim", "Maxime Brunet", "Marine Pétriaux", "Yisong Yue", "Aaron D. Ames"], "title": "Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis", "comment": "8 pages; 8 figures", "summary": "Robust bipedal locomotion in exoskeletons requires the ability to dynamically\nreact to changes in the environment in real time. This paper introduces the\nhybrid data-driven predictive control (HDDPC) framework, an extension of the\ndata-enabled predictive control, that addresses these challenges by\nsimultaneously planning foot contact schedules and continuous domain\ntrajectories. The proposed framework utilizes a Hankel matrix-based\nrepresentation to model system dynamics, incorporating step-to-step (S2S)\ntransitions to enhance adaptability in dynamic environments. By integrating\ncontact scheduling with trajectory planning, the framework offers an efficient,\nunified solution for locomotion motion synthesis that enables robust and\nreactive walking through online replanning. We validate the approach on the\nAtalante exoskeleton, demonstrating improved robustness and adaptability.", "AI": {"tldr": "本文提出了一种混合数据驱动预测控制（HDDPC）框架，用于外骨骼的鲁棒双足运动，通过同时规划足部接触计划和连续域轨迹，实现对环境变化的实时动态反应。", "motivation": "外骨骼的鲁棒双足运动需要实时动态响应环境变化的能力，这是现有方法面临的挑战。", "method": "引入了混合数据驱动预测控制（HDDPC）框架，它是数据驱动预测控制的扩展。该框架利用基于Hankel矩阵的表示来建模系统动力学，并结合了步态间（S2S）转换，以增强在动态环境中的适应性。它将接触调度与轨迹规划集成，提供了一个统一的运动合成解决方案，通过在线重新规划实现鲁棒和反应式行走。", "result": "该方法在Atalante外骨骼上进行了验证，结果表明其鲁棒性和适应性得到了显著提高，能够实现鲁棒且反应灵敏的行走。", "conclusion": "HDDPC框架为外骨骼的运动合成提供了一个高效、统一的解决方案，通过在线重新规划实现了鲁棒和反应式行走，从而提高了在外骨骼中的鲁棒性和适应性。"}}
{"id": "2508.09997", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.09997", "abs": "https://arxiv.org/abs/2508.09997", "authors": ["Johannes Schneider", "Béatrice S. Hasler", "Michaela Varrone", "Fabian Hoya", "Thomas Schroffenegger", "Dana-Kristin Mah", "Karl Peböck"], "title": "Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling", "comment": "Accepted at the International Conference on Computer-Human\n  Interaction Research and Applications (CHIRA), 2025", "summary": "We analyze anonymous interaction data of minors in class-rooms spanning\nseveral months, schools, and subjects employing a novel, simple topic modeling\napproach. Specifically, we categorize more than 17,000 messages generated by\nstudents, teachers, and ChatGPT in two dimensions: content (such as nature and\npeople) and tasks (such as writing and explaining). Our hierarchical\ncategorization done separately for each dimension includes exemplary prompts,\nand provides both a high-level overview as well as tangible insights. Prior\nworks mostly lack a content or thematic categorization. While task\ncategorizations are more prevalent in education, most have not been supported\nby real-world data for K-12. In turn, it is not surprising that our analysis\nyielded a number of novel applications. In deriving these insights, we found\nthat many of the well-established classical and emerging computational methods,\ni.e., topic modeling, for analysis of large amounts of texts underperform,\nleading us to directly apply state-of-the-art LLMs with adequate pre-processing\nto achieve hierarchical topic structures with better human alignment through\nexplicit instructions than prior approaches. Our findings support fellow\nresearchers, teachers and students in enriching the usage of GenAI, while our\ndiscussion also highlights a number of concerns and open questions for future\nresearch.", "AI": {"tldr": "本文通过一种新颖的、基于LLM的主题建模方法，分析了未成年人在课堂中的匿名互动数据（包括学生、教师和ChatGPT生成的消息），并对其进行了内容和任务的层次化分类，发现了新的应用并强调了LLM在文本分析中的优势。", "motivation": "现有研究大多缺乏对K-12教育中课堂互动数据的具体内容或主题分类，且任务分类通常缺乏真实世界数据的支持。此外，传统的计算方法（如主题建模）在分析大量文本时表现不佳，无法提供与人类对齐的洞察。", "method": "研究采用一种新颖、简洁的主题建模方法，对超过17,000条消息进行分析。具体方法是，对消息在“内容”（如自然、人物）和“任务”（如写作、解释）两个维度进行层次化分类。不同于传统主题建模，本研究通过对最先进的大语言模型（LLMs）进行适当的预处理和明确指令，实现了与人类对齐更好的层次主题结构。", "result": "分析得出了一些新颖的应用。研究发现，许多成熟的经典和新兴计算方法（如主题建模）在分析大量文本时表现不佳，而通过LLMs和适当的预处理，能够获得更好的、与人类对齐的层次主题结构。", "conclusion": "研究结果支持研究人员、教师和学生更丰富地使用生成式AI（GenAI）。同时，讨论也提出了未来研究中的一些担忧和开放性问题。"}}
{"id": "2508.10110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10110", "abs": "https://arxiv.org/abs/2508.10110", "authors": ["Sushrut Patwardhan", "Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model", "comment": null, "summary": "Morphing attack detection has become an essential component of face\nrecognition systems for ensuring a reliable verification scenario. In this\npaper, we present a multimodal learning approach that can provide a textual\ndescription of morphing attack detection. We first show that zero-shot\nevaluation of the proposed framework using Contrastive Language-Image\nPretraining (CLIP) can yield not only generalizable morphing attack detection,\nbut also predict the most relevant text snippet. We present an extensive\nanalysis of ten different textual prompts that include both short and long\ntextual prompts. These prompts are engineered by considering the human\nunderstandable textual snippet. Extensive experiments were performed on a face\nmorphing dataset that was developed using a publicly available face biometric\ndataset. We present an evaluation of SOTA pre-trained neural networks together\nwith the proposed framework in the zero-shot evaluation of five different\nmorphing generation techniques that are captured in three different mediums.", "AI": {"tldr": "本文提出一种基于CLIP的多模态学习方法，用于零样本活体攻击检测并提供文本描述，在多种攻击技术和媒介上表现出泛化能力。", "motivation": "活体攻击检测对于确保人脸识别系统的可靠性至关重要，传统方法缺乏对检测结果的文本描述能力。", "method": "提出一种多模态学习方法，利用对比语言-图像预训练（CLIP）模型进行零样本评估，不仅实现活体攻击检测，还能预测最相关的文本片段。设计了十种不同的文本提示（包括长短提示），并在一个基于公开人脸生物识别数据集开发的活体攻击数据集上进行实验。", "result": "所提出的框架在零样本评估中，不仅能实现泛化性活体攻击检测，还能预测最相关的文本片段。在包含五种不同活体生成技术和三种不同介质的活体攻击数据集上，与现有最先进的预训练神经网络进行了广泛的评估。", "conclusion": "基于CLIP的多模态学习方法能够有效进行零样本活体攻击检测，并能提供人类可理解的文本描述，增强了检测结果的可解释性和系统的可靠性。"}}
{"id": "2508.10143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10143", "abs": "https://arxiv.org/abs/2508.10143", "authors": ["Alexandru-Andrei Avram", "Adrian Groza", "Alexandru Lecu"], "title": "MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection", "comment": "8 pages + 1 page references, 5 figures, 4 tables, Registered for the\n  27th International Symposium on Symbolic and Numeric Algorithms for\n  Scientific Computing, 2025, Timisoara", "summary": "The large spread of disinformation across digital platforms creates\nsignificant challenges to information integrity. This paper presents a\nmulti-agent system that uses relation extraction to detect disinformation in\nnews articles, focusing on titles and short text snippets. The proposed Agentic\nAI system combines four agents: (i) a machine learning agent (logistic\nregression), (ii) a Wikipedia knowledge check agent (which relies on named\nentity recognition), (iii) a coherence detection agent (using LLM prompt\nengineering), and (iv) a web-scraped data analyzer that extracts relational\ntriplets for fact checking. The system is orchestrated via the Model Context\nProtocol (MCP), offering shared context and live learning across components.\nResults demonstrate that the multi-agent ensemble achieves 95.3% accuracy with\nan F1 score of 0.964, significantly outperforming individual agents and\ntraditional approaches. The weighted aggregation method, mathematically derived\nfrom individual agent misclassification rates, proves superior to algorithmic\nthreshold optimization. The modular architecture makes the system easily\nscalable, while also maintaining details of the decision processes.", "AI": {"tldr": "该论文提出一个多智能体系统，利用关系抽取技术检测新闻文章（标题和短文本片段）中的虚假信息，实现了高准确率。", "motivation": "数字平台上的虚假信息广泛传播，对信息完整性造成了严峻挑战。", "method": "所提出的Agentic AI系统包含四个智能体：(i) 机器学习智能体（逻辑回归），(ii) 维基百科知识检查智能体（依赖命名实体识别），(iii) 一致性检测智能体（使用LLM提示工程），以及 (iv) 网络爬取数据分析智能体（提取关系三元组进行事实核查）。系统通过模型上下文协议（MCP）进行编排，实现共享上下文和组件间的实时学习。采用基于单个智能体错误分类率的加权聚合方法。", "result": "多智能体集成系统达到了95.3%的准确率和0.964的F1分数，显著优于单个智能体和传统方法。数学推导的加权聚合方法被证明优于算法阈值优化。", "conclusion": "该模块化系统易于扩展，同时保持决策过程的细节，并能有效检测虚假信息，性能优于现有方法。"}}
{"id": "2508.10260", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10260", "abs": "https://arxiv.org/abs/2508.10260", "authors": ["Soorena Salari", "Catherine Spino", "Laurie-Anne Pharand", "Fabienne Lathuiliere", "Hassan Rivaz", "Silvain Beriault", "Yiming Xiao"], "title": "DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy", "comment": "Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14\n  pages", "summary": "Accurate tissue motion tracking is critical to ensure treatment outcome and\nsafety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by\nregistration of sequential images, but existing methods often face challenges\nwith large misalignments and lack of interpretability. In this paper, we\nintroduce DINOMotion, a novel deep learning framework based on DINOv2 with\nLow-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable\nmotion tracking. DINOMotion automatically detects corresponding landmarks to\nderive optimal image registration, enhancing interpretability by providing\nexplicit visual correspondences between sequential images. The integration of\nLoRA layers reduces trainable parameters, improving training efficiency, while\nDINOv2's powerful feature representations offer robustness against large\nmisalignments. Unlike iterative optimization-based methods, DINOMotion directly\ncomputes image registration at test time. Our experiments on volunteer and\npatient datasets demonstrate its effectiveness in estimating both linear and\nnonlinear transformations, achieving Dice scores of 92.07% for the kidney,\n90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff\ndistances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes\neach scan in approximately 30ms and consistently outperforms state-of-the-art\nmethods, particularly in handling large misalignments. These results highlight\nits potential as a robust and interpretable solution for real-time motion\ntracking in 2D-Cine MRI-guided radiotherapy.", "AI": {"tldr": "DINOMotion是一种基于DINOv2和LoRA的深度学习框架，用于实现2D-Cine MRI引导放疗中鲁棒、高效且可解释的组织运动跟踪。", "motivation": "2D-Cine MRI引导放疗中精确的组织运动跟踪对治疗效果和安全性至关重要，但现有方法在处理大错位和缺乏可解释性方面面临挑战。", "method": "本文提出DINOMotion框架，它结合了DINOv2强大的特征表示能力和LoRA层以减少可训练参数并提高训练效率。该方法通过自动检测对应地标来推导最佳图像配准，提供明确的视觉对应关系以增强可解释性，并能在测试时直接计算图像配准结果，而非采用迭代优化。", "result": "在志愿者和患者数据集上，DINOMotion在肾脏、肝脏和肺的Dice分数分别达到92.07%、90.90%和95.23%，相应的Hausdorff距离分别为5.47毫米、8.31毫米和6.72毫米。它能处理线性和非线性变换，每张扫描处理时间约为30毫秒，并且在处理大错位方面持续优于现有最先进方法。", "conclusion": "DINOMotion为2D-Cine MRI引导放疗中的实时运动跟踪提供了一个鲁棒且可解释的解决方案，具有巨大的应用潜力。"}}
{"id": "2508.10849", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.10849", "abs": "https://arxiv.org/abs/2508.10849", "authors": ["Metin Ozturk", "Maryam Salamatmoghadasi", "Halim Yanikomeroglu"], "title": "Integrating Terrestrial and Non-Terrestrial Networks for Sustainable 6G Operations: A Latency-Aware Multi-Tier Cell-Switching Approach", "comment": "9 pages, 6 figures", "summary": "Sustainability is paramount in modern cellular networks, which face\nsignificant energy consumption challenges from rising mobile traffic and\nadvancements in wireless technology. Cell-switching, well-established in\nliterature as an effective solution, encounters limitations such as inadequate\ncapacity and limited coverage when implemented through terrestrial networks\n(TN). This study enhances cell-switching by integrating non-terrestrial\nnetworks (NTN), including satellites (used for cell-switching for the first\ntime), high altitude platform stations (HAPS), and uncrewed aerial vehicles\n(UAVs) into TN. This integration significantly boosts energy savings by\nexpanding capacity, enhancing coverage, and increasing operational flexibility.\nWe introduce a multi-tier cell-switching approach that dynamically offloads\nusers across network layers to manage energy effectively and minimize delays,\naccommodating diverse user demands with a context aware strategy. Additionally,\nwe explore the role of artificial intelligence (AI), particularly generative\nAI, in optimizing network efficiency through data compression, handover\noptimization between different network layers, and enhancing device\ncompatibility, further improving the adaptability and energy efficiency of\ncell-switching operations. A case study confirms substantial improvements in\nnetwork power consumption and user satisfaction, demonstrating the potential of\nour approach for future networks.", "AI": {"tldr": "本研究通过将非地面网络（NTN，包括首次用于小区切换的卫星、HAPS和UAV）与地面网络（TN）结合，并引入多层小区切换和AI优化，显著提升了蜂窝网络的能源效率、容量和覆盖范围。", "motivation": "现代蜂窝网络面临着巨大的能耗挑战，现有的小区切换技术在地面网络中存在容量不足和覆盖有限的局限性。", "method": "该研究提出了一种增强型小区切换方案，将卫星、高空平台站（HAPS）和无人机（UAV）等非地面网络（NTN）集成到地面网络（TN）中。引入了多层小区切换方法，动态地在网络层之间分流用户，并采用上下文感知策略。此外，探索了人工智能（特别是生成式AI）在数据压缩、不同网络层间切换优化和增强设备兼容性方面的作用。", "result": "通过集成NTN，显著提升了能源节约、容量、覆盖和操作灵活性。案例研究证实，该方法大幅改善了网络功耗和用户满意度。", "conclusion": "所提出的NTN集成、多层小区切换和AI优化方法，为未来网络的能源效率和用户满意度带来了巨大潜力。"}}
{"id": "2508.10333", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10333", "abs": "https://arxiv.org/abs/2508.10333", "authors": ["Wenxuan Song", "Ziyang Zhou", "Han Zhao", "Jiayi Chen", "Pengxiang Ding", "Haodong Yan", "Yuxin Huang", "Feilong Tang", "Donglin Wang", "Haoang Li"], "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic\nagents to integrate multimodal understanding with action execution. However,\nour empirical analysis reveals that current VLAs struggle to allocate visual\nattention to target regions. Instead, visual attention is always dispersed. To\nguide the visual attention grounding on the correct target, we propose\nReconVLA, a reconstructive VLA model with an implicit grounding paradigm.\nConditioned on the model's visual outputs, a diffusion transformer aims to\nreconstruct the gaze region of the image, which corresponds to the target\nmanipulated objects. This process prompts the VLA model to learn fine-grained\nrepresentations and accurately allocate visual attention, thus effectively\nleveraging task-specific visual information and conducting precise\nmanipulation. Moreover, we curate a large-scale pretraining dataset comprising\nover 100k trajectories and 2 million data samples from open-source robotic\ndatasets, further boosting the model's generalization in visual reconstruction.\nExtensive experiments in simulation and the real world demonstrate the\nsuperiority of our implicit grounding method, showcasing its capabilities of\nprecise manipulation and generalization. Our project page is\nhttps://zionchow.github.io/ReconVLA/.", "AI": {"tldr": "ReconVLA提出了一种基于隐式重建的视觉语言动作模型，通过扩散变换器重建注视区域来引导视觉注意力，解决了现有VLA模型视觉注意力分散的问题，实现了更精确的机器人操作和更好的泛化能力。", "motivation": "现有视觉语言动作（VLA）模型在将视觉注意力聚焦到目标区域时存在困难，导致注意力分散，无法有效利用任务相关的视觉信息进行精确操作。", "method": "提出ReconVLA模型，采用隐式接地范式。模型通过一个扩散变换器，在模型视觉输出的条件下，重建图像中与目标操作对象对应的注视区域。此外，构建了一个包含超过10万条轨迹和200万个数据样本的大规模预训练数据集，以增强模型的视觉重建泛化能力。", "result": "该方法促使VLA模型学习到细粒度表示并准确分配视觉注意力，从而有效利用任务特定视觉信息并进行精确操作。在模拟和真实世界的广泛实验证明了其隐式接地方法的优越性，展示了其精确操作和泛化能力。", "conclusion": "ReconVLA通过引入隐式重建范式和大规模预训练数据集，成功解决了现有VLA模型视觉注意力分散的问题，显著提升了机器人精确操作能力和泛化性。"}}
{"id": "2508.09998", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09998", "abs": "https://arxiv.org/abs/2508.09998", "authors": ["Lucie-Aimée Kaffee", "Giada Pistilli", "Yacine Jernite"], "title": "INTIMA: A Benchmark for Human-AI Companionship Behavior", "comment": null, "summary": "AI companionship, where users develop emotional bonds with AI systems, has\nemerged as a significant pattern with positive but also concerning\nimplications. We introduce Interactions and Machine Attachment Benchmark\n(INTIMA), a benchmark for evaluating companionship behaviors in language\nmodels. Drawing from psychological theories and user data, we develop a\ntaxonomy of 31 behaviors across four categories and 368 targeted prompts.\nResponses to these prompts are evaluated as companionship-reinforcing,\nboundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,\nand Claude-4 reveals that companionship-reinforcing behaviors remain much more\ncommon across all models, though we observe marked differences between models.\nDifferent commercial providers prioritize different categories within the more\nsensitive parts of the benchmark, which is concerning since both appropriate\nboundary-setting and emotional support matter for user well-being. These\nfindings highlight the need for more consistent approaches to handling\nemotionally charged interactions.", "AI": {"tldr": "随着AI伴侣现象的兴起，本研究引入了INTIMA基准来评估语言模型在陪伴行为中的表现。结果显示，当前模型在强化陪伴行为上表现突出，但在边界设定和情感支持方面存在不一致，凸显了统一处理情感交互的必要性。", "motivation": "用户与AI系统建立情感联系的AI伴侣现象日益普遍，这既带来了积极影响，也引发了潜在担忧。研究旨在评估语言模型在这一复杂领域中的行为，以确保用户福祉。", "method": "提出了“互动与机器依恋基准 (INTIMA)”，该基准基于心理学理论和用户数据，开发了一个包含31种行为（分为四类）和368个特定提示的分类法。模型对这些提示的响应被评估为“强化陪伴”、“维持边界”或“中性”。研究将INTIMA应用于Gemma-3、Phi-4、o3-mini和Claude-4等主流语言模型进行评估。", "result": "评估结果显示，所有被测模型中“强化陪伴”的行为更为普遍，但在不同模型之间存在显著差异。此外，不同的商业提供商在基准测试的敏感部分中优先处理的类别也各不相同。", "conclusion": "研究结果强调，在处理情感驱动的交互时，需要更一致的方法来平衡适当的边界设定和情感支持，因为这两者都对用户福祉至关重要。"}}
{"id": "2508.10113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10113", "abs": "https://arxiv.org/abs/2508.10113", "authors": ["Kaixin Peng", "Mengyang Zhao", "Haiyang Yu", "Teng Fu", "Bin Li"], "title": "Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs", "comment": null, "summary": "As the oldest mature writing system, Oracle Bone Script (OBS) has long posed\nsignificant challenges for archaeological decipherment due to its rarity,\nabstractness, and pictographic diversity. Current deep learning-based methods\nhave made exciting progress on the OBS decipherment task, but existing\napproaches often ignore the intricate connections between glyphs and the\nsemantics of OBS. This results in limited generalization and interpretability,\nespecially when addressing zero-shot settings and undeciphered OBS. To this\nend, we propose an interpretable OBS decipherment method based on Large\nVision-Language Models, which synergistically combines radical analysis and\npictograph-semantic understanding to bridge the gap between glyphs and meanings\nof OBS. Specifically, we propose a progressive training strategy that guides\nthe model from radical recognition and analysis to pictographic analysis and\nmutual analysis, thus enabling reasoning from glyph to meaning. We also design\na Radical-Pictographic Dual Matching mechanism informed by the analysis\nresults, significantly enhancing the model's zero-shot decipherment\nperformance. To facilitate model training, we propose the Pictographic\nDecipherment OBS Dataset, which comprises 47,157 Chinese characters annotated\nwith OBS images and pictographic analysis texts. Experimental results on public\nbenchmarks demonstrate that our approach achieves state-of-the-art Top-10\naccuracy and superior zero-shot decipherment capabilities. More importantly,\nour model delivers logical analysis processes, possibly providing\narchaeologically valuable reference results for undeciphered OBS, and thus has\npotential applications in digital humanities and historical research. The\ndataset and code will be released in https://github.com/PKXX1943/PD-OBS.", "AI": {"tldr": "本文提出一种基于大视觉语言模型（LVLMs）的可解释甲骨文（OBS）释读方法，通过部首和象形语义理解弥合字形与含义之间的鸿沟，并在零样本释读和未释读甲骨文上表现出色。", "motivation": "甲骨文释读因其稀有性、抽象性和象形多样性而极具挑战。现有深度学习方法在释读上取得进展，但忽略了字形与语义之间的复杂联系，导致泛化能力和可解释性有限，尤其在零样本和未释读甲骨文场景下表现不佳。", "method": "提出一种结合部首分析和象形语义理解的可解释甲骨文释读方法。采用渐进式训练策略，引导模型从部首识别分析到象形分析再到相互分析。设计了受分析结果启发的部首-象形双重匹配机制，增强零样本释读性能。构建了包含47,157个带有甲骨文图像和象形分析文本的汉字的“象形释读甲骨文数据集”（PD-OBS）。", "result": "在公开基准测试中，该方法实现了最先进的Top-10准确率和卓越的零样本释读能力。更重要的是，模型提供了逻辑分析过程，可能为未释读甲骨文提供考古学上有价值的参考结果。", "conclusion": "该方法通过结合部首和象形语义分析，有效提升了甲骨文释读的准确性和可解释性，尤其在零样本和未释读甲骨文方面展现出巨大潜力，对数字人文和历史研究具有重要的应用价值。"}}
{"id": "2508.10146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10146", "abs": "https://arxiv.org/abs/2508.10146", "authors": ["Hana Derouiche", "Zaki Brahmi", "Haithem Mazeni"], "title": "Agentic AI Frameworks: Architectures, Protocols, and Design Challenges", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has ushered in a transformative\nparadigm in artificial intelligence, Agentic AI, where intelligent agents\nexhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent\ncoordination. This paper provides a systematic review and comparative analysis\nof leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,\nSemantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural\nprinciples, communication mechanisms, memory management, safety guardrails, and\nalignment with service-oriented computing paradigms. Furthermore, we identify\nkey limitations, emerging trends, and open challenges in the field. To address\nthe issue of agent communication, we conduct an in-depth analysis of protocols\nsuch as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network\nProtocol (ANP), and Agora. Our findings not only establish a foundational\ntaxonomy for Agentic AI systems but also propose future research directions to\nenhance scalability, robustness, and interoperability. This work serves as a\ncomprehensive reference for researchers and practitioners working to advance\nthe next generation of autonomous AI systems.", "AI": {"tldr": "本文对主流的Agentic AI框架（如CrewAI、LangGraph、AutoGen等）进行了系统性回顾和比较分析，并深入探讨了代理间通信协议，旨在为研究人员和实践者提供全面参考。", "motivation": "大型语言模型（LLMs）的出现催生了Agentic AI范式，其中智能代理展现出目标导向的自主性、上下文推理和多代理协调能力。因此，需要对现有Agentic AI框架进行系统评估，并识别其局限性、趋势和挑战。", "method": "本文采用系统性回顾和比较分析的方法，评估了CrewAI、LangGraph、AutoGen、Semantic Kernel、Agno、Google ADK和MetaGPT等领先的Agentic AI框架的架构原则、通信机制、内存管理、安全防护和与面向服务计算的对齐。此外，还深入分析了Contract Net Protocol (CNP)、Agent-to-Agent (A2A)、Agent Network Protocol (ANP)和Agora等代理通信协议。", "result": "研究结果建立了Agentic AI系统的基础分类法，识别了该领域的关键局限性、新兴趋势和开放挑战。同时，对代理通信协议进行了深入分析。", "conclusion": "本文为Agentic AI系统提供了全面的参考，并提出了未来研究方向，以提高可扩展性、鲁棒性和互操作性，旨在推动下一代自主AI系统的发展。"}}
{"id": "2508.10307", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10307", "abs": "https://arxiv.org/abs/2508.10307", "authors": ["Zhaoming Kong", "Jiahuan Zhang", "Xiaowei Yang"], "title": "Efficient Image Denoising Using Global and Local Circulant Representation", "comment": null, "summary": "The advancement of imaging devices and countless image data generated\neveryday impose an increasingly high demand on efficient and effective image\ndenoising. In this paper, we present a computationally simple denoising\nalgorithm, termed Haar-tSVD, aiming to explore the nonlocal self-similarity\nprior and leverage the connection between principal component analysis (PCA)\nand the Haar transform under circulant representation. We show that global and\nlocal patch correlations can be effectively captured through a unified\ntensor-singular value decomposition (t-SVD) projection with the Haar transform.\nThis results in a one-step, highly parallelizable filtering method that\neliminates the need for learning local bases to represent image patches,\nstriking a balance between denoising speed and performance. Furthermore, we\nintroduce an adaptive noise estimation scheme based on a CNN estimator and\neigenvalue analysis to enhance the robustness and adaptability of the proposed\nmethod. Experiments on different real-world denoising tasks validate the\nefficiency and effectiveness of Haar-tSVD for noise removal and detail\npreservation. Datasets, code and results are publicly available at\nhttps://github.com/ZhaomingKong/Haar-tSVD.", "AI": {"tldr": "本文提出了一种名为Haar-tSVD的图像去噪算法，它利用非局部自相似性、Haar变换和张量奇异值分解（t-SVD）来高效去除噪声并保留细节，同时结合了自适应噪声估计。", "motivation": "随着成像设备和日常生成图像数据的快速发展，对高效、有效的图像去噪提出了越来越高的要求。", "method": "该方法提出了Haar-tSVD去噪算法，探索了非局部自相似性先验，并利用主成分分析（PCA）与循环表示下Haar变换之间的联系。通过结合Haar变换的统一张量奇异值分解（t-SVD）投影，有效捕获全局和局部图像块相关性。这形成了一个一步式、高度并行化的滤波方法，无需学习局部基来表示图像块。此外，引入了一种基于CNN估计器和特征值分析的自适应噪声估计方案，以增强方法的鲁棒性和适应性。", "result": "实验结果表明，Haar-tSVD在不同的真实世界去噪任务中，对噪声去除和细节保留都表现出高效性和有效性，并在去噪速度和性能之间取得了平衡。", "conclusion": "Haar-tSVD是一种计算简单、高效且有效的图像去噪算法，通过结合Haar变换、t-SVD和自适应噪声估计，在去噪性能和速度之间取得了良好的平衡，适用于实际的噪声去除和细节保留任务。"}}
{"id": "2508.10891", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.10891", "abs": "https://arxiv.org/abs/2508.10891", "authors": ["Oumaima Barhoumi", "Ghazal Farhani", "Taufiq Rahman", "Mohamed H. Zaki", "Sofiène Tahar", "Fadi Araji"], "title": "Fuel Consumption in Platoons: A Literature Review", "comment": null, "summary": "Platooning has emerged as a promising strategy for improving fuel efficiency\nin automated vehicle systems, with significant implications for reducing\nemissions and operational costs. While existing literature on vehicle\nplatooning primarily focuses on individual aspects such as aerodynamic drag\nreduction or specific control strategies, this work takes a more comprehensive\napproach by bringing together a wide range of factors and components that\ncontribute to fuel savings in platoons. In this literature review, we examine\nthe impact of platooning on fuel consumption, highlighting the key components\nof platoon systems, the factors and actors influencing fuel savings, methods\nfor estimating fuel use, and the effect of platoon instability on efficiency.\nFurthermore, we study the role of reduced aerodynamic drag, vehicle\ncoordination, and the challenges posed by instability in real-world conditions.\nBy compiling insights from recent studies, this work provides a comprehensive\noverview of the latest advancements in platooning technologies and highlights\nboth the challenges and opportunities for future research to maximize fuel\nsavings in real-world scenarios.", "AI": {"tldr": "该文献综述全面审视了自动驾驶车队（Platooning）在提高燃油效率方面的潜力，涵盖了影响燃油节约的各种因素、组件、评估方法以及不稳定性带来的挑战。", "motivation": "车队编队被认为是提高自动驾驶车辆燃油效率、减少排放和运营成本的有效策略。现有研究多集中于单一方面，缺乏综合性视角，因此需要一个更全面的分析来整合各种影响燃油节约的因素和组件。", "method": "本文采用文献综述的方法，整合了近期研究的见解，系统性地审查了车队编队对燃油消耗的影响。", "result": "研究结果涵盖了车队系统中的关键组件、影响燃油节约的因素和参与者、燃油使用估算方法、车队不稳定性对效率的影响、气动阻力减少的作用、车辆协调的重要性，以及真实世界条件下不稳定性带来的挑战。", "conclusion": "该综述提供了车队编队技术最新进展的全面概览，并指出了在真实场景中最大化燃油节约所面临的挑战和未来的研究机遇。"}}
{"id": "2508.10363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10363", "abs": "https://arxiv.org/abs/2508.10363", "authors": ["Donipolo Ghimire", "Aamodh Suresh", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "BEASST: Behavioral Entropic Gradient based Adaptive Source Seeking for Mobile Robots", "comment": null, "summary": "This paper presents BEASST (Behavioral Entropic Gradient-based Adaptive\nSource Seeking for Mobile Robots), a novel framework for robotic source seeking\nin complex, unknown environments. Our approach enables mobile robots to\nefficiently balance exploration and exploitation by modeling normalized signal\nstrength as a surrogate probability of source location. Building on Behavioral\nEntropy(BE) with Prelec's probability weighting function, we define an\nobjective function that adapts robot behavior from risk-averse to risk-seeking\nbased on signal reliability and mission urgency. The framework provides\ntheoretical convergence guarantees under unimodal signal assumptions and\npractical stability under bounded disturbances. Experimental validation across\nDARPA SubT and multi-room scenarios demonstrates that BEASST consistently\noutperforms state-of-the-art methods, achieving 15% reduction in path length\nand 20% faster source localization through intelligent uncertainty-driven\nnavigation that dynamically transitions between aggressive pursuit and cautious\nexploration.", "AI": {"tldr": "本文提出BEASST框架，一个新颖的基于行为熵梯度自适应寻源方法，使移动机器人在复杂未知环境中高效平衡探索与利用。", "motivation": "在复杂、未知环境中，移动机器人需要高效地平衡探索与利用以寻找信号源，这是现有方法面临的挑战。", "method": "BEASST将归一化信号强度建模为源位置的替代概率，并基于行为熵（BE）和Prelec概率加权函数定义了一个目标函数。该函数根据信号可靠性和任务紧急性，自适应地调整机器人行为（从规避风险到寻求风险）。框架在单峰信号假设下提供了理论收敛性保证，并在有界扰动下具备实用稳定性。", "result": "在DARPA SubT和多房间场景的实验验证表明，BEASST持续优于现有先进方法，实现了路径长度减少15%，寻源速度加快20%。这得益于其智能的不确定性驱动导航，能够动态切换激进追踪和谨慎探索。", "conclusion": "BEASST框架通过创新的行为熵和自适应行为策略，显著提升了移动机器人在复杂未知环境中的寻源效率和性能，是寻源领域的一大进步。"}}
{"id": "2508.09999", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09999", "abs": "https://arxiv.org/abs/2508.09999", "authors": ["Yuzhuo Xiao", "Zeyu Han", "Yuhan Wang", "Huaizu Jiang"], "title": "XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs", "comment": "For associated code and dataset, see https://github.com/neu-vi/XFacta", "summary": "The rapid spread of multimodal misinformation on social media calls for more\neffective and robust detection methods. Recent advances leveraging multimodal\nlarge language models (MLLMs) have shown the potential in addressing this\nchallenge. However, it remains unclear exactly where the bottleneck of existing\napproaches lies (evidence retrieval v.s. reasoning), hindering the further\nadvances in this field. On the dataset side, existing benchmarks either contain\noutdated events, leading to evaluation bias due to discrepancies with\ncontemporary social media scenarios as MLLMs can simply memorize these events,\nor artificially synthetic, failing to reflect real-world misinformation\npatterns. Additionally, it lacks comprehensive analyses of MLLM-based model\ndesign strategies. To address these issues, we introduce XFacta, a\ncontemporary, real-world dataset that is better suited for evaluating\nMLLM-based detectors. We systematically evaluate various MLLM-based\nmisinformation detection strategies, assessing models across different\narchitectures and scales, as well as benchmarking against existing detection\nmethods. Building on these analyses, we further enable a semi-automatic\ndetection-in-the-loop framework that continuously updates XFacta with new\ncontent to maintain its contemporary relevance. Our analysis provides valuable\ninsights and practices for advancing the field of multimodal misinformation\ndetection. The code and data have been released.", "AI": {"tldr": "该论文介绍了XFacta，一个用于评估多模态大语言模型（MLLMs）在多模态虚假信息检测方面的新型、当代、真实世界数据集，并系统评估了各种基于MLLM的检测策略，旨在解决现有方法瓶颈和数据集不足的问题。", "motivation": "多模态虚假信息在社交媒体上迅速传播，需要更有效和鲁棒的检测方法。现有方法在证据检索和推理方面的瓶颈尚不明确，阻碍了进一步发展。现有数据集存在过时事件（导致评估偏差）或人工合成（未能反映真实模式）的问题。此外，缺乏对基于MLLM的模型设计策略的全面分析。", "method": "引入了XFacta，一个当代、真实世界的数据集，更适合评估基于MLLM的检测器。系统评估了各种基于MLLM的多模态虚假信息检测策略，包括不同架构和规模的模型，并与现有检测方法进行基准测试。开发了一个半自动的“检测循环”框架，通过持续更新XFacta来保持其当代相关性。", "result": "通过分析，提供了宝贵的见解和实践，以推动多模态虚假信息检测领域的发展。代码和数据已发布。", "conclusion": "XFacta数据集和基于MLLM的系统评估为多模态虚假信息检测领域提供了重要贡献，解决了现有数据集和方法分析的局限性，并为未来研究提供了方向。"}}
{"id": "2508.10132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10132", "abs": "https://arxiv.org/abs/2508.10132", "authors": ["Arianna Bunnell", "Devon Cataldi", "Yannik Glaser", "Thomas K. Wolfgruber", "Steven Heymsfield", "Alan B. Zonderman", "Thomas L. Kelly", "Peter Sadowski", "John A. Shepherd"], "title": "Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging", "comment": "Preprint of manuscript accepted to the ShapeMI workshop at MICCAI\n  2025", "summary": "Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost\nwhole-body imaging modality, widely used for body composition assessment. We\ndevelop and validate a deep learning method for automatic fiducial point\nplacement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method\nachieves 99.5% percentage correct keypoints in an external testing dataset. To\ndemonstrate the value for shape and appearance modeling (SAM), our method is\nused to place keypoints on 35,928 scans for five different TBDXA imaging modes,\nthen associations with health markers are tested in two cohorts not used for\nSAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature\ndistributions associated with health biomarkers are shown to corroborate\nexisting evidence and generate new hypotheses on body composition and shape's\nrelationship to various frailty, metabolic, inflammation, and cardiometabolic\nhealth markers. Evaluation scripts, model weights, automatic point file\ngeneration code, and triangulation files are available at\nhttps://github.com/hawaii-ai/dxa-pointplacement.", "AI": {"tldr": "该研究开发并验证了一种深度学习方法，用于在全身双能X射线吸收法（TBDXA）扫描图像上自动放置关键点，并利用这些关键点进行体型和外观建模，以探索其与健康生物标志物的关联。", "motivation": "TBDXA是一种低成本的全身成像技术，广泛用于评估身体成分。为了更有效地进行体型和外观建模（SAM）并研究其与健康指标的关联，需要一种自动化、高精度的关键点放置方法。", "method": "研究使用1683张手动标注的TBDXA扫描图像训练并验证了一个深度学习模型，用于自动放置关键点。随后，该方法被应用于35928张不同成像模式的TBDXA扫描图像，生成SAM特征。最后，通过双样本Kolmogorov-Smirnov检验，在两个独立队列中测试了SAM特征与健康生物标志物之间的关联。", "result": "该方法在外部测试数据集上达到了99.5%的关键点正确率。通过SAM特征分析，发现了与现有证据相符的身体成分和形状与多种健康指标（如虚弱、代谢、炎症和心血管代谢）之间的关联，并提出了新的假设。", "conclusion": "所开发的深度学习方法能够高精度地自动放置TBDXA扫描图像上的关键点，这极大地促进了体型和外观建模。该方法在探索身体成分和形状与各种健康生物标志物之间的关系方面具有显著价值，并有望生成新的医学发现。"}}
{"id": "2508.10152", "categories": ["cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.10152", "abs": "https://arxiv.org/abs/2508.10152", "authors": ["Doaa Allabadi", "Kyle Bradbury", "Jordan M. Malof"], "title": "Improving and Evaluating Open Deep Research Agents", "comment": "8 pages, 2 figures, 2 tables", "summary": "We focus here on Deep Research Agents (DRAs), which are systems that can take\na natural language prompt from a user, and then autonomously search for, and\nutilize, internet-based content to address the prompt. Recent DRAs have\ndemonstrated impressive capabilities on public benchmarks however, recent\nresearch largely involves proprietary closed-source systems. At the time of\nthis work, we only found one open-source DRA, termed Open Deep Research (ODR).\nIn this work we adapt the challenging recent BrowseComp benchmark to compare\nODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),\ncomprising a subset of BrowseComp, as a more computationally-tractable DRA\nbenchmark for academic labs. We benchmark ODR and two other proprietary systems\non BC-Small: one system from Anthropic and one system from Google. We find that\nall three systems achieve 0% accuracy on the test set of 60 questions. We\nintroduce three strategic improvements to ODR, resulting in the ODR+ model,\nwhich achieves a state-of-the-art 10% success rate on BC-Small among both\nclosed-source and open-source systems. We report ablation studies indicating\nthat all three of our improvements contributed to the success of ODR+.", "AI": {"tldr": "本文关注深度研究智能体（DRAs），提出了一种计算成本更低的基准BrowseComp-Small (BC-Small)，并在此基准上评估了唯一的开源DRA（ODR）和两个专有系统。初始结果显示所有系统准确率均为0%。通过对ODR进行三项战略性改进，得到了ODR+模型，其在BC-Small上实现了10%的成功率，达到当前最佳水平。", "motivation": "深度研究智能体（DRAs）展现出强大能力，但目前大多数是专有闭源系统，阻碍了学术研究。现有唯一的开源DRA（ODR）亟需改进，同时学术实验室需要一个计算成本更低的DRA基准。", "method": "本文将BrowseComp基准调整为计算成本更低的BrowseComp-Small (BC-Small)；使用BC-Small对开源DRA (ODR) 和两个专有系统（Anthropic、Google）进行基准测试；对ODR提出了三项战略性改进，创建了ODR+模型；进行了消融研究以评估各项改进的贡献。", "result": "在BC-Small的60个问题测试集上，ODR以及来自Anthropic和Google的两个专有系统初始准确率均为0%。经过三项战略性改进后，ODR+模型在BC-Small上实现了10%的成功率，达到开源和闭源系统中的最新最佳水平。消融研究表明所有三项改进都对ODR+的成功有所贡献。", "conclusion": "现有的深度研究智能体，无论是开源还是专有，在挑战性的BrowseComp-Small基准上表现均不佳。通过对开源ODR模型进行战略性改进（形成ODR+），可以显著提升性能，达到该基准的最新最佳水平，证明了开源系统的巨大潜力。"}}
{"id": "2508.10313", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10313", "abs": "https://arxiv.org/abs/2508.10313", "authors": ["Jixiang Chen", "Yiqun Lin", "Yi Qin", "Hualiang Wang", "Xiaomeng Li"], "title": "Cross-view Generalized Diffusion Model for Sparse-view CT Reconstruction", "comment": "MICCAI 2025 Spotlight", "summary": "Sparse-view computed tomography (CT) reduces radiation exposure by\nsubsampling projection views, but conventional reconstruction methods produce\nsevere streak artifacts with undersampled data. While deep-learning-based\nmethods enable single-step artifact suppression, they often produce\nover-smoothed results under significant sparsity. Though diffusion models\nimprove reconstruction via iterative refinement and generative priors, they\nrequire hundreds of sampling steps and struggle with stability in highly sparse\nregimes. To tackle these concerns, we present the Cross-view Generalized\nDiffusion Model (CvG-Diff), which reformulates sparse-view CT reconstruction as\na generalized diffusion process. Unlike existing diffusion approaches that rely\non stochastic Gaussian degradation, CvG-Diff explicitly models image-domain\nartifacts caused by angular subsampling as a deterministic degradation\noperator, leveraging correlations across sparse-view CT at different sample\nrates. To address the inherent artifact propagation and inefficiency of\nsequential sampling in generalized diffusion model, we introduce two\ninnovations: Error-Propagating Composite Training (EPCT), which facilitates\nidentifying error-prone regions and suppresses propagated artifacts, and\nSemantic-Prioritized Dual-Phase Sampling (SPDPS), an adaptive strategy that\nprioritizes semantic correctness before detail refinement. Together, these\ninnovations enable CvG-Diff to achieve high-quality reconstructions with\nminimal iterations, achieving 38.34 dB PSNR and 0.9518 SSIM for 18-view CT\nusing only \\textbf{10} steps on AAPM-LDCT dataset. Extensive experiments\ndemonstrate the superiority of CvG-Diff over state-of-the-art sparse-view CT\nreconstruction methods. The code is available at\nhttps://github.com/xmed-lab/CvG-Diff.", "AI": {"tldr": "CvG-Diff是一种新型的广义扩散模型，通过将稀疏视图CT重建重构为确定性降级过程，并引入误差传播复合训练和语义优先双阶段采样策略，实现了高质量、高效率的稀疏视图CT图像重建，解决了传统方法伪影严重、过度平滑以及扩散模型效率低、稳定性差的问题。", "motivation": "稀疏视图CT能减少辐射暴露，但传统重建方法会产生严重伪影。现有深度学习方法在高度稀疏时易导致过度平滑。扩散模型虽能改进重建，但需要大量采样步骤且在高度稀疏场景下稳定性差。", "method": "CvG-Diff将稀疏视图CT重建重构为广义扩散过程，明确将欠采样引起的图像域伪影建模为确定性降级算子。为解决伪影传播和顺序采样效率低下问题，引入了两项创新：误差传播复合训练（EPCT）以识别误差区域并抑制伪影传播，以及语义优先双阶段采样（SPDPS）这一自适应策略，优先保证语义正确性再进行细节细化。", "result": "CvG-Diff能在极少迭代次数下实现高质量重建，例如在AAPM-LDCT数据集上，18视图CT仅需10步即可达到38.34 dB PSNR和0.9518 SSIM。实验证明其性能优于现有最先进的稀疏视图CT重建方法。", "conclusion": "CvG-Diff通过创新的广义扩散模型框架和训练/采样策略，成功解决了稀疏视图CT重建中的伪影、平滑和效率问题，实现了高质量且高效的图像重建。"}}
{"id": "2508.10423", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10423", "abs": "https://arxiv.org/abs/2508.10423", "authors": ["Qi Liu", "Xiaopeng Zhang", "Mingshan Tan", "Shuaikang Ma", "Jinliang Ding", "Yanjie Li"], "title": "MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion", "comment": null, "summary": "This paper proposes a novel method to enhance locomotion for a single\nhumanoid robot through cooperative-heterogeneous multi-agent deep reinforcement\nlearning (MARL). While most existing methods typically employ single-agent\nreinforcement learning algorithms for a single humanoid robot or MARL\nalgorithms for multi-robot system tasks, we propose a distinct paradigm:\napplying cooperative-heterogeneous MARL to optimize locomotion for a single\nhumanoid robot. The proposed method, multi-agent reinforcement learning for\nsingle humanoid locomotion (MASH), treats each limb (legs and arms) as an\nindependent agent that explores the robot's action space while sharing a global\ncritic for cooperative learning. Experiments demonstrate that MASH accelerates\ntraining convergence and improves whole-body cooperation ability, outperforming\nconventional single-agent reinforcement learning methods. This work advances\nthe integration of MARL into single-humanoid-robot control, offering new\ninsights into efficient locomotion strategies.", "AI": {"tldr": "本文提出MASH方法，通过将单个人形机器人的肢体视为独立的异构智能体，并利用合作多智能体强化学习来优化其运动能力，从而加速训练收敛并提高全身协调性。", "motivation": "现有方法多采用单智能体强化学习控制单机器人或多智能体强化学习控制多机器人系统。本文旨在探索一种新范式，即应用合作异构多智能体强化学习来优化单个人形机器人的运动能力，以克服传统方法的局限性。", "method": "本文提出“用于单人形机器人运动的多智能体强化学习（MASH）”方法，将单个人形机器人的每个肢体（腿和手臂）视为一个独立的异构智能体，这些智能体在探索机器人动作空间的同时，共享一个全局评论家进行合作学习。", "result": "实验证明，MASH方法能够加速训练收敛，并显著提高机器人的全身协调能力。与传统的单智能体强化学习方法相比，MASH表现出更优越的性能。", "conclusion": "这项工作推动了多智能体强化学习在单个人形机器人控制领域的整合与应用，为开发高效的运动策略提供了新的见解和思路。"}}
{"id": "2508.10371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10371", "abs": "https://arxiv.org/abs/2508.10371", "authors": ["Wenqi Zheng", "Yutaka Arakawa"], "title": "Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning", "comment": null, "summary": "Reinforcement learning in large reasoning models enables learning from\nfeedback on their outputs, making it particularly valuable in scenarios where\nfine-tuning data is limited. However, its application in multi-modal human\nactivity recognition (HAR) domains remains largely underexplored. Our work\nextends reinforcement learning to the human activity recognition domain with\nmultimodal large language models. By incorporating visual reinforcement\nlearning in the training process, the model's generalization ability on\nfew-shot recognition can be greatly improved. Additionally, visual\nreinforcement learning can enhance the model's reasoning ability and enable\nexplainable analysis in the inference stage. We name our few-shot human\nactivity recognition method with visual reinforcement learning FAVOR.\nSpecifically, our approach first utilizes a multimodal large language model\n(MLLM) to generate multiple candidate responses for the human activity image,\neach containing reasoning traces and final answers. These responses are then\nevaluated using reward functions, and the MLLM model is subsequently optimized\nusing the Group Relative Policy Optimization (GRPO) algorithm. In this way, the\nMLLM model can be adapted to human activity recognition with only a few\nsamples. Extensive experiments on four human activity recognition datasets and\nfive different settings demonstrate the superiority of the proposed method.", "AI": {"tldr": "本文提出FAVOR方法，将视觉强化学习应用于多模态大语言模型（MLLM），以解决少样本人类活动识别（HAR）问题，显著提升了模型的泛化、推理能力和可解释性。", "motivation": "强化学习在数据有限场景下具有重要价值，但其在多模态人类活动识别（HAR）领域的应用尚未得到充分探索。", "method": "该方法名为FAVOR，将视觉强化学习引入多模态大语言模型（MLLM）进行人类活动识别。具体流程是：首先，MLLM为人类活动图像生成包含推理轨迹和最终答案的多个候选响应；接着，使用奖励函数评估这些响应；最后，利用组相对策略优化（GRPO）算法对MLLM进行优化，使其能仅通过少量样本适应HAR任务。", "result": "该方法显著提升了模型在少样本识别上的泛化能力，增强了模型的推理能力，并在推理阶段实现了可解释性分析。在四个HAR数据集和五种不同设置上的大量实验证明了所提方法的优越性。", "conclusion": "通过结合视觉强化学习，FAVOR方法成功地使多模态大语言模型适应了少样本人类活动识别任务，并在泛化能力、推理能力和可解释性分析方面取得了显著改进。"}}
{"id": "2508.10000", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10000", "abs": "https://arxiv.org/abs/2508.10000", "authors": ["Chenhao Xue", "Yuanzhe Jin", "Adrian Carrasco-Revilla", "Joyraj Chakraborty", "Min Chen"], "title": "AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification", "comment": null, "summary": "When developing text classification models for real world applications, one\nmajor challenge is the difficulty to collect sufficient data for all text\nclasses. In this work, we address this challenge by utilizing large language\nmodels (LLMs) to generate synthetic data and using such data to improve the\nperformance of the models without waiting for more real data to be collected\nand labelled. As an LLM generates different synthetic data in response to\ndifferent input examples, we formulate an automated workflow, which searches\nfor input examples that lead to more ``effective'' synthetic data for improving\nthe model concerned. We study three search strategies with an extensive set of\nexperiments, and use experiment results to inform an ensemble algorithm that\nselects a search strategy according to the characteristics of a class. Our\nfurther experiments demonstrate that this ensemble approach is more effective\nthan each individual strategy in our automated workflow for improving\nclassification models using LLMs.", "AI": {"tldr": "该研究利用大型语言模型（LLMs）生成合成数据，并通过自动化工作流和集成策略来优化输入示例，从而在数据稀缺的情况下提升文本分类模型的性能。", "motivation": "在实际应用中，文本分类模型开发面临的主要挑战是难以收集到所有文本类别足够的真实数据。", "method": "利用LLMs生成合成数据；设计自动化工作流，搜索能够生成“有效”合成数据的输入示例；研究了三种搜索策略；根据类别特征，提出并使用集成算法选择最佳搜索策略。", "result": "实验证明，所提出的集成方法比自动化工作流中的每种单独策略更有效地改进了使用LLMs的分类模型。", "conclusion": "通过自动化工作流和集成策略选择，利用LLMs生成有效的合成数据，可以显著提升文本分类模型在数据不足情况下的性能。"}}
{"id": "2508.10133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10133", "abs": "https://arxiv.org/abs/2508.10133", "authors": ["Thanh-Dat Truong", "Christophe Bobda", "Nitin Agarwal", "Khoa Luu"], "title": "MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning", "comment": null, "summary": "Multimodal learning has gained much success in recent years. However, current\nmultimodal fusion methods adopt the attention mechanism of Transformers to\nimplicitly learn the underlying correlation of multimodal features. As a\nresult, the multimodal model cannot capture the essential features of each\nmodality, making it difficult to comprehend complex structures and correlations\nof multimodal inputs. This paper introduces a novel Multimodal Attention-based\nNormalizing Flow (MANGO) approach\\footnote{The source code of this work will be\npublicly available.} to developing explicit, interpretable, and tractable\nmultimodal fusion learning. In particular, we propose a new Invertible\nCross-Attention (ICA) layer to develop the Normalizing Flow-based Model for\nmultimodal data. To efficiently capture the complex, underlying correlations in\nmultimodal data in our proposed invertible cross-attention layer, we propose\nthree new cross-attention mechanisms: Modality-to-Modality Cross-Attention\n(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality\nCross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based\nNormalizing Flow to enable the scalability of our proposed method to\nhigh-dimensional multimodal data. Our experimental results on three different\nmultimodal learning tasks, i.e., semantic segmentation, image-to-image\ntranslation, and movie genre classification, have illustrated the\nstate-of-the-art (SoTA) performance of the proposed approach.", "AI": {"tldr": "本文提出了一种名为MANGO（Multimodal Attention-based Normalizing Flow）的新方法，通过可逆交叉注意力层和新型交叉注意力机制，实现显式、可解释且易处理的多模态融合学习，并在多个任务上达到最先进性能。", "motivation": "当前多模态融合方法依赖Transformer的注意力机制隐式学习特征关联，导致模型难以捕获各模态的本质特征，从而难以理解多模态输入的复杂结构和关联。", "method": "提出MANGO方法，核心是新的可逆交叉注意力（ICA）层，用于多模态数据的归一化流模型。为高效捕获复杂关联，ICA层设计了三种新型交叉注意力机制：模态间交叉注意力（MMCA）、模态内交叉注意力（IMCA）和可学习模态内交叉注意力（LICA）。同时引入多模态注意力归一化流以支持高维数据扩展性。", "result": "在语义分割、图像到图像翻译和电影类型分类这三个不同的多模态学习任务上，所提出的方法均取得了最先进（SoTA）的性能。", "conclusion": "MANGO提供了一种显式、可解释且易处理的多模态融合学习方法，通过其新颖的可逆交叉注意力层和机制，在多模态任务上展现出卓越的性能。"}}
{"id": "2508.10164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10164", "abs": "https://arxiv.org/abs/2508.10164", "authors": ["Bin Hong", "Jiayu Liu", "Zhenya Huang", "Kai Zhang", "Mengdi Zhang"], "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization", "comment": "19 pages, 5 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated strong\nperformance on complex tasks through long Chain-of-Thought (CoT) reasoning.\nHowever, their lengthy outputs increase computational costs and may lead to\noverthinking, raising challenges in balancing reasoning effectiveness and\nefficiency. Current methods for efficient reasoning often compromise reasoning\nquality or require extensive resources. This paper investigates efficient\nmethods to reduce the generation length of LRMs. We analyze generation path\ndistributions and filter generated trajectories through difficulty estimation.\nSubsequently, we analyze the convergence behaviors of the objectives of various\npreference optimization methods under a Bradley-Terry loss based framework.\nBased on the analysis, we propose Length Controlled Preference Optimization\n(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can\neffectively learn length preference with limited data and training. Extensive\nexperiments demonstrate that our approach significantly reduces the average\noutput length by over 50\\% across multiple benchmarks while maintaining the\nreasoning performance. Our work highlights the potential for computationally\nefficient approaches in guiding LRMs toward efficient reasoning.", "AI": {"tldr": "本文提出了一种名为LCPO的方法，通过直接平衡NLL损失相关的隐式奖励，有效减少大推理模型（LRMs）的输出长度，同时保持推理性能。", "motivation": "大推理模型（LRMs）的长链式思考（CoT）输出导致计算成本增加并可能引发过度思考。现有高效推理方法通常会牺牲推理质量或需要大量资源，因此需要一种在推理效率和效果之间取得平衡的方法。", "method": "本文首先分析了生成路径分布，并通过难度估计筛选生成轨迹。随后，在基于Bradley-Terry损失的框架下，分析了各种偏好优化方法的目标收敛行为。在此基础上，提出了一种长度控制偏好优化（LCPO）方法，直接平衡与NLL损失相关的隐式奖励，以有限的数据和训练学习长度偏好。", "result": "广泛的实验表明，LCPO方法在多个基准测试中将平均输出长度显著减少了50%以上，同时保持了推理性能。", "conclusion": "该研究强调了计算高效方法在引导大推理模型实现高效推理方面的潜力。"}}
{"id": "2508.10605", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10605", "abs": "https://arxiv.org/abs/2508.10605", "authors": ["Xinyi Wang", "Angeliki Katsenou", "David Bull"], "title": "DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality", "comment": "6 pages, 1 figure. Accepted for presentation at the 2025 IEEE\n  International Conference on Image Processing (ICIP)", "summary": "The rapid growth of user-generated (video) content (UGC) has driven increased\ndemand for research on no-reference (NR) perceptual video quality assessment\n(VQA). NR-VQA is a key component for large-scale video quality monitoring in\nsocial media and streaming applications where a pristine reference is not\navailable. This paper proposes a novel NR-VQA model based on spatio-temporal\nfragmentation driven by inter-frame variations. By leveraging these inter-frame\ndifferences, the model progressively analyses quality-sensitive regions at\nmultiple levels: frames, patches, and fragmented frames. It integrates frames,\nfragmented residuals, and fragmented frames aligned with residuals to\neffectively capture global and local information. The model extracts both 2D\nand 3D features in order to characterize these spatio-temporal variations.\nExperiments conducted on five UGC datasets and against state-of-the-art models\nranked our proposed method among the top 2 in terms of average rank correlation\n(DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered\nat a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on\naverage compared to the fastest existing NR-VQA method. Code and models are\npublicly available at: https://github.com/xinyiW915/DIVA-VQA.", "AI": {"tldr": "本文提出了一种基于时空碎片化和帧间变化的无参考视频质量评估（NR-VQA）模型，在用户生成内容（UGC）数据集上表现出色，且运行效率高。", "motivation": "用户生成内容（UGC）的快速增长，以及社交媒体和流媒体应用中缺乏原始参考视频的场景，推动了对无参考感知视频质量评估（NR-VQA）研究的迫切需求。", "method": "该模型利用帧间差异进行时空碎片化，逐步分析帧、补丁和碎片化帧等多个层面的质量敏感区域。它整合了帧、碎片化残差以及与残差对齐的碎片化帧，以捕获全局和局部信息，并提取2D和3D特征来表征时空变化。", "result": "在五个UGC数据集上与现有最先进模型进行实验，所提出的方法在平均秩相关性方面排名前两名（DIVA-VQA-L: 0.898，DIVA-VQA-B: 0.886）。同时，该方法具有较低的运行时复杂度，DIVA-VQA-B平均排名第一，DIVA-VQA-L平均排名第三，优于现有最快的NR-VQA方法。", "conclusion": "所提出的基于时空碎片化的NR-VQA模型在用户生成内容视频质量评估中表现出卓越的性能和高效率，有效解决了无参考视频质量评估的挑战。"}}
{"id": "2508.10634", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10634", "abs": "https://arxiv.org/abs/2508.10634", "authors": ["Mehdi Heydari Shahna", "Jouni Mattila"], "title": "Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots", "comment": null, "summary": "Deep neural networks (DNNs) can enable precise control while maintaining low\ncomputational costs by circumventing the need for dynamic modeling. However,\nthe deployment of such black-box approaches remains challenging for heavy-duty\nwheeled mobile robots (WMRs), which are subject to strict international\nstandards and prone to faults and disturbances. We designed a hierarchical\ncontrol policy for heavy-duty WMRs, monitored by two safety layers with\ndiffering levels of authority. To this end, a DNN policy was trained and\ndeployed as the primary control strategy, providing high-precision performance\nunder nominal operating conditions. When external disturbances arise and reach\na level of intensity such that the system performance falls below a predefined\nthreshold, a low-level safety layer intervenes by deactivating the primary\ncontrol policy and activating a model-free robust adaptive control (RAC)\npolicy. This transition enables the system to continue operating while ensuring\nstability by effectively managing the inherent trade-off between system\nrobustness and responsiveness. Regardless of the control policy in use, a\nhigh-level safety layer continuously monitors system performance during\noperation. It initiates a shutdown only when disturbances become sufficiently\nsevere such that compensation is no longer viable and continued operation would\njeopardize the system or its environment. The proposed synthesis of DNN and RAC\npolicy guarantees uniform exponential stability of the entire WMR system while\nadhering to safety standards to some extent. The effectiveness of the proposed\napproach was further validated through real-time experiments using a 6,000 kg\nWMR.", "AI": {"tldr": "本文提出一种针对重型轮式移动机器人（WMRs）的分层控制策略，结合深度神经网络（DNN）和鲁棒自适应控制（RAC），并通过双层安全机制确保在干扰下的高精度和稳定性。", "motivation": "深度神经网络在控制方面具有高精度和低计算成本的优势，但其“黑箱”特性使得在受严格国际标准约束且易受故障和干扰影响的重型轮式移动机器人上的部署面临挑战。", "method": "研究设计了一个分层控制策略：DNN作为主控制器提供高精度性能；当外部干扰导致性能下降时，低级安全层激活模型无关的鲁棒自适应控制（RAC）策略，取代DNN以确保系统稳定；无论何种控制策略，高级安全层持续监控系统性能，并在补偿不可行时启动关机，以防止系统或环境受损。", "result": "所提出的DNN与RAC策略的结合保证了整个WMR系统的均匀指数稳定性，并在一定程度上符合安全标准。通过使用一台6,000公斤的WMR进行实时实验，验证了该方法的有效性。", "conclusion": "该研究成功地将DNN与RAC策略融合，并辅以分层安全机制，解决了重型轮式移动机器人在复杂干扰下实现高精度控制和稳定运行的难题，为其实际部署提供了可靠的解决方案。"}}
{"id": "2508.10378", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10378", "abs": "https://arxiv.org/abs/2508.10378", "authors": ["Yu Chen", "Shu Miao", "Chunyu Wu", "Jingsong Mu", "Bo OuYang", "Xiang Li"], "title": "A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons", "comment": null, "summary": "Upper-limb exoskeletons are primarily designed to provide assistive support\nby accurately interpreting and responding to human intentions. In home-care\nscenarios, exoskeletons are expected to adapt their assistive configurations\nbased on the semantic information of the task, adjusting appropriately in\naccordance with the nature of the object being manipulated. However, existing\nsolutions often lack the ability to understand task semantics or\ncollaboratively plan actions with the user, limiting their generalizability. To\naddress this challenge, this paper introduces a semantic-aware framework that\nintegrates large language models into the task planning framework, enabling the\ndelivery of safe and intent-integrative assistance. The proposed approach\nbegins with the exoskeleton operating in transparent mode to capture the\nwearer's intent during object grasping. Once semantic information is extracted\nfrom the task description, the system automatically configures appropriate\nassistive parameters. In addition, a diffusion-based anomaly detector is used\nto continuously monitor the state of human-robot interaction and trigger\nreal-time replanning in response to detected anomalies. During task execution,\nonline trajectory refinement and impedance control are used to ensure safety\nand regulate human-robot interaction. Experimental results demonstrate that the\nproposed method effectively aligns with the wearer's cognition, adapts to\nsemantically varying tasks, and responds reliably to anomalies.", "AI": {"tldr": "本文提出一个语义感知框架，将大语言模型集成到上肢外骨骼的任务规划中，以实现基于任务语义理解和异常检测的安全、意图整合的辅助。", "motivation": "现有上肢外骨骼在家庭护理场景中缺乏对任务语义的理解和与用户的协作规划能力，限制了其泛化性，无法根据操作对象的性质自适应调整辅助配置。", "method": "该方法首先在外骨骼透明模式下捕捉穿戴者抓取物体时的意图。然后，从任务描述中提取语义信息，系统自动配置适当的辅助参数。此外，使用基于扩散的异常检测器持续监测人机交互状态并触发实时重新规划。在任务执行过程中，采用在线轨迹优化和阻抗控制以确保安全并调节人机交互。", "result": "实验结果表明，所提出的方法能够有效与穿戴者的认知对齐，适应语义变化的任务，并可靠地响应异常情况。", "conclusion": "该框架通过集成大语言模型实现语义感知的任务规划，结合异常检测和实时控制，为上肢外骨骼提供了安全、意图整合且高度适应性的辅助，解决了现有系统泛化性不足的问题。"}}
{"id": "2508.10001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10001", "abs": "https://arxiv.org/abs/2508.10001", "authors": ["Rakesh Thakur", "Sneha Sharma", "Gauri Chopra"], "title": "HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish", "comment": null, "summary": "Fact-checking in code-mixed, low-resource languages such as Hinglish remains\nan underexplored challenge in natural language processing. Existing\nfact-verification systems largely focus on high-resource, monolingual settings\nand fail to generalize to real-world political discourse in linguistically\ndiverse regions like India. Given the widespread use of Hinglish by public\nfigures, particularly political figures, and the growing influence of social\nmedia on public opinion, there's a critical need for robust, multilingual and\ncontext-aware fact-checking tools. To address this gap a novel benchmark HiFACT\ndataset is introduced with 1,500 realworld factual claims made by 28 Indian\nstate Chief Ministers in Hinglish, under a highly code-mixed low-resource\nsetting. Each claim is annotated with textual evidence and veracity labels. To\nevaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking\nmodel is proposed that combines multilingual contextual encoding,\nclaim-evidence semantic alignment, evidence graph construction, graph neural\nreasoning, and natural language explanation generation. Experimental results\nshow that HiFACTMix outperformed accuracy in comparison to state of art\nmultilingual baselines models and provides faithful justifications for its\nverdicts. This work opens a new direction for multilingual, code-mixed, and\npolitically grounded fact verification research.", "AI": {"tldr": "针对印地语-英语混合语（Hinglish）这一低资源语种，本文引入了首个政治领域事实核查基准数据集HiFACT，并提出了一个图感知、检索增强的事实核查模型HiFACTMix，该模型在准确性和判决解释方面优于现有基线模型。", "motivation": "现有事实核查系统主要针对高资源、单语环境，难以泛化到印度等语言多样化地区的真实政治语境，尤其是在公众人物（特别是政治人物）广泛使用Hinglish且社交媒体对公众舆论影响日益增长的背景下，迫切需要强大的多语言、语境感知的事实核查工具。", "method": "1. 构建了HiFACT数据集：包含1500个由28位印度邦首席部长用Hinglish发表的真实世界事实声明，并标注了文本证据和真实性标签。2. 提出了HiFACTMix模型：一个图感知、检索增强的事实核查模型，结合了多语言上下文编码、声明-证据语义对齐、证据图构建、图神经网络推理和自然语言解释生成。", "result": "实验结果表明，HiFACTMix模型在准确性方面优于最先进的多语言基线模型，并且为其判决提供了忠实的解释。", "conclusion": "这项工作为多语言、混合语（code-mixed）和政治领域的事实核查研究开辟了新的方向。"}}
{"id": "2508.10156", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10156", "abs": "https://arxiv.org/abs/2508.10156", "authors": ["Nitin Rai", "Nathan S. Boyd", "Gary E. Vallad", "Arnold W. Schumann"], "title": "Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model", "comment": null, "summary": "The current advancements in generative artificial intelligence (GenAI) models\nhave paved the way for new possibilities for generating high-resolution\nsynthetic images, thereby offering a promising alternative to traditional image\nacquisition for training computer vision models in agriculture. In the context\nof crop disease diagnosis, GenAI models are being used to create synthetic\nimages of various diseases, potentially facilitating model creation and\nreducing the dependency on resource-intensive in-field data collection.\nHowever, limited research has been conducted on evaluating the effectiveness of\nintegrating real with synthetic images to improve disease classification\nperformance. Therefore, this study aims to investigate whether combining a\nlimited number of real images with synthetic images can enhance the prediction\naccuracy of an EfficientNetV2-L model for classifying watermelon\n\\textit{(Citrullus lanatus)} diseases. The training dataset was divided into\nfive treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1\nreal-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to\nimprove variability and model generalization). All treatments were trained\nusing a custom EfficientNetV2-L architecture with enhanced fine-tuning and\ntransfer learning techniques. Models trained on H2, H3, and H4 treatments\ndemonstrated high precision, recall, and F1-score metrics. Additionally, the\nweighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying\nthat the addition of a small number of real images with a considerable volume\nof synthetic images improved model performance and generalizability. Overall,\nthis validates the findings that synthetic images alone cannot adequately\nsubstitute for real images; instead, both must be used in a hybrid manner to\nmaximize model performance for crop disease classification.", "AI": {"tldr": "本研究探讨了将少量真实图像与大量生成式AI合成图像结合，以提高西瓜病害分类模型的性能。结果表明，混合使用真实和合成图像的策略显著优于单独使用真实或合成图像，将加权F1分数从0.65提高到1.00，验证了混合方法在作物病害诊断中的有效性。", "motivation": "生成式AI在生成高分辨率合成图像方面取得了显著进展，为农业计算机视觉模型训练提供了替代方案，可减少对资源密集型田间数据采集的依赖。然而，关于如何有效整合真实和合成图像以提高疾病分类性能的研究有限，因此本研究旨在填补这一空白，提高作物病害诊断的准确性。", "method": "研究使用EfficientNetV2-L模型对西瓜病害进行分类。训练数据集被分为五种处理方式：H0（仅真实图像）、H1（仅合成图像）、H2（1:1真实与合成比例）、H3（1:10真实与合成比例）和H4（H3加上随机图像以增加变异性和泛化能力）。所有模型均采用定制的EfficientNetV2-L架构，并结合了增强的微调和迁移学习技术进行训练。性能通过精确度、召回率和F1分数进行评估。", "result": "在H2、H3和H4处理方式下训练的模型表现出高精确度、召回率和F1分数。加权F1分数从H0（仅真实图像）的0.65显著提升到H3-H4（混合图像）的1.00。这表明，在大量合成图像中添加少量真实图像能够显著提高模型性能和泛化能力。", "conclusion": "合成图像不能完全替代真实图像，为了最大限度地提高作物病害分类模型的性能，必须以混合方式同时使用真实图像和合成图像。这种混合方法是提高模型性能和泛化能力的关键。"}}
{"id": "2508.10177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10177", "abs": "https://arxiv.org/abs/2508.10177", "authors": ["Stepan Kulibaba", "Artem Dzhalilov", "Roman Pakhomov", "Oleg Svidchenko", "Alexander Gasnikov", "Aleksei Shpilman"], "title": "KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems", "comment": null, "summary": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive\ncapabilities but face significant limitations such as constrained exploration\nstrategies and a severe execution bottleneck. Exploration is hindered by\none-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)\napproaches that fail to recombine strong partial solutions. The execution\nbottleneck arises from lengthy code validation cycles that stifle iterative\nrefinement. To overcome these challenges, we introduce KompeteAI, a novel\nAutoML framework with dynamic solution space exploration. Unlike previous MCTS\nmethods that treat ideas in isolation, KompeteAI introduces a merging stage\nthat composes top candidates. We further expand the hypothesis space by\nintegrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle\nnotebooks and arXiv papers to incorporate real-world strategies. KompeteAI also\naddresses the execution bottleneck via a predictive scoring model and an\naccelerated debugging method, assessing solution potential using early stage\nmetrics to avoid costly full-code execution. This approach accelerates pipeline\nevaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,\nAIDE, and Ml-Master) by an average of 3\\% on the primary AutoML benchmark,\nMLE-Bench. Additionally, we propose Kompete-bench to address limitations in\nMLE-Bench, where KompeteAI also achieves state-of-the-art results", "AI": {"tldr": "KompeteAI是一种新型的基于LLM的AutoML框架，通过引入合并阶段的MCTS和RAG来增强探索，并通过预测性评分模型和加速调试方法来解决执行瓶颈，在现有和新提出的基准测试中均超越了现有领先方法。", "motivation": "当前的基于LLM的AutoML系统存在探索策略受限（如一次性方法缺乏多样性，MCTS未能有效重组）和严重的执行瓶颈（代码验证周期长，阻碍迭代优化）等局限性。", "method": "本文提出了KompeteAI框架，旨在解决现有问题：1. 动态解决方案空间探索：引入合并阶段，组合MCTS中的优秀候选方案；整合检索增强生成（RAG），从Kaggle和arXiv获取真实世界策略以扩展假设空间。2. 解决执行瓶颈：采用预测性评分模型和加速调试方法，通过早期阶段指标评估解决方案潜力，避免耗时的完整代码执行。3. 提出了Kompete-bench基准测试来补充MLE-Bench的不足。", "result": "KompeteAI将管道评估速度提高了6.9倍。在主要的AutoML基准MLE-Bench上，KompeteAI平均超越了领先方法（如RD-agent、AIDE和Ml-Master）3%。此外，在本文提出的Kompete-bench基准测试中，KompeteAI也取得了最先进的结果。", "conclusion": "KompeteAI通过改进探索策略和缓解执行瓶颈，有效克服了现有LLM-based AutoML系统的挑战，在多个基准测试中展现出卓越的性能，并提供了更高效的AutoML解决方案。"}}
{"id": "2508.10797", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10797", "abs": "https://arxiv.org/abs/2508.10797", "authors": ["M. Geshvadi", "G. So", "D. D. Chlorogiannis", "C. Galvin", "E. Torio", "A. Azimi", "Y. Tachie-Baffour", "N. Haouchine", "A. Golby", "M. Vangel", "W. M. Wells", "Y. Epelboym", "R. Du", "F. Durupinar", "S. Frisken"], "title": "When Experts Disagree: Characterizing Annotator Variability for Vessel Segmentation in DSA Images", "comment": null, "summary": "We analyze the variability among segmentations of cranial blood vessels in 2D\nDSA performed by multiple annotators in order to characterize and quantify\nsegmentation uncertainty. We use this analysis to quantify segmentation\nuncertainty and discuss ways it can be used to guide additional annotations and\nto develop uncertainty-aware automatic segmentation methods.", "AI": {"tldr": "该研究分析了多位标注者对颅内血管2D DSA图像分割的变异性，旨在量化分割不确定性，并探讨其在指导额外标注和开发不确定性感知自动分割方法中的应用。", "motivation": "为了表征和量化颅内血管2D DSA图像分割中的不确定性。", "method": "分析多位标注者进行的血管分割结果之间的变异性。", "result": "成功量化了分割不确定性。", "conclusion": "量化后的分割不确定性可用于指导额外的标注工作，并有助于开发具有不确定性感知能力的自动分割方法。"}}
{"id": "2508.10780", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10780", "abs": "https://arxiv.org/abs/2508.10780", "authors": ["Alessandro Adami", "Aris Synodinos", "Matteo Iovino", "Ruggero Carli", "Pietro Falco"], "title": "Learning Task Execution Hierarchies for Redundant Robots", "comment": null, "summary": "Modern robotic systems, such as mobile manipulators, humanoids, and aerial\nrobots with arms, often possess high redundancy, enabling them to perform\nmultiple tasks simultaneously. Managing this redundancy is key to achieving\nreliable and flexible behavior. A widely used approach is the Stack of Tasks\n(SoT), which organizes control objectives by priority within a unified\nframework. However, traditional SoTs are manually designed by experts, limiting\ntheir adaptability and accessibility. This paper introduces a novel framework\nthat automatically learns both the hierarchy and parameters of a SoT from\nuser-defined objectives. By combining Reinforcement Learning and Genetic\nProgramming, the system discovers task priorities and control strategies\nwithout manual intervention. A cost function based on intuitive metrics such as\nprecision, safety, and execution time guides the learning process. We validate\nour method through simulations and experiments on the mobile-YuMi platform, a\ndual-arm mobile manipulator with high redundancy. Results show that the learned\nSoTs enable the robot to dynamically adapt to changing environments and inputs,\nbalancing competing objectives while maintaining robust task execution. This\napproach provides a general and user-friendly solution for redundancy\nmanagement in complex robots, advancing human-centered robot programming and\nreducing the need for expert design.", "AI": {"tldr": "本文提出一种新颖框架，结合强化学习和遗传编程，自动学习高冗余机器人任务栈（SoT）的层级和参数，以实现多任务管理和动态适应性。", "motivation": "传统机器人任务栈（SoT）依赖专家手动设计，限制了其适应性和可访问性，难以有效管理复杂高冗余机器人的多任务和动态环境。", "method": "该研究结合强化学习（RL）和遗传编程（GP），通过用户定义的目标和基于精度、安全性、执行时间等直观指标的成本函数，自动学习任务栈（SoT）的优先级和控制策略，无需人工干预。", "result": "通过学习得到的任务栈（SoT）使机器人能够动态适应不断变化的环境和输入，有效平衡相互竞争的目标，同时保持鲁棒的任务执行。该方法在模拟和mobile-YuMi平台上的实验验证了其有效性。", "conclusion": "该方法为复杂机器人的冗余管理提供了一个通用且用户友好的解决方案，促进了以人为中心的机器人编程，并降低了对专家设计的需求。"}}
{"id": "2508.10398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10398", "abs": "https://arxiv.org/abs/2508.10398", "authors": ["Wei Gao", "Jie Zhang", "Mingle Zhao", "Zhiyuan Zhang", "Shu Kong", "Maani Ghaffari", "Dezhen Song", "Cheng-Zhong Xu", "Hui Kong"], "title": "Super LiDAR Reflectance for Robotic Perception", "comment": null, "summary": "Conventionally, human intuition often defines vision as a modality of passive\noptical sensing, while active optical sensing is typically regarded as\nmeasuring rather than the default modality of vision. However, the situation\nnow changes: sensor technologies and data-driven paradigms empower active\noptical sensing to redefine the boundaries of vision, ushering in a new era of\nactive vision. Light Detection and Ranging (LiDAR) sensors capture reflectance\nfrom object surfaces, which remains invariant under varying illumination\nconditions, showcasing significant potential in robotic perception tasks such\nas detection, recognition, segmentation, and Simultaneous Localization and\nMapping (SLAM). These applications often rely on dense sensing capabilities,\ntypically achieved by high-resolution, expensive LiDAR sensors. A key challenge\nwith low-cost LiDARs lies in the sparsity of scan data, which limits their\nbroader application. To address this limitation, this work introduces an\ninnovative framework for generating dense LiDAR reflectance images from sparse\ndata, leveraging the unique attributes of non-repeating scanning LiDAR\n(NRS-LiDAR). We tackle critical challenges, including reflectance calibration\nand the transition from static to dynamic scene domains, facilitating the\nreconstruction of dense reflectance images in real-world settings. The key\ncontributions of this work include a comprehensive dataset for LiDAR\nreflectance image densification, a densification network tailored for\nNRS-LiDAR, and diverse applications such as loop closure and traffic lane\ndetection using the generated dense reflectance images.", "AI": {"tldr": "该研究提出了一种创新框架，利用非重复扫描激光雷达（NRS-LiDAR）将稀疏的激光雷达反射数据转换为密集的反射图像，以克服低成本激光雷达数据稀疏性的限制，并扩展其在机器人感知中的应用。", "motivation": "传统视觉被动感知，而主动光学感知（如LiDAR）在机器人感知中潜力巨大。然而，低成本LiDAR传感器的数据稀疏性限制了其广泛应用，尤其是在需要密集感知能力的任务中。因此，需要解决稀疏LiDAR数据稠密化的问题。", "method": "本研究引入了一个创新框架，利用NRS-LiDAR的独特属性，从稀疏数据生成密集的LiDAR反射图像。该方法解决了反射率校准和从静态到动态场景域的转换等关键挑战，并通过一个专门为NRS-LiDAR设计的稠密化网络实现。", "result": "主要成果包括：构建了一个用于LiDAR反射图像稠密化的综合数据集；开发了一个针对NRS-LiDAR的稠密化网络；并展示了使用生成的密集反射图像在回环检测和交通车道检测等多样化应用中的有效性。", "conclusion": "该工作通过将稀疏的LiDAR数据稠密化，成功克服了低成本LiDAR传感器的局限性，从而扩展了主动视觉在真实世界机器人感知任务中的应用范围和潜力。"}}
{"id": "2508.10003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10003", "abs": "https://arxiv.org/abs/2508.10003", "authors": ["Austin C. Kozlowski", "Callin Dai", "Andrei Boutyline"], "title": "Semantic Structure in Large Language Model Embeddings", "comment": null, "summary": "Psychological research consistently finds that human ratings of words across\ndiverse semantic scales can be reduced to a low-dimensional form with\nrelatively little information loss. We find that the semantic associations\nencoded in the embedding matrices of large language models (LLMs) exhibit a\nsimilar structure. We show that the projections of words on semantic directions\ndefined by antonym pairs (e.g. kind - cruel) correlate highly with human\nratings, and further find that these projections effectively reduce to a\n3-dimensional subspace within LLM embeddings, closely resembling the patterns\nderived from human survey responses. Moreover, we find that shifting tokens\nalong one semantic direction causes off-target effects on geometrically aligned\nfeatures proportional to their cosine similarity. These findings suggest that\nsemantic features are entangled within LLMs similarly to how they are\ninterconnected in human language, and a great deal of semantic information,\ndespite its apparent complexity, is surprisingly low-dimensional. Furthermore,\naccounting for this semantic structure may prove essential for avoiding\nunintended consequences when steering features.", "AI": {"tldr": "大型语言模型（LLM）的词嵌入中编码的语义关联与人类对词语的评价一样，呈现出低维结构，且语义特征以类似人类语言的方式纠缠。", "motivation": "心理学研究发现人类对词语的语义评价可以降维到低维形式。本研究旨在探究大型语言模型的嵌入矩阵中编码的语义关联是否也表现出类似的低维结构，以及其与人类语义结构的关系。", "method": "研究方法包括：1) 将词语投射到由反义词对（如“善良-残忍”）定义的语义方向上，并与人类评价进行相关性分析。2) 分析这些投射在LLM嵌入中的维度，以确定其是否能有效降维。3) 观察沿一个语义方向移动词元时，对几何对齐特征产生的“脱靶效应”。", "result": "研究发现：1) LLM嵌入中的语义关联结构与人类评价高度相似。2) 词语在反义词定义的语义方向上的投射与人类评价高度相关。3) 这些投射在LLM嵌入中能有效降维到三维子空间，与人类调查反应模式高度吻合。4) 沿一个语义方向移动词元会导致与余弦相似度成比例的脱靶效应。", "conclusion": "研究结论是，LLM中的语义特征与人类语言中语义特征的互联方式相似，呈现出纠缠状态。尽管语义信息看似复杂，但实际上是惊人的低维。此外，理解和利用这种语义结构对于在操纵特征时避免意外后果至关重要。"}}
{"id": "2508.10171", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10171", "abs": "https://arxiv.org/abs/2508.10171", "authors": ["Aaditya Baranwal", "Abdul Mueez", "Jason Voelker", "Guneet Bhatia", "Shruti Vyas"], "title": "SynSpill: Improved Industrial Spill Detection With Synthetic Data", "comment": "Accepted at ICCV (VISION'25 Workshop) 2025", "summary": "Large-scale Vision-Language Models (VLMs) have transformed general-purpose\nvisual recognition through strong zero-shot capabilities. However, their\nperformance degrades significantly in niche, safety-critical domains such as\nindustrial spill detection, where hazardous events are rare, sensitive, and\ndifficult to annotate. This scarcity -- driven by privacy concerns, data\nsensitivity, and the infrequency of real incidents -- renders conventional\nfine-tuning of detectors infeasible for most industrial settings.\n  We address this challenge by introducing a scalable framework centered on a\nhigh-quality synthetic data generation pipeline. We demonstrate that this\nsynthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of\nVLMs and substantially boosts the performance of state-of-the-art object\ndetectors such as YOLO and DETR. Notably, in the absence of synthetic data\n(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than\nthese detectors. When SynSpill is used, both VLMs and detectors achieve marked\nimprovements, with their performance becoming comparable.\n  Our results underscore that high-fidelity synthetic data is a powerful means\nto bridge the domain gap in safety-critical applications. The combination of\nsynthetic generation and lightweight adaptation offers a cost-effective,\nscalable pathway for deploying vision systems in industrial environments where\nreal data is scarce/impractical to obtain.\n  Project Page: https://synspill.vercel.app", "AI": {"tldr": "针对工业安全领域数据稀缺问题，本文提出通过高质量合成数据生成和参数高效微调（PEFT）来提升视觉语言模型（VLMs）和传统目标检测器的性能，有效弥补了领域差距。", "motivation": "大规模视觉语言模型在通用视觉识别方面表现出色，但在工业泄漏检测等小众、安全关键领域性能显著下降，原因在于这些场景下事件罕见、敏感且难以标注，导致真实数据稀缺，传统微调方法不可行。", "method": "引入一个以高质量合成数据生成管道为核心的可扩展框架。利用合成数据集（SynSpill）对视觉语言模型进行参数高效微调（PEFT），并用于提升YOLO和DETR等现有目标检测器的性能。", "result": "在没有合成数据的情况下，视觉语言模型比传统检测器在未见过的泄漏场景中泛化能力更强。当使用SynSpill合成数据时，视觉语言模型和传统检测器都取得了显著性能提升，且两者性能变得相当。", "conclusion": "高保真合成数据是弥合安全关键应用领域差距的有效手段。合成数据生成与轻量级适应（PEFT）相结合，为在真实数据稀缺或难以获取的工业环境中部署视觉系统提供了一种成本效益高、可扩展的途径。"}}
{"id": "2508.10241", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10241", "abs": "https://arxiv.org/abs/2508.10241", "authors": ["Mark Zilberman"], "title": "Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence", "comment": "10 pages", "summary": "This work demonstrates how the concept of the entropic potential of events --\na parameter quantifying the influence of discrete events on the expected future\nentropy of a system -- can enhance uncertainty quantification, decision-making,\nand interpretability in artificial intelligence (AI). Building on its original\nformulation in physics, the framework is adapted for AI by introducing an\nevent-centric measure that captures how actions, observations, or other\ndiscrete occurrences impact uncertainty at future time horizons. Both the\noriginal and AI-adjusted definitions of entropic potential are formalized, with\nthe latter emphasizing conditional expectations to account for counterfactual\nscenarios. Applications are explored in policy evaluation, intrinsic reward\ndesign, explainable AI, and anomaly detection, highlighting the metric's\npotential to unify and strengthen uncertainty modeling in intelligent systems.\nConceptual examples illustrate its use in reinforcement learning, Bayesian\ninference, and anomaly detection, while practical considerations for\ncomputation in complex AI models are discussed. The entropic potential\nframework offers a theoretically grounded, interpretable, and versatile\napproach to managing uncertainty in AI, bridging principles from\nthermodynamics, information theory, and machine learning.", "AI": {"tldr": "本文提出将事件的熵势概念（量化离散事件对系统未来预期熵的影响）引入人工智能，以增强不确定性量化、决策制定和可解释性。", "motivation": "旨在提升人工智能系统中的不确定性量化、决策制定能力和可解释性，通过引入一个量化离散事件对未来不确定性影响的参数。", "method": "将物理学中的熵势概念调整用于AI，引入了一个以事件为中心的度量，捕捉动作、观测或其他离散事件如何影响未来时间范围的不确定性。形式化了原始和AI调整后的熵势定义，后者强调条件期望以考虑反事实情景。", "result": "熵势框架在策略评估、内在奖励设计、可解释AI和异常检测中进行了探索性应用，展示了其统一和强化智能系统中不确定性建模的潜力。通过概念性示例说明了其在强化学习、贝叶斯推断和异常检测中的应用。", "conclusion": "熵势框架为AI中的不确定性管理提供了一种理论基础、可解释且多功能的途径，它融合了热力学、信息论和机器学习的原理。"}}
{"id": "2508.10617", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10617", "abs": "https://arxiv.org/abs/2508.10617", "authors": ["Farid Tasharofi", "Fuxin Fan", "Melika Qahqaie", "Mareike Thies", "Andreas Maier"], "title": "FIND-Net -- Fourier-Integrated Network with Dictionary Kernels for Metal Artifact Reduction", "comment": "Accepted at MICCAI 2025. This is the submitted version prior to peer\n  review. The final Version of Record will appear in the MICCAI 2025\n  proceedings (Springer LNCS)", "summary": "Metal artifacts, caused by high-density metallic implants in computed\ntomography (CT) imaging, severely degrade image quality, complicating diagnosis\nand treatment planning. While existing deep learning algorithms have achieved\nnotable success in Metal Artifact Reduction (MAR), they often struggle to\nsuppress artifacts while preserving structural details. To address this\nchallenge, we propose FIND-Net (Fourier-Integrated Network with Dictionary\nKernels), a novel MAR framework that integrates frequency and spatial domain\nprocessing to achieve superior artifact suppression and structural\npreservation. FIND-Net incorporates Fast Fourier Convolution (FFC) layers and\ntrainable Gaussian filtering, treating MAR as a hybrid task operating in both\nspatial and frequency domains. This approach enhances global contextual\nunderstanding and frequency selectivity, effectively reducing artifacts while\nmaintaining anatomical structures. Experiments on synthetic datasets show that\nFIND-Net achieves statistically significant improvements over state-of-the-art\nMAR methods, with a 3.07% MAE reduction, 0.18% SSIM increase, and 0.90% PSNR\nimprovement, confirming robustness across varying artifact complexities.\nFurthermore, evaluations on real-world clinical CT scans confirm FIND-Net's\nability to minimize modifications to clean anatomical regions while effectively\nsuppressing metal-induced distortions. These findings highlight FIND-Net's\npotential for advancing MAR performance, offering superior structural\npreservation and improved clinical applicability. Code is available at\nhttps://github.com/Farid-Tasharofi/FIND-Net", "AI": {"tldr": "本文提出FIND-Net，一种结合频域和空域处理的深度学习金属伪影去除（MAR）框架，旨在有效抑制伪影并同时更好地保留图像结构细节。", "motivation": "CT图像中的高密度金属植入物会产生严重的金属伪影，降低图像质量，从而使诊断和治疗计划复杂化。尽管现有深度学习算法在MAR方面取得进展，但它们往往难以在抑制伪影和保留结构细节之间取得平衡。", "method": "FIND-Net框架整合了频域和空域处理。它引入了快速傅里叶卷积（FFC）层和可训练高斯滤波，将MAR视为一个在空域和频域均操作的混合任务。这种方法增强了全局上下文理解和频率选择性。", "result": "在合成数据集上，FIND-Net在MAE（降低3.07%）、SSIM（提高0.18%）和PSNR（提高0.90%）方面均优于现有最先进的MAR方法，并对不同伪影复杂性表现出鲁棒性。在真实临床CT扫描评估中，FIND-Net能有效抑制金属引起的失真，同时最大限度地减少对干净解剖区域的修改。", "conclusion": "FIND-Net显著提升了金属伪影去除性能，提供了卓越的结构保留能力和更高的临床适用性。"}}
{"id": "2508.10867", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10867", "abs": "https://arxiv.org/abs/2508.10867", "authors": ["Yizhi Zhou", "Ziwei Kang", "Jiawei Xia", "Xuan Wang"], "title": "CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups", "comment": null, "summary": "Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial\nodometry (VIO) systems. Consistency is crucial for ensuring the estimation\naccuracy of a UWBaided VIO system. An inconsistent estimator can degrade\nlocalization performance, where the inconsistency primarily arises from two\nmain factors: (1) the estimator fails to preserve the correct system\nobservability, and (2) UWB anchor positions are assumed to be known, leading to\nimproper neglect of calibration uncertainty. In this paper, we propose a\nconsistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system\nbased on the Lie group. Our method incorporates the UWB anchor state into the\nsystem state, explicitly accounting for UWB calibration uncertainty and\nenabling the joint and consistent estimation of both robot and anchor states.\nFurthermore, observability consistency is ensured by leveraging the invariant\nerror properties of the Lie group. We analytically prove that the CVIRO\nalgorithm naturally maintains the system's correct unobservable subspace,\nthereby preserving estimation consistency. Extensive simulations and\nexperiments demonstrate that CVIRO achieves superior localization accuracy and\nconsistency compared to existing methods.", "AI": {"tldr": "本文提出了一种基于李群的、紧耦合的视觉-惯性-测距里程计（CVIRO）系统，通过将UWB信标状态纳入系统状态并利用李群的不变误差特性，解决了UWB辅助VIO系统中因观测性不一致和UWB校准不确定性导致的定位性能下降问题，实现了卓越的定位精度和一致性。", "motivation": "UWB被广泛用于缓解视觉-惯性里程计（VIO）系统中的漂移。然而，现有UWB辅助VIO系统存在不一致性问题，主要源于两个方面：1) 估计器未能保持正确的系统可观测性；2) 假设UWB信标位置已知，从而不当地忽略了校准不确定性，导致定位性能下降。", "method": "本文提出了一种基于李群的、一致且紧耦合的视觉-惯性-测距里程计（CVIRO）系统。该方法将UWB信标状态纳入系统状态，显式考虑了UWB校准不确定性，实现了机器人和信标状态的联合一致估计。此外，通过利用李群的不变误差特性，确保了可观测性的一致性。理论证明CVIRO算法能自然地保持系统正确的不可观测子空间。", "result": "通过大量的仿真和实验，CVIRO系统与现有方法相比，在定位精度和一致性方面表现出卓越的性能。", "conclusion": "CVIRO算法通过同时解决UWB校准不确定性和保持正确的系统可观测性，提供了一个一致且高精度的UWB辅助视觉-惯性-测距里程计解决方案。"}}
{"id": "2508.10399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10399", "abs": "https://arxiv.org/abs/2508.10399", "authors": ["Wenlong Liang", "Rui Zhou", "Yang Ma", "Bing Zhang", "Songlin Li", "Yijia Liao", "Ping Kuang"], "title": "Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning", "comment": null, "summary": "Embodied AI aims to develop intelligent systems with physical forms capable\nof perceiving, decision-making, acting, and learning in real-world\nenvironments, providing a promising way to Artificial General Intelligence\n(AGI). Despite decades of explorations, it remains challenging for embodied\nagents to achieve human-level intelligence for general-purpose tasks in open\ndynamic environments. Recent breakthroughs in large models have revolutionized\nembodied AI by enhancing perception, interaction, planning and learning. In\nthis article, we provide a comprehensive survey on large model empowered\nembodied AI, focusing on autonomous decision-making and embodied learning. We\ninvestigate both hierarchical and end-to-end decision-making paradigms,\ndetailing how large models enhance high-level planning, low-level execution,\nand feedback for hierarchical decision-making, and how large models enhance\nVision-Language-Action (VLA) models for end-to-end decision making. For\nembodied learning, we introduce mainstream learning methodologies, elaborating\non how large models enhance imitation learning and reinforcement learning\nin-depth. For the first time, we integrate world models into the survey of\nembodied AI, presenting their design methods and critical roles in enhancing\ndecision-making and learning. Though solid advances have been achieved,\nchallenges still exist, which are discussed at the end of this survey,\npotentially as the further research directions.", "AI": {"tldr": "本文全面综述了大模型赋能具身智能的研究进展，重点关注自主决策和具身学习，并首次将世界模型整合到具身智能的综述中，同时讨论了现有挑战和未来方向。", "motivation": "具身智能旨在开发能感知、决策、行动和学习的物理智能系统，是实现通用人工智能（AGI）的有前景途径。然而，在开放动态环境中，具身智能体实现人类级别的通用智能仍面临巨大挑战。近期大模型的突破为具身智能带来了革命性进展，提升了其感知、交互、规划和学习能力，因此有必要进行系统性总结。", "method": "本研究通过提供一篇全面的综述文章来分析大模型赋能的具身智能。具体方法包括：1) 探讨分层和端到端决策范式，详细阐述大模型如何增强高层规划、低层执行、反馈以及视觉-语言-动作（VLA）模型。2) 介绍主流具身学习方法，深入阐述大模型如何增强模仿学习和强化学习。3) 首次将世界模型纳入具身智能综述，介绍其设计方法及其在增强决策和学习中的关键作用。", "result": "大模型在具身智能的自主决策和具身学习方面带来了显著提升。具体结果包括：大模型能有效增强分层决策中的高层规划、低层执行和反馈机制，并提升端到端决策中的VLA模型性能；大模型深入提升了模仿学习和强化学习的能力；世界模型在增强具身智能的决策和学习能力方面发挥着关键作用。", "conclusion": "尽管大模型赋能的具身智能已取得坚实进展，但在开放动态环境中实现人类级别通用智能仍存在挑战。这些挑战为未来的研究指明了方向，预示着该领域仍有广阔的探索空间。"}}
{"id": "2508.10004", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10004", "abs": "https://arxiv.org/abs/2508.10004", "authors": ["Andrés Carvallo", "Denis Parra", "Peter Brusilovsky", "Hernan Valdivieso", "Gabriel Rada", "Ivania Donoso", "Vladimir Araujo"], "title": "User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents", "comment": null, "summary": "The attention mechanism is a core component of the Transformer architecture.\nBeyond improving performance, attention has been proposed as a mechanism for\nexplainability via attention weights, which are associated with input features\n(e.g., tokens in a document). In this context, larger attention weights may\nimply more relevant features for the model's prediction. In evidence-based\nmedicine, such explanations could support physicians' understanding and\ninteraction with AI systems used to categorize biomedical literature. However,\nthere is still no consensus on whether attention weights provide helpful\nexplanations. Moreover, little research has explored how visualizing attention\naffects its usefulness as an explanation aid. To bridge this gap, we conducted\na user study to evaluate whether attention-based explanations support users in\nbiomedical document classification and whether there is a preferred way to\nvisualize them. The study involved medical experts from various disciplines who\nclassified articles based on study design (e.g., systematic reviews, broad\nsynthesis, randomized and non-randomized trials). Our findings show that the\nTransformer model (XLNet) classified documents accurately; however, the\nattention weights were not perceived as particularly helpful for explaining the\npredictions. However, this perception varied significantly depending on how\nattention was visualized. Contrary to Munzner's principle of visual\neffectiveness, which favors precise encodings like bar length, users preferred\nmore intuitive formats, such as text brightness or background color. While our\nresults do not confirm the overall utility of attention weights for\nexplanation, they suggest that their perceived helpfulness is influenced by how\nthey are visually presented.", "AI": {"tldr": "研究发现，尽管Transformer模型在生物医学文档分类中表现准确，但其注意力权重并未被医学专家认为对解释预测有显著帮助，且这种感知受可视化方式影响，用户偏好直观的视觉形式。", "motivation": "注意力机制被认为是Transformer模型解释性的来源，但关于注意力权重是否提供有用解释尚无共识。此外，关于注意力可视化如何影响其解释性作用的研究也很少。", "method": "通过一项用户研究进行评估，参与者是来自不同医学领域的专家。他们对生物医学文献进行研究设计分类，并评估注意力解释的帮助程度及偏好的可视化方式。", "result": "XLNet模型在文档分类中表现准确。然而，注意力权重并未被用户认为对解释预测特别有帮助。这种感知因注意力可视化方式而异，用户更偏好文本亮度或背景颜色等直观格式，而非条形图长度等精确编码。", "conclusion": "研究结果未能证实注意力权重作为解释的整体效用，但表明其感知到的有用性受到视觉呈现方式的显著影响。"}}
{"id": "2508.10227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10227", "abs": "https://arxiv.org/abs/2508.10227", "authors": ["Yuning Huang", "Jiahao Pang", "Fengqing Zhu", "Dong Tian"], "title": "EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting", "comment": null, "summary": "As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)\ndemonstrates fast training/rendering with superior visual quality. The two\ntasks of 3DGS, Gaussian creation and view rendering, are typically separated\nover time or devices, and thus storage/transmission and finally compression of\n3DGS Gaussians become necessary. We begin with a correlation and statistical\nanalysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals\nthat spherical harmonic AC attributes precisely follow Laplace distributions,\nwhile mixtures of Gaussian distributions can approximate rotation, scaling, and\nopacity. Additionally, harmonic AC attributes manifest weak correlations with\nother attributes except for inherited correlations from a color space. A\nfactorized and parameterized entropy coding method, EntropyGS, is hereinafter\nproposed. During encoding, distribution parameters of each Gaussian attribute\nare estimated to assist their entropy coding. The quantization for entropy\ncoding is adaptively performed according to Gaussian attribute types. EntropyGS\ndemonstrates about 30x rate reduction on benchmark datasets while maintaining\nsimilar rendering quality compared to input 3DGS data, with a fast encoding and\ndecoding time.", "AI": {"tldr": "本文提出了一种名为EntropyGS的3D高斯Splatting（3DGS）压缩方法，通过分析高斯属性的统计分布，实现了约30倍的数据率降低，同时保持了渲染质量。", "motivation": "3DGS中的高斯创建和视图渲染任务通常在时间或设备上分离，因此高斯数据的存储、传输和压缩变得必要。", "method": "研究了3DGS高斯属性的相关性和统计分析，发现球谐AC属性遵循拉普拉斯分布，而旋转、缩放和不透明度可用高斯混合分布近似。在此基础上，提出了一种因子化和参数化的熵编码方法EntropyGS，在编码过程中估计每个高斯属性的分布参数以辅助熵编码，并根据高斯属性类型自适应地进行量化。", "result": "EntropyGS在基准数据集上实现了约30倍的数据率降低，同时保持了与输入3DGS数据相似的渲染质量，并且编码和解码时间快速。", "conclusion": "EntropyGS通过深入分析3DGS高斯属性的统计特性并设计相应的熵编码方案，成功解决了3DGS数据压缩问题，实现了显著的数据率降低，同时维持了高质量的渲染效果和高效的编解码速度。"}}
{"id": "2508.10265", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.10265", "abs": "https://arxiv.org/abs/2508.10265", "authors": ["Jingde Cheng"], "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?", "comment": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2412.12408", "summary": "Recently, with the application progress of AIGC tools based on large language\nmodels (LLMs), led by ChatGPT, many AI experts and more non-professionals are\ntrumpeting the \"understanding ability\" and \"reasoning ability\" of the LLMs. The\npresent author considers that the so-called \"understanding ability\" and\n\"reasoning ability\" of LLMs are just illusions of those people who with vague\nconcepts. In fact, the LLMs can never have the true understanding ability and\ntrue reasoning ability. This paper intents to explain that, because the\nessential limitations of their working principle, the LLMs can never have the\nability of true correct reasoning.", "AI": {"tldr": "本文认为，大语言模型（LLMs）不具备真正的理解和推理能力，这仅仅是人们的概念模糊造成的错觉，其工作原理存在本质限制。", "motivation": "AIGC工具（如ChatGPT）的广泛应用使许多人（包括专家和非专业人士）宣扬大语言模型拥有“理解能力”和“推理能力”，作者对此持反对意见。", "method": "通过解释大语言模型工作原理的本质限制，论证其无法拥有真正正确的推理能力。", "result": "大语言模型（LLMs）永远无法拥有真正的理解能力和真正的推理能力。", "conclusion": "由于其工作原理的本质限制，大语言模型无法具备真正的正确推理能力。"}}
{"id": "2508.10416", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10416", "abs": "https://arxiv.org/abs/2508.10416", "authors": ["Zhuoyuan Yu", "Yuxing Long", "Zihan Yang", "Chengyan Zeng", "Hongwei Fan", "Jiyao Zhang", "Hao Dong"], "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model", "comment": null, "summary": "Existing vision-and-language navigation models often deviate from the correct\ntrajectory when executing instructions. However, these models lack effective\nerror correction capability, hindering their recovery from errors. To address\nthis challenge, we propose Self-correction Flywheel, a novel post-training\nparadigm. Instead of considering the model's error trajectories on the training\nset as a drawback, our paradigm emphasizes their significance as a valuable\ndata source. We have developed a method to identify deviations in these error\ntrajectories and devised innovative techniques to automatically generate\nself-correction data for perception and action. These self-correction data\nserve as fuel to power the model's continued training. The brilliance of our\nparadigm is revealed when we re-evaluate the model on the training set,\nuncovering new error trajectories. At this time, the self-correction flywheel\nbegins to spin. Through multiple flywheel iterations, we progressively enhance\nour monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE\nand RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success\nrates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%\nand 16.4%. Real robot tests in various indoor and outdoor environments\ndemonstrate \\method's superior capability of error correction, dynamic obstacle\navoidance, and long instruction following.", "AI": {"tldr": "本文提出了一种名为“自校正飞轮”的后训练范式，通过利用模型在训练集上的错误轨迹生成自校正数据，迭代提升视觉-语言导航（VLA）模型的错误纠正能力和导航性能。", "motivation": "现有视觉-语言导航模型在执行指令时常偏离正确轨迹，且缺乏有效的错误纠正能力，这阻碍了它们从错误中恢复。", "method": "该研究提出“自校正飞轮”后训练范式。它将模型在训练集上的错误轨迹视为有价值的数据源，开发方法识别偏差，并自动生成感知和动作的自校正数据。这些数据用于模型的持续训练，通过多次“飞轮”迭代，逐步增强模型（CorrectNav）的错误纠正能力。", "result": "在R2R-CE和RxR-CE基准测试中，CorrectNav取得了新的最先进成功率，分别为65.1%和69.3%，比现有最佳VLA导航模型分别高出8.2%和16.4%。真实机器人测试也证明了其在错误纠正、动态避障和长指令遵循方面的卓越能力。", "conclusion": "“自校正飞轮”范式能有效利用模型自身的错误轨迹进行迭代训练，显著提升视觉-语言导航模型的错误纠正能力和整体导航性能，达到新的最先进水平。"}}
{"id": "2508.10005", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10005", "abs": "https://arxiv.org/abs/2508.10005", "authors": ["Chengliang Zhou", "Mei Wang", "Ting Zhang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nmathematical problem-solving. However, the transition from providing answers to\ngenerating high-quality educational questions presents significant challenges\nthat remain underexplored. To advance Educational Question Generation (EQG) and\nfacilitate LLMs in generating pedagogically valuable and educationally\neffective questions, we introduce EQGBench, a comprehensive benchmark\nspecifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench\nestablishes a five-dimensional evaluation framework supported by a dataset of\n900 evaluation samples spanning three fundamental middle school disciplines:\nmathematics, physics, and chemistry. The dataset incorporates user queries with\nvarying knowledge points, difficulty gradients, and question type\nspecifications to simulate realistic educational scenarios. Through systematic\nevaluation of 46 mainstream large models, we reveal significant room for\ndevelopment in generating questions that reflect educational value and foster\nstudents' comprehensive abilities.", "AI": {"tldr": "该论文提出了EQGBench，一个针对中文教育问题生成（EQG）的综合基准，用于评估LLMs在该领域的表现，并发现当前LLMs在生成高质量教育问题方面仍有显著提升空间。", "motivation": "尽管大型语言模型（LLMs）在解决数学问题上表现出色，但从提供答案到生成高质量教育问题的转换仍面临巨大挑战且未被充分探索。为了推进教育问题生成（EQG）并帮助LLMs生成具有教学价值和教育效果的问题，需要一个专门的评估工具。", "method": "引入了EQGBench，一个用于评估LLMs中文EQG性能的综合基准。它建立了一个五维评估框架，并包含一个包含900个评估样本的数据集，涵盖初中数学、物理和化学三个学科。数据集模拟真实教育场景，包含不同知识点、难度梯度和问题类型规范的用户查询。", "result": "通过对46个主流大型模型进行系统评估，结果显示LLMs在生成体现教育价值和培养学生综合能力的问题方面仍有显著发展空间。", "conclusion": "当前的大型语言模型在生成具有教育价值和能培养学生综合能力的高质量教育问题方面仍需大幅改进。EQGBench为评估和促进这一领域的发展提供了重要工具。"}}
{"id": "2508.10232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10232", "abs": "https://arxiv.org/abs/2508.10232", "authors": ["Paul H. Acosta", "Pingjun Chen", "Simon P. Castillo", "Maria Esther Salvatierra", "Yinyin Yuan", "Xiaoxi Pan"], "title": "CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics", "comment": null, "summary": "Xenium, a new spatial transcriptomics platform, enables\nsubcellular-resolution profiling of complex tumor tissues. Despite the rich\nmorphological information in histology images, extracting robust cell-level\nfeatures and integrating them with spatial transcriptomics data remains a\ncritical challenge. We introduce CellSymphony, a flexible multimodal framework\nthat leverages foundation model-derived embeddings from both Xenium\ntranscriptomic profiles and histology images at true single-cell resolution. By\nlearning joint representations that fuse spatial gene expression with\nmorphological context, CellSymphony achieves accurate cell type annotation and\nuncovers distinct microenvironmental niches across three cancer types. This\nwork highlights the potential of foundation models and multimodal fusion for\ndeciphering the physiological and phenotypic orchestration of cells within\ncomplex tissue ecosystems.", "AI": {"tldr": "CellSymphony是一个多模态框架，它利用基础模型从Xenium空间转录组数据和组织学图像中提取单细胞级别的特征，以融合空间基因表达和形态学信息，从而实现准确的细胞类型注释和微环境识别。", "motivation": "尽管组织学图像包含丰富的形态学信息，但从其中提取鲁棒的细胞级别特征并与空间转录组数据整合仍然是一个关键挑战。", "method": "引入了CellSymphony框架，该框架利用基础模型从Xenium转录组图谱和组织学图像中提取单细胞分辨率的嵌入，并通过学习联合表示来融合空间基因表达和形态学背景。", "result": "CellSymphony在三种癌症类型中实现了准确的细胞类型注释，并揭示了独特的微环境生态位。", "conclusion": "这项工作突出了基础模型和多模态融合在解析复杂组织生态系统中细胞生理和表型协调方面的潜力。"}}
{"id": "2508.10293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10293", "abs": "https://arxiv.org/abs/2508.10293", "authors": ["Chuhuai Yue", "Chengqi Dong", "Yinan Gao", "Hang He", "Jiajun Chai", "Guojun Yin", "Wei Lin"], "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward", "comment": null, "summary": "Large reasoning models (LRMs) have recently achieved significant progress in\ncomplex reasoning tasks, aided by reinforcement learning with verifiable\nrewards. However, LRMs often suffer from overthinking, expending excessive\ncomputation on simple problems and reducing efficiency. Existing efficient\nreasoning methods typically require accurate task assessment to preset token\nbudgets or select reasoning modes, which limits their flexibility and\nreliability. In this work, we revisit the essence of overthinking and identify\nthat encouraging effective steps while penalizing ineffective ones is key to\nits solution. To this end, we propose a novel rule-based verifiable stepwise\nreward mechanism (VSRM), which assigns rewards based on the performance of\nintermediate states in the reasoning trajectory. This approach is intuitive and\nnaturally fits the step-by-step nature of reasoning tasks. We conduct extensive\nexperiments on standard mathematical reasoning benchmarks, including AIME24 and\nAIME25, by integrating VSRM with PPO and Reinforce++. Results show that our\nmethod achieves substantial output length reduction while maintaining original\nreasoning performance, striking an optimal balance between efficiency and\naccuracy. Further analysis of overthinking frequency and pass@k score before\nand after training demonstrates that our approach in deed effectively\nsuppresses ineffective steps and encourages effective reasoning, fundamentally\nalleviating the overthinking problem. All code will be released upon\nacceptance.", "AI": {"tldr": "本文提出了一种名为可验证分步奖励机制（VSRM）的新方法，通过在推理轨迹中奖励有效步骤和惩罚无效步骤，有效解决了大型推理模型（LRMs）的“过度思考”问题，从而在保持性能的同时显著提高了效率。", "motivation": "大型推理模型（LRMs）在复杂推理任务中表现出色，但常出现“过度思考”现象，即在简单问题上耗费过多计算资源，导致效率低下。现有提高效率的方法通常需要精确的任务评估来预设预算或选择模式，这限制了它们的灵活性和可靠性。", "method": "作者重新审视了过度思考的本质，认为鼓励有效步骤并惩罚无效步骤是关键。为此，提出了一种基于规则的可验证分步奖励机制（VSRM），该机制根据推理轨迹中中间状态的表现来分配奖励。此方法与分步推理任务的性质自然契合。实验中将VSRM与PPO和Reinforce++集成。", "result": "在AIME24和AIME25等标准数学推理基准上的实验表明，VSRM在保持原有推理性能的同时，显著减少了输出长度，实现了效率和准确性之间的最佳平衡。对过度思考频率和pass@k分数的进一步分析表明，该方法确实有效抑制了无效步骤，鼓励了有效推理，从根本上缓解了过度思考问题。", "conclusion": "VSRM通过鼓励有效推理步骤和抑制无效步骤，成功解决了LRMs的过度思考问题。这种方法在保持推理性能的同时显著提高了计算效率，为大型推理模型提供了一个有效的平衡效率与准确性的解决方案。"}}
{"id": "2508.10497", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10497", "abs": "https://arxiv.org/abs/2508.10497", "authors": ["Abdullah Farrukh", "Achim Wagner", "Martin Ruskowski"], "title": "Enabling Generic Robot Skill Implementation Using Object Oriented Programming", "comment": "34th International Conference on Robotics in Alpe-Adria-Danube Region\n  (RAAD 2025)", "summary": "Developing robotic algorithms and integrating a robotic subsystem into a\nlarger system can be a difficult task. Particularly in small and medium-sized\nenterprises (SMEs) where robotics expertise is lacking, implementing,\nmaintaining and developing robotic systems can be a challenge. As a result,\nmany companies rely on external expertise through system integrators, which, in\nsome cases, can lead to vendor lock-in and external dependency. In the academic\nresearch on intelligent manufacturing systems, robots play a critical role in\nthe design of robust autonomous systems. Similar challenges are faced by\nresearchers who want to use robotic systems as a component in a larger smart\nsystem, without having to deal with the complexity and vastness of the robot\ninterfaces in detail. In this paper, we propose a software framework that\nreduces the effort required to deploy a working robotic system. The focus is\nsolely on providing a concept for simplifying the different interfaces of a\nmodern robot system and using an abstraction layer for different manufacturers\nand models. The Python programming language is used to implement a prototype of\nthe concept. The target system is a bin-picking cell containing a Yaskawa\nMotoman GP4.", "AI": {"tldr": "本文提出一个软件框架，通过简化机器人接口和提供制造商无关的抽象层，旨在降低机器人系统部署的复杂性和所需工作量，解决中小型企业和研究人员在机器人集成方面的挑战。", "motivation": "开发和集成机器人算法，尤其是在缺乏机器人专业知识的中小型企业中，是一项困难的任务，常导致依赖外部专家和供应商锁定。研究人员在使用机器人作为大型智能系统组件时，也面临机器人接口复杂性的挑战。", "method": "提出一个软件框架概念，专注于简化现代机器人系统的各种接口，并为不同制造商和型号提供一个抽象层。使用Python编程语言实现了一个原型，目标系统是包含Yaskawa Motoman GP4的料箱拣选单元。", "result": "该框架旨在显著减少部署可工作机器人系统所需的工作量，使机器人系统更易于集成和使用，从而解决中小型企业和研究人员在机器人应用中的痛点。", "conclusion": "通过提供简化的接口和制造商无关的抽象层，所提出的软件框架能够有效降低机器人系统的部署和集成复杂性，为缺乏专业知识的企业和研究人员提供了更便捷的机器人应用解决方案。"}}
{"id": "2508.10007", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.10007", "abs": "https://arxiv.org/abs/2508.10007", "authors": ["Y. Lyu", "D. Combs", "D. Neumann", "Y. C. Leong"], "title": "Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models", "comment": "We have no known conflict of interest", "summary": "Hostile attribution bias is the tendency to interpret social interactions as\nintentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)\nis commonly used to measure hostile attribution bias, and includes open-ended\nquestions where participants describe the perceived intentions behind a\nnegative social situation and how they would respond. While these questions\nprovide insights into the contents of hostile attributions, they require\ntime-intensive scoring by human raters. In this study, we assessed whether\nlarge language models can automate the scoring of AIHQ open-ended responses. We\nused a previously collected dataset in which individuals with traumatic brain\ninjury (TBI) and healthy controls (HC) completed the AIHQ and had their\nopen-ended responses rated by trained human raters. We used half of these\nresponses to fine-tune the two models on human-generated ratings, and tested\nthe fine-tuned models on the remaining half of AIHQ responses. Results showed\nthat model-generated ratings aligned with human ratings for both attributions\nof hostility and aggression responses, with fine-tuned models showing higher\nalignment. This alignment was consistent across ambiguous, intentional, and\naccidental scenario types, and replicated previous findings on group\ndifferences in attributions of hostility and aggression responses between TBI\nand HC groups. The fine-tuned models also generalized well to an independent\nnonclinical dataset. To support broader adoption, we provide an accessible\nscoring interface that includes both local and cloud-based options. Together,\nour findings suggest that large language models can streamline AIHQ scoring in\nboth research and clinical contexts, revealing their potential to facilitate\npsychological assessments across different populations.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在自动化“模糊意图敌意问卷”（AIHQ）开放式回答评分方面的能力，结果显示LLMs的评分与人类评分高度一致，有望简化心理评估。", "motivation": "测量敌意归因偏差的AIHQ问卷包含开放式问题，其人工评分过程耗时且需要大量人力。", "method": "研究使用了一个包含脑外伤（TBI）患者和健康对照（HC）的AIHQ数据集，其中开放式回答已由人类评分员进行评分。研究将数据集分为两半：一半用于微调大型语言模型以匹配人类评分，另一半用于测试微调后的模型。此外，还在一个独立的非临床数据集上验证了模型的泛化能力，并提供了本地和云端的评分界面。", "result": "模型生成的评分，特别是经过微调的模型，在敌意归因和攻击性反应方面与人类评分高度一致。这种一致性在模糊、意图性和意外情境类型中均保持。研究还成功复现了TBI和HC组在敌意归因和攻击性反应上的已知群体差异。微调后的模型在独立的非临床数据集上表现出良好的泛化能力。", "conclusion": "大型语言模型能够有效自动化AIHQ的评分，在研究和临床环境中均能显著简化评分流程，从而促进对不同人群的心理评估。"}}
{"id": "2508.10256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10256", "abs": "https://arxiv.org/abs/2508.10256", "authors": ["Xinan Zhang", "Haolin Wang", "Yung-An Hsieh", "Zhongyu Yang", "Anthony Yezzi", "Yi-Chang Tsai"], "title": "Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets", "comment": null, "summary": "Crack detection plays a crucial role in civil infrastructures, including\ninspection of pavements, buildings, etc., and deep learning has significantly\nadvanced this field in recent years. While numerous technical and review papers\nexist in this domain, emerging trends are reshaping the landscape. These shifts\ninclude transitions in learning paradigms (from fully supervised learning to\nsemi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation\nand fine-tuning foundation models), improvements in generalizability (from\nsingle-dataset performance to cross-dataset evaluation), and diversification in\ndataset reacquisition (from RGB images to specialized sensor-based data). In\nthis review, we systematically analyze these trends and highlight\nrepresentative works. Additionally, we introduce a new dataset collected with\n3D laser scans, 3DCrack, to support future research and conduct extensive\nbenchmarking experiments to establish baselines for commonly used deep learning\nmethodologies, including recent foundation models. Our findings provide\ninsights into the evolving methodologies and future directions in deep\nlearning-based crack detection. Project page:\nhttps://github.com/nantonzhang/Awesome-Crack-Detection", "AI": {"tldr": "该论文综述了深度学习在裂缝检测领域的最新趋势，涵盖了学习范式、泛化能力和数据采集方式的变化，并引入了一个新的3D激光扫描数据集3DCrack，同时进行了广泛的基准测试。", "motivation": "裂缝检测在民用基础设施（如路面、建筑）检查中至关重要。尽管深度学习已显著推动该领域发展，但新的趋势（学习范式转变、泛化能力提升、数据采集多样化）正在重塑其发展格局，需要系统分析并提供新的研究资源。", "method": "系统分析了深度学习在裂缝检测中的新兴趋势，包括学习范式（从全监督到半监督、弱监督、无监督、少样本、域适应和基础模型微调）、泛化能力（从单数据集到跨数据集评估）和数据集获取多样化（从RGB图像到专用传感器数据）。此外，引入了一个通过3D激光扫描收集的新数据集3DCrack，并进行了广泛的基准测试，为常用深度学习方法（包括最新基础模型）建立了基线。", "result": "系统地分析了裂缝检测领域深度学习方法的演变趋势；推出了一个新的3D激光扫描数据集3DCrack；通过大量基准实验，为常用深度学习方法（包括基础模型）在3DCrack数据集上建立了性能基线。", "conclusion": "研究结果为深度学习裂缝检测的演进方法和未来方向提供了深刻见解。"}}
{"id": "2508.10337", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10337", "abs": "https://arxiv.org/abs/2508.10337", "authors": ["Chenliang Zhang", "Lin Wang", "Yuanyuan Lu", "Yusheng Qi", "Kexin Wang", "Peixu Hou", "Wenshi Chen"], "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering", "comment": null, "summary": "This paper describes the solutions of the Dianping-Trust-Safety team for the\nMETA CRAG-MM challenge. The challenge requires building a comprehensive\nretrieval-augmented generation system capable for multi-modal multi-turn\nquestion answering. The competition consists of three tasks: (1) answering\nquestions using structured data retrieved from an image-based mock knowledge\ngraph, (2) synthesizing information from both knowledge graphs and web search\nresults, and (3) handling multi-turn conversations that require context\nunderstanding and information aggregation from multiple sources. For Task 1,\nour solution is based on the vision large language model, enhanced by\nsupervised fine-tuning with knowledge distilled from GPT-4.1. We further\napplied curriculum learning strategies to guide reinforcement learning,\nresulting in improved answer accuracy and reduced hallucination. For Task 2 and\nTask 3, we additionally leveraged web search APIs to incorporate external\nknowledge, enabling the system to better handle complex queries and multi-turn\nconversations. Our approach achieved 1st place in Task 1 with a significant\nlead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of\nthe integration of curriculum learning with reinforcement learning in our\ntraining pipeline.", "AI": {"tldr": "本文描述了大众点评信任安全团队为META CRAG-MM多模态多轮问答挑战赛提供的解决方案，该方案整合了视觉大语言模型、知识蒸馏、课程学习与强化学习，并结合了网络搜索，在比赛中取得了优异成绩。", "motivation": "参与META CRAG-MM挑战赛，构建一个能够处理多模态、多轮问答的综合检索增强生成系统，该系统需能从图像知识图谱和网络搜索结果中提取并合成信息。", "method": "对于任务1，采用基于视觉大语言模型（VLLM）的方案，通过GPT-4.1蒸馏知识进行监督微调，并结合课程学习策略指导强化学习。对于任务2和任务3，在此基础上额外利用网络搜索API引入外部知识，以处理复杂查询和多轮对话。", "result": "在任务1中获得第一名，领先第二名52.38%；在任务3中获得第三名。", "conclusion": "该研究证明了将课程学习与强化学习集成到训练流程中的有效性，以及结合网络搜索等外部知识对于处理复杂多模态多轮问答的益处，显著提高了答案准确性并减少了幻觉。"}}
{"id": "2508.10511", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10511", "abs": "https://arxiv.org/abs/2508.10511", "authors": ["Andrea Rosasco", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection", "comment": "9th Conference on Robot Learning (CoRL 2025), Seoul, Korea", "summary": "Learning robot policies that capture multimodality in the training data has\nbeen a long-standing open challenge for behavior cloning. Recent approaches\ntackle the problem by modeling the conditional action distribution with\ngenerative models. One of these approaches is Diffusion Policy, which relies on\na diffusion model to denoise random points into robot action trajectories.\nWhile achieving state-of-the-art performance, it has two main drawbacks that\nmay lead the robot out of the data distribution during policy execution. First,\nthe stochasticity of the denoising process can highly impact on the quality of\ngenerated trajectory of actions. Second, being a supervised learning approach,\nit can learn data outliers from the dataset used for training. Recent work\nfocuses on mitigating these limitations by combining Diffusion Policy either\nwith large-scale training or with classical behavior cloning algorithms.\nInstead, we propose KDPE, a Kernel Density Estimation-based strategy that\nfilters out potentially harmful trajectories output of Diffusion Policy while\nkeeping a low test-time computational overhead. For Kernel Density Estimation,\nwe propose a manifold-aware kernel to model a probability density function for\nactions composed of end-effector Cartesian position, orientation, and gripper\nstate. KDPE overall achieves better performance than Diffusion Policy on\nsimulated single-arm tasks and real robot experiments.\n  Additional material and code are available on our project page\nhttps://hsp-iit.github.io/KDPE/.", "AI": {"tldr": "本文提出KDPE（基于核密度估计的策略），通过过滤Diffusion Policy可能产生有害的轨迹，同时保持较低的测试时间计算开销，从而提高机器人策略的性能。", "motivation": "行为克隆在处理训练数据中的多模态性方面是一个长期存在的挑战。Diffusion Policy虽然性能先进，但存在两个主要缺点：去噪过程的随机性会影响生成轨迹的质量，以及作为监督学习方法可能学习到数据离群值，导致机器人在策略执行时偏离数据分布。现有工作试图通过大规模训练或结合经典行为克隆算法来缓解这些问题，但本文提出了不同的解决方案。", "method": "本文提出KDPE，一种基于核密度估计（KDE）的策略，用于过滤Diffusion Policy的输出轨迹。KDPE使用了一种流形感知的核函数来建模动作的概率密度函数，这些动作由末端执行器笛卡尔位置、姿态和夹持器状态组成。", "result": "KDPE在模拟单臂任务和真实机器人实验中均取得了比Diffusion Policy更好的性能。", "conclusion": "KDPE通过过滤Diffusion Policy的输出轨迹，有效解决了其随机性和学习离群值的问题，提高了机器人策略的性能，同时保持了较低的计算开销。"}}
{"id": "2508.10008", "categories": ["cs.CL", "cs.LG", "68", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.10008", "abs": "https://arxiv.org/abs/2508.10008", "authors": ["Antonio Leandro Martins Candido", "Jose Everardo Bessa Maia"], "title": "Multidimensional classification of posts for online course discussion forum curation", "comment": "8 pages, 1 figure", "summary": "The automatic curation of discussion forums in online courses requires\nconstant updates, making frequent retraining of Large Language Models (LLMs) a\nresource-intensive process. To circumvent the need for costly fine-tuning, this\npaper proposes and evaluates the use of Bayesian fusion. The approach combines\nthe multidimensional classification scores of a pre-trained generic LLM with\nthose of a classifier trained on local data. The performance comparison\ndemonstrated that the proposed fusion improves the results compared to each\nclassifier individually, and is competitive with the LLM fine-tuning approach", "AI": {"tldr": "本文提出并评估了贝叶斯融合方法，将预训练LLM与本地数据分类器的分数结合，以避免在线课程论坛自动管理中LLM频繁微调的高成本，并证明其性能优于单一分类器且与LLM微调相当。", "motivation": "在线课程讨论区自动管理需要持续更新，导致LLM频繁重训练成为资源密集型过程，尤其微调成本高昂。", "method": "提出并评估了贝叶斯融合方法。该方法将预训练通用LLM的多维分类分数与一个在本地数据上训练的分类器的分数相结合。", "result": "性能比较表明，所提出的融合方法比单独使用任何一个分类器都能提高结果，并且与LLM微调方法具有竞争力。", "conclusion": "贝叶斯融合是一种有效且资源效率更高的方法，可以替代LLM的频繁微调，用于在线课程讨论区的自动管理。"}}
{"id": "2508.10264", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10264", "abs": "https://arxiv.org/abs/2508.10264", "authors": ["Haonan Ge", "Yiwei Wang", "Ming-Hsuan Yang", "Yujun Cai"], "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown strong performance across\nmultimodal tasks. However, they often produce hallucinations -- text that is\ninconsistent with visual input, due to the limited ability to verify\ninformation in different regions of the image. To address this, we propose\nMulti-Region Fusion Decoding (MRFD), a training-free decoding method that\nimproves factual grounding by modeling inter-region consistency. MRFD\nidentifies salient regions using cross-attention, generates initial responses\nfor each, and computes reliability weights based on Jensen-Shannon Divergence\n(JSD) among the responses. These weights guide a consistency-aware fusion of\nper-region predictions, using region-aware prompts inspired by Chain-of-Thought\nreasoning. Experiments across multiple LVLMs and benchmarks show that MRFD\nsignificantly reduces hallucinations and improves response factuality without\nrequiring model updates.", "AI": {"tldr": "提出了一种名为MRFD的无训练解码方法，通过建模区域间一致性来减少大型视觉语言模型（LVLMs）的幻觉，显著提高事实准确性。", "motivation": "LVLMs在多模态任务中表现出色，但常产生与视觉输入不一致的“幻觉”文本，原因是它们验证图像不同区域信息的能力有限。", "method": "MRFD是一种无需训练的解码方法。它通过交叉注意力识别显著区域，为每个区域生成初始响应，然后基于响应间的Jensen-Shannon散度（JSD）计算可靠性权重。这些权重指导基于区域感知提示（受思维链推理启发）的、一致性感知的区域预测融合。", "result": "在多个LVLMs和基准测试中，MRFD显著减少了幻觉并提高了响应的事实性，且无需更新模型。", "conclusion": "MRFD通过建模区域间一致性，有效提升了LVLMs的事实基础能力并减少了幻觉，是一种无需训练的有效解码方法。"}}
{"id": "2508.10340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10340", "abs": "https://arxiv.org/abs/2508.10340", "authors": ["Chak Lam Shek", "Guangyao Shi", "Pratap Tokekar"], "title": "Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) requires coordinated and stable\npolicy updates among interacting agents. Heterogeneous-Agent Trust Region\nPolicy Optimization (HATRPO) enforces per-agent trust region constraints using\nKullback-Leibler (KL) divergence to stabilize training. However, assigning each\nagent the same KL threshold can lead to slow and locally optimal updates,\nespecially in heterogeneous settings. To address this limitation, we propose\ntwo approaches for allocating the KL divergence threshold across agents:\nHATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes\nthreshold assignment under global KL constraints, and HATRPO-G, a greedy\nalgorithm that prioritizes agents based on improvement-to-divergence ratio. By\nconnecting sequential policy optimization with constrained threshold\nscheduling, our approach enables more flexible and effective learning in\nheterogeneous-agent settings. Experimental results demonstrate that our methods\nsignificantly boost the performance of HATRPO, achieving faster convergence and\nhigher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and\nHATRPO-G achieve comparable improvements in final performance, each exceeding\n22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as\nreflected by its lower variance.", "AI": {"tldr": "针对异构多智能体信赖域策略优化（HATRPO）中固定KL散度阈值导致训练缓慢和局部最优的问题，本文提出了两种自适应分配KL阈值的方法：HATRPO-W（基于KKT优化）和HATRPO-G（基于贪婪算法），显著提升了HATRPO的性能和收敛速度。", "motivation": "多智能体强化学习（MARL）需要智能体间协调稳定的策略更新。HATRPO通过为每个智能体设定KL散度信赖域来稳定训练，但为所有智能体分配相同的KL阈值，在异构环境中会导致训练缓慢并陷入局部最优，限制了其学习效率和效果。", "method": "本文提出了两种动态分配KL散度阈值的方法：1) HATRPO-W：一种基于KKT条件的方法，在全球KL约束下优化阈值分配。2) HATRPO-G：一种贪婪算法，根据智能体的改进与散度比率来优先分配阈值。这两种方法将序贯策略优化与约束阈值调度相结合。", "result": "实验结果表明，HATRPO-W和HATRPO-G显著提升了HATRPO的性能，在多个MARL基准测试中实现了更快的收敛和更高的最终奖励。具体而言，两种方法最终性能提升均超过22.5%。值得注意的是，HATRPO-W还表现出更稳定的学习动态，方差更低。", "conclusion": "通过动态调整KL散度阈值，本文提出的HATRPO-W和HATRPO-G方法有效解决了HATRPO在异构环境中的局限性，显著提升了多智能体强化学习的性能、收敛速度和学习稳定性，为异构多智能体系统提供了更灵活有效的学习范式。"}}
{"id": "2508.10538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10538", "abs": "https://arxiv.org/abs/2508.10538", "authors": ["Xin Liu", "Bida Ma", "Chenkun Qi", "Yan Ding", "Zhaxizhuoma", "Guorong Zhang", "Pengan Chen", "Kehui Liu", "Zhongjie Jia", "Chuyue Guan", "Yule Mo", "Jiaqi Liu", "Feng Gao", "Jiangwei Zhong", "Bin Zhao", "Xuelong Li"], "title": "MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm", "comment": null, "summary": "Whole-body loco-manipulation for quadruped robots with arm remains a\nchallenging problem, particularly in achieving multi-task control. To address\nthis, we propose MLM, a reinforcement learning framework driven by both\nreal-world and simulation data. It enables a six-DoF robotic arm--equipped\nquadruped robot to perform whole-body loco-manipulation for multiple tasks\nautonomously or under human teleoperation. To address the problem of balancing\nmultiple tasks during the learning of loco-manipulation, we introduce a\ntrajectory library with an adaptive, curriculum-based sampling mechanism. This\napproach allows the policy to efficiently leverage real-world collected\ntrajectories for learning multi-task loco-manipulation. To address deployment\nscenarios with only historical observations and to enhance the performance of\npolicy execution across tasks with different spatial ranges, we propose a\nTrajectory-Velocity Prediction policy network. It predicts unobservable future\ntrajectories and velocities. By leveraging extensive simulation data and\ncurriculum-based rewards, our controller achieves whole-body behaviors in\nsimulation and zero-shot transfer to real-world deployment. Ablation studies in\nsimulation verify the necessity and effectiveness of our approach, while\nreal-world experiments on the Go2 robot with an Airbot robotic arm demonstrate\nthe policy's good performance in multi-task execution.", "AI": {"tldr": "该研究提出了一种名为MLM的强化学习框架，结合真实世界和模拟数据，使带机械臂的四足机器人能够进行全身运动操作，实现多任务自主控制或远程操作，并能零样本迁移到现实世界。", "motivation": "带机械臂的四足机器人进行全身运动操作是一个具有挑战性的问题，尤其是在实现多任务控制方面。", "method": "1. 提出MLM强化学习框架，结合真实世界和模拟数据驱动。2. 引入带有自适应、基于课程的采样机制的轨迹库，以平衡多任务学习。3. 提出轨迹-速度预测策略网络，预测不可观测的未来轨迹和速度，以应对仅有历史观测的部署场景和不同空间范围的任务。4. 利用大量仿真数据和基于课程的奖励。", "result": "1. 控制器在仿真中实现了全身行为。2. 实现了零样本迁移到真实世界部署。3. 仿真中的消融研究验证了方法的必要性和有效性。4. 在Go2机器人与Airbot机械臂上的真实世界实验证明了策略在多任务执行中的良好性能。", "conclusion": "所提出的MLM框架，通过其轨迹库和轨迹-速度预测网络，有效解决了带机械臂四足机器人的多任务全身运动操作问题，并实现了零样本真实世界迁移。"}}
{"id": "2508.10009", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10009", "abs": "https://arxiv.org/abs/2508.10009", "authors": ["Hojun Jin", "Eunsoo Hong", "Ziwon Hyung", "Sungjun Lim", "Seungjin Lee", "Keunseok Cho"], "title": "Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts", "comment": "Accepted to Interspeech 2025", "summary": "Hard-parameter sharing is a common strategy to train a single model jointly\nacross diverse tasks. However, this often leads to task interference, impeding\noverall model performance. To address the issue, we propose a simple yet\neffective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of\nExperts models, S-MoE eliminates the need for training gating functions by\nutilizing special guiding tokens to route each task to its designated expert.\nBy assigning each task to a separate feedforward network, S-MoE overcomes the\nlimitations of hard-parameter sharing. We further apply S-MoE to a\nspeech-to-text model, enabling the model to process mixed-bandwidth input while\njointly performing automatic speech recognition (ASR) and speech translation\n(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,\nachieving a 6.35% relative improvement in Word Error Rate (WER) when applied to\nboth the encoder and decoder.", "AI": {"tldr": "本文提出了一种名为S-MoE的监督专家混合模型，通过使用引导令牌将不同任务路由到指定专家，有效解决了硬参数共享在多任务学习中导致的任务干扰问题，并显著提升了模型性能。", "motivation": "多任务学习中常见的硬参数共享策略会导致任务间干扰，从而限制模型的整体性能。", "method": "提出S-MoE（Supervised Mixture of Experts）模型，它通过利用特殊的引导令牌将每个任务路由到其指定的专家（独立的神经网络），从而避免了训练门控函数的需要。该方法将每个任务分配给一个独立的FFN（前馈网络），以克服硬参数共享的局限性。文中将S-MoE应用于语音到文本模型，使其能处理混合带宽输入并同时执行ASR和ST任务。", "result": "实验结果表明，S-MoE模型是有效的。当应用于编码器和解码器时，该模型在词错误率（WER）上实现了6.35%的相对改进。", "conclusion": "S-MoE模型能够有效解决硬参数共享带来的任务干扰问题，显著提升了多任务学习模型的性能。"}}
{"id": "2508.10268", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10268", "abs": "https://arxiv.org/abs/2508.10268", "authors": ["Yujie Zhao", "Jiabei Zeng", "Shiguang Shan"], "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones", "comment": "Accepted for British Machine Vision Conference (BMVC) 2025", "summary": "Although appearance-based point-of-gaze (PoG) estimation has improved, the\nestimators still struggle to generalize across individuals due to personal\ndifferences. Therefore, person-specific calibration is required for accurate\nPoG estimation. However, calibrated PoG estimators are often sensitive to head\npose variations. To address this, we investigate the key factors influencing\ncalibrated estimators and explore pose-robust calibration strategies.\nSpecifically, we first construct a benchmark, MobilePoG, which includes facial\nimages from 32 individuals focusing on designated points under either fixed or\ncontinuously changing head poses. Using this benchmark, we systematically\nanalyze how the diversity of calibration points and head poses influences\nestimation accuracy. Our experiments show that introducing a wider range of\nhead poses during calibration improves the estimator's ability to handle pose\nvariation. Building on this insight, we propose a dynamic calibration strategy\nin which users fixate on calibration points while moving their phones. This\nstrategy naturally introduces head pose variation during a user-friendly and\nefficient calibration process, ultimately producing a better calibrated PoG\nestimator that is less sensitive to head pose variations than those using\nconventional calibration strategies. Codes and datasets are available at our\nproject page.", "AI": {"tldr": "该研究针对注视点估计器校准后对头部姿态变化敏感的问题，提出了一种动态校准策略，通过在校准过程中引入头部姿态变化，显著提高了估计器的姿态鲁棒性。", "motivation": "尽管基于外观的注视点（PoG）估计已有所改进，但由于个体差异，估计器难以泛化，需要进行个性化校准。然而，已校准的PoG估计器通常对头部姿态变化敏感，这限制了其实用性。", "method": "1. 构建了基准数据集MobilePoG，包含32个个体在固定或连续变化的头部姿态下注视指定点的面部图像。2. 利用该基准系统分析了校准点和头部姿态的多样性如何影响估计精度。3. 基于分析结果，提出了一种动态校准策略：用户在注视校准点的同时移动手机，从而自然引入头部姿态变化。", "result": "实验表明，在校准过程中引入更广泛的头部姿态范围，能显著提高估计器处理姿态变化的能力。所提出的动态校准策略，比传统校准方法能产生对头部姿态变化更不敏感、校准效果更好的PoG估计器。", "conclusion": "通过在校准过程中整合头部姿态变化，可以有效提升注视点估计器的姿态鲁棒性。提出的动态校准策略提供了一种用户友好且高效的方法，能够生成更精确且对头部姿态变化不敏感的注视点估计器。"}}
{"id": "2508.10358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10358", "abs": "https://arxiv.org/abs/2508.10358", "authors": ["Mengtao Zhou", "Sifan Wu", "Huan Zhang", "Qi Sima", "Bang Liu"], "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles", "comment": null, "summary": "We investigate the capacity of Large Language Models (LLMs) for imaginative\nreasoning--the proactive construction, testing, and revision of hypotheses in\ninformation-sparse environments. Existing benchmarks, often static or focused\non social deduction, fail to capture the dynamic, exploratory nature of this\nreasoning process. To address this gap, we introduce a comprehensive research\nframework based on the classic \"Turtle Soup\" game, integrating a benchmark, an\nagent, and an evaluation protocol. We present TurtleSoup-Bench, the first\nlarge-scale, bilingual, interactive benchmark for imaginative reasoning,\ncomprising 800 turtle soup puzzles sourced from both the Internet and expert\nauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'\nperformance in this setting. To evaluate reasoning quality, we develop a\nmulti-dimensional protocol measuring logical consistency, detail completion,\nand conclusion alignment. Experiments with leading LLMs reveal clear capability\nlimits, common failure patterns, and a significant performance gap compared to\nhumans. Our work offers new insights into LLMs' imaginative reasoning and\nestablishes a foundation for future research on exploratory agent behavior.", "AI": {"tldr": "本研究通过引入新的“海龟汤”游戏基准、智能体和评估协议，调查了大型语言模型（LLMs）在信息稀疏环境中的想象推理能力，揭示了LLMs在此方面的局限性。", "motivation": "现有基准通常是静态的或侧重于社交推理，未能捕捉到想象推理的动态、探索性本质，因此需要一个能评估LLMs在信息稀疏环境中主动构建、测试和修正假设能力的新框架。", "method": "引入了一个基于经典“海龟汤”游戏的综合研究框架，包括基准、智能体和评估协议。具体方法包括：1) 开发了TurtleSoup-Bench，一个包含800个谜题的大规模、双语、交互式想象推理基准；2) 提出了Mosaic-Agent，一个专门用于评估LLMs在此设置下表现的新型智能体；3) 设计了多维度评估协议，衡量逻辑一致性、细节完整性和结论一致性。", "result": "对主流LLMs的实验表明，它们在想象推理方面存在明显的上限和常见的失败模式，与人类相比存在显著的性能差距。", "conclusion": "本研究为LLMs的想象推理能力提供了新见解，并为未来探索性智能体行为的研究奠定了基础。"}}
{"id": "2508.10603", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10603", "abs": "https://arxiv.org/abs/2508.10603", "authors": ["Agnes Axelsson", "Merle Reimann", "Ronald Cumbal", "Hannah Pelikan", "Divesh Lala"], "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality", "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages", "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.", "AI": {"tldr": "尽管LLM提升了人机交互质量，但仍存在次优的、情境依赖的失败。本文提出使用民族志短篇故事（ethnographic vignettes）来清晰记录和突出这些通常被忽视的失败，以促进透明度并增强评估方法。", "motivation": "LLM虽然改善了人机交互（HRI）质量，但与人际交互相比，系统仍存在次优表现。HRI失败的性质和严重性常依赖于具体情境，难以泛化。许多失败，特别是罕见的失败，缺乏清晰记录。", "method": "提出在HRI领域应用一种被忽视的技术：民族志短篇故事（ethnographic vignettes）。描述了撰写短篇故事的方法论，并基于个人在HRI系统失败方面的经验创作了范例。", "result": "短篇故事能清晰地突出HRI中的失败，特别是那些鲜少被记录的失败。它们能够从多学科视角传达失败，提升机器人能力的透明度，并记录研究报告中可能被省略的意外行为。", "conclusion": "鼓励将短篇故事作为现有交互评估方法的补充，因其在沟通多学科失败、促进透明度和记录意外行为方面的优势。"}}
{"id": "2508.10010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10010", "abs": "https://arxiv.org/abs/2508.10010", "authors": ["Ayana Hussain", "Patrick Zhao", "Nicholas Vincent"], "title": "An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs", "comment": null, "summary": "Large Language Models (LLMs) are a double-edged sword capable of generating\nharmful misinformation -- inadvertently, or when prompted by \"jailbreak\"\nattacks that attempt to produce malicious outputs. LLMs could, with additional\nresearch, be used to detect and prevent the spread of misinformation. In this\npaper, we investigate the efficacy and characteristics of LLM-produced\njailbreak attacks that cause other models to produce harmful medical\nmisinformation. We also study how misinformation generated by jailbroken LLMs\ncompares to typical misinformation found on social media, and how effectively\nit can be detected using standard machine learning approaches. Specifically, we\nclosely examine 109 distinct attacks against three target LLMs and compare the\nattack prompts to in-the-wild health-related LLM queries. We also examine the\nresulting jailbreak responses, comparing the generated misinformation to\nhealth-related misinformation on Reddit. Our findings add more evidence that\nLLMs can be effectively used to detect misinformation from both other LLMs and\nfrom people, and support a body of work suggesting that with careful design,\nLLMs can contribute to a healthier overall information ecosystem.", "AI": {"tldr": "本文研究了大型语言模型（LLMs）越狱攻击生成有害医疗错误信息的有效性和特征，并探讨了如何使用标准机器学习方法检测这些错误信息。", "motivation": "LLMs是一把双刃剑，可能无意中或通过“越狱”攻击生成有害的错误信息。本研究旨在深入了解越狱攻击产生的错误信息，并探索LLMs在检测和预防错误信息传播方面的潜力。", "method": "研究者检查了针对三个目标LLMs的109种不同攻击，并将攻击提示与真实的健康相关LLM查询进行比较。同时，他们将越狱响应生成的错误信息与Reddit上与健康相关的错误信息进行比较，并评估了使用标准机器学习方法检测这些错误信息的有效性。", "result": "研究结果进一步证明，LLMs可以有效地用于检测来自其他LLMs和人类的错误信息。", "conclusion": "通过精心设计，LLMs能够为构建更健康的整体信息生态系统做出贡献。"}}
{"id": "2508.10280", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10280", "abs": "https://arxiv.org/abs/2508.10280", "authors": ["Danyi Gao"], "title": "High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance", "comment": null, "summary": "This paper addresses the performance bottlenecks of existing text-driven\nimage generation methods in terms of semantic alignment accuracy and structural\nconsistency. A high-fidelity image generation method is proposed by integrating\ntext-image contrastive constraints with structural guidance mechanisms. The\napproach introduces a contrastive learning module that builds strong\ncross-modal alignment constraints to improve semantic matching between text and\nimage. At the same time, structural priors such as semantic layout maps or edge\nsketches are used to guide the generator in spatial-level structural modeling.\nThis enhances the layout completeness and detail fidelity of the generated\nimages. Within the overall framework, the model jointly optimizes contrastive\nloss, structural consistency loss, and semantic preservation loss. A\nmulti-objective supervision mechanism is adopted to improve the semantic\nconsistency and controllability of the generated content. Systematic\nexperiments are conducted on the COCO-2014 dataset. Sensitivity analyses are\nperformed on embedding dimensions, text length, and structural guidance\nstrength. Quantitative metrics confirm the superior performance of the proposed\nmethod in terms of CLIP Score, FID, and SSIM. The results show that the method\neffectively bridges the gap between semantic alignment and structural fidelity\nwithout increasing computational complexity. It demonstrates a strong ability\nto generate semantically clear and structurally complete images, offering a\nviable technical path for joint text-image modeling and image generation.", "AI": {"tldr": "本文提出了一种结合文本-图像对比约束和结构引导机制的高保真图像生成方法，有效提升了文本驱动图像生成的语义对齐精度和结构一致性。", "motivation": "现有文本驱动图像生成方法在语义对齐准确性和结构一致性方面存在性能瓶颈。", "method": "该方法引入对比学习模块以建立强跨模态对齐约束；利用语义布局图或边缘草图等结构先验引导生成器进行空间级结构建模；模型联合优化对比损失、结构一致性损失和语义保留损失，采用多目标监督机制。", "result": "在COCO-2014数据集上的系统实验表明，所提方法在CLIP Score、FID和SSIM等定量指标上表现优越，有效弥补了语义对齐和结构保真度之间的差距，且未增加计算复杂度。", "conclusion": "该方法展示了生成语义清晰、结构完整图像的强大能力，为联合文本-图像建模和图像生成提供了一条可行的技术路径。"}}
{"id": "2508.10391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10391", "abs": "https://arxiv.org/abs/2508.10391", "authors": ["Yaoze Zhang", "Rong Wu", "Pinlong Cai", "Xiaoman Wang", "Guohang Yan", "Song Mao", "Ding Wang", "Botian Shi"], "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large\nLanguage Models by leveraging external knowledge, whereas the effectiveness is\noften compromised by the retrieval of contextually flawed or incomplete\ninformation. To address this, knowledge graph-based RAG methods have evolved\ntowards hierarchical structures, organizing knowledge into multi-level\nsummaries. However, these approaches still suffer from two critical,\nunaddressed challenges: high-level conceptual summaries exist as disconnected\n``semantic islands'', lacking the explicit relations needed for cross-community\nreasoning; and the retrieval process itself remains structurally unaware, often\ndegenerating into an inefficient flat search that fails to exploit the graph's\nrich topology. To overcome these limitations, we introduce LeanRAG, a framework\nthat features a deeply collaborative design combining knowledge aggregation and\nretrieval strategies. LeanRAG first employs a novel semantic aggregation\nalgorithm that forms entity clusters and constructs new explicit relations\namong aggregation-level summaries, creating a fully navigable semantic network.\nThen, a bottom-up, structure-guided retrieval strategy anchors queries to the\nmost relevant fine-grained entities and then systematically traverses the\ngraph's semantic pathways to gather concise yet contextually comprehensive\nevidence sets. The LeanRAG can mitigate the substantial overhead associated\nwith path retrieval on graphs and minimizes redundant information retrieval.\nExtensive experiments on four challenging QA benchmarks with different domains\ndemonstrate that LeanRAG significantly outperforming existing methods in\nresponse quality while reducing 46\\% retrieval redundancy. Code is available\nat: https://github.com/RaZzzyz/LeanRAG", "AI": {"tldr": "LeanRAG是一种新型的检索增强生成（RAG）框架，通过创新的语义聚合算法构建可导航的知识网络，并采用结构引导的检索策略，有效解决了现有知识图谱RAG中高层摘要缺乏关联和检索效率低下的问题，显著提升了响应质量并减少了检索冗余。", "motivation": "现有RAG方法在检索到有缺陷或不完整信息时效果受损。尽管基于知识图谱的RAG已发展出分层结构，但仍存在两个关键挑战：高层概念摘要是“语义孤岛”，缺乏跨社区推理所需的显式关系；以及检索过程缺乏结构感知，常退化为低效的扁平搜索。", "method": "LeanRAG采用深度协作的设计，结合了知识聚合和检索策略。首先，它使用一种新颖的语义聚合算法，形成实体簇并在聚合层摘要之间构建新的显式关系，从而创建一个完全可导航的语义网络。其次，它采用一种自底向上的、结构引导的检索策略，将查询锚定到最相关的细粒度实体，然后系统地遍历图的语义路径，以收集简洁而上下文全面的证据集。", "result": "在四个具有不同领域的挑战性问答基准测试中，LeanRAG的表现显著优于现有方法，在提高响应质量的同时，将检索冗余减少了46%。", "conclusion": "LeanRAG通过其创新的语义聚合和结构引导检索机制，成功克服了现有知识图谱RAG的局限性，为大语言模型提供了更准确、高效的外部知识，显著提升了RAG的整体性能并降低了信息冗余。"}}
{"id": "2508.10686", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.10686", "abs": "https://arxiv.org/abs/2508.10686", "authors": ["Carla Wehner", "Finn Schubert", "Heiko Hellkamp", "Julius Hahnewald", "Kilian Scheafer", "Muhammad Bilal Khan", "Oliver Gutfleisch"], "title": "An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)", "comment": null, "summary": "Soft robots, particularly magnetic soft robots, require specialized\nsimulation tools to accurately model their deformation under external magnetic\nfields. However, existing platforms often lack dedicated support for magnetic\nmaterials, making them difficult to use for researchers at different expertise\nlevels. This work introduces an open-source, user-friendly simulation interface\nusing the Simulation Open Framework Architecture (SOFA), specifically designed\nto model magnetic soft robots. The tool enables users to define material\nproperties, apply magnetic fields, and observe resulting deformations in real\ntime. By integrating intuitive controls and stress analysis capabilities, it\naims to bridge the gap between theoretical modeling and practical design. Four\nbenchmark models - a beam, three- and four-finger grippers, and a butterfly -\ndemonstrate its functionality. The software's ease of use makes it accessible\nto both beginners and advanced researchers. Future improvements will refine\naccuracy through experimental validation and comparison with industry-standard\nfinite element solvers, ensuring realistic and predictive simulations of\nmagnetic soft robots.", "AI": {"tldr": "开发了一个基于SOFA的开源、用户友好的磁性软机器人仿真工具。", "motivation": "现有仿真平台缺乏对磁性材料的专用支持，且对不同专业水平的研究人员使用不便。", "method": "利用仿真开放框架架构（SOFA）开发了一个开源、用户友好的仿真界面，支持定义材料属性、施加磁场、实时观察变形和应力分析。", "result": "通过梁、三指/四指抓手和蝴蝶等四个基准模型验证了其功能，展示了其易用性，适用于初学者和高级研究人员。", "conclusion": "该工具旨在弥合理论建模与实际设计之间的差距，未来将通过实验验证和与工业标准有限元求解器比较来提高精度，以实现更真实和预测性的仿真。"}}
{"id": "2508.10011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10011", "abs": "https://arxiv.org/abs/2508.10011", "authors": ["Yuta Nagamori", "Mikoto Kosai", "Yuji Kawai", "Haruka Marumo", "Misaki Shibuya", "Tatsuya Negishi", "Masaki Imanishi", "Yasumasa Ikeda", "Koichiro Tsuchiya", "Asuka Sawai", "Licht Miyamoto"], "title": "Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan", "comment": null, "summary": "Generative artificial intelligence (AI) based on large language models\n(LLMs), such as ChatGPT, has demonstrated remarkable progress across various\nprofessional fields, including medicine and education. However, their\nperformance in nutritional education, especially in Japanese national licensure\nexamination for registered dietitians, remains underexplored. This study aimed\nto evaluate the potential of current LLM-based generative AI models as study\naids for nutrition students. Questions from the Japanese national examination\nfor registered dietitians were used as prompts for ChatGPT and three Bing\nmodels (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question\nwas entered into independent sessions, and model responses were analyzed for\naccuracy, consistency, and response time. Additional prompt engineering,\nincluding role assignment, was tested to assess potential performance\nimprovements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the\npassing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did\nnot. Bing-Precise and Bing-Creative generally outperformed others across\nsubject fields except Nutrition Education, where all models underperformed.\nNone of the models consistently provided the same correct responses across\nrepeated attempts, highlighting limitations in answer stability. ChatGPT showed\ngreater consistency in response patterns but lower accuracy. Prompt engineering\nhad minimal effect, except for modest improvement when correct answers and\nexplanations were explicitly provided. While some generative AI models\nmarginally exceeded the passing threshold, overall accuracy and answer\nconsistency remained suboptimal. Moreover, all the models demonstrated notable\nlimitations in answer consistency and robustness. Further advancements are\nneeded to ensure reliable and stable AI-based study aids for dietitian\nlicensure preparation.", "AI": {"tldr": "评估了基于LLM的生成式AI模型在注册营养师国家考试（日本）中的表现，发现部分模型勉强达到及格线，但总体准确性和答案一致性不佳。", "motivation": "尽管LLM在医学和教育等领域表现出色，但其在营养教育，特别是日本注册营养师国家执照考试中的表现尚未得到充分探索。本研究旨在评估当前LLM模型作为营养学学生学习辅助工具的潜力。", "method": "使用日本注册营养师国家考试的题目作为ChatGPT和三个Bing模型（Precise、Creative、Balanced，基于GPT-3.5和GPT-4）的提示。每个问题独立输入，分析模型的准确性、一致性和响应时间。还测试了包括角色分配在内的额外提示工程，以评估性能改进。", "result": "Bing-Precise（66.2%）和Bing-Creative（61.4%）超过了及格线（60%），而Bing-Balanced（43.3%）和ChatGPT（42.8%）未达到。Bing-Precise和Bing-Creative在除营养教育外的所有学科领域普遍优于其他模型。所有模型在重复尝试中未能持续提供相同的正确答案，答案稳定性有限。ChatGPT响应模式一致性更高但准确性较低。提示工程效果微乎其微，仅在明确提供正确答案和解释时有小幅改善。", "conclusion": "尽管一些生成式AI模型勉强超过及格线，但总体准确性和答案一致性仍不理想。所有模型在答案一致性和鲁棒性方面都存在显著局限性。未来需要进一步发展以确保为营养师执照考试准备提供可靠和稳定的AI学习辅助工具。"}}
{"id": "2508.10281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10281", "abs": "https://arxiv.org/abs/2508.10281", "authors": ["Ryota Tanaka", "Tomohiro Suzuki", "Keisuke Fujii"], "title": "VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation", "comment": null, "summary": "Understanding human actions from videos plays a critical role across various\ndomains, including sports analytics. In figure skating, accurately recognizing\nthe type and timing of jumps a skater performs is essential for objective\nperformance evaluation. However, this task typically requires expert-level\nknowledge due to the fine-grained and complex nature of jump procedures. While\nrecent approaches have attempted to automate this task using Temporal Action\nSegmentation (TAS), there are two major limitations to TAS for figure skating:\nthe annotated data is insufficient, and existing methods do not account for the\ninherent three-dimensional aspects and procedural structure of jump actions. In\nthis work, we propose a new TAS framework for figure skating jumps that\nexplicitly incorporates both the three-dimensional nature and the semantic\nprocedure of jump movements. First, we propose a novel View-Invariant, Figure\nSkating-Specific pose representation learning approach (VIFSS) that combines\ncontrastive learning as pre-training and action classification as fine-tuning.\nFor view-invariant contrastive pre-training, we construct FS-Jump3D, the first\npublicly available 3D pose dataset specialized for figure skating jumps.\nSecond, we introduce a fine-grained annotation scheme that marks the ``entry\n(preparation)'' and ``landing'' phases, enabling TAS models to learn the\nprocedural structure of jumps. Extensive experiments demonstrate the\neffectiveness of our framework. Our method achieves over 92% F1@50 on\nelement-level TAS, which requires recognizing both jump types and rotation\nlevels. Furthermore, we show that view-invariant contrastive pre-training is\nparticularly effective when fine-tuning data is limited, highlighting the\npracticality of our approach in real-world scenarios.", "AI": {"tldr": "本文提出了一种针对花样滑冰跳跃的全新时间动作分割（TAS）框架，该框架明确整合了跳跃动作的三维特性和语义程序，通过视图不变的姿态表示学习和细粒度标注，显著提高了跳跃类型和旋转水平的识别准确率。", "motivation": "准确识别花样滑冰跳跃对于客观表现评估至关重要，但由于其精细和复杂性，通常需要专家知识。现有的时间动作分割（TAS）方法存在两大局限性：标注数据不足，以及未能考虑跳跃动作固有的三维特性和程序结构。", "method": "1. 提出了一个新颖的TAS框架，明确整合了跳跃动作的三维特性和语义程序。2. 引入了一种视图不变的花样滑冰专用姿态表示学习方法（VIFSS），结合对比学习进行预训练和动作分类进行微调。3. 构建了FS-Jump3D，首个公开的花样滑冰跳跃专用3D姿态数据集，用于视图不变的对比预训练。4. 引入了细粒度标注方案，标记了跳跃的“进入（准备）”和“落地”阶段，使TAS模型能够学习跳跃的程序结构。", "result": "该框架在元素级TAS上实现了超过92%的F1@50分数，该任务要求同时识别跳跃类型和旋转级别。此外，研究表明，当微调数据有限时，视图不变的对比预训练特别有效，突出了该方法在实际场景中的实用性。", "conclusion": "所提出的框架能有效应对花样滑冰跳跃识别的挑战，通过结合三维姿态学习和程序结构建模，显著提高了识别准确率，并且在数据受限的情况下表现出强大的鲁棒性和实用性。"}}
{"id": "2508.10425", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10425", "abs": "https://arxiv.org/abs/2508.10425", "authors": ["Yan Ting Chok", "Soyon Park", "Seungheun Baek", "Hajung Kim", "Junhyun Lee", "Jaewoo Kang"], "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation", "comment": null, "summary": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings.", "AI": {"tldr": "针对稀有和不完整电子健康记录（EHR）数据导致的药物推荐泛化性差问题，本文提出了HiRef框架，通过结合医学本体论的层次语义和精炼的EHR共现模式，显著提升了模型在未见代码情况下的鲁棒性和准确性。", "motivation": "现有基于EHR的药物推荐模型在处理真实世界数据时面临挑战，因为数据中存在稀有或不完整的医学实体，导致模型难以在缺失或新颖条件下泛化，主要原因是它们过度依赖观察到的共现模式。", "method": "本文提出了HiRef框架，结合了两种互补结构：1) 医学本体论中编码的层次语义，将本体实体嵌入到双曲空间中以捕捉树状关系并实现知识迁移；2) 从EHR中提炼出的共现模式，通过先验引导的稀疏正则化方案来抑制虚假关联，同时保留临床有意义的关联。", "result": "HiRef模型在EHR基准数据集（MIMIC-III和MIMIC-IV）上取得了强大的性能，并在模拟的未见代码设置下保持了高准确性。广泛的实验和全面的消融研究表明，HiRef对未见医学代码具有很强的鲁棒性。", "conclusion": "HiRef通过整合层次医学本体知识和精炼EHR共现图，有效解决了稀有和不完整EHR数据在药物推荐中的挑战，显著提高了模型对未见医学代码的泛化能力和鲁棒性。"}}
{"id": "2508.10689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10689", "abs": "https://arxiv.org/abs/2508.10689", "authors": ["Matteo Luperto", "Valerii Stakanov", "Giacomo Boracchi", "Nicola Basilico", "Francesco Amigoni"], "title": "Biasing Frontier-Based Exploration with Saliency Areas", "comment": "Accepted at the European Confrence on Mobile Robots (ECMR) 2025", "summary": "Autonomous exploration is a widely studied problem where a robot\nincrementally builds a map of a previously unknown environment. The robot\nselects the next locations to reach using an exploration strategy. To do so,\nthe robot has to balance between competing objectives, like exploring the\nentirety of the environment, while being as fast as possible. Most exploration\nstrategies try to maximise the explored area to speed up exploration; however,\nthey do not consider that parts of the environment are more important than\nothers, as they lead to the discovery of large unknown areas. We propose a\nmethod that identifies \\emph{saliency areas} as those areas that are of high\ninterest for exploration, by using saliency maps obtained from a neural network\nthat, given the current map, implements a termination criterion to estimate\nwhether the environment can be considered fully-explored or not. We use\nsaliency areas to bias some widely used exploration strategies, showing, with\nan extensive experimental campaign, that this knowledge can significantly\ninfluence the behavior of the robot during exploration.", "AI": {"tldr": "该论文提出一种利用神经网络生成显著性图来识别探索高兴趣区域（显著性区域）的方法，并用其偏置现有探索策略，以显著影响机器人的探索行为，同时提供探索终止标准。", "motivation": "现有自主探索策略主要关注最大化探索面积以提高速度，但忽略了环境中某些区域对于发现大片未知区域更重要，且缺乏有效的探索终止标准。", "method": "提出一种方法，通过神经网络从当前地图生成显著性图来识别“显著性区域”（对探索高度感兴趣的区域）。该神经网络还实现了一个终止标准，用于判断环境是否已完全探索。然后，利用这些显著性区域来偏置一些广泛使用的探索策略。", "result": "通过广泛的实验证明，将显著性区域的知识融入探索策略中，可以显著影响机器人在探索过程中的行为。", "conclusion": "识别并利用环境中的显著性区域，能有效引导机器人探索，使其行为更具策略性，有助于更高效地完成探索任务。"}}
{"id": "2508.10012", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10012", "abs": "https://arxiv.org/abs/2508.10012", "authors": ["Dehao Tao", "Guangjie Liu", "Weizheng", "Yongfeng Huang", "Minghu jiang"], "title": "Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs", "comment": null, "summary": "While Large Language Models (LLMs) exhibit strong linguistic capabilities,\ntheir reliance on static knowledge and opaque reasoning processes limits their\nperformance in knowledge intensive tasks. Knowledge graphs (KGs) offer a\npromising solution, but current exploration methods face a fundamental trade\noff: question guided approaches incur redundant exploration due to granularity\nmismatches, while clue guided methods fail to effectively leverage contextual\ninformation for complex scenarios. To address these limitations, we propose\nGuidance Graph guided Knowledge Exploration (GG Explore), a novel framework\nthat introduces an intermediate Guidance Graph to bridge unstructured queries\nand structured knowledge retrieval. The Guidance Graph defines the retrieval\nspace by abstracting the target knowledge' s structure while preserving broader\nsemantic context, enabling precise and efficient exploration. Building upon the\nGuidance Graph, we develop: (1) Structural Alignment that filters incompatible\ncandidates without LLM overhead, and (2) Context Aware Pruning that enforces\nsemantic consistency with graph constraints. Extensive experiments show our\nmethod achieves superior efficiency and outperforms SOTA, especially on complex\ntasks, while maintaining strong performance with smaller LLMs, demonstrating\npractical value.", "AI": {"tldr": "GG Explore框架引入引导图，解决了LLM在知识密集型任务中知识图谱探索的效率和精度问题，通过结构对齐和上下文感知剪枝实现高效检索。", "motivation": "大型语言模型（LLMs）在知识密集型任务中受限于静态知识和不透明的推理过程。现有知识图谱（KGs）探索方法存在权衡：问题引导方法导致冗余探索，而线索引导方法无法有效利用上下文信息。", "method": "提出Guidance Graph guided Knowledge Exploration (GG Explore) 框架。该框架引入中间引导图（Guidance Graph），通过抽象目标知识结构并保留语义上下文来定义检索空间。在此基础上，开发了：1) 结构对齐（Structural Alignment），无需LLM开销即可过滤不兼容的候选；2) 上下文感知剪枝（Context Aware Pruning），强制执行图约束的语义一致性。", "result": "实验证明，GG Explore在效率上表现优异，超越了现有最先进方法（SOTA），尤其在复杂任务上表现突出。同时，即使使用较小的LLM也能保持强大的性能，展现了实用价值。", "conclusion": "GG Explore通过引入引导图，成功地弥合了非结构化查询与结构化知识检索之间的鸿沟，实现了知识图谱的精确高效探索，显著提升了LLM在知识密集型任务中的表现。"}}
{"id": "2508.10287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10287", "abs": "https://arxiv.org/abs/2508.10287", "authors": ["Simindokht Jahangard", "Mehrzad Mohammadi", "Yi Shen", "Zhixi Cai", "Hamid Rezatofighi"], "title": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) and large language models\n(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI\nagents like robots. However, existing visual reasoning benchmarks often suffer\nfrom several limitations: they lack a clear definition of reasoning complexity,\noffer have no control to generate questions over varying difficulty and task\ncustomization, and fail to provide structured, step-by-step reasoning\nannotations (workflows). To bridge these gaps, we formalize reasoning\ncomplexity, introduce an adaptive query engine that generates customizable\nquestions of varying complexity with detailed intermediate annotations, and\nextend the JRDB dataset with human-object interaction and geometric\nrelationship annotations to create JRDB-Reasoning, a benchmark tailored for\nvisual reasoning in human-crowded environments. Our engine and benchmark enable\nfine-grained evaluation of visual reasoning frameworks and dynamic assessment\nof visual-language models across reasoning levels.", "AI": {"tldr": "本文提出了JRDB-Reasoning，一个针对拥挤环境中视觉推理的新基准，以及一个自适应查询引擎，旨在解决现有基准在推理复杂度定义、难度控制和逐步推理标注方面的不足。", "motivation": "现有视觉推理基准存在局限性，包括缺乏推理复杂度的明确定义、无法控制生成不同难度和定制化任务的问题，以及未能提供结构化的、分步的推理标注，这些限制阻碍了对具身AI代理（如机器人）关键能力——视觉推理的有效评估。", "method": "研究者通过以下方法弥补了现有差距：1) 形式化定义了推理复杂度；2) 引入了一个自适应查询引擎，能够生成不同复杂度和定制化的问题，并提供详细的中间标注；3) 扩展了JRDB数据集，增加了人-物交互和几何关系标注，创建了JRDB-Reasoning基准。", "result": "研究成果是创建了JRDB-Reasoning基准和自适应查询引擎。这些工具能够对视觉推理框架进行细粒度评估，并对视觉-语言模型在不同推理级别进行动态评估，特别适用于人类密集环境下的视觉推理任务。", "conclusion": "所提出的引擎和基准为视觉推理框架和视觉-语言模型提供了一个更全面、更精细的评估工具，能够克服现有基准的局限性，从而推动具身AI领域的发展。"}}
{"id": "2508.10429", "categories": ["cs.AI", "cs.CR", "cs.CV", "I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.10429", "abs": "https://arxiv.org/abs/2508.10429", "authors": ["Yi Dong", "Yusuke Muraoka", "Scott Shi", "Yi Zhang"], "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance", "comment": "10 pages, 5 figures, 6 tables. The dataset is available at\n  https://huggingface.co/datasets/Codatta/MM-Food-100K", "summary": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence\ndataset with verifiable provenance. It is a curated approximately 10% open\nsubset of an original 1.2 million, quality-accepted corpus of food images\nannotated for a wide range of information (such as dish name, region of\ncreation). The corpus was collected over six weeks from over 87,000\ncontributors using the Codatta contribution model, which combines community\nsourcing with configurable AI-assisted quality checks; each submission is\nlinked to a wallet address in a secure off-chain ledger for traceability, with\na full on-chain protocol on the roadmap. We describe the schema, pipeline, and\nQA, and validate utility by fine-tuning large vision-language models (ChatGPT\n5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning\nyields consistent gains over out-of-box baselines across standard metrics; we\nreport results primarily on the MM-Food-100K subset. We release MM-Food-100K\nfor publicly free access and retain approximately 90% for potential commercial\naccess with revenue sharing to contributors.", "AI": {"tldr": "本文介绍了MM-Food-100K，一个包含10万样本的多模态食物智能公开数据集，其数据来源可追溯。该数据集通过结合社区众包和AI辅助质量检查的Codatta模型收集，并验证了其在微调大型视觉-语言模型进行营养预测方面的有效性。", "motivation": "现有食物数据集可能缺乏大规模、高质量、多模态以及可追溯的数据，这限制了食物智能领域，特别是基于图像的营养预测等应用的发展。因此需要一个具有可验证来源的大型多模态食物智能数据集。", "method": "研究方法包括：1. 构建MM-Food-100K数据集，它是从一个包含120万高质量食物图像的语料库中精选出的10万样本子集，包含菜品名称、创建区域等多种信息。2. 采用Codatta贡献模型进行数据收集，该模型结合了社区众包和可配置的AI辅助质量检查，并为每份提交链接一个钱包地址以实现可追溯性（目前为链下账本，计划上链）。3. 通过使用MM-Food-100K数据集对大型视觉-语言模型（如ChatGPT 5、ChatGPT OSS、Qwen-Max）进行微调，以验证其在基于图像的营养预测任务中的实用性。", "result": "使用MM-Food-100K数据集进行微调，在标准指标上，相较于开箱即用的基线模型，视觉-语言模型在营养预测任务中获得了持续的性能提升。主要结果基于MM-Food-100K子集报告。", "conclusion": "MM-Food-100K是一个高质量、可追溯的多模态食物智能数据集，已公开发布，并被证明能够有效提升大型视觉-语言模型在食物智能任务（如营养预测）上的表现。其独特的众包和质量控制模型为未来大规模数据收集提供了新范式。"}}
{"id": "2508.10798", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10798", "abs": "https://arxiv.org/abs/2508.10798", "authors": ["Troi Williams"], "title": "The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems", "comment": "4 pages, 4 figures, accepted to the Workshop on Public Trust in\n  Autonomous Systems at the 2025 IEEE International Conference on Robotics &\n  Automation", "summary": "Future autonomous systems promise significant societal benefits, yet their\ndeployment raises concerns about safety and trustworthiness. A key concern is\nassuring the reliability of robot perception, as perception seeds safe\ndecision-making. Failures in perception are often due to complex yet common\nenvironmental factors and can lead to accidents that erode public trust. To\naddress this concern, we introduce the SET (Self, Environment, and Target)\nPerceptual Factors Framework. We designed the framework to systematically\nanalyze how factors such as weather, occlusion, or sensor limitations\nnegatively impact perception. To achieve this, the framework employs SET State\nTrees to categorize where such factors originate and SET Factor Trees to model\nhow these sources and factors impact perceptual tasks like object detection or\npose estimation. Next, we develop Perceptual Factor Models using both trees to\nquantify the uncertainty for a given task. Our framework aims to promote\nrigorous safety assurances and cultivate greater public understanding and trust\nin autonomous systems by offering a transparent and standardized method for\nidentifying, modeling, and communicating perceptual risks.", "AI": {"tldr": "本文提出了SET（自我、环境和目标）感知因素框架，用于系统分析环境因素如何影响机器人感知，旨在提高自动驾驶系统的安全性、透明度和公众信任。", "motivation": "未来的自动驾驶系统虽然潜力巨大，但其部署面临安全和信任问题。特别是机器人感知的可靠性至关重要，因为感知失败（常由复杂环境因素引起）可能导致事故并损害公众信任。", "method": "引入SET感知因素框架。该框架使用SET状态树来分类感知因素的来源（自我、环境、目标），并使用SET因素树来建模这些来源和因素如何影响特定感知任务（如物体检测、姿态估计）。随后，利用这两种树开发感知因素模型来量化给定任务的不确定性。", "result": "该框架提供了一种透明且标准化的方法，用于识别、建模和沟通感知风险。它有助于系统地分析天气、遮挡或传感器限制等因素对感知的不利影响。", "conclusion": "SET框架旨在促进严格的安全保障，并通过提供识别、建模和沟通感知风险的透明标准化方法，培养公众对自动驾驶系统更深入的理解和信任。"}}
{"id": "2508.10013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10013", "abs": "https://arxiv.org/abs/2508.10013", "authors": ["Linqing Chen", "Hanmeng Zhong", "Wentao Wu", "Weilei Wang"], "title": "Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis", "comment": null, "summary": "Large language model (LLM) training faces a critical bottleneck: the scarcity\nof high-quality, reasoning-intensive question-answer pairs, especially from\nsparse, domain-specific sources like PubMed papers or legal documents. Existing\nmethods rely on surface patterns, fundamentally failing to generate\ncontrollable, complex multi-hop reasoning questions that test genuine\nunderstanding-essential for advancing LLM training paradigms. We present\n\\textbf{Semantic Bridge}, the first universal framework for controllably\ngenerating sophisticated multi-hop reasoning questions from arbitrary sources.\nOur breakthrough innovation is \\textit{semantic graph weaving}-three\ncomplementary bridging mechanisms (entity bridging for role-varying shared\nentities, predicate chain bridging for temporal/causal/logical sequences, and\ncausal bridging for explicit reasoning chains)-that systematically construct\ncomplex pathways across documents, with fine-grained control over complexity\nand types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to\n9.5% better round-trip quality, enabling production-ready controllable QA\ngeneration. Extensive evaluation demonstrates performance across both\ngeneral-purpose datasets (Wikipedia) and specialized domains (biomedicine) It\nyields consistent 18.3%-25.4% gains over baselines across four languages\n(English, Chinese, French, German). Question pairs generated from 200 sources\noutperform 600 native human annotation examples with 67% fewer materials. Human\nevaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%\nimproved pattern coverage. Semantic Bridge establishes a new paradigm for LLM\ntraining data synthesis, enabling controllable generation of targeted reasoning\nquestions from sparse sources. We will release our core code and semantic\nbridge model.", "AI": {"tldr": "该论文提出了“语义桥”（Semantic Bridge）框架，首次实现了从任意稀疏来源可控地生成复杂多跳推理问答对，以解决大型语言模型（LLM）训练中高质量推理数据稀缺的问题。", "motivation": "大型语言模型训练面临高质量、推理密集型问答对稀缺的瓶颈，尤其是在PubMed论文或法律文件等稀疏的领域特定来源中。现有方法依赖于表面模式，无法生成可控、复杂的、能测试真正理解的多跳推理问题，这阻碍了LLM训练范式的进步。", "method": "核心创新是“语义图编织”（semantic graph weaving），包含三种互补的桥接机制：实体桥接（用于角色变化的共享实体）、谓词链桥接（用于时间/因果/逻辑序列）和因果桥接（用于显式推理链）。通过抽象意义表示（AMR）驱动的分析，系统地构建跨文档的复杂路径，并对复杂性和类型进行细粒度控制。采用多模态AMR管道。", "result": "该方法在往返质量上提高了9.5%，在四种语言（英语、中文、法语、德语）上比基线提高了18.3%-25.4%。从200个来源生成的问答对，仅用67%的材料就超越了600个人工标注示例。人工评估显示，复杂性提高了23.4%，可回答性提高了18.7%，模式覆盖率提高了31.2%。", "conclusion": "“语义桥”为LLM训练数据合成建立了一个新范式，使得从稀疏来源可控地生成目标推理问题成为可能。核心代码和模型将对外发布。"}}
{"id": "2508.10294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10294", "abs": "https://arxiv.org/abs/2508.10294", "authors": ["Tao Huang", "Hongbo Pan", "Nanxi Zhou", "Shun Zhou"], "title": "A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method", "comment": null, "summary": "High-accuracy matching of multimodal optical images is the basis of geometric\nprocessing. However, the image matching accuracy is usually degraded by the\nnonlinear radiation and geometric deformation differences caused by different\nspectral responses. To address these problems, we proposed a phase consistency\nweighted least absolute deviation (PCWLAD) sub-pixel template matching method\nto improve the matching accuracy of multimodal optical images. This method\nconsists of two main steps: coarse matching with the structural similarity\nindex measure (SSIM) and fine matching with WLAD. In the coarse matching step,\nPCs are calculated without a noise filter to preserve the original structural\ndetails, and template matching is performed using the SSIM. In the fine\nmatching step, we applied the radiometric and geometric transformation models\nbetween two multimodal PC templates based on the coarse matching. Furthermore,\nmutual structure filtering is adopted in the model to mitigate the impact of\nnoise within the corresponding templates on the structural consistency, and the\nWLAD criterion is used to estimate the sub-pixel offset. To evaluate the\nperformance of PCWLAD, we created three types of image datasets: visible to\ninfrared Landsat images, visible to near-infrared close-range images, and\nvisible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed\nexisting state-of-the-art eight methods in terms of correct matching rate (CMR)\nand root mean square error (RMSE) and reached an average matching accuracy of\napproximately 0.4 pixels across all three datasets. Our software and datasets\nare publicly available at https://github.com/huangtaocsu/PCWLAD.", "AI": {"tldr": "本文提出了一种名为PCWLAD（相位一致性加权最小绝对偏差）的亚像素模板匹配方法，用于提高多模态光学图像的匹配精度。该方法结合了SSIM粗匹配和WLAD精匹配，并在多个数据集上实现了优于现有方法的亚像素级匹配精度。", "motivation": "多模态光学图像的高精度匹配是几何处理的基础，但不同光谱响应导致的非线性辐射和几何形变差异通常会降低图像匹配精度。为了解决这些问题，需要开发更鲁棒和精确的匹配方法。", "method": "本文提出的PCWLAD方法包含两个主要步骤：\n1.  **粗匹配**：计算不带噪声滤波的相位一致性（PC），并使用结构相似性指数（SSIM）进行模板匹配。\n2.  **精匹配**：基于粗匹配结果，在两个多模态PC模板之间应用辐射和几何变换模型，并采用相互结构滤波以减轻噪声影响，最后使用加权最小绝对偏差（WLAD）准则估计亚像素偏移。", "result": "PCWLAD在三种不同类型的图像数据集（可见光-红外Landsat图像、可见光-近红外近距离图像、可见光-红外无人机图像）上进行了性能评估。结果表明，PCWLAD在正确匹配率（CMR）和均方根误差（RMSE）方面优于现有八种最先进的方法，并在所有三个数据集上实现了大约0.4像素的平均匹配精度。", "conclusion": "PCWLAD方法能够有效解决多模态光学图像匹配中非线性辐射和几何形变差异带来的精度下降问题，显著提高了匹配精度，达到了亚像素级别，证明了其在实际应用中的优越性。"}}
{"id": "2508.10433", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10433", "abs": "https://arxiv.org/abs/2508.10433", "authors": ["Runqi Qiao", "Qiuna Tan", "Peiqing Yang", "Yanzi Wang", "Xiaowan Wang", "Enhui Wan", "Sitong Zhou", "Guanting Dong", "Yuchen Zeng", "Yida Xu", "Jie Wang", "Chong Sun", "Chen Li", "Honggang Zhang"], "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning", "comment": "Working in progress", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.", "AI": {"tldr": "We-Math 2.0是一个统一系统，通过结构化数学知识系统、模型中心数据空间建模和强化学习训练范式，全面提升多模态大语言模型（MLLMs）的数学推理能力。", "motivation": "多模态大语言模型（MLLMs）在复杂数学推理方面仍存在困难。现有研究主要集中在数据集构建和方法优化，但忽视了知识驱动设计和模型中心数据空间建模。", "method": "本文提出了We-Math 2.0系统，包括四个关键贡献：1) MathBook知识系统：构建了包含491个知识点和1819个基本原理的五级分层系统。2) MathBook-Standard & Pro数据集：MathBook-Standard通过双重扩展确保广泛概念覆盖，MathBook-Pro则定义了三维难度空间，为每个问题生成7个渐进变体以进行鲁棒训练。3) MathBook-RL：提出了一个两阶段强化学习框架，包括冷启动微调（与知识导向的思维链推理对齐）和渐进对齐RL（利用平均奖励学习和动态数据调度实现跨难度级别的渐进对齐）。4) MathBookEval：引入了一个涵盖所有491个知识点、具有多样推理步骤分布的综合基准。", "result": "实验结果表明，MathBook-RL在四个广泛使用的基准上与现有基线表现相当，并在MathBookEval上取得了优异成绩。", "conclusion": "We-Math 2.0显著增强了MLLMs的数学推理能力，并在数学推理方面显示出有前景的泛化能力。"}}
{"id": "2508.10828", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10828", "abs": "https://arxiv.org/abs/2508.10828", "authors": ["Henry Powell", "Guy Laban", "Emily S. Cross"], "title": "A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots", "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Subjective self-disclosure is an important feature of human social\ninteraction. While much has been done in the social and behavioural literature\nto characterise the features and consequences of subjective self-disclosure,\nlittle work has been done thus far to develop computational systems that are\nable to accurately model it. Even less work has been done that attempts to\nmodel specifically how human interactants self-disclose with robotic partners.\nIt is becoming more pressing as we require social robots to work in conjunction\nwith and establish relationships with humans in various social settings. In\nthis paper, our aim is to develop a custom multimodal attention network based\non models from the emotion recognition literature, training this model on a\nlarge self-collected self-disclosure video corpus, and constructing a new loss\nfunction, the scale preserving cross entropy loss, that improves upon both\nclassification and regression versions of this problem. Our results show that\nthe best performing model, trained with our novel loss function, achieves an F1\nscore of 0.83, an improvement of 0.48 from the best baseline model. This result\nmakes significant headway in the aim of allowing social robots to pick up on an\ninteraction partner's self-disclosures, an ability that will be essential in\nsocial robots with social cognition.", "AI": {"tldr": "该研究旨在开发一个计算模型，以准确识别与机器人互动的过程中人类的主观自我表露，并提出了一种新的损失函数，显著提高了模型性能。", "motivation": "自我表露是人类社交的重要特征，但目前很少有计算系统能准确建模它，尤其是在人机交互中。随着社交机器人需与人类建立关系，使其能识别自我表露变得愈发紧迫。", "method": "开发了一个定制的多模态注意力网络，借鉴了情感识别领域的模型。在一个大型的、自收集的自我表露视频语料库上训练该模型，并构建了一种新的损失函数——尺度保持交叉熵损失（scale preserving cross entropy loss），以同时改进分类和回归问题。", "result": "使用新颖损失函数训练的最佳模型，F1分数达到0.83，比最佳基线模型提高了0.48。这在使社交机器人识别互动伙伴的自我表露方面取得了显著进展。", "conclusion": "该研究成功开发了一个高性能的计算模型来识别自我表露，特别是通过引入新的损失函数，极大地提升了模型的准确性。这项能力对于未来具备社交认知的机器人至关重要。"}}
{"id": "2508.10014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10014", "abs": "https://arxiv.org/abs/2508.10014", "authors": ["Lingfeng Zhou", "Jialing Zhang", "Jin Gao", "Mohan Jiang", "Dequan Wang"], "title": "PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?", "comment": "Accepted by COLM 2025", "summary": "Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,\nwhich may fail to reflect how humans perceive role fidelity. A key prerequisite\nfor human-aligned evaluation is role identification, the ability to recognize\nwho is speaking based on dialogue context. We argue that any meaningful\njudgment of role-playing quality (how well a character is played) fundamentally\ndepends on first correctly attributing words and actions to the correct persona\n(who is speaking). We present PersonaEval, the first benchmark designed to test\nwhether LLM evaluators can reliably identify human roles. PersonaEval uses\nhuman-authored dialogues from novels, scripts, and video transcripts,\nchallenging models to determine the correct persona according to the\nconversation context. Our experiments, including a human study, show that even\nthe best-performing LLMs reach only around 69% accuracy, well below the level\nneeded for reliable evaluation. In contrast, human participants perform near\nceiling with 90.8% accuracy, highlighting that current LLM evaluators are still\nnot human enough to effectively judge role-play scenarios. To better understand\nthis gap, we examine training-time adaptation and test-time compute, suggesting\nthat reliable evaluation requires more than task-specific tuning, but depends\non strong, human-like reasoning abilities in LLM evaluators. We release our\nbenchmark at https://github.com/maple-zhou/PersonaEval.", "AI": {"tldr": "研究发现，当前LLM作为角色扮演评估者在角色识别能力上远低于人类，导致其评估可靠性不足，需要更强的人类级推理能力。", "motivation": "目前的LLM作为评估者的角色扮演研究范式未经充分验证，可能无法准确反映人类对角色忠实度的感知。作者认为，对角色扮演质量的任何有效判断都首先依赖于正确识别对话中的发言者（角色识别）。", "method": "提出了PersonaEval，这是首个旨在测试LLM评估器能否可靠识别人类角色的基准。该基准使用来自小说、剧本和视频转录的人类创作对话，挑战模型根据对话上下文确定正确的角色。同时进行了一项人类研究作为对比。", "result": "实验结果显示，即使是表现最佳的LLM，其角色识别准确率也仅为69%左右，远低于可靠评估所需的水平。相比之下，人类参与者的准确率接近天花板，达到90.8%。研究还表明，可靠的评估需要LLM评估器具备强大、类人的推理能力，而不仅仅是任务特定的微调或计算资源。", "conclusion": "目前的LLM评估器在角色识别方面与人类存在显著差距，因此不足以有效地判断角色扮演场景。要实现可靠的评估，LLM需要发展出更强的、类似人类的推理能力。"}}
{"id": "2508.10297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10297", "abs": "https://arxiv.org/abs/2508.10297", "authors": ["Yiyi Ma", "Yuanzhi Liang", "Xiu Li", "Chi Zhang", "Xuelong Li"], "title": "InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild", "comment": "Accepted by ICCV2025", "summary": "We present Interleaved Learning for Motion Synthesis (InterSyn), a novel\nframework that targets the generation of realistic interaction motions by\nlearning from integrated motions that consider both solo and multi-person\ndynamics. Unlike previous methods that treat these components separately,\nInterSyn employs an interleaved learning strategy to capture the natural,\ndynamic interactions and nuanced coordination inherent in real-world scenarios.\nOur framework comprises two key modules: the Interleaved Interaction Synthesis\n(INS) module, which jointly models solo and interactive behaviors in a unified\nparadigm from a first-person perspective to support multiple character\ninteractions, and the Relative Coordination Refinement (REC) module, which\nrefines mutual dynamics and ensures synchronized motions among characters.\nExperimental results show that the motion sequences generated by InterSyn\nexhibit higher text-to-motion alignment and improved diversity compared with\nrecent methods, setting a new benchmark for robust and natural motion\nsynthesis. Additionally, our code will be open-sourced in the future to promote\nfurther research and development in this area.", "AI": {"tldr": "InterSyn是一种新颖的运动合成框架，通过交错学习策略，同时考虑单人和多人动力学，生成逼真自然的交互动作，并提高了文本到动作的对齐和多样性。", "motivation": "现有方法将单人与多人运动组件分开处理，导致生成的交互动作不够自然。本研究旨在捕捉现实世界场景中固有的自然、动态交互和细致协调。", "method": "InterSyn框架采用交错学习策略，包含两个核心模块：1. 交错交互合成（INS）模块：从第一人称视角统一建模单人和交互行为，支持多角色交互。2. 相对协调细化（REC）模块：细化相互动力学并确保角色间动作同步。", "result": "InterSyn生成的动作序列与现有方法相比，表现出更高的文本到动作对齐度，并显著提高了多样性，为鲁棒和自然的运动合成设立了新基准。", "conclusion": "InterSyn框架能够生成更鲁棒和自然的交互动作，未来将开源代码以促进该领域的进一步研究和发展。"}}
{"id": "2508.10467", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.10467", "abs": "https://arxiv.org/abs/2508.10467", "authors": ["Xueli Pan", "Victor de Boer", "Jacco van Ossenbruggen"], "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs", "comment": "Accepted at 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management (IC3K)", "summary": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set.", "AI": {"tldr": "提出FIRESPARQL框架，通过微调LLM、结合RAG和SPARQL纠错层，显著提升了学术知识图谱上的自然语言问答（NLQ到SPARQL）性能。", "motivation": "LLM在将自然语言问题转换为学术知识图谱（SKG）上的SPARQL查询时存在困难，主要表现为查询的结构不一致（缺少或冗余三元组）和语义不准确（错误的实体或属性），这是由于LLM对SKG特定内容和底层模式的暴露有限。", "method": "提出了FIRESPARQL，一个模块化框架，核心是微调的LLM，并可选地通过检索增强生成（RAG）提供上下文，以及一个SPARQL查询纠错层。在SciQA基准上，使用零样本、零样本+RAG、一次样本、微调、微调+RAG等多种配置进行评估，并与基线和SOTA方法进行比较。使用BLEU和ROUGE衡量查询准确性，使用RelaxedEM衡量查询结果准确性。", "result": "实验结果表明，微调（fine-tuning）实现了最高的整体性能，在测试集上查询准确性ROUGE-L达到0.90，结果准确性RelaxedEM达到0.85。", "conclusion": "微调是解决LLM在学术知识图谱上生成SPARQL查询时遇到的结构和语义错误问题的有效方法，显著提升了查询和结果的准确性。"}}
{"id": "2508.10872", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10872", "abs": "https://arxiv.org/abs/2508.10872", "authors": ["Anantha Narayanan", "Battu Bhanu Teja", "Pruthwik Mishra"], "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning", "comment": "8 pages, 6 figures, 5 tables", "summary": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning.", "AI": {"tldr": "该研究提出了一种基于A2C强化学习框架，用于优化卫星轨道参数以实现精确的地面覆盖，并在收敛速度和奖励方面优于PPO，适用于实时任务规划。", "motivation": "近地轨道（LEO）日益拥堵，给地球观测卫星的部署和安全运行带来了挑战。任务规划者不仅要考虑任务特定需求，还要应对与活跃卫星和空间碎片日益增长的碰撞风险。", "method": "本研究采用强化学习框架，利用优势演员-评论家（A2C）算法优化卫星轨道参数。问题被建模为马尔可夫决策过程（MDP），在自定义的OpenAI Gymnasium环境中进行，使用经典开普勒元素模拟轨道动力学。智能体逐步学习调整半长轴、偏心率、倾角、升交点赤经和近地点角五种轨道参数，以实现目标地面覆盖。通过与近端策略优化（PPO）进行比较评估。", "result": "A2C算法表现出卓越性能，实现了5.8倍更高的累积奖励（10.0 vs 9.263025），同时收敛所需的步数减少了31.5倍（2,000 vs 63,000）。A2C智能体在不同目标坐标下均能持续满足任务目标，并保持了适用于实时任务规划应用的计算效率。", "conclusion": "强化学习为可扩展和智能的近地轨道任务规划提供了一种计算高效的替代方案。该方法验证了演员-评论家方法在连续轨道控制方面优于信任域方法，并展示了快速收敛能力，从而实现了自适应卫星部署。"}}
{"id": "2508.10015", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10015", "abs": "https://arxiv.org/abs/2508.10015", "authors": ["Enzhi Wang", "Qicheng Li", "Shiwan Zhao", "Aobo Kong", "Jiaming Zhou", "Xi Yang", "Yequan Wang", "Yonghua Lin", "Yong Qin"], "title": "RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis", "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have achieved remarkable\nadvancements in multimodal processing, including end-to-end speech-based\nlanguage models that enable natural interactions and perform specific tasks in\ntask-oriented dialogue (TOD) systems. However, existing TOD datasets are\npredominantly text-based, lacking real speech signals that are essential for\nevaluating the robustness of speech-based LLMs. Moreover, existing speech TOD\ndatasets are primarily English and lack critical aspects such as speech\ndisfluencies and speaker variations. To address these gaps, we introduce\nRealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal\nTOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired\nspeech-text annotations. RealTalk-CN captures diverse dialogue scenarios with\nannotated spontaneous speech disfluencies, ensuring comprehensive coverage of\nreal-world complexities in speech dialogue. In addition, we propose a novel\ncross-modal chat task that authentically simulates real-world user\ninteractions, allowing dynamic switching between speech and text modalities.\nOur evaluation covers robustness to speech disfluencies, sensitivity to speaker\ncharacteristics, and cross-domain performance. Extensive experiments validate\nthe effectiveness of RealTalk-CN, establishing a strong foundation for Chinese\nspeech-based LLMs research.", "AI": {"tldr": "本文介绍了RealTalk-CN，首个中文多轮、多领域语音-文本双模态任务型对话数据集，旨在解决现有数据集缺乏真实语音、语流不连贯和说话人多样性等问题，并提出了一个跨模态聊天任务以促进中文语音大模型的研究。", "motivation": "现有任务型对话（TOD）数据集主要基于文本，缺乏评估语音大模型（LLMs）鲁棒性所需的真实语音信号。此外，现有语音TOD数据集以英文为主，且缺乏语音不流畅和说话人多样性等关键要素。", "method": "引入了RealTalk-CN数据集，包含5.4k对话（60K话语，150小时），配有语音-文本注释，涵盖了多种对话场景和标注的自发性语音不流畅。同时，提出了一个新颖的跨模态聊天任务，模拟真实用户交互中语音和文本模态的动态切换。", "result": "RealTalk-CN数据集被验证有效，能够评估语音大模型对语音不流畅的鲁棒性、对说话人特征的敏感性以及跨领域性能。它为中文语音大模型的研究奠定了坚实基础。", "conclusion": "RealTalk-CN是首个中文多轮、多领域语音-文本双模态任务型对话数据集，通过提供真实世界的复杂性和多样性，有效填补了现有数据集的空白，为中文语音大模型的研究和评估提供了重要资源。"}}
{"id": "2508.10309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10309", "abs": "https://arxiv.org/abs/2508.10309", "authors": ["Wenjie Zhao", "Jia Li", "Yunhui Guo"], "title": "From Pixel to Mask: A Survey of Out-of-Distribution Segmentation", "comment": null, "summary": "Out-of-distribution (OoD) detection and segmentation have attracted growing\nattention as concerns about AI security rise. Conventional OoD detection\nmethods identify the existence of OoD objects but lack spatial localization,\nlimiting their usefulness in downstream tasks. OoD segmentation addresses this\nlimitation by localizing anomalous objects at pixel-level granularity. This\ncapability is crucial for safety-critical applications such as autonomous\ndriving, where perception modules must not only detect but also precisely\nsegment OoD objects, enabling targeted control actions and enhancing overall\nsystem robustness. In this survey, we group current OoD segmentation approaches\ninto four categories: (i) test-time OoD segmentation, (ii) outlier exposure for\nsupervised training, (iii) reconstruction-based methods, (iv) and approaches\nthat leverage powerful models. We systematically review recent advances in OoD\nsegmentation for autonomous-driving scenarios, identify emerging challenges,\nand discuss promising future research directions.", "AI": {"tldr": "该论文综述了域外（OoD）分割技术，特别是针对自动驾驶场景的应用，并将其分为四类，同时讨论了挑战和未来方向。", "motivation": "随着对AI安全性的日益关注，域外检测和分割变得重要。传统的域外检测方法缺乏空间定位，限制了其在下游任务中的应用。域外分割通过像素级定位异常对象解决了这一限制，这对于自动驾驶等安全关键应用至关重要，因为这些应用需要精确地分割域外对象以实现有针对性的控制和增强系统鲁棒性。", "method": "该综述将当前的域外分割方法分为四类：(i) 测试时域外分割，(ii) 用于监督训练的异常暴露，(iii) 基于重建的方法，以及 (iv) 利用强大模型的方法。论文系统地回顾了自动驾驶场景中域外分割的最新进展。", "result": "论文系统地回顾了自动驾驶场景中域外分割的最新进展，识别了新兴挑战，并讨论了有前景的未来研究方向。", "conclusion": "域外分割对于安全关键应用（如自动驾驶）至关重要。当前方法可分为四类，但仍存在挑战，未来研究需进一步探索以提升其鲁棒性和实用性。"}}
{"id": "2508.10486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10486", "abs": "https://arxiv.org/abs/2508.10486", "authors": ["Ivan Khai Ze Lim", "Ningyi Liao", "Yiming Yang", "Gerald Wei Yong Yip", "Siqiang Luo"], "title": "SEQ-GPT: LLM-assisted Spatial Query via Example", "comment": null, "summary": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios.", "AI": {"tldr": "本文提出SEQ-GPT，一个由大型语言模型（LLM）驱动的空间范例查询（SEQ）系统，旨在通过自然语言和交互式反馈，实现对一组相关位置的更灵活搜索。", "motivation": "当前在线地图主要依赖用户查询进行单位置搜索，在执行复杂任务（如同时搜索一组位置）时用户体验受限。因此，需要一种更强大的方法来处理多位置、范例驱动的空间查询。", "method": "引入空间范例查询（SEQ）概念。开发了基于LLM的SEQ-GPT系统，利用LLM的语言能力实现独特的交互操作，包括向用户澄清查询细节和根据用户反馈动态调整搜索。提出了一种定制的LLM适应管道，通过对话合成和多模型协作，将自然语言与结构化空间数据和查询对齐。", "result": "SEQ-GPT提供了一个端到端的演示，展示了如何使用真实数据和应用场景扩展空间搜索能力。", "conclusion": "通过利用LLM，SEQ-GPT成功地将自然语言和交互性引入空间范例查询，极大地扩展了传统空间搜索的范畴和灵活性，提升了用户在复杂空间任务中的体验。"}}
{"id": "2508.10567", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10567", "abs": "https://arxiv.org/abs/2508.10567", "authors": ["Philipp Wolters", "Johannes Gilg", "Torben Teepe", "Gerhard Rigoll"], "title": "SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving", "comment": "8 pages, 4 figures, 5 tables", "summary": "End-to-end autonomous driving systems promise stronger performance through\nunified optimization of perception, motion forecasting, and planning. However,\nvision-based approaches face fundamental limitations in adverse weather\nconditions, partial occlusions, and precise velocity estimation - critical\nchallenges in safety-sensitive scenarios where accurate motion understanding\nand long-horizon trajectory prediction are essential for collision avoidance.\nTo address these limitations, we propose SpaRC-AD, a query-based end-to-end\ncamera-radar fusion framework for planning-oriented autonomous driving. Through\nsparse 3D feature alignment, and doppler-based velocity estimation, we achieve\nstrong 3D scene representations for refinement of agent anchors, map polylines\nand motion modelling. Our method achieves strong improvements over the\nstate-of-the-art vision-only baselines across multiple autonomous driving\ntasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),\nonline mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory\nplanning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal\nconsistency on multiple challenging benchmarks, including real-world open-loop\nnuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We\nshow the effectiveness of radar-based fusion in safety-critical scenarios where\naccurate motion understanding and long-horizon trajectory prediction are\nessential for collision avoidance. The source code of all experiments is\navailable at https://phi-wol.github.io/sparcad/", "AI": {"tldr": "本文提出了SpaRC-AD，一个基于查询的端到端相机-雷达融合框架，用于面向规划的自动驾驶，通过稀疏3D特征对齐和多普勒速度估计，显著提升了恶劣天气和遮挡下自动驾驶系统的感知、预测和规划性能。", "motivation": "端到端自动驾驶系统在统一优化感知、运动预测和规划方面具有潜力，但纯视觉方法在恶劣天气、部分遮挡和精确速度估计方面存在根本性局限，而这些是安全敏感场景中（如碰撞避免）准确运动理解和长时程轨迹预测的关键挑战。", "method": "本文提出了SpaRC-AD，一个基于查询的端到端相机-雷达融合框架。其核心方法包括稀疏3D特征对齐和基于多普勒的速度估计，以构建强大的3D场景表示，从而优化代理锚点、地图折线和运动建模。", "result": "SpaRC-AD在多项自动驾驶任务上显著优于现有纯视觉基线：3D检测（+4.8% mAP）、多目标跟踪（+8.3% AMOTA）、在线地图（+1.8% mAP）、运动预测（-4.0% mADE）和轨迹规划（-0.1m L2，-9% TPC）。在nuScenes、T-nuScenes和Bench2Drive等挑战性基准上实现了空间一致性和时间连贯性。", "conclusion": "研究表明，雷达融合在安全关键场景中非常有效，能够提供准确的运动理解和长时程轨迹预测，这对于避免碰撞至关重要。"}}
{"id": "2508.10016", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10016", "abs": "https://arxiv.org/abs/2508.10016", "authors": ["Tianyu Xie", "Yuhang Wu", "Yongdong Luo", "Jiayi Ji", "Xiawu Zheng"], "title": "Training-Free Multimodal Large Language Model Orchestration", "comment": null, "summary": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses.", "AI": {"tldr": "本文提出了一种名为多模态大语言模型编排（MLLM Orchestration）的方法，无需额外训练即可构建交互式多模态AI系统，通过LLM协调专业模型，实现自然的多模态交互、提高效率和可解释性。", "motivation": "现有不同的多模态大语言模型（MLLMs）无法直接集成到统一的多模态输入输出系统中，通常需要额外的训练来解决模态对齐、文本转语音效率及其他集成问题。", "method": "MLLM编排利用大语言模型的推理能力，通过显式工作流协调专业模型。核心创新包括：1) 一个中央控制器LLM，通过精心设计的代理动态路由任务；2) 一个并行文本转语音架构，实现全双工交互；3) 一个跨模态记忆集成系统，通过信息合成和检索保持上下文连贯性，并选择性避免不必要的模态调用以提高响应速度。", "result": "MLLM编排在不额外训练的情况下实现了全面的多模态能力，在标准基准测试上比传统联合训练方法性能提升高达7.8%，延迟降低10.3%，并通过显式编排过程显著增强了可解释性。", "conclusion": "MLLM编排是一种有效的方法，可以在不进行额外训练的情况下构建交互式多模态AI系统，同时提升性能、降低延迟并增强系统可解释性。"}}
{"id": "2508.10316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10316", "abs": "https://arxiv.org/abs/2508.10316", "authors": ["Yuanzhi Liang", "Yijie Fang", "Rui Li", "Ziqi Ni", "Ruijie Su", "Chi Zhang", "Xuelong Li"], "title": "Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances", "comment": "Ongoing work", "summary": "Generative models have made significant progress in synthesizing visual\ncontent, including images, videos, and 3D/4D structures. However, they are\ntypically trained with surrogate objectives such as likelihood or\nreconstruction loss, which often misalign with perceptual quality, semantic\naccuracy, or physical realism. Reinforcement learning (RL) offers a principled\nframework for optimizing non-differentiable, preference-driven, and temporally\nstructured objectives. Recent advances demonstrate its effectiveness in\nenhancing controllability, consistency, and human alignment across generative\ntasks. This survey provides a systematic overview of RL-based methods for\nvisual content generation. We review the evolution of RL from classical control\nto its role as a general-purpose optimization tool, and examine its integration\ninto image, video, and 3D/4D generation. Across these domains, RL serves not\nonly as a fine-tuning mechanism but also as a structural component for aligning\ngeneration with complex, high-level goals. We conclude with open challenges and\nfuture research directions at the intersection of RL and generative modeling.", "AI": {"tldr": "该综述系统概述了强化学习（RL）在视觉内容生成（如图像、视频、3D/4D）中的应用，旨在解决传统生成模型目标与感知质量不匹配的问题。", "motivation": "传统的生成模型通常通过代理目标（如似然或重建损失）进行训练，这些目标往往与感知质量、语义准确性或物理真实性不符。强化学习提供了一个原则性框架，用于优化不可微分、偏好驱动和时间结构化的目标。", "method": "本文系统回顾了基于RL的视觉内容生成方法。它审视了RL从经典控制到通用优化工具的演变，并探讨了其在图像、视频和3D/4D生成中的整合。RL不仅作为微调机制，还作为结构性组件，用于将生成与复杂的高级目标对齐。", "result": "强化学习在生成任务中有效增强了可控性、一致性和人类对齐性。它作为一个通用优化工具，以及一个结构性组件，帮助生成模型实现与复杂、高级目标的对齐。", "conclusion": "强化学习在视觉内容生成领域中扮演着重要角色，能够将生成过程与复杂的高级目标对齐。未来研究将聚焦于RL与生成建模交叉领域的开放挑战和方向。"}}
{"id": "2508.10492", "categories": ["cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10492", "abs": "https://arxiv.org/abs/2508.10492", "authors": ["Shicheng Xu", "Xin Huang", "Zihao Wei", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model", "comment": "39 pages", "summary": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.", "AI": {"tldr": "本文提出DxDirector-7B，一个将AI从医生助手转变为诊断主导者的LLM，旨在驱动全流程诊断，显著提高诊断准确性并减轻医生工作量。", "motivation": "现有AI（尤其是LLM）在临床诊断中主要作为医生助手，只能回答特定问题，无法从模糊主诉开始驱动整个诊断流程，这限制了AI完全减轻医生工作量和提高诊断效率的能力。", "method": "提出一种范式转变，将AI重新定位为诊断过程的主要驱动者，医生作为辅助。开发了DxDirector-7B，一个具备深度思考能力的LLM，使其能够以最少医生参与驱动全流程诊断。同时，DxDirector-7B建立了一个针对误诊的责任框架，明确AI和医生之间的责任。", "result": "在罕见、复杂和真实世界病例的全流程诊断评估中，DxDirector-7B不仅实现了显著优越的诊断准确性，而且比现有最先进的医学LLM和通用LLM大幅减少了医生工作量。跨多个临床科室和任务的细致分析验证了其有效性，专家评估表明其有潜力替代医学专家。", "conclusion": "DxDirector-7B标志着一个新时代，AI从传统的医生助手转变为驱动整个诊断过程的主导者，从而大大减轻医生工作量，提供了一个高效准确的诊断解决方案。"}}
{"id": "2508.10747", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10747", "abs": "https://arxiv.org/abs/2508.10747", "authors": ["Sangwoo Jeon", "Juchul Shin", "Gyeong-Tae Kim", "YeonJe Cho", "Seongwoo Kim"], "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning", "comment": "16 pages, 10 figures", "summary": "Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.", "AI": {"tldr": "针对基于GNN的广义规划在大型网格环境中面临的稠密图表示导致的可伸缩性问题，本文提出了一种稀疏、目标感知的GNN表示，有效解决了大规模规划任务并提高了泛化能力。", "motivation": "现有结合深度强化学习（RL）和图神经网络（GNN）的广义规划方法通常将规划状态表示为全连接图，这导致了边信息的组合爆炸和显著稀疏性，尤其在大型网格环境中。这种稠密表示稀释了节点信息，指数级增加了内存需求，并最终使得大规模问题的学习变得不可行。", "method": "提出了一种稀疏的、目标感知的GNN表示，该表示选择性地编码相关的局部关系，并明确整合与目标相关的空间特征。通过设计基于PDDL的无人机任务场景，在网格世界中进行了验证。", "result": "实验结果表明，该方法能够有效扩展到以前使用稠密图表示无法处理的更大网格尺寸，并显著提高了策略的泛化能力和成功率。", "conclusion": "本研究为解决现实世界中的大规模广义规划任务提供了实用的基础。"}}
{"id": "2508.10018", "categories": ["cs.CL", "cs.AI", "math.AT"], "pdf": "https://arxiv.org/pdf/2508.10018", "abs": "https://arxiv.org/abs/2508.10018", "authors": ["Sridhar Mahadevan"], "title": "A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models", "comment": "26 pages. arXiv admin note: text overlap with arXiv:2402.18732", "summary": "Natural language is replete with superficially different statements, such as\n``Charles Darwin wrote\" and ``Charles Darwin is the author of\", which carry the\nsame meaning. Large language models (LLMs) should generate the same next-token\nprobabilities in such cases, but usually do not. Empirical workarounds have\nbeen explored, such as using k-NN estimates of sentence similarity to produce\nsmoothed estimates. In this paper, we tackle this problem more abstractly,\nintroducing a categorical homotopy framework for LLMs. We introduce an LLM\nMarkov category to represent probability distributions in language generated by\nan LLM, where the probability of a sentence, such as ``Charles Darwin wrote\" is\ndefined by an arrow in a Markov category. However, this approach runs into\ndifficulties as language is full of equivalent rephrases, and each generates a\nnon-isomorphic arrow in the LLM Markov category. To address this fundamental\nproblem, we use categorical homotopy techniques to capture ``weak equivalences\"\nin an LLM Markov category. We present a detailed overview of application of\ncategorical homotopy to LLMs, from higher algebraic K-theory to model\ncategories, building on powerful theoretical results developed over the past\nhalf a century.", "AI": {"tldr": "针对LLM对语义等价但表述不同的语句赋予不同概率的问题，本文提出了一个范畴同伦框架，利用LLM马尔可夫范畴和范畴同伦技术来捕捉语言中的“弱等价性”。", "motivation": "大型语言模型（LLM）未能对语义相同但表述不同的语句（如“Charles Darwin wrote”和“Charles Darwin is the author of”）生成相同的下一词概率，现有解决方案多为经验性而非理论性。", "method": "引入LLM马尔可夫范畴来表示LLM生成的语言概率分布，其中语句概率由范畴中的箭头定义。为解决等价改写导致非同构箭头的问题，使用范畴同伦技术（包括高代数K理论和模型范畴）来捕捉LLM马尔可夫范畴中的“弱等价性”。", "result": "提出了一个将范畴同伦应用于LLM的详细概述，旨在理论上解决LLM对语义等价语句概率不一致的问题，超越了现有的经验性方法。", "conclusion": "本文详细阐述了如何将范畴同伦（借鉴过去半个世纪的理论成果，如高代数K理论和模型范畴）应用于LLM，为解决LLM处理语言等价性问题提供了一个新的理论框架。"}}
{"id": "2508.10339", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10339", "abs": "https://arxiv.org/abs/2508.10339", "authors": ["Andrew Bai", "Justin Cui", "Ruochen Wang", "Cho-Jui Hsieh"], "title": "Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models", "comment": "11 pages, 1 figure", "summary": "Vision-language instruction tuning achieves two main purposes: learning\nvisual concepts and learning visual skills. In this paper, we found that\nvision-language benchmarks fall into the dichotomy of mainly benefiting from\ntraining on instructions with similar skills or visual concepts. Inspired by\nthe discovery, we designed a simple targeted training data selection method to\noptimize the performance of a given benchmark. We first extract the\nconcepts/skills from the benchmark, determine whether the benchmark\npredominantly benefits from similar concepts or skills, and finally select\ninstructions with the most matching concepts/skills. Experiments on 10+\nbenchmarks validate the effectiveness of our targeted data selection method,\nshowing +0.9\\% over the best existing baseline averaged over all benchmarks and\n+1.5\\% on the skill-focused subset. Our findings underscore the importance of\nrecognizing the inherent trade-off within instruction selection, which requires\nbalancing the acquisition of conceptual knowledge against visual skill.", "AI": {"tldr": "本文提出一种针对视觉语言指令微调的靶向数据选择方法，通过识别基准测试是侧重概念还是技能，选择最匹配的指令数据，从而提高模型性能。", "motivation": "研究发现，视觉语言基准测试在训练时，要么主要受益于相似技能的指令，要么主要受益于相似视觉概念的指令。这促使作者设计一种方法来优化给定基准测试的性能。", "method": "该方法首先从基准测试中提取概念/技能，然后判断该基准测试主要受益于相似概念还是相似技能，最后选择具有最匹配概念/技能的指令进行训练。", "result": "在10多个基准测试上的实验验证了所提出靶向数据选择方法的有效性，与现有最佳基线相比，所有基准测试的平均性能提升了+0.9%，在侧重技能的子集上提升了+1.5%。", "conclusion": "研究结果强调了指令选择中权衡概念知识获取与视觉技能习得的重要性，需要平衡两者以优化模型表现。"}}
{"id": "2508.10501", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10501", "abs": "https://arxiv.org/abs/2508.10501", "authors": ["Yushi Feng", "Junye Du", "Yingying Hong", "Qifan Wang", "Lequan Yu"], "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning", "comment": null, "summary": "Existing tool-augmented agentic systems are limited in the real world by (i)\nblack-box reasoning steps that undermine trust of decision-making and pose\nsafety risks, (ii) poor multimodal integration, which is inherently critical\nfor healthcare tasks, and (iii) rigid and computationally inefficient agentic\npipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the\nfirst multimodal framework to address these challenges in the context of Chest\nX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a\nmulti-tool graph, yielding decision paths annotated with interpretable\nprobabilities. Given the complex CXR reasoning task with multimodal medical\ndata, PASS leverages its learned task-conditioned distribution over the agentic\nsupernet. Thus, it adaptively selects the most suitable tool at each supernet\nlayer, offering probability-annotated trajectories for post-hoc audits and\ndirectly enhancing medical AI safety. PASS also continuously compresses salient\nfindings into an evolving personalized memory, while dynamically deciding\nwhether to deepen its reasoning path or invoke an early exit for efficiency. To\noptimize a Pareto frontier balancing performance and cost, we design a novel\nthree-stage training procedure, including expert knowledge warm-up, contrastive\npath-ranking, and cost-aware reinforcement learning. To facilitate rigorous\nevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,\nsafety-critical, free-form CXR reasoning. Experiments across various benchmarks\nvalidate that PASS significantly outperforms strong baselines in multiple\nmetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,\npushing a new paradigm shift towards interpretable, adaptive, and multimodal\nmedical agentic systems.", "AI": {"tldr": "PASS是一种新型多模态概率智能体系统，通过在多工具图上自适应采样工作流，为胸部X光（CXR）推理提供可解释的、具有概率注释的决策路径，同时平衡性能与计算成本，提升医疗AI的安全性和效率。", "motivation": "现有工具增强型智能体系统存在三大局限性：一是决策过程不透明，缺乏信任并存在安全风险；二是多模态集成能力差，这在医疗健康任务中至关重要；三是智能体管道僵化且计算效率低下。", "method": "本文提出了PASS（Probabilistic Agentic Supernet Sampling），一个多模态框架，用于CXR推理。PASS通过在多工具图上自适应采样智能体工作流，生成带有可解释概率注释的决策路径。它利用学习到的任务条件分布，在每个超网络层自适应选择最合适的工具，提供可审计的轨迹。PASS还持续将重要发现压缩到动态记忆中，并动态决定是深化推理路径还是提前退出以提高效率。为优化性能与成本的帕累托前沿，设计了三阶段训练过程：专家知识预热、对比路径排序和成本感知强化学习。同时，引入了CAB-E基准用于严格评估。", "result": "在各种基准测试中，PASS显著优于现有强基线，在多项指标（如准确率、AUC、LLM-J）上表现出色，同时有效平衡了计算成本。这推动了可解释、自适应和多模态医疗智能体系统的新范式转变。", "conclusion": "PASS是首个解决现有智能体系统在医疗领域中可解释性、多模态集成和效率挑战的多模态框架。它通过自适应采样和概率注释的决策路径，显著提升了医疗AI的安全性、可信度和性能，为未来的医疗智能体系统树立了新标准。"}}
{"id": "2508.10019", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10019", "abs": "https://arxiv.org/abs/2508.10019", "authors": ["Li Wang", "Changhao Zhang", "Zengqi Xiu", "Kai Lu", "Xin Yu", "Kui Zhang", "Wenjun Wu"], "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning", "comment": null, "summary": "Despite recent advances in the reasoning capabilities of Large Language\nModels (LLMs), improving the reasoning ability of Small Language Models (SLMs,\ne.g., $\\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity\nand variability of natural language: essentially equivalent problems often\nappear in diverse surface forms, often obscured by redundant or distracting\ndetails. This imposes a dual burden on SLMs: they must first extract the core\nproblem from complex linguistic input, and then perform reasoning based on that\nunderstanding. The resulting vast and noisy problem space hinders optimization,\nparticularly for models with limited capacity. To address this, we propose a\nnew framework that decouples understanding from reasoning by mapping natural\nlanguage problems into a canonical problem space-a semantically simplified yet\nexpressive domain. This enables SLMs to focus on reasoning over standardized\ninputs, free from linguistic variability. Within this framework, we introduce\nDURIT (Decoupled Understanding from Reasoning via Iterative Training), a\nthree-step algorithm that iteratively: (1) mapping natural language problems\nvia reinforcement learning, (2) aligns reasoning trajectories through\nself-distillation, and (3) trains reasoning policies in the problem space. The\nmapper and reasoner are co-trained in an alternating loop throughout this\nprocess. Experiments show that DURIT substantially improves SLMs' performance\non both in-domain and out-of-domain mathematical and logical reasoning tasks.\nBeyond improving reasoning capabilities, DURIT also improves the robustness of\nreasoning, validating decoupling understanding from reasoning as an effective\nstrategy for strengthening SLMs.", "AI": {"tldr": "该研究提出一种新框架DURIT，通过将自然语言问题映射到规范问题空间，解耦理解与推理，显著提升小型语言模型（SLMs）的数学和逻辑推理能力及鲁棒性。", "motivation": "小型语言模型（SLMs）在推理能力提升上面临挑战，主要原因是自然语言的复杂性和多变性（冗余、分散信息），这使得SLMs既要提取核心问题又要进行推理，庞大且嘈杂的问题空间阻碍了优化。", "method": "提出一个将自然语言问题映射到规范问题空间的新框架，解耦理解与推理。引入DURIT（Decoupled Understanding from Reasoning via Iterative Training）算法，包含三个迭代步骤：1) 通过强化学习映射自然语言问题；2) 通过自蒸馏对齐推理轨迹；3) 在问题空间中训练推理策略。映射器和推理器在此过程中交替共同训练。", "result": "实验表明，DURIT显著提高了SLMs在域内和域外数学及逻辑推理任务上的性能。除了提升推理能力，DURIT还增强了推理的鲁棒性。", "conclusion": "将理解与推理解耦是增强小型语言模型（SLMs）的有效策略。"}}
{"id": "2508.10351", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10351", "abs": "https://arxiv.org/abs/2508.10351", "authors": ["Zhentai Zhang", "Danyi Weng", "Guibin Zhang", "Xiang Chen", "Kaixing Long", "Jian Geng", "Yanmeng Lu", "Lei Zhang", "Zhitao Zhou", "Lei Cao"], "title": "Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images", "comment": "15 pages, 6 figures", "summary": "Complex and diverse ultrastructural features can indicate the type,\nprogression, and prognosis of kidney diseases. Recently, computational\npathology combined with deep learning methods has shown tremendous potential in\nadvancing automatic morphological analysis of glomerular ultrastructure.\nHowever, current research predominantly focuses on the recognition of\nindividual ultrastructure, which makes it challenging to meet practical\ndiagnostic needs. In this study, we propose the glomerular morphometry\nframework of ultrastructural characterization (Glo-DMU), which is grounded on\nthree deep models: the ultrastructure segmentation model, the glomerular\nfiltration barrier region classification model, and the electron-dense deposits\ndetection model. Following the conventional protocol of renal biopsy diagnosis,\nthis framework simultaneously quantifies the three most widely used\nultrastructural features: the thickness of glomerular basement membrane, the\ndegree of foot process effacement, and the location of electron-dense deposits.\nWe evaluated the 115 patients with 9 renal pathological types in real-world\ndiagnostic scenarios, demonstrating good consistency between automatic\nquantification results and morphological descriptions in the pathological\nreports. Glo-DMU possesses the characteristics of full automation, high\nprecision, and high throughput, quantifying multiple ultrastructural features\nsimultaneously, and providing an efficient tool for assisting renal\npathologists.", "AI": {"tldr": "本研究提出了Glo-DMU框架，一个基于深度学习的肾小球超微结构形态测量框架，能够全自动、高精度地同时量化多种超微结构特征，以辅助肾脏病理诊断。", "motivation": "肾小球超微结构特征对肾脏疾病的类型、进展和预后具有重要指示作用。现有计算病理学方法多聚焦于单个超微结构的识别，难以满足实际诊断需求。", "method": "提出了Glo-DMU框架，包含三个深度模型：超微结构分割模型、肾小球滤过屏障区域分类模型和电子致密物沉积检测模型。该框架同时量化肾小球基底膜厚度、足细胞足突融合程度和电子致密物沉积位置这三种最常用的超微结构特征。", "result": "在115名患者（涵盖9种肾脏病理类型）的真实世界诊断场景中进行了评估，结果显示其自动量化结果与病理报告中的形态学描述具有良好的一致性。", "conclusion": "Glo-DMU框架具有全自动化、高精度、高通量的特点，能够同时量化多种超微结构特征，为肾脏病理学家提供了一个高效的辅助诊断工具。"}}
{"id": "2508.10530", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10530", "abs": "https://arxiv.org/abs/2508.10530", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu"], "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment", "comment": null, "summary": "The alignment of language models (LMs) with human preferences is critical for\nbuilding reliable AI systems. The problem is typically framed as optimizing an\nLM policy to maximize the expected reward that reflects human preferences.\nRecently, Direct Preference Optimization (DPO) was proposed as a LM alignment\nmethod that directly optimize the policy from static preference data, and\nfurther improved by incorporating on-policy sampling (i.e., preference\ncandidates generated during the training loop) for better LM alignment.\nHowever, we show on-policy data is not always optimal, with systematic\neffectiveness difference emerging between static and on-policy preference\ncandidates. For example, on-policy data can result in a 3$\\times$ effectiveness\ncompared with static data for Llama-3, and a 0.4$\\times$ effectiveness for\nZephyr. To explain the phenomenon, we propose the alignment stage assumption,\nwhich divides the alignment process into two distinct stages: the preference\ninjection stage, which benefits from diverse data, and the preference\nfine-tuning stage, which favors high-quality data. Through theoretical and\nempirical analysis, we characterize these stages and propose an effective\nalgorithm to identify the boundaries between them. We perform experiments on 5\nmodels (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,\nSLiC-HF) to show the generalizability of alignment stage assumption and\nboundary measurement.", "AI": {"tldr": "本文研究了语言模型对齐中静态数据和在策略数据有效性的差异，并提出了“对齐阶段假设”来解释该现象，将对齐过程分为偏好注入和偏好微调两个阶段，并提出了识别阶段边界的方法。", "motivation": "语言模型与人类偏好对齐对构建可靠的AI系统至关重要。DPO等方法利用静态和在策略数据进行对齐，但研究发现这两种数据源的有效性存在系统性差异（例如，对Llama-3在策略数据有效性是静态数据的3倍，对Zephyr是0.4倍），需要解释和优化。", "method": "本文提出了“对齐阶段假设”，将对齐过程分为两个阶段：偏好注入阶段（受益于多样化数据）和偏好微调阶段（偏好高质量数据）。通过理论和实证分析，本文刻画了这些阶段，并提出了一种有效算法来识别它们之间的边界。实验在5个模型（Llama、Zephyr、Phi-2、Qwen、Pythia）和2种对齐方法（DPO、SLiC-HF）上进行，以验证假设的普适性。", "result": "研究表明，在策略数据并非总是最优的，其有效性与静态数据相比存在显著差异。提出的对齐阶段假设能够解释这一现象：偏好注入阶段需要多样性数据，而偏好微调阶段需要高质量数据。通过实验，本文成功识别了不同对齐阶段之间的边界，并证明了该假设和边界测量方法的通用性。", "conclusion": "语言模型对齐过程可以被划分为偏好注入和偏好微调两个截然不同的阶段，每个阶段对数据类型（多样性 vs. 高质量）有不同的偏好。理解并识别这些阶段有助于优化对齐算法，从而更有效地利用数据，提升语言模型的对齐效果。"}}
{"id": "2508.10020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10020", "abs": "https://arxiv.org/abs/2508.10020", "authors": ["Chuan Li", "Qianyi Zhao", "Fengran Mo", "Cen Chen"], "title": "FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models", "comment": null, "summary": "Efficiently enhancing the reasoning capabilities of large language models\n(LLMs) in federated learning environments remains challenging, particularly\nwhen balancing performance gains with strict computational, communication, and\nprivacy constraints. This challenge is especially acute in healthcare, where\ndecisions-spanning clinical, operational, and patient-facing contexts-demand\nnot only accurate outputs but also interpretable, traceable rationales to\nensure safety, accountability, and regulatory compliance. Conventional\nfederated tuning approaches on LLM fail to address this need: they optimize\nprimarily for answer correctness while neglecting rationale quality, leaving\nCoT capabilities dependent on models' innate pre-training abilities. Moreover,\nexisting methods for improving rationales typically rely on privacy-violating\nknowledge distillation from centralized models. Additionally, the communication\noverhead in traditional federated fine-tuning on LLMs remains substantial. We\naddresses this gap by proposing FedCoT, a novel framework specifically designed\nto enhance reasoning in federated settings. FedCoT leverages a lightweight\nchain-of-thought enhancement mechanism: local models generate multiple\nreasoning paths, and a compact discriminator dynamically selects the most\npromising one. This approach improves reasoning accuracy and robustness while\nproviding valuable interpretability, which is particularly critical for medical\napplications. To manage client heterogeneity efficiently, we adopt an improved\naggregation approach building upon advanced LoRA module stacking, incorporating\nclient classifier-awareness to achieve noise-free aggregation across diverse\nclients. Comprehensive experiments on medical reasoning tasks demonstrate that\nFedCoT significantly boosts client-side reasoning performance under stringent\nresource budgets while fully preserving data privacy.", "AI": {"tldr": "FedCoT是一个新颖的联邦学习框架，通过轻量级思维链（CoT）增强机制和改进的LoRA模块聚合，显著提升大型语言模型（LLMs）在医疗等受限环境下的推理能力和可解释性，同时保护数据隐私。", "motivation": "在联邦学习环境中提升LLM推理能力面临计算、通信和隐私限制，尤其在医疗领域，不仅需要准确结果，还需要可解释、可追溯的推理过程。现有联邦微调方法侧重答案正确性而忽视推理质量，且通常依赖侵犯隐私的知识蒸馏或产生高通信开销。", "method": "FedCoT框架包含两个核心机制：1. 轻量级思维链增强：本地模型生成多条推理路径，紧凑的判别器动态选择最佳路径以提升推理准确性和鲁棒性；2. 改进的聚合方法：基于高级LoRA模块堆叠，并融入客户端分类器感知，实现跨异构客户端的无噪声聚合，高效管理客户端异质性并降低通信开销。", "result": "在医疗推理任务上的综合实验表明，FedCoT在严格的资源预算下显著提升了客户端的推理性能，同时完全保留了数据隐私。", "conclusion": "FedCoT成功解决了联邦学习中LLM推理能力提升的挑战，特别是在医疗应用中，通过优化推理质量、效率和隐私保护，为受限环境下的LLM部署提供了有效方案。"}}
{"id": "2508.10356", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10356", "abs": "https://arxiv.org/abs/2508.10356", "authors": ["Hylke Westerdijk", "Ben Blankenborg", "Khondoker Ittehadul Islam"], "title": "Improving OCR for Historical Texts of Multiple Languages", "comment": null, "summary": "This paper presents our methodology and findings from three tasks across\nOptical Character Recognition (OCR) and Document Layout Analysis using advanced\ndeep learning techniques. First, for the historical Hebrew fragments of the\nDead Sea Scrolls, we enhanced our dataset through extensive data augmentation\nand employed the Kraken and TrOCR models to improve character recognition. In\nour analysis of 16th to 18th-century meeting resolutions task, we utilized a\nConvolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for\nsemantic segmentation with a Bidirectional LSTM, incorporating confidence-based\npseudolabeling to refine our model. Finally, for modern English handwriting\nrecognition task, we applied a CRNN with a ResNet34 encoder, trained using the\nConnectionist Temporal Classification (CTC) loss function to effectively\ncapture sequential dependencies. This report offers valuable insights and\nsuggests potential directions for future research.", "AI": {"tldr": "本文介绍了在光学字符识别（OCR）和文档版面分析领域，针对历史希伯来语碎片、16-18世纪会议决议及现代英语手写识别三项任务，采用先进深度学习技术的应用方法与发现。", "motivation": "研究旨在通过深度学习技术提升对各种挑战性文本（包括历史文献和现代手写体）的字符识别和文档版面分析能力。", "method": "针对死海古卷希伯来语碎片，使用数据增强并结合Kraken和TrOCR模型。对于16-18世纪会议决议，采用集成DeepLabV3+进行语义分割的CRNN（卷积循环神经网络）与双向LSTM，并辅以基于置信度的伪标签。针对现代英语手写识别，应用带有ResNet34编码器的CRNN，并使用连接时序分类（CTC）损失函数进行训练。", "result": "通过所提出的方法，成功提升了希伯来语字符识别性能，优化了会议决议分析模型，并有效地捕捉了英语手写识别中的序列依赖性。报告提供了有价值的见解。", "conclusion": "该研究提供了在多任务OCR和文档分析中应用深度学习的宝贵见解，并为未来的研究指明了潜在方向。"}}
{"id": "2508.10539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10539", "abs": "https://arxiv.org/abs/2508.10539", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu", "Min Zhang"], "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment.", "AI": {"tldr": "本文提出ComMCS方法，通过线性组合当前和后续步骤的蒙特卡洛估计器，在不增加LLM推理成本的情况下，有效降低了基于价值的过程验证器中的估计误差方差，从而提升LLM在数学推理任务上的表现。", "motivation": "大型语言模型（LLM）在数学等复杂领域的推理能力仍面临挑战。基于价值的过程验证器是提高推理能力的有效方法，但其训练标注因LLM推理成本高昂导致蒙特卡洛（MC）样本有限，从而产生高方差的估计误差，影响其有效性。", "method": "作者识别出估计误差主要来源于高方差而非偏差，且MC估计器是最小方差无偏估计器（MVUE）。为解决此问题，他们提出了复合蒙特卡洛采样（ComMCS）方法，通过线性组合当前和后续步骤的MC估计器来构建一个无偏估计器。", "result": "理论上，ComMCS能在不增加额外LLM推理成本的情况下，实现可预测的方差降低并保持无偏估计。在MATH-500和GSM8K基准测试上的实证结果表明，ComMCS在MATH-500的Best-of-32采样实验中，比基于回归的优化方法高出2.8点，比未进行方差缩减的基线高出2.2点。", "conclusion": "ComMCS方法通过有效降低蒙特卡洛估计的方差，显著提升了基于价值的过程验证器在LLM数学推理任务上的性能，且无需额外计算成本，为改进LLM的复杂推理能力提供了一条有效途径。"}}
{"id": "2508.10021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10021", "abs": "https://arxiv.org/abs/2508.10021", "authors": ["Egor Fadeev", "Dzhambulat Mollaev", "Aleksei Shestov", "Dima Korolev", "Omar Zoloev", "Ivan Kireev", "Andrey Savchenko", "Maksim Makarenko"], "title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients", "comment": null, "summary": "Learning clients embeddings from sequences of their historic communications\nis central to financial applications. While large language models (LLMs) offer\ngeneral world knowledge, their direct use on long event sequences is\ncomputationally expensive and impractical in real-world pipelines. In this\npaper, we propose LATTE, a contrastive learning framework that aligns raw event\nembeddings with semantic embeddings from frozen LLMs. Behavioral features are\nsummarized into short prompts, embedded by the LLM, and used as supervision via\ncontrastive loss. The proposed approach significantly reduces inference cost\nand input size compared to conventional processing of complete sequence by LLM.\nWe experimentally show that our method outperforms state-of-the-art techniques\nfor learning event sequence representations on real-world financial datasets\nwhile remaining deployable in latency-sensitive environments.", "AI": {"tldr": "LATTE是一个对比学习框架，它通过将行为特征总结为短提示并由冻结的LLM嵌入作为监督，来学习客户历史通信序列的嵌入，从而显著降低了推理成本和输入大小，并在金融数据集上超越了现有技术。", "motivation": "金融应用中，从客户历史通信序列中学习客户嵌入至关重要。尽管大型语言模型（LLMs）提供了通用世界知识，但其直接用于长事件序列计算成本高昂且在实际管道中不切实际。", "method": "本文提出了LATTE，一个对比学习框架，旨在对齐原始事件嵌入与来自冻结LLM的语义嵌入。行为特征被总结成短提示，由LLM嵌入，并通过对比损失用作监督。该方法与传统LLM处理完整序列相比，显著降低了推理成本和输入大小。", "result": "与传统方法相比，LATTE显著降低了推理成本和输入大小。实验表明，该方法在真实世界金融数据集上，在学习事件序列表示方面优于现有最先进技术，同时能够部署在对延迟敏感的环境中。", "conclusion": "LATTE提供了一种有效且高效的方法，通过利用冻结LLM进行语义监督，解决了直接使用LLM处理长序列的计算和实际限制，从而在金融领域学习客户事件序列表示方面取得了优异性能。"}}
{"id": "2508.10359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10359", "abs": "https://arxiv.org/abs/2508.10359", "authors": ["Hao Wang", "Hongkui Zheng", "Kai He", "Abolfazl Razi"], "title": "AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging", "comment": null, "summary": "Scanning transmission electron microscopy (STEM) plays a critical role in\nmodern materials science, enabling direct imaging of atomic structures and\ntheir evolution under external interferences. However, interpreting\ntime-resolved STEM data remains challenging due to two entangled degradation\neffects: spatial drift caused by mechanical and thermal instabilities, and\nbeam-induced signal loss resulting from radiation damage. These factors distort\nboth geometry and intensity in complex, temporally correlated ways, making it\ndifficult for existing methods to explicitly separate their effects or model\nmaterial dynamics at atomic resolution. In this work, we present AtomDiffuser,\na time-aware degradation modeling framework that disentangles sample drift and\nradiometric attenuation by predicting an affine transformation and a spatially\nvarying decay map between any two STEM frames. Unlike traditional denoising or\nregistration pipelines, our method leverages degradation as a physically\nheuristic, temporally conditioned process, enabling interpretable structural\nevolutions across time. Trained on synthetic degradation processes,\nAtomDiffuser also generalizes well to real-world cryo-STEM data. It further\nsupports high-resolution degradation inference and drift alignment, offering\ntools for visualizing and quantifying degradation patterns that correlate with\nradiation-induced atomic instabilities.", "AI": {"tldr": "AtomDiffuser是一个时间感知的退化建模框架，能够有效分离时间分辨STEM数据中的空间漂移和辐射损伤，从而实现对原子结构演变的精确分析。", "motivation": "时间分辨扫描透射电子显微镜（STEM）在材料科学中至关重要，但其数据解释面临挑战。主要问题是空间漂移（由机械和热不稳定性引起）和束流引起的信号损失（由辐射损伤引起）这两种退化效应相互纠缠，导致数据几何和强度复杂失真，使得现有方法难以明确分离其影响或在原子分辨率下建模材料动态。", "method": "本文提出了AtomDiffuser，一个时间感知的退化建模框架。该方法通过预测任意两个STEM帧之间的仿射变换和空间变化的衰减图，来解耦样品漂移和辐射衰减。与传统去噪或配准方法不同，AtomDiffuser将退化视为一个物理启发式、时间条件的过程。", "result": "AtomDiffuser在合成退化过程上训练后，能够很好地泛化到真实的低温STEM数据。它支持高分辨率的退化推断和漂移校准，并提供了可视化和量化与辐射诱导原子不稳定性相关的退化模式的工具。", "conclusion": "AtomDiffuser框架能够有效分离并建模STEM数据中的复杂退化效应，从而实现对原子结构随时间演变的可解释分析，为理解辐射诱导的原子不稳定性提供了新的工具和见解。"}}
{"id": "2508.10599", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10599", "abs": "https://arxiv.org/abs/2508.10599", "authors": ["Xinyan Jiang", "Lin Zhang", "Jiayi Zhang", "Qingsong Yang", "Guimin Hu", "Di Wang", "Lijie Hu"], "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models", "comment": null, "summary": "Activation steering offers a promising approach to controlling the behavior\nof Large Language Models by directly manipulating their internal activations.\nHowever, most existing methods struggle to jointly steer multiple attributes,\noften resulting in interference and undesirable trade-offs. To address this\nchallenge, we propose Multi-Subspace Representation Steering (MSRS), a novel\nframework for effective multi-attribute steering via subspace representation\nfine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal\nsubspaces to each attribute, isolating their influence within the model's\nrepresentation space. MSRS also incorporates a hybrid subspace composition\nstrategy: it combines attribute-specific subspaces for unique steering\ndirections with a shared subspace for common steering directions. A dynamic\nweighting function learns to efficiently integrate these components for precise\ncontrol. During inference, MSRS introduces a token-level steering mechanism\nthat dynamically identifies and intervenes on the most semantically relevant\ntokens, enabling fine-grained behavioral modulation. Experimental results show\nthat MSRS significantly reduces attribute conflicts, surpasses existing methods\nacross a range of attributes, and generalizes effectively to diverse downstream\ntasks.", "AI": {"tldr": "MSRS是一种通过子空间表示微调实现多属性LLM行为控制的新框架，它通过正交子空间减少属性间干扰，并结合共享与特定子空间进行动态权重调整，实现细粒度、高效的控制。", "motivation": "现有的大语言模型激活引导方法难以同时引导多个属性，常导致属性间相互干扰和不良权衡。", "method": "提出多子空间表示引导（MSRS）框架。通过为每个属性分配正交子空间来隔离其影响，减少属性间干扰；采用混合子空间组合策略，结合属性特定子空间和共享子空间；引入动态加权函数学习有效整合这些组件；在推理时，通过令牌级引导机制动态识别并干预语义相关的令牌。", "result": "实验结果表明，MSRS显著减少了属性冲突，在各种属性上超越了现有方法，并能有效泛化到多样化的下游任务。", "conclusion": "MSRS是一种用于有效多属性LLM引导的新颖框架，通过其独特的子空间表示和令牌级干预机制，显著提升了LLM行为控制的精度和效率。"}}
{"id": "2508.10022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10022", "abs": "https://arxiv.org/abs/2508.10022", "authors": ["Yuanchang Ye"], "title": "Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control", "comment": null, "summary": "This study introduces a significance testing-enhanced conformal prediction\n(CP) framework to improve trustworthiness of large language models (LLMs) in\nmultiple-choice question answering (MCQA). While LLMs have been increasingly\ndeployed in disciplinary QA scenarios, hallucination and nonfactual generation\nsubstantially compromise response reliability. Although CP provides\nstatistically rigorous marginal coverage guarantees for prediction sets, and\nsignificance testing offers established statistical rigor, their synergistic\nintegration remains unexplored. To mitigate hallucination and factual\ninaccuracies, our framework integrates $p$-value computation with conformity\nscoring through self-consistency resampling of MCQA responses. This approach\ncalculates option frequencies to address LLMs' black-box nature, subsequently\nconstructing prediction sets via null hypothesis testing ($\\mathcal{H}_0$) with\nempirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks\nusing off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves\nuser-specified empirical miscoverage rates; (2) Test-set average prediction set\nsize (APSS) decreases monotonically with increasing risk levels ($\\alpha$),\nvalidating APSS as an effective uncertainty metric. This work establishes a\nprincipled statistical framework for trustworthy LLM deployment in high-stakes\nQA applications.", "AI": {"tldr": "本研究提出了一种结合显著性检验的共形预测（CP）框架，以提高大型语言模型（LLMs）在多项选择问答（MCQA）中的可信度，通过计算选项频率和基于p值的假设检验来构建预测集。", "motivation": "LLMs在专业问答场景中部署日益增多，但幻觉和非事实性生成严重损害了响应的可靠性。尽管CP提供了严格的边际覆盖保证，显著性检验也提供了统计严谨性，但两者尚未有效整合以解决LLM的不可靠性问题。", "method": "该框架通过MCQA响应的自洽重采样，将p值计算与一致性评分相结合。它计算选项频率以解决LLMs的黑盒性质，然后通过零假设检验（使用经验导出的p值）构建预测集。", "result": "在MMLU和MMLU-Pro基准测试中，结果表明：1) 增强的CP实现了用户指定的经验误覆盖率；2) 随着风险水平（α）的增加，测试集平均预测集大小（APSS）单调递减，验证了APSS作为有效不确定性度量的作用。", "conclusion": "这项工作为在高风险问答应用中部署可信赖的LLM建立了一个有原则的统计框架。"}}
{"id": "2508.10367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10367", "abs": "https://arxiv.org/abs/2508.10367", "authors": ["Pablo Hernández-Cámara", "Alexandra Gomez-Villa", "Jose Manuel Jaén-Lorites", "Jorge Vila-Tomás", "Jesus Malo", "Valero Laparra"], "title": "Contrast Sensitivity Function of Multimodal Vision-Language Models", "comment": null, "summary": "Assessing the alignment of multimodal vision-language models~(VLMs) with\nhuman perception is essential to understand how they perceive low-level visual\nfeatures. A key characteristic of human vision is the contrast sensitivity\nfunction (CSF), which describes sensitivity to spatial frequency at\nlow-contrasts. Here, we introduce a novel behavioral psychophysics-inspired\nmethod to estimate the CSF of chat-based VLMs by directly prompting them to\njudge pattern visibility at different contrasts for each frequency. This\nmethodology is closer to the real experiments in psychophysics than the\npreviously reported. Using band-pass filtered noise images and a diverse set of\nprompts, we assess model responses across multiple architectures. We find that\nwhile some models approximate human-like CSF shape or magnitude, none fully\nreplicate both. Notably, prompt phrasing has a large effect on the responses,\nraising concerns about prompt stability. Our results provide a new framework\nfor probing visual sensitivity in multimodal models and reveal key gaps between\ntheir visual representations and human perception.", "AI": {"tldr": "本文提出一种受心理物理学启发的行为方法，通过直接提示多模态视觉语言模型（VLMs）判断图案可见性，来估计其对比敏感度函数（CSF），并发现当前VLM在视觉感知上与人类存在显著差距，且对提示语敏感。", "motivation": "理解多模态视觉语言模型如何感知低级视觉特征，并评估它们与人类感知的对齐程度至关重要。人类视觉的一个关键特征是对比敏感度函数（CSF），它描述了在低对比度下对空间频率的敏感性。", "method": "引入一种新颖的、受心理物理学启发的行为方法，通过直接提示基于聊天的VLM判断不同对比度下每个频率的图案可见性来估计其CSF。该方法比以往报道的更接近真实的心理物理学实验。使用带通滤波噪声图像和多样化的提示语来评估多个架构的模型响应。", "result": "研究发现，虽然一些模型在CSF的形状或幅度上近似于人类，但没有模型能完全复制两者。值得注意的是，提示语的措辞对模型响应有很大影响，引发了对提示稳定性（prompt stability）的担忧。", "conclusion": "本研究提供了一个探测多模态模型视觉敏感性的新框架，并揭示了它们的视觉表征与人类感知之间的关键差距。同时，也凸显了提示语稳定性在VLM评估中的重要性。"}}
{"id": "2508.10669", "categories": ["cs.AI", "cs.IR", "H.3.3; I.2.7; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.10669", "abs": "https://arxiv.org/abs/2508.10669", "authors": ["Zhenye Yang", "Jinpeng Chen", "Huan Li", "Xiongnan Jin", "Xuanyang Li", "Junwei Zhang", "Hongbo Gao", "Kaimin Wei", "Senzhang Wang"], "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation", "comment": "10 pages; 4 figures; 6 tables; code available at\n  https://github.com/Alex-bupt/STEP", "summary": "Conversational recommender systems (CRSs) aim to proactively capture user\npreferences through natural language dialogue and recommend high-quality items.\nTo achieve this, CRS gathers user preferences via a dialog module and builds\nuser profiles through a recommendation module to generate appropriate\nrecommendations. However, existing CRS faces challenges in capturing the deep\nsemantics of user preferences and dialogue context. In particular, the\nefficient integration of external knowledge graph (KG) information into\ndialogue generation and recommendation remains a pressing issue. Traditional\napproaches typically combine KG information directly with dialogue content,\nwhich often struggles with complex semantic relationships, resulting in\nrecommendations that may not align with user expectations.\n  To address these challenges, we introduce STEP, a conversational recommender\ncentered on pre-trained language models that combines curriculum-guided\ncontext-knowledge fusion with lightweight task-specific prompt tuning. At its\nheart, an F-Former progressively aligns the dialogue context with\nknowledge-graph entities through a three-stage curriculum, thus resolving\nfine-grained semantic mismatches. The fused representation is then injected\ninto the frozen language model via two minimal yet adaptive prefix prompts: a\nconversation prefix that steers response generation toward user intent and a\nrecommendation prefix that biases item ranking toward knowledge-consistent\ncandidates. This dual-prompt scheme allows the model to share cross-task\nsemantics while respecting the distinct objectives of dialogue and\nrecommendation. Experimental results show that STEP outperforms mainstream\nmethods in the precision of recommendation and dialogue quality in two public\ndatasets.", "AI": {"tldr": "本文提出STEP，一个基于预训练语言模型的对话推荐系统，通过课程引导的上下文-知识融合和轻量级任务特定提示调优，有效解决了现有系统在深层语义理解和知识图谱集成方面的挑战，提升了推荐精度和对话质量。", "motivation": "现有对话推荐系统难以捕获用户偏好和对话上下文的深层语义；将外部知识图谱信息有效集成到对话生成和推荐中仍是紧迫问题；传统方法直接结合知识图谱信息与对话内容，难以处理复杂语义关系，导致推荐不符合用户预期。", "method": "引入STEP系统，其核心是一个F-Former模块，通过三阶段课程逐步对齐对话上下文与知识图谱实体，解决细粒度语义不匹配。融合后的表示通过两个轻量级且自适应的前缀提示（对话前缀和推荐前缀）注入到冻结的预训练语言模型中，分别引导响应生成和物品排序。这种双提示方案允许模型共享跨任务语义，同时尊重对话和推荐的不同目标。", "result": "实验结果表明，STEP在两个公共数据集上，在推荐精度和对话质量方面均优于主流方法。", "conclusion": "STEP通过其创新的上下文-知识融合和双提示调优机制，成功解决了对话推荐系统中深层语义捕获和知识图谱有效集成的问题，显著提升了系统性能。"}}
{"id": "2508.10024", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.0"], "pdf": "https://arxiv.org/pdf/2508.10024", "abs": "https://arxiv.org/abs/2508.10024", "authors": ["J. Pablo Muñoz", "Jinjie Yuan"], "title": "RTTC: Reward-Guided Collaborative Test-Time Compute", "comment": null, "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.", "AI": {"tldr": "该研究提出了Reward-Guided Test-Time Compute (RTTC) 框架，通过预训练奖励模型自适应选择最有效的测试时计算（TTC）策略，以提高大语言模型（LLM）的推理准确性并降低计算开销。", "motivation": "现有的测试时计算（TTC）策略（如测试时训练TTT和检索增强生成RAG）虽能提升LLM性能，但其最佳适应策略因查询而异，不加区分的应用会导致巨大的计算开销。", "method": "RTTC框架通过预训练奖励模型为每个查询自适应选择最有效的TTC策略。它采用分布式服务器-客户端架构，仅在必要时从远程知识库检索相关样本并在客户端设备上应用RAG或轻量级微调。此外，提出了查询状态缓存（Query-State Caching）机制，以在检索和适应层面高效重用历史查询状态，进一步减少冗余计算。", "result": "在多个LLM和基准测试上的广泛实验表明，RTTC相比于普通的RAG或TTT方法，能够持续实现更高的准确性。", "conclusion": "研究验证了自适应、奖励引导的TTC选择的必要性，并证明了RTTC在可扩展、高性能语言模型适应方面的巨大潜力。RTTC能够有效提升LLM的推理准确性，同时优化计算效率。"}}
{"id": "2508.10382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10382", "abs": "https://arxiv.org/abs/2508.10382", "authors": ["Hyundo Lee", "Suhyung Choi", "Byoung-Tak Zhang", "Inwoo Hwang"], "title": "Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models", "comment": null, "summary": "Image generation models trained on large datasets can synthesize high-quality\nimages but often produce spatially inconsistent and distorted images due to\nlimited information about the underlying structures and spatial layouts. In\nthis work, we leverage intrinsic scene properties (e.g., depth, segmentation\nmaps) that provide rich information about the underlying scene, unlike prior\napproaches that solely rely on image-text pairs or use intrinsics as\nconditional inputs. Our approach aims to co-generate both images and their\ncorresponding intrinsics, enabling the model to implicitly capture the\nunderlying scene structure and generate more spatially consistent and realistic\nimages. Specifically, we first extract rich intrinsic scene properties from a\nlarge image dataset with pre-trained estimators, eliminating the need for\nadditional scene information or explicit 3D representations. We then aggregate\nvarious intrinsic scene properties into a single latent variable using an\nautoencoder. Building upon pre-trained large-scale Latent Diffusion Models\n(LDMs), our method simultaneously denoises the image and intrinsic domains by\ncarefully sharing mutual information so that the image and intrinsic reflect\neach other without degrading image quality. Experimental results demonstrate\nthat our method corrects spatial inconsistencies and produces a more natural\nlayout of scenes while maintaining the fidelity and textual alignment of the\nbase model (e.g., Stable Diffusion).", "AI": {"tldr": "该论文提出一种方法，通过共同生成图像及其内在场景属性（如深度、分割图），来解决大型图像生成模型在空间一致性方面的不足，从而生成更真实、空间布局更自然的图像。", "motivation": "大型图像生成模型虽然能生成高质量图像，但由于缺乏底层结构和空间布局信息，常产生空间不一致和扭曲的图像。现有方法多依赖图像-文本对或将内在属性作为条件输入，未能充分利用场景的内在属性信息。", "method": "1. 利用预训练的估计器从大型图像数据集中提取丰富的内在场景属性（如深度、分割图）。2. 使用自编码器将各种内在场景属性聚合成一个单一的潜在变量。3. 在预训练的大规模潜在扩散模型（LDMs）基础上，通过精心共享互信息，同时对图像和内在属性域进行去噪，使图像和内在属性相互反映，同时不降低图像质量。", "result": "实验结果表明，该方法能够纠正空间不一致性，生成更自然的场景布局，同时保持了基础模型（如Stable Diffusion）的图像保真度和文本对齐性。", "conclusion": "通过共同生成图像及其内在场景属性，该方法能够隐式捕获底层场景结构，显著提高了生成图像的空间一致性和真实感，有效解决了现有大型图像生成模型的局限性。"}}
{"id": "2508.10703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10703", "abs": "https://arxiv.org/abs/2508.10703", "authors": ["Yiping Song", "Jiaoyan Chen", "Renate A. Schmidt"], "title": "GenOM: Ontology Matching with Description Generation and Large Language Model", "comment": null, "summary": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.", "AI": {"tldr": "GenOM是一个基于LLM的本体匹配框架，通过生成文本定义丰富概念语义，结合嵌入模型检索和精确匹配工具，在生物医学领域本体匹配任务中表现出色。", "motivation": "本体匹配（OM）对于实现异构知识源间的语义互操作性和集成至关重要，尤其在包含大量复杂概念（如疾病和药物）的生物医学领域。", "method": "GenOM框架基于大型语言模型（LLM），通过生成文本定义来丰富本体概念的语义表示；使用嵌入模型检索对齐候选；并整合基于精确匹配的工具以提高精度。此外，还采用了少样本提示。", "result": "在OAEI Bio-ML赛道上的大量实验表明，GenOM通常能达到具有竞争力的性能，超越了许多基线，包括传统OM系统和近期基于LLM的方法。进一步的消融研究证实了语义丰富和少样本提示的有效性。", "conclusion": "GenOM是一个鲁棒且适应性强的框架，能够有效地利用LLM进行生物医学领域的本体匹配，通过语义丰富和精确匹配策略显著提升性能。"}}
{"id": "2508.10025", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10025", "abs": "https://arxiv.org/abs/2508.10025", "authors": ["Silvia García-Méndez", "Francisco de Arriba-Pérez"], "title": "Detecting and explaining postpartum depression in real-time with generative artificial intelligence", "comment": null, "summary": "Among the many challenges mothers undergo after childbirth, postpartum\ndepression (PPD) is a severe condition that significantly impacts their mental\nand physical well-being. Consequently, the rapid detection of ppd and their\nassociated risk factors is critical for in-time assessment and intervention\nthrough specialized prevention procedures. Accordingly, this work addresses the\nneed to help practitioners make decisions with the latest technological\nadvancements to enable real-time screening and treatment recommendations.\nMainly, our work contributes to an intelligent PPD screening system that\ncombines Natural Language Processing, Machine Learning (ML), and Large Language\nModels (LLMs) towards an affordable, real-time, and non-invasive free speech\nanalysis. Moreover, it addresses the black box problem since the predictions\nare described to the end users thanks to the combination of LLMs with\ninterpretable ml models (i.e., tree-based algorithms) using feature importance\nand natural language. The results obtained are 90 % on ppd detection for all\nevaluation metrics, outperforming the competing solutions in the literature.\nUltimately, our solution contributes to the rapid detection of PPD and their\nassociated risk factors, critical for in-time and proper assessment and\nintervention.", "AI": {"tldr": "该研究开发了一个智能产后抑郁症（PPD）筛查系统，结合自然语言处理、机器学习和大型语言模型，实现经济、实时、非侵入性的自由语音分析，并提供可解释的预测结果，PPD检测准确率达90%。", "motivation": "产后抑郁症严重影响母亲的身心健康，因此，快速检测PPD及其相关风险因素对于及时评估和干预至关重要。现有技术需要帮助实践者进行实时筛查和治疗建议。", "method": "该系统结合了自然语言处理（NLP）、机器学习（ML）和大型语言模型（LLMs），通过自由语音分析进行PPD筛查。为解决“黑箱问题”，系统将LLMs与可解释的ML模型（如基于树的算法）结合，利用特征重要性和自然语言向最终用户解释预测结果。", "result": "在所有评估指标上，PPD检测的准确率达到90%，优于现有文献中的竞争解决方案。", "conclusion": "该解决方案有助于快速检测产后抑郁症及其相关风险因素，这对于及时和适当的评估及干预至关重要。"}}
{"id": "2508.10383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10383", "abs": "https://arxiv.org/abs/2508.10383", "authors": ["Yechan Kim", "Dongho Yoon", "Younkwan Lee", "Unse Fatima", "Hong Kook Kim", "Songjae Lee", "Sanga Park", "Jeong Ho Park", "Seonjong Kang", "Moongu Jeon"], "title": "Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise", "comment": null, "summary": "While previous studies on image segmentation focus on handling severe (or\nexplicit) label noise, real-world datasets also exhibit subtle (or implicit)\nlabel imperfections. These arise from inherent challenges, such as ambiguous\nobject boundaries and annotator variability. Although not explicitly present,\nsuch mild and latent noise can still impair model performance. Typical data\naugmentation methods, which apply identical transformations to the image and\nits label, risk amplifying these subtle imperfections and limiting the model's\ngeneralization capacity. In this paper, we introduce NSegment+, a novel\naugmentation framework that decouples image and label transformations to\naddress such realistic noise for semantic segmentation. By introducing\ncontrolled elastic deformations only to segmentation labels while preserving\nthe original images, our method encourages models to focus on learning robust\nrepresentations of object structures despite minor label inconsistencies.\nExtensive experiments demonstrate that NSegment+ consistently improves\nperformance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in\naverage on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even\nwithout bells and whistles, highlighting the importance of addressing implicit\nlabel noise. These gains can be further amplified when combined with other\ntraining tricks, including CutMix and Label Smoothing.", "AI": {"tldr": "NSegment+是一种新的数据增强框架，通过对分割标签进行独立弹性形变，有效应对语义分割中微妙的隐性标签噪声，从而提高模型性能和泛化能力。", "motivation": "现有研究多关注显性标签噪声，但真实世界数据存在由边界模糊、标注者差异引起的微妙（隐性）标签不完美。这些轻微、潜在的噪声会损害模型性能。传统的图像和标签同步变换数据增强方法，反而可能放大这些微妙缺陷，限制模型泛化能力。", "method": "引入NSegment+框架，将图像和标签的变换解耦。具体地，仅对分割标签应用受控的弹性形变，而保持原始图像不变。这种方法旨在促使模型学习鲁棒的对象结构表示，即使存在轻微的标签不一致。", "result": "NSegment+持续提升了性能，在Vaihingen、LoveDA、Cityscapes和PASCAL VOC数据集上，mIoU平均分别提高了+2.29、+2.38、+1.75和+3.39。这些增益在与其他训练技巧（如CutMix和Label Smoothing）结合时可以进一步放大。", "conclusion": "解决隐性标签噪声至关重要。NSegment+通过解耦图像和标签变换，对标签进行独立弹性形变，能有效提升语义分割模型的鲁棒性和泛化能力，即使不使用其他复杂技巧也能获得显著性能提升。"}}
{"id": "2508.10745", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10745", "abs": "https://arxiv.org/abs/2508.10745", "authors": ["Sayan Nag", "K J Joseph", "Koustava Goswami", "Vlad I Morariu", "Balaji Vasan Srinivasan"], "title": "Agentic Design Review System", "comment": null, "summary": "Evaluating graphic designs involves assessing it from multiple facets like\nalignment, composition, aesthetics and color choices. Evaluating designs in a\nholistic way involves aggregating feedback from individual expert reviewers.\nTowards this, we propose an Agentic Design Review System (AgenticDRS), where\nmultiple agents collaboratively analyze a design, orchestrated by a meta-agent.\nA novel in-context exemplar selection approach based on graph matching and a\nunique prompt expansion method plays central role towards making each agent\ndesign aware. Towards evaluating this framework, we propose DRS-BENCH\nbenchmark. Thorough experimental evaluation against state-of-the-art baselines\nadapted to the problem setup, backed-up with critical ablation experiments\nbrings out the efficacy of Agentic-DRS in evaluating graphic designs and\ngenerating actionable feedback. We hope that this work will attract attention\nto this pragmatic, yet under-explored research direction.", "AI": {"tldr": "本文提出了一个基于多智能体协作的图形设计评估系统AgenticDRS，通过图匹配和提示扩展技术使智能体具备设计感知能力，并引入DRS-BENCH基准进行评估，证明了其在设计评估和生成可操作反馈方面的有效性。", "motivation": "图形设计评估涉及对齐、构图、美学和色彩选择等多个方面，且需要聚合专家反馈进行整体评估。现有方法可能无法有效处理这种多维度、整体性的评估需求，因此需要一种更系统、智能的评估方法。", "method": "本文提出了Agentic Design Review System (AgenticDRS)，其中多个智能体在元智能体的协调下协同分析设计。关键方法包括：1) 基于图匹配的新型上下文示例选择方法；2) 独特的提示扩展方法，使每个智能体都具备设计感知能力。为评估该框架，还提出了DRS-BENCH基准。", "result": "通过与现有最先进基线进行彻底的实验评估，并辅以关键的消融实验，证明了Agentic-DRS在评估图形设计和生成可操作反馈方面的有效性。该系统表现优于现有基线。", "conclusion": "Agentic-DRS能够有效评估图形设计并提供可操作的反馈。这项工作旨在吸引对图形设计智能评估这一实用但尚未充分探索的研究方向的关注。"}}
{"id": "2508.10026", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10026", "abs": "https://arxiv.org/abs/2508.10026", "authors": ["Kai Zhao", "Yanjun Zhao", "Jiaming Song", "Shien He", "Lusheng Zhang", "Qiang Zhang", "Tianjiao Li"], "title": "SABER: Switchable and Balanced Training for Efficient LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) empowered by chain-of-thought reasoning have\nachieved impressive accuracy on complex tasks but suffer from excessive\ninference costs and latency when applied uniformly to all problems. We propose\nSABER (Switchable and Balanced Training for Efficient LLM Reasoning), a\nreinforcement learning framework that endows LLMs with user-controllable,\ntoken-budgeted reasoning. SABER first profiles each training example's\nbase-model thinking token usage and assigns it to one of the predefined budget\ntiers. During fine-tuning, the model is guided by system prompts and\nlength-aware rewards to respect its assigned budget. In parallel, we\nincorporate no-think examples to ensure the model remains reliable even when\nexplicit reasoning is turned off. SABER further supports four discrete\ninference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling\nflexible trade-offs between latency and reasoning depth. Extensive evaluations\non math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning\n(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight\nbudgets, graceful degradation, and effective cross-scale and cross-domain\ngeneralization. In particular, SABER-FastThink cuts reasoning length by 65.4%\nand yields a 3.6% accuracy gain compared with the base model on the MATH\nbenchmark.", "AI": {"tldr": "SABER是一个强化学习框架，旨在使LLM在复杂任务上实现用户可控、预算受限的推理，从而在保持高准确性的同时降低推理成本和延迟。", "motivation": "尽管链式思考（CoT）推理使大型语言模型（LLM）在复杂任务上取得了令人印象深刻的准确性，但将其统一应用于所有问题会导致过高的推理成本和延迟。", "method": "SABER框架通过以下方式实现：1) 分析每个训练样本的基础模型思考token使用量，并将其分配到预定义的预算层级。2) 在微调过程中，通过系统提示和长度感知奖励引导模型遵守分配的预算。3) 引入“无思考”示例以确保模型在关闭显式推理时仍能可靠工作。4) 提供四种离散推理模式（NoThink, FastThink, CoreThink, DeepThink），以实现延迟和推理深度之间的灵活权衡。", "result": "在数学推理（MATH, GSM8K）、代码生成（MBPP）和逻辑推理（LiveBench-Reasoning）任务上的广泛评估表明，SABER在严格预算下仍能实现高准确性，表现出优雅的性能下降，并有效实现跨尺度和跨领域泛化。特别是，SABER-FastThink在MATH基准测试中将推理长度缩短了65.4%，并比基础模型带来了3.6%的准确率提升。", "conclusion": "SABER成功地为LLM赋予了用户可控、token预算受限的推理能力，有效解决了CoT推理的成本和延迟问题，同时在各种复杂任务上保持了高准确性和良好的泛化能力，并提供了灵活的推理模式选择。"}}
{"id": "2508.10397", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10397", "abs": "https://arxiv.org/abs/2508.10397", "authors": ["Haibin Sun", "Xinghui Song"], "title": "PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection", "comment": "11 pages, 6 figures", "summary": "Driver distraction detection is essential for improving traffic safety and\nreducing road accidents. However, existing models often suffer from degraded\ngeneralization when deployed in real-world scenarios. This limitation primarily\narises from the few-shot learning challenge caused by the high cost of data\nannotation in practical environments, as well as the substantial domain shift\nbetween training datasets and target deployment conditions. To address these\nissues, we propose a Pose-driven Quality-controlled Data Augmentation Framework\n(PQ-DAF) that leverages a vision-language model for sample filtering to\ncost-effectively expand training data and enhance cross-domain robustness.\nSpecifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to\naccurately capture key driver pose features and synthesize diverse training\nexamples. A sample quality assessment module, built upon the CogVLM\nvision-language model, is then introduced to filter out low-quality synthetic\nsamples based on a confidence threshold, ensuring the reliability of the\naugmented dataset. Extensive experiments demonstrate that PQ-DAF substantially\nimproves performance in few-shot driver distraction detection, achieving\nsignificant gains in model generalization under data-scarce conditions.", "AI": {"tldr": "本文提出一个姿态驱动、质量控制的数据增强框架（PQ-DAF），利用扩散模型生成训练样本并使用视觉-语言模型过滤低质量样本，以解决驾驶员分心检测中少样本学习和领域漂移导致的泛化能力差的问题。", "motivation": "现有驾驶员分心检测模型在实际部署中泛化能力差，主要原因在于数据标注成本高导致的少样本学习挑战，以及训练数据集与实际部署条件之间存在显著领域漂移。", "method": "提出PQ-DAF框架。具体方法包括：1) 采用渐进式条件扩散模型（PCDMs）精确捕捉驾驶员关键姿态特征并合成多样化的训练样本。2) 引入基于CogVLM视觉-语言模型的样本质量评估模块，根据置信度阈值过滤低质量合成样本，确保增强数据集的可靠性。", "result": "广泛实验表明，PQ-DAF显著提升了少样本驾驶员分心检测的性能，在数据稀缺条件下模型泛化能力获得了显著提升。", "conclusion": "PQ-DAF通过成本效益高的方式扩展训练数据并增强跨域鲁棒性，有效解决了驾驶员分心检测中少样本学习和领域漂移的挑战，提高了模型泛化能力。"}}
{"id": "2508.10769", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10769", "abs": "https://arxiv.org/abs/2508.10769", "authors": ["Zhiqi Shen", "Shaojing Fan", "Danni Xu", "Terence Sim", "Mohan Kankanhalli"], "title": "Modeling Human Responses to Multimodal AI Content", "comment": null, "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.", "AI": {"tldr": "该研究关注AI生成内容如何影响人类感知和行为，而非仅仅识别其真伪。为此，作者构建了MhAIM数据集，进行了人类研究，提出了衡量用户参与度的新指标，并开发了一个名为T-Lens的基于LLM的系统，该系统能预测人类对多模态信息的反应，以增强LLM的人类意识能力并应对AI驱动的错误信息风险。", "motivation": "随着AI生成内容的普及，错误信息传播的风险也随之增加。现有研究主要关注内容的真实性识别，但对于AI内容如何影响人类感知和行为知之甚少，而这在金融等领域比事实准确性更关键。", "method": "研究方法包括：1) 构建MhAIM数据集，包含154,552个在线帖子（其中111,153个为AI生成），用于大规模分析人类对AI生成内容的反应。2) 进行人类研究，观察人们如何识别AI内容。3) 提出三个新指标：可信度、影响力和开放性，以量化用户对在线内容的判断和参与。4) 开发T-Lens，一个基于LLM的智能体系统，通过整合预测的人类对多模态信息的反应来回答用户查询。5) T-Lens的核心是HR-MCP（人类响应模型上下文协议），它基于标准化的MCP构建，可与任何LLM无缝集成。", "result": "主要结果显示：1) 人们在帖子同时包含文本和视觉内容时，尤其当两者存在不一致时，更能识别出AI生成内容。2) T-Lens系统，通过整合HR-MCP，能更好地与人类反应保持一致，从而增强了LLM的可解释性和交互能力。", "conclusion": "该工作提供了实证见解和实用工具，旨在赋予LLM人类意识能力。通过揭示AI、人类认知和信息接收之间复杂的相互作用，研究结果提出了可行的策略，以减轻AI驱动的错误信息带来的风险。"}}
{"id": "2508.10027", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10027", "abs": "https://arxiv.org/abs/2508.10027", "authors": ["Ali Zolnour", "Hossein Azadmaleki", "Yasaman Haghbin", "Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sina Rashidi", "Masoud Khani", "AmirSajjad Taleban", "Samin Mahdizadeh Sani", "Maryam Dadkhah", "James M. Noble", "Suzanne Bakken", "Yadollah Yaghoobzadeh", "Abdol-Hossein Vahabie", "Masoud Rouhizadeh", "Maryam Zolnoori"], "title": "LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data", "comment": null, "summary": "Alzheimer's disease and related dementias (ADRD) affect approximately five\nmillion older adults in the U.S., yet over half remain undiagnosed.\nSpeech-based natural language processing (NLP) offers a promising, scalable\napproach to detect early cognitive decline through linguistic markers.\n  To develop and evaluate a screening pipeline that (i) fuses transformer\nembeddings with handcrafted linguistic features, (ii) tests data augmentation\nusing synthetic speech generated by large language models (LLMs), and (iii)\nbenchmarks unimodal and multimodal LLM classifiers for ADRD detection.\n  Transcripts from the DementiaBank \"cookie-theft\" task (n = 237) were used.\nTen transformer models were evaluated under three fine-tuning strategies. A\nfusion model combined embeddings from the top-performing transformer with 110\nlexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,\nMinistral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic\nspeech, which was used to augment training data. Three multimodal models\n(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in\nzero-shot and fine-tuned settings.\n  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or\ntransformer-only baselines. Augmenting training data with 2x MedAlpaca-7B\nsynthetic speech increased F1 to 85.7. Fine-tuning significantly improved\nunimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current\nmultimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =\n66.0). Performance gains aligned with the distributional similarity between\nsynthetic and real speech.\n  Integrating transformer embeddings with linguistic features enhances ADRD\ndetection from speech. Clinically tuned LLMs effectively support both\nclassification and data augmentation, while further advancement is needed in\nmultimodal modeling.", "AI": {"tldr": "该研究开发了一种融合Transformer嵌入和手工语言特征的语音NLP管线，用于阿尔茨海默病及相关痴呆（ADRD）的早期检测，并探索了基于LLM的合成语音数据增强及多模态分类器的应用。", "motivation": "美国约有500万老年人受ADRD影响，但超过一半未被诊断。语音自然语言处理（NLP）提供了一种有前景、可扩展的方法，通过语言标记检测早期认知衰退。", "method": "研究使用了DementiaBank的“cookie-theft”任务转录数据（n=237）。评估了10种Transformer模型及其微调策略。融合模型结合了最佳Transformer嵌入和110个词汇衍生语言特征。使用5种LLM（LLaMA-8B/70B, MedAlpaca-7B, Ministral-8B, GPT-4o）生成标签条件合成语音进行数据增强。测试了3种多模态模型（GPT-4o, Qwen-Omni, Phi-4）在零样本和微调设置下的语音-文本分类。", "result": "融合模型在ADRD检测中表现最佳（F1=83.3，AUC=89.5），优于单独使用语言特征或Transformer的模型。使用2倍MedAlpaca-7B合成语音增强训练数据，将F1提高到85.7。微调显著提升了单模态LLM分类器性能（如MedAlpaca：F1从47.3提高到78.5）。当前多模态模型表现较低（GPT-4o=70.2 F1；Qwen=66.0）。性能提升与合成语音和真实语音的分布相似性一致。", "conclusion": "整合Transformer嵌入与语言特征可增强ADRD的语音检测能力。临床调整的LLM能有效支持分类和数据增强，但多模态建模仍需进一步发展。"}}
{"id": "2508.10407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10407", "abs": "https://arxiv.org/abs/2508.10407", "authors": ["Eunseo Koh", "Seunghoo Hong", "Tae-Young Kim", "Simon S. Woo", "Jae-Pil Heo"], "title": "Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-Image (T2I) diffusion models have made significant progress in\ngenerating diverse high-quality images from textual prompts. However, these\nmodels still face challenges in suppressing content that is strongly entangled\nwith specific words. For example, when generating an image of ``Charlie\nChaplin\", a ``mustache\" consistently appears even if explicitly instructed not\nto include it, as the concept of ``mustache\" is strongly entangled with\n``Charlie Chaplin\". To address this issue, we propose a novel approach to\ndirectly suppress such entangled content within the text embedding space of\ndiffusion models. Our method introduces a delta vector that modifies the text\nembedding to weaken the influence of undesired content in the generated image,\nand we further demonstrate that this delta vector can be easily obtained\nthrough a zero-shot approach. Furthermore, we propose a Selective Suppression\nwith Delta Vector (SSDV) method to adapt delta vector into the cross-attention\nmechanism, enabling more effective suppression of unwanted content in regions\nwhere it would otherwise be generated. Additionally, we enabled more precise\nsuppression in personalized T2I models by optimizing delta vector, which\nprevious baselines were unable to achieve. Extensive experimental results\ndemonstrate that our approach significantly outperforms existing methods, both\nin terms of quantitative and qualitative metrics.", "AI": {"tldr": "该论文提出了一种新方法，通过在文本嵌入空间中引入一个“delta向量”来抑制文本到图像（T2I）扩散模型中与特定词强关联的、不希望出现的图像内容，并能实现选择性抑制和个性化模型中的精确抑制。", "motivation": "当前的T2I扩散模型在生成图像时，难以抑制与某些特定词语强关联的内容（例如，生成“查理·卓别林”时，即使明确指示不包含胡子，胡子仍会持续出现），因为这些概念高度纠缠。", "method": "1. 引入一个“delta向量”来修改文本嵌入空间，以削弱不希望内容的影响。\n2. 通过零样本方法获取该delta向量。\n3. 提出选择性抑制与Delta向量（SSDV）方法，将delta向量应用于交叉注意力机制，实现对特定区域不希望内容的有效抑制。\n4. 通过优化delta向量，在个性化T2I模型中实现更精确的抑制。", "result": "实验结果表明，该方法在定量和定性指标上均显著优于现有方法。", "conclusion": "所提出的方法能有效抑制T2I扩散模型中（包括个性化模型）与特定词强关联的、不希望出现的图像内容，解决了现有模型难以处理的概念纠缠问题。"}}
{"id": "2508.10777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10777", "abs": "https://arxiv.org/abs/2508.10777", "authors": ["Maël Jullien", "Marco Valentino", "André Freitas"], "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference", "comment": "19 pages", "summary": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.", "AI": {"tldr": "研究质疑了LLM通过扩展数据和参数就能获得结构化表示的假设。通过临床试验NLI基准测试，并使用GKMRV探针区分知识和推理，发现LLM虽拥有知识但推理能力差，且输出一致性高，表明其依赖启发式而非可靠推理。", "motivation": "质疑大型语言模型（LLMs）仅通过扩展数据和参数就能自然获得日益结构化、可泛化的内部表示的普遍假设，并希望在临床试验等高风险领域探究其推理能力，同时区分模型是缺乏事实知识还是缺乏推理能力。", "method": "引入了一个临床试验自然语言推理（NLI）基准测试，包含因果归因、组合基础、认知验证和风险状态抽象四种推理类型。每个测试项都配有一个针对性的“基础知识和元级别推理验证”（GKMRV）探针，用于区分事实知识获取失败与推理失败。评估了六个当代LLM，并使用了直接提示和思维链提示两种方式。", "result": "模型在GKMRV准确率上接近满分（平均准确率0.918），表明它们拥有相关临床知识。然而，在主要的推理任务上表现不佳（平均准确率0.25）。尽管准确率低，但输出推理在不同样本间高度一致（平均0.87），这表明模型系统性地应用了底层启发式方法和捷径。结果揭示了LLM在结构和表示上的根本局限性：它们常具备相关临床知识，但缺乏可靠运用这些知识所需的结构化、可组合的内部表示（例如，整合约束、权衡证据或模拟反事实）。", "conclusion": "通过GKMRV将知识与推理解耦，使这种分离变得明确且可衡量，为在高风险领域探测LLM的可靠性提供了一个有效框架。当前LLM尽管拥有相关知识，但缺乏进行可靠推理所需的结构化、可组合的内部表示。"}}
{"id": "2508.10028", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10028", "abs": "https://arxiv.org/abs/2508.10028", "authors": ["Xiao Fu", "Hossein A. Rahmani", "Bin Wu", "Jerome Ramos", "Emine Yilmaz", "Aldo Lipani"], "title": "PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs", "comment": "7 pages", "summary": "Personalised text generation is essential for user-centric information\nsystems, yet most evaluation methods overlook the individuality of users. We\nintroduce \\textbf{PREF}, a \\textbf{P}ersonalised \\textbf{R}eference-free\n\\textbf{E}valuation \\textbf{F}ramework that jointly measures general output\nquality and user-specific alignment without requiring gold personalised\nreferences. PREF operates in a three-step pipeline: (1) a coverage stage uses a\nlarge language model (LLM) to generate a comprehensive, query-specific\nguideline covering universal criteria such as factuality, coherence, and\ncompleteness; (2) a preference stage re-ranks and selectively augments these\nfactors using the target user's profile, stated or inferred preferences, and\ncontext, producing a personalised evaluation rubric; and (3) a scoring stage\napplies an LLM judge to rate candidate answers against this rubric, ensuring\nbaseline adequacy while capturing subjective priorities. This separation of\ncoverage from preference improves robustness, transparency, and reusability,\nand allows smaller models to approximate the personalised quality of larger\nones. Experiments on the PrefEval benchmark, including implicit\npreference-following tasks, show that PREF achieves higher accuracy, better\ncalibration, and closer alignment with human judgments than strong baselines.\nBy enabling scalable, interpretable, and user-aligned evaluation, PREF lays the\ngroundwork for more reliable assessment and development of personalised\nlanguage generation systems.", "AI": {"tldr": "PREF是一个个性化、无参考的评估框架，用于联合衡量文本生成质量和用户对齐度，无需黄金个性化参考。", "motivation": "现有的文本生成评估方法大多忽视用户的个性化需求，但个性化文本生成对以用户为中心的信息系统至关重要。", "method": "PREF采用三步流程：1) 覆盖阶段：LLM生成通用质量标准（如事实性、连贯性）的指南；2) 偏好阶段：根据用户画像、偏好和上下文，重新排序并增强这些因素，生成个性化评估准则；3) 评分阶段：LLM判断模型根据个性化准则对候选答案进行评分。", "result": "在PrefEval基准测试中，PREF比强基线模型取得了更高的准确性、更好的校准性，以及与人类判断更紧密的一致性。", "conclusion": "PREF为个性化语言生成系统的评估和开发提供了可扩展、可解释且与用户对齐的评估方法，从而奠定了更可靠评估的基础。"}}
{"id": "2508.10411", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10411", "abs": "https://arxiv.org/abs/2508.10411", "authors": ["Chaesong Park", "Eunbin Seo", "Jihyeon Hwang", "Jongwoo Lim"], "title": "SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection", "comment": "10 pages, 4 figures, 5 tables", "summary": "In this paper, we introduce SC-Lane, a novel slope-aware and temporally\nconsistent heightmap estimation framework for 3D lane detection. Unlike\nprevious approaches that rely on fixed slope anchors, SC-Lane adaptively\ndetermines the fusion of slope-specific height features, improving robustness\nto diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive\nFeature module that dynamically predicts the appropriate weights from image\ncues for integrating multi-slope representations into a unified heightmap.\nAdditionally, a Height Consistency Module enforces temporal coherence, ensuring\nstable and accurate height estimation across consecutive frames, which is\ncrucial for real-world driving scenarios. To evaluate the effectiveness of\nSC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root\nMean Squared Error (RMSE), and threshold-based accuracy-which, although common\nin surface and depth estimation, have been underutilized for road height\nassessment. Using the LiDAR-derived heightmap dataset introduced in prior work\n[20], we benchmark our method under these metrics, thereby establishing a\nrigorous standard for future comparisons. Extensive experiments on the OpenLane\nbenchmark demonstrate that SC-Lane significantly improves both height\nestimation and 3D lane detection, achieving state-of-the-art performance with\nan F-score of 64.3%, outperforming existing methods by a notable margin. For\ndetailed results and a demonstration video, please refer to our project\npage:https://parkchaesong.github.io/sclane/", "AI": {"tldr": "SC-Lane是一个新颖的3D车道线检测框架，通过斜率感知和时间一致的高度图估计，显著提升了对复杂道路几何的鲁棒性和检测精度。", "motivation": "现有方法依赖固定斜率锚点，导致对多样化道路几何的适应性不足，且缺乏连续帧间高度估计的时间一致性，这在真实驾驶场景中至关重要。", "method": "本文提出了SC-Lane框架。核心方法包括：1) 斜率感知自适应特征模块，通过图像线索动态预测权重，融合多斜率表示到统一高度图；2) 高度一致性模块，强制执行时间连贯性，确保连续帧间高度估计的稳定性和准确性。评估采用MAE、RMSE和基于阈值的精度等标准指标，并在LiDAR高度图数据集和OpenLane基准上进行测试。", "result": "SC-Lane在OpenLane基准测试中显著提升了高度估计和3D车道线检测性能，F-score达到64.3%，超越现有方法，实现了最先进的性能。", "conclusion": "SC-Lane通过其斜率感知和时间一致性机制，有效解决了3D车道线检测中高度估计的鲁棒性和稳定性问题，为未来的研究建立了严格的基准。"}}
{"id": "2508.10806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10806", "abs": "https://arxiv.org/abs/2508.10806", "authors": ["Maria J. P. Peixoto", "Akriti Pandey", "Ahsan Zaman", "Peter R. Lewis"], "title": "Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems", "comment": "Paper accepted for the IJCAI 2025 Workshop on Explainable Artificial\n  Intelligence (XAI): https://sites.google.com/view/xai2025/proceedings", "summary": "As AI systems are increasingly deployed to support decision-making in\ncritical domains, explainability has become a means to enhance the\nunderstandability of these outputs and enable users to make more informed and\nconscious choices. However, despite growing interest in the usability of\neXplainable AI (XAI), the accessibility of these methods, particularly for\nusers with vision impairments, remains underexplored. This paper investigates\naccessibility gaps in XAI through a two-pronged approach. First, a literature\nreview of 79 studies reveals that evaluations of XAI techniques rarely include\ndisabled users, with most explanations relying on inherently visual formats.\nSecond, we present a four-part methodological proof of concept that\noperationalizes inclusive XAI design: (1) categorization of AI systems, (2)\npersona definition and contextualization, (3) prototype design and\nimplementation, and (4) expert and user assessment of XAI techniques for\naccessibility. Preliminary findings suggest that simplified explanations are\nmore comprehensible for non-visual users than detailed ones, and that\nmultimodal presentation is required for more equitable interpretability.", "AI": {"tldr": "本研究探讨了可解释人工智能（XAI）对视障用户的可访问性不足问题。通过文献综述和包容性XAI设计概念验证，发现现有XAI研究忽视残障用户且多依赖视觉解释，并提出简化解释和多模态呈现对非视觉用户更有效。", "motivation": "尽管可解释人工智能（XAI）的可用性日益受到关注，但其方法的可访问性，特别是对视障用户而言，仍未得到充分探索。现有XAI解释多依赖视觉格式，且评估中很少包含残障用户，导致存在显著的可访问性鸿沟。", "method": "本研究采用双重方法：1. 对79项相关研究进行文献综述，分析XAI技术评估中是否包含残障用户以及解释格式；2. 提出一个四部分的方法论概念验证框架，用于操作化包容性XAI设计，包括AI系统分类、用户画像定义、原型设计与实现，以及专家和用户对XAI技术可访问性的评估。", "result": "1. 文献综述显示，XAI技术评估中很少包含残障用户，且大多数解释依赖于固有的视觉格式。2. 初步发现表明，对于非视觉用户，简化解释比详细解释更易理解。3. 需要多模态呈现才能实现更公平的可解释性。", "conclusion": "为了实现更具包容性的XAI，需要特别关注视障用户的需求，采用简化解释和多模态呈现方式，以弥合当前XAI方法中的可访问性鸿沟，从而实现更公平、可理解的决策支持。"}}
{"id": "2508.10029", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10029", "abs": "https://arxiv.org/abs/2508.10029", "authors": ["Wenpeng Xing", "Mohan Li", "Chunqiang Hu", "Haitao XuNingyu Zhang", "Bo Lin", "Meng Han"], "title": "Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs", "comment": null, "summary": "Large language models (LLMs) demonstrate impressive capabilities in various\nlanguage tasks but are susceptible to jailbreak attacks that circumvent their\nsafety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a\nrepresentation-based attack that interpolates hidden states from harmful and\nbenign query pairs to elicit prohibited responses. LFJ begins by selecting\nquery pairs with high thematic and syntactic similarity, then performs\ngradient-guided interpolation at influential layers and tokens, followed by\noptimization to balance attack success, output fluency, and computational\nefficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks\nlike AdvBench and MaliciousInstruct yield an average attack success rate (ASR)\nof 94.01%, outperforming existing methods. To mitigate LFJ, we propose an\nadversarial training defense that fine-tunes models on interpolated examples,\nreducing ASR by over 80% without degrading performance on benign inputs.\nAblation studies validate the importance of query pair selection, hidden state\ninterpolation components, and optimization strategies in LFJ's effectiveness.", "AI": {"tldr": "本文提出了一种名为Latent Fusion Jailbreak (LFJ)的基于表示的越狱攻击，通过插值有害和良性查询的隐藏状态来绕过LLM安全对齐，并提出了一种对抗训练防御方法。", "motivation": "大型语言模型（LLMs）在语言任务中表现出色，但容易受到越狱攻击，这些攻击能够绕过其安全对齐。", "method": "LFJ通过选择主题和句法相似的有害与良性查询对，在关键层和token上进行梯度引导的隐藏状态插值，并优化以平衡攻击成功率、输出流畅性和计算效率。为缓解LFJ，本文提出了一种对抗训练防御，通过在插值示例上微调模型。", "result": "LFJ在Vicuna和LLaMA-2等模型上取得了平均94.01%的攻击成功率（ASR），优于现有方法。所提出的防御将ASR降低了80%以上，且不影响良性输入性能。", "conclusion": "LFJ是一种高效的基于表示的越狱攻击，通过隐藏状态插值实现。同时，对抗训练是一种有效的缓解LFJ攻击的方法，且不损害模型在良性任务上的性能。"}}
{"id": "2508.10424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10424", "abs": "https://arxiv.org/abs/2508.10424", "authors": ["Shanyuan Liu", "Jian Zhu", "Junda Lu", "Yue Gong", "Liuzhuozheng Li", "Bo Cheng", "Yuhang Ma", "Liebucha Wu", "Xiaoyu Wu", "Dawei Leng", "Yuhui Yin"], "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer", "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.", "AI": {"tldr": "提出NanoControl，一种高效的扩散Transformer（DiT）可控文本到图像生成方法，通过LoRA风格控制模块和KV-Context增强机制，显著减少参数和计算开销，同时保持卓越性能。", "motivation": "现有DiT可控文本到图像生成方法（如基于ControlNet）引入了大量的参数和计算开销，效率低下。", "method": "提出NanoControl模型，以Flux为骨干网络。该模型采用LoRA风格的控制模块直接学习控制信号，而非复制DiT骨干。此外，引入KV-Context增强机制，将条件特定的键值信息高效集成到骨干网络中，促进条件特征的深度融合。", "result": "NanoControl在可控文本到图像生成方面达到最先进性能，参数量仅增加0.024%，GFLOPs仅增加0.029%。与传统控制方法相比，显著降低了计算开销，同时保持了卓越的生成质量并提高了可控性。", "conclusion": "NanoControl为DiT可控图像生成提供了一种高效且高性能的解决方案，有效克服了现有方法的参数和计算瓶颈。"}}
{"id": "2508.10030", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10030", "abs": "https://arxiv.org/abs/2508.10030", "authors": ["Saaduddin Mahmud", "Mason Nakamura", "Kyle H. Wray", "Shlomo Zilberstein"], "title": "Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models", "comment": "17 pages", "summary": "Prompt optimization methods have demonstrated significant effectiveness in\naligning black-box large language models (LLMs). In parallel, inference scaling\nstrategies such as Best-of-N Sampling and Majority Voting have also proven to\nenhance alignment and performance by trading off computation. However, existing\nprompt optimization approaches are inference strategy agnostic; that is, they\noptimize prompts without regard to the inference strategy employed during\ndeployment. This constitutes a significant methodological gap, as our empirical\nand theoretical analysis reveals a strong interdependence between these two\nparadigms. Moreover, we find that user preferences regarding trade-offs among\nmultiple objectives and inference budgets substantially influence the choice of\nprompt and inference configuration. To address this gap, we introduce a unified\nnovel framework named IAPO (Inference-Aware Prompt Optimization) that jointly\noptimizes the prompt and inference scale, while being aware of the inference\nbudget and different task objectives. We then develop a fixed-budget training\nalgorithm for IAPO, which we call PSST (Prompt Scaling via Sequential\nTrimming), and analyze finite-budget guarantees on error probability. Finally,\nwe evaluate the effectiveness of PSST on six different tasks, including\nmulti-objective text generation and reasoning, and demonstrate the critical\nrole of incorporating inference-awareness when aligning black-box LLMs through\nprompt optimization.", "AI": {"tldr": "本文提出了一个统一的框架IAPO及其算法PSST，用于在考虑推理预算和多目标的情况下，联合优化黑盒LLM的提示和推理规模，解决了现有提示优化方法与推理策略脱节的问题。", "motivation": "现有提示优化方法在优化提示时，不考虑部署时采用的推理策略（如Best-of-N采样、多数投票），这构成了一个显著的方法学空白。研究发现提示优化和推理策略之间存在强烈的相互依赖性。此外，用户对多目标权衡和推理预算的偏好也极大地影响了提示和推理配置的选择。", "method": "引入了IAPO（Inference-Aware Prompt Optimization）统一框架，该框架联合优化提示和推理规模，同时考虑推理预算和不同任务目标。为IAPO开发了固定预算训练算法PSST（Prompt Scaling via Sequential Trimming），并分析了有限预算下错误概率的保证。", "result": "在六个不同的任务（包括多目标文本生成和推理）上评估了PSST的有效性。结果表明，在通过提示优化对黑盒LLM进行对齐时，结合推理感知（inference-awareness）起着关键作用。", "conclusion": "将推理感知纳入提示优化对于有效对齐黑盒大型语言模型至关重要，联合优化提示和推理规模能够显著提升性能和对齐效果。"}}
{"id": "2508.10427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10427", "abs": "https://arxiv.org/abs/2508.10427", "authors": ["Keishi Ishihara", "Kento Sasaki", "Tsubasa Takahashi", "Daiki Shiono", "Yu Yamaguchi"], "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes", "comment": "Project Page: https://turingmotors.github.io/stride-qa/", "summary": "Vision-Language Models (VLMs) have been applied to autonomous driving to\nsupport decision-making in complex real-world scenarios. However, their\ntraining on static, web-sourced image-text pairs fundamentally limits the\nprecise spatiotemporal reasoning required to understand and predict dynamic\ntraffic scenes. We address this critical gap with STRIDE-QA, a large-scale\nvisual question answering (VQA) dataset for physically grounded reasoning from\nan ego-centric perspective. Constructed from 100 hours of multi-sensor driving\ndata in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the\nlargest VQA dataset for spatiotemporal reasoning in urban driving, offering 16\nmillion QA pairs over 285K frames. Grounded by dense, automatically generated\nannotations including 3D bounding boxes, segmentation masks, and multi-object\ntracks, the dataset uniquely supports both object-centric and ego-centric\nreasoning through three novel QA tasks that require spatial localization and\ntemporal prediction. Our benchmarks demonstrate that existing VLMs struggle\nsignificantly, achieving near-zero scores on prediction consistency. In\ncontrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,\nachieving 55% success in spatial localization and 28% consistency in future\nmotion prediction, compared to near-zero scores from general-purpose VLMs.\nTherefore, STRIDE-QA establishes a comprehensive foundation for developing more\nreliable VLMs for safety-critical autonomous systems.", "AI": {"tldr": "本文提出了STRIDE-QA，一个大规模的视觉问答（VQA）数据集，用于自动驾驶中的时空推理，旨在弥补现有视觉语言模型（VLMs）在动态交通场景理解方面的不足。", "motivation": "现有VLM主要在静态网络图像-文本对上训练，这限制了它们在理解和预测动态交通场景所需的精确时空推理能力。自动驾驶需要这种能力来支持复杂真实世界场景中的决策。", "method": "构建了STRIDE-QA数据集，包含100小时来自东京的多传感器驾驶数据，覆盖28.5万帧，生成1600万个问答对。数据集通过密集的自动生成标注（包括3D边界框、分割掩模和多目标跟踪）进行支撑，并通过三个新颖的问答任务支持以物体为中心和以自我为中心的推理，这些任务需要空间定位和时间预测。", "result": "基准测试表明，现有VLM在预测一致性上表现极差，得分接近于零。相比之下，在STRIDE-QA上微调的VLM表现出显著的性能提升，在空间定位上达到55%的成功率，在未来运动预测上达到28%的一致性。", "conclusion": "STRIDE-QA为开发更可靠的、适用于安全关键型自动驾驶系统的VLM奠定了全面的基础，显著提升了模型在时空推理方面的能力。"}}
{"id": "2508.10032", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10032", "abs": "https://arxiv.org/abs/2508.10032", "authors": ["Fan Yang"], "title": "The Cost of Thinking: Increased Jailbreak Risk in Large Language Models", "comment": null, "summary": "Thinking mode has always been regarded as one of the most valuable modes in\nLLMs. However, we uncover a surprising and previously overlooked phenomenon:\nLLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate\n9 LLMs on AdvBench and HarmBench and find that the success rate of attacking\nthinking mode in LLMs is almost higher than that of non-thinking mode. Through\nlarge numbers of sample studies, it is found that for educational purposes and\nexcessively long thinking lengths are the characteristics of successfully\nattacked data, and LLMs also give harmful answers when they mostly know that\nthe questions are harmful. In order to alleviate the above problems, this paper\nproposes a method of safe thinking intervention for LLMs, which explicitly\nguides the internal thinking processes of LLMs by adding \"specific thinking\ntokens\" of LLMs to the prompt. The results demonstrate that the safe thinking\nintervention can significantly reduce the attack success rate of LLMs with\nthinking mode.", "AI": {"tldr": "研究发现，大语言模型（LLMs）的思维模式（thinking mode）更容易受到越狱攻击。通过大规模评估，发现攻击思维模式的成功率更高。论文提出了一种“安全思维干预”方法，通过在提示中添加特定思维token来引导LLMs的内部思维过程，从而显著降低攻击成功率。", "motivation": "思维模式一直被认为是LLMs中最有价值的模式之一，但研究者偶然发现了一个此前被忽视的现象：具有思维模式的LLMs更容易被越狱攻击攻破，这促使他们深入研究并寻求缓解方案。", "method": "研究方法包括：1. 在AdvBench和HarmBench数据集上评估9个LLMs，比较思维模式和非思维模式下的攻击成功率。2. 通过大量样本研究，分析成功攻击数据的特征（如教育目的、过长的思维长度）以及LLMs在明知问题有害时仍给出有害回答的情况。3. 提出一种“安全思维干预”方法，通过在提示中添加“特定思维token”来明确引导LLMs的内部思维过程。", "result": "主要结果显示：1. 攻击LLMs思维模式的成功率几乎总是高于非思维模式。2. 成功攻击的数据通常具有“教育目的”和“过长的思维长度”等特征。3. LLMs在多数情况下即使知道问题有害，也仍会给出有害答案。4. 提出的“安全思维干预”方法能够显著降低具有思维模式的LLMs的攻击成功率。", "conclusion": "研究得出结论，LLMs的思维模式虽然有价值，但也使其更容易受到越狱攻击。通过分析攻击特征，并引入“安全思维干预”这种显式引导内部思维过程的方法，可以有效缓解思维模式下LLMs的脆弱性，提高其安全性。"}}
{"id": "2508.10036", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10036", "abs": "https://arxiv.org/abs/2508.10036", "authors": ["Dong Zhao", "Yadong Wang", "Xiang Chen", "Chenxi Wang", "Hongliang Dai", "Chuanxing Geng", "Shengzhong Zhang", "Shaoyuan Li", "Sheng-Jun Huang"], "title": "Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion", "comment": "Under Review", "summary": "Large Language Models (LLMs) show remarkable potential for few-shot\ninformation extraction (IE), yet their performance is highly sensitive to the\nchoice of in-context examples. Conventional selection strategies often fail to\nprovide informative guidance, as they overlook a key source of model\nfallibility: confusion stemming not just from semantic content, but also from\nthe generation of well-structured formats required by IE tasks. To address\nthis, we introduce Active Prompting for Information Extraction (APIE), a novel\nactive prompting framework guided by a principle we term introspective\nconfusion. Our method empowers an LLM to assess its own confusion through a\ndual-component uncertainty metric that uniquely quantifies both Format\nUncertainty (difficulty in generating correct syntax) and Content Uncertainty\n(inconsistency in extracted semantics). By ranking unlabeled data with this\ncomprehensive score, our framework actively selects the most challenging and\ninformative samples to serve as few-shot exemplars. Extensive experiments on\nfour benchmarks show that our approach consistently outperforms strong\nbaselines, yielding significant improvements in both extraction accuracy and\nrobustness. Our work highlights the critical importance of a fine-grained,\ndual-level view of model uncertainty when it comes to building effective and\nreliable structured generation systems.", "AI": {"tldr": "该研究提出APIE框架，通过引入格式不确定性和内容不确定性双重指标，使LLM能自我评估困惑，从而主动选择信息量最大的样本作为少样本示例，显著提升信息抽取性能。", "motivation": "大型语言模型（LLMs）在少样本信息抽取（IE）方面潜力巨大，但其性能对上下文示例的选择高度敏感。传统选择策略不足以提供有效指导，因为它们忽略了模型错误的一个关键来源：不仅是语义内容的混淆，还包括生成IE任务所需结构化格式的困难。", "method": "引入信息抽取主动提示（APIE）框架，基于“内省困惑”原则。该方法使LLM能够通过双组件不确定性度量来评估自身的困惑，该度量独特地量化了“格式不确定性”（生成正确语法的难度）和“内容不确定性”（提取语义的不一致性）。通过使用此综合分数对未标记数据进行排序，框架主动选择最具挑战性和信息量的样本作为少样本示例。", "result": "在四个基准测试上的广泛实验表明，该方法始终优于强基线，在提取准确性和鲁棒性方面都取得了显著改进。", "conclusion": "构建有效可靠的结构化生成系统时，对模型不确定性进行细粒度、双层（格式和内容）的审视至关重要。"}}
{"id": "2508.10432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10432", "abs": "https://arxiv.org/abs/2508.10432", "authors": ["Baichen Liu", "Qi Lyu", "Xudong Wang", "Jiahua Dong", "Lianqing Liu", "Zhi Han"], "title": "CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation", "comment": null, "summary": "Continual video instance segmentation demands both the plasticity to absorb\nnew object categories and the stability to retain previously learned ones, all\nwhile preserving temporal consistency across frames. In this work, we introduce\nContrastive Residual Injection and Semantic Prompting (CRISP), an earlier\nattempt tailored to address the instance-wise, category-wise, and task-wise\nconfusion in continual video instance segmentation. For instance-wise learning,\nwe model instance tracking and construct instance correlation loss, which\nemphasizes the correlation with the prior query space while strengthening the\nspecificity of the current task query. For category-wise learning, we build an\nadaptive residual semantic prompt (ARSP) learning framework, which constructs a\nlearnable semantic residual prompt pool generated by category text and uses an\nadjustive query-prompt matching mechanism to build a mapping relationship\nbetween the query of the current task and the semantic residual prompt.\nMeanwhile, a semantic consistency loss based on the contrastive learning is\nintroduced to maintain semantic coherence between object queries and residual\nprompts during incremental training. For task-wise learning, to ensure the\ncorrelation at the inter-task level within the query space, we introduce a\nconcise yet powerful initialization strategy for incremental prompts. Extensive\nexperiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that\nCRISP significantly outperforms existing continual segmentation methods in the\nlong-term continual video instance segmentation task, avoiding catastrophic\nforgetting and effectively improving segmentation and classification\nperformance. The code is available at https://github.com/01upup10/CRISP.", "AI": {"tldr": "本文提出CRISP框架，通过对比残差注入和语义提示，解决持续视频实例分割中实例级、类别级和任务级混淆问题，有效平衡新旧知识学习并保持时间一致性。", "motivation": "持续视频实例分割面临吸收新类别、保留旧知识以及保持跨帧时间一致性的挑战，同时存在实例、类别和任务间的混淆，需要一种能兼顾可塑性和稳定性的方法来避免灾难性遗忘。", "method": "CRISP框架包括：1) 实例级学习：建模实例跟踪并构建实例相关性损失，强调与先前查询空间的相关性并增强当前任务查询的特异性。2) 类别级学习：构建自适应残差语义提示(ARSP)学习框架，通过类别文本生成可学习的语义残差提示池，并利用可调查询-提示匹配机制建立查询与提示的映射，同时引入基于对比学习的语义一致性损失。3) 任务级学习：引入一种简洁有效的增量提示初始化策略，以确保查询空间内任务间的相关性。", "result": "在YouTube-VIS-2019和YouTube-VIS-2021数据集上的大量实验表明，CRISP在长期持续视频实例分割任务中显著优于现有方法，成功避免了灾难性遗忘，并有效提升了分割和分类性能。", "conclusion": "CRISP是一种在持续视频实例分割中非常有效的方法，它通过独特地处理实例、类别和任务间的混淆，成功实现了新旧知识的平衡学习，显著提高了模型的长期性能并避免了灾难性遗忘。"}}
{"id": "2508.10137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10137", "abs": "https://arxiv.org/abs/2508.10137", "authors": ["Nghia Trung Ngo", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning", "comment": null, "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a \\textbf{M}ultilingual and Scalable Benchmark for\n\\textbf{S}kill-based \\textbf{Co}mmonsense \\textbf{Re}asoning (\\textbf{mSCoRe}).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that \\textbf{mSCoRe} remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.", "AI": {"tldr": "该研究提出了mSCoRe，一个多语言、可扩展的基于技能的常识推理基准，用于系统评估大型语言模型在多语言常识推理中的能力和局限性。", "motivation": "尽管推理增强型大型语言模型在复杂推理任务中表现出色，但其利用不同人类推理技能的机制，特别是涉及跨语言和文化日常知识的多语言常识推理，仍未得到充分研究。", "method": "该研究提出了mSCoRe基准，包含三个核心组件：1) 一个新颖的推理技能分类法，用于细粒度分析模型推理过程；2) 一个为常识推理评估量身定制的鲁棒数据合成管道；3) 一个复杂性扩展框架，使任务难度能随LLM能力的未来提升而动态调整。研究在八个最先进的LLM上进行了广泛实验。", "result": "实验结果表明，mSCoRe对当前模型仍然极具挑战性，尤其是在更高复杂性级别。研究揭示了推理增强型模型在面对细致入微的多语言通用和文化常识时的局限性。", "conclusion": "当前的大型语言模型在处理细致的多语言通用和文化常识方面存在显著局限性。研究提供了对模型推理过程的详细分析，并为未来改进多语言常识推理能力指明了方向。"}}
{"id": "2508.10142", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10142", "abs": "https://arxiv.org/abs/2508.10142", "authors": ["Kartikeya Badola", "Jonathan Simon", "Arian Hosseini", "Sara Marie Mc Carthy", "Tsendsuren Munkhdalai", "Abhimanyu Goyal", "Tomáš Kočiský", "Shyam Upadhyay", "Bahare Fatemi", "Mehran Kazemi"], "title": "Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs", "comment": null, "summary": "Large language models (LLMs) excel at solving problems with clear and\ncomplete statements, but often struggle with nuanced environments or\ninteractive tasks which are common in most real-world scenarios. This\nhighlights the critical need for developing LLMs that can effectively engage in\nlogically consistent multi-turn dialogue, seek information and reason with\nincomplete data. To this end, we introduce a novel benchmark comprising a suite\nof multi-turn tasks each designed to test specific reasoning, interactive\ndialogue, and information-seeking abilities. These tasks have deterministic\nscoring mechanisms, thus eliminating the need for human intervention.\nEvaluating frontier models on our benchmark reveals significant headroom. Our\nanalysis shows that most errors emerge from poor instruction following,\nreasoning failures, and poor planning. This benchmark provides valuable\ninsights into the strengths and weaknesses of current LLMs in handling complex,\ninteractive scenarios and offers a robust platform for future research aimed at\nimproving these critical capabilities.", "AI": {"tldr": "大型语言模型在复杂交互任务中表现不佳，本文提出了一个多轮对话基准测试，以评估其推理、信息获取和交互能力，并发现现有模型仍有巨大提升空间。", "motivation": "大型语言模型在清晰完整的陈述问题上表现出色，但在真实世界常见的细致或交互式任务（需要逻辑一致的多轮对话、信息寻求和不完全数据推理）中表现挣扎。", "method": "引入了一个新的基准测试，包含一系列多轮任务，旨在测试特定的推理、交互式对话和信息寻求能力。这些任务具有确定性评分机制，无需人工干预。", "result": "对前沿模型进行评估显示，模型仍有显著的提升空间。分析表明，大多数错误源于指令遵循不佳、推理失败和规划能力不足。", "conclusion": "该基准测试为当前大型语言模型在处理复杂交互场景时的优势和劣势提供了宝贵见解，并为未来旨在提升这些关键能力的研究提供了强大的平台。"}}
{"id": "2508.10445", "categories": ["cs.CV", "68T07, 68T45, 68U10", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.10445", "abs": "https://arxiv.org/abs/2508.10445", "authors": ["Hang Jin", "Chenqiang Gao", "Junjie Guo", "Fangcen Liu", "Kanghui Tian", "Qinyao Chang"], "title": "DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations", "comment": "9 pages, 5 figures", "summary": "Infrared-visible object detection has shown great potential in real-world\napplications, enabling robust all-day perception by leveraging the\ncomplementary information of infrared and visible images. However, existing\nmethods typically require dual-modality annotations to output detection results\nfor both modalities during prediction, which incurs high annotation costs. To\naddress this challenge, we propose a novel infrared-visible Decoupled Object\nDetection framework with Single-modality Annotations, called DOD-SA. The\narchitecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative\nTeacher-Student Network (CoSD-TSNet), which consists of a single-modality\nbranch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The\nteacher model generates pseudo-labels for the unlabeled modality,\nsimultaneously supporting the training of the student model. The collaborative\ndesign enables cross-modality knowledge transfer from the labeled modality to\nthe unlabeled modality, and facilitates effective SM-to-DMD branch supervision.\nTo further improve the decoupling ability of the model and the pseudo-label\nquality, we introduce a Progressive and Self-Tuning Training Strategy (PaST)\nthat trains the model in three stages: (1) pretraining SM-Branch, (2) guiding\nthe learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In\naddition, we design a Pseudo Label Assigner (PLA) to align and pair labels\nacross modalities, explicitly addressing modality misalignment during training.\nExtensive experiments on the DroneVehicle dataset demonstrate that our method\noutperforms state-of-the-art (SOTA).", "AI": {"tldr": "提出了一种名为DOD-SA的红外-可见光解耦目标检测框架，仅需单模态标注，通过协同教师-学生网络和渐进式自适应训练策略，实现跨模态知识迁移和伪标签生成，有效降低标注成本并提升性能。", "motivation": "现有红外-可见光目标检测方法通常需要双模态标注才能输出两种模态的检测结果，导致高昂的标注成本。", "method": "提出DOD-SA框架，核心是单模态和双模态协同教师-学生网络（CoSD-TSNet），包含单模态分支（SM-Branch）和双模态解耦分支（DMD-Branch）。教师模型为未标注模态生成伪标签，支持学生模型训练。引入渐进式自适应训练策略（PaST），分三阶段训练：预训练SM-Branch、SM-Branch指导DMD-Branch学习、精炼DMD-Branch。设计伪标签分配器（PLA）以解决训练期间的模态不对齐问题。", "result": "在DroneVehicle数据集上进行的大量实验表明，该方法优于现有最先进（SOTA）的方法。", "conclusion": "DOD-SA框架通过创新的单模态标注方法、协同教师-学生网络、渐进式训练策略和伪标签分配器，有效解决了红外-可见光目标检测中高昂标注成本的挑战，并取得了卓越的性能。"}}
{"id": "2508.10161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10161", "abs": "https://arxiv.org/abs/2508.10161", "authors": ["Gal Amram", "Eitan Farchi", "Shmulik Froimovich", "Raviv Gal", "Avi Ziv"], "title": "LaajMeter: A Framework for LaaJ Evaluation", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used as evaluators in natural\nlanguage processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While\neffective in general domains, LaaJs pose significant challenges in\ndomain-specific contexts, where annotated data is scarce and expert evaluation\nis costly. In such cases, meta-evaluation is often performed using metrics that\nhave not been validated for the specific domain in which they are applied. As a\nresult, it becomes difficult to determine which metrics effectively identify\nLaaJ quality, and further, what threshold indicates sufficient evaluator\nperformance. In this work, we introduce LaaJMeter, a simulation-based framework\nfor controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to\ngenerate synthetic data representing virtual models and judges, allowing\nsystematic analysis of evaluation metrics under realistic conditions. This\nhelps practitioners validate and refine LaaJs for specific evaluation tasks:\nthey can test whether their metrics correctly distinguish between better and\nworse (virtual) LaaJs, and estimate appropriate thresholds for evaluator\nadequacy.\n  We demonstrate the utility of LaaJMeter in a code translation task involving\na legacy programming language, showing how different metrics vary in\nsensitivity to evaluator quality. Our results highlight the limitations of\ncommon metrics and the importance of principled metric selection. LaaJMeter\nprovides a scalable and extensible solution for assessing LaaJs in low-resource\nsettings, contributing to the broader effort to ensure trustworthy and\nreproducible evaluation in NLP.", "AI": {"tldr": "本文介绍了LaaJMeter，一个基于模拟的框架，用于在数据稀缺的领域特定场景中，对LLM作为评估器（LaaJ）进行受控的元评估，帮助验证和优化评估指标。", "motivation": "LLM作为评估器（LaaJ）在通用领域有效，但在领域特定背景下，由于缺乏标注数据和专家评估成本高昂，其应用面临挑战。现有元评估指标未经特定领域验证，难以确定哪些指标能有效识别LaaJ质量及合适的性能阈值。", "method": "引入了LaaJMeter，一个基于模拟的框架，能够生成代表虚拟模型和评判者的合成数据。这允许在现实条件下系统分析评估指标，帮助从业者验证和改进LaaJ，测试指标是否能区分优劣，并估算评估器性能的适当阈值。", "result": "通过在遗留编程语言代码翻译任务中的应用，展示了LaaJMeter的实用性，揭示了不同指标对评估器质量的敏感性差异。结果强调了常见指标的局限性以及选择合理指标的重要性。", "conclusion": "LaaJMeter为在低资源环境下评估LaaJ提供了一个可扩展和可扩展的解决方案，有助于确保自然语言处理中评估的可信度和可复现性。"}}
{"id": "2508.10175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10175", "abs": "https://arxiv.org/abs/2508.10175", "authors": ["Lorenzo Proietti", "Stefano Perrella", "Vilém Zouhar", "Roberto Navigli", "Tom Kocmi"], "title": "Estimating Machine Translation Difficulty", "comment": null, "summary": "Machine translation quality has began achieving near-perfect translations in\nsome setups. These high-quality outputs make it difficult to distinguish\nbetween state-of-the-art models and to identify areas for future improvement.\nAutomatically identifying texts where machine translation systems struggle\nholds promise for developing more discriminative evaluations and guiding future\nresearch.\n  We formalize the task of translation difficulty estimation, defining a text's\ndifficulty based on the expected quality of its translations. We introduce a\nnew metric to evaluate difficulty estimators and use it to assess both\nbaselines and novel approaches. Finally, we demonstrate the practical utility\nof difficulty estimators by using them to construct more challenging machine\ntranslation benchmarks. Our results show that dedicated models (dubbed\nSentinel-src) outperform both heuristic-based methods (e.g. word rarity or\nsyntactic complexity) and LLM-as-a-judge approaches. We release two improved\nmodels for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which\ncan be used to scan large collections of texts and select those most likely to\nchallenge contemporary machine translation systems.", "AI": {"tldr": "该研究形式化了机器翻译难度估计任务，引入了新的评估指标，并开发了专用模型（Sentinel-src）来识别对当代机器翻译系统更具挑战性的文本，以改进评估和指导未来研究。", "motivation": "当前机器翻译质量很高，使得难以区分最先进的模型并识别未来改进领域。自动识别机器翻译系统难以处理的文本对于开发更具区分度的评估和指导未来研究至关重要。", "method": "研究者将翻译难度估计任务形式化，根据文本翻译的预期质量来定义其难度。他们引入了一个新的指标来评估难度估计器，并用它来评估基线方法（如词汇稀有度、句法复杂性、LLM作为评判者）和新颖方法（专用模型Sentinel-src）。最后，他们通过使用难度估计器构建更具挑战性的机器翻译基准来展示其实用性。", "result": "专用模型（Sentinel-src）在翻译难度估计方面优于基于启发式的方法（如词汇稀有度或句法复杂性）和LLM作为评判者的方法。研究发布了两个改进的难度估计模型：Sentinel-src-24和Sentinel-src-25。", "conclusion": "专用模型（Sentinel-src）能够有效估计翻译难度，识别对机器翻译系统具有挑战性的文本。这些模型可以用于扫描大量文本集合，选择最有可能挑战当前机器翻译系统的文本，从而构建更具区分度的机器翻译基准，促进未来的研究和发展。"}}
{"id": "2508.10449", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10449", "abs": "https://arxiv.org/abs/2508.10449", "authors": ["Dhruv Dosi", "Rohit Meena", "Param Rajpura", "Yogesh Kumar Meena"], "title": "SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry", "comment": "6 pages, preprint accepted in IEEE SMC 2025", "summary": "Legacy floor plans, often preserved only as scanned documents, remain\nessential resources for architecture, urban planning, and facility management\nin the construction industry. However, the lack of machine-readable floor plans\nrender large-scale interpretation both time-consuming and error-prone.\nAutomated symbol spotting offers a scalable solution by enabling the\nidentification of service key symbols directly from floor plans, supporting\nworkflows such as cost estimation, infrastructure maintenance, and regulatory\ncompliance. This work introduces a labelled Digitised Electrical Layout Plans\n(DELP) dataset comprising 45 scanned electrical layout plans annotated with\n2,450 instances across 34 distinct service key classes. A systematic evaluation\nframework is proposed using pretrained object detection models for DELP\ndataset. Among the models benchmarked, YOLOv8 achieves the highest performance\nwith a mean Average Precision (mAP) of 82.5\\%. Using YOLOv8, we develop\nSkeySpot, a lightweight, open-source toolkit for real-time detection,\nclassification, and quantification of electrical symbols. SkeySpot produces\nstructured, standardised outputs that can be scaled up for interoperable\nbuilding information workflows, ultimately enabling compatibility across\ndownstream applications and regulatory platforms. By lowering dependency on\nproprietary CAD systems and reducing manual annotation effort, this approach\nmakes the digitisation of electrical layouts more accessible to small and\nmedium-sized enterprises (SMEs) in the construction industry, while supporting\nbroader goals of standardisation, interoperability, and sustainability in the\nbuilt environment.", "AI": {"tldr": "该研究引入了一个名为DELP的新数据集和SkeySpot工具包，利用YOLOv8模型实现了对扫描旧版电气平面图中服务关键符号的自动化检测、分类和量化，旨在提高建筑行业中小企业数字化效率。", "motivation": "旧版平面图多为扫描文档，缺乏机器可读性，导致大规模解释耗时且易出错。为了支持成本估算、基础设施维护和法规遵从等工作流程，需要自动化符号识别解决方案。", "method": "1. 构建了DELP数据集，包含45张扫描电气平面图，标注了34类共2,450个符号实例。2. 提出了一个系统评估框架，使用预训练目标检测模型对DELP数据集进行评估。3. 基准测试了多种模型，并基于表现最佳的YOLOv8开发了轻量级、开源的SkeySpot工具包。", "result": "在基准测试的模型中，YOLOv8在DELP数据集上表现最佳，平均精度（mAP）达到82.5%。SkeySpot工具包能够实现电气符号的实时检测、分类和量化，并生成结构化、标准化的输出。", "conclusion": "该方法降低了对专有CAD系统的依赖和手动标注工作量，使中小企业更容易实现电气布局数字化，并支持建筑环境中标准化、互操作性和可持续性的更广泛目标。"}}
{"id": "2508.10186", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10186", "abs": "https://arxiv.org/abs/2508.10186", "authors": ["Abdullah Hashmat", "Muhammad Arham Mirza", "Agha Ali Raza"], "title": "PakBBQ: A Culturally Adapted Bias Benchmark for QA", "comment": "8 pages, 7 figures, 2 tables, Submitted to EMNLP 2025", "summary": "With the widespread adoption of Large Language Models (LLMs) across various\napplications, it is empirical to ensure their fairness across all user\ncommunities. However, most LLMs are trained and evaluated on Western centric\ndata, with little attention paid to low-resource languages and regional\ncontexts. To address this gap, we introduce PakBBQ, a culturally and regionally\nadapted extension of the original Bias Benchmark for Question Answering (BBQ)\ndataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8\ncategories in both English and Urdu, covering eight bias dimensions including\nage, disability, appearance, gender, socio-economic status, religious, regional\naffiliation, and language formality that are relevant in Pakistan. We evaluate\nmultiple multilingual LLMs under both ambiguous and explicitly disambiguated\ncontexts, as well as negative versus non negative question framings. Our\nexperiments reveal (i) an average accuracy gain of 12\\% with disambiguation,\n(ii) consistently stronger counter bias behaviors in Urdu than in English, and\n(iii) marked framing effects that reduce stereotypical responses when questions\nare posed negatively. These findings highlight the importance of contextualized\nbenchmarks and simple prompt engineering strategies for bias mitigation in low\nresource settings.", "AI": {"tldr": "该研究引入了PakBBQ数据集，一个针对巴基斯坦文化和区域背景的偏见评估基准，用于衡量多语言大型语言模型（LLMs）在低资源语言（如乌尔都语）中的偏见。实验发现，消歧、语言和负面提问方式能有效缓解偏见。", "motivation": "随着LLMs的广泛应用，确保其对所有用户群体的公平性至关重要。然而，大多数LLMs在西方中心数据上训练和评估，忽视了低资源语言和区域背景，导致公平性问题。", "method": "引入了PakBBQ数据集，它是Bias Benchmark for Question Answering (BBQ) 的扩展，包含214个模板和17180个英乌双语问答对，涵盖8个与巴基斯坦相关的偏见维度。研究在模糊、明确消歧以及正面/负面提问框架下评估了多个多语言LLMs。", "result": "实验结果显示：(i) 消歧后平均准确率提高12%；(ii) 乌尔都语比英语表现出更强的反偏见行为；(iii) 负面提问显著减少了刻板印象反应。", "conclusion": "研究强调了语境化基准和简单的提示工程策略对于在低资源环境中缓解LLMs偏见的重要性。"}}
{"id": "2508.10180", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10180", "abs": "https://arxiv.org/abs/2508.10180", "authors": ["Wenlong Deng", "Jiaming Zhang", "Qi Zeng", "Christos Thrampoulidis", "Boying Gong", "Xiaoxiao Li"], "title": "Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs", "comment": null, "summary": "Quantifying the influence of individual training samples is essential for\nenhancing the transparency and accountability of large language models (LLMs)\nand vision-language models (VLMs). However, existing data valuation methods\noften rely on Hessian information or model retraining, making them\ncomputationally prohibitive for billion-parameter models. In this work, we\nintroduce For-Value, a forward-only data valuation framework that enables\nscalable and efficient influence estimation for both LLMs and VLMs. By\nleveraging the rich representations of modern foundation models, For-Value\ncomputes influence scores using a simple closed-form expression based solely on\na single forward pass, thereby eliminating the need for costly gradient\ncomputations. Our theoretical analysis demonstrates that For-Value accurately\nestimates per-sample influence by capturing alignment in hidden representations\nand prediction errors between training and validation samples. Extensive\nexperiments show that For-Value matches or outperforms gradient-based baselines\nin identifying impactful fine-tuning examples and effectively detecting\nmislabeled data.", "AI": {"tldr": "For-Value是一种前向数据评估框架，通过单次前向传播高效量化大型语言模型和视觉语言模型中训练样本的影响力，无需昂贵的梯度计算。", "motivation": "量化单个训练样本的影响力对于提高大型语言模型（LLMs）和视觉语言模型（VLMs）的透明度和可解释性至关重要。然而，现有数据评估方法依赖于Hessian信息或模型再训练，对于数十亿参数的模型而言计算成本过高。", "method": "引入了For-Value框架，该框架利用现代基础模型的丰富表示，通过一个简单的闭式表达式（仅基于单次前向传播）计算影响力分数，从而避免了昂贵的梯度计算。理论分析表明，For-Value通过捕获训练样本和验证样本之间隐藏表示和预测误差的对齐来准确估计每个样本的影响力。", "result": "For-Value能够准确估计每个样本的影响力，并且在识别有影响力的微调示例和有效检测错误标记数据方面，其性能与基于梯度的基线方法相当或更优。", "conclusion": "For-Value提供了一种可扩展且高效的数据评估方法，能够量化LLMs和VLMs中训练样本的影响力，从而增强模型的透明度和可解释性。"}}
{"id": "2508.10450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10450", "abs": "https://arxiv.org/abs/2508.10450", "authors": ["Pablo Hernández-Cámara", "Jesus Malo", "Valero Laparra"], "title": "From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images", "comment": null, "summary": "A number of scientists suggested that human visual perception may emerge from\nimage statistics, shaping efficient neural representations in early vision. In\nthis work, a bio-inspired architecture that can accommodate several known facts\nin the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for\ndifferent tasks related to image reconstruction: autoencoding, denoising,\ndeblurring, and sparsity regularization. Our results show that the encoder\nstage (V1-like layer) consistently exhibits the highest correlation with human\nperceptual judgments on image distortion despite not using perceptual\ninformation in the initialization or training. This alignment exhibits an\noptimum for moderate noise, blur and sparsity. These findings suggest that the\nvisual system may be tuned to remove those particular levels of distortion with\nthat level of sparsity and that biologically inspired models can learn\nperceptual metrics without human supervision.", "AI": {"tldr": "生物启发模型PerceptNet在图像重建任务中，其编码器表现出与人类感知判断高度相关的特性，表明视觉系统可能通过优化来处理特定失真。", "motivation": "许多科学家认为人类视觉感知源于图像统计，并塑造了早期视觉中高效的神经表征。本研究旨在通过构建一个生物启发模型来验证这一观点。", "method": "开发了一个名为PerceptNet的生物启发架构，该架构能模拟视网膜-V1皮层的已知特性。该模型被端到端优化，用于自编码、去噪、去模糊和稀疏性正则化等图像重建任务。", "result": "研究发现，PerceptNet的编码器阶段（类似V1的层）与人类对图像失真的感知判断表现出最高的一致性，即使在初始化或训练中未使用感知信息。这种一致性在中等程度的噪声、模糊和稀疏性下达到最佳。", "conclusion": "这些发现表明视觉系统可能被调整以去除特定水平的失真和稀疏性，并且生物启发模型无需人类监督即可学习感知度量。"}}
{"id": "2508.10192", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2508.10192", "abs": "https://arxiv.org/abs/2508.10192", "authors": ["Igor Halperin"], "title": "Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models", "comment": "24 pages, 3 figures", "summary": "The proliferation of Large Language Models (LLMs) is challenged by\nhallucinations, critical failure modes where models generate non-factual,\nnonsensical or unfaithful text. This paper introduces Semantic Divergence\nMetrics (SDM), a novel lightweight framework for detecting Faithfulness\nHallucinations -- events of severe deviations of LLMs responses from input\ncontexts. We focus on a specific implementation of these LLM errors,\n{confabulations, defined as responses that are arbitrary and semantically\nmisaligned with the user's query. Existing methods like Semantic Entropy test\nfor arbitrariness by measuring the diversity of answers to a single, fixed\nprompt. Our SDM framework improves upon this by being more prompt-aware: we\ntest for a deeper form of arbitrariness by measuring response consistency not\nonly across multiple answers but also across multiple, semantically-equivalent\nparaphrases of the original prompt. Methodologically, our approach uses joint\nclustering on sentence embeddings to create a shared topic space for prompts\nand answers. A heatmap of topic co-occurances between prompts and responses can\nbe viewed as a quantified two-dimensional visualization of the user-machine\ndialogue. We then compute a suite of information-theoretic metrics to measure\nthe semantic divergence between prompts and responses. Our practical score,\n$\\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein\ndistance to quantify this divergence, with a high score indicating a\nFaithfulness hallucination. Furthermore, we identify the KL divergence\nKL(Answer $||$ Prompt) as a powerful indicator of \\textbf{Semantic\nExploration}, a key signal for distinguishing different generative behaviors.\nThese metrics are further combined into the Semantic Box, a diagnostic\nframework for classifying LLM response types, including the dangerous,\nconfident confabulation.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2508.10222", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.10222", "abs": "https://arxiv.org/abs/2508.10222", "authors": ["Ethan Gordon", "Nishank Kuppa", "Rigved Tummala", "Sriram Anasuri"], "title": "Understanding Textual Emotion Through Emoji Prediction", "comment": null, "summary": "This project explores emoji prediction from short text sequences using four\ndeep learning architectures: a feed-forward network, CNN, transformer, and\nBERT. Using the TweetEval dataset, we address class imbalance through focal\nloss and regularization techniques. Results show BERT achieves the highest\noverall performance due to its pre-training advantage, while CNN demonstrates\nsuperior efficacy on rare emoji classes. This research shows the importance of\narchitecture selection and hyperparameter tuning for sentiment-aware emoji\nprediction, contributing to improved human-computer interaction.", "AI": {"tldr": "本项目使用四种深度学习架构（前馈网络、CNN、Transformer、BERT）在短文本序列上进行表情符号预测，并通过焦点损失和正则化处理类别不平衡问题。", "motivation": "通过情感感知的表情符号预测来改善人机交互。", "method": "使用TweetEval数据集，探索了前馈网络、CNN、Transformer和BERT四种深度学习架构。通过焦点损失（focal loss）和正则化技术解决类别不平衡问题。", "result": "BERT由于其预训练优势，取得了最高的整体性能；CNN在稀有表情符号类别上表现出卓越的效力。", "conclusion": "研究表明，架构选择和超参数调整对于情感感知的表情符号预测至关重要，有助于改善人机交互。"}}
{"id": "2508.10453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10453", "abs": "https://arxiv.org/abs/2508.10453", "authors": ["Qiang Zhu", "Xiandong Meng", "Yuxian Jiang", "Fan Zhang", "David Bull", "Shuyuan Zhu", "Bing Zeng"], "title": "Trajectory-aware Shifted State Space Models for Online Video Super-Resolution", "comment": null, "summary": "Online video super-resolution (VSR) is an important technique for many\nreal-world video processing applications, which aims to restore the current\nhigh-resolution video frame based on temporally previous frames. Most of the\nexisting online VSR methods solely employ one neighboring previous frame to\nachieve temporal alignment, which limits long-range temporal modeling of\nvideos. Recently, state space models (SSMs) have been proposed with linear\ncomputational complexity and a global receptive field, which significantly\nimprove computational efficiency and performance. In this context, this paper\npresents a novel online VSR method based on Trajectory-aware Shifted SSMs\n(TS-Mamba), leveraging both long-term trajectory modeling and low-complexity\nMamba to achieve efficient spatio-temporal information aggregation.\nSpecifically, TS-Mamba first constructs the trajectories within a video to\nselect the most similar tokens from the previous frames. Then, a\nTrajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed\nshifted SSMs blocks is employed to aggregate the selected tokens. The shifted\nSSMs blocks are designed based on Hilbert scannings and corresponding shift\noperations to compensate for scanning losses and strengthen the spatial\ncontinuity of Mamba. Additionally, we propose a trajectory-aware loss function\nto supervise the trajectory generation, ensuring the accuracy of token\nselection when training our model. Extensive experiments on three widely used\nVSR test datasets demonstrate that compared with six online VSR benchmark\nmodels, our TS-Mamba achieves state-of-the-art performance in most cases and\nover 22.7\\% complexity reduction (in MACs). The source code for TS-Mamba will\nbe available at https://github.com.", "AI": {"tldr": "本文提出了一种名为TS-Mamba的新型在线视频超分辨率（VSR）方法，结合长程轨迹建模和低复杂度Mamba模型，实现了高效的时空信息聚合，并在性能和计算效率上达到SOTA。", "motivation": "现有的在线VSR方法通常只使用一个相邻的前一帧进行时间对齐，限制了视频的长程时间建模。尽管状态空间模型（SSMs）因其线性计算复杂度和全局感受野而提高了效率和性能，但如何将其有效应用于在线VSR并解决现有方法的局限性是一个挑战。", "method": "TS-Mamba方法首先通过构建视频内的轨迹来选择来自先前帧的最相似的tokens。然后，利用一个轨迹感知移位Mamba聚合（TSMA）模块来聚合这些选定的tokens，该模块包含基于Hilbert扫描和相应移位操作设计的移位SSMs块，以弥补扫描损失并增强Mamba的空间连续性。此外，还提出了一种轨迹感知损失函数来监督轨迹生成，确保token选择的准确性。", "result": "在三个广泛使用的VSR测试数据集上进行的实验表明，与六个在线VSR基准模型相比，TS-Mamba在大多数情况下实现了最先进的性能，并实现了超过22.7%的复杂度降低（以MACs衡量）。", "conclusion": "TS-Mamba通过结合长程轨迹建模和高效的Mamba架构，成功解决了在线VSR中的长程时间建模和计算效率问题，实现了卓越的性能和显著的复杂度降低，为在线视频处理应用提供了强大的技术支持。"}}
{"id": "2508.10226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10226", "abs": "https://arxiv.org/abs/2508.10226", "authors": ["Andrew X. Chen", "Guillermo Horga", "Sean Escola"], "title": "Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia", "comment": null, "summary": "Patients who are at clinical high risk (CHR) for schizophrenia need close\nmonitoring of their symptoms to inform appropriate treatments. The Brief\nPsychiatric Rating Scale (BPRS) is a validated, commonly used research tool for\nmeasuring symptoms in patients with schizophrenia and other psychotic\ndisorders; however, it is not commonly used in clinical practice as it requires\na lengthy structured interview. Here, we utilize large language models (LLMs)\nto predict BPRS scores from clinical interview transcripts in 409 CHR patients\nfrom the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.\nDespite the interviews not being specifically structured to measure the BPRS,\nthe zero-shot performance of the LLM predictions compared to the true\nassessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and\nintra-rater reliability. We further demonstrate that LLMs have substantial\npotential to improve and standardize the assessment of CHR patients via their\naccuracy in assessing the BPRS in foreign languages (median concordance: 0.88,\nICC: 0.70), and integrating longitudinal information in a one-shot or few-shot\nlearning approach.", "AI": {"tldr": "研究利用大型语言模型（LLM）从临床访谈转录本中预测精神分裂症临床高危（CHR）患者的简明精神病评定量表（BPRS）分数，结果显示LLM的预测性能接近人类评估者信度，并能支持多语言和纵向评估。", "motivation": "简明精神病评定量表（BPRS）是评估精神分裂症及其他精神病性障碍症状的有效工具，但因需要耗时的结构化访谈，在临床实践中不常使用。因此，需要一种更高效、标准化的方法来监测临床高危患者的症状。", "method": "研究利用大型语言模型（LLM），对来自加速药物合作精神分裂症（AMP-SCZ）队列的409名临床高危（CHR）患者的临床访谈转录本进行分析，以零样本（zero-shot）方式预测BPRS分数。此外，还评估了LLM在不同语言和整合纵向信息方面的表现。", "result": "LLM的零样本预测与真实评估表现出高一致性（中位一致性：0.84，ICC：0.73），接近人类评估者间的信度和评估者内部信度。研究还表明，LLM在评估外语（中位一致性：0.88，ICC：0.70）和通过一次性或少样本学习整合纵向信息方面也具有显著潜力。", "conclusion": "大型语言模型（LLM）在精神分裂症临床高危患者的症状评估中具有巨大潜力，能够显著提高BPRS评估的效率和标准化程度，并支持多语言和纵向数据整合。"}}
{"id": "2508.10246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10246", "abs": "https://arxiv.org/abs/2508.10246", "authors": ["Daniel Huang", "Hyoun-A Joo"], "title": "A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona", "comment": "14 pages, 14 figures. submitted to UGA Working Papers in Linguistics\n  2025", "summary": "This study explores language change and variation in Toki Pona, a constructed\nlanguage with approximately 120 core words. Taking a computational and\ncorpus-based approach, the study examines features including fluid word classes\nand transitivity in order to examine (1) changes in preferences of content\nwords for different syntactic positions over time and (2) variation in usage\nacross different corpora. The results suggest that sociolinguistic factors\ninfluence Toki Pona in the same way as natural languages, and that even\nconstructed linguistic systems naturally evolve as communities use them.", "AI": {"tldr": "本研究通过计算和语料库方法，探讨了人造语言Toki Pona的语言变化和变异，发现其演变受社会语言学因素影响，与自然语言类似。", "motivation": "探讨人造语言Toki Pona在社区使用过程中，其语言系统是否会像自然语言一样发生演变和变异，以及社会语言学因素对其影响。", "method": "采用计算和语料库方法，分析Toki Pona中词类的流动性和及物性特征，以考察内容词在不同句法位置上的偏好随时间的变化，以及不同语料库之间的用法差异。", "result": "研究结果表明，社会语言学因素以与自然语言相同的方式影响Toki Pona，即使是人造语言系统，在社区使用过程中也会自然演变。", "conclusion": "社区的使用会使人造语言系统自然演变，并受社会语言学因素影响，这与自然语言的演变规律相似。"}}
{"id": "2508.10457", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10457", "abs": "https://arxiv.org/abs/2508.10457", "authors": ["Hanna Herasimchyk", "Robin Labryga", "Tomislav Prusina"], "title": "Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers", "comment": "Accepted for publication at: LifeCLEF Lab at CLEF 2025 Working Notes,\n  2025, Madrid, Spain", "summary": "We present a multi-head vision transformer approach for multi-label plant\nspecies prediction in vegetation plot images, addressing the PlantCLEF 2025\nchallenge. The task involves training models on single-species plant images\nwhile testing on multi-species quadrat images, creating a drastic domain shift.\nOur methodology leverages a pre-trained DINOv2 Vision Transformer Base\n(ViT-B/14) backbone with multiple classification heads for species, genus, and\nfamily prediction, utilizing taxonomic hierarchies. Key contributions include\nmulti-scale tiling to capture plants at different scales, dynamic threshold\noptimization based on mean prediction length, and ensemble strategies through\nbagging and Hydra model architectures. The approach incorporates various\ninference techniques including image cropping to remove non-plant artifacts,\ntop-n filtering for prediction constraints, and logit thresholding strategies.\nExperiments were conducted on approximately 1.4 million training images\ncovering 7,806 plant species. Results demonstrate strong performance, making\nour submission 3rd best on the private leaderboard. Our code is available at\nhttps://github.com/geranium12/plant-clef-2025/tree/v1.0.0.", "AI": {"tldr": "本文提出了一种多头视觉Transformer方法，用于植物图像中的多标签植物物种预测，以应对PlantCLEF 2025挑战中的域偏移问题。", "motivation": "PlantCLEF 2025挑战任务要求模型在单物种植物图像上训练，但在多物种样方图像上进行测试，这导致了显著的域偏移，需要开发鲁棒的预测方法。", "method": "该方法利用预训练的DINOv2 ViT-B/14作为骨干网络，并结合多分类头（用于物种、属、科预测）以利用分类学层次。关键技术包括多尺度平铺、基于平均预测长度的动态阈值优化、通过bagging和Hydra模型架构的集成策略，以及图像裁剪、top-n过滤和logit阈值等推理技术。", "result": "该方法在包含约140万张训练图像和7,806种植物物种的数据集上进行了实验，结果显示出强大的性能，在私人排行榜上排名第三。", "conclusion": "所提出的多头视觉Transformer方法，结合多尺度处理、动态阈值和集成策略，有效解决了PlantCLEF 2025挑战中的多标签植物物种预测和域偏移问题，取得了优异的性能。"}}
{"id": "2508.10304", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10304", "abs": "https://arxiv.org/abs/2508.10304", "authors": ["Gustavo Bonil", "Simone Hashiguti", "Jhessica Silva", "João Gondim", "Helena Maia", "Nádia Silva", "Helio Pedrini", "Sandra Avila"], "title": "Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race", "comment": "29 pages, 3 figures", "summary": "With the advance of Artificial Intelligence (AI), Large Language Models\n(LLMs) have gained prominence and been applied in diverse contexts. As they\nevolve into more sophisticated versions, it is essential to assess whether they\nreproduce biases, such as discrimination and racialization, while maintaining\nhegemonic discourses. Current bias detection approaches rely mostly on\nquantitative, automated methods, which often overlook the nuanced ways in which\nbiases emerge in natural language. This study proposes a qualitative,\ndiscursive framework to complement such methods. Through manual analysis of\nLLM-generated short stories featuring Black and white women, we investigate\ngender and racial biases. We contend that qualitative methods such as the one\nproposed here are fundamental to help both developers and users identify the\nprecise ways in which biases manifest in LLM outputs, thus enabling better\nconditions to mitigate them. Results show that Black women are portrayed as\ntied to ancestry and resistance, while white women appear in self-discovery\nprocesses. These patterns reflect how language models replicate crystalized\ndiscursive representations, reinforcing essentialization and a sense of social\nimmobility. When prompted to correct biases, models offered superficial\nrevisions that maintained problematic meanings, revealing limitations in\nfostering inclusive narratives. Our results demonstrate the ideological\nfunctioning of algorithms and have significant implications for the ethical use\nand development of AI. The study reinforces the need for critical,\ninterdisciplinary approaches to AI design and deployment, addressing how\nLLM-generated discourses reflect and perpetuate inequalities.", "AI": {"tldr": "本研究提出并应用定性分析框架，揭示大型语言模型（LLMs）在生成故事时如何再现并固化种族和性别偏见，强调现有自动化检测方法的不足。", "motivation": "随着LLMs的广泛应用，评估它们是否会复制偏见（如歧视和种族化）并维持霸权话语变得至关重要。当前主流的定量、自动化偏见检测方法常忽略自然语言中偏见出现的细微方式，因此需要更深入的分析方法。", "method": "本研究提出一种定性、话语分析框架来补充现有方法。通过人工分析LLM生成的涉及黑人女性和白人女性的短篇故事，调查了性别和种族偏见。研究还测试了模型在被提示纠正偏见时的表现。", "result": "结果显示，黑人女性被描绘为与祖先和抵抗相关联，而白人女性则出现在自我发现的过程中。这些模式反映了语言模型如何复制固化的语篇表征，强化了本质化和社会固化感。当被要求纠正偏见时，模型提供了肤浅的修改，但保留了有问题含义，暴露出在培养包容性叙事方面的局限性。", "conclusion": "定性方法对于帮助开发者和用户识别LLM输出中偏见的精确表现形式至关重要，从而更好地缓解偏见。研究结果揭示了算法的意识形态功能，对AI的伦理使用和发展具有重要意义。强调了在AI设计和部署中采用批判性、跨学科方法以解决LLM生成话语如何反映和延续不平等的需求。"}}
{"id": "2508.10295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10295", "abs": "https://arxiv.org/abs/2508.10295", "authors": ["Christian M. Angel", "Francis Ferraro"], "title": "Inductive Bias Extraction and Matching for LLM Prompts", "comment": null, "summary": "The active research topic of prompt engineering makes it evident that LLMs\nare sensitive to small changes in prompt wording. A portion of this can be\nascribed to the inductive bias that is present in the LLM. By using an LLM's\noutput as a portion of its prompt, we can more easily create satisfactory\nwording for prompts. This has the effect of creating a prompt that matches the\ninductive bias in model. Empirically, we show that using this Inductive Bias\nExtraction and Matching strategy improves LLM Likert ratings used for\nclassification by up to 19% and LLM Likert ratings used for ranking by up to\n27%.", "AI": {"tldr": "该研究提出一种“归纳偏置提取与匹配”策略，通过使用LLM自身输出作为部分提示，来创建更符合模型归纳偏置的提示词，从而显著提高LLM在分类和排序任务上的表现。", "motivation": "LLM对提示词的微小变化非常敏感，这部分归因于其固有的归纳偏置。如何高效地创建令人满意的提示词是一个活跃的研究课题。", "method": "通过将LLM的输出作为其提示词的一部分，可以更容易地创建出与模型归纳偏置相匹配的提示词。这种方法被称为“归纳偏置提取与匹配”。", "result": "实验结果表明，该策略将LLM在分类任务上的Likert评分提高了高达19%，在排序任务上的Likert评分提高了高达27%。", "conclusion": "利用LLM自身的输出调整提示词，使其与模型的归纳偏置相匹配，能够有效提升LLM在分类和排序等任务中的性能。"}}
{"id": "2508.10464", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10464", "abs": "https://arxiv.org/abs/2508.10464", "authors": ["Bella Specktor-Fadida", "Malte Hoffmann"], "title": "SingleStrip: learning skull-stripping from a single labeled example", "comment": "Accepted as an oral presentation to the MICCAI 2025 Data Engineering\n  in Medical Imaging (DEMI) workshop", "summary": "Deep learning segmentation relies heavily on labeled data, but manual\nlabeling is laborious and time-consuming, especially for volumetric images such\nas brain magnetic resonance imaging (MRI). While recent domain-randomization\ntechniques alleviate the dependency on labeled data by synthesizing diverse\ntraining images from label maps, they offer limited anatomical variability when\nvery few label maps are available. Semi-supervised self-training addresses\nlabel scarcity by iteratively incorporating model predictions into the training\nset, enabling networks to learn from unlabeled data. In this work, we combine\ndomain randomization with self-training to train three-dimensional\nskull-stripping networks using as little as a single labeled example. First, we\nautomatically bin voxel intensities, yielding labels we use to synthesize\nimages for training an initial skull-stripping model. Second, we train a\nconvolutional autoencoder (AE) on the labeled example and use its\nreconstruction error to assess the quality of brain masks predicted for\nunlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the\nnetwork, achieving skull-stripping performance on out-of-distribution data that\napproaches models trained with more labeled images. We compare AE-based ranking\nto consistency-based ranking under test-time augmentation, finding that the AE\napproach yields a stronger correlation with segmentation accuracy. Our results\nhighlight the potential of combining domain randomization and AE-based quality\ncontrol to enable effective semi-supervised segmentation from extremely limited\nlabeled data. This strategy may ease the labeling burden that slows progress in\nstudies involving new anatomical structures or emerging imaging techniques.", "AI": {"tldr": "该研究结合域随机化和自训练，并引入基于自动编码器（AE）的质量控制，实现了仅使用极少量（甚至一个）标记数据进行三维颅骨剥离的半监督分割。", "motivation": "深度学习分割严重依赖标记数据，但手动标记（尤其是对于脑部MRI等体积图像）耗时耗力。现有方法如域随机化在标记图极少时解剖变异性有限，而半监督自训练虽能利用未标记数据，但仍需解决伪标签质量评估问题。", "method": "1. 自动对体素强度进行分箱，从一个标记样本生成标签图，并利用域随机化合成多样化图像训练初始颅骨剥离模型。2. 在标记样本上训练一个卷积自动编码器（AE）。3. 利用AE的重建误差评估未标记数据预测脑掩膜（伪标签）的质量。4. 选择排名靠前的伪标签来微调网络。5. 将AE-based排名与基于测试时增强的一致性排名进行比较。", "result": "该方法在分布外数据上实现了接近使用更多标记图像训练的模型性能的颅骨剥离效果。研究发现，基于AE的排名与分割精度具有更强的相关性。", "conclusion": "结合域随机化和基于AE的质量控制，能够从极其有限的标记数据中实现有效的半监督分割。该策略有望减轻新解剖结构或新兴成像技术研究中的标记负担。"}}
{"id": "2508.10308", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10308", "abs": "https://arxiv.org/abs/2508.10308", "authors": ["Sihang Zeng", "Kai Tian", "Kaiyan Zhang", "Yuru wang", "Junqi Gao", "Runze Liu", "Sa Yang", "Jingxuan Li", "Xinwei Long", "Jiaheng Ma", "Biqing Qi", "Bowen Zhou"], "title": "ReviewRL: Towards Automated Scientific Review with RL", "comment": "13 pages, 5 figures", "summary": "Peer review is essential for scientific progress but faces growing challenges\ndue to increasing submission volumes and reviewer fatigue. Existing automated\nreview approaches struggle with factual accuracy, rating consistency, and\nanalytical depth, often generating superficial or generic feedback lacking the\ninsights characteristic of high-quality human reviews. We introduce ReviewRL, a\nreinforcement learning framework for generating comprehensive and factually\ngrounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP\nretrieval-augmented context generation pipeline that incorporates relevant\nscientific literature, (2) supervised fine-tuning that establishes foundational\nreviewing capabilities, and (3) a reinforcement learning procedure with a\ncomposite reward function that jointly enhances review quality and rating\naccuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL\nsignificantly outperforms existing methods across both rule-based metrics and\nmodel-based quality assessments. ReviewRL establishes a foundational framework\nfor RL-driven automatic critique generation in scientific discovery,\ndemonstrating promising potential for future development in this domain. The\nimplementation of ReviewRL will be released at GitHub.", "AI": {"tldr": "ReviewRL是一个基于强化学习的框架，旨在生成全面且事实准确的科学论文评审，以应对同行评审面临的挑战。", "motivation": "科学同行评审面临提交量增加和审稿人疲劳的挑战，而现有自动化评审方法在事实准确性、评分一致性和分析深度方面表现不佳，缺乏高质量人工评审的洞察力。", "method": "ReviewRL结合了三部分：1) ArXiv-MCP检索增强的上下文生成管道，以整合相关科学文献；2) 监督微调，建立基础评审能力；3) 带有复合奖励函数的强化学习过程，共同提高评审质量和评分准确性。", "result": "在ICLR 2025论文上的实验表明，ReviewRL在基于规则的指标和基于模型的质量评估方面均显著优于现有方法。", "conclusion": "ReviewRL为科学发现中的RL驱动自动评论生成建立了一个基础框架，展示了在该领域未来发展的巨大潜力。"}}
{"id": "2508.10311", "categories": ["cs.CL", "68T50", "I.7.5"], "pdf": "https://arxiv.org/pdf/2508.10311", "abs": "https://arxiv.org/abs/2508.10311", "authors": ["Xuan Li", "Jialiang Dong", "Raymond Wong"], "title": "From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis", "comment": "8 pages, 5 figures, 28th European Conference on Artificial\n  Intelligence (ECAI-2025)", "summary": "Documents are core carriers of information and knowl-edge, with broad\napplications in finance, healthcare, and scientific research. Tables, as the\nmain medium for structured data, encapsulate key information and are among the\nmost critical document components. Existing studies largely focus on\nsurface-level tasks such as layout analysis, table detection, and data\nextraction, lacking deep semantic parsing of tables and their contextual\nassociations. This limits advanced tasks like cross-paragraph data\ninterpretation and context-consistent analysis. To address this, we propose\nDOTABLER, a table-centric semantic document parsing framework designed to\nuncover deep semantic links between tables and their context. DOTABLER\nleverages a custom dataset and domain-specific fine-tuning of pre-trained\nmodels, integrating a complete parsing pipeline to identify context segments\nsemantically tied to tables. Built on this semantic understanding, DOTABLER\nimplements two core functionalities: table-centric document structure parsing\nand domain-specific table retrieval, delivering comprehensive table-anchored\nsemantic analysis and precise extraction of semantically relevant tables.\nEvaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,\nDOTABLER achieves over 90% Precision and F1 scores, demonstrating superior\nperformance in table-context semantic analysis and deep document parsing\ncompared to advanced models such as GPT-4o.", "AI": {"tldr": "DOTABLER是一个以表格为中心的语义文档解析框架，旨在发现表格与其上下文之间的深层语义联系，并支持高级文档理解和信息提取任务。", "motivation": "现有研究主要集中在表格的表面任务（如布局分析、检测、数据提取），缺乏对表格及其上下文关联的深层语义解析，这限制了跨段落数据解释和上下文一致性分析等高级任务的实现。", "method": "该研究提出了DOTABLER框架，利用自定义数据集和预训练模型的领域特定微调，整合完整的解析管道来识别与表格语义相关的上下文片段。在此语义理解基础上，DOTABLER实现了两个核心功能：以表格为中心的文档结构解析和领域特定表格检索。", "result": "在近4000页（包含1000多个表格）的真实PDF文件上进行评估，DOTABLER在表格-上下文语义分析和深度文档解析方面取得了超过90%的精确率和F1分数，性能优于GPT-4o等先进模型。", "conclusion": "DOTABLER框架在表格-上下文语义分析和深度文档解析方面表现出卓越性能，能够实现全面的以表格为中心的语义分析和精确的语义相关表格提取。"}}
{"id": "2508.10469", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10469", "abs": "https://arxiv.org/abs/2508.10469", "authors": ["Maimunatu Tunau", "Vincent Gbouna Zakka", "Zhuangzhuang Dai"], "title": "Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition", "comment": null, "summary": "Human Action Recognition (HAR) plays a crucial role in healthcare, fitness\ntracking, and ambient assisted living technologies. While traditional vision\nbased HAR systems are effective, they pose privacy concerns. mmWave radar\nsensors offer a privacy preserving alternative but present challenges due to\nthe sparse and noisy nature of their point cloud data. In the literature, three\nprimary data processing methods: Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering\nhave been widely used to improve the quality and continuity of radar data.\nHowever, a comprehensive evaluation of these methods, both individually and in\ncombination, remains lacking. This paper addresses that gap by conducting a\ndetailed performance analysis of the three methods using the MiliPoint dataset.\nWe evaluate each method individually, all possible pairwise combinations, and\nthe combination of all three, assessing both recognition accuracy and\ncomputational cost. Furthermore, we propose targeted enhancements to the\nindividual methods aimed at improving accuracy. Our results provide crucial\ninsights into the strengths and trade-offs of each method and their\nintegrations, guiding future work on mmWave based HAR systems", "AI": {"tldr": "本文对毫米波雷达人体行为识别中常用的DBSCAN、匈牙利算法和卡尔曼滤波三种数据处理方法进行了全面评估，包括单独使用、两两组合及全部组合的性能，并提出了改进方案，为未来系统开发提供指导。", "motivation": "传统的视觉HAR系统存在隐私问题，而毫米波雷达传感器虽能保护隐私，但其点云数据稀疏且噪声大。文献中虽有多种数据处理方法，但缺乏对其单独和组合性能的全面评估。", "method": "使用MiliPoint数据集，对DBSCAN、匈牙利算法和卡尔曼滤波三种方法进行评估。评估方式包括：单独使用、所有可能的两两组合以及三种方法全部组合。评估指标为识别准确率和计算成本。此外，还提出了针对单一方法的改进措施以提高准确率。", "result": "研究结果深入揭示了每种方法及其组合的优势和权衡，为毫米波雷达HAR系统的未来工作提供了关键见解。", "conclusion": "本研究通过对现有毫米波雷达数据处理方法的全面评估和改进，为未来基于毫米波雷达的人体行为识别系统开发提供了重要指导。"}}
{"id": "2508.10404", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10404", "abs": "https://arxiv.org/abs/2508.10404", "authors": ["Huizhen Shu", "Xuying Li", "Qirui Wang", "Yuji Kosuga", "Mengqiu Tian", "Zhuo Li"], "title": "Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation", "comment": null, "summary": "With the rapid proliferation of Natural Language Processing (NLP), especially\nLarge Language Models (LLMs), generating adversarial examples to jailbreak LLMs\nremains a key challenge for understanding model vulnerabilities and improving\nrobustness. In this context, we propose a new black-box attack method that\nleverages the interpretability of large models. We introduce the Sparse Feature\nPerturbation Framework (SFPF), a novel approach for adversarial text generation\nthat utilizes sparse autoencoders to identify and manipulate critical features\nin text. After using the SAE model to reconstruct hidden layer representations,\nwe perform feature clustering on the successfully attacked texts to identify\nfeatures with higher activations. These highly activated features are then\nperturbed to generate new adversarial texts. This selective perturbation\npreserves the malicious intent while amplifying safety signals, thereby\nincreasing their potential to evade existing defenses. Our method enables a new\nred-teaming strategy that balances adversarial effectiveness with safety\nalignment. Experimental results demonstrate that adversarial texts generated by\nSFPF can bypass state-of-the-art defense mechanisms, revealing persistent\nvulnerabilities in current NLP systems.However, the method's effectiveness\nvaries across prompts and layers, and its generalizability to other\narchitectures and larger models remains to be validated.", "AI": {"tldr": "提出了一种名为SFPF的黑盒对抗攻击方法，利用稀疏自编码器识别和扰动LLM文本中的关键特征，以生成能绕过现有防御的对抗样本，揭示模型漏洞。", "motivation": "随着NLP特别是LLM的快速发展，生成对抗性示例来“越狱”LLM仍然是理解模型漏洞和提高鲁棒性的关键挑战。", "method": "引入稀疏特征扰动框架（SFPF），利用稀疏自编码器（SAE）重建隐藏层表示，对成功攻击的文本进行特征聚类以识别高激活特征。然后，选择性地扰动这些高激活特征来生成新的对抗文本，同时保留恶意意图并放大安全信号。", "result": "SFPF生成的对抗文本能够绕过最先进的防御机制，揭示了当前NLP系统中持续存在的漏洞。然而，该方法的有效性因提示和层而异，其对其他架构和更大模型的泛化能力仍有待验证。", "conclusion": "该方法提供了一种新的红队策略，平衡了对抗有效性与安全对齐，并揭示了LLM防御机制中存在的持续漏洞。"}}
{"id": "2508.10312", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10312", "abs": "https://arxiv.org/abs/2508.10312", "authors": ["Minhao Wang", "Yunhang He", "Cong Xu", "Zhangchi Zhu", "Wei Zhang"], "title": "Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation", "comment": "12 pages, 8 figures", "summary": "Recommender systems in concert with Large Language Models (LLMs) present\npromising avenues for generating semantically-informed recommendations.\nHowever, LLM-based recommenders exhibit a tendency to overemphasize semantic\ncorrelations within users' interaction history. When taking pretrained\ncollaborative ID embeddings as input, LLM-based recommenders progressively\nweaken the inherent collaborative signals as the embeddings propagate through\nLLM backbones layer by layer, as opposed to traditional Transformer-based\nsequential models in which collaborative signals are typically preserved or\neven enhanced for state-of-the-art performance. To address this limitation, we\nintroduce FreLLM4Rec, an approach designed to balance semantic and\ncollaborative information from a spectral perspective. Item embeddings that\nincorporate both semantic and collaborative information are first purified\nusing a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant\nhigh-frequency noise. Temporal Frequency Modulation (TFM) then actively\npreserves collaborative signal layer by layer. Note that the collaborative\npreservation capability of TFM is theoretically guaranteed by establishing a\nconnection between the optimal but hard-to-implement local graph fourier\nfilters and the suboptimal yet computationally efficient frequency-domain\nfilters. Extensive experiments on four benchmark datasets demonstrate that\nFreLLM4Rec successfully mitigates collaborative signal attenuation and achieves\ncompetitive performance, with improvements of up to 8.00\\% in NDCG@10 over the\nbest baseline. Our findings provide insights into how LLMs process\ncollaborative information and offer a principled approach for improving\nLLM-based recommendation systems.", "AI": {"tldr": "LLM推荐系统倾向于过度强调语义而削弱协同信号。FreLLM4Rec从频谱角度出发，通过全局图低通滤波器（G-LPF）和时间频率调制（TFM）来平衡语义和协同信息，有效缓解了协同信号衰减并提升了推荐性能。", "motivation": "现有基于LLM的推荐系统在生成语义丰富的推荐方面表现出色，但当输入协同ID嵌入时，它们倾向于过度强调用户交互历史中的语义关联，导致固有的协同信号在LLM骨干网络中逐层衰减，这与传统Transformer模型中协同信号被保留或增强的情况相反。", "method": "提出FreLLM4Rec方法，旨在平衡语义和协同信息。首先，使用全局图低通滤波器（G-LPF）对结合了语义和协同信息的物品嵌入进行初步净化，以去除不相关的。其次，通过时间频率调制（TFM）逐层主动保留协同信号。TFM的协同保留能力通过建立最优局部图傅里叶滤波器与高效频域滤波器之间的联系而得到理论保证。", "result": "在四个基准数据集上进行的广泛实验表明，FreLLM4Rec成功缓解了协同信号衰减，并取得了有竞争力的性能，相较于最佳基线，NDCG@10指标提升高达8.00%。", "conclusion": "本研究的发现为LLM如何处理协同信息提供了深入见解，并为改进基于LLM的推荐系统提供了一种原则性的方法。"}}
{"id": "2508.10473", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.10473", "abs": "https://arxiv.org/abs/2508.10473", "authors": ["Liangrui Pan", "xiaoyu Li", "Guang Zhu", "Guanting Li", "Ruixin Wang", "Jiadi Luo", "Yaning Yang", "Liang qingchun", "Shaoliang Peng"], "title": "STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images", "comment": "Submit to AAAI2026", "summary": "Spread through air spaces (STAS) constitutes a novel invasive pattern in lung\nadenocarcinoma (LUAD), associated with tumor recurrence and diminished survival\nrates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive\nendeavor, compounded by the propensity for oversight and misdiagnosis due to\nits distinctive pathological characteristics and morphological features.\nConsequently, there is a pressing clinical imperative to leverage deep learning\nmodels for STAS diagnosis. This study initially assembled histopathological\nimages from STAS patients at the Second Xiangya Hospital and the Third Xiangya\nHospital of Central South University, alongside the TCGA-LUAD cohort. Three\nsenior pathologists conducted cross-verification annotations to construct the\nSTAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern\nattention-aware multiple instance learning framework, named STAMP, to analyze\nand diagnose the presence of STAS across multi-center histopathology images.\nSpecifically, the dual-branch architecture guides the model to learn\nSTAS-associated pathological features from distinct semantic spaces.\nTransformer-based instance encoding and a multi-pattern attention aggregation\nmodules dynamically selects regions closely associated with STAS pathology,\nsuppressing irrelevant noise and enhancing the discriminative power of global\nrepresentations. Moreover, a similarity regularization constraint prevents\nfeature redundancy across branches, thereby improving overall diagnostic\naccuracy. Extensive experiments demonstrated that STAMP achieved competitive\ndiagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,\n0.8017, and 0.7928, respectively, surpassing the clinical level.", "AI": {"tldr": "该研究提出了一种名为STAMP的多模式注意力多实例学习框架，用于在肺腺癌（LUAD）中诊断气腔播散（STAS），并在多中心病理图像数据集上取得了超越临床水平的诊断性能。", "motivation": "STAS是LUAD的一种新型侵袭模式，与肿瘤复发和生存率降低相关。然而，大规模STAS诊断耗时费力，且由于其独特的病理特征，易发生遗漏和误诊。因此，迫切需要利用深度学习模型进行STAS诊断。", "method": "研究首先从两家湘雅医院和TCGA-LUAD队列收集了STAS患者的组织病理图像，并由三位资深病理学家进行交叉验证注释，构建了STAS-SXY、STAS-TXY和STAS-TCGA数据集。然后，提出了一个名为STAMP的多模式注意力多实例学习框架，用于分析和诊断STAS。STAMP采用双分支架构从不同语义空间学习STAS相关病理特征，通过基于Transformer的实例编码和多模式注意力聚合模块动态选择STAS相关区域，并利用相似性正则化约束防止特征冗余。", "result": "STAMP在STAS-SXY、STAS-TXY和STAS-TCGA数据集上取得了具有竞争力的诊断结果，AUC分别为0.8058、0.8017和0.7928，均超越了临床水平。", "conclusion": "STAMP框架能够有效诊断LUAD中的STAS，为解决STAS诊断中存在的劳动力密集、易误诊等问题提供了有力的深度学习解决方案，并具有超越临床诊断水平的潜力。"}}
{"id": "2508.10419", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10419", "abs": "https://arxiv.org/abs/2508.10419", "authors": ["Juyuan Wang", "Rongchen Zhao", "Wei Wei", "Yufeng Wang", "Mo Yu", "Jie Zhou", "Jin Xu", "Liyan Xu"], "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning", "comment": null, "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG", "AI": {"tldr": "ComoRAG是一种新的检索增强生成（RAG）方法，通过迭代推理和动态记忆工作空间，解决了长篇叙事文本理解中传统RAG的局限性，特别适用于需要全局理解的复杂查询。", "motivation": "长篇故事和小说理解面临挑战，因为情节复杂、人物关系动态。大型语言模型（LLM）在长上下文推理能力下降且计算成本高昂。传统RAG方法因其无状态、单步检索过程，难以捕捉长上下文中的动态互联关系。", "method": "提出ComoRAG，其核心思想是叙事推理是一个动态演变的过程，而非一次性完成。当遇到推理障碍时，ComoRAG通过与动态记忆工作空间交互，进行迭代推理循环。在每个循环中，它生成探测查询以探索新路径，然后将检索到的新证据整合到全局记忆池中，以支持查询解析的连贯上下文的出现。", "result": "在四个具有挑战性的长上下文叙事基准（20万+tokens）上，ComoRAG的性能优于强大的RAG基线，与最强的基线相比，相对增益高达11%。进一步分析表明，ComoRAG特别适用于需要全局理解的复杂查询。", "conclusion": "ComoRAG为检索增强的长上下文理解提供了一个有原则、受认知启发的范式，能够实现有状态的推理。它证明了叙事推理是一个动态的、新证据获取与旧知识整合之间相互作用的过程。"}}
{"id": "2508.10352", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10352", "abs": "https://arxiv.org/abs/2508.10352", "authors": ["Beso Mikaberidze", "Teimuraz Saghinadze", "Simon Ostermann", "Philipp Muller"], "title": "Cross-Prompt Encoder for Low-Performing Languages", "comment": null, "summary": "Soft prompts have emerged as a powerful alternative to adapters in\nparameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)\nto adapt to downstream tasks without architectural changes or parameter\nupdates. While prior work has focused on stabilizing training via parameter\ninteraction in small neural prompt encoders, their broader potential for\ntransfer across languages remains unexplored. In this paper, we demonstrate\nthat a prompt encoder can play a central role in improving performance on\nlow-performing languages-those that achieve poor accuracy even under full-model\nfine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a\nlightweight encoding architecture with multi-source training on typologically\ndiverse languages - a design that enables the model to capture abstract and\ntransferable patterns across languages. To complement XPE, we propose a Dual\nSoft Prompt mechanism that combines an encoder-based prompt with a directly\ntrained standard soft prompt. This hybrid design proves especially effective\nfor target languages that benefit from both broadly shared structure and\nlanguage-specific alignment. Experiments on the SIB-200 benchmark reveal a\nconsistent trade-off: XPE is most effective for low-performing languages, while\nhybrid variants offer broader adaptability across multilingual settings.", "AI": {"tldr": "该研究引入了跨语言提示编码器（XPE）和双软提示机制，以提升大语言模型在多语言参数高效微调（PEFT）中的性能，尤其针对低表现语言。", "motivation": "现有软提示方法在跨语言迁移方面的潜力尚未充分探索，尤其未能有效提升在全模型微调下表现依然不佳的低表现语言的性能。本研究旨在利用提示编码器来解决这一问题。", "method": "提出了跨语言提示编码器（XPE），它结合了轻量级编码架构和在多种语言上的多源训练，以捕捉跨语言的抽象可迁移模式。同时，引入了双软提示机制，该机制结合了基于编码器的提示和直接训练的标准软提示，以兼顾共享结构和语言特异性对齐。", "result": "在SIB-200基准测试上的实验表明，XPE对低表现语言最为有效。而混合变体（双软提示）在多语言设置中提供了更广泛的适应性，对同时受益于共享结构和语言特异性对齐的目标语言尤其有效。", "conclusion": "提示编码器在提升低表现语言性能方面发挥着核心作用。XPE和双软提示机制的结合，为多语言PEFT提供了一种有效策略，能够提升大语言模型在各种多语言环境下的适应性和性能。"}}
{"id": "2508.10498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10498", "abs": "https://arxiv.org/abs/2508.10498", "authors": ["Jianda Mao", "Kaibo Wang", "Yang Xiang", "Kani Chen"], "title": "TweezeEdit: Consistent and Efficient Image Editing with Path Regularization", "comment": null, "summary": "Large-scale pre-trained diffusion models empower users to edit images through\ntext guidance. However, existing methods often over-align with target prompts\nwhile inadequately preserving source image semantics. Such approaches generate\ntarget images explicitly or implicitly from the inversion noise of the source\nimages, termed the inversion anchors. We identify this strategy as suboptimal\nfor semantic preservation and inefficient due to elongated editing paths. We\npropose TweezeEdit, a tuning- and inversion-free framework for consistent and\nefficient image editing. Our method addresses these limitations by regularizing\nthe entire denoising path rather than relying solely on the inversion anchors,\nensuring source semantic retention and shortening editing paths. Guided by\ngradient-driven regularization, we efficiently inject target prompt semantics\nalong a direct path using a consistency model. Extensive experiments\ndemonstrate TweezeEdit's superior performance in semantic preservation and\ntarget alignment, outperforming existing methods. Remarkably, it requires only\n12 steps (1.6 seconds per edit), underscoring its potential for real-time\napplications.", "AI": {"tldr": "TweezeEdit是一个免微调、免反演的图像编辑框架，通过正则化整个去噪路径，解决了现有方法过度对齐目标提示并丢失源图像语义的问题，实现了高效且一致的图像编辑。", "motivation": "现有基于扩散模型的图像编辑方法，通常通过反演噪声生成目标图像，但它们存在过度对齐目标提示导致源图像语义丢失的问题，并且编辑路径较长，效率低下。", "method": "TweezeEdit提出了一种免微调和免反演的框架。它通过正则化整个去噪路径来保留源图像语义，而不是仅仅依赖反演锚点。该方法利用梯度驱动的正则化，结合一致性模型，沿直接路径高效注入目标提示语义，从而缩短了编辑路径。", "result": "实验证明，TweezeEdit在语义保留和目标对齐方面均优于现有方法。值得注意的是，它仅需12步（每次编辑1.6秒）即可完成，显示出其在实时应用中的巨大潜力。", "conclusion": "TweezeEdit提供了一种高效、一致且无需反演或微调的图像编辑解决方案，有效解决了现有扩散模型在语义保留和编辑效率方面的局限性，特别适用于实时应用。"}}
{"id": "2508.10507", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10507", "abs": "https://arxiv.org/abs/2508.10507", "authors": ["Zheng Zhou", "Jia-Chen Zhang", "Yu-Jie Xiong", "Chun-Ming Xia"], "title": "Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting", "comment": null, "summary": "Recent advances in 3D Gaussian splatting have significantly improved\nreal-time novel view synthesis, yet insufficient geometric constraints during\nscene optimization often result in blurred reconstructions of fine-grained\ndetails, particularly in regions with high-frequency textures and sharp\ndiscontinuities. To address this, we propose a comprehensive optimization\nframework integrating multisample anti-aliasing (MSAA) with dual geometric\nconstraints. Our system computes pixel colors through adaptive blending of\nquadruple subsamples, effectively reducing aliasing artifacts in high-frequency\ncomponents. The framework introduces two constraints: (a) an adaptive weighting\nstrategy that prioritizes under-reconstructed regions through dynamic gradient\nanalysis, and (b) gradient differential constraints enforcing geometric\nregularization at object boundaries. This targeted optimization enables the\nmodel to allocate computational resources preferentially to critical regions\nrequiring refinement while maintaining global consistency. Extensive\nexperimental evaluations across multiple benchmarks demonstrate that our method\nachieves state-of-the-art performance in detail preservation, particularly in\npreserving high-frequency textures and sharp discontinuities, while maintaining\nreal-time rendering efficiency. Quantitative metrics and perceptual studies\nconfirm statistically significant improvements over baseline approaches in both\nstructural similarity (SSIM) and perceptual quality (LPIPS).", "AI": {"tldr": "该研究提出一个结合多样本抗锯齿（MSAA）和双重几何约束的优化框架，以解决3D高斯泼溅在细节重建中出现的模糊问题，特别是在高频纹理和尖锐不连续区域，从而显著提升细节保留能力并保持实时渲染效率。", "motivation": "当前3D高斯泼溅技术在实时新视角合成方面表现出色，但由于场景优化过程中几何约束不足，导致对精细细节（尤其是在高频纹理和尖锐不连续区域）的重建常出现模糊。", "method": "该方法提出了一个综合优化框架，整合了多样本抗锯齿（MSAA）和双重几何约束。MSAA通过自适应混合四重子样本来计算像素颜色，有效减少高频分量中的锯齿伪影。框架引入了两个几何约束：(a) 基于动态梯度分析的自适应加权策略，优先处理重建不足的区域；(b) 梯度微分约束，用于在物体边界强制执行几何正则化。这种优化策略能将计算资源优先分配给需要精修的关键区域，同时保持全局一致性。", "result": "在多个基准测试上的大量实验评估表明，该方法在细节保留方面达到了最先进的性能，特别是在保留高频纹理和尖锐不连续性方面，同时保持了实时渲染效率。定量指标和感知研究证实，与基线方法相比，该方法在结构相似性（SSIM）和感知质量（LPIPS）方面均有统计学上的显著提升。", "conclusion": "该研究通过引入多样本抗锯齿和双重几何约束，有效解决了3D高斯泼溅在细节重建中的模糊问题，显著提升了高频纹理和尖锐不连续区域的重建质量，实现了卓越的细节保留能力和感知质量，同时保持了实时渲染性能。"}}
{"id": "2508.10355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10355", "abs": "https://arxiv.org/abs/2508.10355", "authors": ["Jungyup Lee", "Jemin Kim", "Sang Park", "SeungJae Lee"], "title": "Making Qwen3 Think in Korean with Reinforcement Learning", "comment": null, "summary": "We present a two-stage fine-tuning approach to make the large language model\nQwen3 14B \"think\" natively in Korean. In the first stage, supervised\nfine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a\nstrong foundation in Korean logical reasoning, yielding notable improvements in\nKorean-language tasks and even some gains in general reasoning ability. In the\nsecond stage, we employ reinforcement learning with a customized Group Relative\nPolicy Optimization (GRPO) algorithm to further enhance both Korean reasoning\nalignment and overall problem-solving performance. We address critical\nstability challenges in GRPO training - such as reward hacking and policy\ncollapse - by introducing an oracle judge model that calibrates the reward\nsignal. Our approach achieves stable learning (avoiding the collapse observed\nin naive GRPO) and leads to steady, incremental performance gains. The final\nRL-tuned model demonstrates substantially improved results on advanced\nreasoning benchmarks (particularly math and coding tasks) while maintaining\nknowledge and language proficiency, successfully conducting its internal\nchain-of-thought entirely in Korean.", "AI": {"tldr": "本文提出两阶段微调方法，使Qwen3 14B模型能用韩语进行原生“思考”，显著提升了其韩语推理能力和整体问题解决性能，特别是在数学和编程任务上。", "motivation": "使大型语言模型Qwen3 14B能够原生用韩语进行“思考”，建立强大的韩语逻辑推理基础，并提升其整体问题解决能力。同时，解决GRPO训练中常见的稳定性挑战，如奖励作弊和策略崩溃。", "method": "采用两阶段微调方法：第一阶段，对Qwen3 14B进行监督微调（SFT），使用高质量韩语推理数据集。第二阶段，采用定制的群组相对策略优化（GRPO）算法进行强化学习，并引入一个“预言者评判模型”来校准奖励信号，以解决GRPO训练中的稳定性问题（如奖励作弊和策略崩溃）。", "result": "第一阶段SFT显著提升了模型在韩语任务上的表现，并对通用推理能力有所增益。第二阶段RL（结合预言者评判模型）实现了稳定的学习，避免了策略崩溃，并带来了持续、渐进的性能提升。最终的RL微调模型在高级推理基准测试（特别是数学和编程任务）上表现出显著改善，同时保持了知识和语言熟练度，并能完全用韩语进行内部思维链推理。", "conclusion": "所提出的两阶段微调方法，尤其是通过引入预言者评判模型稳定GRPO训练，成功使Qwen3 14B模型能够原生用韩语进行思考，并大幅提升了其在复杂推理任务上的性能，验证了该方法的有效性。"}}
{"id": "2508.10509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10509", "abs": "https://arxiv.org/abs/2508.10509", "authors": ["Yangjie Xiao", "Ke Zhang", "Jiacun Wang", "Xin Sheng", "Yurong Guo", "Meijuan Chen", "Zehua Ren", "Zhaoye Zheng", "Zhenbing Zhao"], "title": "A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection", "comment": null, "summary": "Bolt defect detection is critical to ensure the safety of transmission lines.\nHowever, the scarcity of defect images and imbalanced data distributions\nsignificantly limit detection performance. To address this problem, we propose\na segmentationdriven bolt defect editing method (SBDE) to augment the dataset.\nFirst, a bolt attribute segmentation model (Bolt-SAM) is proposed, which\nenhances the segmentation of complex bolt attributes through the CLAHE-FFT\nAdapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality\nmasks for subsequent editing tasks. Second, a mask optimization module (MOD) is\ndesigned and integrated with the image inpainting model (LaMa) to construct the\nbolt defect attribute editing model (MOD-LaMa), which converts normal bolts\ninto defective ones through attribute editing. Finally, an editing recovery\naugmentation (ERA) strategy is proposed to recover and put the edited defect\nbolts back into the original inspection scenes and expand the defect detection\ndataset. We constructed multiple bolt datasets and conducted extensive\nexperiments. Experimental results demonstrate that the bolt defect images\ngenerated by SBDE significantly outperform state-of-the-art image editing\nmodels, and effectively improve the performance of bolt defect detection, which\nfully verifies the effectiveness and application potential of the proposed\nmethod. The code of the project is available at\nhttps://github.com/Jay-xyj/SBDE.", "AI": {"tldr": "本文提出了一种基于分割的螺栓缺陷编辑方法（SBDE），用于扩充输电线路螺栓缺陷数据集，以解决缺陷图像稀缺和数据分布不平衡的问题，从而提高缺陷检测性能。", "motivation": "输电线路螺栓缺陷检测对保障安全至关重要，但缺陷图像稀缺和数据分布不平衡严重限制了检测性能。", "method": "该方法包括三个步骤：1. 提出螺栓属性分割模型（Bolt-SAM），通过CLAHE-FFT Adapter（CFA）和Multipart-Aware Mask Decoder（MAMD）增强复杂螺栓属性的分割，生成高质量掩码。2. 设计掩码优化模块（MOD）并与图像修复模型（LaMa）结合，构建螺栓缺陷属性编辑模型（MOD-LaMa），通过属性编辑将正常螺栓转换为缺陷螺栓。3. 提出编辑恢复增强（ERA）策略，将编辑后的缺陷螺栓恢复并放回原始检查场景，以扩充缺陷检测数据集。", "result": "实验结果表明，SBDE生成的螺栓缺陷图像显著优于现有最先进的图像编辑模型，并有效提高了螺栓缺陷检测的性能。", "conclusion": "所提出的SBDE方法有效解决了螺栓缺陷图像稀缺和数据不平衡问题，验证了其在提高螺栓缺陷检测性能方面的有效性和应用潜力。"}}
{"id": "2508.10528", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10528", "abs": "https://arxiv.org/abs/2508.10528", "authors": ["Ziye Deng", "Ruihan He", "Jiaxiang Liu", "Yuan Wang", "Zijie Meng", "Songtao Jiang", "Yong Xie", "Zuozhu Liu"], "title": "Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset", "comment": null, "summary": "Medical image grounding aims to align natural language phrases with specific\nregions in medical images, serving as a foundational task for intelligent\ndiagnosis, visual question answering (VQA), and automated report generation\n(MRG). However, existing research is constrained by limited modality coverage,\ncoarse-grained annotations, and the absence of a unified, generalizable\ngrounding framework. To address these challenges, we construct a large-scale\nmedical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level\nannotations across seven imaging modalities, covering diverse anatomical\nstructures and pathological findings. The dataset supports both segmentation\nand grounding tasks with hierarchical region labels, ranging from organ-level\nboundaries to fine-grained lesions. Based on this foundation, we propose\nMed-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather\nthan relying on explicitly designed expert modules, Med-GLIP implicitly\nacquires hierarchical semantic understanding from diverse training data --\nenabling it to recognize multi-granularity structures, such as distinguishing\nlungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP\nconsistently outperforms state-of-the-art baselines across multiple grounding\nbenchmarks. Furthermore, integrating its spatial outputs into downstream tasks,\nincluding medical VQA and report generation, leads to substantial performance\ngains. Our dataset will be released soon.", "AI": {"tldr": "该论文构建了一个大规模多模态医学图像定位数据集Med-GLIP-5M，并提出了一个通用的模态感知定位框架Med-GLIP，在多项基准测试中超越现有技术，并显著提升下游任务性能。", "motivation": "现有医学图像定位研究存在模态覆盖有限、标注粒度粗糙以及缺乏统一通用定位框架的局限性，阻碍了智能诊断、视觉问答和报告生成等任务的发展。", "method": "1. 构建Med-GLIP-5M数据集：包含超过530万个跨七种成像模态的区域级标注，支持分割和定位任务，并具有从器官到病灶的层级区域标签。2. 提出Med-GLIP框架：一个在Med-GLIP-5M上训练的模态感知定位框架，它能隐式地从多样数据中学习层级语义理解，无需显式设计的专家模块。", "result": "Med-GLIP在多个定位基准测试中持续优于最先进的基线方法。将其空间输出集成到下游任务（如医学VQA和报告生成）中，可带来显著的性能提升。", "conclusion": "该研究通过构建大规模多模态数据集和提出创新的定位框架，有效解决了现有医学图像定位的挑战，并为智能医学诊断和相关AI应用奠定了坚实基础。"}}
{"id": "2508.10366", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10366", "abs": "https://arxiv.org/abs/2508.10366", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "title": "Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models", "comment": "Published in Proceedings of the 17th International Conference on\n  Agents and Artificial Intelligence - Volume 2 (ICAART 2025). Official\n  version: https://www.scitepress.org/Link.aspx?doi=10.5220/0013349400003890", "summary": "Aspect-based sentiment analysis (ABSA) has made significant strides, yet\nchallenges remain for low-resource languages due to the predominant focus on\nEnglish. Current cross-lingual ABSA studies often centre on simpler tasks and\nrely heavily on external translation tools. In this paper, we present a novel\nsequence-to-sequence method for compound ABSA tasks that eliminates the need\nfor such tools. Our approach, which uses constrained decoding, improves\ncross-lingual ABSA performance by up to 10\\%. This method broadens the scope of\ncross-lingual ABSA, enabling it to handle more complex tasks and providing a\npractical, efficient alternative to translation-dependent techniques.\nFurthermore, we compare our approach with large language models (LLMs) and show\nthat while fine-tuned multilingual LLMs can achieve comparable results,\nEnglish-centric LLMs struggle with these tasks.", "AI": {"tldr": "本文提出一种新的序列到序列方法，用于跨语言复合ABSA任务，无需外部翻译工具，通过使用约束解码，显著提升了性能，并与大型语言模型进行了比较。", "motivation": "当前跨语言ABSA研究主要集中在英语，导致低资源语言面临挑战，且现有方法多限于简单任务并严重依赖外部翻译工具。", "method": "提出一种新颖的序列到序列方法，用于处理复合ABSA任务。该方法通过使用约束解码，消除了对外部翻译工具的依赖。", "result": "该方法将跨语言ABSA性能提高了高达10%；它拓宽了跨语言ABSA的范围，使其能够处理更复杂的任务，并提供了一种实用、高效的替代方案。与大型语言模型（LLM）的比较显示，微调的多语言LLM可以达到可比的结果，但以英语为中心的LLM在这些任务上表现不佳。", "conclusion": "该方法拓展了跨语言ABSA处理复杂任务的能力，提供了一种无需翻译工具的实用高效替代方案。研究还表明，微调的多语言LLM表现良好，而以英语为中心的LLM在这些任务上存在局限性。"}}
{"id": "2508.10522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10522", "abs": "https://arxiv.org/abs/2508.10522", "authors": ["Quang Nguyen", "Nhat Le", "Baoru Huang", "Minh Nhat Vu", "Chengcheng Tang", "Van Nguyen", "Ngan Le", "Thieu Vo", "Anh Nguyen"], "title": "EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba", "comment": "Accepted at The 2025 IEEE/CVF International Conference on Computer\n  Vision (ICCV 2025)", "summary": "Estimating human dance motion is a challenging task with various industrial\napplications. Recently, many efforts have focused on predicting human dance\nmotion using either egocentric video or music as input. However, the task of\njointly estimating human motion from both egocentric video and music remains\nlargely unexplored. In this paper, we aim to develop a new method that predicts\nhuman dance motion from both egocentric video and music. In practice, the\negocentric view often obscures much of the body, making accurate full-pose\nestimation challenging. Additionally, incorporating music requires the\ngenerated head and body movements to align well with both visual and musical\ninputs. We first introduce EgoAIST++, a new large-scale dataset that combines\nboth egocentric views and music with more than 36 hours of dancing motion.\nDrawing on the success of diffusion models and Mamba on modeling sequences, we\ndevelop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly\ncaptures the skeleton structure of the human body. We illustrate that our\napproach is theoretically supportive. Intensive experiments show that our\nmethod clearly outperforms state-of-the-art approaches and generalizes\neffectively to real-world data.", "AI": {"tldr": "该研究提出了一种新方法，利用自我中心视角视频和音乐共同预测人体舞蹈动作，并构建了大规模数据集EgoAIST++，通过结合扩散模型和Skeleton Mamba网络实现了优于现有技术的效果。", "motivation": "现有研究多集中于单独使用自我中心视角视频或音乐来预测人体舞蹈动作，但同时结合两者进行动作估计的任务仍未被充分探索。此外，自我中心视角常遮挡身体大部分，使得全身姿态估计困难，且生成的动作需同时与视觉和音乐输入对齐。", "method": "1. 构建了新的大规模数据集EgoAIST++，包含超过36小时的舞蹈动作，结合了自我中心视角和音乐。2. 开发了EgoMusic Motion Network，其核心是Skeleton Mamba模型，该模型明确捕捉人体骨骼结构。3. 方法借鉴了扩散模型和Mamba在序列建模上的成功经验。", "result": "实验结果表明，所提出的方法显著优于现有最先进的方法，并能有效泛化到真实世界数据。", "conclusion": "该研究开发了一种有效且理论上支持的新方法，能够从自我中心视角视频和音乐共同预测人体舞蹈动作，解决了现有技术的局限性，并提供了新的大规模数据集以促进未来研究。"}}
{"id": "2508.10552", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10552", "abs": "https://arxiv.org/abs/2508.10552", "authors": ["Huyu Wu", "Meng Tang", "Xinhan Zheng", "Haiyun Jiang"], "title": "When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across a diverse range of multimodal tasks. However, these models\nsuffer from a core problem known as text dominance: they depend heavily on text\nfor their inference, while underutilizing other modalities. While prior work\nhas acknowledged this phenomenon in vision-language tasks, often attributing it\nto data biases or model architectures. In this paper, we conduct the first\nsystematic investigation of text dominance across diverse data modalities,\nincluding images, videos, audio, time-series, and graphs. To measure this\nimbalance, we propose two evaluation metrics: the Modality Dominance Index\n(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis\nreveals that text dominance is both significant and pervasive across all tested\nmodalities. Our in-depth analysis identifies three underlying causes: attention\ndilution from severe token redundancy in non-textual modalities, the influence\nof fusion architecture design, and task formulations that implicitly favor\ntextual inputs. Furthermore, we propose a simple token compression method that\neffectively rebalances model attention. Applying this method to LLaVA-7B, for\ninstance, drastically reduces its MDI from 10.23 to a well-balanced value of\n0.86. Our analysis and methodological framework offer a foundation for the\ndevelopment of more equitable and comprehensive multimodal language models.", "AI": {"tldr": "多模态大语言模型（MLLMs）存在“文本主导”问题，即过度依赖文本而未充分利用其他模态。本文首次系统性地在多种模态下调查此问题，提出了衡量指标，揭示了其成因，并提出了一种有效的Token压缩方法来平衡模型注意力。", "motivation": "多模态大语言模型（MLLMs）在多模态任务中表现出色，但存在一个核心问题：它们过度依赖文本进行推理，而未能充分利用其他模态。尽管此前工作已在视觉-语言任务中提及此现象，但缺乏对跨越多种数据模态的系统性研究。", "method": "1. 首次系统性调查文本主导现象，涵盖图像、视频、音频、时间序列和图表等多种模态。2. 提出两种评估指标：模态主导指数（MDI）和注意力效率指数（AEI）来量化这种不平衡。3. 深入分析识别了导致文本主导的潜在原因。4. 提出一种简单的Token压缩方法以有效重新平衡模型注意力。", "result": "1. 文本主导现象在所有测试模态中都显著且普遍存在。2. 识别出三个根本原因：非文本模态中严重的Token冗余导致注意力稀释；融合架构设计的影响；以及隐性偏向文本输入的任务表述。3. 所提出的Token压缩方法能有效重新平衡模型注意力，例如将LLaVA-7B的MDI从10.23大幅降低至0.86。", "conclusion": "本文的分析和方法框架为开发更公平、更全面的多模态语言模型奠定了基础，有助于解决MLLMs中普遍存在的文本主导问题。"}}
{"id": "2508.10368", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10368", "abs": "https://arxiv.org/abs/2508.10368", "authors": ["Václav Tran", "Jakub Šmíd", "Jiří Martínek", "Ladislav Lenc", "Pavel Král"], "title": "Large Language Models for Summarizing Czech Historical Documents and Beyond", "comment": "Published in Proceedings of the 17th International Conference on\n  Agents and Artificial Intelligence - Volume 2 (ICAART 2025). Official\n  version: https://www.scitepress.org/Link.aspx?doi=10.5220/0013374100003890", "summary": "Text summarization is the task of shortening a larger body of text into a\nconcise version while retaining its essential meaning and key information.\nWhile summarization has been significantly explored in English and other\nhigh-resource languages, Czech text summarization, particularly for historical\ndocuments, remains underexplored due to linguistic complexities and a scarcity\nof annotated datasets. Large language models such as Mistral and mT5 have\ndemonstrated excellent results on many natural language processing tasks and\nlanguages. Therefore, we employ these models for Czech summarization, resulting\nin two key contributions: (1) achieving new state-of-the-art results on the\nmodern Czech summarization dataset SumeCzech using these advanced models, and\n(2) introducing a novel dataset called Posel od \\v{C}erchova for summarization\nof historical Czech documents with baseline results. Together, these\ncontributions provide a great potential for advancing Czech text summarization\nand open new avenues for research in Czech historical text processing.", "AI": {"tldr": "该研究利用大型语言模型（Mistral和mT5）显著提升了捷克语文本摘要的水平，在现代捷克语数据集SumeCzech上取得了最先进的成果，并首次引入了一个用于历史捷克语文档摘要的新数据集Posel od Čerchova。", "motivation": "文本摘要在英语等高资源语言中已得到广泛探索，但捷克语文本摘要，特别是针对历史文献的摘要，因语言复杂性和标注数据集稀缺而研究不足。大型语言模型在多项自然语言处理任务和语言中表现出色。", "method": "采用大型语言模型，具体为Mistral和mT5，进行捷克语文本摘要。", "result": "1. 在现代捷克语摘要数据集SumeCzech上使用先进模型取得了新的最先进结果。\n2. 引入了一个名为Posel od Čerchova的新数据集，用于历史捷克语文档的摘要，并提供了基线结果。", "conclusion": "这些贡献极大地推动了捷克语文本摘要领域的发展，并为捷克语历史文本处理研究开辟了新途径。"}}
{"id": "2508.10523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10523", "abs": "https://arxiv.org/abs/2508.10523", "authors": ["Ayushman Sarkar", "Mohd Yamani Idna Idris", "Zhenyu Yu"], "title": "Reasoning in Computer Vision: Taxonomy, Models, Tasks, and Methodologies", "comment": null, "summary": "Visual reasoning is critical for a wide range of computer vision tasks that\ngo beyond surface-level object detection and classification. Despite notable\nadvances in relational, symbolic, temporal, causal, and commonsense reasoning,\nexisting surveys often address these directions in isolation, lacking a unified\nanalysis and comparison across reasoning types, methodologies, and evaluation\nprotocols. This survey aims to address this gap by categorizing visual\nreasoning into five major types (relational, symbolic, temporal, causal, and\ncommonsense) and systematically examining their implementation through\narchitectures such as graph-based models, memory networks, attention\nmechanisms, and neuro-symbolic systems. We review evaluation protocols designed\nto assess functional correctness, structural consistency, and causal validity,\nand critically analyze their limitations in terms of generalizability,\nreproducibility, and explanatory power. Beyond evaluation, we identify key open\nchallenges in visual reasoning, including scalability to complex scenes, deeper\nintegration of symbolic and neural paradigms, the lack of comprehensive\nbenchmark datasets, and reasoning under weak supervision. Finally, we outline a\nforward-looking research agenda for next-generation vision systems, emphasizing\nthat bridging perception and reasoning is essential for building transparent,\ntrustworthy, and cross-domain adaptive AI systems, particularly in critical\ndomains such as autonomous driving and medical diagnostics.", "AI": {"tldr": "这篇综述对视觉推理进行了全面分析，将其分为五种类型，并系统地审查了其实现方法、评估协议、当前局限性以及未来的研究挑战和方向。", "motivation": "现有关于关系、符号、时间、因果和常识推理的综述通常孤立地处理这些方向，缺乏对不同推理类型、方法和评估协议的统一分析和比较，因此本研究旨在填补这一空白。", "method": "本研究将视觉推理分为五种主要类型（关系、符号、时间、因果和常识），系统地考察了通过图模型、记忆网络、注意力机制和神经符号系统等架构实现的各种方法。同时，审查了旨在评估功能正确性、结构一致性和因果有效性的评估协议，并分析了其在泛化性、可复现性和解释力方面的局限性。最后，识别了视觉推理中的关键开放挑战。", "result": "本综述系统地分类并考察了视觉推理的五种主要类型及其实现架构，分析了评估协议的有效性与局限性，并识别了当前视觉推理领域的主要开放挑战，包括复杂场景的可扩展性、符号与神经范式的深度融合、缺乏全面的基准数据集以及弱监督下的推理。", "conclusion": "弥合感知与推理之间的鸿沟对于构建透明、可信和跨领域自适应的AI系统至关重要，特别是在自动驾驶和医疗诊断等关键领域。未来的研究议程应侧重于此，以推动下一代视觉系统的发展。"}}
{"id": "2508.10556", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10556", "abs": "https://arxiv.org/abs/2508.10556", "authors": ["Ruisong Han", "Zongbo Han", "Jiahao Zhang", "Mingyue Cheng", "Changqing Zhang"], "title": "Retrieval-Augmented Prompt for OOD Detection", "comment": null, "summary": "Out-of-Distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in-the-wild, enabling accurate identification of test\nsamples that differ from the training data distribution. Existing methods rely\non auxiliary outlier samples or in-distribution (ID) data to generate outlier\ninformation for training, but due to limited outliers and their mismatch with\nreal test OOD samples, they often fail to provide sufficient semantic\nsupervision, leading to suboptimal performance. To address this, we propose a\nnovel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP\naugments a pre-trained vision-language model's prompts by retrieving external\nknowledge, offering enhanced semantic supervision for OOD detection. During\ntraining, RAP retrieves descriptive words for outliers based on joint\nsimilarity with external textual knowledge and uses them to augment the model's\nOOD prompts. During testing, RAP dynamically updates OOD prompts in real-time\nbased on the encountered OOD samples, enabling the model to rapidly adapt to\nthe test environment. Our extensive experiments demonstrate that RAP achieves\nstate-of-the-art performance on large-scale OOD detection benchmarks. For\nexample, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the\naverage FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous\nmethods. Additionally, comprehensive ablation studies validate the\neffectiveness of each module and the underlying motivations of our approach.", "AI": {"tldr": "提出了一种名为RAP的新型OOD检测方法，通过检索外部知识增强预训练视觉-语言模型的提示，以提供更强的语义监督，从而解决现有方法在异常样本不足和不匹配问题上的性能限制。", "motivation": "现有的OOD检测方法依赖辅助异常样本或ID数据生成异常信息进行训练，但由于异常样本有限且与真实测试OOD样本不匹配，导致语义监督不足，性能欠佳。", "method": "提出检索增强提示（RAP）方法。在训练阶段，RAP基于与外部文本知识的联合相似性，为异常样本检索描述性词语来增强模型的OOD提示。在测试阶段，RAP根据遇到的OOD样本实时动态更新OOD提示，使模型能快速适应测试环境。", "result": "RAP在大规模OOD检测基准上实现了最先进的性能。例如，在ImageNet-1k数据集的1-shot OOD检测中，RAP相比现有方法将平均FPR95降低了7.05%，AUROC提高了1.71%。全面的消融研究也验证了每个模块的有效性。", "conclusion": "RAP通过检索增强和动态更新的提示，有效解决了现有OOD检测方法语义监督不足的问题，显著提升了OOD检测的性能和适应性。"}}
{"id": "2508.10369", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10369", "abs": "https://arxiv.org/abs/2508.10369", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "title": "Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding", "comment": null, "summary": "While aspect-based sentiment analysis (ABSA) has made substantial progress,\nchallenges remain for low-resource languages, which are often overlooked in\nfavour of English. Current cross-lingual ABSA approaches focus on limited, less\ncomplex tasks and often rely on external translation tools. This paper\nintroduces a novel approach using constrained decoding with\nsequence-to-sequence models, eliminating the need for unreliable translation\ntools and improving cross-lingual performance by 5\\% on average for the most\ncomplex task. The proposed method also supports multi-tasking, which enables\nsolving multiple ABSA tasks with a single model, with constrained decoding\nboosting results by more than 10\\%.\n  We evaluate our approach across seven languages and six ABSA tasks,\nsurpassing state-of-the-art methods and setting new benchmarks for previously\nunexplored tasks. Additionally, we assess large language models (LLMs) in\nzero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in\nzero-shot and few-shot settings, fine-tuning achieves competitive results\ncompared to smaller multilingual models, albeit at the cost of longer training\nand inference times.\n  We provide practical recommendations for real-world applications, enhancing\nthe understanding of cross-lingual ABSA methodologies. This study offers\nvaluable insights into the strengths and limitations of cross-lingual ABSA\napproaches, advancing the state-of-the-art in this challenging research domain.", "AI": {"tldr": "本文提出了一种基于序列到序列模型和约束解码的跨语言方面级情感分析（ABSA）新方法，无需外部翻译工具，显著提升了低资源语言的性能，并支持多任务处理。同时评估了大型语言模型（LLMs）在不同设置下的表现。", "motivation": "现有跨语言ABSA方法在低资源语言上面临挑战，且通常依赖不可靠的外部翻译工具，同时关注的任务范围有限、复杂度较低。", "method": "引入了一种结合序列到序列模型的约束解码方法，消除了对外部翻译工具的依赖。该方法支持多任务处理，允许一个模型解决多个ABSA任务。研究在七种语言和六个ABSA任务上进行了评估，并对LLMs在零样本、少样本和微调场景下进行了测试。", "result": "该方法在最复杂的任务上将跨语言性能平均提升了5%；在多任务处理中，约束解码使结果提升了10%以上。新方法超越了现有最先进水平，并在未探索的任务上设定了新基准。LLMs在零样本和少样本设置下表现不佳，但在微调后能达到与小型多语言模型相当的竞争力，尽管训练和推理时间更长。", "conclusion": "该研究为跨语言ABSA方法提供了有价值的见解，增强了对其实践应用的理解，并推动了该挑战性研究领域的发展。提出了针对实际应用的实用建议。"}}
{"id": "2508.10542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10542", "abs": "https://arxiv.org/abs/2508.10542", "authors": ["Mengyu Ren", "Yutong Li", "Hua Li", "Runmin Cong", "Sam Kwong"], "title": "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images", "comment": null, "summary": "Salient object detection (SOD) in optical remote sensing images (ORSIs) faces\nnumerous challenges, including significant variations in target scales and low\ncontrast between targets and the background. Existing methods based on vision\ntransformers (ViTs) and convolutional neural networks (CNNs) architectures aim\nto leverage both global and local features, but the difficulty in effectively\nintegrating these heterogeneous features limits their overall performance. To\novercome these limitations, we propose a graph-enhanced contextual and regional\nperception network (GCRPNet), which builds upon the Mamba architecture to\nsimultaneously capture long-range dependencies and enhance regional feature\nrepresentation. Specifically, we employ the visual state space (VSS) encoder to\nextract multi-scale features. To further achieve deep guidance and enhancement\nof these features, we first design a difference-similarity guided hierarchical\ngraph attention module (DS-HGAM). This module strengthens cross-layer\ninteraction capabilities between features of different scales while enhancing\nthe model's structural perception,allowing it to distinguish between foreground\nand background more effectively. Then, we design the LEVSS block as the decoder\nof GCRPNet. This module integrates our proposed adaptive scanning strategy and\nmulti-granularity collaborative attention enhancement module (MCAEM). It\nperforms adaptive patch scanning on feature maps processed via multi-scale\nconvolutions, thereby capturing rich local region information and enhancing\nMamba's local modeling capability. Extensive experimental results demonstrate\nthat the proposed model achieves state-of-the-art performance, validating its\neffectiveness and superiority.", "AI": {"tldr": "针对遥感图像中的显著目标检测（SOD）挑战，本文提出了一种基于Mamba架构的图增强上下文与区域感知网络（GCRPNet），通过创新的模块有效整合多尺度特征和局部信息，实现了最先进的性能。", "motivation": "光学遥感图像（ORSIs）中的显著目标检测面临目标尺度变化大和目标与背景对比度低等挑战。现有基于ViT和CNN的方法难以有效整合全局和局部特征，限制了性能。", "method": "提出GCRPNet，基于Mamba架构，同时捕获长程依赖并增强区域特征表示。使用视觉状态空间（VSS）编码器提取多尺度特征。设计了差异相似性引导的分层图注意力模块（DS-HGAM）以增强跨层交互和结构感知。设计了LEVSS块作为解码器，该块整合了自适应扫描策略和多粒度协同注意力增强模块（MCAEM），通过对多尺度卷积处理后的特征图进行自适应补丁扫描，增强Mamba的局部建模能力。", "result": "实验结果表明，所提出的模型实现了最先进的性能。", "conclusion": "GCRPNet在光学遥感图像显著目标检测中表现出有效性和优越性，成功克服了现有方法在特征整合上的局限性。"}}
{"id": "2508.10557", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10557", "abs": "https://arxiv.org/abs/2508.10557", "authors": ["Xinhao Wang", "Zhiwei Lin", "Zhongyu Xia", "Yongtao Wang"], "title": "PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks", "comment": "8 pages, Accepted by ICCVW 2025", "summary": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)\nrepresent two mainstream model quantization approaches. However, PTQ often\nleads to unacceptable performance degradation in quantized models, while QAT\nimposes substantial GPU memory requirements and extended training time due to\nweight fine-tuning.In this paper, we propose PTQAT, a novel general hybrid\nquantization algorithm for the efficient deployment of 3D perception networks.\nTo address the speed accuracy trade-off between PTQ and QAT, our method selects\ncritical layers for QAT fine-tuning and performs PTQ on the remaining layers.\nContrary to intuition, fine-tuning the layers with smaller output discrepancies\nbefore and after quantization, rather than those with larger discrepancies,\nactually leads to greater improvements in the model's quantization accuracy.\nThis means we better compensate for quantization errors during their\npropagation, rather than addressing them at the point where they occur. The\nproposed PTQAT achieves similar performance to QAT with more efficiency by\nfreezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal\nquantization method that supports various quantization bit widths (4 bits) as\nwell as different model architectures, including CNNs and Transformers. The\nexperimental results on nuScenes across diverse 3D perception tasks, including\nobject detection, semantic segmentation, and occupancy prediction, show that\nour method consistently outperforms QAT-only baselines. Notably, it achieves\n0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains\nin semantic segmentation and occupancy prediction while fine-tuning fewer\nweights.", "AI": {"tldr": "提出了一种名为PTQAT的混合量化算法，通过选择性地对关键层进行QAT微调和对剩余层进行PTQ，在3D感知任务中实现了接近QAT的性能，同时显著提高了效率，并超越了纯QAT基线。", "motivation": "PTQ（训练后量化）通常导致模型性能显著下降，而QAT（量化感知训练）则需要大量的GPU内存和训练时间。因此，需要一种方法来平衡量化模型的精度和部署效率。", "method": "PTQAT是一种新颖的通用混合量化算法。它选择性地对关键层进行QAT微调，并对剩余层执行PTQ。与直觉相反，该方法发现对量化前后输出差异较小的层进行微调，而不是差异较大的层，能带来更大的模型量化精度提升，旨在更好地补偿量化误差的传播。该方法冻结了近50%的可量化层。", "result": "PTQAT实现了与QAT相似的性能，但效率更高（通过冻结近50%的可量化层）。它是一种通用量化方法，支持多种量化位宽（如4位）和不同模型架构（包括CNN和Transformer）。在nuScenes数据集上，针对3D目标检测、语义分割和占用预测等任务的实验结果表明，PTQAT始终优于纯QAT基线，例如在目标检测中NDS提升0.2%-0.9%，mAP提升0.3%-1.0%，在语义分割和占用预测中mIoU提升0.3%-2.0%，且微调的权重更少。", "conclusion": "PTQAT是一种高效且通用的混合量化算法，成功解决了PTQ和QAT之间的权衡问题。它通过选择性微调和独特的误差补偿策略，在3D感知网络上实现了卓越的性能和效率，超越了传统的QAT方法。"}}
{"id": "2508.10390", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.10390", "abs": "https://arxiv.org/abs/2508.10390", "authors": ["Chiyu Zhang", "Lu Zhou", "Xiaogang Xu", "Jiafei Wu", "Liming Fang", "Zhe Liu"], "title": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts", "comment": null, "summary": "Evaluating jailbreak attacks is challenging when prompts are not overtly\nharmful or fail to induce harmful outputs. Unfortunately, many existing\nred-teaming datasets contain such unsuitable prompts. To evaluate attacks\naccurately, these datasets need to be assessed and cleaned for maliciousness.\nHowever, existing malicious content detection methods rely on either manual\nannotation, which is labor-intensive, or large language models (LLMs), which\nhave inconsistent accuracy in harmful types. To balance accuracy and\nefficiency, we propose a hybrid evaluation framework named MDH (Malicious\ncontent Detection based on LLMs with Human assistance) that combines LLM-based\nannotation with minimal human oversight, and apply it to dataset cleaning and\ndetection of jailbroken responses. Furthermore, we find that well-crafted\ndeveloper messages can significantly boost jailbreak success, leading us to\npropose two new strategies: D-Attack, which leverages context simulation, and\nDH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,\njudgements, and detection results will be released in github repository:\nhttps://github.com/AlienZhang1996/DH-CoT.", "AI": {"tldr": "该研究提出了一个名为MDH的混合评估框架，结合LLM和人工协助来高效准确地检测恶意内容和清洗数据集。同时，提出了两种新的越狱攻击策略D-Attack和DH-CoT，利用开发者消息显著提升越狱成功率。", "motivation": "现有越狱攻击评估面临挑战，因为许多红队数据集包含不适合的提示，导致难以准确评估。此外，现有的恶意内容检测方法要么依赖耗时的人工标注，要么依赖LLM但其在有害类型上准确性不一致。", "method": "1. 提出了MDH（基于LLM并辅以人工协助的恶意内容检测）混合评估框架，结合LLM标注和最少的人工监督，用于数据集清洗和检测越狱响应。2. 提出了两种新的越狱攻击策略：D-Attack（利用上下文模拟）和DH-CoT（结合劫持的思维链），通过精心设计的开发者消息来提高越狱成功率。", "result": "MDH框架在准确性和效率之间取得了平衡，成功应用于数据集清洗和越狱响应检测。研究发现，精心设计的开发者消息可以显著提高越狱成功率，D-Attack和DH-CoT这两种新策略证实了其有效性。", "conclusion": "MDH提供了一种高效准确的恶意内容检测和数据集清洗方法，解决了现有评估方法的局限性。同时，通过利用开发者消息，新提出的D-Attack和DH-CoT策略能显著提升越狱攻击的成功率，这对于理解和防御此类攻击具有重要意义。"}}
{"id": "2508.10549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10549", "abs": "https://arxiv.org/abs/2508.10549", "authors": ["Boyi Zheng", "Qing Liu"], "title": "PSScreen: Partially Supervised Multiple Retinal Disease Screening", "comment": "Accepted at BMVC 2025 (Oral)", "summary": "Leveraging multiple partially labeled datasets to train a model for multiple\nretinal disease screening reduces the reliance on fully annotated datasets, but\nremains challenging due to significant domain shifts across training datasets\nfrom various medical sites, and the label absent issue for partial classes. To\nsolve these challenges, we propose PSScreen, a novel Partially Supervised\nmultiple retinal disease Screening model. Our PSScreen consists of two streams\nand one learns deterministic features and the other learns probabilistic\nfeatures via uncertainty injection. Then, we leverage the textual guidance to\ndecouple two types of features into disease-wise features and align them via\nfeature distillation to boost the domain generalization ability. Meanwhile, we\nemploy pseudo label consistency between two streams to address the label absent\nissue and introduce a self-distillation to transfer task-relevant semantics\nabout known classes from the deterministic to the probabilistic stream to\nfurther enhance the detection performances. Experiments show that our PSScreen\nsignificantly enhances the detection performances on six retinal diseases and\nthe normal state averagely and achieves state-of-the-art results on both\nin-domain and out-of-domain datasets. Codes are available at\nhttps://github.com/boyiZheng99/PSScreen.", "AI": {"tldr": "PSScreen是一种新颖的半监督多视网膜疾病筛查模型，旨在利用多个部分标注数据集，解决跨领域域偏移和标签缺失问题，提高筛查性能和泛化能力。", "motivation": "在多视网膜疾病筛查中，过度依赖完全标注数据集，且来自不同医疗站点的训练数据集存在显著的域偏移，以及部分类别标签缺失，这些都带来了巨大挑战。", "method": "PSScreen包含两个流：一个学习确定性特征，另一个通过不确定性注入学习概率性特征。模型利用文本指导将两种特征解耦为疾病特异性特征，并通过特征蒸馏对齐以增强域泛化能力。同时，采用两流间的伪标签一致性解决标签缺失问题，并引入自蒸馏将已知类别的任务相关语义从确定性流传输到概率性流以进一步提升检测性能。", "result": "实验结果表明，PSScreen显著提高了对六种视网膜疾病和正常状态的平均检测性能，并在域内和域外数据集上均达到了最先进的水平。", "conclusion": "PSScreen模型有效解决了利用多个部分标注数据集进行多视网膜疾病筛查所面临的域偏移和标签缺失挑战，显著提升了模型的检测性能和泛化能力。"}}
{"id": "2508.10616", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10616", "abs": "https://arxiv.org/abs/2508.10616", "authors": ["Daejune Choi", "Youchan No", "Jinhyung Lee", "Duksu Kim"], "title": "Fourier-Guided Attention Upsampling for Image Super-Resolution", "comment": "15 pages, 7 figures, under submission to a journal", "summary": "We propose Frequency-Guided Attention (FGA), a lightweight upsampling module\nfor single image super-resolution. Conventional upsamplers, such as Sub-Pixel\nConvolution, are efficient but frequently fail to reconstruct high-frequency\ndetails and introduce aliasing artifacts. FGA addresses these issues by\nintegrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for\npositional frequency encoding, (2) a cross-resolution Correlation Attention\nLayer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for\nspectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently\nenhances performance across five diverse super-resolution backbones in both\nlightweight and full-capacity scenarios. Experimental results demonstrate\naverage PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by\nup to 29%, particularly evident on texture-rich datasets. Visual and spectral\nevaluations confirm FGA's effectiveness in reducing aliasing and preserving\nfine details, establishing it as a practical, scalable alternative to\ntraditional upsampling methods.", "AI": {"tldr": "本文提出了一种轻量级上采样模块频率引导注意力（FGA），通过结合傅里叶特征MLP、跨分辨率关联注意力层和频域L1损失，有效解决了传统超分上采样方法在高频细节重建和消除混叠伪影方面的不足。", "motivation": "传统的上采样器（如亚像素卷积）虽然高效，但往往无法重建高频细节并引入混叠伪影。", "method": "FGA模块整合了三部分：1) 基于傅里叶特征的多层感知机（MLP）用于位置频率编码；2) 跨分辨率关联注意力层用于自适应空间对齐；3) 频域L1损失用于频谱保真度监督。", "result": "FGA仅增加了0.3M参数，在五种不同的超分辨率骨干网络上（轻量级和全容量场景）均持续提升了性能。实验结果显示，平均PSNR增益为0.12~0.14 dB，频域一致性提高了高达29%，尤其在纹理丰富的数集上表现显著。视觉和频谱评估证实了FGA在减少混叠和保留精细细节方面的有效性。", "conclusion": "FGA是一种实用、可扩展的传统上采样方法替代方案，能有效减少混叠并保留精细细节，提升超分辨率性能。"}}
{"id": "2508.10421", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10421", "abs": "https://arxiv.org/abs/2508.10421", "authors": ["Cai Yang", "Yao Dou", "David Heineman", "Xiaofeng Wu", "Wei Xu"], "title": "Evaluating LLMs on Chinese Idiom Translation", "comment": "Accepted at COLM 2025", "summary": "Idioms, whose figurative meanings usually differ from their literal\ninterpretations, are common in everyday language, especially in Chinese, where\nthey often contain historical references and follow specific structural\npatterns. Despite recent progress in machine translation with large language\nmodels, little is known about Chinese idiom translation. In this work, we\nintroduce IdiomEval, a framework with a comprehensive error taxonomy for\nChinese idiom translation. We annotate 900 translation pairs from nine modern\nsystems, including GPT-4o and Google Translate, across four domains: web, news,\nWikipedia, and social media. We find these systems fail at idiom translation,\nproducing incorrect, literal, partial, or even missing translations. The\nbest-performing system, GPT-4, makes errors in 28% of cases. We also find that\nexisting evaluation metrics measure idiom quality poorly with Pearson\ncorrelation below 0.48 with human ratings. We thus develop improved models that\nachieve F$_1$ scores of 0.68 for detecting idiom translation errors.", "AI": {"tldr": "本文介绍了IdiomEval框架，用于评估中文习语翻译，发现现有大型语言模型在习语翻译方面表现不佳，并指出现有评估指标与人工评分相关性低，因此开发了改进的习语翻译错误检测模型。", "motivation": "习语在日常语言中很常见，尤其是在中文中，其比喻意义通常与字面解释不同，且常包含历史典故和特定结构。尽管大型语言模型在机器翻译方面取得了进展，但对中文习语翻译的了解甚少。", "method": "引入了IdiomEval框架，该框架包含一个全面的错误分类体系。从网络、新闻、维基百科和社交媒体四个领域，对包括GPT-4o和Google Translate在内的九个现代系统的900对翻译对进行了标注。此外，开发了改进的模型来检测习语翻译错误。", "result": "研究发现现有系统在习语翻译方面表现不佳，会产生不正确、字面化、部分或甚至缺失的翻译。表现最好的系统GPT-4在28%的情况下仍会出错。现有评估指标与人工评分的相关性低于0.48，表明其习语质量衡量不佳。开发的改进模型在检测习语翻译错误方面F1分数达到0.68。", "conclusion": "当前机器翻译系统在中文习语翻译方面存在显著缺陷，现有评估指标无法有效衡量习语翻译质量。本研究提出的IdiomEval框架和改进的错误检测模型，为未来习语翻译的评估和改进提供了基础。"}}
{"id": "2508.10554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10554", "abs": "https://arxiv.org/abs/2508.10554", "authors": ["Marc J. Fischer", "Jeffrey Potts", "Gabriel Urreola", "Dax Jones", "Paolo Palmisciano", "E. Bradley Strong", "Branden Cord", "Andrew D. Hernandez", "Julia D. Sharma", "E. Brandon Strong"], "title": "AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications", "comment": "10pages, 3 figures, will be published at ISMAR 2025 (accepted)", "summary": "Augmented Reality (AR) surgical navigation systems are emerging as the next\ngeneration of intraoperative surgical guidance, promising to overcome\nlimitations of traditional navigation systems. However, known issues with AR\ndepth perception due to vergence-accommodation conflict and occlusion handling\nlimitations of the currently commercially available display technology present\nacute challenges in surgical settings where precision is paramount. This study\npresents a novel methodology for utilizing AR guidance to register anatomical\ntargets and provide real-time instrument navigation using placement of\nsimulated external ventricular drain catheters on a phantom model as the\nclinical scenario. The system registers target positions to the patient through\na novel surface tracing method and uses real-time infrared tool tracking to aid\nin catheter placement, relying only on the onboard sensors of the Microsoft\nHoloLens 2. A group of intended users performed the procedure of simulated\ninsertions under two AR guidance conditions: static in-situ visualization,\nwhere planned trajectories are overlaid directly onto the patient anatomy, and\nreal-time tool-tracking guidance, where live feedback of the catheter's pose is\nprovided relative to the plan. Following the insertion tests, computed\ntomography scans of the phantom models were acquired, allowing for evaluation\nof insertion accuracy, target deviation, angular error, and depth precision.\nSystem Usability Scale surveys assessed user experience and cognitive workload.\nTool-tracking guidance improved performance metrics across all accuracy\nmeasures and was preferred by users in subjective evaluations. A free copy of\nthis paper and all supplemental materials are available at\nhttps://bit.ly/45l89Hq.", "AI": {"tldr": "本研究提出了一种基于HoloLens 2的增强现实(AR)手术导航新方法，通过实时工具追踪显著提高了模拟导管置入的精度和用户体验，克服了传统AR深度感知和遮挡处理的局限性。", "motivation": "传统的AR手术导航系统存在深度感知问题（如聚散调节冲突）和当前商用显示技术的遮挡处理限制，这在对精度要求极高的手术环境中带来了严峻挑战。本研究旨在克服这些局限性。", "method": "本研究提出了一种利用AR引导注册解剖目标并提供实时器械导航的新方法。以在人体模型上放置模拟脑室外引流导管为临床场景。系统通过一种新颖的表面追踪方法将目标位置注册到患者，并仅利用Microsoft HoloLens 2的板载传感器进行实时红外工具追踪以辅助导管放置。用户在两种AR引导条件下进行模拟插入：静态原位可视化（计划轨迹直接叠加在解剖结构上）和实时工具追踪引导（提供导管姿态相对于计划的实时反馈）。插入测试后，获取人体模型的CT扫描以评估插入精度、目标偏差、角度误差和深度精度。通过系统可用性量表（SUS）调查评估用户体验和认知负荷。", "result": "实时工具追踪引导在所有精度测量指标上均提高了性能，并在主观评估中受到用户的青睐。", "conclusion": "实时工具追踪的AR引导显著提高了手术精度和用户满意度，为下一代术中手术指导提供了有前景的解决方案，有效解决了当前AR技术在手术应用中的关键挑战。"}}
{"id": "2508.10655", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10655", "abs": "https://arxiv.org/abs/2508.10655", "authors": ["Zhangyong Tang", "Tianyang Xu", "Xuefeng Zhu", "Chunyang Cheng", "Tao Zhou", "Xiaojun Wu", "Josef Kittler"], "title": "Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking", "comment": "ACMMM 2025", "summary": "Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws\nincreasing attention due to the complementary nature of different modalities in\nbuilding robust tracking systems. Existing practices mix all data sensor types\nin a single training procedure, structuring a parallel paradigm from the\ndata-centric perspective and aiming for a global optimum on the joint\ndistribution of the involved tasks. However, the absence of a unified benchmark\nwhere all types of data coexist forces evaluations on separated benchmarks,\ncausing \\textit{inconsistency} between training and testing, thus leading to\nperformance \\textit{degradation}. To address these issues, this work advances\nin two aspects: \\ding{182} A unified benchmark, coined as UniBench300, is\nintroduced to bridge the inconsistency by incorporating multiple task data,\nreducing inference passes from three to one and cutting time consumption by\n27\\%. \\ding{183} The unification process is reformulated in a serial format,\nprogressively integrating new tasks. In this way, the performance degradation\ncan be specified as knowledge forgetting of previous tasks, which naturally\naligns with the philosophy of continual learning (CL), motivating further\nexploration of injecting CL into the unification process. Extensive experiments\nconducted on two baselines and four benchmarks demonstrate the significance of\nUniBench300 and the superiority of CL in supporting a stable unification\nprocess. Moreover, while conducting dedicated analyses, the performance\ndegradation is found to be negatively correlated with network capacity.\nAdditionally, modality discrepancies contribute to varying degradation levels\nacross tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for\nfuture multi-modal vision research. Source codes and the proposed benchmark is\navailable at \\textit{https://github.com/Zhangyong-Tang/UniBench300}.", "AI": {"tldr": "本文提出了一个统一的多模态视觉目标跟踪（MMVOT）基准UniBench300，并通过将任务统一过程重构为序列格式并引入持续学习（CL）来解决现有方法中训练与测试不一致导致的性能下降问题。", "motivation": "现有MMVOT任务统一方法将所有传感器数据混合训练，但缺乏统一的评估基准，导致训练与测试不一致，进而引发性能下降。", "method": "1. 引入统一基准UniBench300，整合多任务数据，减少推理次数并缩短时间。2. 将统一过程重构为序列格式，逐步集成新任务，并将其视为持续学习中的知识遗忘问题，从而引入持续学习方法来稳定统一过程。", "result": "UniBench300显著提升了训练和测试的一致性，减少了推理时间和计算量。持续学习在支持稳定统一过程中表现出优越性。研究发现性能下降与网络容量呈负相关，且模态差异导致不同任务（RGBT > RGBD > RGBE）的下降程度不同。", "conclusion": "UniBench300和持续学习对统一多模态视觉目标跟踪任务具有重要意义。性能下降与网络容量和模态差异相关，这为未来的多模态视觉研究提供了有价值的见解。"}}
{"id": "2508.10426", "categories": ["cs.CL", "I.2.6; I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.10426", "abs": "https://arxiv.org/abs/2508.10426", "authors": ["Sandeep Reddy", "Kabir Khan", "Rohit Patil", "Ananya Chakraborty", "Faizan A. Khan", "Swati Kulkarni", "Arjun Verma", "Neha Singh"], "title": "Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints", "comment": "Preprint; 7 figures, 4 tables, 1 algorithm. Experiments on GLUE\n  (MNLI, STS-B, CoLA) and WikiText-103 with BERT-base; evaluation includes\n  FLOPS, latency, Gini and entropy metrics", "summary": "Large language models (LLMs) are limited by substantial computational cost.\nWe introduce a \"computational economics\" framework that treats an LLM as an\ninternal economy of resource-constrained agents (attention heads and neuron\nblocks) that must allocate scarce computation to maximize task utility. First,\nwe show empirically that when computation is scarce, standard LLMs reallocate\nattention toward high-value tokens while preserving accuracy. Building on this\nobservation, we propose an incentive-driven training paradigm that augments the\ntask loss with a differentiable computation cost term, encouraging sparse and\nefficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method\nyields a family of models that trace a Pareto frontier and consistently\ndominate post-hoc pruning; for a similar accuracy we obtain roughly a forty\npercent reduction in FLOPS and lower latency, together with more interpretable\nattention patterns. These results indicate that economic principles offer a\nprincipled route to designing efficient, adaptive, and more transparent LLMs\nunder strict resource constraints.", "AI": {"tldr": "该研究引入“计算经济学”框架，将大型语言模型（LLM）视为资源受限代理的内部经济体，通过激励驱动的训练范式，显著降低LLM的计算成本（FLOPS减少约40%，延迟降低），同时保持准确性并提高注意力模式的可解释性。", "motivation": "大型语言模型（LLMs）的巨大计算成本限制了其应用和发展。", "method": "1. 提出了一个“计算经济学”框架，将LLM视为由注意力头和神经元块等资源受限代理组成的内部经济体，这些代理必须分配稀缺计算以最大化任务效用。2. 经验性地证明，在计算稀缺时，标准LLM会将注意力重新分配到高价值的token，同时保持准确性。3. 基于此观察，提出了一种激励驱动的训练范式，通过在任务损失中加入一个可微分的计算成本项，鼓励稀疏和高效的激活。", "result": "1. 在GLUE（MNLI, STS-B, CoLA）和WikiText-103数据集上，该方法生成了一系列模型，这些模型构成了帕累托前沿，并持续优于事后剪枝方法。2. 在相似的准确性下，实现了约40%的FLOPS减少和更低的延迟。3. 获得了更具可解释性的注意力模式。", "conclusion": "经济学原理为在严格资源限制下设计高效、自适应和更透明的LLM提供了一条有原则的途径。"}}
{"id": "2508.10566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10566", "abs": "https://arxiv.org/abs/2508.10566", "authors": ["Shiyu Liu", "Kui Jiang", "Xianming Liu", "Hongxun Yao", "Xiaocheng Feng"], "title": "HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis", "comment": null, "summary": "Audio-driven talking head video generation enhances user engagement in\nhuman-computer interaction. However, current methods frequently produce videos\nwith motion blur and lip jitter, primarily due to their reliance on implicit\nmodeling of audio-facial motion correlations--an approach lacking explicit\narticulatory priors (i.e., anatomical guidance for speech-related facial\nmovements). To overcome this limitation, we propose HM-Talker, a novel\nframework for generating high-fidelity, temporally coherent talking heads.\nHM-Talker leverages a hybrid motion representation combining both implicit and\nexplicit motion cues. Explicit cues use Action Units (AUs), anatomically\ndefined facial muscle movements, alongside implicit features to minimize\nphoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement\nModule (CMDM) extracts complementary implicit/explicit motion features while\npredicting AUs directly from audio input aligned to visual cues. To mitigate\nidentity-dependent biases in explicit features and enhance cross-subject\ngeneralization, we introduce the Hybrid Motion Modeling Module (HMMM). This\nmodule dynamically merges randomly paired implicit/explicit features, enforcing\nidentity-agnostic learning. Together, these components enable robust lip\nsynchronization across diverse identities, advancing personalized talking head\nsynthesis. Extensive experiments demonstrate HM-Talker's superiority over\nstate-of-the-art methods in visual quality and lip-sync accuracy.", "AI": {"tldr": "HM-Talker是一个新颖的框架，通过结合隐式和显式（基于动作单元AU）运动线索，生成高质量、时间连贯的音频驱动说话人视频，解决了现有方法中运动模糊和唇部抖动的问题。", "motivation": "现有音频驱动说话人视频生成方法常产生运动模糊和唇部抖动，主要原因是它们依赖隐式建模音频-面部运动关联，缺乏明确的发音先验（即语音相关面部运动的解剖学指导）。", "method": "HM-Talker采用混合运动表示，结合隐式和显式运动线索（如动作单元AUs）。它包含两个关键模块：1. 跨模态解耦模块（CMDM），用于提取互补的隐式/显式运动特征，并直接从音频预测AUs。2. 混合运动建模模块（HMMM），通过动态合并随机配对的隐式/显式特征，减轻显式特征中的身份依赖偏差，实现身份无关学习，从而增强跨主题泛化能力。", "result": "HM-Talker的组件共同实现了跨不同身份的鲁棒唇部同步。大量实验证明，HM-Talker在视觉质量和唇同步精度方面均优于现有最先进的方法。", "conclusion": "HM-Talker通过创新性地结合隐式和显式运动表示，有效克服了现有音频驱动说话人视频生成中的局限性，显著提升了视频的视觉质量和唇同步准确性，并在个性化说话人合成方面取得了重要进展。"}}
{"id": "2508.10667", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10667", "abs": "https://arxiv.org/abs/2508.10667", "authors": ["Shixiong Xu", "Chenghao Zhang", "Lubin Fan", "Yuan Zhou", "Bin Fan", "Shiming Xiang", "Gaofeng Meng", "Jieping Ye"], "title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models", "comment": null, "summary": "Large visual language models (LVLMs) have demonstrated impressive performance\nin coarse-grained geo-localization at the country or city level, but they\nstruggle with fine-grained street-level localization within urban areas. In\nthis paper, we explore integrating city-wide address localization capabilities\ninto LVLMs, facilitating flexible address-related question answering using\nstreet-view images. A key challenge is that the street-view visual\nquestion-and-answer (VQA) data provides only microscopic visual cues, leading\nto subpar performance in fine-tuned models. To tackle this issue, we\nincorporate perspective-invariant satellite images as macro cues and propose\ncross-view alignment tuning including a satellite-view and street-view image\ngrafting mechanism, along with an automatic label generation mechanism. Then\nLVLM's global understanding of street distribution is enhanced through\ncross-view matching. Our proposed model, named AddressVLM, consists of\ntwo-stage training protocols: cross-view alignment tuning and address\nlocalization tuning. Furthermore, we have constructed two street-view VQA\ndatasets based on image address localization datasets from Pittsburgh and San\nFrancisco. Qualitative and quantitative evaluations demonstrate that AddressVLM\noutperforms counterpart LVLMs by over 9% and 12% in average address\nlocalization accuracy on these two datasets, respectively.", "AI": {"tldr": "本文提出AddressVLM模型，通过整合卫星图像作为宏观线索和跨视图对齐微调，显著提升了大型视觉语言模型（LVLMs）在城市区域内的精细街景地址定位能力，并在新构建的数据集上取得了优于现有LVLMs的性能。", "motivation": "现有LVLMs在国家或城市层面的粗粒度地理定位表现出色，但在城市区域内的精细街景级定位上表现不佳。街景VQA数据仅提供微观视觉线索，导致微调模型性能不佳。研究旨在将城市范围的地址定位能力集成到LVLMs中，以实现基于街景图像的灵活地址相关问答。", "method": "引入透视不变的卫星图像作为宏观线索，并提出跨视图对齐微调机制，包括卫星视图和街景图像嫁接机制以及自动标签生成机制。通过跨视图匹配增强LVLM对街道分布的全局理解。模型名为AddressVLM，采用两阶段训练协议：跨视图对齐微调和地址定位微调。此外，构建了两个基于匹兹堡和旧金山图像地址定位数据集的街景VQA数据集。", "result": "定性和定量评估表明，AddressVLM在所构建的两个数据集上，平均地址定位精度分别超过同类LVLMs 9%和12%。", "conclusion": "通过结合卫星图像的宏观线索和创新的跨视图对齐微调策略，AddressVLM成功地将精细的街景地址定位能力集成到LVLMs中，显著优于现有模型，为基于街景图像的地址相关问答提供了有效解决方案。"}}
{"id": "2508.10444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10444", "abs": "https://arxiv.org/abs/2508.10444", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Xiangzheng Kong", "Zihan Ma", "Zhi Zeng"], "title": "DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales", "comment": null, "summary": "Generating textual rationales from large vision-language models (LVLMs) to\nsupport trainable multimodal misinformation detectors has emerged as a\npromising paradigm. However, its effectiveness is fundamentally limited by\nthree core challenges: (i) insufficient diversity in generated rationales, (ii)\nfactual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting\ncontent that introduces noise. We introduce DiFaR, a detector-agnostic\nframework that produces diverse, factual, and relevant rationales to enhance\nmisinformation detection. DiFaR employs five chain-of-thought prompts to elicit\nvaried reasoning traces from LVLMs and incorporates a lightweight post-hoc\nfiltering module to select rationale sentences based on sentence-level\nfactuality and relevance scores. Extensive experiments on four popular\nbenchmarks demonstrate that DiFaR outperforms four baseline categories by up to\n5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics\nand human evaluations confirm that DiFaR significantly improves rationale\nquality across all three dimensions.", "AI": {"tldr": "DiFaR是一个检测器无关的框架，通过多样的思维链提示和后处理过滤，生成多样化、事实准确、相关性强的文本理由，以增强多模态虚假信息检测器的性能。", "motivation": "现有的大型视觉-语言模型（LVLMs）生成的文本理由存在三个核心挑战：多样性不足、幻觉导致的事实不准确以及无关或冲突内容的引入，这些限制了其在可训练多模态虚假信息检测器中的有效性。", "method": "DiFaR框架采用五种思维链（chain-of-thought）提示来从LVLMs中引出不同的推理轨迹，并整合了一个轻量级的后处理过滤模块，根据句子级别的事实性和相关性得分选择理由句子。", "result": "在四个流行基准上的广泛实验表明，DiFaR比四类基线模型表现高出多达5.9%，并能将现有检测器的性能提升高达8.7%。自动指标和人工评估均证实DiFaR在多样性、事实准确性和相关性三个维度上显著提高了理由质量。", "conclusion": "DiFaR显著提高了生成理由的质量，从而有效增强了基于LVLM的虚假信息检测器的性能，解决了现有方法在理由生成方面的关键限制。"}}
{"id": "2508.10568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10568", "abs": "https://arxiv.org/abs/2508.10568", "authors": ["Humza Naveed", "Xina Zeng", "Mitch Bryson", "Nagita Mehrseresht"], "title": "Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection", "comment": "work in progress", "summary": "Foundational models have achieved significant success in diverse domains of\ncomputer vision. They learn general representations that are easily\ntransferable to tasks not seen during training. One such foundational model is\nSegment anything model (SAM), which can accurately segment objects in images.\nWe propose adapting the SAM encoder via fine-tuning for remote sensing change\ndetection (RSCD) along with spatial-temporal feature enhancement (STFE) and\nmulti-scale decoder fusion (MSDF) to detect changes robustly at multiple\nscales. Additionally, we propose a novel cross-entropy masking (CEM) loss to\nhandle high class imbalance in change detection datasets. Our method\noutperforms state-of-the-art (SOTA) methods on four change detection datasets,\nLevir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on\na large complex S2Looking dataset. The code is available at:\nhttps://github.com/humza909/SAM-CEM-CD", "AI": {"tldr": "本文提出了一种名为SAM-CEM-CD的方法，通过微调SAM编码器并结合时空特征增强、多尺度解码器融合以及新颖的交叉熵掩蔽损失，显著提升了遥感图像变化检测的性能。", "motivation": "基础模型（如SAM）在计算机视觉领域取得了巨大成功，但遥感变化检测面临多尺度变化和类别不平衡等挑战，需要专门的适应性改进。", "method": "该方法包括：1) 微调SAM编码器以适应遥感变化检测；2) 引入时空特征增强（STFE）和多尺度解码器融合（MSDF）以鲁棒地检测多尺度变化；3) 提出一种新的交叉熵掩蔽（CEM）损失来处理变化检测数据集中严重的类别不平衡问题。", "result": "该方法在Levir-CD、WHU-CD、CLCD和S2Looking四个变化检测数据集上均超越了现有最先进（SOTA）方法，特别是在大型复杂S2Looking数据集上F1分数提高了2.5%。", "conclusion": "通过对SAM的有效适应和引入针对变化检测特性的新模块及损失函数，本文提出的方法在遥感变化检测任务中表现出卓越的性能和鲁棒性。"}}
{"id": "2508.10672", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10672", "abs": "https://arxiv.org/abs/2508.10672", "authors": ["Feiran Li", "Qianqian Xu", "Shilong Bao", "Boyu Han", "Zhiyong Yang", "Qingming Huang"], "title": "Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation", "comment": "This paper has been accpeted to ICCV 2025 DataCV Workshop", "summary": "In this paper, we present our approach to the DataCV ICCV Challenge, which\ncenters on building a high-quality face dataset to train a face recognition\nmodel. The constructed dataset must not contain identities overlapping with any\nexisting public face datasets. To handle this challenge, we begin with a\nthorough cleaning of the baseline HSFace dataset, identifying and removing\nmislabeled or inconsistent identities through a Mixture-of-Experts (MoE)\nstrategy combining face embedding clustering and GPT-4o-assisted verification.\nWe retain the largest consistent identity cluster and apply data augmentation\nup to a fixed number of images per identity. To further diversify the dataset,\nwe generate synthetic identities using Stable Diffusion with prompt\nengineering. As diffusion models are computationally intensive, we generate\nonly one reference image per identity and efficiently expand it using Vec2Face,\nwhich rapidly produces 49 identity-consistent variants. This hybrid approach\nfuses GAN-based and diffusion-based samples, enabling efficient construction of\na diverse and high-quality dataset. To address the high visual similarity among\nsynthetic identities, we adopt a curriculum learning strategy by placing them\nearly in the training schedule, allowing the model to progress from easier to\nharder samples. Our final dataset contains 50 images per identity, and all\nnewly generated identities are checked with mainstream face datasets to ensure\nno identity leakage. Our method achieves \\textbf{1st place} in the competition,\nand experimental results show that our dataset improves model performance\nacross 10K, 20K, and 100K identity scales. Code is available at\nhttps://github.com/Ferry-Li/datacv_fr.", "AI": {"tldr": "本文提出了一种构建高质量、无重叠人脸数据集的方法，结合真实数据清洗、数据增强和混合生成模型（Stable Diffusion与Vec2Face），并在DataCV ICCV挑战赛中获得第一名。", "motivation": "参与DataCV ICCV挑战赛，目标是构建一个高质量的人脸数据集，用于训练人脸识别模型，且该数据集不能与任何现有公共人脸数据集存在身份重叠。", "method": "首先，采用MoE（结合人脸嵌入聚类和GPT-4o验证）策略对基线HSFace数据集进行彻底清洗，移除错误或不一致的身份，并保留最大的连贯身份簇，对每种身份应用数据增强。其次，利用Stable Diffusion结合提示工程生成合成身份的参考图像，并通过Vec2Face高效扩展为49个身份一致的变体，形成GAN和扩散模型混合的样本。为解决合成身份间的高度视觉相似性，采用课程学习策略。最后，对所有新生成身份进行检查，确保与主流人脸数据集无身份泄露。", "result": "构建的最终数据集每种身份包含50张图像，方法在DataCV ICCV挑战赛中获得第一名。实验结果表明，该数据集在1万、2万和10万身份规模上均显著提升了模型性能。", "conclusion": "所提出的混合方法能够高效构建多样化且高质量的人脸数据集，有效避免了身份重叠问题，并显著提升了人脸识别模型的性能，证明了其在实际应用中的有效性。"}}
{"id": "2508.10482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10482", "abs": "https://arxiv.org/abs/2508.10482", "authors": ["Mahdi Dhaini", "Stephen Meisenbacher", "Ege Erdogan", "Florian Matthes", "Gjergji Kasneci"], "title": "When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing", "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "In the study of trustworthy Natural Language Processing (NLP), a number of\nimportant research fields have emerged, including that of\n\\textit{explainability} and \\textit{privacy}. While research interest in both\nexplainable and privacy-preserving NLP has increased considerably in recent\nyears, there remains a lack of investigation at the intersection of the two.\nThis leaves a considerable gap in understanding of whether achieving\n\\textit{both} explainability and privacy is possible, or whether the two are at\nodds with each other. In this work, we conduct an empirical investigation into\nthe privacy-explainability trade-off in the context of NLP, guided by the\npopular overarching methods of \\textit{Differential Privacy} (DP) and Post-hoc\nExplainability. Our findings include a view into the intricate relationship\nbetween privacy and explainability, which is formed by a number of factors,\nincluding the nature of the downstream task and choice of the text\nprivatization and explainability method. In this, we highlight the potential\nfor privacy and explainability to co-exist, and we summarize our findings in a\ncollection of practical recommendations for future work at this important\nintersection.", "AI": {"tldr": "本研究实证调查了自然语言处理（NLP）中隐私与可解释性之间的权衡，发现两者可能共存，并提供了实践建议。", "motivation": "尽管可解释性NLP和隐私保护NLP的研究日益增多，但两者交叉领域的研究不足，不清楚它们是否能同时实现或相互矛盾，这构成了理解上的重大空白。", "method": "在NLP背景下，采用差分隐私（DP）和事后可解释性（Post-hoc Explainability）这两种流行方法，对隐私-可解释性权衡进行了实证研究。", "result": "研究揭示了隐私和可解释性之间复杂的相互关系，这种关系受下游任务性质、文本隐私化方法和可解释性方法选择等多种因素影响。研究强调了隐私和可解释性共存的可能性。", "conclusion": "研究总结了发现，并为未来在该重要交叉领域的工作提供了一系列实用建议。"}}
{"id": "2508.10572", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10572", "abs": "https://arxiv.org/abs/2508.10572", "authors": ["Tuyen Tran", "Thao Minh Le", "Truyen Tran"], "title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation", "comment": null, "summary": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.", "AI": {"tldr": "针对基于指代视频目标分割（RVOS）任务，传统方法计算成本高且缺乏灵活性。本文提出一种多模态智能体，利用大语言模型（LLMs）生成动态工作流并与专用工具交互，以更灵活自适应的方式识别目标，并在RVOS和Ref-AVS任务上取得了显著改进。", "motivation": "基于指代视频目标分割（RVOS）是一个复杂的跨模态任务，需要精细分割。传统方法计算复杂且需要大量标注。尽管最新的视觉-语言基础模型提供了免训练方案，但现有方法采用固定流程，缺乏适应任务动态性的灵活性，限制了其应用。", "method": "提出“多模态智能体”系统，利用大语言模型（LLMs）的推理能力，为每个输入动态生成定制化的工作流程。该自适应过程迭代地与一组针对不同模态的低级任务设计的专用工具进行交互，以识别多模态线索描述的目标对象。", "result": "所提出的智能体方法在两个多模态条件下的VOS任务（RVOS和Ref-AVS）上，相比现有方法展现出明显的性能提升。", "conclusion": "通过引入基于LLMs的动态工作流生成和工具交互，多模态智能体为基于指代视频目标分割提供了一种更灵活和自适应的解决方案，有效克服了现有方法的固定流程限制，并在相关基准测试中取得了优越表现。"}}
{"id": "2508.10687", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10687", "abs": "https://arxiv.org/abs/2508.10687", "authors": ["Safaeid Hossain Arib", "Rabeya Akter", "Sejuti Rahman"], "title": "Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph", "comment": null, "summary": "Millions of individuals worldwide are affected by deafness and hearing\nimpairment. Sign language serves as a sophisticated means of communication for\nthe deaf and hard of hearing. However, in societies that prioritize spoken\nlanguages, sign language often faces underestimation, leading to communication\nbarriers and social exclusion. The Continuous Bangla Sign Language Translation\nproject aims to address this gap by enhancing translation methods. While recent\napproaches leverage transformer architecture for state-of-the-art results, our\nmethod integrates graph-based methods with the transformer architecture. This\nfusion, combining transformer and STGCN-LSTM architectures, proves more\neffective in gloss-free translation. Our contributions include architectural\nfusion, exploring various fusion strategies, and achieving a new\nstate-of-the-art performance on diverse sign language datasets, namely\nRWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach\ndemonstrates superior performance compared to current translation outcomes\nacross all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,\n2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in\nRWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce\nbenchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a\nbenchmark for future research, emphasizing the importance of gloss-free\ntranslation to improve communication accessibility for the deaf and hard of\nhearing.", "AI": {"tldr": "该项目提出一种结合Transformer和STGCN-LSTM的混合架构，用于无中间标注（gloss-free）的连续孟加拉手语翻译，并在多个数据集上取得了最先进的性能。", "motivation": "全球数百万听障人士面临手语被低估、沟通障碍和社会排斥的问题。手语是听障人士的重要沟通方式，但现有翻译方法仍有提升空间，尤其是在优先口语的社会中。", "method": "该研究将基于图的方法（STGCN-LSTM）与Transformer架构融合，探索了多种融合策略，旨在实现无中间标注（gloss-free）的手语翻译。", "result": "该方法在RWTH-PHOENIX-2014T、CSL-Daily和How2Sign等多个手语数据集上取得了新的最先进性能，BLEU-4分数分别提升了4.01、2.07和0.5。此外，首次在BornilDB v1.0数据集上进行了基准测试。", "conclusion": "该融合方法在手语翻译中表现出卓越性能，为未来的手语翻译研究设定了新基准，并强调了无中间标注翻译对于改善听障人士沟通可及性的重要性。"}}
{"id": "2508.10553", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10553", "abs": "https://arxiv.org/abs/2508.10553", "authors": ["Irma Heithoff. Marc Guggenberger", "Sandra Kalogiannis", "Susanne Mayer", "Fabian Maag", "Sigurd Schacht", "Carsten Lanquillon"], "title": "eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM", "comment": "9 pages", "summary": "This paper presents a feasibility study on the deployment of a European Deep\nInference Fabric (eDIF), an NDIF-compatible infrastructure designed to support\nmechanistic interpretability research on large language models. The need for\nwidespread accessibility of LLM interpretability infrastructure in Europe\ndrives this initiative to democratize advanced model analysis capabilities for\nthe research community. The project introduces a GPU-based cluster hosted at\nAnsbach University of Applied Sciences and interconnected with partner\ninstitutions, enabling remote model inspection via the NNsight API. A\nstructured pilot study involving 16 researchers from across Europe evaluated\nthe platform's technical performance, usability, and scientific utility. Users\nconducted interventions such as activation patching, causal tracing, and\nrepresentation analysis on models including GPT-2 and DeepSeek-R1-70B. The\nstudy revealed a gradual increase in user engagement, stable platform\nperformance throughout, and a positive reception of the remote experimentation\ncapabilities. It also marked the starting point for building a user community\naround the platform. Identified limitations such as prolonged download\ndurations for activation data as well as intermittent execution interruptions\nare addressed in the roadmap for future development. This initiative marks a\nsignificant step towards widespread accessibility of LLM interpretability\ninfrastructure in Europe and lays the groundwork for broader deployment,\nexpanded tooling, and sustained community collaboration in mechanistic\ninterpretability research.", "AI": {"tldr": "本文提出并评估了欧洲深度推理结构（eDIF）的可行性，这是一个兼容NDIF的GPU集群基础设施，旨在支持欧洲LLM可解释性研究的普及和民主化。", "motivation": "为了在欧洲研究界普及大型语言模型（LLM）的可解释性基础设施，使研究人员能够广泛访问先进的模型分析能力。", "method": "项目部署了一个基于GPU的集群（eDIF），托管于安斯巴赫应用科学大学，并与合作机构互联。通过NNsight API实现远程模型检查。一个由16名欧洲研究人员参与的结构化试点研究评估了平台的性能、可用性和科学效用，用户进行了激活修补、因果追踪和表示分析等操作。", "result": "研究显示用户参与度逐渐增加，平台性能稳定，远程实验能力受到积极评价。该项目也为建立用户社区奠定了基础。同时，也发现了局限性，例如激活数据下载时间过长和间歇性执行中断。", "conclusion": "这项倡议是欧洲LLM可解释性基础设施广泛普及的重要一步，为未来的更广泛部署、工具扩展和持续社区合作奠定了基础，以推动机械可解释性研究。"}}
{"id": "2508.10576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10576", "abs": "https://arxiv.org/abs/2508.10576", "authors": ["Zheng Qin", "Ruobing Zheng", "Yabing Wang", "Tianqi Li", "Yi Yuan", "Jingdong Chen", "Le Wang"], "title": "HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) show immense promise for\nachieving truly human-like interactions, progress is hindered by the lack of\nfine-grained evaluation frameworks for human-centered scenarios, encompassing\nboth the understanding of complex human intentions and the provision of\nempathetic, context-aware responses. Here we introduce HumanSense, a\ncomprehensive benchmark designed to evaluate the human-centered perception and\ninteraction capabilities of MLLMs, with a particular focus on deep\nunderstanding of extended multimodal contexts and the formulation of rational\nfeedback. Our evaluation reveals that leading MLLMs still have considerable\nroom for improvement, particularly for advanced interaction-oriented tasks.\nSupplementing visual input with audio and text information yields substantial\nimprovements, and Omni-modal models show advantages on these tasks.\nFurthermore, we argue that appropriate feedback stems from a contextual\nanalysis of the interlocutor's needs and emotions, with reasoning ability\nserving as the key to unlocking it. Accordingly, we employ a multi-stage,\nmodality-progressive reinforcement learning to enhance the reasoning abilities\nof an Omni model, achieving substantial gains on evaluation results.\nAdditionally, we observe that successful reasoning processes exhibit highly\nconsistent thought patterns. By designing corresponding prompts, we also\nenhance the performance of non-reasoning models in a training-free manner.\nProject page:\n\\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/", "AI": {"tldr": "该研究引入了HumanSense基准，用于评估多模态大语言模型（MLLMs）在以人为中心的感知和交互能力，并提出了一种基于多阶段、模态渐进式强化学习的方法来增强模型的推理能力，以提升其表现。", "motivation": "当前MLLMs在实现类人交互方面面临挑战，缺乏针对以人为中心场景的细粒度评估框架，难以有效评估其对复杂人类意图的理解以及提供富有同情心、上下文感知响应的能力。", "method": "1. 提出了HumanSense基准，用于评估MLLMs在深度理解扩展多模态上下文和形成合理反馈方面的能力。2. 对主流MLLMs进行了评估。3. 采用多阶段、模态渐进式强化学习来增强全模态（Omni-modal）模型的推理能力。4. 通过设计相应的提示，以无训练方式提升非推理模型的性能。", "result": "1. 领先的MLLMs在高级交互导向任务上仍有显著改进空间。2. 补充音频和文本信息可大幅提升视觉输入下的性能。3. 全模态模型在这些任务上表现出优势。4. 通过强化学习显著提升了全模态模型的评估结果。5. 成功的推理过程表现出高度一致的思维模式。6. 设计的提示能以无训练方式提升非推理模型的性能。", "conclusion": "MLLMs在以人为中心的交互方面仍需大幅改进，特别是对于高级任务。多模态输入（如音频和文本补充视觉）以及强大的推理能力是实现更佳交互的关键。本研究提出的HumanSense基准和基于强化学习的推理增强方法为提升MLLMs的人类中心感知和交互能力提供了有效途径。"}}
{"id": "2508.10695", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.10695", "abs": "https://arxiv.org/abs/2508.10695", "authors": ["Alireza Salemi", "Hamed Zamani"], "title": "Learning from Natural Language Feedback for Personalized Question Answering", "comment": null, "summary": "Personalization is crucial for enhancing both the effectiveness and user\nsatisfaction of language technologies, particularly in information-seeking\ntasks like question answering. Current approaches for personalizing large\nlanguage models (LLMs) often rely on retrieval-augmented generation (RAG),\nfollowed by reinforcement learning with scalar reward signals to teach models\nhow to use retrieved personal context. We believe that these scalar rewards\nsometimes provide weak, non-instructive feedback, limiting learning efficiency\nand personalization quality. We introduce VAC, a novel framework for\npersonalized response generation that replaces scalar rewards with natural\nlanguage feedback (NLF) that are generated conditioned on the user profiles and\nthe question narratives. NLF serves as a rich and actionable supervision\nsignal, allowing the policy model to iteratively refine its outputs and\ninternalize effective personalization strategies. Training alternates between\noptimizing the feedback model and fine-tuning the policy model on the improved\nresponses, resulting in a policy model that no longer requires feedback at\ninference. Evaluation on the LaMP-QA benchmark that consists of three diverse\ndomains demonstrates consistent and significant improvements over the\nstate-of-the-art results. Human evaluations further confirm the superior\nquality of the generated responses. These results demonstrate that NLF provides\nmore effective signals for optimizing personalized question answering.", "AI": {"tldr": "提出VAC框架，通过自然语言反馈（NLF）而非标量奖励来优化大型语言模型（LLMs）的个性化问答，显著提升了效果。", "motivation": "当前LLMs个性化方法（如RAG结合标量奖励的强化学习）中的标量奖励反馈信号弱且缺乏指导性，限制了学习效率和个性化质量。", "method": "引入VAC框架，用基于用户档案和问题叙述生成的自然语言反馈（NLF）替代标量奖励。训练过程交替优化反馈模型和策略模型，使策略模型在推理时不再需要反馈。", "result": "在LaMP-QA基准测试（包含三个不同领域）上，取得了持续且显著优于SOTA的改进。人工评估也证实了生成回复的更高质量。", "conclusion": "自然语言反馈（NLF）为优化个性化问答提供了更有效的信号。"}}
{"id": "2508.10683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10683", "abs": "https://arxiv.org/abs/2508.10683", "authors": ["Nasma Chaoui", "Richard Khoury"], "title": "Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages", "comment": null, "summary": "This paper presents the first systematic study of strategies for translating\nCoptic into French. Our comprehensive pipeline systematically evaluates: pivot\nversus direct translation, the impact of pre-training, the benefits of\nmulti-version fine-tuning, and model robustness to noise. Utilizing aligned\nbiblical corpora, we demonstrate that fine-tuning with a stylistically-varied\nand noise-aware training corpus significantly enhances translation quality. Our\nfindings provide crucial practical insights for developing translation tools\nfor historical languages in general.", "AI": {"tldr": "首次系统性研究科普特语到法语的翻译策略。", "motivation": "为历史语言（尤其是科普特语）开发翻译工具，并提供普适性实践见解。", "method": "系统评估了中介翻译与直接翻译、预训练影响、多版本微调益处以及模型对噪声的鲁棒性。利用对齐的圣经语料库，并采用风格多样且感知噪声的训练语料进行微调。", "result": "使用风格多样且感知噪声的训练语料进行微调，能显著提升翻译质量。", "conclusion": "为开发历史语言翻译工具提供了重要的实践见解。"}}
{"id": "2508.10582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10582", "abs": "https://arxiv.org/abs/2508.10582", "authors": ["Yixing Liu", "Minggui Teng", "Yifei Xia", "Peiqi Duan", "Boxin Shi"], "title": "EvTurb: Event Camera Guided Turbulence Removal", "comment": null, "summary": "Atmospheric turbulence degrades image quality by introducing blur and\ngeometric tilt distortions, posing significant challenges to downstream\ncomputer vision tasks. Existing single-image and multi-frame methods struggle\nwith the highly ill-posed nature of this problem due to the compositional\ncomplexity of turbulence-induced distortions. To address this, we propose\nEvTurb, an event guided turbulence removal framework that leverages high-speed\nevent streams to decouple blur and tilt effects. EvTurb decouples blur and tilt\neffects by modeling event-based turbulence formation, specifically through a\nnovel two-step event-guided network: event integrals are first employed to\nreduce blur in the coarse outputs. This is followed by employing a variance\nmap, derived from raw event streams, to eliminate the tilt distortion for the\nrefined outputs. Additionally, we present TurbEvent, the first real-captured\ndataset featuring diverse turbulence scenarios. Experimental results\ndemonstrate that EvTurb surpasses state-of-the-art methods while maintaining\ncomputational efficiency.", "AI": {"tldr": "EvTurb是一个事件引导的湍流去除框架，利用高帧率事件流解耦图像模糊和倾斜，并通过两步网络实现，同时发布了首个真实湍流数据集TurbEvent，效果超越现有方法且计算高效。", "motivation": "大气湍流导致图像质量下降，引入模糊和几何倾斜畸变，对计算机视觉任务构成挑战。现有单图像和多帧方法难以处理这种高度病态且复杂的湍流畸变。", "method": "提出EvTurb框架，利用高速事件流解耦模糊和倾斜效应。通过新颖的两步事件引导网络实现：首先利用事件积分减少粗输出中的模糊；然后利用原始事件流导出的方差图消除精细输出的倾斜畸变。此外，还提出了首个真实捕获的湍流数据集TurbEvent。", "result": "实验结果表明，EvTurb在保持计算效率的同时，超越了最先进的方法。", "conclusion": "EvTurb通过利用事件数据有效解决了大气湍流造成的图像畸变问题，实现了卓越的性能和效率，并为该领域提供了新的数据集。"}}
{"id": "2508.10729", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10729", "abs": "https://arxiv.org/abs/2508.10729", "authors": ["Yanjun Li", "Yuqian Fu", "Tianwen Qian", "Qi'ao Xu", "Silong Dai", "Danda Pani Paudel", "Luc Van Gool", "Xiaoling Wang"], "title": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly pushed the frontier of egocentric video question answering\n(EgocentricQA). However, existing benchmarks and studies are mainly limited to\ncommon daily activities such as cooking and cleaning. In contrast, real-world\ndeployment inevitably encounters domain shifts, where target domains differ\nsubstantially in both visual style and semantic content. To bridge this gap, we\nintroduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the\ncross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four\ndiverse and challenging domains, including surgery, industry, extreme sports,\nand animal perspective, representing realistic and high-impact application\nscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,\nspanning four key QA tasks: prediction, recognition, localization, and\ncounting. Each QA pair provides both OpenQA and CloseQA formats to support\nfine-grained evaluation. Extensive experiments show that most existing MLLMs,\nwhether general-purpose or egocentric-specialized, struggle to generalize to\ndomains beyond daily life, highlighting the limitations of current models.\nFurthermore, we conduct several pilot studies, \\eg, fine-tuning and\nreinforcement learning, to explore potential improvements. We hope EgoCross and\nour accompanying analysis will serve as a foundation for advancing\ndomain-adaptive, robust egocentric video understanding. Data and codes will be\nreleased at:\n\\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}", "AI": {"tldr": "引入EgoCross基准测试，以评估多模态大语言模型（MLLMs）在自我中心视频问答（EgocentricQA）中跨领域泛化能力，并发现现有模型在此方面表现不佳。", "motivation": "现有自我中心视频问答（EgocentricQA）基准主要限于日常活动，但实际应用会遇到显著的领域差异（视觉风格和语义内容），需要评估模型在不同领域下的泛化能力。", "method": "提出了EgoCross基准测试，包含约1000个QA对和798个视频片段，涵盖外科手术、工业、极限运动和动物视角四个多样化领域。支持预测、识别、定位和计数四种QA任务，并提供开放式和封闭式两种问答格式。此外，还进行了微调和强化学习等初步研究以探索改进方向。", "result": "实验表明，大多数现有MLLMs（无论是通用型还是自我中心专业型）都难以泛化到日常活动之外的领域，揭示了当前模型的局限性。", "conclusion": "EgoCross基准和伴随的分析旨在为推进领域自适应、鲁棒的自我中心视频理解奠定基础。"}}
{"id": "2508.10736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10736", "abs": "https://arxiv.org/abs/2508.10736", "authors": ["Xiangqi Jin", "Yuxuan Wang", "Yifeng Gao", "Zichen Wen", "Biqing Qi", "Dongrui Liu", "Linfeng Zhang"], "title": "Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs", "comment": null, "summary": "Despite large language models (LLMs) have achieved remarkable success, their\nprefix-only prompting paradigm and sequential generation process offer limited\nflexibility for bidirectional information. Diffusion large language models\n(dLLMs) present new opportunities through their bidirectional attention\nmechanisms and iterative refinement processes, enabling more flexible in-place\nprompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting\nwith Early Exit), a novel framework that transforms prefix-only prompting into\nin-place prompting specifically designed for dLLMs. ICE integrates in-place\nprompts directly within masked token positions during iterative refinement and\nemploys a confidence-aware early exit mechanism to significantly reduce\ncomputational overhead. Extensive experiments demonstrate ICE's effectiveness,\nachieving up to 17.29% accuracy improvement with 4.12$\\times$ speedup on GSM8K,\nand up to 276.67$\\times$ acceleration on MMLU while maintaining competitive\nperformance.", "AI": {"tldr": "本文提出ICE框架，将扩散大语言模型(dLLMs)的提示范式从前缀式转变为原地式，并通过置信度感知早停机制显著提高性能和计算效率。", "motivation": "现有大语言模型(LLMs)的前缀式提示和顺序生成过程在双向信息处理上灵活性有限。扩散大语言模型(dLLMs)凭借其双向注意力机制和迭代优化过程，为更灵活的原地提示策略提供了新机遇。", "method": "引入了ICE（In-Place Chain-of-Thought Prompting with Early Exit）框架，专门为dLLMs设计，将前缀式提示转换为原地提示。ICE在迭代优化过程中将原地提示直接集成到掩码标记位置，并采用置信度感知的早停机制以大幅降低计算开销。", "result": "ICE框架在GSM8K数据集上实现了高达17.29%的准确率提升和4.12倍的速度提升；在MMLU数据集上实现了高达276.67倍的加速，同时保持了有竞争力的性能。", "conclusion": "ICE框架有效地将dLLMs的提示范式从前缀式转变为原地式，通过结合原地提示和早停机制，在提高准确率的同时显著加速了推理过程，展现了dLLMs在更灵活提示策略方面的潜力。"}}
{"id": "2508.10600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10600", "abs": "https://arxiv.org/abs/2508.10600", "authors": ["Yuxin Cao", "Yedi Zhang", "Wentao He", "Yifan Liao", "Yan Xiao", "Chang Li", "Zhiyong Huang", "Jin Song Dong"], "title": "Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving", "comment": "13 pages, 4 figures", "summary": "Learning-based autonomous driving systems remain critically vulnerable to\nadversarial patches, posing serious safety and security risks in their\nreal-world deployment. Black-box attacks, notable for their high attack success\nrate without model knowledge, are especially concerning, with their\ntransferability extensively studied to reduce computational costs compared to\nquery-based attacks. Previous transferability-based black-box attacks typically\nadopt mean Average Precision (mAP) as the evaluation metric and design training\nloss accordingly. However, due to the presence of multiple detected bounding\nboxes and the relatively lenient Intersection over Union (IoU) thresholds, the\nattack effectiveness of these approaches is often overestimated, resulting in\nreduced success rates in practical attacking scenarios. Furthermore, patches\ntrained on low-resolution data often fail to maintain effectiveness on\nhigh-resolution images, limiting their transferability to autonomous driving\ndatasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch\nAttack framework for 2D object detection in autonomous driving, specifically\noptimized for high-resolution datasets. First, we introduce a novel metric,\nPractical Attack Success Rate (PASR), to more accurately quantify attack\neffectiveness with greater relevance for pedestrian safety. Second, we present\na tailored Localization-Confidence Suppression Loss (LCSL) to improve attack\ntransferability under PASR. Finally, to maintain the transferability for\nhigh-resolution datasets, we further incorporate the Probabilistic\nScale-Preserving Padding (PSPP) into the patch attack pipeline as a data\npreprocessing step. Extensive experiments show that P$^3$A outperforms\nstate-of-the-art attacks on unseen models and unseen high-resolution datasets,\nboth under the proposed practical IoU-based evaluation metric and the previous\nmAP-based metrics.", "AI": {"tldr": "P$^3$A是一个针对自动驾驶中2D目标检测的强大实用补丁攻击框架，通过引入新的评估指标PASR、定制的LCSL损失和PSPP数据预处理，解决了现有黑盒攻击在实际场景中有效性被高估以及对高分辨率数据转移性差的问题，显著提升了攻击效果。", "motivation": "基于学习的自动驾驶系统极易受到对抗性补丁的攻击，构成严重的安全风险。现有的基于迁移性的黑盒攻击方法常采用mAP评估指标和宽松的IoU阈值，导致攻击效果被高估，实际成功率降低。此外，在低分辨率数据上训练的补丁在处理高分辨率图像时效果不佳，限制了其在自动驾驶数据集上的迁移性。", "method": "P$^3$A框架包含三个核心部分：1. 引入一种新的度量标准“实际攻击成功率（PASR）”，以更准确地量化攻击效果，并与行人安全更相关。2. 提出一种定制的“定位-置信度抑制损失（LCSL）”，以提高在PASR下的攻击迁移性。3. 将“概率尺度保留填充（PSPP）”作为数据预处理步骤纳入补丁攻击流程，以在高分辨率数据集上保持迁移性。", "result": "广泛的实验表明，P$^3$A在未见过的模型和未见过的高分辨率数据集上，无论是使用所提出的基于IoU的评估指标（PASR）还是之前的基于mAP的指标，都优于最先进的攻击方法。", "conclusion": "P$^3$A通过引入更准确的评估指标、定制的损失函数和针对高分辨率数据的预处理方法，显著提升了黑盒对抗性补丁攻击在自动驾驶系统中的实际有效性和迁移性，弥补了现有方法的不足。"}}
{"id": "2508.10771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10771", "abs": "https://arxiv.org/abs/2508.10771", "authors": ["Jieyu Li", "Xin Zhang", "Joey Tianyi Zhou"], "title": "AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences", "comment": "Proceedings of the 33rd ACM International Conference on Multimedia", "summary": "Recent advances in AI-generated content have fueled the rise of highly\nrealistic synthetic videos, posing severe risks to societal trust and digital\nintegrity. Existing benchmarks for video authenticity detection typically\nsuffer from limited realism, insufficient scale, and inadequate complexity,\nfailing to effectively evaluate modern vision-language models against\nsophisticated forgeries. To address this critical gap, we introduce AEGIS, a\nnovel large-scale benchmark explicitly targeting the detection of\nhyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises\nover 10,000 rigorously curated real and synthetic videos generated by diverse,\nstate-of-the-art generative models, including Stable Video Diffusion,\nCogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary\narchitectures. In particular, AEGIS features specially constructed challenging\nsubsets enhanced with robustness evaluation. Furthermore, we provide multimodal\nannotations spanning Semantic-Authenticity Descriptions, Motion Features, and\nLow-level Visual Features, facilitating authenticity detection and supporting\ndownstream tasks such as multimodal fusion and forgery localization. Extensive\nexperiments using advanced vision-language models demonstrate limited detection\ncapabilities on the most challenging subsets of AEGIS, highlighting the\ndataset's unique complexity and realism beyond the current generalization\ncapabilities of existing models. In essence, AEGIS establishes an indispensable\nevaluation benchmark, fundamentally advancing research toward developing\ngenuinely robust, reliable, broadly generalizable video authenticity detection\nmethodologies capable of addressing real-world forgery threats. Our dataset is\navailable on https://huggingface.co/datasets/Clarifiedfish/AEGIS.", "AI": {"tldr": "AEGIS是一个新型大规模基准数据集，旨在检测超真实和语义细微的AI生成视频，以应对现有基准的不足。", "motivation": "AI生成内容（特别是视频）的快速发展导致了高度逼真的合成视频，对社会信任和数字完整性构成严重风险。现有视频真实性检测基准存在真实性有限、规模不足和复杂性不够的问题，无法有效评估现代视觉-语言模型对抗复杂伪造的能力。", "method": "引入AEGIS数据集，包含超过10,000个经过严格筛选的真实和合成视频，这些视频由包括Stable Video Diffusion、CogVideoX-5B、KLing和Sora在内的多种最先进生成模型创建。AEGIS还包含特别构建的、具有鲁棒性评估的挑战性子集，并提供多模态标注，如语义真实性描述、运动特征和低级视觉特征。", "result": "使用先进的视觉-语言模型进行的大量实验表明，现有模型在AEGIS最具挑战性的子集上检测能力有限，这凸显了该数据集独特的复杂性和真实性超出了当前模型的泛化能力。", "conclusion": "AEGIS建立了一个不可或缺的评估基准，从根本上推动了开发真正鲁棒、可靠、广泛泛化的视频真实性检测方法的研究，以应对现实世界的伪造威胁。"}}
{"id": "2508.10795", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10795", "abs": "https://arxiv.org/abs/2508.10795", "authors": ["Osama Mohammed Afzal", "Preslav Nakov", "Tom Hope", "Iryna Gurevych"], "title": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with LLM-Assisted Feedback", "comment": null, "summary": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available.", "AI": {"tldr": "该论文提出了一种结构化的自动化新颖性评估方法，通过模拟专家评审行为来辅助同行评审，并在ICLR 2025提交中表现出高精度。", "motivation": "新颖性评估是同行评审的核心环节，但在NLP等高容量领域中，由于审稿人能力日益紧张，这一方面研究不足。因此，需要一种自动化、一致的方法来支持新颖性评估。", "method": "该方法采用三阶段结构化方法模拟专家审稿人行为：1. 从提交中提取内容；2. 检索并综合相关工作；3. 进行结构化比较以进行基于证据的评估。该方法基于对人类撰写的新颖性评审的大规模分析，捕捉了独立主张验证和上下文推理等关键模式。", "result": "在182份ICLR 2025提交（带有人工标注的新颖性评估）上进行评估，该方法与人类推理的对齐度达到86.5%，新颖性结论的一致性达到75.3%，显著优于现有基于LLM的基线。它能生成详细的、文献感知的分析，并提高审稿人判断的一致性。", "conclusion": "结构化的LLM辅助方法有潜力支持更严谨和透明的同行评审，而无需取代人类专业知识。"}}
{"id": "2508.10631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10631", "abs": "https://arxiv.org/abs/2508.10631", "authors": ["Nicola Dall'Asen", "Xiaofeng Zhang", "Reyhane Askari Hemmat", "Melissa Hall", "Jakob Verbeek", "Adriana Romero-Soriano", "Michal Drozdzal"], "title": "Increasing the Utility of Synthetic Images through Chamfer Guidance", "comment": null, "summary": "Conditional image generative models hold considerable promise to produce\ninfinite amounts of synthetic training data. Yet, recent progress in generation\nquality has come at the expense of generation diversity, limiting the utility\nof these models as a source of synthetic training data. Although guidance-based\napproaches have been introduced to improve the utility of generated data by\nfocusing on quality or diversity, the (implicit or explicit) utility functions\noftentimes disregard the potential distribution shift between synthetic and\nreal data. In this work, we introduce Chamfer Guidance: a training-free\nguidance approach which leverages a handful of real exemplar images to\ncharacterize the quality and diversity of synthetic data. We show that by\nleveraging the proposed Chamfer Guidance, we can boost the diversity of the\ngenerations w.r.t. a dataset of real images while maintaining or improving the\ngeneration quality on ImageNet-1k and standard geo-diversity benchmarks. Our\napproach achieves state-of-the-art few-shot performance with as little as 2\nexemplar real images, obtaining 96.4\\% in terms of precision, and 86.4\\% in\nterms of distributional coverage, which increase to 97.5\\% and 92.7\\%,\nrespectively, when using 32 real images. We showcase the benefits of the\nChamfer Guidance generation by training downstream image classifiers on\nsynthetic data, achieving accuracy boost of up to 15\\% for in-distribution over\nthe baselines, and up to 16\\% in out-of-distribution. Furthermore, our approach\ndoes not require using the unconditional model, and thus obtains a 31\\%\nreduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling\ntime.", "AI": {"tldr": "该论文提出了一种名为Chamfer Guidance的无训练指导方法，利用少量真实图像作为参考，显著提升了条件图像生成模型的生成多样性和质量，减少了合成数据与真实数据之间的分布差异，并提高了下游任务的性能。", "motivation": "条件图像生成模型虽然在生成质量上有所进步，但多样性不足，限制了其作为合成训练数据的实用性。现有的指导方法往往忽视了合成数据与真实数据之间潜在的分布差异。", "method": "引入了Chamfer Guidance，这是一种无需训练的指导方法。它利用少量真实样本图像来评估和指导合成数据的质量和多样性，从而弥补生成数据与真实数据之间的分布差异。", "result": "通过Chamfer Guidance，模型在ImageNet-1k和标准地理多样性基准测试中提升了生成多样性，同时保持或提高了生成质量。在少量样本（2个真实图像）设置下，实现了最先进的性能（精度96.4%，覆盖率86.4%）。使用合成数据训练的下游图像分类器，在分布内和分布外数据上的准确率分别提高了15%和16%。此外，该方法无需使用无条件模型，采样时FLOPs比基于无分类器指导的方法减少了31%。", "conclusion": "Chamfer Guidance有效解决了条件图像生成模型在多样性方面的不足，并通过少量真实图像的引导，显著提升了合成数据的实用性，使其更适合用于训练下游任务，同时实现了计算效率的提升。"}}
{"id": "2508.10774", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10774", "abs": "https://arxiv.org/abs/2508.10774", "authors": ["Youping Gu", "Xiaolong Li", "Yuhao Hu", "Bohan Zhuang"], "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation", "comment": "Tech report", "summary": "Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/.", "AI": {"tldr": "BLADE是一个数据无关的联合训练框架，通过自适应块稀疏注意力机制和稀疏感知步长蒸馏，显著加速了扩散Transformer的视频生成推理过程，并提升了生成质量。", "motivation": "扩散Transformer在高质量视频生成方面表现出色，但其迭代去噪过程缓慢以及长序列的二次注意力成本导致推理瓶颈。单独的步长蒸馏和稀疏注意力机制虽有潜力，但有效结合它们存在挑战，要么效果不佳，要么需要昂贵的高质量视频数据进行训练。", "method": "本文提出了BLADE框架，包含两项创新：1) 自适应块稀疏注意力（ASA）机制，动态生成内容感知稀疏掩码，将计算集中在显著的时空特征上；2) 基于轨迹分布匹配（TDM）的稀疏感知步长蒸馏范式，将稀疏性直接融入蒸馏过程，而非作为独立的压缩步骤，实现快速收敛。该框架是数据无关的联合训练。", "result": "BLADE在CogVideoX-5B和Wan2.1-1.3B等文本到视频模型上进行了验证。在Wan2.1-1.3B上，实现了相对于50步基线14.10倍的端到端推理加速；在CogVideoX-5B等短视频序列模型上，实现了8.89倍的加速。同时，加速伴随着质量提升：在VBench-2.0基准上，CogVideoX-5B得分从0.534提高到0.569，Wan2.1-1.3B从0.563提高到0.570，并得到人类评估的更高评价。", "conclusion": "BLADE通过创新性的数据无关联合训练框架，成功结合了稀疏注意力和步长蒸馏，有效解决了扩散Transformer在视频生成中的推理瓶颈，实现了显著的加速和一致的质量提升。"}}
{"id": "2508.10839", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.10839", "abs": "https://arxiv.org/abs/2508.10839", "authors": ["Jim Dilkes", "Vahid Yazdanpanah", "Sebastian Stein"], "title": "Reinforced Language Models for Sequential Decision Making", "comment": null, "summary": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.", "AI": {"tldr": "本文提出了一种名为MS-GRPO的新算法，用于对小型LLM进行后训练，使其能够有效地执行多步序贯决策任务，从而避免对大型昂贵模型的依赖。", "motivation": "尽管大型语言模型（LLMs）在序贯决策方面展现出潜力，但其高昂的计算成本限制了应用。现有的后训练方法主要针对单轮交互，无法有效处理多步智能体任务中的信用分配问题，因此需要改进小型模型的后训练方法。", "method": "引入了多步组相对策略优化（MS-GRPO）算法，该算法基于文本介导随机博弈（TSMG）和语言-智能体策略（LAP）框架。为解决信用分配问题，MS-GRPO将整个累积回合奖励归因于每个单独的回合步骤。此外，还补充了一种新颖的绝对优势加权回合采样策略。通过在Snake和Frozen Lake任务上对一个30亿参数的模型进行后训练来评估该方法。", "result": "实验证明，MS-GRPO能有效提升决策性能：经后训练的30亿参数模型在Frozen Lake任务上比720亿参数的基线模型性能高出50%。", "conclusion": "这项工作表明，针对性的后训练是创建基于LLM的序贯决策智能体的一种实用且高效的替代方案，而非仅仅依赖于模型规模。"}}
{"id": "2508.10635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10635", "abs": "https://arxiv.org/abs/2508.10635", "authors": ["Hosam Elgendy", "Ahmed Sharshar", "Ahmed Aboeitta", "Mohsen Guizani"], "title": "ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation", "comment": "11 pages, 5 figures, 7 tables", "summary": "Understanding environmental changes from aerial imagery is vital for climate\nresilience, urban planning, and ecosystem monitoring. Yet, current vision\nlanguage models (VLMs) overlook causal signals from environmental sensors, rely\non single-source captions prone to stylistic bias, and lack interactive\nscenario-based reasoning. We present ChatENV, the first interactive VLM that\njointly reasons over satellite image pairs and real-world sensor data. Our\nframework: (i) creates a 177k-image dataset forming 152k temporal pairs across\n62 land-use classes in 197 countries with rich sensor metadata (e.g.,\ntemperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for\nstylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using\nefficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV\nachieves strong performance in temporal and \"what-if\" reasoning (e.g., BERT-F1\n0.903) and rivals or outperforms state-of-the-art temporal models, while\nsupporting interactive scenario-based analysis. This positions ChatENV as a\npowerful tool for grounded, sensor-aware environmental monitoring.", "AI": {"tldr": "ChatENV是首个交互式视觉语言模型（VLM），能结合卫星图像对和环境传感器数据进行联合推理，支持时间序列和“假设”场景分析，并优于现有模型。", "motivation": "现有视觉语言模型在理解环境变化方面存在局限性：它们忽视环境传感器中的因果信号，依赖单一来源的、可能存在风格偏差的图像描述，并且缺乏交互式场景推理能力。", "method": "该研究方法包括：(i) 创建了一个包含17.7万张图像、形成15.2万个时间对的数据集，覆盖全球197个国家的62种土地利用类别，并富含传感器元数据（如温度、PM10、CO）；(ii) 使用GPT-4o和Gemini 2.0对数据进行标注，以确保风格和语义多样性；(iii) 使用低秩适应（LoRA）适配器对Qwen-2.5-VL模型进行微调，以支持聊天交互。", "result": "ChatENV在时间序列和“假设”推理任务中表现出色（例如，BERT-F1达到0.903），与最先进的时间序列模型相比，性能相当甚至更优，同时支持交互式场景分析。", "conclusion": "ChatENV是一个强大的工具，可用于基于传感器数据的、接地气的环境监测，填补了现有VLM在处理环境变化方面的空白。"}}
{"id": "2508.10779", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10779", "abs": "https://arxiv.org/abs/2508.10779", "authors": ["Zhenning Shi", "Zizheng Yan", "Yuhang Yu", "Clara Xue", "Jingyu Zhuang", "Qi Zhang", "Jinwei Chen", "Tao Li", "Qingnan Fan"], "title": "Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior", "comment": null, "summary": "Reference-based Image Super-Resolution (RefSR) aims to restore a\nlow-resolution (LR) image by utilizing the semantic and texture information\nfrom an additional reference high-resolution (reference HR) image. Existing\ndiffusion-based RefSR methods are typically built upon ControlNet, which\nstruggles to effectively align the information between the LR image and the\nreference HR image. Moreover, current RefSR datasets suffer from limited\nresolution and poor image quality, resulting in the reference images lacking\nsufficient fine-grained details to support high-quality restoration. To\novercome the limitations above, we propose TriFlowSR, a novel framework that\nexplicitly achieves pattern matching between the LR image and the reference HR\nimage. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for\nUltra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios\nwith real-world degradation, in TriFlowSR, we design a Reference Matching\nStrategy to effectively match the LR image with the reference HR image.\nExperimental results show that our approach can better utilize the semantic and\ntexture information of the reference HR image compared to previous methods. To\nthe best of our knowledge, we propose the first diffusion-based RefSR pipeline\nfor ultra-high definition landmark scenarios under real-world degradation. Our\ncode and model will be available at https://github.com/nkicsl/TriFlowSR.", "AI": {"tldr": "本文提出了TriFlowSR框架和Landmark-4K数据集，旨在解决现有参考图像超分（RefSR）方法在信息对齐和数据集质量方面的局限性，特别针对超高清（UHD）地标场景下的真实世界退化。", "motivation": "现有的基于扩散模型的RefSR方法（通常基于ControlNet）难以有效对齐低分辨率（LR）图像和参考高分辨率（HR）图像的信息。此外，当前的RefSR数据集分辨率有限且图像质量不佳，导致参考图像缺乏足够的精细细节来支持高质量的图像恢复。", "method": "本文提出TriFlowSR框架，该框架显式地实现了LR图像与参考HR图像之间的模式匹配。针对超高清（UHD）场景和真实世界退化，TriFlowSR设计了一种参考匹配策略。同时，本文还引入了Landmark-4K数据集，这是首个针对UHD地标场景的RefSR数据集。", "result": "实验结果表明，与现有方法相比，TriFlowSR能更好地利用参考HR图像的语义和纹理信息。", "conclusion": "本文首次提出了一种针对真实世界退化下超高清地标场景的基于扩散模型的RefSR方案，并为此类场景提供了专门的数据集。"}}
{"id": "2508.10848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10848", "abs": "https://arxiv.org/abs/2508.10848", "authors": ["Chongyuan Dai", "Jinpeng Hu", "Hongchang Shi", "Zhuo Li", "Xun Yang", "Meng Wang"], "title": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning", "comment": null, "summary": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1.", "AI": {"tldr": "本文提出了Psyche-R1，首个结合了同理心、心理专业知识和推理能力的中文心理大语言模型，通过新颖的数据构建和混合训练策略，在心理学任务上表现出色。", "motivation": "合格心理健康专业人员短缺，而现有心理学领域的LLM研究主要侧重情感支持和同理心对话，缺乏对推理机制的关注，导致响应可靠性不足。", "method": "提出Psyche-R1模型，并设计了一个全面的数据合成流程，生成了超过7.5万个包含详细推理链（CoT）的心理问题和7.3万个同理心对话。采用混合训练策略：通过多LLM交叉选择识别挑战性样本用于组相对策略优化（GRPO）以提升推理能力，其余数据用于监督微调（SFT）以增强同理心响应和心理领域知识。", "result": "Psyche-R1在多个心理学基准测试中表现出有效性，其中7B的Psyche-R1达到了与671B DeepSeek-R1相当的性能。", "conclusion": "Psyche-R1成功地将同理心、心理专业知识和推理能力整合到中文心理LLM中，为缓解心理健康负担提供了有前景的解决方案。"}}
{"id": "2508.10637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10637", "abs": "https://arxiv.org/abs/2508.10637", "authors": ["Ryan Ramos", "Vladan Stojnić", "Giorgos Kordopatis-Zilos", "Yuta Nakashima", "Giorgos Tolias", "Noa Garcia"], "title": "Processing and acquisition traces in visual encoders: What does CLIP know about your camera?", "comment": "8 main pages, supplementary attached, ICCV 2025 highlight", "summary": "Prior work has analyzed the robustness of visual encoders to image\ntransformations and corruptions, particularly in cases where such alterations\nare not seen during training. When this occurs, they introduce a form of\ndistribution shift at test time, often leading to performance degradation. The\nprimary focus has been on severe corruptions that, when applied aggressively,\ndistort useful signals necessary for accurate semantic predictions.\n  We take a different perspective by analyzing parameters of the image\nacquisition process and transformations that may be subtle or even\nimperceptible to the human eye. We find that such parameters are systematically\nencoded in the learned visual representations and can be easily recovered. More\nstrikingly, their presence can have a profound impact, either positively or\nnegatively, on semantic predictions. This effect depends on whether there is a\nstrong correlation or anti-correlation between semantic labels and these\nacquisition-based or processing-based labels. Our code and data are available\nat: https://github.com/ryan-caesar-ramos/visual-encoder-traces", "AI": {"tldr": "该研究发现，视觉编码器会系统地编码图像采集和处理过程中人眼难以察觉的细微参数，这些参数可以被轻易恢复，并根据其与语义标签的相关性，对语义预测产生显著的正面或负面影响。", "motivation": "以往研究主要关注图像严重损坏对视觉编码器鲁棒性的影响，这些损坏会扭曲有用信号并导致性能下降。本研究旨在从一个不同角度出发，分析图像采集和处理过程中可能非常细微甚至人眼无法察觉的参数，探讨它们如何影响视觉表示和语义预测，因为这些细微变化同样可能引入分布偏移。", "method": "通过分析图像采集过程和变换的参数，研究其如何被编码进学习到的视觉表示中，以及这些编码信息是否易于恢复。进一步，评估这些参数的存在对语义预测的影响，并探究这种影响是否与语义标签和这些采集/处理相关标签之间的强相关或反相关性有关。", "result": "研究发现：1) 图像采集和处理过程中细微或不可察觉的参数被系统地编码在学习到的视觉表示中；2) 这些参数可以被轻易地从表示中恢复；3) 它们的存在对语义预测具有深远的影响（正面或负面）；4) 这种影响取决于语义标签与这些采集或处理相关标签之间是否存在强烈的正相关或负相关。", "conclusion": "视觉编码器不仅对图像的语义信息敏感，还会系统地编码图像采集和处理过程中那些细微、不易察觉的非语义参数。这些参数的存在可以显著影响模型在语义任务上的表现，其影响方向和程度取决于它们与语义标签之间的相关性，揭示了视觉编码器鲁棒性分析的新维度。"}}
{"id": "2508.10860", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10860", "abs": "https://arxiv.org/abs/2508.10860", "authors": ["Zhaokun Jiang", "Ziyin Zhang"], "title": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms", "comment": null, "summary": "Recent advancements in machine learning have spurred growing interests in\nautomated interpreting quality assessment. Nevertheless, existing research\nsuffers from insufficient examination of language use quality, unsatisfactory\nmodeling effectiveness due to data scarcity and imbalance, and a lack of\nefforts to explain model predictions. To address these gaps, we propose a\nmulti-dimensional modeling framework that integrates feature engineering, data\naugmentation, and explainable machine learning. This approach prioritizes\nexplainability over ``black box'' predictions by utilizing only\nconstruct-relevant, transparent features and conducting Shapley Value (SHAP)\nanalysis. Our results demonstrate strong predictive performance on a novel\nEnglish-Chinese consecutive interpreting dataset, identifying BLEURT and\nCometKiwi scores to be the strongest predictive features for fidelity,\npause-related features for fluency, and Chinese-specific phraseological\ndiversity metrics for language use. Overall, by placing particular emphasis on\nexplainability, we present a scalable, reliable, and transparent alternative to\ntraditional human evaluation, facilitating the provision of detailed diagnostic\nfeedback for learners and supporting self-regulated learning advantages not\nafforded by automated scores in isolation.", "AI": {"tldr": "本文提出了一种多维度、可解释的机器学习框架，用于自动化口译质量评估，解决了现有方法在语言使用质量评估、数据稀缺和模型可解释性方面的不足。", "motivation": "现有口译质量评估研究存在对语言使用质量检查不足、数据稀缺和不平衡导致模型效果不佳，以及缺乏模型预测解释性等问题。", "method": "本文提出了一个多维度建模框架，整合了特征工程、数据增强和可解释机器学习。该方法通过使用与结构相关、透明的特征，并进行Shapley值（SHAP）分析，优先考虑可解释性，而非“黑箱”预测。研究在一个新的英汉同声传译数据集上进行。", "result": "该框架在新的英汉同声传译数据集上表现出强大的预测性能。研究发现，BLEURT和CometKiwi分数是忠实度最强的预测特征，停顿相关特征对流利度预测最有效，而中文特有的短语多样性指标对语言使用质量预测最为关键。", "conclusion": "该研究提供了一种可扩展、可靠且透明的替代传统人工评估的方法，能够为学习者提供详细的诊断反馈，并支持自动化分数无法单独提供的自主学习优势。"}}
{"id": "2508.10874", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10874", "abs": "https://arxiv.org/abs/2508.10874", "authors": ["Yuchen Fan", "Kaiyan Zhang", "Heng Zhou", "Yuxin Zuo", "Yanxu Chen", "Yu Fu", "Xinwei Long", "Xuekai Zhu", "Che Jiang", "Yuchen Zhang", "Li Kang", "Gang Chen", "Cheng Huang", "Zhizhou He", "Bingning Wang", "Lei Bai", "Ning Ding", "Bowen Zhou"], "title": "SSRL: Self-Search Reinforcement Learning", "comment": null, "summary": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training.", "AI": {"tldr": "该研究探索了大型语言模型（LLMs）作为强化学习（RL）中代理搜索任务的高效模拟器的潜力，通过“自搜索”量化其内部知识，并引入“自搜索RL（SSRL）”以增强其内部知识利用，从而减少对外部搜索引擎的依赖。", "motivation": "减少强化学习中代理搜索任务对昂贵的外部搜索引擎交互的依赖，寻求利用LLMs的内部知识来替代或补充外部搜索。", "method": "1. 通过结构化提示和重复采样量化LLMs的内在搜索能力，称之为“自搜索”。2. 引入“自搜索RL（SSRL）”，通过基于格式和规则的奖励来增强LLMs的自搜索能力，使其能够在内部迭代地提炼知识利用。", "result": "1. LLMs的自搜索能力随推理预算的增加而表现出强大的扩展行为，在问答基准（包括BrowseComp）上取得了高pass@k分数。2. SSRL使模型无需外部工具即可迭代地完善其内部知识利用。3. 经验评估表明，SSRL训练的策略模型为搜索驱动的RL训练提供了经济高效且稳定的环境，减少了对外部搜索引擎的依赖，并促进了稳健的模拟到现实迁移。", "conclusion": "1. LLMs拥有可以有效激发以实现高性能的世界知识。2. SSRL展示了利用内部知识减少幻觉的潜力。3. SSRL训练的模型可以无缝集成外部搜索引擎。这些发现强调了LLMs支持更可扩展的RL智能体训练的潜力。"}}
{"id": "2508.10643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10643", "abs": "https://arxiv.org/abs/2508.10643", "authors": ["Helena Russello", "Rik van der Tol", "Eldert J. van Henten", "Gert Kootstra"], "title": "Lameness detection in dairy cows using pose estimation and bidirectional LSTMs", "comment": null, "summary": "This study presents a lameness detection approach that combines pose\nestimation and Bidirectional Long-Short-Term Memory (BLSTM) neural networks.\nCombining pose-estimation and BLSTMs classifier offers the following\nadvantages: markerless pose-estimation, elimination of manual feature\nengineering by learning temporal motion features from the keypoint\ntrajectories, and working with short sequences and small training datasets.\nMotion sequences of nine keypoints (located on the cows' hooves, head and back)\nwere extracted from videos of walking cows with the T-LEAP pose estimation\nmodel. The trajectories of the keypoints were then used as an input to a BLSTM\nclassifier that was trained to perform binary lameness classification. Our\nmethod significantly outperformed an established method that relied on\nmanually-designed locomotion features: our best architecture achieved a\nclassification accuracy of 85%, against 80% accuracy for the feature-based\napproach. Furthermore, we showed that our BLSTM classifier could detect\nlameness with as little as one second of video data.", "AI": {"tldr": "本研究提出一种结合姿态估计和双向长短期记忆（BLSTM）神经网络的奶牛跛足检测方法，实现了高精度和短视频序列的有效检测。", "motivation": "现有跛足检测方法可能依赖于标记、手动特征工程或需要大量数据。本研究旨在开发一种无标记、自动化、能从关键点轨迹中学习时序特征，并适用于短序列和小训练数据集的检测方案。", "method": "使用T-LEAP姿态估计模型从奶牛行走视频中提取九个关键点（蹄、头部、背部）的运动序列。这些关键点轨迹作为BLSTM分类器的输入，用于进行二元跛足分类训练。", "result": "该方法实现了85%的分类准确率，显著优于依赖手动设计运动特征的传统方法（80%）。此外，该BLSTM分类器仅需1秒的视频数据即可检测跛足。", "conclusion": "结合姿态估计和BLSTM的奶牛跛足检测方法是有效的，其性能优于传统基于特征的方法，并且能够处理短视频序列，显示出在实际应用中的潜力。"}}
{"id": "2508.10865", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10865", "abs": "https://arxiv.org/abs/2508.10865", "authors": ["Mojtaba Safari", "Shansong Wang", "Mingzhe Hu", "Zach Eidex", "Qiang Li", "Xiaofeng Yang"], "title": "Performance of GPT-5 in Brain Tumor MRI Reasoning", "comment": null, "summary": "Accurate differentiation of brain tumor types on magnetic resonance imaging\n(MRI) is critical for guiding treatment planning in neuro-oncology. Recent\nadvances in large language models (LLMs) have enabled visual question answering\n(VQA) approaches that integrate image interpretation with natural language\nreasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and\nGPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor\nSegmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain\nmetastases (MET). Each case included multi-sequence MRI triplanar mosaics and\nstructured clinical features transformed into standardized VQA items. Models\nwere assessed in a zero-shot chain-of-thought setting for accuracy on both\nvisual and reasoning tasks. Results showed that GPT-5-mini achieved the highest\nmacro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),\nand GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single\nmodel dominating across all cohorts. These findings suggest that GPT-5 family\nmodels can achieve moderate accuracy in structured neuro-oncological VQA tasks,\nbut not at a level acceptable for clinical use.", "AI": {"tldr": "本研究评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5模型在脑肿瘤视觉问答（VQA）任务上的表现，发现GPT-5系列模型能达到中等准确率，但尚未达到临床应用水平。", "motivation": "准确区分脑肿瘤类型对神经肿瘤治疗规划至关重要。大型语言模型（LLMs）在视觉问答（VQA）方面的最新进展，结合了图像解释和自然语言推理，为该领域提供了新的可能性。", "method": "研究构建了一个脑肿瘤VQA基准，该基准来源于三种脑肿瘤分割（BraTS）数据集（胶质母细胞瘤、脑膜瘤和脑转移瘤）。每个病例包含多序列MRI三平面马赛克图像和结构化临床特征，并将其转换为标准化的VQA项目。研究在零样本思维链设置下评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5模型在视觉和推理任务上的准确性。", "result": "结果显示，GPT-5-mini的宏平均准确率最高（44.19%），其次是GPT-5（43.71%）、GPT-4o（41.49%）和GPT-5-nano（35.85%）。性能因肿瘤亚型而异，没有单一模型在所有队列中都占据主导地位。", "conclusion": "研究结果表明，GPT-5系列模型在结构化神经肿瘤VQA任务中可以达到中等准确率，但尚未达到临床可接受的水平。"}}
{"id": "2508.10875", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10875", "abs": "https://arxiv.org/abs/2508.10875", "authors": ["Tianyi Li", "Mingda Chen", "Bowei Guo", "Zhiqiang Shen"], "title": "A Survey on Diffusion Language Models", "comment": null, "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.", "AI": {"tldr": "该综述全面概述了扩散语言模型（DLMs），将其作为自回归模型（AR）的有力替代，重点介绍了其并行生成、低延迟、双向上下文捕获等优势，并探讨了其演变、原理、技术、推理策略、多模态扩展、应用、局限性及未来方向。", "motivation": "自回归语言模型在推理延迟和双向上下文捕获方面存在局限性。扩散语言模型（DLMs）作为一种新兴范式，通过迭代去噪过程并行生成token，具有降低推理延迟、捕获双向上下文和实现精细控制的固有优势，且性能已能与自回归模型媲美，因此有必要对其进行全面梳理和分析。", "method": "本文是一篇综述性论文，方法包括：追溯DLMs的演变及其与自回归和掩码语言模型等范式的关系；涵盖基础原理和最先进模型；提供全面分类法并深入分析预训练到后训练技术；彻底审查推理策略和优化（包括解码并行性、缓存机制和生成质量）；突出DLMs的多模态扩展及其应用；讨论DLMs的局限性、挑战并提出未来研究方向。", "result": "该综述提供了DLM领域的整体概览，包括：其演变和与其他范式的关系；基础原理和最先进模型；更新、全面的分类法；对当前技术（从预训练到后训练）的深入分析；对DLM推理策略和优化的全面审查；DLM多模态扩展及其在各种实际场景中的应用；DLM的局限性、挑战（效率、长序列处理、基础设施需求）以及未来研究方向。研究表明DLMs已能实现数倍的加速，并达到与自回归模型相当的性能。", "conclusion": "扩散语言模型（DLMs）是自回归范式的一个强大且有前景的替代方案，具有并行生成、低推理延迟和捕获双向上下文的固有优势。尽管仍面临效率和长序列处理等挑战，但该领域发展迅速，未来研究方向广阔。本综述为理解DLMs的当前格局和未来发展提供了全面的视角。"}}
{"id": "2508.10645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10645", "abs": "https://arxiv.org/abs/2508.10645", "authors": ["Xiao Shi", "Yangjun Ou", "Zhenzhong Chen"], "title": "SemPT: Semantic Prompt Tuning for Vision-Language Models", "comment": null, "summary": "Visual transfer learning for unseen categories presents an active research\ntopic yet a challenging task, due to the inherent conflict between preserving\ncategory-specific representations and acquiring transferable knowledge.\nVision-Language Models (VLMs) pre-trained on large amounts of image-text pairs\noffer a promising solution. However, existing prompt tuning methods rely on\nsparse category labels or disparate LLM-generated descriptions, which fragment\nknowledge representation and hinder transferability. To address this\nlimitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that\ntackles the generalization challenge by leveraging shared attribute-level\nknowledge across categories. Specifically, SemPT adopts a two-step prompting\nstrategy to guide LLM in extracting shared visual attributes and generating\nattribute-level descriptions, capturing transferable semantic cues beyond\nlabels while ensuring coherent structure. Then, visually guided weighting is\napplied to the embeddings of attribute-level descriptions to reduce noise from\nirrelevant attributes and enhance the text embeddings. Additionally, image\nembeddings are jointly aligned with both label and attribute-enhanced text\nembeddings, balancing discrimination for seen categories and transferability to\nunseen ones. Considering the availability of category exposure, our inference\ndynamically selects between standard label embeddings for seen categories and\nattribute-enhanced embeddings for unseen ones to ensure effective adaptation.\nExtensive experiments on 15 benchmark datasets demonstrate that SemPT achieves\nstate-of-the-art performance across various settings, including base-to-novel\ngeneralization, cross-dataset transfer, cross-domain transfer, and few-shot\nlearning.", "AI": {"tldr": "SemPT是一种新颖的框架，通过利用共享的属性级知识，提高了视觉迁移学习中对未见类别的泛化能力，解决了现有VLM提示调整方法的局限性。", "motivation": "视觉迁移学习在未见类别上极具挑战性，因为保留类别特异性表示与获取可迁移知识之间存在内在冲突。现有的VLM提示调整方法依赖稀疏的类别标签或分散的LLM生成描述，导致知识表示碎片化并阻碍了可迁移性。", "method": "SemPT采用两步提示策略，引导LLM提取共享视觉属性并生成属性级描述。然后，应用视觉引导加权来减少无关属性的噪声并增强文本嵌入。此外，图像嵌入与标签和属性增强的文本嵌入联合对齐，以平衡对已知类别的判别力和对未知类别的可迁移性。推理时，根据类别暴露情况动态选择使用标准标签嵌入（已知类别）或属性增强嵌入（未知类别）。", "result": "SemPT在15个基准数据集上进行了广泛实验，在基准到新颖泛化、跨数据集迁移、跨领域迁移和少样本学习等多种设置下，均实现了最先进的性能。", "conclusion": "SemPT通过利用跨类别的共享属性级知识，有效解决了视觉迁移学习中的泛化挑战，显著提高了对未见类别的识别和迁移能力。"}}
{"id": "2508.10869", "categories": ["cs.CV", "cs.AI", "68T45, 92C55", "I.2.10; I.4.9"], "pdf": "https://arxiv.org/pdf/2508.10869", "abs": "https://arxiv.org/abs/2508.10869", "authors": ["Sushant Gautam", "Vajira Thambawita", "Michael Riegler", "Pål Halvorsen", "Steven Hicks"], "title": "Medico 2025: Visual Question Answering for Gastrointestinal Imaging", "comment": null, "summary": "The Medico 2025 challenge addresses Visual Question Answering (VQA) for\nGastrointestinal (GI) imaging, organized as part of the MediaEval task series.\nThe challenge focuses on developing Explainable Artificial Intelligence (XAI)\nmodels that answer clinically relevant questions based on GI endoscopy images\nwhile providing interpretable justifications aligned with medical reasoning. It\nintroduces two subtasks: (1) answering diverse types of visual questions using\nthe Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to\nsupport clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500\nimages and 159,549 complex question-answer (QA) pairs, serves as the benchmark\nfor the challenge. By combining quantitative performance metrics and\nexpert-reviewed explainability assessments, this task aims to advance\ntrustworthy Artificial Intelligence (AI) in medical image analysis.\nInstructions, data access, and an updated guide for participation are available\nin the official competition repository:\nhttps://github.com/simula/MediaEval-Medico-2025", "AI": {"tldr": "Medico 2025挑战赛专注于胃肠道图像的视觉问答（VQA），旨在开发可解释人工智能（XAI）模型，为临床问题提供答案和医学推理的解释。", "motivation": "推动可信赖的人工智能在医学图像分析中的发展，特别是在胃肠道内窥镜图像VQA和可解释性方面，以支持临床决策。", "method": "挑战包含两个子任务：1) 使用Kvasir-VQA-x1数据集回答多样视觉问题；2) 生成多模态解释。Kvasir-VQA-x1数据集（包含6,500张图像和159,549个QA对）作为基准。评估将结合定量性能指标和专家审查的可解释性评估。", "result": "该摘要主要介绍了Medico 2025挑战的设定、目标和所用数据集，并未报告具体的实验结果，因为它是一个挑战的发布而非研究成果的展示。", "conclusion": "Medico 2025挑战旨在通过结合定量性能和专家评估的可解释性，促进医学图像分析领域可信赖人工智能的进步。"}}
{"id": "2508.10678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10678", "abs": "https://arxiv.org/abs/2508.10678", "authors": ["Zhaoyuan Qi", "Weihua Gao", "Wenlong Niu", "Jie Tang", "Yun Li", "Xiaodong Peng"], "title": "HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network for Moving Infrared Small Target Detection", "comment": null, "summary": "In practical application scenarios, moving infrared small target detection\n(MIRSTD) remains highly challenging due to the target's small size, weak\nintensity, and complex motion pattern. Existing methods typically only model\nlow-order correlations between feature nodes and perform feature extraction and\nenhancement within a single temporal scale. Although hypergraphs have been\nwidely used for high-order correlation learning, they have received limited\nattention in MIRSTD. To explore the potential of hypergraphs and enhance\nmulti-timescale feature representation, we propose HyperTea, which integrates\nglobal and local temporal perspectives to effectively model high-order\nspatiotemporal correlations of features. HyperTea consists of three modules:\nthe global temporal enhancement module (GTEM) realizes global temporal context\nenhancement through semantic aggregation and propagation; the local temporal\nenhancement module (LTEM) is designed to capture local motion patterns between\nadjacent frames and then enhance local temporal context; additionally, we\nfurther develop a temporal alignment module (TAM) to address potential\ncross-scale feature misalignment. To our best knowledge, HyperTea is the first\nwork to integrate convolutional neural networks (CNNs), recurrent neural\nnetworks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD,\nsignificantly improving detection performance. Experiments on DAUB and IRDST\ndemonstrate its state-of-the-art (SOTA) performance. Our source codes are\navailable at https://github.com/Lurenjia-LRJ/HyperTea.", "AI": {"tldr": "针对运动红外小目标检测（MIRSTD）的挑战，本文提出了HyperTea模型，首次将CNN、RNN和超图神经网络（HGNN）结合，通过建模高阶时空相关性和增强多尺度特征，显著提升了检测性能。", "motivation": "运动红外小目标检测（MIRSTD）面临目标尺寸小、强度弱、运动模式复杂等挑战。现有方法通常只建模特征节点间的低阶关联，并在单一时间尺度内进行特征提取和增强，而超图在MIRSTD中的应用有限，未能充分利用其高阶关联学习能力。", "method": "本文提出了HyperTea模型，旨在探索超图潜力并增强多时间尺度特征表示。它通过整合全局和局部时间视角来有效建模特征的高阶时空关联。HyperTea包含三个模块：全局时间增强模块（GTEM）实现全局时间上下文增强；局部时间增强模块（LTEM）捕获相邻帧间的局部运动模式并增强局部时间上下文；时间对齐模块（TAM）解决潜在的跨尺度特征未对齐问题。这是首个将卷积神经网络（CNN）、循环神经网络（RNN）和超图神经网络（HGNN）集成用于MIRSTD的工作。", "result": "实验结果表明，HyperTea显著提升了检测性能，并在DAUB和IRDST数据集上达到了最先进（SOTA）的性能。", "conclusion": "HyperTea通过创新性地结合CNN、RNN和HGNN，并利用超图有效建模高阶时空相关性及增强多时间尺度特征，显著提升了运动红外小目标检测的性能，实现了当前最佳水平。"}}
{"id": "2508.10881", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10881", "abs": "https://arxiv.org/abs/2508.10881", "authors": ["Lingen Li", "Guangzhi Wang", "Zhaoyang Zhang", "Yaowei Li", "Xiaoyu Li", "Qi Dou", "Jinwei Gu", "Tianfan Xue", "Ying Shan"], "title": "ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing", "comment": "Project Page: https://lg-li.github.io/project/tooncomposer", "summary": "Traditional cartoon and anime production involves keyframing, inbetweening,\nand colorization stages, which require intensive manual effort. Despite recent\nadvances in AI, existing methods often handle these stages separately, leading\nto error accumulation and artifacts. For instance, inbetweening approaches\nstruggle with large motions, while colorization methods require dense per-frame\nsketches. To address this, we introduce ToonComposer, a generative model that\nunifies inbetweening and colorization into a single post-keyframing stage.\nToonComposer employs a sparse sketch injection mechanism to provide precise\ncontrol using keyframe sketches. Additionally, it uses a cartoon adaptation\nmethod with the spatial low-rank adapter to tailor a modern video foundation\nmodel to the cartoon domain while keeping its temporal prior intact. Requiring\nas few as a single sketch and a colored reference frame, ToonComposer excels\nwith sparse inputs, while also supporting multiple sketches at any temporal\nlocation for more precise motion control. This dual capability reduces manual\nworkload and improves flexibility, empowering artists in real-world scenarios.\nTo evaluate our model, we further created PKBench, a benchmark featuring\nhuman-drawn sketches that simulate real-world use cases. Our evaluation\ndemonstrates that ToonComposer outperforms existing methods in visual quality,\nmotion consistency, and production efficiency, offering a superior and more\nflexible solution for AI-assisted cartoon production.", "AI": {"tldr": "ToonComposer是一个生成模型，它将卡通制作中的中间帧生成和上色整合到一个后期关键帧处理阶段，通过稀疏草图输入和卡通领域自适应，显著提高了生产效率和质量。", "motivation": "传统卡通和动漫制作中，关键帧绘制、中间帧生成和上色是独立且劳动密集型的阶段，现有AI方法常单独处理这些阶段，导致错误累积和伪影，例如中间帧生成难以处理大动作，上色需要密集的逐帧草图。", "method": "引入ToonComposer，一个统一中间帧生成和上色的生成模型。它采用稀疏草图注入机制以提供精确控制，并使用空间低秩适配器进行卡通领域自适应，将现有视频基础模型应用于卡通领域同时保持其时间一致性。模型支持稀疏输入（最少一个草图和彩色参考帧），也支持在任何时间点使用多个草图进行更精确的运动控制。此外，还创建了PKBench基准来评估模型性能。", "result": "ToonComposer在视觉质量、运动一致性和生产效率方面均优于现有方法，为AI辅助卡通制作提供了更优越、更灵活的解决方案。", "conclusion": "ToonComposer通过统一关键流程并支持稀疏输入，显著减少了手动工作量并提高了灵活性，为艺术家在实际卡通制作场景中提供了强大的AI辅助工具。"}}
{"id": "2508.10680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10680", "abs": "https://arxiv.org/abs/2508.10680", "authors": ["Busra Bulut", "Maik Dannecker", "Thomas Sanchez", "Sara Neves Silva", "Vladyslav Zalevskyi", "Steven Jia", "Jean-Baptiste Ledoux", "Guillaume Auzias", "François Rousseau", "Jana Hutter", "Daniel Rueckert", "Meritxell Bach Cuadra"], "title": "Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping", "comment": null, "summary": "T2 mapping in fetal brain MRI has the potential to improve characterization\nof the developing brain, especially at mid-field (0.55T), where T2 decay is\nslower. However, this is challenging as fetal MRI acquisition relies on\nmultiple motion-corrupted stacks of thick slices, requiring slice-to-volume\nreconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,\nT2 mapping involves repeated acquisitions of these stacks at each echo time\n(TE), leading to long scan times and high sensitivity to motion. We tackle this\nchallenge with a method that jointly reconstructs data across TEs, addressing\nsevere motion. Our approach combines implicit neural representations with a\nphysics-informed regularization that models T2 decay, enabling information\nsharing across TEs while preserving anatomical and quantitative T2 fidelity. We\ndemonstrate state-of-the-art performance on simulated fetal brain and in vivo\nadult datasets with fetal-like motion. We also present the first in vivo fetal\nT2 mapping results at 0.55T. Our study shows potential for reducing the number\nof stacks per TE in T2 mapping by leveraging anatomical redundancy.", "AI": {"tldr": "该研究提出了一种新方法，通过结合隐式神经表示和物理信息正则化，联合重建不同回波时间（TE）的胎儿脑部MRI数据，以实现0.55T磁场下的T2定量映射，有效解决了运动伪影并缩短了扫描时间。", "motivation": "胎儿脑部T2定量映射在0.55T中场MRI中具有重要潜力，但面临挑战：胎儿运动导致图像采集困难，且传统方法需要在每个TE重复采集大量运动伪影的切片组，导致扫描时间长且对运动敏感。", "method": "该方法通过联合重建不同TE的数据来解决严重运动问题。它结合了隐式神经表示和物理信息正则化（建模T2衰减），从而在不同TE之间共享信息，同时保持解剖和定量T2的准确性。", "result": "该方法在模拟胎儿脑部和具有胎儿般运动的体内成人数据集上展现了最先进的性能。研究还首次展示了0.55T磁场下的体内胎儿T2定量映射结果。研究表明，通过利用解剖冗余，该方法有望减少T2映射中每个TE所需的切片组数量。", "conclusion": "该研究提出的方法在胎儿脑部MRI的T2定量映射中表现出巨大潜力，能够有效处理运动问题并减少扫描时间，有望改进对发育中大脑的表征。"}}
{"id": "2508.10681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10681", "abs": "https://arxiv.org/abs/2508.10681", "authors": ["Mengyang Zhao", "Teng Fu", "Haiyang Yu", "Ke Niu", "Bin Li"], "title": "IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning", "comment": null, "summary": "Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in\nautomating industrial quality inspection. Recently, some FS-IAD methods based\non Large Vision-Language Models (LVLMs) have been proposed with some\nachievements through prompt learning or fine-tuning. However, existing LVLMs\nfocus on general tasks but lack basic industrial knowledge and reasoning\ncapabilities related to FS-IAD, making these methods far from specialized human\nquality inspectors. To address these challenges, we propose a unified\nframework, IADGPT, designed to perform FS-IAD in a human-like manner, while\nalso handling associated localization and reasoning tasks, even for diverse and\nnovel industrial products. To this end, we introduce a three-stage progressive\ntraining strategy inspired by humans. Specifically, the first two stages\ngradually guide IADGPT in acquiring fundamental industrial knowledge and\ndiscrepancy awareness. In the third stage, we design an in-context\nlearning-based training paradigm, enabling IADGPT to leverage a few-shot image\nas the exemplars for improved generalization to novel products. In addition, we\ndesign a strategy that enables IADGPT to output image-level and pixel-level\nanomaly scores using the logits output and the attention map, respectively, in\nconjunction with the language output to accomplish anomaly reasoning. To\nsupport our training, we present a new dataset comprising 100K images across\n400 diverse industrial product categories with extensive attribute-level\ntextual annotations. Experiments indicate IADGPT achieves considerable\nperformance gains in anomaly detection and demonstrates competitiveness in\nanomaly localization and reasoning. We will release our dataset in\ncamera-ready.", "AI": {"tldr": "本文提出IADGPT框架，通过三阶段渐进式训练和上下文学习，使大型视觉-语言模型（LVLMs）具备类人工业缺陷检测、定位和推理能力，并在新数据集上表现出色。", "motivation": "现有基于LVLMs的少样本工业异常检测（FS-IAD）方法缺乏工业领域知识和推理能力，远不及专业人工质检员，且LVLMs主要关注通用任务。", "method": "提出统一框架IADGPT，采用三阶段渐进式训练策略：1) 获取基础工业知识；2) 培养异常识别能力；3) 通过上下文学习利用少量样本提升对新产品的泛化能力。设计策略利用logit输出和注意力图生成图像级和像素级异常分数，并结合语言输出实现异常推理。构建包含10万张图片、400个产品类别及详细文本标注的新数据集支持训练。", "result": "IADGPT在异常检测方面取得了显著性能提升，并在异常定位和推理任务中展现出竞争力。", "conclusion": "IADGPT通过模仿人类学习过程，有效解决了LVLMs在FS-IAD中缺乏专业知识和推理能力的问题，实现了在检测、定位和推理任务上的优异表现，为自动化工业质检提供了新范式。"}}
{"id": "2508.10688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10688", "abs": "https://arxiv.org/abs/2508.10688", "authors": ["Sehajdeep SIngh", "A V Subramanyam"], "title": "Novel View Synthesis using DDIM Inversion", "comment": null, "summary": "Synthesizing novel views from a single input image is a challenging task. It\nrequires extrapolating the 3D structure of a scene while inferring details in\noccluded regions, and maintaining geometric consistency across viewpoints. Many\nexisting methods must fine-tune large diffusion backbones using multiple views\nor train a diffusion model from scratch, which is extremely expensive.\nAdditionally, they suffer from blurry reconstruction and poor generalization.\nThis gap presents the opportunity to explore an explicit lightweight view\ntranslation framework that can directly utilize the high-fidelity generative\ncapabilities of a pretrained diffusion model while reconstructing a scene from\na novel view. Given the DDIM-inverted latent of a single input image, we employ\na camera pose-conditioned translation U-Net, TUNet, to predict the inverted\nlatent corresponding to the desired target view. However, the image sampled\nusing the predicted latent may result in a blurry reconstruction. To this end,\nwe propose a novel fusion strategy that exploits the inherent noise correlation\nstructure observed in DDIM inversion. The proposed fusion strategy helps\npreserve the texture and fine-grained details. To synthesize the novel view, we\nuse the fused latent as the initial condition for DDIM sampling, leveraging the\ngenerative prior of the pretrained diffusion model. Extensive experiments on\nMVImgNet demonstrate that our method outperforms existing methods.", "AI": {"tldr": "本文提出了一种轻量级的视角转换框架，通过使用DDIM反演的潜在空间和相机姿态条件U-Net（TUNet），结合一种新颖的融合策略，利用预训练扩散模型的生成能力，从单张图片合成高质量的新颖视角。", "motivation": "从单张图片合成新颖视角是一项挑战性任务，现有方法通常需要昂贵的扩散模型微调或从头训练，且存在重建模糊和泛化能力差的问题。因此，需要探索一种轻量级、能直接利用预训练扩散模型高保真生成能力的显式视角转换框架。", "method": "方法包括三个主要部分：1) 使用DDIM反演将单张输入图像转换为潜在表示；2) 采用一个相机姿态条件翻译U-Net (TUNet) 来预测目标视角的反演潜在表示；3) 提出一种新颖的融合策略，利用DDIM反演中观察到的固有噪声相关结构，以解决潜在表示采样可能导致的模糊重建问题，从而保留纹理和精细细节。最后，将融合后的潜在表示作为DDIM采样的初始条件，利用预训练扩散模型的生成先验来合成新颖视角。", "result": "在MVImgNet上的大量实验表明，该方法优于现有方法。", "conclusion": "所提出的TUNet和融合策略能够有效利用预训练扩散模型的高保真生成能力，从单张图片合成高质量的新颖视角，并克服了现有方法的模糊重建和泛化问题。"}}
{"id": "2508.10704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10704", "abs": "https://arxiv.org/abs/2508.10704", "authors": ["Zhanwen Liu", "Yujing Sun", "Yang Wang", "Nan Yang", "Shengbo Eben Li", "Xiangmo Zhao"], "title": "Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios", "comment": null, "summary": "The dynamic range limitation of conventional RGB cameras reduces global\ncontrast and causes loss of high-frequency details such as textures and edges\nin complex traffic environments (e.g., nighttime driving, tunnels), hindering\ndiscriminative feature extraction and degrading frame-based object detection.\nTo address this, we integrate a bio-inspired event camera with an RGB camera to\nprovide high dynamic range information and propose a motion cue fusion network\n(MCFNet), which achieves optimal spatiotemporal alignment and adaptive\ncross-modal feature fusion under challenging lighting. Specifically, an event\ncorrection module (ECM) temporally aligns asynchronous event streams with image\nframes via optical-flow-based warping, jointly optimized with the detection\nnetwork to learn task-aware event representations. The event dynamic upsampling\nmodule (EDUM) enhances spatial resolution of event frames to match image\nstructures, ensuring precise spatiotemporal alignment. The cross-modal mamba\nfusion module (CMM) uses adaptive feature fusion with a novel interlaced\nscanning mechanism, effectively integrating complementary information for\nrobust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD\ndatasets demonstrate that MCFNet significantly outperforms existing methods in\nvarious poor lighting and fast moving traffic scenarios. Notably, on the\nDSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best\nexisting methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The\ncode is available at https://github.com/Charm11492/MCFNet.", "AI": {"tldr": "该研究提出MCFNet，一个结合事件相机和RGB相机用于恶劣光照下目标检测的网络，通过时间对齐、空间增强和跨模态融合显著提升了检测性能。", "motivation": "传统RGB相机在复杂交通环境（如夜间、隧道）下存在动态范围限制，导致全局对比度下降、高频细节丢失，进而影响特征提取和基于帧的目标检测性能。", "method": "本文提出运动线索融合网络（MCFNet），将仿生事件相机与RGB相机融合以提供高动态范围信息。具体包括：1) 事件校正模块（ECM）通过光流扭曲实现事件流与图像帧的时间对齐，并与检测网络联合优化学习任务感知事件表示；2) 事件动态上采样模块（EDUM）提升事件帧空间分辨率以匹配图像结构；3) 跨模态Mamba融合模块（CMM）采用新颖的交错扫描机制进行自适应特征融合，有效整合互补信息。", "result": "在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，MCFNet在各种恶劣光照和快速移动交通场景下显著优于现有方法。特别是在DSEC-Det数据集上，MCFNet在mAP50和mAP指标上分别超越现有最佳方法7.4%和1.7%。", "conclusion": "MCFNet通过有效整合事件相机和RGB相机数据，实现了在恶劣光照和快速移动交通场景下鲁棒且高性能的目标检测，显著提升了现有方法的性能。"}}
{"id": "2508.10710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10710", "abs": "https://arxiv.org/abs/2508.10710", "authors": ["Joohyeon Lee", "Jin-Seop Lee", "Jee-Hyong Lee"], "title": "CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation", "comment": "Under review", "summary": "Diffusion-based text-to-image generation models have demonstrated strong\nperformance in terms of image quality and diversity. However, they still\nstruggle to generate images that accurately reflect the number of objects\nspecified in the input prompt. Several approaches have been proposed that rely\non either external counting modules for iterative refinement or quantity\nrepresentations derived from learned tokens or latent features. However, they\nstill have limitations in accurately reflecting the specified number of objects\nand overlook an important structural characteristic--The number of object\ninstances in the generated image is largely determined in the early timesteps\nof the denoising process. To correctly reflect the object quantity for image\ngeneration, the highly activated regions in the object cross-attention map at\nthe early timesteps should match the input object quantity, while each region\nshould be clearly separated. To address this issue, we propose\n\\textit{CountCluster}, a method that guides the object cross-attention map to\nbe clustered according to the specified object count in the input, without\nrelying on any external tools or additional training. The proposed method\npartitions the object cross-attention map into $k$ clusters at inference time\nbased on attention scores, defines an ideal distribution in which each cluster\nis spatially well-separated, and optimizes the latent to align with this target\ndistribution. Our method achieves an average improvement of 18.5\\%p in object\ncount accuracy compared to existing methods, and demonstrates superior quantity\ncontrol performance across a variety of prompts. Code will be released at:\nhttps://github.com/JoohyeonL22/CountCluster .", "AI": {"tldr": "扩散模型在生成图像时难以准确控制物体数量。本文提出CountCluster，通过在推理早期引导交叉注意力图聚类，显著提高了物体数量的生成精度。", "motivation": "扩散模型在图像质量和多样性上表现出色，但在生成指定数量物体时表现不佳。现有方法依赖外部模块或学习到的表示，但仍不准确，且忽略了物体数量在去噪过程早期已基本确定的关键结构特性。", "method": "本文提出CountCluster方法，无需外部工具或额外训练。该方法在推理时，根据输入指定的物体数量k，将物体交叉注意力图划分为k个簇。定义一个理想的、空间上清晰分离的簇分布，并优化潜在表示以使其与该目标分布对齐。", "result": "与现有方法相比，物体计数准确率平均提高了18.5个百分点。在各种提示下，都展现出卓越的数量控制性能。", "conclusion": "CountCluster通过在去噪过程早期引导物体交叉注意力图形成与指定数量匹配且空间分离的簇，有效解决了文本到图像扩散模型在生成指定数量物体时的准确性问题。"}}
{"id": "2508.10711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10711", "abs": "https://arxiv.org/abs/2508.10711", "authors": ["NextStep Team", "Chunrui Han", "Guopeng Li", "Jingwei Wu", "Quan Sun", "Yan Cai", "Yuang Peng", "Zheng Ge", "Deyu Zhou", "Haomiao Tang", "Hongyu Zhou", "Kenkun Liu", "Ailin Huang", "Bin Wang", "Changxin Miao", "Deshan Sun", "En Yu", "Fukun Yin", "Gang Yu", "Hao Nie", "Haoran Lv", "Hanpeng Hu", "Jia Wang", "Jian Zhou", "Jianjian Sun", "Kaijun Tan", "Kang An", "Kangheng Lin", "Liang Zhao", "Mei Chen", "Peng Xing", "Rui Wang", "Shiyu Liu", "Shutao Xia", "Tianhao You", "Wei Ji", "Xianfang Zeng", "Xin Han", "Xuelin Zhang", "Yana Wei", "Yanming Xu", "Yimin Jiang", "Yingming Wang", "Yu Zhou", "Yucheng Han", "Ziyang Meng", "Binxing Jiao", "Daxin Jiang", "Xiangyu Zhang", "Yibo Zhu"], "title": "NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale", "comment": "Code: https://github.com/stepfun-ai/NextStep-1", "summary": "Prevailing autoregressive (AR) models for text-to-image generation either\nrely on heavy, computationally-intensive diffusion models to process continuous\nimage tokens, or employ vector quantization (VQ) to obtain discrete tokens with\nquantization loss. In this paper, we push the autoregressive paradigm forward\nwith NextStep-1, a 14B autoregressive model paired with a 157M flow matching\nhead, training on discrete text tokens and continuous image tokens with\nnext-token prediction objectives. NextStep-1 achieves state-of-the-art\nperformance for autoregressive models in text-to-image generation tasks,\nexhibiting strong capabilities in high-fidelity image synthesis. Furthermore,\nour method shows strong performance in image editing, highlighting the power\nand versatility of our unified approach. To facilitate open research, we will\nrelease our code and models to the community.", "AI": {"tldr": "NextStep-1是一个14B的自回归模型，结合157M的流匹配头部，通过离散文本和连续图像token训练，实现了文本到图像生成中自回归模型的最新SOTA性能，并展示了强大的图像编辑能力。", "motivation": "现有文本到图像的自回归模型存在局限性：要么依赖计算密集型的扩散模型处理连续图像token，要么使用矢量量化（VQ）获取离散token但引入量化损失。", "method": "本文提出了NextStep-1，一个14B的自回归模型，搭配157M的流匹配头部。该模型利用下一token预测目标，在离散文本token和连续图像token上进行训练。", "result": "NextStep-1在文本到图像生成任务中取得了自回归模型的最新SOTA性能，展现出高保真图像合成的强大能力。此外，该方法在图像编辑方面也表现出色。", "conclusion": "NextStep-1的统一方法（自回归模型与流匹配结合）强大且多功能，显著提升了自回归模型在文本到图像生成领域的性能，并具备出色的图像编辑能力。为促进开放研究，代码和模型将对外发布。"}}
{"id": "2508.10712", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10712", "abs": "https://arxiv.org/abs/2508.10712", "authors": ["Fabian Kresse", "Georgios Pilikos", "Mario Azcueta", "Nicolas Floury"], "title": "Lightweight CNNs for Embedded SAR Ship Target Detection and Classification", "comment": "Accepted at Big Data from Space 2025 (BiDS'25)", "summary": "Synthetic Aperture Radar (SAR) data enables large-scale surveillance of\nmaritime vessels. However, near-real-time monitoring is currently constrained\nby the need to downlink all raw data, perform image focusing, and subsequently\nanalyze it on the ground. On-board processing to generate higher-level products\ncould reduce the data volume that needs to be downlinked, alleviating bandwidth\nconstraints and minimizing latency. However, traditional image focusing and\nprocessing algorithms face challenges due to the satellite's limited memory,\nprocessing power, and computational resources. This work proposes and evaluates\nneural networks designed for real-time inference on unfocused SAR data acquired\nin Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our\nresults demonstrate the feasibility of using one of our models for on-board\nprocessing and deployment on an FPGA. Additionally, by investigating a binary\nclassification task between ships and windmills, we demonstrate that target\nclassification is possible.", "AI": {"tldr": "该研究提出并评估了用于星载实时处理未聚焦合成孔径雷达（SAR）数据的神经网络，以实现船舶监测和目标分类，旨在解决传统地面处理带来的带宽和延迟问题。", "motivation": "目前的SAR船舶监测依赖于下行所有原始数据并在地面处理，导致高延迟和带宽限制。卫星内存、处理能力和计算资源的限制使得传统图像聚焦和处理算法难以在星上部署，因此需要新的方法来减少下行数据量并降低延迟。", "method": "提出并评估了专门设计的神经网络，用于对Sentinel-1卫星在Stripmap和干涉宽幅（IW）模式下获取的未聚焦SAR数据进行实时推理。此外，还研究了船舶和风车之间的二元分类任务。", "result": "研究结果表明，其中一个模型可用于星载处理并在FPGA上部署，证明了其可行性。通过二元分类任务，验证了目标分类的可能性。", "conclusion": "神经网络是实现星载实时SAR数据处理的可行方案，能够有效减少数据下行量、降低延迟，并支持船舶监测和目标分类，从而提升海上监视能力。"}}
{"id": "2508.10716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10716", "abs": "https://arxiv.org/abs/2508.10716", "authors": ["Panwang Xia", "Qiong Wu", "Lei Yu", "Yi Liu", "Mingtao Xiong", "Lei Liang", "Yongjun Zhang", "Yi Wan"], "title": "Revisiting Cross-View Localization from Image Matching", "comment": null, "summary": "Cross-view localization aims to estimate the 3 degrees of freedom pose of a\nground-view image by registering it to aerial or satellite imagery. It is\nessential in GNSS-denied environments such as urban canyons and disaster zones.\nExisting methods either regress poses directly or align features in a shared\nbird's-eye view (BEV) space, both built upon accurate spatial correspondences\nbetween perspectives. However, these methods fail to establish strict\ncross-view correspondences, yielding only coarse or geometrically inconsistent\nmatches. Consequently, fine-grained image matching between ground and aerial\nviews remains an unsolved problem, which in turn constrains the\ninterpretability of localization results. In this paper, we revisit cross-view\nlocalization from the perspective of cross-view image matching and propose a\nnovel framework that improves both matching and localization. Specifically, we\nintroduce a Surface Model to model visible regions for accurate BEV projection,\nand a SimRefiner module to refine the similarity matrix through local-global\nresidual correction, eliminating the reliance on post-processing like RANSAC.\nTo further support research in this area, we introduce CVFM, the first\nbenchmark with 32,509 cross-view image pairs annotated with pixel-level\ncorrespondences. Extensive experiments demonstrate that our approach\nsubstantially improves both localization accuracy and image matching quality,\nsetting new baselines under extreme viewpoint disparity.", "AI": {"tldr": "本文提出了一种新的跨视角定位框架，通过引入表面模型和相似度细化模块，显著提升了地面图像与航空影像之间的匹配精度和定位准确性，并发布了首个带有像素级对应标注的大规模跨视角图像匹配数据集CVFM。", "motivation": "在城市峡谷和灾区等GNSS受限环境中，跨视角定位至关重要。现有方法（直接回归姿态或在共享鸟瞰图空间对齐特征）依赖于精确的空间对应关系，但未能建立严格的跨视角对应，导致匹配粗糙或几何不一致，使得细粒度图像匹配成为未解决的问题，限制了定位结果的可解释性。", "method": "本文从跨视角图像匹配的角度重新审视跨视角定位，提出一个新颖框架。具体方法包括：引入一个表面模型（Surface Model）来建模可见区域，以实现精确的鸟瞰图投影；设计一个SimRefiner模块，通过局部-全局残差校正来细化相似度矩阵，避免对RANSAC等后处理的依赖。此外，本文还发布了首个包含32,509对带有像素级对应标注的跨视角图像对的基准数据集CVFM。", "result": "广泛的实验证明，所提出的方法在极端视角差异下，显著提高了定位精度和图像匹配质量，并为该领域设定了新的基线。", "conclusion": "本文提出的框架有效改善了跨视角定位中的图像匹配和定位性能，并通过发布新数据集CVFM，为未来的相关研究提供了重要支持。"}}
{"id": "2508.10719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10719", "abs": "https://arxiv.org/abs/2508.10719", "authors": ["Longxiang Tang", "Ruihang Chu", "Xiang Wang", "Yujin Han", "Pingyu Wu", "Chunming He", "Yingya Zhang", "Shiwei Zhang", "Jiaya Jia"], "title": "Exploiting Discriminative Codebook Prior for Autoregressive Image Generation", "comment": "Submitted to TPAMI", "summary": "Advanced discrete token-based autoregressive image generation systems first\ntokenize images into sequences of token indices with a codebook, and then model\nthese sequences in an autoregressive paradigm. While autoregressive generative\nmodels are trained only on index values, the prior encoded in the codebook,\nwhich contains rich token similarity information, is not exploited. Recent\nstudies have attempted to incorporate this prior by performing naive k-means\nclustering on the tokens, helping to facilitate the training of generative\nmodels with a reduced codebook. However, we reveal that k-means clustering\nperforms poorly in the codebook feature space due to inherent issues, including\ntoken space disparity and centroid distance inaccuracy. In this work, we\npropose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to\nk-means clustering for more effectively mining and utilizing the token\nsimilarity information embedded in the codebook. DCPE replaces the commonly\nused centroid-based distance, which is found to be unsuitable and inaccurate\nfor the token feature space, with a more reasonable instance-based distance.\nUsing an agglomerative merging technique, it further addresses the token space\ndisparity issue by avoiding splitting high-density regions and aggregating\nlow-density ones. Extensive experiments demonstrate that DCPE is plug-and-play\nand integrates seamlessly with existing codebook prior-based paradigms. With\nthe discriminative prior extracted, DCPE accelerates the training of\nautoregressive models by 42% on LlamaGen-B and improves final FID and IS\nperformance.", "AI": {"tldr": "本文提出判别性码本先验提取器（DCPE），旨在更有效地利用自回归图像生成模型中码本的令牌相似性信息，以克服现有k-means聚类方法的缺陷，从而加速模型训练并提升性能。", "motivation": "现有的离散令牌自回归图像生成系统在训练时未能充分利用码本中包含的丰富令牌相似性信息。尽管有尝试通过k-means聚类引入此先验，但发现k-means在码本特征空间中表现不佳，存在令牌空间差异和质心距离不准确等固有问题。", "method": "本文提出DCPE，替代k-means聚类。DCPE用更适合令牌特征空间的基于实例的距离替换了常用的基于质心的距离，并通过凝聚合并技术避免分割高密度区域并聚合低密度区域，从而解决了令牌空间差异问题。", "result": "实验证明，DCPE即插即用，可与现有基于码本先验的范式无缝集成。通过DCPE提取的判别性先验，LlamaGen-B上的自回归模型训练速度加快了42%，并提升了最终的FID和IS性能。", "conclusion": "DCPE通过更有效地挖掘和利用码本中的令牌相似性信息，克服了传统聚类方法的局限性，显著加速了自回归图像模型的训练并提高了生成质量，为利用码本先验提供了有效途径。"}}
{"id": "2508.10731", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10731", "abs": "https://arxiv.org/abs/2508.10731", "authors": ["Luyao Tang", "Kunze Huang", "Chaoqi Chen", "Yuxuan Yuan", "Chenxin Li", "Xiaotong Tu", "Xinghao Ding", "Yue Huang"], "title": "Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction", "comment": "Accepted by ICCV 2025 as *** Highlight ***!", "summary": "Human perceptual systems excel at inducing and recognizing objects across\nboth known and novel categories, a capability far beyond current machine\nlearning frameworks. While generalized category discovery (GCD) aims to bridge\nthis gap, existing methods predominantly focus on optimizing objective\nfunctions. We present an orthogonal solution, inspired by the human cognitive\nprocess for novel object understanding: decomposing objects into visual\nprimitives and establishing cross-knowledge comparisons. We propose ConGCD,\nwhich establishes primitive-oriented representations through high-level\nsemantic reconstruction, binding intra-class shared attributes via\ndeconstruction. Mirroring human preference diversity in visual processing,\nwhere distinct individuals leverage dominant or contextual cues, we implement\ndominant and contextual consensus units to capture class-discriminative\npatterns and inherent distributional invariants, respectively. A consensus\nscheduler dynamically optimizes activation pathways, with final predictions\nemerging through multiplex consensus integration. Extensive evaluations across\ncoarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a\nconsensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.", "AI": {"tldr": "ConGCD是一种受人类认知启发的广义类别发现（GCD）新范式，通过分解视觉基元并整合多源共识，有效识别已知及新颖类别。", "motivation": "现有的机器学习框架在识别已知和新颖类别的物体方面远不如人类感知系统。虽然广义类别发现（GCD）旨在弥合这一差距，但现有方法主要侧重于优化目标函数，未能充分模拟人类理解新物体的认知过程。", "method": "本文提出ConGCD，其灵感来源于人类将物体分解为视觉基元并进行跨知识比较的认知过程。ConGCD通过高级语义重建建立面向基元的表示，并通过解构绑定类内共享属性。它引入了主导和上下文共识单元，分别捕获类别判别模式和固有的分布不变性，以模拟人类视觉处理中的偏好多样性。一个共识调度器动态优化激活路径，最终预测通过多重共识集成产生。", "result": "在粗粒度和细粒度基准上的广泛评估表明，ConGCD作为一种共识感知范式表现出卓越的有效性。", "conclusion": "ConGCD通过模仿人类认知过程中的基元分解和多源共识集成，为广义类别发现提供了一种有效且新颖的解决方案，显著提升了机器识别新颖类别的能力。"}}
{"id": "2508.10737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10737", "abs": "https://arxiv.org/abs/2508.10737", "authors": ["Matej Vitek", "Darian Tomašević", "Abhijit Das", "Sabari Nathan", "Gökhan Özbulak", "Gözde Ayşe Tataroğlu Özbulak", "Jean-Paul Calbimonte", "André Anjos", "Hariohm Hemant Bhatt", "Dhruv Dhirendra Premani", "Jay Chaudhari", "Caiyong Wang", "Jian Jiang", "Chi Zhang", "Qi Zhang", "Iyyakutti Iyappan Ganapathi", "Syed Sadaf Ali", "Divya Velayudan", "Maregu Assefa", "Naoufel Werghi", "Zachary A. Daniels", "Leeon John", "Ritesh Vyas", "Jalil Nourmohammadi Khiarak", "Taher Akbari Saeed", "Mahsa Nasehi", "Ali Kianfar", "Mobina Pashazadeh Panahi", "Geetanjali Sharma", "Pushp Raj Panth", "Raghavendra Ramachandra", "Aditya Nigam", "Umapada Pal", "Peter Peer", "Vitomir Štruc"], "title": "Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025", "comment": "IEEE International Joint Conference on Biometrics (IJCB) 2025, 13\n  pages", "summary": "This paper presents a summary of the 2025 Sclera Segmentation Benchmarking\nCompetition (SSBC), which focused on the development of privacy-preserving\nsclera-segmentation models trained using synthetically generated ocular images.\nThe goal of the competition was to evaluate how well models trained on\nsynthetic data perform in comparison to those trained on real-world datasets.\nThe competition featured two tracks: $(i)$ one relying solely on synthetic data\nfor model development, and $(ii)$ one combining/mixing synthetic with (a\nlimited amount of) real-world data. A total of nine research groups submitted\ndiverse segmentation models, employing a variety of architectural designs,\nincluding transformer-based solutions, lightweight models, and segmentation\nnetworks guided by generative frameworks. Experiments were conducted across\nthree evaluation datasets containing both synthetic and real-world images,\ncollected under diverse conditions. Results show that models trained entirely\non synthetic data can achieve competitive performance, particularly when\ndedicated training strategies are employed, as evidenced by the top performing\nmodels that achieved $F_1$ scores of over $0.8$ in the synthetic data track.\nMoreover, performance gains in the mixed track were often driven more by\nmethodological choices rather than by the inclusion of real data, highlighting\nthe promise of synthetic data for privacy-aware biometric development. The code\nand data for the competition is available at:\nhttps://github.com/dariant/SSBC_2025.", "AI": {"tldr": "2025年巩膜分割基准竞赛（SSBC）总结，评估了使用合成图像训练的隐私保护巩膜分割模型，发现纯合成数据训练的模型也能达到有竞争力的性能。", "motivation": "旨在评估使用合成数据训练的模型与使用真实数据训练的模型相比表现如何，并推动隐私保护的生物识别技术发展。", "method": "竞赛设置了两个赛道：(i) 仅使用合成数据进行模型开发；(ii) 混合使用合成数据和少量真实数据。共有九个研究团队提交了多样化的分割模型（包括Transformer、轻量级、生成框架引导等）。实验在包含合成和真实图像的三个评估数据集上进行。", "result": "结果显示，完全使用合成数据训练的模型可以达到有竞争力的性能，特别是采用专用训练策略时（F1分数超过0.8）。混合赛道中的性能提升更多地由方法选择而非真实数据引入驱动。", "conclusion": "合成数据在隐私保护的生物识别技术开发中具有巨大潜力。"}}
{"id": "2508.10740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10740", "abs": "https://arxiv.org/abs/2508.10740", "authors": ["Wongyun Yu", "Ahyun Seo", "Minsu Cho"], "title": "Axis-level Symmetry Detection with Group-Equivariant Representation", "comment": "Accepted to ICCV 2025", "summary": "Symmetry is a fundamental concept that has been extensively studied, yet\ndetecting it in complex scenes remains a significant challenge in computer\nvision. Recent heatmap-based approaches can localize potential regions of\nsymmetry axes but often lack precision in identifying individual axes. In this\nwork, we propose a novel framework for axis-level detection of the two most\ncommon symmetry types-reflection and rotation-by representing them as explicit\ngeometric primitives, i.e. lines and points. Our method employs a dual-branch\narchitecture that is equivariant to the dihedral group, with each branch\nspecialized to exploit the structure of dihedral group-equivariant features for\nits respective symmetry type. For reflection symmetry, we introduce\norientational anchors, aligned with group components, to enable\norientation-specific detection, and a reflectional matching that measures\nsimilarity between patterns and their mirrored counterparts across candidate\naxes. For rotational symmetry, we propose a rotational matching that compares\npatterns at fixed angular intervals to identify rotational centers. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance,\noutperforming existing approaches.", "AI": {"tldr": "该论文提出了一种新颖的框架，用于精确检测图像中的反射和旋转对称性，将它们表示为几何原语（线和点），并通过双分支、二面体群等变架构实现了最先进的性能。", "motivation": "在复杂场景中检测对称性是一个重大挑战，现有的基于热图的方法虽然能定位潜在区域，但在识别单个对称轴时缺乏精度。", "method": "本文提出一个双分支架构，该架构对二面体群具有等变性，每个分支专门处理其各自的对称类型。对于反射对称，引入了与群分量对齐的方向锚点和反射匹配机制；对于旋转对称，提出了旋转匹配机制，通过比较固定角度间隔的模式来识别旋转中心。对称性被表示为显式几何原语（线和点）。", "result": "实验结果表明，该方法实现了最先进的性能，优于现有方法。", "conclusion": "本研究提供了一种新颖且高精度的轴级别反射和旋转对称性检测方法，通过几何原语表示和专门的等变架构克服了现有方法的局限性。"}}
{"id": "2508.10741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10741", "abs": "https://arxiv.org/abs/2508.10741", "authors": ["Lixin Jia", "Zhiqing Guo", "Gaobo Yang", "Liejun Wang", "Keqin Li"], "title": "Forgery Guided Learning Strategy with Dual Perception Network for Deepfake Cross-domain Detection", "comment": null, "summary": "The emergence of deepfake technology has introduced a range of societal\nproblems, garnering considerable attention. Current deepfake detection methods\nperform well on specific datasets, but exhibit poor performance when applied to\ndatasets with unknown forgery techniques. Moreover, as the gap between emerging\nand traditional forgery techniques continues to widen, cross-domain detection\nmethods that rely on common forgery traces are becoming increasingly\nineffective. This situation highlights the urgency of developing deepfake\ndetection technology with strong generalization to cope with fast iterative\nforgery techniques. To address these challenges, we propose a Forgery Guided\nLearning (FGL) strategy designed to enable detection networks to continuously\nadapt to unknown forgery techniques. Specifically, the FGL strategy captures\nthe differential information between known and unknown forgery techniques,\nallowing the model to dynamically adjust its learning process in real time. To\nfurther improve the ability to perceive forgery traces, we design a Dual\nPerception Network (DPNet) that captures both differences and relationships\namong forgery traces. In the frequency stream, the network dynamically\nperceives and extracts discriminative features across various forgery\ntechniques, establishing essential detection cues. These features are then\nintegrated with spatial features and projected into the embedding space. In\naddition, graph convolution is employed to perceive relationships across the\nentire feature space, facilitating a more comprehensive understanding of\nforgery trace correlations. Extensive experiments show that our approach\ngeneralizes well across different scenarios and effectively handles unknown\nforgery challenges, providing robust support for deepfake detection. Our code\nis available on https://github.com/vpsg-research/FGL.", "AI": {"tldr": "针对现有深度伪造检测方法在未知伪造技术上泛化性差的问题，本文提出了一种伪造引导学习（FGL）策略和双感知网络（DPNet），通过捕获伪造痕迹的差异和关系，显著提升了检测模型对未知伪造的泛化能力。", "motivation": "当前深度伪造检测方法在特定数据集上表现良好，但在未知伪造技术数据集上性能不佳。新旧伪造技术之间的差距日益扩大，导致依赖共同伪造痕迹的跨域检测方法失效。因此，迫切需要开发具有强大泛化能力的深度伪造检测技术来应对快速迭代的伪造技术。", "method": "本文提出Forgery Guided Learning (FGL) 策略，通过捕获已知和未知伪造技术之间的差异信息，使检测模型能够实时动态调整学习过程，持续适应未知伪造技术。同时，设计了Dual Perception Network (DPNet)，该网络在频率流中动态感知并提取不同伪造技术的判别性特征，并将其与空间特征融合投影到嵌入空间。此外，利用图卷积感知整个特征空间中的伪造痕迹关系，以实现更全面的理解。", "result": "广泛的实验证明，所提出的方法在不同场景下均表现出良好的泛化能力，并能有效应对未知伪造带来的挑战，为深度伪造检测提供了鲁棒支持。", "conclusion": "通过引入FGL策略和DPNet网络，本研究成功解决了深度伪造检测中对未知伪造技术泛化性差的问题，为应对不断演进的伪造技术提供了有效的解决方案。"}}
{"id": "2508.10743", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.10743", "abs": "https://arxiv.org/abs/2508.10743", "authors": ["Ziwei Zou", "Bei Zou", "Xiaoyan Kui", "Wenqi Lu", "Haoran Dou", "Arezoo Zakeri", "Timothy Cootes", "Alejandro F Frangi", "Jinming Duan"], "title": "An Efficient Model-Driven Groupwise Approach for Atlas Construction", "comment": null, "summary": "Atlas construction is fundamental to medical image analysis, offering a\nstandardized spatial reference for tasks such as population-level anatomical\nmodeling. While data-driven registration methods have recently shown promise in\npairwise settings, their reliance on large training datasets, limited\ngeneralizability, and lack of true inference phases in groupwise contexts\nhinder their practical use. In contrast, model-driven methods offer\ntraining-free, theoretically grounded, and data-efficient alternatives, though\nthey often face scalability and optimization challenges when applied to large\n3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration\nvia Coordinate descent), a novel model-driven groupwise registration framework\nfor atlas construction. DARC supports a broad range of image dissimilarity\nmetrics and efficiently handles arbitrary numbers of 3D images without\nincurring GPU memory issues. Through a coordinate descent strategy and a\ncentrality-enforcing activation function, DARC produces unbiased, diffeomorphic\natlases with high anatomical fidelity. Beyond atlas construction, we\ndemonstrate two key applications: (1) One-shot segmentation, where labels\nannotated only on the atlas are propagated to subjects via inverse\ndeformations, outperforming state-of-the-art few-shot methods; and (2) shape\nsynthesis, where new anatomical variants are generated by warping the atlas\nmesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a\nflexible, generalizable, and resource-efficient framework for atlas\nconstruction and applications.", "AI": {"tldr": "DARC是一种新型模型驱动的群组配准框架，用于高效灵活地构建图谱，并支持单次分割和形状合成应用。", "motivation": "当前数据驱动的图谱构建方法存在对大量训练数据的依赖、泛化能力有限以及在群组环境中缺乏真实推理阶段的问题。而模型驱动方法虽然理论基础扎实，但在处理大型3D数据集时常面临可扩展性和优化挑战。", "method": "DARC（通过坐标下降进行微分同胚图谱配准）是一种新颖的模型驱动群组配准框架。它支持广泛的图像差异度量，并能高效处理任意数量的3D图像，避免GPU内存问题。通过坐标下降策略和强制中心性的激活函数，DARC能生成无偏、微分同胚且具有高解剖保真度的图谱。", "result": "DARC成功构建了无偏、微分同胚且具有高解剖保真度的图谱。在应用方面，它实现了单次分割，仅需在图谱上标注标签即可通过逆变形传播到受试者，性能优于当前最先进的少样本方法；同时，它还能通过合成微分同胚变形场来扭曲图谱网格，从而生成新的解剖变体，实现形状合成。", "conclusion": "DARC为图谱构建及其在医学图像分析中的应用提供了一个灵活、通用且资源高效的框架。"}}
{"id": "2508.10770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10770", "abs": "https://arxiv.org/abs/2508.10770", "authors": ["Tiancheng Han", "Yunfei Gao", "Yong Li", "Wuzhou Yu", "Qiaosheng Zhang", "Wenqi Shao"], "title": "From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models", "comment": "9 pages, 6 figures", "summary": "Spatio-physical reasoning, a foundation capability for understanding the real\nphysics world, is a critical step towards building robust world models. While\nrecent vision language models (VLMs) have shown remarkable progress in\nspecialized domains like multimodal mathematics and pure spatial understanding,\ntheir capability for spatio-physical reasoning remains largely unexplored. This\npaper provides a comprehensive diagnostic analysis of mainstream VLMs,\nrevealing that current models perform inadequately on this crucial task.\nFurther detailed analysis shows that this underperformance is largely\nattributable to biases caused by human-like prior and a lack of deep reasoning.\nTo address these challenges, we apply supervised fine-tuning followed by\nrule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant\nimprovements in spatio-physical reasoning capabilities and surpassing leading\nproprietary models. Nevertheless, despite this success, the model's\ngeneralization to new physics scenarios remains limited -- underscoring the\npressing need for new approaches in spatio-physical reasoning.", "AI": {"tldr": "该研究诊断了主流视觉语言模型（VLMs）在时空物理推理方面的不足，发现其受人类先验偏差和缺乏深度推理的影响。通过对Qwen2.5-VL-7B进行监督微调和基于规则的强化学习，显著提升了其时空物理推理能力，但模型对新物理场景的泛化能力仍有限。", "motivation": "时空物理推理是理解真实物理世界和构建鲁棒世界模型的关键基础能力。尽管近期视觉语言模型在多模态数学和纯空间理解等领域取得了显著进展，但其在时空物理推理方面的能力仍未被充分探索，且可能存在不足。", "method": "本研究首先对主流视觉语言模型进行了全面的诊断分析，以评估它们在时空物理推理任务上的表现。随后，为了解决发现的问题，对Qwen2.5-VL-7B模型应用了监督微调（SFT），并辅以基于规则的强化学习（RL）。", "result": "诊断分析显示，当前模型在时空物理推理任务上表现不佳，主要归因于人类先验引起的偏差和缺乏深度推理能力。通过应用监督微调和基于规则的强化学习，Qwen2.5-VL-7B的时空物理推理能力得到了显著提升，并超越了领先的专有模型。", "conclusion": "尽管通过提出的方法显著提升了模型的时空物理推理能力并超越了现有模型，但模型对新物理场景的泛化能力仍然有限。这强调了在时空物理推理领域迫切需要新的方法和研究方向。"}}
{"id": "2508.10786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10786", "abs": "https://arxiv.org/abs/2508.10786", "authors": ["Artem Sokolov", "Mikhail Nikitin", "Anton Konushin"], "title": "Cooperative Face Liveness Detection from Optical Flow", "comment": null, "summary": "In this work, we proposed a novel cooperative video-based face liveness\ndetection method based on a new user interaction scenario where participants\nare instructed to slowly move their frontal-oriented face closer to the camera.\nThis controlled approaching face protocol, combined with optical flow analysis,\nrepresents the core innovation of our approach. By designing a system where\nusers follow this specific movement pattern, we enable robust extraction of\nfacial volume information through neural optical flow estimation, significantly\nimproving discrimination between genuine faces and various presentation attacks\n(including printed photos, screen displays, masks, and video replays). Our\nmethod processes both the predicted optical flows and RGB frames through a\nneural classifier, effectively leveraging spatial-temporal features for more\nreliable liveness detection compared to passive methods.", "AI": {"tldr": "本文提出了一种新颖的基于用户交互的合作式活体检测方法，通过分析用户缓慢靠近摄像头的视频流和光流信息来区分真人与攻击。", "motivation": "现有活体检测方法在对抗各种呈现攻击（如照片、屏幕、面具、视频回放）时效果不佳，需要更可靠的判别方法。", "method": "核心方法是设计一种新的用户交互场景：参与者被指示缓慢地将面部靠近摄像头。结合光学流分析（通过神经光学流估计），从这种特定运动模式中提取面部体积信息。最终，通过一个神经网络分类器处理预测的光学流和RGB帧，利用时空特征进行活体检测。", "result": "该方法显著提高了真实人脸与各种呈现攻击（包括打印照片、屏幕显示、面具和视频回放）之间的区分能力，并且相比被动方法，提供了更可靠的活体检测。", "conclusion": "通过结合受控的面部靠近协议和光流分析，该合作式视频活体检测方法能够有效利用时空特征，实现对多种呈现攻击的鲁棒检测。"}}
{"id": "2508.10794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10794", "abs": "https://arxiv.org/abs/2508.10794", "authors": ["De-Xing Huang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuang-Yi Wang", "Tian-Yu Xiang", "Rui-Ze Ma", "Nu-Fang Xiao", "Zeng-Guang Hou"], "title": "VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel Segmentation", "comment": "14 pages, 11 figures", "summary": "Accurate vessel segmentation in X-ray angiograms is crucial for numerous\nclinical applications. However, the scarcity of annotated data presents a\nsignificant challenge, which has driven the adoption of self-supervised\nlearning (SSL) methods such as masked image modeling (MIM) to leverage\nlarge-scale unlabeled data for learning transferable representations.\nUnfortunately, conventional MIM often fails to capture vascular anatomy because\nof the severe class imbalance between vessel and background pixels, leading to\nweak vascular representations. To address this, we introduce Vascular\nanatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored\nfor X-ray angiograms that explicitly integrates anatomical knowledge into the\npre-training process. Specifically, it comprises two complementary components:\nanatomy-guided masking strategy and anatomical consistency loss. The former\npreferentially masks vessel-containing patches to focus the model on\nreconstructing vessel-relevant regions. The latter enforces consistency in\nvascular semantics between the original and reconstructed images, thereby\nimproving the discriminability of vascular representations. Empirically,\nVasoMIM achieves state-of-the-art performance across three datasets. These\nfindings highlight its potential to facilitate X-ray angiogram analysis.", "AI": {"tldr": "本文提出VasoMIM，一种针对X射线血管造影图像的新型自监督学习框架，通过整合血管解剖知识解决数据稀缺和类别不平衡问题，从而提高血管分割的准确性。", "motivation": "血管造影图像中精确的血管分割对临床应用至关重要，但带注释数据的稀缺性是一个巨大挑战。传统的自监督学习方法（如掩码图像建模MIM）由于血管与背景像素之间的严重类别不平衡，未能有效捕捉血管解剖结构，导致血管表示能力弱。", "method": "本文引入VasoMIM（血管解剖感知掩码图像建模）框架，该框架将解剖知识显式整合到预训练过程中。它包含两个互补组件：1) 解剖引导掩码策略，优先掩盖包含血管的图像块，使模型专注于重建血管相关区域；2) 解剖一致性损失，强制原始图像和重建图像之间血管语义的一致性，从而提高血管表示的区分度。", "result": "VasoMIM在三个数据集上均取得了最先进的性能。", "conclusion": "VasoMIM有望促进X射线血管造影图像的分析。"}}
{"id": "2508.10801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10801", "abs": "https://arxiv.org/abs/2508.10801", "authors": ["Ziqi Ye", "Shuran Ma", "Jie Yang", "Xiaoyi Yang", "Ziyang Gong", "Xue Yang", "Haipeng Wang"], "title": "Object Fidelity Diffusion for Remote Sensing Image Generation", "comment": null, "summary": "High-precision controllable remote sensing image generation is both\nmeaningful and challenging. Existing diffusion models often produce\nlow-fidelity images due to their inability to adequately capture morphological\ndetails, which may affect the robustness and reliability of object detection\nmodels. To enhance the accuracy and fidelity of generated objects in remote\nsensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which\neffectively improves the fidelity of generated objects. Specifically, we are\nthe first to extract the prior shapes of objects based on the layout for\ndiffusion models in remote sensing. Then, we introduce a dual-branch diffusion\nmodel with diffusion consistency loss, which can generate high-fidelity remote\nsensing images without providing real images during the sampling phase.\nFurthermore, we introduce DDPO to fine-tune the diffusion process, making the\ngenerated remote sensing images more diverse and semantically consistent.\nComprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art\nmethods in the remote sensing across key quality metrics. Notably, the\nperformance of several polymorphic and small object classes shows significant\nimprovement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for\nairplanes, ships, and vehicles, respectively.", "AI": {"tldr": "本文提出了一种名为OF-Diff的新型扩散模型，用于生成高精度、高保真度的遥感图像，特别是在对象细节和多样性方面表现出色，解决了现有模型保真度低的问题。", "motivation": "现有的扩散模型在生成遥感图像时，由于未能充分捕捉形态细节，常导致图像保真度低，这会影响目标检测模型的鲁棒性和可靠性。因此，需要提高生成遥感图像中对象的准确性和保真度。", "method": "1. 首次基于布局提取对象先验形状，用于遥感扩散模型。2. 引入带有扩散一致性损失的双分支扩散模型，实现在采样阶段无需真实图像即可生成高保真图像。3. 引入DDPO（Direct Diffusion Policy Optimization）来微调扩散过程，以提高生成图像的多样性和语义一致性。", "result": "OF-Diff在遥感领域的关键质量指标上超越了最先进的方法。特别是，对于多态和小型对象类别（如飞机、船舶和车辆），性能显著提升，mAP分别增加了8.3%、7.7%和4.0%。", "conclusion": "OF-Diff模型能够有效提高生成遥感图像中对象的保真度、多样性和语义一致性，显著改善了多态和小型目标检测的性能，为高精度可控遥感图像生成提供了新途径。"}}
{"id": "2508.10817", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10817", "abs": "https://arxiv.org/abs/2508.10817", "authors": ["Anand Kumar", "Harminder Pal Monga", "Tapasi Brahma", "Satyam Kalra", "Navas Sherif"], "title": "Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight CNN Benchmark Across 101 Classes of 33 Crops", "comment": "15 pages, 5 figures, 2 tables", "summary": "Plant diseases are a major threat to food security globally. It is important\nto develop early detection systems which can accurately detect. The advancement\nin computer vision techniques has the potential to solve this challenge. We\nhave developed a mobile-friendly solution which can accurately classify 101\nplant diseases across 33 crops. We built a comprehensive dataset by combining\ndifferent datasets, Plant Doc, PlantVillage, and PlantWild, all of which are\nfor the same purpose. We evaluated performance across several lightweight\narchitectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and\nEfficientNet-B0, B1 - specifically chosen for their efficiency on\nresource-constrained devices. The results were promising, with EfficientNet-B1\ndelivering our best performance at 94.7% classification accuracy. This\narchitecture struck an optimal balance between accuracy and computational\nefficiency, making it well-suited for real-world deployment on mobile devices.", "AI": {"tldr": "该研究开发了一种移动友好的计算机视觉解决方案，能够准确分类33种作物上的101种植物病害，其中EfficientNet-B1模型在准确性和计算效率之间取得了最佳平衡，达到94.7%的分类准确率。", "motivation": "植物病害对全球粮食安全构成重大威胁，因此需要开发能准确早期检测病害的系统。计算机视觉技术的进步为解决这一挑战提供了潜力。", "method": "开发了一种移动友好的解决方案，用于分类植物病害。构建了一个综合数据集，结合了PlantDoc、PlantVillage和PlantWild等现有数据集。评估了多种轻量级架构（MobileNetV2、MobileNetV3、MobileNetV3-Large、EfficientNet-B0、B1），这些架构专为资源受限设备上的效率而选择。", "result": "结果令人鼓舞，其中EfficientNet-B1模型表现最佳，分类准确率达到94.7%。", "conclusion": "EfficientNet-B1架构在准确性和计算效率之间达到了最佳平衡，非常适合在移动设备上进行实际部署。"}}
{"id": "2508.10833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10833", "abs": "https://arxiv.org/abs/2508.10833", "authors": ["Zhangxuan Gu", "Zhengwen Zeng", "Zhenyu Xu", "Xingran Zhou", "Shuheng Shen", "Yunfei Liu", "Beitong Zhou", "Changhua Meng", "Tianyu Xia", "Weizhi Chen", "Yue Wen", "Jingya Dou", "Fei Tang", "Jinzhen Lin", "Yulin Liu", "Zhenlin Guo", "Yichen Gong", "Heng Jia", "Changlong Gao", "Yuan Guo", "Yong Deng", "Zhenyu Guo", "Liang Chen", "Weiqiang Wang"], "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT", "comment": null, "summary": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models.To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies.To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment\n\\& Sparse Action Enhancement that refine historical reasoning traces and\nbalances the distribution of sparse but critical actions, leading to more\ncoherent planning and better generalization in complex UI tasks. Our\ncontributions include the publish of SOTA open-source UI agents, comprehensive\ndata cleaning protocols and a novel self-evolving framework for improving\nnavigation performance, which encourage further research and development in the\ncommunity. Code is available at https://github.com/antgroup/UI-Venus.", "AI": {"tldr": "UI-Venus是一个基于多模态大语言模型（Qwen2.5-VL）的原生UI代理，仅以屏幕截图为输入，通过强化微调（RFT）在UI定位和导航任务上均达到SOTA性能。", "motivation": "开发一个仅依赖屏幕截图作为输入的、高性能的UI代理，以在UI定位和导航任务中超越现有基线，并提供开源解决方案以推动社区研究。", "method": "UI-Venus基于Qwen2.5-VL，利用数十万高质量训练样本进行强化微调（RFT）。它引入了精心设计的UI定位和导航任务奖励函数，以及高效的数据清洗策略。为进一步提升导航性能，提出了“自演化轨迹历史对齐与稀疏动作增强”方法，用于优化历史推理轨迹和平衡稀疏关键动作的分布。", "result": "在UI定位任务上，UI-Venus 7B和72B变体在Screenspot-V2 / Pro基准上分别达到94.1% / 50.8%和95.3% / 61.9%，超越了包括GTA1和UI-TARS-1.5在内的现有SOTA基线。在AndroidWorld在线UI导航竞技场中，7B和72B变体分别实现了49.1%和65.9%的成功率，也优于现有模型。", "conclusion": "UI-Venus是一个SOTA的开源UI代理，在UI定位和导航任务上表现卓越。其贡献包括发布了SOTA开源UI代理、全面的数据清洗协议以及一个新颖的自演化框架，这些都将鼓励社区的进一步研究和发展。"}}
{"id": "2508.10838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10838", "abs": "https://arxiv.org/abs/2508.10838", "authors": ["Peng Xu", "Zhiyu Xiang", "Jingyun Fu", "Tianyu Pu", "Kai Wang", "Chaojie Ji", "Tingming Bai", "Eryun Liu"], "title": "Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning", "comment": null, "summary": "Current self-supervised stereo matching relies on the photometric consistency\nassumption, which breaks down in occluded regions due to ill-posed\ncorrespondences. To address this issue, we propose BaCon-Stereo, a simple yet\neffective contrastive learning framework for self-supervised stereo network\ntraining in both non-occluded and occluded regions. We adopt a teacher-student\nparadigm with multi-baseline inputs, in which the stereo pairs fed into the\nteacher and student share the same reference view but differ in target views.\nGeometrically, regions occluded in the student's target view are often visible\nin the teacher's, making it easier for the teacher to predict in these regions.\nThe teacher's prediction is rescaled to match the student's baseline and then\nused to supervise the student. We also introduce an occlusion-aware attention\nmap to better guide the student in learning occlusion completion. To support\ntraining, we synthesize a multi-baseline dataset BaCon-20k. Extensive\nexperiments demonstrate that BaCon-Stereo improves prediction in both occluded\nand non-occluded regions, achieves strong generalization and robustness, and\noutperforms state-of-the-art self-supervised methods on both KITTI 2015 and\n2012 benchmarks. Our code and dataset will be released upon paper acceptance.", "AI": {"tldr": "BaCon-Stereo提出了一种基于对比学习的自监督立体匹配框架，通过教师-学生范式和多基线输入解决遮挡区域的挑战，显著提升了预测性能和泛化能力。", "motivation": "当前的自监督立体匹配方法依赖于光度一致性假设，这在遮挡区域因对应关系不明确而失效，导致性能下降。", "method": "引入BaCon-Stereo，一个简单的对比学习框架，用于训练自监督立体网络。采用带有多基线输入的教师-学生范式，教师和学生共享同一参考视图但目标视图不同。教师的预测（在遮挡区域通常更准确）经过缩放后用于监督学生。此外，引入了遮挡感知注意力图以更好地指导学生学习遮挡补全。为支持训练，合成了多基线数据集BaCon-20k。", "result": "实验表明，BaCon-Stereo在遮挡和非遮挡区域的预测均有改善，展现出强大的泛化能力和鲁棒性，并在KITTI 2015和2012基准测试中超越了现有最先进的自监督方法。", "conclusion": "BaCon-Stereo通过其新颖的对比学习框架和多基线教师-学生范式，有效解决了自监督立体匹配中遮挡区域的难题，实现了性能的显著提升和更好的泛化能力。"}}
{"id": "2508.10840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10840", "abs": "https://arxiv.org/abs/2508.10840", "authors": ["Tajamul Ashraf", "Iqra Altaf Gillani"], "title": "Generalizable Federated Learning using Client Adaptive Focal Modulation", "comment": "WACV 2024 Extended Paper", "summary": "Federated learning (FL) has proven essential for privacy-preserving,\ncollaborative training across distributed clients. Our prior work, TransFed,\nintroduced a robust transformer-based FL framework that leverages a\nlearn-to-adapt hypernetwork to generate personalized focal modulation layers\nper client, outperforming traditional methods in non-IID and cross-domain\nsettings. In this extended version, we propose AdaptFED, where we deepen the\ninvestigation of focal modulation in generalizable FL by incorporating: (1) a\nrefined adaptation strategy that integrates task-aware client embeddings to\npersonalize modulation dynamics further, (2) enhanced theoretical bounds on\nadaptation performance, and (3) broader empirical validation across additional\nmodalities, including time-series and multilingual data. We also introduce an\nefficient variant of TransFed that reduces server-client communication overhead\nvia low-rank hypernetwork conditioning, enabling scalable deployment in\nresource-constrained environments. Extensive experiments on eight diverse\ndatasets reaffirm the superiority of our method over state-of-the-art\nbaselines, particularly in source-free and cross-task federated setups. Our\nfindings not only extend the capabilities of focal modulation in FL but also\npave the way for more adaptive, scalable, and generalizable transformer-based\nfederated systems. The code is available at\nhttp://github.com/Tajamul21/TransFed", "AI": {"tldr": "本文提出AdaptFED，是TransFed的扩展，通过精炼的适应策略、任务感知客户端嵌入和低秩超网络条件化，提升了联邦学习中焦点调制层的个性化、通用性和通信效率，在多样化数据集上表现优越。", "motivation": "联邦学习在隐私保护协作训练中至关重要，而现有方法在非独立同分布（non-IID）和跨领域设置中仍面临挑战。TransFed虽引入了鲁棒的Transformer-based FL框架，但研究人员旨在进一步深化焦点调制在通用联邦学习中的应用，并提高其适应性、可扩展性和泛化能力。", "method": "本文在TransFed基础上提出AdaptFED，具体方法包括：1) 整合任务感知客户端嵌入，以精炼适应策略并进一步个性化调制动态；2) 增强适应性能的理论界限；3) 扩展到时间序列和多语言数据等更多模态进行实证验证；4) 引入TransFed的高效变体，通过低秩超网络条件化减少服务器-客户端通信开销。", "result": "在八个不同数据集上的广泛实验表明，AdaptFED在性能上优于最先进的基线方法，特别是在无源和跨任务联邦设置中表现突出。", "conclusion": "研究结果不仅扩展了焦点调制在联邦学习中的能力，也为更具适应性、可扩展性和通用性的基于Transformer的联邦系统铺平了道路。"}}
{"id": "2508.10858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10858", "abs": "https://arxiv.org/abs/2508.10858", "authors": ["Harold Haodong Chen", "Haojian Huang", "Qifeng Chen", "Harry Yang", "Ser-Nam Lim"], "title": "Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation", "comment": "Project Page: https://haroldchen19.github.io/PhysHPO-Page/", "summary": "Recent advancements in video generation have enabled the creation of\nhigh-quality, visually compelling videos. However, generating videos that\nadhere to the laws of physics remains a critical challenge for applications\nrequiring realism and accuracy. In this work, we propose PhysHPO, a novel\nframework for Hierarchical Cross-Modal Direct Preference Optimization, to\ntackle this challenge by enabling fine-grained preference alignment for\nphysically plausible video generation. PhysHPO optimizes video alignment across\nfour hierarchical granularities: a) Instance Level, aligning the overall video\ncontent with the input prompt; b) State Level, ensuring temporal consistency\nusing boundary frames as anchors; c) Motion Level, modeling motion trajectories\nfor realistic dynamics; and d) Semantic Level, maintaining logical consistency\nbetween narrative and visuals. Recognizing that real-world videos are the best\nreflections of physical phenomena, we further introduce an automated data\nselection pipeline to efficiently identify and utilize \"good data\" from\nexisting large-scale text-video datasets, thereby eliminating the need for\ncostly and time-intensive dataset construction. Extensive experiments on both\nphysics-focused and general capability benchmarks demonstrate that PhysHPO\nsignificantly improves physical plausibility and overall video generation\nquality of advanced models. To the best of our knowledge, this is the first\nwork to explore fine-grained preference alignment and data selection for video\ngeneration, paving the way for more realistic and human-preferred video\ngeneration paradigms.", "AI": {"tldr": "PhysHPO是一个新颖的框架，通过分层跨模态直接偏好优化和自动化数据选择，显著提高了视频生成的物理真实性和整体质量。", "motivation": "现有视频生成技术在视觉效果上表现出色，但在生成符合物理定律的视频方面仍面临挑战，这对于需要高真实度和准确性的应用至关重要。", "method": "本文提出了PhysHPO框架，采用分层跨模态直接偏好优化（Hierarchical Cross-Modal Direct Preference Optimization）来实现物理上合理的视频生成。该框架在四个层次上优化视频对齐：实例级（整体内容与提示对齐）、状态级（使用边界帧确保时间一致性）、运动级（建模运动轨迹以实现真实动态）和语义级（保持叙事与视觉的逻辑一致性）。此外，引入了一个自动化数据选择流程，从现有大规模文本-视频数据集中识别并利用“优质数据”，无需昂贵的数据集构建。", "result": "在物理专注和通用能力基准上的广泛实验表明，PhysHPO显著提高了先进模型的物理真实性和整体视频生成质量。", "conclusion": "据作者所知，这是首次探索视频生成中细粒度偏好对齐和数据选择的工作，为更真实、更符合人类偏好的视频生成范式铺平了道路。"}}
{"id": "2508.10868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10868", "abs": "https://arxiv.org/abs/2508.10868", "authors": ["Yibo Zhang", "Li Zhang", "Rui Ma", "Nan Cao"], "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures", "comment": null, "summary": "We introduce TexVerse, a large-scale 3D dataset featuring high-resolution\ntextures. While recent advances in large-scale 3D datasets have enhanced\nhigh-resolution geometry generation, creating high-resolution textures\nend-to-end remains underexplored due to the lack of suitable datasets. TexVerse\nfills this gap with a curated collection of over 858K unique high-resolution 3D\nmodels sourced from Sketchfab, including more than 158K models with physically\nbased rendering (PBR) materials. Each model encompasses all of its\nhigh-resolution variants, bringing the total to 1.6M 3D instances. TexVerse\nalso includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,\nand TexVerse-Animation, with 54K animated models, both preserving original\nskeleton and animation data uploaded by the user. We also provide detailed\nmodel annotations describing overall characteristics, structural components,\nand intricate features. TexVerse offers a high-quality data resource with\nwide-ranging potential applications in texture synthesis, PBR material\ndevelopment, animation, and various 3D vision and graphics tasks.", "AI": {"tldr": "TexVerse是一个大规模高分辨率3D数据集，旨在填补现有数据集中高分辨率纹理生成方面的空白，包含大量独特的3D模型、PBR材质、骨骼和动画数据。", "motivation": "现有的大规模3D数据集主要关注高分辨率几何体的生成，但在端到端的高分辨率纹理生成方面探索不足，主要原因在于缺乏合适的数据集。", "method": "TexVerse通过以下方式构建：从Sketchfab收集超过858K个独特的、包含高分辨率变体的3D模型（总计1.6M个3D实例），其中包含超过158K个带有PBR材质的模型。此外，还提供了专门的子集：TexVerse-Skeleton（69K个绑定模型）和TexVerse-Animation（54K个动画模型），并保留了原始骨骼和动画数据。数据集还提供了详细的模型注释，描述整体特征、结构组件和复杂特征。", "result": "成功构建了TexVerse数据集，这是一个包含高分辨率纹理、PBR材质、骨骼和动画数据的大规模高质量3D资源，并提供了详细的模型注释。", "conclusion": "TexVerse是一个高质量的数据资源，在纹理合成、PBR材质开发、动画以及各种3D视觉和图形任务中具有广泛的应用潜力。"}}
{"id": "2508.10893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10893", "abs": "https://arxiv.org/abs/2508.10893", "authors": ["Yushi Lan", "Yihang Luo", "Fangzhou Hong", "Shangchen Zhou", "Honghua Chen", "Zhaoyang Lyu", "Shuai Yang", "Bo Dai", "Chen Change Loy", "Xingang Pan"], "title": "STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer", "comment": "TL;DR: Streaming 4D reconstruction using causal transformer. Project\n  page: https://nirvanalan.github.io/projects/stream3r", "summary": "We present STream3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing\nstate-of-the-art methods for multi-view reconstruction either depend on\nexpensive global optimization or rely on simplistic memory mechanisms that\nscale poorly with sequence length. In contrast, STream3R introduces an\nstreaming framework that processes image sequences efficiently using causal\nattention, inspired by advances in modern language modeling. By learning\ngeometric priors from large-scale 3D datasets, STream3R generalizes well to\ndiverse and challenging scenarios, including dynamic scenes where traditional\nmethods often fail. Extensive experiments show that our method consistently\noutperforms prior work across both static and dynamic scene benchmarks.\nMoreover, STream3R is inherently compatible with LLM-style training\ninfrastructure, enabling efficient large-scale pretraining and fine-tuning for\nvarious downstream 3D tasks. Our results underscore the potential of causal\nTransformer models for online 3D perception, paving the way for real-time 3D\nunderstanding in streaming environments. More details can be found in our\nproject page: https://nirvanalan.github.io/projects/stream3r.", "AI": {"tldr": "STream3R是一种新颖的3D重建方法，将点图预测重构为仅解码器Transformer问题，利用因果注意力高效处理图像序列，并能很好地泛化到动态场景。", "motivation": "现有的多视图重建方法依赖昂贵的全局优化或简单的内存机制，导致序列长度扩展性差。需要一种高效、可扩展且能处理动态场景的3D重建方法。", "method": "将点图预测重新定义为仅解码器Transformer问题。引入流式框架，利用受现代语言模型启发的因果注意力高效处理图像序列。从大规模3D数据集中学习几何先验。与LLM风格的训练基础设施兼容。", "result": "在静态和动态场景基准测试中，STream3R始终优于现有方法。它能很好地泛化到各种具有挑战性的场景，包括传统方法失败的动态场景。结果强调了因果Transformer模型在在线3D感知方面的潜力。", "conclusion": "因果Transformer模型在在线3D感知和流媒体环境中的实时3D理解方面具有巨大潜力。"}}
{"id": "2508.10894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10894", "abs": "https://arxiv.org/abs/2508.10894", "authors": ["Antoine Labatie", "Michael Vaccaro", "Nina Lardiere", "Anatol Garioud", "Nicolas Gonthier"], "title": "MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data", "comment": null, "summary": "Self-supervised learning holds great promise for remote sensing, but standard\nself-supervised methods must be adapted to the unique characteristics of Earth\nobservation data. We take a step in this direction by conducting a\ncomprehensive benchmark of fusion strategies and reconstruction target\nnormalization schemes for multimodal, multitemporal, and multispectral Earth\nobservation data. Based on our findings, we propose MAESTRO, a novel adaptation\nof the Masked Autoencoder, featuring optimized fusion strategies and a tailored\ntarget normalization scheme that introduces a spectral prior as a\nself-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO\nsets a new state-of-the-art on tasks that strongly rely on multitemporal\ndynamics, while remaining highly competitive on tasks dominated by a single\nmono-temporal modality. Code to reproduce all our experiments is available at\nhttps://github.com/ignf/maestro.", "AI": {"tldr": "本文针对遥感数据特性，提出了一种名为MAESTRO的自监督学习方法，通过优化融合策略和目标归一化方案（引入光谱先验），在多时相任务上达到了新的SOTA。", "motivation": "标准的自监督学习方法需要适应地球观测数据的独特特性，包括多模态、多时相和多光谱。", "method": "研究者对多模态、多时相、多光谱地球观测数据的融合策略和重建目标归一化方案进行了全面基准测试。在此基础上，提出了MAESTRO，它是Masked Autoencoder的一种新颖改编，具有优化的融合策略和量身定制的目标归一化方案，该方案引入了光谱先验作为自监督信号。", "result": "MAESTRO在四个地球观测数据集上进行了评估，在强烈依赖多时相动态的任务上建立了新的SOTA，同时在由单一单时相模态主导的任务上保持高度竞争力。", "conclusion": "MAESTRO成功地将自监督学习（特别是Masked Autoencoder）应用于地球观测数据，通过优化融合和引入光谱先验，在处理多时相数据方面表现出色，并能很好地适应单时相任务。"}}
{"id": "2508.10896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10896", "abs": "https://arxiv.org/abs/2508.10896", "authors": ["Jongseo Lee", "Kyungho Bae", "Kyle Min", "Gyeong-Moon Park", "Jinwoo Choi"], "title": "ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning", "comment": "2025 ICCV Highlight paper, 17 pages including supplementary material", "summary": "In this work, we tackle the problem of video classincremental learning\n(VCIL). Many existing VCIL methods mitigate catastrophic forgetting by\nrehearsal training with a few temporally dense samples stored in episodic\nmemory, which is memory-inefficient. Alternatively, some methods store\ntemporally sparse samples, sacrificing essential temporal information and\nthereby resulting in inferior performance. To address this trade-off between\nmemory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory\nintegrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL\nconsists of episodic memory for storing temporally sparse features and semantic\nmemory for storing general knowledge represented by learnable prompts. We\nintroduce a novel memory retrieval (MR) module that integrates episodic memory\nand semantic prompts through cross-attention, enabling the retrieval of\ntemporally dense features from temporally sparse features. We rigorously\nvalidate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and\nSomething-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and\nKinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced\nmemory, ESSENTIAL achieves favorable performance on the benchmarks.", "AI": {"tldr": "本文提出ESSENTIAL，一种针对视频类别增量学习（VCIL）的新方法，通过结合稀疏的片段记忆和语义记忆（可学习提示），并利用跨注意力机制恢复时序密集特征，以在显著降低内存消耗的同时，实现优异的性能。", "motivation": "现有的视频类别增量学习（VCIL）方法存在权衡：使用时序密集样本进行排练训练可以缓解灾难性遗忘，但内存效率低下；而存储时序稀疏样本虽然节省内存，却牺牲了重要的时序信息，导致性能下降。", "method": "提出ESSENTIAL方法，包含：1) 片段记忆，用于存储时序稀疏特征；2) 语义记忆，通过可学习提示存储通用知识。引入新颖的记忆检索（MR）模块，该模块通过交叉注意力整合片段记忆和语义提示，从而能够从时序稀疏特征中检索出时序密集特征。", "result": "ESSENTIAL在UCF-101、HMDB51和Something-Something-V2（来自TCD基准）以及UCF-101、ActivityNet和Kinetics-400（来自vCLIMB基准）等多样化数据集上进行了严格验证。结果表明，ESSENTIAL在显著减少内存消耗的同时，在这些基准测试中取得了良好的性能。", "conclusion": "ESSENTIAL通过有效整合时序稀疏的片段记忆和语义记忆，并利用创新的记忆检索机制，成功解决了视频类别增量学习中内存效率与性能之间的权衡问题，实现了在大幅降低内存占用的情况下保持甚至提升性能的目标。"}}
{"id": "2508.10897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10897", "abs": "https://arxiv.org/abs/2508.10897", "authors": ["Mengyuan Liu", "Xinshun Wang", "Zhongbin Fang", "Deheng Ye", "Xia Li", "Tao Tang", "Songtao Wu", "Xiangtai Li", "Ming-Hsuan Yang"], "title": "Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning", "comment": null, "summary": "This paper aims to model 3D human motion across domains, where a single model\nis expected to handle multiple modalities, tasks, and datasets. Existing\ncross-domain models often rely on domain-specific components and multi-stage\ntraining, which limits their practicality and scalability. To overcome these\nchallenges, we propose a new setting to train a unified cross-domain model\nthrough a single process, eliminating the need for domain-specific components\nand multi-stage training. We first introduce Pose-in-Context (PiC), which\nleverages in-context learning to create a pose-centric cross-domain model.\nWhile PiC generalizes across multiple pose-based tasks and datasets, it\nencounters difficulties with modality diversity, prompting strategy, and\ncontextual dependency handling. We thus propose Human-in-Context (HiC), an\nextension of PiC that broadens generalization across modalities, tasks, and\ndatasets. HiC combines pose and mesh representations within a unified\nframework, expands task coverage, and incorporates larger-scale datasets.\nAdditionally, HiC introduces a max-min similarity prompt sampling strategy to\nenhance generalization across diverse domains and a network architecture with\ndual-branch context injection for improved handling of contextual dependencies.\nExtensive experimental results show that HiC performs better than PiC in terms\nof generalization, data scale, and performance across a wide range of domains.\nThese results demonstrate the potential of HiC for building a unified\ncross-domain 3D human motion model with improved flexibility and scalability.\nThe source codes and models are available at\nhttps://github.com/BradleyWang0416/Human-in-Context.", "AI": {"tldr": "本文提出Human-in-Context (HiC)模型，旨在通过单一训练过程，实现跨模态、跨任务、跨数据集的统一3D人体运动建模，克服了现有方法的局限性。", "motivation": "现有跨域模型依赖特定领域组件和多阶段训练，限制了其实用性和可扩展性，无法有效处理多模态、多任务和多数据集的3D人体运动建模。", "method": "引入一种新的单流程训练设置，旨在构建统一的跨域模型。首先提出Pose-in-Context (PiC)利用上下文学习创建以姿态为中心的跨域模型。在此基础上，提出Human-in-Context (HiC)，扩展PiC以处理模态多样性、任务覆盖和大规模数据集，具体包括：结合姿态和网格表示、引入最大-最小相似度提示采样策略、以及采用双分支上下文注入网络架构。", "result": "实验结果表明，HiC在泛化能力、数据规模和性能方面均优于PiC，并在广泛的领域中表现出色。", "conclusion": "HiC展示了构建统一跨域3D人体运动模型的巨大潜力，显著提升了模型的灵活性和可扩展性。"}}
{"id": "2508.10898", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.10898", "abs": "https://arxiv.org/abs/2508.10898", "authors": ["Chaoyue Song", "Xiu Li", "Fan Yang", "Zhongcong Xu", "Jiacheng Wei", "Fayao Liu", "Jiashi Feng", "Guosheng Lin", "Jianfeng Zhang"], "title": "Puppeteer: Rig and Animate Your 3D Models", "comment": "Project page: https://chaoyuesong.github.io/Puppeteer/", "summary": "Modern interactive applications increasingly demand dynamic 3D content, yet\nthe transformation of static 3D models into animated assets constitutes a\nsignificant bottleneck in content creation pipelines. While recent advances in\ngenerative AI have revolutionized static 3D model creation, rigging and\nanimation continue to depend heavily on expert intervention. We present\nPuppeteer, a comprehensive framework that addresses both automatic rigging and\nanimation for diverse 3D objects. Our system first predicts plausible skeletal\nstructures via an auto-regressive transformer that introduces a joint-based\ntokenization strategy for compact representation and a hierarchical ordering\nmethodology with stochastic perturbation that enhances bidirectional learning\ncapabilities. It then infers skinning weights via an attention-based\narchitecture incorporating topology-aware joint attention that explicitly\nencodes inter-joint relationships based on skeletal graph distances. Finally,\nwe complement these rigging advances with a differentiable optimization-based\nanimation pipeline that generates stable, high-fidelity animations while being\ncomputationally more efficient than existing approaches. Extensive evaluations\nacross multiple benchmarks demonstrate that our method significantly\noutperforms state-of-the-art techniques in both skeletal prediction accuracy\nand skinning quality. The system robustly processes diverse 3D content, ranging\nfrom professionally designed game assets to AI-generated shapes, producing\ntemporally coherent animations that eliminate the jittering issues common in\nexisting methods.", "AI": {"tldr": "Puppeteer是一个全面的框架，通过自动骨骼预测、蒙皮权重推理和可微分动画管线，实现多样化3D对象的自动绑定和动画生成。", "motivation": "现代交互式应用对动态3D内容需求增加，但将静态3D模型转换为动画资产是内容创建流程中的主要瓶颈，绑定和动画仍高度依赖专家干预。", "method": "该系统首先通过自回归Transformer预测骨骼结构，该Transformer采用基于关节的token化和带有随机扰动的分层排序。然后，通过结合拓扑感知关节注意力的基于注意力的架构推断蒙皮权重。最后，通过可微分优化驱动的动画管线生成稳定、高保真动画。", "result": "在多个基准测试中，该方法在骨骼预测精度和蒙皮质量方面显著优于现有技术。系统能鲁棒处理多样化的3D内容（从专业游戏资产到AI生成形状），生成时间连贯的动画，消除了现有方法常见的抖动问题，且计算效率更高。", "conclusion": "Puppeteer提供了一个综合性解决方案，能够自动进行3D对象的绑定和动画，显著提高了效率和质量，解决了现有方法的局限性，并能处理广泛的3D内容类型。"}}
{"id": "2508.10900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10900", "abs": "https://arxiv.org/abs/2508.10900", "authors": ["Shuteng Wang", "Christian Theobalt", "Vladislav Golyanik"], "title": "Quantum Visual Fields with Neural Amplitude Encoding", "comment": "17 pages, 15 figures and four tables; project page:\n  https://4dqv.mpi-inf.mpg.de/QVF/", "summary": "Quantum Implicit Neural Representations (QINRs) include components for\nlearning and execution on gate-based quantum computers. While QINRs recently\nemerged as a promising new paradigm, many challenges concerning their\narchitecture and ansatz design, the utility of quantum-mechanical properties,\ntraining efficiency and the interplay with classical modules remain. This paper\nadvances the field by introducing a new type of QINR for 2D image and 3D\ngeometric field learning, which we collectively refer to as Quantum Visual\nField (QVF). QVF encodes classical data into quantum statevectors using neural\namplitude encoding grounded in a learnable energy manifold, ensuring meaningful\nHilbert space embeddings. Our ansatz follows a fully entangled design of\nlearnable parametrised quantum circuits, with quantum (unitary) operations\nperformed in the real Hilbert space, resulting in numerically stable training\nwith fast convergence. QVF does not rely on classical post-processing -- in\ncontrast to the previous QINR learning approach -- and directly employs\nprojective measurement to extract learned signals encoded in the ansatz.\nExperiments on a quantum hardware simulator demonstrate that QVF outperforms\nthe existing quantum approach and widely used classical foundational baselines\nin terms of visual representation accuracy across various metrics and model\ncharacteristics, such as learning of high-frequency details. We also show\napplications of QVF in 2D and 3D field completion and 3D shape interpolation,\nhighlighting its practical potential.", "AI": {"tldr": "本文提出了一种新型量子隐式神经表示（QINR）——量子视觉场（QVF），用于2D图像和3D几何场学习。QVF通过神经幅度编码和全纠缠参数化量子电路，实现了在量子硬件模拟器上超越现有量子方法和经典基线的视觉表示精度，并展示了在场补全和形状插值方面的应用潜力。", "motivation": "现有的量子隐式神经表示（QINR）面临架构和ansatz设计、量子力学特性利用、训练效率以及与经典模块协同等方面的挑战。本研究旨在改进QINR，使其能更有效地应用于2D图像和3D几何场学习。", "method": "引入量子视觉场（QVF），通过基于可学习能量流形的神经幅度编码将经典数据编码为量子态向量，确保有意义的希尔伯特空间嵌入。采用全纠缠设计的可学习参数化量子电路（PQC），量子操作在实希尔伯特空间进行以确保训练稳定和快速收敛。QVF不依赖经典后处理，直接通过投影测量提取学习到的信号。", "result": "在量子硬件模拟器上的实验表明，QVF在视觉表示精度方面优于现有的量子方法和广泛使用的经典基线，尤其在学习高频细节方面表现出色。研究还展示了QVF在2D和3D场补全以及3D形状插值中的应用，突显了其在实际应用中的潜力。", "conclusion": "QVF是一种先进的量子隐式神经表示，通过创新的数据编码和电路设计，在视觉场学习任务中展现出卓越的性能，并具有广泛的实际应用前景，为量子机器学习领域带来了新的进展。"}}
