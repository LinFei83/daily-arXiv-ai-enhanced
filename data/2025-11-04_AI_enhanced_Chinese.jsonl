{"id": "2511.00020", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms.", "AI": {"tldr": "本研究提出了一种多模态虚假评论检测框架，结合BERT文本特征和ResNet-50视觉特征，有效提升了对数字商务平台中虚假评论的识别能力，F1分数达到0.934。", "motivation": "数字商务中用户生成评论对消费者行为和平台信誉至关重要，但虚假评论（由机器人、付费代理或AI生成）泛滥，严重威胁信任和透明度。现有检测模型主要依赖单模态（通常是文本）数据，无法捕捉跨模态的语义不一致性。", "method": "研究提出一个鲁棒的多模态虚假评论检测框架，整合了BERT编码的文本特征和ResNet-50提取的视觉特征。这些特征通过一个分类头进行融合，共同预测评论的真实性。该方法利用了一个包含21,142张用户上传图像的精选数据集，涵盖外卖、酒店和电商领域。", "result": "实验结果表明，多模态模型优于单模态基线，在测试集上F1分数达到0.934。混淆矩阵和定性分析也突出显示了模型检测细微不一致（例如，夸大其词的文本赞美与不相关或低质量图片配对）的能力，这在欺骗性内容中很常见。", "conclusion": "本研究证明了多模态学习在维护数字信任方面的关键作用，并为各种在线平台的内容审核提供了一个可扩展的解决方案。"}}
{"id": "2511.00094", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00094", "abs": "https://arxiv.org/abs/2511.00094", "authors": ["Angelos Alexopoulos", "Agorakis Bompotas", "Nikitas Rigas Kalogeropoulos", "Panagiotis Kechagias", "Athanasios P. Kalogeras", "Christos Alexakos"], "title": "Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments", "comment": "Accepted for presentation to 11th IEEE International Smart Cities\n  Conference (ISC2 2025)", "summary": "Robotic systems have become integral to smart environments, enabling\napplications ranging from urban surveillance and automated agriculture to\nindustrial automation. However, their effective operation in dynamic settings -\nsuch as smart cities and precision farming - is challenged by continuously\nevolving topographies and environmental conditions. Traditional control systems\noften struggle to adapt quickly, leading to inefficiencies or operational\nfailures. To address this limitation, we propose a novel framework for\nautonomous and dynamic reconfiguration of robotic controllers using Digital\nTwin technology. Our approach leverages a virtual replica of the robot's\noperational environment to simulate and optimize movement trajectories in\nresponse to real-world changes. By recalculating paths and control parameters\nin the Digital Twin and deploying the updated code to the physical robot, our\nmethod ensures rapid and reliable adaptation without manual intervention. This\nwork advances the integration of Digital Twins in robotics, offering a scalable\nsolution for enhancing autonomy in smart, dynamic environments.", "AI": {"tldr": "本文提出了一种利用数字孪生技术，实现机器人在动态环境中自主、动态地重新配置控制器的新框架，以提高其适应性和自主性。", "motivation": "机器人系统在智能环境中面临动态地形和环境条件的挑战，传统控制系统难以快速适应，导致效率低下或操作失败。", "method": "该研究提出了一种利用数字孪生技术实现机器人控制器自主动态重配置的新颖框架。通过机器人操作环境的虚拟副本，模拟并优化运动轨迹以响应现实世界的变化。在数字孪生中重新计算路径和控制参数，并将更新后的代码部署到物理机器人。", "result": "该方法确保了机器人能够快速可靠地适应动态环境，无需人工干预。", "conclusion": "这项工作推动了数字孪生技术在机器人领域的整合，为增强智能、动态环境中的机器人自主性提供了一个可扩展的解决方案。"}}
{"id": "2511.00033", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00033", "abs": "https://arxiv.org/abs/2511.00033", "authors": ["Diqi He", "Xuehao Gao", "Hao Li", "Junwei Han", "Dingwen Zhang"], "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization", "comment": null, "summary": "The Zero-shot Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) task requires agents to navigate previously unseen 3D environments\nusing natural language instructions, without any scene-specific training. A\ncritical challenge in this setting lies in ensuring agents' actions align with\nboth spatial structure and task intent over long-horizon execution. Existing\nmethods often fail to achieve robust navigation due to a lack of structured\ndecision-making and insufficient integration of feedback from previous actions.\nTo address these challenges, we propose STRIDER (Instruction-Aligned Structural\nDecision Space Optimization), a novel framework that systematically optimizes\nthe agent's decision space by integrating spatial layout priors and dynamic\ntask feedback. Our approach introduces two key innovations: 1) a Structured\nWaypoint Generator that constrains the action space through spatial structure,\nand 2) a Task-Alignment Regulator that adjusts behavior based on task progress,\nensuring semantic alignment throughout navigation. Extensive experiments on the\nR2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms\nstrong SOTA across key metrics; in particular, it improves Success Rate (SR)\nfrom 29% to 35%, a relative gain of 20.7%. Such results highlight the\nimportance of spatially constrained decision-making and feedback-guided\nexecution in improving navigation fidelity for zero-shot VLN-CE.", "AI": {"tldr": "本文提出STRIDER框架，通过整合空间结构先验和动态任务反馈来优化决策空间，显著提升了零样本视觉-语言导航（VLN-CE）在连续环境中的表现，尤其是在成功率方面。", "motivation": "零样本视觉-语言导航（VLN-CE）任务中，智能体在未知3D环境中执行指令时，难以确保其动作与空间结构和任务意图长期保持一致。现有方法因缺乏结构化决策和对先前动作反馈的整合不足，导致导航不够稳健。", "method": "本文提出了STRIDER（Instruction-Aligned Structural Decision Space Optimization）框架，通过整合空间布局先验和动态任务反馈来系统性地优化智能体的决策空间。其核心创新包括：1) 结构化路径点生成器（Structured Waypoint Generator），通过空间结构约束动作空间；2) 任务对齐调节器（Task-Alignment Regulator），根据任务进度调整行为，确保整个导航过程的语义对齐。", "result": "在R2R-CE和RxR-CE基准测试中，STRIDER显著优于现有最先进方法。特别是，成功率（SR）从29%提升到35%，相对增益达20.7%。", "conclusion": "实验结果强调了空间受限决策和反馈引导执行对于提高零样本VLN-CE导航准确性的重要性。"}}
{"id": "2511.00041", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00041", "abs": "https://arxiv.org/abs/2511.00041", "authors": ["Yingzhao Jian", "Zhongan Wang", "Yi Yang", "Hehe Fan"], "title": "Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World", "comment": null, "summary": "Humanoid agents often struggle to handle flexible and diverse interactions in\nopen environments. A common solution is to collect massive datasets to train a\nhighly capable model, but this approach can be prohibitively expensive. In this\npaper, we explore an alternative solution: empowering off-the-shelf\nVision-Language Models (VLMs, such as GPT-4) to control humanoid agents,\nthereby leveraging their strong open-world generalization to mitigate the need\nfor extensive data collection. To this end, we present \\textbf{BiBo}\n(\\textbf{B}uilding humano\\textbf{I}d agent \\textbf{B}y \\textbf{O}ff-the-shelf\nVLMs). It consists of two key components: (1) an \\textbf{embodied instruction\ncompiler}, which enables the VLM to perceive the environment and precisely\ntranslate high-level user instructions (e.g., {\\small\\itshape ``have a rest''})\ninto low-level primitive commands with control parameters (e.g.,\n{\\small\\itshape ``sit casually, location: (1, 2), facing: 90$^\\circ$''}); and\n(2) a diffusion-based \\textbf{motion executor}, which generates human-like\nmotions from these commands, while dynamically adapting to physical feedback\nfrom the environment. In this way, BiBo is capable of handling not only basic\ninteractions but also diverse and complex motions. Experiments demonstrate that\nBiBo achieves an interaction task success rate of 90.2\\% in open environments,\nand improves the precision of text-guided motion execution by 16.3\\% over prior\nmethods. The code will be made publicly available.", "AI": {"tldr": "本文提出BiBo框架，通过利用现成的视觉-语言模型（VLMs）控制人形智能体，将其高泛化能力应用于开放环境中的多样化交互，从而避免了大量数据收集的需要。", "motivation": "现有的人形智能体在开放环境中处理灵活多样的交互时表现不佳，而收集大量数据集来训练高性能模型成本过高。", "method": "BiBo框架包含两个核心组件：1) 一个具身指令编译器，使VLM能够感知环境并将高级用户指令（如“休息”）精确转换为带控制参数的低级原始命令；2) 一个基于扩散的运动执行器，根据这些命令生成类人动作，并动态适应环境中的物理反馈。", "result": "BiBo在开放环境中的交互任务成功率达到90.2%，并且将文本引导运动执行的精度比现有方法提高了16.3%。", "conclusion": "BiBo通过赋能现成的VLM来控制人形智能体，成功解决了开放环境中多样化复杂运动的挑战，显著提高了交互任务的成功率和运动执行的精度。"}}
{"id": "2511.00026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00026", "abs": "https://arxiv.org/abs/2511.00026", "authors": ["Chaitanya Shinde", "Divya Garikapati"], "title": "Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience", "comment": null, "summary": "Generative Artificial Intelligence is emerging as a transformative force in\nthe automotive industry, enabling novel applications across vehicle design,\nmanufacturing, autonomous driving, predictive maintenance, and in vehicle user\nexperience. This paper provides a comprehensive review of the current state of\nGenAI in automotive, highlighting enabling technologies such as Generative\nAdversarial Networks and Variational Autoencoders. Key opportunities include\naccelerating autonomous driving validation through synthetic data generation,\noptimizing component design, and enhancing human machine interaction via\npersonalized and adaptive interfaces. At the same time, the paper identifies\nsignificant technical, ethical, and safety challenges, including computational\ndemands, bias, intellectual property concerns, and adversarial robustness, that\nmust be addressed for responsible deployment. A case study on Mercedes Benzs\nMBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more\nnatural, proactive, and personalized in car interactions compared to legacy\nrule based assistants. Through this review and case study, the paper outlines\nboth the promise and limitations of GenAI integration in the automotive sector\nand presents directions for future research and development aimed at achieving\nsafer, more efficient, and user centric mobility. Unlike prior reviews that\nfocus solely on perception or manufacturing, this paper emphasizes generative\nAI in voice based HMI, bridging safety and user experience perspectives.", "AI": {"tldr": "本文全面综述了生成式AI在汽车行业的应用、使能技术、机遇、挑战及未来方向，并以奔驰MBUX虚拟助手为例，强调了其在语音HMI、安全和用户体验方面的潜力与局限。", "motivation": "生成式AI正成为汽车行业的变革力量，在多个领域带来创新应用。当前缺乏一篇全面覆盖车辆设计、制造、自动驾驶、预测性维护及车载用户体验，并特别关注基于语音的人机交互（HMI）和安全视角的综述。", "method": "本文采用综合性综述方法，回顾了生成式AI在汽车领域的现状，突出了生成对抗网络（GANs）和变分自编码器（VAEs）等关键技术，识别了主要机遇和技术、伦理、安全挑战。此外，还通过奔驰MBUX虚拟助手的案例研究，具体说明了生成式AI在语音系统中的应用效果。", "result": "生成式AI在汽车行业具有显著机遇，包括通过合成数据加速自动驾驶验证、优化部件设计以及通过个性化和自适应界面增强人机交互。然而，也面临计算需求、偏见、知识产权和对抗性鲁棒性等重大技术、伦理和安全挑战。案例研究表明，生成式AI驱动的语音系统能提供比传统基于规则的助手更自然、主动和个性化的车内交互。", "conclusion": "生成式AI在汽车行业的整合既有巨大前景也存在局限性。未来的研发应致力于解决这些挑战，以实现更安全、高效和以用户为中心的出行方式。"}}
{"id": "2511.00242", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00242", "abs": "https://arxiv.org/abs/2511.00242", "authors": ["Arne Burdack", "Maximilian Stargardt", "Christoph Winkler", "Konrad Klein", "Detlef Stolten", "Jochen Linssen", "Heidi Heinrichs"], "title": "Which Top Energy-Intensive Manufacturing Countries Can Compete in a Renewable Energy Future?", "comment": "29 pages, 16 figures", "summary": "In a world increasingly powered by renewables and aiming for greenhouse\ngas-neutral industrial production, the future competitiveness of todays top\nmanufacturing countries is questioned. This study applies detailed energy\nsystem modeling to quantify the Renewable Pull, an incentive for industry\nrelocation exerted by countries with favorable renewable conditions. Results\nreveal that the Renewable Pull is not a cross-industrial phenomenon but\nstrongly depends on the relationship between energy costs and transport costs.\nThe intensity of the Renewable Pull varies, with China, India, and Japan facing\na significantly stronger effect than Germany and the United States.\nIncorporating national capital cost assumptions proves critical, reducing\nGermanys Renewable Pull by a factor of six and positioning it as the second\nleast affected top manufacturing country after Saudi Arabia. Using Germany as a\ncase study, the analysis moreover illustrates that targeted import strategies,\nespecially within the EU, can nearly eliminate the Renewable Pull, offering\npolicymakers clear options for risk mitigation.", "AI": {"tldr": "本研究通过能源系统模型量化了“可再生能源吸引力”（即有利的可再生能源条件对工业搬迁的激励），发现其并非跨行业现象，强度因国家而异，并受能源、运输和资本成本影响。有针对性的进口策略可有效缓解其影响。", "motivation": "随着全球日益依赖可再生能源并追求温室气体中和的工业生产，当前主要制造业国家的未来竞争力受到质疑。本研究旨在量化因有利可再生能源条件而产生的工业搬迁激励。", "method": "应用详细的能源系统建模。", "result": "“可再生能源吸引力”并非跨行业现象，而是强烈依赖于能源成本与运输成本之间的关系。其强度各异，中国、印度和日本面临的影响显著强于德国和美国。纳入国家资本成本假设至关重要，能使德国的“可再生能源吸引力”降低六倍，使其成为继沙特阿拉伯之后受影响最小的第二大制造业国家。此外，针对性的进口策略（尤其是在欧盟内部）几乎可以消除德国的“可再生能源吸引力”。", "conclusion": "“可再生能源吸引力”是一个复杂现象，受多重因素影响。政策制定者可以利用明确的风险缓解方案，如实施有针对性的进口策略，来应对潜在的工业搬迁风险。"}}
{"id": "2511.00011", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00011", "abs": "https://arxiv.org/abs/2511.00011", "authors": ["Alexander Okupnik", "Johannes Schneider", "Kyriakos Flouris"], "title": "Generative human motion mimicking through feature extraction in denoising diffusion settings", "comment": null, "summary": "Recent success with large language models has sparked a new wave of verbal\nhuman-AI interaction. While such models support users in a variety of creative\ntasks, they lack the embodied nature of human interaction. Dance, as a primal\nform of human expression, is predestined to complement this experience. To\nexplore creative human-AI interaction exemplified by dance, we build an\ninteractive model based on motion capture (MoCap) data. It generates an\nartificial other by partially mimicking and also \"creatively\" enhancing an\nincoming sequence of movement data. It is the first model, which leverages\nsingle-person motion data and high level features in order to do so and, thus,\nit does not rely on low level human-human interaction data. It combines ideas\nof two diffusion models, motion inpainting, and motion style transfer to\ngenerate movement representations that are both temporally coherent and\nresponsive to a chosen movement reference. The success of the model is\ndemonstrated by quantitatively assessing the convergence of the feature\ndistribution of the generated samples and the test set which serves as\nsimulating the human performer. We show that our generations are first steps to\ncreative dancing with AI as they are both diverse showing various deviations\nfrom the human partner while appearing realistic.", "AI": {"tldr": "该研究构建了一个基于单人动作捕捉数据的交互式AI模型，用于生成舞蹈动作，以探索人类-AI在舞蹈中的创意互动，模型能生成多样且逼真的动作，是实现与AI共舞的第一步。", "motivation": "大型语言模型（LLMs）虽然在创意任务中表现出色，但缺乏具身交互体验。舞蹈作为一种原始的人类表达形式，被认为是弥补这一缺陷的理想方式。现有模型通常依赖于低级的人类-人类交互数据，本研究旨在通过利用单人动作数据和高级特征来解决这一限制。", "method": "研究建立了一个基于动作捕捉（MoCap）数据的交互式模型。该模型通过部分模仿并“创造性地”增强输入的动作序列来生成“虚拟舞伴”。它首次利用单人动作数据和高级特征，而非低级人类-人类交互数据。技术上结合了两种扩散模型、动作修复（motion inpainting）和动作风格迁移（motion style transfer）的思想，以生成时间连贯且响应所选动作参考的运动表示。", "result": "通过定量评估生成样本的特征分布与模拟人类表演者的测试集之间的收敛性，证明了模型的成功。结果表明，生成的动作既多样（显示出与人类舞伴的各种偏差）又逼真。这标志着与AI进行创意舞蹈的第一步。", "conclusion": "该模型成功地生成了多样化且逼真的舞蹈动作，为探索人类-AI在舞蹈中的创意互动提供了初步解决方案。通过利用单人动作数据和高级特征，它克服了对低级人类-人类交互数据的依赖，为未来更丰富的具身AI交互奠定了基础。"}}
{"id": "2511.00449", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00449", "abs": "https://arxiv.org/abs/2511.00449", "authors": ["Xiaolong Li", "Zhi-Qin John Xu", "Yan Ren", "Tianming Qiu", "Xiaowen Wang"], "title": "Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements", "comment": null, "summary": "Accurate segmentation of pediatric brain tumors in multi-parametric magnetic\nresonance imaging (mpMRI) is critical for diagnosis, treatment planning, and\nmonitoring, yet faces unique challenges due to limited data, high anatomical\nvariability, and heterogeneous imaging across institutions. In this work, we\npresent an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the\nlargest public dataset of pre-treatment pediatric high-grade gliomas. Our\ncontributions include: (1) a widened residual encoder with\nsqueeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions;\n(3) a specificity-driven regularization term; and (4) small-scale Gaussian\nweight initialization. We further refine predictions with two postprocessing\nsteps. Our models achieved first place on the Task-6 validation leaderboard,\nattaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910\n(NET), 0.928 (TC) and 0.928 (WT).", "AI": {"tldr": "本文提出了一种针对BraTS 2025 Task-6 (PED)任务优化的nnU-Net框架，用于儿科脑肿瘤分割，通过多项创新改进，在验证排行榜上取得了第一名。", "motivation": "儿科脑肿瘤在多参数磁共振成像(mpMRI)中的准确分割对于诊断、治疗计划和监测至关重要，但面临数据有限、解剖变异性高和跨机构成像异质性等独特挑战。", "method": "研究方法包括：(1) 带有挤压-激励(SE)注意力的加宽残差编码器；(2) 3D深度可分离卷积；(3) 特异性驱动的正则化项；(4) 小尺度高斯权重初始化。此外，还通过两个后处理步骤进一步优化预测。", "result": "所提出的模型在Task-6验证排行榜上获得第一名，实现了以下病灶级别Dice分数：CC为0.759，ED为0.967，ET为0.826，NET为0.910，TC为0.928，WT为0.928。", "conclusion": "本研究提出的先进nnU-Net框架及其创新改进，在儿科脑肿瘤分割任务中表现出色，达到了领先水平，尤其是在处理有限数据和高变异性方面展现了有效性。"}}
{"id": "2511.00088", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00088", "abs": "https://arxiv.org/abs/2511.00088", "authors": ["NVIDIA", ":", "Yan Wang", "Wenjie Luo", "Junjie Bai", "Yulong Cao", "Tong Che", "Ke Chen", "Yuxiao Chen", "Jenna Diamond", "Yifan Ding", "Wenhao Ding", "Liang Feng", "Greg Heinrich", "Jack Huang", "Peter Karkus", "Boyi Li", "Pinyi Li", "Tsung-Yi Lin", "Dongran Liu", "Ming-Yu Liu", "Langechuan Liu", "Zhijian Liu", "Jason Lu", "Yunxiang Mao", "Pavlo Molchanov", "Lindsey Pavao", "Zhenghao Peng", "Mike Ranzinger", "Ed Schmerling", "Shida Shen", "Yunfei Shi", "Sarah Tariq", "Ran Tian", "Tilman Wekel", "Xinshuo Weng", "Tianjun Xiao", "Eric Yang", "Xiaodong Yang", "Yurong You", "Xiaohui Zeng", "Wenyuan Zhang", "Boris Ivanovic", "Marco Pavone"], "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "comment": null, "summary": "End-to-end architectures trained via imitation learning have advanced\nautonomous driving by scaling model size and data, yet performance remains\nbrittle in safety-critical long-tail scenarios where supervision is sparse and\ncausal understanding is limited. To address this, we introduce Alpamayo-R1\n(AR1), a vision-language-action model (VLA) that integrates Chain of Causation\nreasoning with trajectory planning to enhance decision-making in complex\ndriving scenarios. Our approach features three key innovations: (1) the Chain\nof Causation (CoC) dataset, built through a hybrid auto-labeling and\nhuman-in-the-loop pipeline producing decision-grounded, causally linked\nreasoning traces aligned with driving behaviors; (2) a modular VLA architecture\ncombining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI\napplications, with a diffusion-based trajectory decoder that generates\ndynamically feasible plans in real time; (3) a multi-stage training strategy\nusing supervised fine-tuning to elicit reasoning and reinforcement learning\n(RL) to optimize reasoning quality via large reasoning model feedback and\nenforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%\nimprovement in planning accuracy on challenging cases compared to a\ntrajectory-only baseline, with a 35% reduction in off-road rate and 25%\nreduction in close encounter rate in closed-loop simulation. RL post-training\nimproves reasoning quality by 45% as measured by a large reasoning model critic\nand reasoning-action consistency by 37%. Model scaling from 0.5B to 7B\nparameters shows consistent improvements. On-vehicle road tests confirm\nreal-time performance (99 ms latency) and successful urban deployment. By\nbridging interpretable reasoning with precise control, AR1 demonstrates a\npractical path towards Level 4 autonomous driving. We plan to release AR1\nmodels and a subset of the CoC in a future update.", "AI": {"tldr": "本文提出了Alpamayo-R1 (AR1)，一个视觉-语言-动作(VLA)模型，通过整合因果链推理和轨迹规划，显著提升了复杂自动驾驶场景中的决策能力和安全性，为L4级自动驾驶提供了实用路径。", "motivation": "现有的端到端模仿学习自动驾驶模型在安全关键的长尾场景中表现脆弱，因为这些场景缺乏监督数据且因果理解能力有限。", "method": "本文引入了Alpamayo-R1 (AR1)模型，包含三项关键创新：(1) 构建了因果链 (CoC) 数据集，通过混合自动标注和人工辅助流程生成与驾驶行为对齐的决策导向型因果推理轨迹；(2) 设计了模块化VLA架构，结合了预训练用于物理AI应用的视觉-语言模型Cosmos-Reason和基于扩散的实时轨迹解码器；(3) 采用了多阶段训练策略，先通过监督微调诱导推理能力，再利用强化学习（RL）通过大型推理模型反馈优化推理质量并强制推理-动作一致性。", "result": "评估结果显示，AR1在挑战性场景下规划精度比仅基于轨迹的基线提高了12%，在闭环仿真中越野率降低了35%，近距离接触率降低了25%。RL后训练将推理质量提高了45%（通过大型推理模型批评器衡量），推理-动作一致性提高了37%。模型从0.5B扩展到7B参数显示出持续的改进。车载路测证实了实时性能（99毫秒延迟）和成功的城市部署。", "conclusion": "AR1通过连接可解释推理与精确控制，展示了实现L4级自动驾驶的实用路径。作者计划未来发布AR1模型和部分CoC数据集。"}}
{"id": "2511.00021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00021", "abs": "https://arxiv.org/abs/2511.00021", "authors": ["Julio Jerison E. Macrohon", "Gordon Hung"], "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets", "comment": "15 pages, 10 figures", "summary": "Coral reefs support numerous marine organisms and are an important source of\ncoastal protection from storms and floods, representing a major part of marine\necosystems. However coral reefs face increasing threats from pollution, ocean\nacidification, and sea temperature anomalies, making efficient protection and\nmonitoring heavily urgent. Therefore, this study presents a novel\nmachine-learning-based coral bleaching classification system based on a diverse\nglobal dataset with samples of healthy and bleached corals under varying\nenvironmental conditions, including deep seas, marshes, and coastal zones. We\nbenchmarked and compared three state-of-the-art models: Residual Neural Network\n(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).\nAfter comprehensive hyperparameter tuning, the CNN model achieved the highest\naccuracy of 88%, outperforming existing benchmarks. Our findings offer\nimportant insights into autonomous coral monitoring and present a comprehensive\nanalysis of the most widely used computer vision models.", "AI": {"tldr": "本研究开发了一种基于机器学习的珊瑚白化分类系统，利用全球多样化数据集，并通过超参数调优发现CNN模型在珊瑚白化检测中表现最佳，准确率达88%，为自主珊瑚监测提供了新见解。", "motivation": "珊瑚礁对海洋生态系统至关重要，提供海岸保护并支持大量海洋生物。然而，它们正面临污染、海洋酸化和海温异常等日益严重的威胁，因此，高效的保护和监测变得极其紧迫。", "method": "研究构建了一个基于机器学习的珊瑚白化分类系统，使用了包含健康和白化珊瑚的全球多样化数据集，涵盖深海、沼泽和沿海等不同环境条件。研究对三种先进模型进行了基准测试和比较：残差神经网络（ResNet）、Vision Transformer（ViT）和卷积神经网络（CNN），并进行了全面的超参数调优。", "result": "经过全面的超参数调优，CNN模型取得了最高的准确率，达到88%，优于现有基准。", "conclusion": "本研究的发现为自主珊瑚监测提供了重要见解，并对最广泛使用的计算机视觉模型进行了全面分析。"}}
{"id": "2511.00180", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation.", "AI": {"tldr": "本文提出了一种残差流解码器框架，用于从语言模型激活中探测段落和文档级别的长期规划信息，并发现可以解码出相当于5个以上未来token的信息。", "motivation": "随着语言模型能够执行更长时间跨度的任务，现有的可解释性研究方法通常仅限于测试特定的概念或token，而无法理解模型如何编码长期规划信息。", "method": "开发了“残差流解码器”（Residual Stream Decoders）框架，作为一种探测模型激活的方法，以理解模型在段落和文档级别的规划。测试了几种解码方法。", "result": "在小型模型中，可以解码出相当于5个以上未来上下文token的信息。", "conclusion": "这些结果为更好地监控语言模型以及更好地理解它们如何编码长期规划信息奠定了基础。"}}
{"id": "2511.00010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "该研究引入了PlotCraft基准来评估大型语言模型（LLMs）在复杂数据可视化方面的能力，发现现有LLMs表现不足。为弥补这一差距，作者开发了SynthVis-30K数据集和小型高效模型PlotCraftor，后者在复杂可视化任务上表现出色，尤其在困难任务上性能提升超过50%。", "motivation": "尽管LLMs在代码生成方面表现出色，但它们创建复杂数据可视化的能力尚未得到充分评估和开发。现有LLMs在处理规模化和结构化数据的复杂可视化方面存在空白。", "method": "1. 引入PlotCraft基准：包含1000个挑战性可视化任务，覆盖金融、科研、社会学等领域，涵盖7种高级可视化任务和48种图表类型，系统评估单轮生成和多轮优化。2. 评估：对23个主流LLMs在PlotCraft上进行综合评估。3. 开发SynthVis-30K：一个通过协作代理框架合成的大规模、高质量复杂可视化代码数据集。4. 开发PlotCraftor：一个基于SynthVis-30K数据集构建的、体积小但能力强的代码生成模型。", "result": "1. 评估显示，现有LLMs在处理复杂可视化任务时存在明显的性能缺陷。2. PlotCraftor模型在VisEval、PandasPlotBench和PlotCraft等基准测试中，表现与领先的专有方法相当。3. 尤其在困难任务上，PlotCraftor的性能提升超过50%。", "conclusion": "大型语言模型在复杂数据可视化方面存在明显不足。通过引入专门的基准测试PlotCraft、构建大规模高质量数据集SynthVis-30K，并开发出小型高效的PlotCraftor模型，可以显著提升复杂数据可视化的生成能力，甚至超越现有领先的专有方法，尤其在处理困难任务时表现出卓越的性能改进。"}}
{"id": "2511.00039", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00039", "abs": "https://arxiv.org/abs/2511.00039", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "comment": null, "summary": "Dynamic pricing in retail requires policies that adapt to shifting demand\nwhile coordinating decisions across related products. We present a systematic\nempirical study of multi-agent reinforcement learning for retail price\noptimization, comparing a strong MAPPO baseline with a\ngraph-attention-augmented variant (MAPPO+GAT) that leverages learned\ninteractions among products. Using a simulated pricing environment derived from\nreal transaction data, we evaluate profit, stability across random seeds,\nfairness across products, and training efficiency under a standardized\nevaluation protocol. The results indicate that MAPPO provides a robust and\nreproducible foundation for portfolio-level price control, and that MAPPO+GAT\nfurther enhances performance by sharing information over the product graph\nwithout inducing excessive price volatility. These results indicate that\ngraph-integrated MARL provides a more scalable and stable solution than\nindependent learners for dynamic retail pricing, offering practical advantages\nin multi-product decision-making.", "AI": {"tldr": "本研究通过多智能体强化学习（MARL）系统地实证研究了零售动态定价优化问题，比较了MAPPO基线模型与结合图注意力网络（GAT）的MAPPO+GAT模型，发现后者通过利用产品间学习到的交互信息，在利润、稳定性、公平性和训练效率方面表现更优，提供了更具可扩展性和稳定性的解决方案。", "motivation": "零售动态定价需要政策能够适应不断变化的需求，并协调相关产品之间的决策。", "method": "研究采用多智能体强化学习（MARL）方法，比较了MAPPO基线模型和增强了图注意力网络（GAT）的MAPPO+GAT变体。模型在一个基于真实交易数据构建的模拟定价环境中进行训练和评估。评估指标包括利润、跨随机种子的稳定性、产品间的公平性以及训练效率。", "result": "结果表明，MAPPO为产品组合级别的价格控制提供了一个鲁棒且可复现的基础。MAPPO+GAT通过在产品图中共享信息，进一步提升了性能，且未引入过度的价格波动。", "conclusion": "研究得出结论，图集成多智能体强化学习（MARL）为动态零售定价提供了比独立学习器更具可扩展性和稳定性的解决方案，在多产品决策中具有实际优势。"}}
{"id": "2511.00291", "categories": ["eess.SY", "cs.NI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00291", "abs": "https://arxiv.org/abs/2511.00291", "authors": ["Christos Mavridis", "Fernando S. Barbosa", "Hamed Farhadi", "Karl H. Johansson"], "title": "Learning a Network Digital Twin as a Hybrid System", "comment": null, "summary": "Network digital twin (NDT) models are virtual models that replicate the\nbehavior of physical communication networks and are considered a key technology\ncomponent to enable novel features and capabilities in future 6G networks. In\nthis work, we focus on NDTs that model the communication quality properties of\na multi-cell, dynamically changing wireless network over a workspace populated\nwith multiple moving users. We propose an NDT modeled as a hybrid system, where\neach mode corresponds to a different base station and comprises sub-modes that\ncorrespond to areas of the workspace with similar network characteristics. The\nproposed hybrid NDT is identified and continuously improved through an\nannealing optimization-based learning algorithm, driven by online data\nmeasurements collected by the users. The advantages of the proposed hybrid NDT\nare studied with respect to memory and computational efficiency, data\nconsumption, and the ability to timely adapt to network changes. Finally, we\nvalidate the proposed methodology on real experimental data collected from a\ntwo-cell 5G testbed.", "AI": {"tldr": "本文提出了一种基于混合系统模型的网络数字孪生（NDT），用于模拟多小区动态无线网络的通信质量，并通过退火优化学习算法和在线数据进行持续改进，并在5G测试台上进行了验证。", "motivation": "网络数字孪生（NDT）被认为是未来6G网络实现新功能和能力的关键技术组件。这项研究旨在为多小区、动态变化的无线网络中，对具有多个移动用户的通信质量属性进行建模。", "method": "作者提出将NDT建模为一个混合系统，其中每个模式对应一个不同的基站，并包含对应于具有相似网络特征的工作区子模式。该混合NDT通过一种基于退火优化的学习算法进行识别和持续改进，该算法由用户收集的在线数据测量驱动。", "result": "研究了所提出的混合NDT在内存和计算效率、数据消耗以及及时适应网络变化方面的优势。该方法在从双小区5G测试平台收集的真实实验数据上得到了验证。", "conclusion": "该研究成功地提出并验证了一种基于混合系统的网络数字孪生方法，该方法能够高效、及时地模拟和适应动态无线网络的通信质量变化，并在真实5G环境中表现出良好的性能。"}}
{"id": "2511.00112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00112", "abs": "https://arxiv.org/abs/2511.00112", "authors": ["Yanbing Mao", "Yihao Cai", "Lui Sha"], "title": "Real-DRL: Teach and Learn in Reality", "comment": "37 pages", "summary": "This paper introduces the Real-DRL framework for safety-critical autonomous\nsystems, enabling runtime learning of a deep reinforcement learning (DRL) agent\nto develop safe and high-performance action policies in real plants (i.e., real\nphysical systems to be controlled), while prioritizing safety! The Real-DRL\nconsists of three interactive components: a DRL-Student, a PHY-Teacher, and a\nTrigger. The DRL-Student is a DRL agent that innovates in the dual\nself-learning and teaching-to-learn paradigm and the real-time safety-informed\nbatch sampling. On the other hand, PHY-Teacher is a physics-model-based design\nof action policies that focuses solely on safety-critical functions.\nPHY-Teacher is novel in its real-time patch for two key missions: i) fostering\nthe teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of\nreal plants. The Trigger manages the interaction between the DRL-Student and\nthe PHY-Teacher. Powered by the three interactive components, the Real-DRL can\neffectively address safety challenges that arise from the unknown unknowns and\nthe Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,\nii) automatic hierarchy learning (i.e., safety-first learning and then\nhigh-performance learning), and iii) safety-informed batch sampling to address\nthe learning experience imbalance caused by corner cases. Experiments with a\nreal quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole\nsystem, along with comparisons and ablation studies, demonstrate the Real-DRL's\neffectiveness and unique features.", "AI": {"tldr": "本文提出Real-DRL框架，用于在真实物理系统中实现安全关键型自主系统的运行时深度强化学习，优先保障安全并提升性能。", "motivation": "在安全关键型自主系统中，如何在真实物理环境下（即实际工厂）让DRL智能体学习到安全且高性能的动作策略，同时解决“未知未知”问题和Sim2Real（模拟到现实）差距带来的安全挑战。", "method": "Real-DRL框架包含三个交互组件：1) DRL-Student：一个DRL智能体，采用双重自学习和教学学习范式，以及实时安全知情批次采样。2) PHY-Teacher：基于物理模型设计的动作策略，专注于安全关键功能，提供实时补丁以促进教学学习并保障系统安全。3) Trigger：管理DRL-Student和PHY-Teacher之间的交互。该框架还具备安全保障、自动分层学习（先安全后高性能）和安全知情批次采样等特点。", "result": "Real-DRL框架通过在真实四足机器人、NVIDIA Isaac Gym中的四足机器人和倒立摆系统上的实验，以及对比和消融研究，证明了其有效性及独特功能，包括确保安全、自动分层学习和解决角点案例导致的学习经验不平衡。", "conclusion": "Real-DRL框架能够有效解决自主系统在真实物理环境中面临的安全挑战，通过创新的组件交互和学习机制，实现安全优先且高性能的运行时深度强化学习。"}}
{"id": "2511.00022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00022", "abs": "https://arxiv.org/abs/2511.00022", "authors": ["Jules Gerard", "Leandro Di Bella", "Filip Huyghe", "Marc Kochzius"], "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline", "comment": "Accepted to EUVIP2025, student session", "summary": "Coral reef monitoring in the Western Indian Ocean is limited by the labor\ndemands of underwater visual censuses. This work evaluates a YOLOv8-based deep\nlearning pipeline for automating family-level fish identification from video\ntransects collected in Kenya and Tanzania. A curated dataset of 24 families was\ntested under different configurations, providing the first region-specific\nbenchmark for automated reef fish monitoring in the Western Indian Ocean. The\nbest model achieved mAP@0.5 of 0.52, with high accuracy for abundant families\nbut weaker detection of rare or complex taxa. Results demonstrate the potential\nof deep learning as a scalable complement to traditional monitoring methods.", "AI": {"tldr": "本研究评估了基于YOLOv8的深度学习管道，用于从西印度洋地区的视频样带中自动化识别珊瑚礁鱼类家族，并建立了该区域的首个自动化监测基准。", "motivation": "西印度洋的珊瑚礁监测受限于传统水下视觉普查（UVC）所需的大量人力，因此需要更高效、可扩展的自动化方法。", "method": "研究采用基于YOLOv8的深度学习管道，利用在肯尼亚和坦桑尼亚收集的视频样带数据，构建了一个包含24个鱼类家族的精选数据集进行测试。", "result": "最佳模型在mAP@0.5上达到了0.52，对常见鱼类家族的识别准确率较高，但对稀有或复杂分类群的检测能力较弱。这是西印度洋地区自动化珊瑚礁鱼类监测的首次区域特定基准。", "conclusion": "结果表明，深度学习作为传统监测方法的可扩展补充，在珊瑚礁鱼类监测中具有巨大潜力。"}}
{"id": "2511.00048", "categories": ["cs.AI", "cs.CY", "62-11", "E.5; G.3; I.6.4; I.6.6; J.3; J.4"], "pdf": "https://arxiv.org/pdf/2511.00048", "abs": "https://arxiv.org/abs/2511.00048", "authors": ["Martin Bicher", "Maximilian Viehauser", "Daniele Giannandrea", "Hannah Kastinger", "Dominik Brunmeir", "Claire Rippinger", "Christoph Urach", "Niki Popper"], "title": "GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0", "comment": "134 pages, 75 figures, 19 tables", "summary": "GEPOC, short for Generic Population Concept, is a collection of models and\nmethods for analysing population-level research questions. For the valid\napplication of the models for a specific country or region, stable and\nreproducible data processes are necessary, which provide valid and ready-to-use\nmodel parameters. This work contains a complete description of the\ndata-processing methods for computation of model parameters for Austria, based\nexclusively on freely and publicly accessible data. In addition to the\ndescription of the source data used, this includes all algorithms used for\naggregation, disaggregation, fusion, cleansing or scaling of the data, as well\nas a description of the resulting parameter files. The document places\nparticular emphasis on the computation of parameters for the most important\nGEPOC model, GEPOC ABM, a continuous-time agent-based population model. An\nextensive validation study using this particular model was made and is\npresented at the end of this work.", "AI": {"tldr": "本文详细描述了为奥地利GEPOC模型（特别是GEPOC ABM）计算模型参数的数据处理方法，这些方法基于公开数据并涵盖了数据处理的各个环节，并进行了验证。", "motivation": "为确保GEPOC模型在特定国家或地区的有效应用，需要稳定、可复用的数据处理流程来提供有效且可直接使用的模型参数。本研究旨在为奥地利提供基于免费公开数据的此类流程。", "method": "本文描述了为奥地利计算GEPOC模型参数的完整数据处理方法。这包括：使用免费和公开的数据源；详细说明了数据聚合、分解、融合、清洗和缩放的算法；以及对所得参数文件的描述。特别强调了用于GEPOC ABM（一种连续时间基于Agent的人口模型）的参数计算。最后进行了广泛的验证研究。", "result": "本文提供了奥地利GEPOC模型参数计算的完整数据处理方法描述，涵盖了从数据源到参数文件生成的所有算法。特别是为GEPOC ABM模型计算了参数，并对结果进行了验证。", "conclusion": "成功开发并描述了为奥地利GEPOC模型，特别是GEPOC ABM模型，计算有效参数的完整数据处理方法，这些方法基于免费公开数据且经过验证，确保了模型在特定区域应用的有效性。"}}
{"id": "2511.00477", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00477", "abs": "https://arxiv.org/abs/2511.00477", "authors": ["Aditya Parikh", "Sneha Das", "Aasa Feragen"], "title": "Investigating Label Bias and Representational Sources of Age-Related Disparities in Medical Segmentation", "comment": "Submitted to ISBI 2026", "summary": "Algorithmic bias in medical imaging can perpetuate health disparities, yet\nits causes remain poorly understood in segmentation tasks. While fairness has\nbeen extensively studied in classification, segmentation remains underexplored\ndespite its clinical importance. In breast cancer segmentation, models exhibit\nsignificant performance disparities against younger patients, commonly\nattributed to physiological differences in breast density. We audit the\nMAMA-MIA dataset, establishing a quantitative baseline of age-related bias in\nits automated labels, and reveal a critical Biased Ruler effect where\nsystematically flawed labels for validation misrepresent a model's actual bias.\nHowever, whether this bias originates from lower-quality annotations (label\nbias) or from fundamentally more challenging image characteristics remains\nunclear. Through controlled experiments, we systematically refute hypotheses\nthat the bias stems from label quality sensitivity or quantitative case\ndifficulty imbalance. Balancing training data by difficulty fails to mitigate\nthe disparity, revealing that younger patient cases are intrinsically harder to\nlearn. We provide direct evidence that systemic bias is learned and amplified\nwhen training on biased, machine-generated labels, a critical finding for\nautomated annotation pipelines. This work introduces a systematic framework for\ndiagnosing algorithmic bias in medical segmentation and demonstrates that\nachieving fairness requires addressing qualitative distributional differences\nrather than merely balancing case counts.", "AI": {"tldr": "该研究发现医学图像分割（特别是乳腺癌）中存在与年龄相关的算法偏见，揭示了“偏尺效应”，并证明这种偏见并非主要源于标注质量或定量难度不平衡，而是因为年轻患者的病例本身更难学习，且偏见在训练中使用有偏见的机器生成标签时会被学习和放大。", "motivation": "医学影像中的算法偏见可能加剧健康不平等，但在分割任务中其原因尚不清楚。尽管公平性在分类任务中已被广泛研究，但分割任务（临床上很重要）仍未被充分探索。在乳腺癌分割中，模型对年轻患者表现出显著的性能差异，通常归因于乳腺密度的生理差异，但其根本原因仍不明确。", "method": "研究通过审计 MAMA-MIA 数据集，建立了年龄相关偏见的定量基线，并揭示了“偏尺效应”。通过受控实验，系统性地驳斥了偏见源于标注质量敏感性或定量病例难度不平衡的假设。通过难度平衡训练数据来尝试缓解差异，并提供了系统性偏见在训练中使用有偏见的机器生成标签时被学习和放大的直接证据。", "result": "研究建立了 MAMA-MIA 数据集中年龄相关偏见的定量基线，并发现了一个关键的“偏尺效应”，即验证用的系统性缺陷标签错误地代表了模型的实际偏见。通过受控实验，驳斥了偏见源于标注质量或定量病例难度不平衡的假设。结果表明，年轻患者的病例本质上更难学习，并且在有偏见的机器生成标签上进行训练时，系统性偏见会被学习和放大。", "conclusion": "该工作引入了一个诊断医学分割中算法偏见的系统框架，并证明实现公平性需要解决定性分布差异，而不仅仅是平衡病例数量。这一发现对自动化标注流程具有重要意义，表明系统性偏见是可学习和放大的。"}}
{"id": "2511.00548", "categories": ["eess.IV", "cs.CV", "cs.GR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00548", "abs": "https://arxiv.org/abs/2511.00548", "authors": ["Baochao Wang", "Xingyu Zhang", "Qingtao Zong", "Alim Pulatov", "Shuqi Shang", "Dongwei Wang"], "title": "Image-based ground distance detection for crop-residue-covered soil", "comment": "under review at Computers and Electronics in Agriculture", "summary": "Conservation agriculture features a soil surface covered with crop residues,\nwhich brings benefits of improving soil health and saving water. However, one\nsignificant challenge in conservation agriculture lies in precisely controlling\nthe seeding depth on the soil covered with crop residues. This is constrained\nby the lack of ground distance information, since current distance measurement\ntechniques, like laser, ultrasonic, or mechanical displacement sensors, are\nincapable of differentiating whether the distance information comes from the\nresidue or the soil. This paper presents an image-based method to get the\nground distance information for the crop-residues-covered soil. This method is\nperformed with 3D camera and RGB camera, obtaining depth image and color image\nat the same time. The color image is used to distinguish the different areas of\nresidues and soil and finally generates a mask image. The mask image is applied\nto the depth image so that only the soil area depth information can be used to\ncalculate the ground distance, and residue areas can be recognized and excluded\nfrom ground distance detection. Experimentation shows that this distance\nmeasurement method is feasible for real-time implementation, and the\nmeasurement error is within plus or minus 3mm. It can be applied in\nconservation agriculture machinery for precision depth seeding, as well as\nother depth-control-demanding applications like transplant or tillage.", "AI": {"tldr": "本文提出了一种基于图像的方法，利用3D和RGB相机区分作物残留物和土壤，从而实现精准测量覆盖作物残留物土壤的地面距离，以解决保护性耕作中播种深度控制不准确的问题。", "motivation": "保护性耕作中的精准播种深度控制面临挑战，现有距离测量技术（如激光、超声波或机械位移传感器）无法区分距离信息是来自作物残留物还是土壤，导致无法获取准确的地面距离信息。", "method": "该方法结合使用3D相机和RGB相机，同时获取深度图像和彩色图像。彩色图像用于区分残留物和土壤区域，生成掩膜图像。然后将掩膜图像应用于深度图像，仅使用土壤区域的深度信息来计算地面距离，并排除残留物区域。", "result": "实验结果表明，该距离测量方法具有实时实施的可行性，测量误差在±3毫米以内。", "conclusion": "该方法可应用于保护性耕作机械中的精准深度播种，以及其他需要精确深度控制的应用，如移栽或耕作。"}}
{"id": "2511.00296", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00296", "abs": "https://arxiv.org/abs/2511.00296", "authors": ["Peng Wang", "Zhengmao Li", "Luis Badesa"], "title": "Analyzing the Impact of Demand Response on Short-Circuit Current via a Unit Commitment Model", "comment": "1-5 pages. submitted to PESGM 2026, Canada", "summary": "In low-carbon grids, system flexibility can be enhanced through mechanisms\nsuch as Demand Response (DR), enabling the efficient utilization of renewable\nenergy. However, as Synchronous Generators (SGs) are being replaced with\nrenewable energy characterized by Inverter-Based Resources (IBR), system\nstability is severely affected. Due to the limited overload capability of IBR,\ntheir Short-Circuit Current (SCC) contribution is much smaller than that of\nSGs, which may result in protection devices failing to trip during faults.\nConsequently, the remaining SGs play a key role in offering sufficient SCC\nvolumes. Given that the commitment of SGs is closely related to system load, DR\ncan thus indirectly affect their SCC provision, a relationship that has not\nbeen investigated. Therefore, this paper incorporates both DR and SCC\nconstraints into a unit commitment model and conducts studies on an IEEE 30-bus\nsystem. The results show that although DR can reduce social costs by lowering\npower demand, it may also lead to inadequate SCC levels. Nevertheless, the cost\nincreases by only 0.3% when DR is combined with SCC constraints, indicating\nthat DR can actually help achieve a stable system in a cost-effective manner.", "AI": {"tldr": "本文研究了在低碳电网中，需求响应（DR）如何影响同步发电机（SG）的短路电流（SCC）供应，并将其与SCC约束纳入机组组合模型。结果显示，DR虽然能降低成本，但可能导致SCC不足，但结合SCC约束后，DR仍能以较低的成本实现系统稳定。", "motivation": "随着低碳电网中同步发电机（SG）被逆变器型资源（IBR）取代，系统稳定性受到严重影响，主要原因是IBR的短路电流（SCC）贡献远小于SG，可能导致保护装置故障时无法跳闸。剩余的SG在提供足够的SCC方面发挥关键作用。由于DR影响SG的承诺，进而可能间接影响SCC供应，这种关系尚未被研究。", "method": "将需求响应（DR）和短路电流（SCC）约束纳入机组组合模型中，并在IEEE 30节点系统上进行研究。", "result": "虽然需求响应（DR）可以通过降低电力需求来减少社会成本，但它也可能导致短路电流（SCC）水平不足。然而，当DR与SCC约束结合时，成本仅增加0.3%。", "conclusion": "需求响应（DR）可以以具有成本效益的方式帮助实现系统稳定，即使在考虑短路电流（SCC）约束的情况下，其成本增幅也很小。"}}
{"id": "2511.00115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling.", "AI": {"tldr": "ProtoMBTI是一个认知对齐的MBTI推断框架，它将原型理论融入基于大型语言模型的管道中，通过LLM增强语料库和LoRA微调编码器来学习原型，并在推断时使用检索-重用-修订-保留循环，显著提高了文本个性识别的准确性、可解释性和泛化能力。", "motivation": "传统的文本个性识别通常被视为硬标签分类问题，这掩盖了人类个性判断中分级、原型化的本质。", "method": "首先，通过LLM引导的多维度（语义、语言、情感）增强，构建了一个平衡且质量受控的语料库。其次，使用LoRA微调了一个轻量级（<=2B）编码器，以学习判别性嵌入并标准化一系列人格原型。在推断时，模型为查询文本检索top-k原型，并执行“检索-重用-修订-保留”循环：通过提示词投票聚合原型证据，在出现不一致时进行修订，并在预测正确后保留样本以持续丰富原型库。", "result": "ProtoMBTI在Kaggle和Pandora基准测试中，无论是对于MBTI的四个二分法还是完整的16种类型任务，都超越了基线模型，并表现出强大的跨数据集泛化能力。", "conclusion": "将推断过程与心理原型推理对齐，可以显著提高文本个性建模的准确性、可解释性和迁移能力。"}}
{"id": "2511.00198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies.", "AI": {"tldr": "本文提出通过预测信息丰富的token而非传统的下一token预测来训练大型语言模型（LLMs），以优化训练性能。", "motivation": "优化大型语言模型（LLMs）的训练性能是一个关键挑战，尤其是在提高模型性能的同时控制计算成本。传统训练方法（下一token预测，NTP）可能不是最有效的方式。", "method": "研究者提出一种通过预测信息丰富的token来训练LLMs的方法，并将其影响在算术、文本多标签分类和自然语言生成三类任务中进行调查。", "result": "通过这种方法，模型性能和对目标token选择策略的理论理解都得到了提升。", "conclusion": "该工作为优化LLM训练提供了一种原则性方法，通过目标token选择策略提高了模型性能和理论理解。"}}
{"id": "2511.00092", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research.", "AI": {"tldr": "本文介绍了QuantumBench，这是首个用于评估大型语言模型在量子科学领域理解和应用能力的基准测试数据集。", "motivation": "大型语言模型（LLMs）已广泛应用于科学工作流程，但通用基准测试往往无法准确反映其在量子科学等专业领域中捕捉特定知识和符号的能力，尤其是在量子科学这种非直观且需要高级数学的领域，这种差距尤为明显。", "method": "研究人员构建了QuantumBench，一个包含约800个多项选择题（每题8个选项）的数据集，涵盖量子科学的九个领域。这些问题及其答案均从公开可用材料中编译而来。利用此基准，研究人员评估了现有LLMs在量子领域的表现，并分析了它们对问题格式变化的敏感性。", "result": "QuantumBench是首个专为量子领域构建的LLM评估数据集。通过该基准，研究人员评估了多种现有LLMs在量子领域的性能，并分析了它们对问题格式变化的敏感性。", "conclusion": "QuantumBench旨在指导LLMs在量子研究中的有效应用，填补了LLMs在量子领域评估数据集的空白。"}}
{"id": "2511.00139", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00139", "abs": "https://arxiv.org/abs/2511.00139", "authors": ["Yu Cui", "Yujian Zhang", "Lina Tao", "Yang Li", "Xinyu Yi", "Zhibin Li"], "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection", "comment": null, "summary": "Achieving human-like dexterous manipulation remains a major challenge for\ngeneral-purpose robots. While Vision-Language-Action (VLA) models show\npotential in learning skills from demonstrations, their scalability is limited\nby scarce high-quality training data. Existing data collection methods face\ninherent constraints: manual teleoperation overloads human operators, while\nautomated planning often produces unnatural motions. We propose a Shared\nAutonomy framework that divides control between macro and micro motions. A\nhuman operator guides the robot's arm pose through intuitive VR teleoperation,\nwhile an autonomous DexGrasp-VLA policy handles fine-grained hand control using\nreal-time tactile and visual feedback. This division significantly reduces\ncognitive load and enables efficient collection of high-quality coordinated\narm-hand demonstrations. Using this data, we train an end-to-end VLA policy\nenhanced with our novel Arm-Hand Feature Enhancement module, which captures\nboth distinct and shared representations of macro and micro movements for more\nnatural coordination. Our Corrective Teleoperation system enables continuous\npolicy improvement through human-in-the-loop failure recovery. Experiments\ndemonstrate that our framework generates high-quality data with minimal\nmanpower and achieves a 90% success rate across diverse objects, including\nunseen instances. Comprehensive evaluations validate the system's effectiveness\nin developing dexterous manipulation capabilities.", "AI": {"tldr": "本文提出了一种共享自主框架，用于高效收集高质量的机械臂-手协调操作数据，并训练了一个增强的视觉-语言-动作（VLA）策略，以实现机器人类似人类的灵巧操作，在多样化物体上取得了90%的成功率。", "motivation": "通用机器人实现类人灵巧操作面临巨大挑战，主要受限于高质量训练数据的稀缺性。现有数据收集方法（如手动遥操作和自动化规划）各有局限，前者认知负荷高，后者动作不自然。", "method": "研究提出了一个共享自主框架，将控制分为宏观（人类操作VR引导机械臂姿态）和微观（自主DexGrasp-VLA策略处理精细手部控制，利用触觉和视觉反馈）。该框架用于高效收集高质量的机械臂-手协调演示数据。然后，利用这些数据训练一个端到端的VLA策略，并引入一个新颖的机械臂-手特征增强模块，以捕捉宏观和微观运动的独特及共享表示。此外，还设计了一个纠错遥操作系统，通过人机协作的故障恢复实现策略的持续改进。", "result": "该框架能够以最少的人力生成高质量数据，并在包括未见实例在内的多样化物体上实现了90%的成功率。全面的评估验证了该系统在开发灵巧操作能力方面的有效性。", "conclusion": "该研究通过共享自主的数据收集方法、增强的VLA策略以及人机协作的纠错机制，有效解决了机器人灵巧操作中的数据稀缺和协调性问题，显著提升了机器人的操作能力和成功率。"}}
{"id": "2511.00297", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00297", "abs": "https://arxiv.org/abs/2511.00297", "authors": ["Linhan Fang", "Xingpeng Li"], "title": "Optimal BESS Sizing and Placement for Mitigating EV-Induced Voltage Violations: A Scalable Spatio-Temporal Adaptive Targeting Strategy", "comment": null, "summary": "The escalating adoption of electric vehicles (EVs) and the growing demand for\ncharging solutions are driving a surge in EV charger installations in\ndistribution networks. However, this rising EV load strains the distribution\ngrid, causing severe voltage drops, particularly at feeder extremities. This\nstudy proposes a proactive voltage management (PVM) framework that can\nintegrate Monte Carlo-based simulations of varying EV charging loads to (i)\nidentify potential voltage violations through a voltage violation analysis\n(VVA) model, and (ii) then mitigate those violations with optimally-invested\nbattery energy storage systems (BESS) through an optimal expansion planning\n(OEP) model. A novel spatio-temporal adaptive targeting (STAT) strategy is\nproposed to alleviate the computational complexity of the OEP model by defining\na targeted OEP (T-OEP) model, solved by applying the OEP model to (i) a reduced\nset of representative critical time periods and (ii) candidate BESS\ninstallation nodes. The efficacy and scalability of the proposed approach are\nvalidated on 33-bus, 69-bus, and a large-scale 240-bus system. Results\ndemonstrate that the strategic sizing and placement of BESS not only\neffectively mitigate voltage violations but also yield substantial cost savings\non electricity purchases under time-of-use tariffs. This research offers a\ncost-effective and scalable solution for integrating high penetrations of EVs,\nproviding crucial insights for future distribution network planning.", "AI": {"tldr": "本研究提出了一种主动电压管理（PVM）框架，通过结合蒙特卡洛模拟、电压违规分析（VVA）和优化扩展规划（OEP），利用电池储能系统（BESS）来识别并缓解电动汽车（EV）充电负荷导致的电压骤降问题，并通过时空自适应目标（STAT）策略提高计算效率。", "motivation": "电动汽车的普及和充电需求的增长给配电网带来了巨大压力，导致电压严重下降，尤其是在馈线末端，因此需要有效的解决方案来管理这些电压问题。", "method": "该研究提出PVM框架，包含以下步骤：1) 使用蒙特卡洛模拟不同电动汽车充电负荷；2) 通过电压违规分析（VVA）模型识别潜在电压违规；3) 通过优化扩展规划（OEP）模型，利用最优投资的电池储能系统（BESS）来缓解这些违规。为降低OEP模型的计算复杂性，引入了时空自适应目标（STAT）策略，通过将OEP模型应用于一组精简的代表性关键时间段和候选BESS安装节点，定义了目标OEP（T-OEP）模型。该方法在33节点、69节点和240节点系统上进行了验证。", "result": "研究结果表明，战略性地确定BESS的规模和位置不仅能有效缓解电压违规，还能在分时电价下显著节省购电成本。", "conclusion": "该研究为整合高渗透率的电动汽车提供了一种经济高效且可扩展的解决方案，为未来的配电网规划提供了重要见解。"}}
{"id": "2511.00153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00153", "abs": "https://arxiv.org/abs/2511.00153", "authors": ["Justin Yu", "Yide Shentu", "Di Wu", "Pieter Abbeel", "Ken Goldberg", "Philipp Wu"], "title": "EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations", "comment": null, "summary": "Imitation learning from human demonstrations offers a promising approach for\nrobot skill acquisition, but egocentric human data introduces fundamental\nchallenges due to the embodiment gap. During manipulation, humans actively\ncoordinate head and hand movements, continuously reposition their viewpoint and\nuse pre-action visual fixation search strategies to locate relevant objects.\nThese behaviors create dynamic, task-driven head motions that static robot\nsensing systems cannot replicate, leading to a significant distribution shift\nthat degrades policy performance. We present EgoMI (Egocentric Manipulation\nInterface), a framework that captures synchronized end-effector and active head\ntrajectories during manipulation tasks, resulting in data that can be\nretargeted to compatible semi-humanoid robot embodiments. To handle rapid and\nwide-spanning head viewpoint changes, we introduce a memory-augmented policy\nthat selectively incorporates historical observations. We evaluate our approach\non a bimanual robot equipped with an actuated camera head and find that\npolicies with explicit head-motion modeling consistently outperform baseline\nmethods. Results suggest that coordinated hand-eye learning with EgoMI\neffectively bridges the human-robot embodiment gap for robust imitation\nlearning on semi-humanoid embodiments. Project page:\nhttps://egocentric-manipulation-interface.github.io", "AI": {"tldr": "该研究提出了EgoMI框架，通过捕捉人类主动头部运动和手部轨迹，并结合记忆增强策略，有效解决了模仿学习中人类与机器人之间的具身差距，从而在半人形机器人上实现了更鲁棒的模仿学习。", "motivation": "从人类示范中进行模仿学习时，由于人类与机器人之间存在具身差距（embodiment gap），特别是人类主动的头部和手部协调运动、视点调整以及预动作视觉注视策略，导致静态机器人感知系统无法复制这些动态行为，进而产生显著的分布偏移，影响策略性能。", "method": "研究提出了EgoMI（Egocentric Manipulation Interface）框架，用于同步捕捉操作任务中的末端执行器和主动头部轨迹。为处理快速且宽范围的头部视点变化，引入了一种记忆增强策略，选择性地整合历史观测数据。捕获的数据可重新定向到兼容的半人形机器人具身。", "result": "在配备有可驱动摄像头头部的双臂机器人上进行评估，结果表明，明确建模头部运动的策略始终优于基线方法。", "conclusion": "EgoMI框架下的协调手眼学习有效弥合了人类与机器人之间的具身差距，为半人形机器人上的鲁棒模仿学习提供了有效途径。"}}
{"id": "2511.00037", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.00037", "abs": "https://arxiv.org/abs/2511.00037", "authors": ["Riya Gupta", "Alexander Chowdhury", "Sahil Nalawade"], "title": "Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra", "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in medical\nAI, enabling collaborative model training across institutions without direct\ndata sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,\nFlower, and Owkin Substra to evaluate their suitability for medical imaging\napplications in real-world settings. Using the PathMNIST dataset, we assess\nmodel performance, convergence efficiency, communication overhead, scalability,\nand developer experience. Results indicate that NVIDIA FLARE offers superior\nproduction scalability, Flower provides flexibility for prototyping and\nacademic research, and Owkin Substra demonstrates exceptional privacy and\ncompliance features. Each framework exhibits strengths optimized for distinct\nuse cases, emphasizing their relevance to practical deployment in healthcare\nenvironments.", "AI": {"tldr": "该研究基准测试了NVIDIA FLARE、Flower和Owkin Substra三个联邦学习框架在医学影像应用中的表现，发现它们各自在生产规模、原型开发和隐私合规方面具有优势。", "motivation": "联邦学习在医疗AI领域具有变革性潜力，但需要评估现有主流框架在真实世界医学影像应用中的适用性。", "method": "研究使用了PathMNIST数据集，对NVIDIA FLARE、Flower和Owkin Substra三个联邦学习框架进行了基准测试，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。", "result": "NVIDIA FLARE在生产可扩展性方面表现出色，Flower在原型设计和学术研究中提供灵活性，而Owkin Substra则展示了卓越的隐私和合规性功能。", "conclusion": "每个联邦学习框架都有其独特的优势，适用于医疗保健环境中不同的实际部署场景。"}}
{"id": "2511.00222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users.", "AI": {"tldr": "该研究提出了一个统一框架，用于评估和改进大型语言模型（LLMs）在模拟人类用户对话中的角色一致性。通过定义三个自动一致性指标，并将其作为奖励信号，使用多轮强化学习微调LLMs，显著降低了角色不一致性。", "motivation": "在治疗、教育和社交角色扮演等互动场景中，LLMs越来越多地被用于模拟人类用户。然而，现有的LLMs常偏离其设定的角色，前后矛盾，或放弃角色应有的行为，导致模拟效果不佳。", "method": "研究引入了三个自动指标来衡量角色一致性：提示到对话行一致性、对话行之间的一致性以及问答一致性，并对照人工标注进行了验证。随后，将这些指标作为奖励信号，应用多轮强化学习对LLMs进行微调，使其适应患者、学生和社交聊天伙伴三种用户角色。", "result": "该方法将LLM生成对话中的不一致性降低了超过55%，从而产生了更连贯和忠实于角色的模拟用户。", "conclusion": "该研究提供了一个有效的框架，能够显著提高LLMs在模拟人类用户时保持角色一致性的能力，使其在互动设置中表现更佳。"}}
{"id": "2511.00598", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00598", "abs": "https://arxiv.org/abs/2511.00598", "authors": ["Zixuan Sun", "Shuaifeng Zhi", "Ruize Li", "Jingyuan Xia", "Yongxiang Liu", "Weidong Jiang"], "title": "GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations", "comment": "To be published in IEEE Transactions on Geoscience and Remote Sensing\n  (T-GRS) 2025", "summary": "Registration of optical and synthetic aperture radar (SAR) remote sensing\nimages serves as a critical foundation for image fusion and visual navigation\ntasks. This task is particularly challenging because of their modal\ndiscrepancy, primarily manifested as severe nonlinear radiometric differences\n(NRD), geometric distortions, and noise variations. Under large geometric\ntransformations, existing classical template-based and sparse keypoint-based\nstrategies struggle to achieve reliable registration results for optical-SAR\nimage pairs. To address these limitations, we propose GDROS, a geometry-guided\ndense registration framework leveraging global cross-modal image interactions.\nFirst, we extract cross-modal deep features from optical and SAR images through\na CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D\ncorrelation volume is constructed and iteratively refined to establish\npixel-wise dense correspondences. Subsequently, we implement a least squares\nregression (LSR) module to geometrically constrain the predicted dense optical\nflow field. Such geometry guidance mitigates prediction divergence by directly\nimposing an estimated affine transformation on the final flow predictions.\nExtensive experiments have been conducted on three representative datasets\nWHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial\nresolutions, demonstrating robust performance of our proposed method across\ndifferent imaging resolutions. Qualitative and quantitative results show that\nGDROS significantly outperforms current state-of-the-art methods in all\nmetrics. Our source code will be released at:\nhttps://github.com/Zi-Xuan-Sun/GDROS.", "AI": {"tldr": "本文提出GDROS，一个几何引导的稠密配准框架，用于解决光学与SAR遥感图像之间由于模态差异导致的配准难题。该方法结合CNN-Transformer特征提取、多尺度4D相关体以及最小二乘回归几何约束，在多个数据集上显著优于现有SOTA方法。", "motivation": "光学与SAR遥感图像的配准是图像融合和视觉导航的基础，但由于严重的非线性辐射差异、几何畸变和噪声变化，这项任务极具挑战性。在大的几何变换下，现有方法难以实现可靠的配准结果。", "method": "GDROS框架首先通过CNN-Transformer混合特征提取模块从光学和SAR图像中提取跨模态深度特征，然后构建并迭代优化多尺度4D相关体以建立像素级稠密对应。随后，通过最小二乘回归(LSR)模块对预测的稠密光流场施加几何约束，以估计仿射变换来指导最终流预测，从而减轻预测发散。", "result": "在WHU-Opt-SAR、OS和UBCv2三个代表性数据集上进行了广泛实验，GDROS在不同空间分辨率下均表现出鲁棒性能。定性和定量结果表明，GDROS在所有指标上都显著优于当前的最新方法。", "conclusion": "GDROS通过利用全局跨模态图像交互和几何引导，有效解决了光学-SAR图像配准的挑战，实现了卓越的性能，为图像融合和视觉导航提供了坚实基础。"}}
{"id": "2511.00028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00028", "abs": "https://arxiv.org/abs/2511.00028", "authors": ["Hanyang Chen", "Yanchao Yang"], "title": "Mutual Information guided Visual Contrastive Learning", "comment": "Tech Report - Undergraduate Thesis - 2023", "summary": "Representation learning methods utilizing the InfoNCE loss have demonstrated\nconsiderable capacity in reducing human annotation effort by training invariant\nneural feature extractors. Although different variants of the training\nobjective adhere to the information maximization principle between the data and\nlearned features, data selection and augmentation still rely on human\nhypotheses or engineering, which may be suboptimal. For instance, data\naugmentation in contrastive learning primarily focuses on color jittering,\naiming to emulate real-world illumination changes. In this work, we investigate\nthe potential of selecting training data based on their mutual information\ncomputed from real-world distributions, which, in principle, should endow the\nlearned features with better generalization when applied in open environments.\nSpecifically, we consider patches attached to scenes that exhibit high mutual\ninformation under natural perturbations, such as color changes and motion, as\npositive samples for learning with contrastive loss. We evaluate the proposed\nmutual-information-informed data augmentation method on several benchmarks\nacross multiple state-of-the-art representation learning frameworks,\ndemonstrating its effectiveness and establishing it as a promising direction\nfor future research.", "AI": {"tldr": "本文提出一种基于互信息的数据增强方法，通过选择在自然扰动下具有高互信息的训练样本（如图像块），来优化InfoNCE损失下的表征学习，以提高模型在开放环境中的泛化能力。", "motivation": "InfoNCE损失的表征学习方法在减少人工标注方面表现出色，但数据选择和增强仍依赖于人工假设或工程，可能不是最优的。例如，对比学习中的数据增强主要集中于颜色抖动。作者认为基于真实世界分布计算的互信息来选择训练数据，有望赋予学习到的特征更好的泛化能力。", "method": "研究人员提出了一种互信息引导的数据增强方法。具体而言，他们将场景中在自然扰动（如颜色变化和运动）下表现出高互信息的图像块视为正样本，用于对比损失的学习。", "result": "该方法在多个基准测试和最先进的表征学习框架上进行了评估，结果表明其有效性，并被确认为未来研究的一个有前景的方向。", "conclusion": "通过利用真实世界分布的互信息来指导训练数据的选择和增强，可以有效提高基于InfoNCE的表征学习模型的泛化能力，尤其是在开放环境中。这种互信息引导的数据增强是一个值得进一步探索的方向。"}}
{"id": "2511.00652", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00652", "abs": "https://arxiv.org/abs/2511.00652", "authors": ["Ali Khalid", "Jaiaid Mobin", "Sumanth Rao Appala", "Avinash Maurya", "Stephany Berrio Perez", "M. Mustafa Rafique", "Fawad Ahmad"], "title": "Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars", "comment": null, "summary": "An autonomous vehicle can generate several terabytes of sensor data per day.\nA significant portion of this data consists of 3D point clouds produced by\ndepth sensors such as LiDARs. This data must be transferred to cloud storage,\nwhere it is utilized for training machine learning models or conducting\nanalyses, such as forensic investigations in the event of an accident. To\nreduce network and storage costs, this paper introduces DejaView. Although\nprior work uses interframe redundancies to compress data, DejaView searches for\nand uses redundancies on larger temporal scales (days and months) for more\neffective compression. We designed DejaView with the insight that the operating\narea of autonomous vehicles is limited and that vehicles mostly traverse the\nsame routes daily. Consequently, the 3D data they collect daily is likely\nsimilar to the data they have captured in the past. To capture this, the core\nof DejaView is a diff operation that compactly represents point clouds as delta\nw.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end\nimplementation of DejaView can compress point clouds by a factor of 210 at a\nreconstruction error of only 15 cm.", "AI": {"tldr": "DejaView是一种利用长期时间冗余（数天至数月）来高效压缩自动驾驶车辆LiDAR点云数据的方法，显著降低了网络和存储成本。", "motivation": "自动驾驶车辆每天产生大量（数TB）传感器数据，其中大部分是LiDAR生成的3D点云。这些数据需要传输到云端进行模型训练或事故分析，但高昂的网络和存储成本是一个主要问题。", "method": "DejaView的核心思想是自动驾驶车辆的运行区域有限且经常沿相同路线行驶，因此每天收集的3D数据很可能与过去捕获的数据相似。它通过一种“diff”操作，将新的点云数据紧凑地表示为相对于过去3D数据的增量（delta）。", "result": "使用两个月的LiDAR数据，DejaView的端到端实现可以将点云压缩210倍，同时重建误差仅为15厘米。", "conclusion": "DejaView通过利用自动驾驶车辆数据中存在的长期时间冗余，实现了对LiDAR点云数据的高效压缩，有效降低了数据传输和存储成本。"}}
{"id": "2511.00122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy.", "AI": {"tldr": "本文提出了Engineering.ai平台，一个用于计算设计中AI工程师团队的协作框架。它采用分层多智能体架构，通过LLM驱动的专业智能体进行文件介导的通信和全面的记忆系统，实现了自主、多学科的工程设计，并在无人机机翼优化中取得了100%的成功率。", "motivation": "现代工程实践中，人类工程师团队协作设计复杂产品虽然有效，但耗时且成本高昂。为了解决这一问题，并基于OpenFOAMGPT和turbulence.ai等早期AI工程师的成功经验，研究旨在开发一个能够自主执行复杂工程任务的AI平台。", "method": "该框架采用分层多智能体架构，由一名首席工程师协调多个专业智能体（如空气动力学、结构、声学、优化工程师），每个智能体都由具备领域特定知识的LLM驱动。智能体间通过文件介导通信实现数据溯源和可复现性。一个全面的记忆系统维护项目上下文、执行历史和检索增强的领域知识。系统集成了FreeCAD、Gmsh、OpenFOAM、CalculiX和BPM声学分析等工具，支持并行多学科仿真。", "result": "该框架通过无人机机翼优化进行了验证。在超过400种参数配置中，自动化工作流程取得了100%的成功率，没有出现网格生成失败、求解器收敛问题或需要人工干预的情况。", "conclusion": "研究表明，由智能体AI驱动的AI工程师有潜力自主执行复杂的工程任务。该框架的100%成功率和零故障率证明了其可靠性和可信赖性。"}}
{"id": "2511.00337", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00337", "abs": "https://arxiv.org/abs/2511.00337", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Large Language Models for Control", "comment": null, "summary": "This paper investigates using large language models (LLMs) to generate\ncontrol actions directly, without requiring control-engineering expertise or\nhand-tuned algorithms. We implement several variants: (i) prompt-only, (ii)\ntool-assisted with access to historical data, and (iii) prediction-assisted\nusing learned or simple models to score candidate actions. We compare them on\ntracking accuracy and actuation effort, with and without a prompt that requests\nlower actuator usage. Results show prompt-only LLMs already produce viable\ncontrol, while tool-augmented versions adapt better to changing objectives but\ncan be more sensitive to constraints, supporting LLM-in-the-loop control for\nevolving cyber-physical systems today and operator and human inputs.", "AI": {"tldr": "本文探讨了直接使用大型语言模型（LLMs）生成控制动作的可能性，无需专业的控制工程知识或手动调优算法。", "motivation": "研究动机是消除对控制工程专业知识和手动调优算法的需求，使控制系统设计更易于访问和自动化。", "method": "研究实现了三种LLM变体：(i) 仅提示（prompt-only），(ii) 工具辅助（可访问历史数据），以及(iii) 预测辅助（使用学习模型或简单模型评估候选动作）。通过跟踪精度和执行器使用量进行比较，并测试了是否包含降低执行器使用量的提示。", "result": "结果显示，仅提示的LLM已能产生可行的控制。工具增强版本能更好地适应变化的目标，但可能对约束更敏感。", "conclusion": "研究支持LLM在循环控制（LLM-in-the-loop control）中应用于不断发展的网络物理系统，并能支持操作员和人类输入。"}}
{"id": "2511.00046", "categories": ["cs.CV", "68U10, 94A08", "I.4.3; I.4.4; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2511.00046", "abs": "https://arxiv.org/abs/2511.00046", "authors": ["Rupjyoti Chutia", "Dibya Jyoti Bora"], "title": "Enhancing rice leaf images: An overview of image denoising techniques", "comment": "18 pages, 6 figures. Research Article published in the International\n  Journal of Agricultural and Natural Sciences (IJANS), Vol. 18, Issue 2, 2025.\n  This paper presents a comparative study of image denoising and CLAHE\n  techniques for enhancing rice leaf images corrupted by Gaussian,\n  Salt-and-pepper, Speckle, and Random noise for agricultural analysis", "summary": "Digital image processing involves the systematic handling of images using\nadvanced computer algorithms, and has gained significant attention in both\nacademic and practical fields. Image enhancement is a crucial preprocessing\nstage in the image-processing chain, improving image quality and emphasizing\nfeatures. This makes subsequent tasks (segmentation, feature extraction,\nclassification) more reliable. Image enhancement is essential for rice leaf\nanalysis, aiding in disease detection, nutrient deficiency evaluation, and\ngrowth analysis. Denoising followed by contrast enhancement are the primary\nsteps. Image filters, generally employed for denoising, transform or enhance\nvisual characteristics like brightness, contrast, and sharpness, playing a\ncrucial role in improving overall image quality and enabling the extraction of\nuseful information. This work provides an extensive comparative study of\nwell-known image-denoising methods combined with CLAHE (Contrast Limited\nAdaptive Histogram Equalization) for efficient denoising of rice leaf images.\nThe experiments were performed on a rice leaf image dataset to ensure the data\nis relevant and representative. Results were examined using various metrics to\ncomprehensively test enhancement methods. This approach provides a strong basis\nfor assessing the effectiveness of methodologies in digital image processing\nand reveals insights useful for future adaptation in agricultural research and\nother domains.", "AI": {"tldr": "本文对多种图像去噪方法与CLAHE结合进行水稻叶片图像增强进行了广泛的比较研究，旨在提高图像质量并为农业研究提供见解。", "motivation": "图像增强是数字图像处理中的关键预处理步骤，能提高图像质量并强调特征，使后续任务（如分割、特征提取、分类）更可靠。对于水稻叶片分析，图像增强对于疾病检测、营养缺乏评估和生长分析至关重要。", "method": "本研究对知名图像去噪方法与CLAHE（对比度受限自适应直方图均衡化）相结合的方案进行了广泛的比较研究，用于水稻叶片图像的有效去噪。实验在一个水稻叶片图像数据集上进行，并使用多种指标评估结果。", "result": "研究结果通过各种指标全面测试了增强方法的有效性，为评估数字图像处理中的方法论提供了坚实基础，并揭示了对农业研究及其他领域未来适应有用的见解。", "conclusion": "该研究提供了一个评估数字图像处理方法有效性的强大基础，并为农业研究及其他领域的未来应用提供了有价值的见解。"}}
{"id": "2511.00265", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00265", "abs": "https://arxiv.org/abs/2511.00265", "authors": ["Arman Anwar", "Zefang Liu"], "title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "comment": null, "summary": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training\nbut are often scripted, resource-intensive, and difficult to scale. We\nintroduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches\ngame that integrates large language model teammates with a Bloom-aligned,\nretrieval-augmented copilot (C2D2). The system expands a curated corpus into\nfactual, conceptual, procedural, and metacognitive snippets, delivering\non-demand, cognitively targeted hints. Prompt-engineered agents employ a\nscaffolding ladder that gradually fades as learner confidence grows. In a\nsolo-player pilot with four graduate students, participants reported greater\nintention to use the agent-based version compared to the physical card deck and\nviewed it as more scalable, though a ceiling effect emerged on a simple\nknowledge quiz. Despite limitations of small sample size, single-player focus,\nand narrow corpus, these early findings suggest that large language model\naugmented TTXs can provide lightweight, repeatable practice without the\nlogistical burden of traditional exercises. Planned extensions include\nmulti-player modes, telemetry-driven coaching, and comparative studies with\nlarger cohorts.", "AI": {"tldr": "本文介绍AgentBnB，一个将大型语言模型（LLM）集成到网络安全桌面演练（TTX）中的浏览器游戏，旨在提供可扩展、低成本的训练，并初步显示出优于传统方法的潜力。", "motivation": "传统的网络安全桌面演练（TTX）虽然有价值，但通常是预设剧本、资源密集且难以扩展。", "method": "研究者开发了AgentBnB，一个基于浏览器的Backdoors & Breaches游戏重构版。它集成了LLM队友和一个与布鲁姆分类法对齐的检索增强型（RAG）辅助系统（C2D2），该系统能从精选语料库中提供认知导向的提示。提示工程（prompt-engineered）代理采用一种逐渐淡出的支架式学习方法。", "result": "在一个四名研究生的单人飞行测试中，参与者表示更倾向于使用基于代理的版本而非实体卡牌，并认为它更具可扩展性。尽管在简单的知识测验中出现了天花板效应，但早期发现表明LLM增强的TTX可以提供轻量级、可重复的练习。", "conclusion": "尽管存在样本量小、单人模式和语料库狭窄的局限性，但研究结果表明，大型语言模型增强的桌面演练可以提供轻量化、可重复的练习，且无需传统演练的后勤负担。"}}
{"id": "2511.00194", "categories": ["cs.AI", "F.2.2, F.4.1"], "pdf": "https://arxiv.org/pdf/2511.00194", "abs": "https://arxiv.org/abs/2511.00194", "authors": ["Jovial Cheukam Ngouonou", "Ramiz Gindullin", "Claude-Guy Quimper", "Nicolas Beldiceanu", "Remi Douence"], "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "comment": null, "summary": "We present an improved incremental selection algorithm of the selection\nalgorithm presented in [1] and prove all the selected conjectures.", "AI": {"tldr": "本文提出了一种改进的增量选择算法，并证明了所有相关的猜想。", "motivation": "原始选择算法（如参考文献[1]所述）可能存在改进空间，且相关的猜想需要被正式证明。", "method": "通过提出一种“改进的增量选择算法”来优化现有方法，并通过数学证明或形式验证来“证明所有选定的猜想”。", "result": "成功地开发出了一种优于现有技术的增量选择算法，并为之前未经验证的猜想提供了确凿的证明。", "conclusion": "本研究不仅提升了现有选择算法的性能，还为相关理论猜想提供了坚实的数学基础。"}}
{"id": "2511.00162", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship.", "AI": {"tldr": "本文介绍了ARC-GEN，一个开源的程序生成器，旨在忠实地扩展原有的ARC-AGI训练数据集，以解决其示范集基数小的问题，并可用于建立基准测试套件。", "motivation": "抽象推理语料库（ARC-AGI）是评估通用人工智能进展的挑战性基准，但其示范集数量有限（每任务只有少量输入/输出对），这限制了机器学习系统学习效率的提升，而学习效率是其目前所缺乏的特质。", "method": "本文引入了ARC-GEN，一个开源的程序生成器，用于扩展原始的ARC-AGI训练数据集。该生成器是详尽的（覆盖所有四百个任务），并且是模仿性的（更忠实地遵循了ARC-AGI-1版本中体现的分布特性和特征）。", "result": "ARC-GEN成功地扩展了ARC-AGI训练数据集，为解决数据稀缺问题提供了途径。此外，该生成器还被用于建立一个静态基准测试套件，以验证提交给2025年Google Code Golf Championship的程序的正确性。", "conclusion": "ARC-GEN通过生成忠实且详尽的额外样本对，有效缓解了ARC-AGI数据集的样本稀缺问题，有望促进通用人工智能的学习效率提升，并已在实际应用中作为编程竞赛的基准测试工具。"}}
{"id": "2511.00259", "categories": ["cs.RO", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00259", "abs": "https://arxiv.org/abs/2511.00259", "authors": ["Andria J. Farrens", "Luis Garcia-Fernandez", "Raymond Diaz Rojas", "Jillian Obeso Estrada", "Dylan Reinsdorf", "Vicky Chan", "Disha Gupta", "Joel Perry", "Eric Wolbrecht", "An Do", "Steven C. Cramer", "David J. Reinkensmeyer"], "title": "Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial", "comment": "Main manuscript: 38 pages (double spaced, with references), 6\n  figures, 2 tables and collated supplemental materials (17 pages, double\n  spaced)", "summary": "Precision rehabilitation aims to tailor movement training to improve\noutcomes. We tested whether proprioceptively-tailored robotic training improves\nhand function and neural processing in stroke survivors. Using a robotic finger\nexoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel\nTraining, which uses robot-facilitated, gamified movements to enhance\nproprioceptive processing, and Virtual Assistance Training, which reduces\nrobotic aid to increase reliance on self-generated feedback. In a randomized\ncontrolled trial, forty-six chronic stroke survivors completed nine 2-hour\nsessions of Standard, Propriopixel or Virtual training. Among participants with\nproprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)\nand Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand\nfunction (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with\nimprovements in hand function. Tailored training enhanced neural sensitivity to\nproprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive\nContingent Negative Variation. These findings support proprioceptively-tailored\ntraining as a pathway to precision neurorehabilitation.", "AI": {"tldr": "本研究发现，针对本体感觉定制的机器人训练能有效改善中风幸存者的手部功能和神经处理能力，尤其对于存在本体感觉缺陷的患者。", "motivation": "精准康复旨在通过定制化的运动训练来改善治疗效果。本研究旨在验证针对本体感觉定制的机器人训练是否能改善中风幸存者的手部功能和神经处理能力。", "method": "一项随机对照试验，纳入46名慢性中风幸存者。参与者分为三组：标准训练组、Propriopixel训练组（机器人辅助的、游戏化的运动以增强本体感觉处理）和虚拟辅助训练组（减少机器人辅助以增加对自我生成反馈的依赖）。每组完成9次，每次2小时的训练。通过Box and Block Test评估手部功能，并通过一种新型脑电图（EEG）生物标志物（本体感觉条件负变，proprioceptive Contingent Negative Variation）评估神经敏感性。", "result": "对于存在本体感觉缺陷的参与者，Propriopixel训练组（手部功能改善7 ± 4.2块，p=0.002）和虚拟辅助训练组（4.5 ± 4.4块，p=0.068）比标准训练组（0.8 ± 2.3块）在手部功能上取得了更大的进步。本体感觉的改善与手部功能的提高呈正相关。定制化训练增强了对本体感觉线索的神经敏感性，这通过脑电图生物标志物得到了证实。", "conclusion": "这些发现支持本体感觉定制化训练作为实现精准神经康复的一种有效途径。"}}
{"id": "2511.00420", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00420", "abs": "https://arxiv.org/abs/2511.00420", "authors": ["Ali Taghavian", "Ali Safi", "Esmaeel Khanmirza"], "title": "Constrained computational hybrid controller for Input Affine Hybrid Dynamical Systems", "comment": null, "summary": "Hybrid dynamical systems are viewed as the most complicated systems with\ncontinuous and event-based behaviors. Since traditional controllers cannot\nhandle these systems, some newly-developed controllers have been published in\nrecent decades to deal with them. This paper presents a novel implementable\nconstrained final-state controller based on partitioning the system's\nstate-space, computational simulations, and graph theory. Experimental results\nand a comparison with Model Predictive Controller on the three tank benchmark\nand swing-up control of a pendulum show the effectiveness of the proposed\nComputational Hybrid Controller(CHC).", "AI": {"tldr": "本文提出了一种基于状态空间划分、计算仿真和图论的计算混合控制器（CHC），用于解决混合动力系统的控制问题，并通过实验验证了其有效性。", "motivation": "混合动力系统因其连续和事件驱动行为而被认为是复杂的系统，传统控制器难以有效处理。因此，需要开发新的控制器来应对这些挑战。", "method": "本文提出了一种新颖的可实现的约束终态控制器，该控制器基于系统状态空间划分、计算仿真和图论。", "result": "在三罐基准系统和摆的摆起控制上的实验结果以及与模型预测控制器的比较表明，所提出的计算混合控制器（CHC）是有效的。", "conclusion": "所提出的计算混合控制器（CHC）能够有效处理混合动力系统，并在实验中展现出优于或至少与模型预测控制器相当的性能。"}}
{"id": "2511.00193", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00193", "abs": "https://arxiv.org/abs/2511.00193", "authors": ["Faranak Akbarifar", "Nooshin Maghsoodi", "Sean P Dukelow", "Stephen Scott", "Parvin Mousavi"], "title": "Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach", "comment": null, "summary": "Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive\nkinematic biomarkers but requires 40-64 reaches, imposing time and fatigue\nburdens. We evaluate whether time-series foundation models can replace\nunrecorded trials from an early subset of reaches while preserving the\nreliability of standard Kinarm parameters.\n  Methods: We analyzed VGR speed signals from 461 stroke and 599 control\nparticipants across 4- and 8-target reaching protocols. We withheld all but the\nfirst 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,\nfine-tuned on 70 percent of subjects, to forecast synthetic trials. We\nrecomputed four kinematic features of reaching (reaction time, movement time,\nposture speed, maximum speed) on combined recorded plus forecasted trials and\ncompared them to full-length references using ICC(2,1).\n  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only\n8 recorded trials plus forecasts, matching the reliability of 24-28 recorded\nreaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA\nimprovements were minimal. Across cohorts and protocols, synthetic trials\nreplaced reaches without materially compromising feature reliability.\n  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR\nassessment time. For the most impaired stroke survivors, sessions drop from 4-5\nminutes to about 1 minute while preserving kinematic precision. This\nforecast-augmented paradigm promises efficient robotic evaluations for\nassessing motor impairments following stroke.", "AI": {"tldr": "本研究评估了时间序列基础模型（如Chronos）是否能通过预测未记录的试验，显著缩短Kinarm机器人视觉引导式抓取（VGR）评估时间，同时保持运动学参数的可靠性。", "motivation": "Kinarm VGR评估能提供敏感的运动学生物标志物，但需要进行40-64次抓取，耗时且易导致疲劳，给参与者带来负担。", "method": "研究分析了461名中风患者和599名对照组参与者的VGR速度信号。他们仅保留了前8或16次抓取试验，并使用ARIMA、MOMENT和Chronos模型（在70%的受试者数据上进行微调）来预测合成试验。然后，将记录的试验与预测的试验结合，重新计算了反应时间、运动时间、姿势速度和最大速度这四个运动学特征，并使用ICC(2,1)与完整试验的参考值进行比较。", "result": "Chronos模型预测仅使用8次记录试验加上预测，就使所有参数的ICC值恢复到0.90以上，其可靠性与24-28次记录试验相当（Delta ICC <= 0.07）。MOMENT模型取得了中等程度的提升，而ARIMA模型的改进最小。在不同队列和协议中，合成试验替代了部分抓取，而未实质性损害特征可靠性。", "conclusion": "基础模型预测能大大缩短Kinarm VGR评估时间。对于受损最严重的中风幸存者，评估时间可从4-5分钟缩短至约1分钟，同时保持运动学精度。这种预测增强范式有望为中风后运动障碍评估提供高效的机器人评估方法。"}}
{"id": "2511.00881", "categories": ["eess.IV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00881", "abs": "https://arxiv.org/abs/2511.00881", "authors": ["Simone Sarrocco", "Philippe C. Cattin", "Peter M. Maloca", "Paul Friedrich", "Philippe Valmaggia"], "title": "Deep Generative Models for Enhanced Vitreous OCT Imaging", "comment": null, "summary": "Purpose: To evaluate deep learning (DL) models for enhancing vitreous optical\ncoherence tomography (OCT) image quality and reducing acquisition time.\nMethods: Conditional Denoising Diffusion Probabilistic Models (cDDPMs),\nBrownian Bridge Diffusion Models (BBDMs), U-Net, Pix2Pix, and Vector-Quantised\nGenerative Adversarial Network (VQ-GAN) were used to generate high-quality\nspectral-domain (SD) vitreous OCT images. Inputs were SD ART10 images, and\noutputs were compared to pseudoART100 images obtained by averaging ten ART10\nimages per eye location. Model performance was assessed using image quality\nmetrics and Visual Turing Tests, where ophthalmologists ranked generated images\nand evaluated anatomical fidelity. The best model's performance was further\ntested within the manually segmented vitreous on newly acquired data. Results:\nU-Net achieved the highest Peak Signal-to-Noise Ratio (PSNR: 30.230) and\nStructural Similarity Index Measure (SSIM: 0.820), followed by cDDPM. For\nLearned Perceptual Image Patch Similarity (LPIPS), Pix2Pix (0.697) and cDDPM\n(0.753) performed best. In the first Visual Turing Test, cDDPM ranked highest\n(3.07); in the second (best model only), cDDPM achieved a 32.9% fool rate and\n85.7% anatomical preservation. On newly acquired data, cDDPM generated vitreous\nregions more similar in PSNR to the ART100 reference than true ART1 or ART10\nB-scans and achieved higher PSNR on whole images when conditioned on ART1 than\nART10. Conclusions: Results reveal discrepancies between quantitative metrics\nand clinical evaluation, highlighting the need for combined assessment. cDDPM\nshowed strong potential for generating clinically meaningful vitreous OCT\nimages while reducing acquisition time fourfold. Translational Relevance:\ncDDPMs show promise for clinical integration, supporting faster, higher-quality\nvitreous imaging. Dataset and code will be made publicly available.", "AI": {"tldr": "本研究评估了深度学习模型，特别是cDDPM，在提高玻璃体OCT图像质量和缩短采集时间方面的潜力，并发现cDDPM在临床评估中表现出色。", "motivation": "研究目的是为了提高玻璃体光学相干断层扫描（OCT）图像的质量，同时减少图像采集时间。", "method": "研究使用了多种深度学习模型，包括条件去噪扩散概率模型（cDDPMs）、布朗桥扩散模型（BBDMs）、U-Net、Pix2Pix和矢量量化生成对抗网络（VQ-GAN），从低质量的SD ART10图像生成高质量的玻璃体OCT图像。模型性能通过图像质量指标（如PSNR、SSIM、LPIPS）和眼科医生参与的视觉图灵测试进行评估。最佳模型（cDDPM）还在新采集数据上手动分割的玻璃体区域内进行了进一步测试。", "result": "U-Net在PSNR和SSIM方面表现最佳（PSNR: 30.230, SSIM: 0.820），其次是cDDPM。在LPIPS方面，Pix2Pix（0.697）和cDDPM（0.753）表现最好。在第一次视觉图灵测试中，cDDPM排名最高（3.07）。在第二次测试中，cDDPM达到了32.9%的愚弄率和85.7%的解剖结构保留率。在新采集的数据上，cDDPM生成的玻璃体区域在PSNR上比真实的ART1或ART10 B扫描更接近ART100参考图像，并且在以ART1为条件时，在整个图像上获得了比ART10更高的PSNR。", "conclusion": "结果表明定量指标与临床评估之间存在差异，强调了综合评估的必要性。cDDPM在生成具有临床意义的玻璃体OCT图像方面显示出巨大潜力，同时可以将采集时间缩短四倍，具有临床整合前景。"}}
{"id": "2511.00060", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00060", "abs": "https://arxiv.org/abs/2511.00060", "authors": ["Zhiqi Qi", "Runxin Zhao", "Hanyang Zhuang", "Chunxiang Wang", "Ming Yang"], "title": "Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?", "comment": null, "summary": "LiDAR-based roadside perception is a cornerstone of advanced Intelligent\nTransportation Systems (ITS). While considerable research has addressed optimal\nLiDAR placement for infrastructure, the profound impact of differing LiDAR\nscanning patterns on perceptual performance remains comparatively\nunder-investigated. The inherent nature of various scanning modes - such as\ntraditional repetitive (mechanical/solid-state) versus emerging non-repetitive\n(e.g. prism-based) systems - leads to distinct point cloud distributions at\nvarying distances, critically dictating the efficacy of object detection and\noverall environmental understanding. To systematically investigate these\ndifferences in infrastructure-based contexts, we introduce the \"InfraLiDARs'\nBenchmark,\" a novel dataset meticulously collected in the CARLA simulation\nenvironment using concurrently operating infrastructure-based LiDARs exhibiting\nboth scanning paradigms. Leveraging this benchmark, we conduct a comprehensive\nstatistical analysis of the respective LiDAR scanning abilities and evaluate\nthe impact of these distinct patterns on the performance of various leading 3D\nobject detection algorithms. Our findings reveal that non-repetitive scanning\nLiDAR and the 128-line repetitive LiDAR were found to exhibit comparable\ndetection performance across various scenarios. Despite non-repetitive LiDAR's\nlimited perception range, it's a cost-effective option considering its low\nprice. Ultimately, this study provides insights for setting up roadside\nperception system with optimal LiDAR scanning patterns and compatible\nalgorithms for diverse roadside applications, and publicly releases the\n\"InfraLiDARs' Benchmark\" dataset to foster further research.", "AI": {"tldr": "本研究通过引入“InfraLiDARs' Benchmark”数据集，系统性地调查了不同LiDAR扫描模式（重复性与非重复性）对路边感知性能的影响，并评估了其对3D目标检测算法表现的作用。", "motivation": "尽管LiDAR在基础设施中的最佳放置已得到广泛研究，但不同LiDAR扫描模式（如传统重复性与新兴非重复性）对感知性能的深远影响却相对未被充分探讨。这些扫描模式导致不同距离处独特的点云分布，从而关键性地决定了目标检测和环境理解的效率。", "method": "研究引入了“InfraLiDARs' Benchmark”数据集，该数据集在CARLA仿真环境中，通过同时运行的、具备重复性和非重复性扫描模式的基础设施LiDAR精心收集。利用此基准，研究对LiDAR的扫描能力进行了全面的统计分析，并评估了这些不同模式对各种领先3D目标检测算法性能的影响。", "result": "研究发现，非重复性扫描LiDAR与128线重复性LiDAR在多种场景下表现出可比的检测性能。尽管非重复性LiDAR的感知范围有限，但考虑到其低廉的价格，它是一个具有成本效益的选择。", "conclusion": "本研究为建立具有最佳LiDAR扫描模式和兼容算法的路边感知系统提供了见解，适用于各种路边应用。同时，研究公开了“InfraLiDARs' Benchmark”数据集以促进未来的研究。"}}
{"id": "2511.00268", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance.", "AI": {"tldr": "本文提出了一个名为IL-PCR的印度法律语料库，用于同时进行法律条文和先例检索，旨在利用这两个任务之间的内在依赖性，并通过基于LLM的重排序方法实现了最佳性能。", "motivation": "法律从业者需要检索相关法律条文和先例，但现有研究将这两个任务独立处理，开发了不同的数据集和模型。然而，这两个检索任务本质上是相关的（例如，相似的案件倾向于引用相似的法律条文）。本文旨在解决这一研究空白。", "method": "本文提出了IL-PCR（印度法律语料库，用于先例和法律条文检索），这是一个独特的语料库，为开发能够利用两个任务之间依赖性的模型提供了一个通用测试平台。研究人员在多个基线模型上进行了广泛实验，包括词汇模型、语义模型和基于GNN的集成模型。此外，为了利用两个任务之间的依赖性，开发了一种基于LLM的重排序方法。", "result": "基于LLM的重排序方法在实验中取得了最佳性能。", "conclusion": "通过提出IL-PCR语料库和开发基于LLM的重排序方法，本文成功地利用了法律条文检索和先例检索任务之间的依赖性，并取得了优异的性能。"}}
{"id": "2511.00453", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00453", "abs": "https://arxiv.org/abs/2511.00453", "authors": ["Jiale Han", "Wei Ouyang", "Maoran Zhu", "Yuanxin Wu"], "title": "CT-ESKF: A General Framework of Covariance Transformation-Based Error-State Kalman Filter", "comment": "19 pages, 12 figures", "summary": "Invariant extended Kalman filter (InEKF) possesses excellent\ntrajectory-independent property and better consistency compared to conventional\nextended Kalman filter (EKF). However, when applied to scenarios involving both\nglobal-frame and body-frame observations, InEKF may fail to preserve its\ntrajectory-independent property. This work introduces the concept of\nequivalence between error states and covariance matrices among different\nerror-state Kalman filters, and shows that although InEKF exhibits trajectory\nindependence, its covariance propagation is actually equivalent to EKF. A\ncovariance transformation-based error-state Kalman filter (CT-ESKF) framework\nis proposed that unifies various error-state Kalman filtering algorithms. The\nframework gives birth to novel filtering algorithms that demonstrate improved\nperformance in integrated navigation systems that incorporate both global and\nbody-frame observations. Experimental results show that the EKF with covariance\ntransformation outperforms both InEKF and original EKF in a representative\nINS/GNSS/Odometer integrated navigation system.", "AI": {"tldr": "当存在全局和本体坐标系观测时，不变扩展卡尔曼滤波器（InEKF）会失去其轨迹无关性。本文提出了协方差变换误差状态卡尔曼滤波器（CT-ESKF）框架，并证明了带有协方差变换的扩展卡尔曼滤波器（EKF）在集成导航系统中表现优于InEKF和原始EKF。", "motivation": "InEKF在处理同时包含全局坐标系和本体坐标系观测的场景时，可能无法保持其轨迹无关性，且其协方差传播实际上与EKF等效，这促使研究人员寻求更优的滤波算法。", "method": "引入了不同误差状态卡尔曼滤波器之间误差状态和协方差矩阵等效性的概念。在此基础上，提出了一个基于协方差变换的误差状态卡尔曼滤波器（CT-ESKF）框架，该框架统一了各种误差状态卡尔曼滤波算法，并诞生了新的滤波算法。", "result": "实验结果表明，在具有代表性的惯性导航系统/全球导航卫星系统/里程计集成导航系统中，带有协方差变换的EKF（CT-ESKF框架下的新算法）的性能优于InEKF和原始EKF。", "conclusion": "所提出的CT-ESKF框架能够统一各种误差状态卡尔曼滤波算法，并生成在处理包含全局和本体坐标系观测的集成导航系统时性能更优的新滤波算法，尤其以带有协方差变换的EKF为代表。"}}
{"id": "2511.00306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00306", "abs": "https://arxiv.org/abs/2511.00306", "authors": ["Baoshan Song", "Ruijie Xu", "Li-Ta Hsu"], "title": "FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications", "comment": null, "summary": "Sliding window-factor graph optimization (SW-FGO) has gained more and more\nattention in navigation research due to its robust approximation to\nnon-Gaussian noises and nonlinearity of measuring models. There are lots of\nworks focusing on its application performance compared to extended Kalman\nfilter (EKF) but there is still a myth at the theoretical relationship between\nthe SW-FGO and EKF. In this paper, we find the necessarily fair condition to\nconnect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF\n(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the\nconditions, we propose a recursive FGO (Re-FGO) framework to represent KFV\nunder SW-FGO formulation. Under explicit conditions (Markov assumption,\nGaussian noise with L2 loss, and a one-state window), Re-FGO regenerates\nexactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in\nnonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after\nclarifying the connection between them, we highlight the unique advantages of\nSW-FGO in practical phases, especially on numerical estimation and deep\nlearning integration. The code and data used in this work is open sourced at\nhttps://github.com/Baoshan-Song/KFV-FGO-Comparison.", "AI": {"tldr": "本文揭示了滑动窗口因子图优化 (SW-FGO) 与卡尔曼滤波变体 (KFV) 之间的理论联系，提出了一个递归因子图优化 (Re-FGO) 框架，并在特定条件下证明了其与KFV的等价性，同时强调了SW-FGO在非线性、非高斯环境中的独特优势。", "motivation": "尽管SW-FGO在导航研究中日益受到关注，并且与扩展卡尔曼滤波 (EKF) 的应用性能对比众多，但SW-FGO与EKF之间的理论关系仍然存在争议。", "method": "研究人员首先找到了连接SW-FGO与KFV（如EKF、IEKF、REKF、RIEKF）的必要公平条件。在此基础上，提出了一个递归因子图优化 (Re-FGO) 框架，用于在SW-FGO公式下表示KFV。通过明确的条件（马尔可夫假设、L2损失的高斯噪声和一个单状态窗口），Re-FGO被证明可以精确地再现EKF/IEKF/REKF/RIEKF。", "result": "在明确的条件下（马尔可夫假设、L2损失的高斯噪声和一个单状态窗口），所提出的Re-FGO框架可以精确地再现EKF/IEKF/REKF/RIEKF。同时，SW-FGO在非线性、非高斯环境下显示出可衡量的优势，且计算成本可预测。研究还澄清了两者之间的联系，并强调了SW-FGO在数值估计和深度学习集成等实际应用中的独特优势。", "conclusion": "SW-FGO与卡尔曼滤波变体之间存在明确的理论联系，通过Re-FGO框架可以在特定条件下实现相互表示。在非线性、非高斯环境下，SW-FGO相比传统卡尔曼滤波变体具有显著优势，尤其在数值估计和深度学习集成方面展现出独特的应用潜力。"}}
{"id": "2511.00270", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings.", "AI": {"tldr": "本文提出了一种名为POSESTITCH-SLT的新型预训练方案，该方案利用模板生成的句子对显著提升了手语翻译在低资源环境下的性能，超越了现有技术水平。", "motivation": "手语翻译面临大规模、句子对齐数据集稀缺的挑战，以往的研究主要集中于特征提取和架构改进。", "method": "本文提出POSESTITCH-SLT，一种受语言学模板句子生成技术启发的预训练方案。该方法在训练中利用模板生成的句子对，并结合一个简单的基于Transformer的编码器-解码器架构。", "result": "在How2Sign数据集上，BLEU-4分数从1.97提升至4.56；在iSign数据集上，从0.55提升至3.43。这些结果均超越了基于姿态的无手语词汇翻译的现有最佳方法。", "conclusion": "模板驱动的合成监督在低资源手语翻译环境中表现出显著的有效性。"}}
{"id": "2511.00062", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00062", "abs": "https://arxiv.org/abs/2511.00062", "authors": ["NVIDIA", ":", "Arslan Ali", "Junjie Bai", "Maciej Bala", "Yogesh Balaji", "Aaron Blakeman", "Tiffany Cai", "Jiaxin Cao", "Tianshi Cao", "Elizabeth Cha", "Yu-Wei Chao", "Prithvijit Chattopadhyay", "Mike Chen", "Yongxin Chen", "Yu Chen", "Shuai Cheng", "Yin Cui", "Jenna Diamond", "Yifan Ding", "Jiaojiao Fan", "Linxi Fan", "Liang Feng", "Francesco Ferroni", "Sanja Fidler", "Xiao Fu", "Ruiyuan Gao", "Yunhao Ge", "Jinwei Gu", "Aryaman Gupta", "Siddharth Gururani", "Imad El Hanafi", "Ali Hassani", "Zekun Hao", "Jacob Huffman", "Joel Jang", "Pooya Jannaty", "Jan Kautz", "Grace Lam", "Xuan Li", "Zhaoshuo Li", "Maosheng Liao", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yen-Chen Lin", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Yifan Lu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Kaichun Mo", "Seungjun Nah", "Yashraj Narang", "Abhijeet Panaskar", "Lindsey Pavao", "Trung Pham", "Morteza Ramezanali", "Fitsum Reda", "Scott Reed", "Xuanchi Ren", "Haonan Shao", "Yue Shen", "Stella Shi", "Shuran Song", "Bartosz Stefaniak", "Shangkun Sun", "Shitao Tang", "Sameena Tasmeen", "Lyne Tchapmi", "Wei-Cheng Tseng", "Jibin Varghese", "Andrew Z. Wang", "Hao Wang", "Haoxiang Wang", "Heng Wang", "Ting-Chun Wang", "Fangyin Wei", "Jiashu Xu", "Dinghao Yang", "Xiaodong Yang", "Haotian Ye", "Seonghyeon Ye", "Xiaohui Zeng", "Jing Zhang", "Qinsheng Zhang", "Kaiwen Zheng", "Andrew Zhu", "Yuke Zhu"], "title": "World Simulation with Video Foundation Models for Physical AI", "comment": null, "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5$\\times$ smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.", "AI": {"tldr": "Cosmos-Predict2.5 是物理AI的最新世界基础模型，统一了Text2World、Image2World和Video2World生成，并结合Cosmos-Reason1增强控制。Cosmos-Transfer2.5 提供Sim2Real和Real2Real转换。两者均大幅提升了视频质量和指令对齐，并支持具身智能的扩展。", "motivation": "为机器人和自主系统提供更可靠的合成数据生成、策略评估和闭环仿真，并扩展具身智能的能力。", "method": "采用基于流的架构，统一了Text2World、Image2World和Video2World生成。利用物理AI视觉-语言模型Cosmos-Reason1提供更丰富的文本基础和更精细的世界模拟控制。模型在2亿个精选视频片段上训练，并通过强化学习进行后训练优化。Cosmos-Transfer2.5 采用Control-Net风格框架实现Sim2Real和Real2Real世界转换。", "result": "Cosmos-Predict2.5 在视频质量和指令对齐方面显著优于Cosmos-Predict1，并发布了2B和14B规模的模型。Cosmos-Transfer2.5 尽管比Cosmos-Transfer1小3.5倍，但提供了更高的保真度和稳健的长期视频生成。这些能力使得合成数据生成、策略评估和闭环仿真更加可靠，并为扩展具身智能提供了多功能工具。", "conclusion": "Cosmos-Predict2.5 和 Cosmos-Transfer2.5 是扩展具身智能的多功能工具，通过开源代码和预训练模型，旨在降低物理AI研究和部署的门槛，促进具身智能的创新发展。"}}
{"id": "2511.00969", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00969", "abs": "https://arxiv.org/abs/2511.00969", "authors": ["Benjamin Herb", "Rakesh Rao Ramachandra Rao", "Steve Göring", "Alexander Raake"], "title": "Evaluating Video Quality Metrics for Neural and Traditional Codecs using 4K/UHD-1 Videos", "comment": "Accepted for the 2025 Picture Coding Symposium (PCS)", "summary": "With neural video codecs (NVCs) emerging as promising alternatives for\ntraditional compression methods, it is increasingly important to determine\nwhether existing quality metrics remain valid for evaluating their performance.\nHowever, few studies have systematically investigated this using well-designed\nsubjective tests. To address this gap, this paper presents a subjective quality\nassessment study using two traditional (AV1 and VVC) and two variants of a\nneural video codec (DCVC-FM and DCVC-RT). Six source videos (8-10 seconds each,\n4K/UHD-1, 60 fps) were encoded at four resolutions (360p to 2160p) using nine\ndifferent QP values, resulting in 216 sequences that were rated in a controlled\nenvironment by 30 participants. These results were used to evaluate a range of\nfull-reference, hybrid, and no-reference quality metrics to assess their\napplicability to the induced quality degradations. The objective quality\nassessment results show that VMAF and AVQBits|H0|f demonstrate strong Pearson\ncorrelation, while FasterVQA performed best among the tested no-reference\nmetrics. Furthermore, PSNR shows the highest Spearman rank order correlation\nfor within-sequence comparisons across the different codecs. Importantly, no\nsignificant performance differences in metric reliability are observed between\ntraditional and neural video codecs across the tested metrics. The dataset,\nconsisting of source videos, encoded videos, and both subjective and quality\nmetric scores will be made publicly available following an open-science\napproach\n(https://github.com/Telecommunication-Telemedia-Assessment/AVT-VQDB-UHD-1-NVC).", "AI": {"tldr": "本研究通过系统的主观质量评估，比较了传统与神经视频编解码器，并评估了现有质量度量指标对其性能评估的有效性，发现多数指标对神经编解码器仍然适用。", "motivation": "随着神经视频编解码器（NVCs）的兴起，有必要确定现有质量度量指标是否仍能有效评估其性能。然而，很少有研究通过精心设计的主观测试系统地调查这一点，因此本研究旨在填补这一空白。", "method": "研究采用主观质量评估方法，使用了两种传统编解码器（AV1和VVC）和两种神经视频编解码器变体（DCVC-FM和DCVC-RT）。选取了6个源视频（4K/UHD-1, 60 fps），在4种分辨率和9种QP值下进行编码，生成了216个序列。30名参与者在受控环境中对这些序列进行了评分。这些主观结果被用于评估一系列全参考、混合和无参考质量度量指标的适用性。", "result": "客观质量评估结果显示，VMAF和AVQBits|H0|f表现出强大的皮尔逊相关性，而FasterVQA在测试的无参考指标中表现最佳。此外，PSNR在不同编解码器内序列比较中显示出最高的斯皮尔曼等级相关性。重要的是，在测试的指标中，传统和神经视频编解码器之间的度量可靠性没有观察到显著的性能差异。相关数据集将公开提供。", "conclusion": "本研究得出结论，对于神经视频编解码器引入的质量降级，现有的质量度量指标（如VMAF、AVQBits|H0|f、FasterVQA和PSNR）仍然有效。在度量可靠性方面，传统编解码器和神经编解码器之间没有显著差异，表明这些指标对新兴的神经视频压缩技术具有良好的泛化能力。"}}
{"id": "2511.00206", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise.", "AI": {"tldr": "这篇综述探讨了大型语言模型（LLMs）如何帮助认知科学克服知识综合和概念清晰度方面的挑战，强调其作为人类专业知识补充工具的潜力。", "motivation": "认知科学因其多学科和跨学科性质，在知识综合和概念清晰度方面持续面临挑战。人工智能，特别是大型语言模型（LLMs）的最新进展，可能提供解决这些问题的工具。", "method": "本文通过综述的方式，审视了LLMs如何支持认知科学领域长期存在的难题，包括建立跨学科联系、理论形式化、开发清晰的测量分类、通过集成建模框架实现泛化以及捕捉情境和个体差异。同时，文章也概述了LLMs在这些领域当前的S能力、局限性及潜在风险。", "result": "LLMs能够支持认知科学在建立跨学科联系、理论形式化、开发清晰的测量分类、通过集成建模框架实现泛化以及捕捉情境和个体差异等方面的努力。但它们也存在当前的局限性和潜在的风险。", "conclusion": "LLMs可作为工具，在审慎使用以补充而非取代人类专业知识的前提下，促进认知科学更加整合和累积发展。"}}
{"id": "2511.00562", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00562", "abs": "https://arxiv.org/abs/2511.00562", "authors": ["Shuaijun Li", "Jie Tang", "Beixiong Zheng", "Lipeng Zhu", "Cui Yang", "Nan Zhao", "Xiu Yin Zhang", "Kai-Kit Wong"], "title": "Rotatable Antenna System Empowered Low-Altitude Economy: Opportunities and Challenges", "comment": "8 pages, 5 figures, accepted in IEEE Wireless Communication (Early\n  Access)", "summary": "Low-altitude economy (LAE) is an emerging technological paradigm that enables\ncontinuous airspace coverage at multiple altitudes by providing highly reliable\ndata connectivity for numerous low-altitude applications. However, existing\nnetworks cannot sufficiently support LAE development, as current base stations\n(BSs) are primarily designed for terrestrial users and lack the capability to\nprovide continuous coverage at low altitudes. To overcome these challenges,\nrotatable antenna system (RAS) is introduced in LAE, enabling flexible\nbeamforming by dynamically adjusting the boresight of directional antennas to\nextend low-altitude coverage and enhance the stability of data transmission. In\nthis article, we first provide an overview of RAS-empowered LAE applications,\nincluding low-altitude communication, sensing, control, and computation. Then,\nwe present two practical RAS deployment strategies for LAE scenarios, namely\nRAS-aided multi-BS and multi-unmanned aerial vehicle (UAV) cooperative\ncoverages, as well as provide detailed discussions on their system\narchitectures and performance benefits. Additionally, key design issues of RAS\nin LAE are discussed, including channel modeling and estimation, cellular\naccess and interference cancellation, as well as RAS configuration and\nboresight optimization. Finally, we demonstrate the performance gains of RAS in\nLAE networks through experimental and simulation results.", "AI": {"tldr": "本文提出可旋转天线系统（RAS）以解决现有网络在低空经济（LAE）中覆盖不足的问题。RAS通过灵活波束赋形提供连续低空覆盖和稳定数据传输，并讨论了其应用、部署策略、设计挑战及性能增益。", "motivation": "低空经济（LAE）需要连续的低空空域覆盖和可靠的数据连接，但现有基站主要为地面用户设计，无法有效支持LAE发展，缺乏连续的低空覆盖能力。", "method": "引入可旋转天线系统（RAS）以实现灵活波束赋形；概述RAS赋能的LAE应用（通信、感知、控制、计算）；提出两种RAS部署策略：RAS辅助多基站和多无人机协同覆盖；讨论RAS在LAE中的关键设计问题，包括信道建模与估计、蜂窝接入与干扰消除、RAS配置与波束指向优化；通过实验和仿真结果验证性能增益。", "result": "可旋转天线系统（RAS）能够通过动态调整定向天线波束指向，有效扩展低空覆盖范围，增强数据传输稳定性。实验和仿真结果表明，RAS在LAE网络中带来了显著的性能提升。", "conclusion": "可旋转天线系统（RAS）是支持低空经济（LAE）发展的关键技术，通过灵活的波束赋形解决了现有网络的覆盖和稳定性挑战，具有广泛的应用前景和显著的性能优势。"}}
{"id": "2511.01620", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2511.01620", "abs": "https://arxiv.org/abs/2511.01620", "authors": ["Piyush Narhari Pise", "Sanjay Ghosh"], "title": "Learned Adaptive Kernels for High-Fidelity Image Downscaling", "comment": "10 pages, 6 figures, and 3 tables", "summary": "Image downscaling is a fundamental operation in image processing, crucial for\nadapting high-resolution content to various display and storage constraints.\nWhile classic methods often introduce blurring or aliasing, recent\nlearning-based approaches offer improved adaptivity. However, achieving maximal\nfidelity against ground-truth low-resolution (LR) images, particularly by\naccounting for channel-specific characteristics, remains an open challenge.\nThis paper introduces ADK-Net (Adaptive Downscaling Kernel Network), a novel\ndeep convolutional neural network framework for high-fidelity supervised image\ndownscaling. ADK-Net explicitly addresses channel interdependencies by learning\nto predict spatially-varying, adaptive resampling kernels independently for\neach pixel and uniquely for each color channel (RGB). The architecture employs\na hierarchical design featuring a ResNet-based feature extractor and parallel\nchannel-specific kernel generators, themselves composed of ResNet-based trunk\nand branch sub-modules, enabling fine-grained kernel prediction. Trained\nend-to-end using an L1 reconstruction loss against ground-truth LR data,\nADK-Net effectively learns the target downscaling transformation. Extensive\nquantitative and qualitative experiments on standard benchmarks, including the\nRealSR dataset, demonstrate that ADK-Net establishes a new state-of-the-art in\nsupervised image downscaling, yielding significant improvements in PSNR and\nSSIM metrics compared to existing learning-based and traditional methods.", "AI": {"tldr": "本文提出ADK-Net，一个深度卷积神经网络框架，通过为每个像素和每个颜色通道独立预测空间自适应重采样核，实现了高保真监督图像下采样，并在PSNR和SSIM指标上达到SOTA。", "motivation": "图像下采样是图像处理中的基础操作，但经典方法常引入模糊或混叠。尽管基于学习的方法有所改进，但如何通过考虑通道特定特性来最大化对真实低分辨率图像的保真度，仍是一个未解决的挑战。", "method": "本文引入ADK-Net（Adaptive Downscaling Kernel Network），一个新颖的深度卷积神经网络框架。它通过学习为每个像素和每个RGB颜色通道独立预测空间自适应重采样核，显式处理通道间的相互依赖性。其架构采用分层设计，包含一个基于ResNet的特征提取器和并行的通道特定核生成器，后者由基于ResNet的主干和分支子模块组成，以实现精细的核预测。模型使用L1重建损失，针对真实低分辨率数据进行端到端训练。", "result": "ADK-Net在标准基准测试（包括RealSR数据集）上的大量定量和定性实验表明，它在监督图像下采样方面建立了新的最先进水平，与现有基于学习和传统方法相比，在PSNR和SSIM指标上取得了显著提升。", "conclusion": "ADK-Net通过其创新的自适应重采样核预测机制，有效学习了目标下采样变换，解决了通道间相互依赖性的挑战，并在图像下采样任务中取得了卓越的性能，达到了新的技术高度。"}}
{"id": "2511.00267", "categories": ["cs.AI", "cs.CY", "cs.GL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00267", "abs": "https://arxiv.org/abs/2511.00267", "authors": ["Christian Prothmann", "Vijay Gadepally", "Jeremy Kepner", "Koley Borchard", "Luca Carlone", "Zachary Folcik", "J. Daniel Grith", "Michael Houle", "Jonathan P. How", "Nathan Hughes", "Ifueko Igbinedion", "Hayden Jananthan", "Tejas Jayashankar", "Michael Jones", "Sertac Karaman", "Binoy G. Kurien", "Alejandro Lancho", "Giovanni Lavezzi", "Gary C. F. Lee", "Charles E. Leiserson", "Richard Linares", "Lindsey McEvoy", "Peter Michaleas", "Chasen Milner", "Alex Pentland", "Yury Polyanskiy", "Jovan Popovich", "Jeffrey Price", "Tim W. Reid", "Stephanie Riley", "Siddharth Samsi", "Peter Saunders", "Olga Simek", "Mark S. Veillette", "Amir Weiss", "Gregory W. Wornell", "Daniela Rus", "Scott T. Ruppel"], "title": "Advancing AI Challenges for the United States Department of the Air Force", "comment": "8 pages, 8 figures, 59 references. To appear in IEEE HPEC 2025", "summary": "The DAF-MIT AI Accelerator is a collaboration between the United States\nDepartment of the Air Force (DAF) and the Massachusetts Institute of Technology\n(MIT). This program pioneers fundamental advances in artificial intelligence\n(AI) to expand the competitive advantage of the United States in the defense\nand civilian sectors. In recent years, AI Accelerator projects have developed\nand launched public challenge problems aimed at advancing AI research in\npriority areas. Hallmarks of AI Accelerator challenges include large, publicly\navailable, and AI-ready datasets to stimulate open-source solutions and engage\nthe wider academic and private sector AI ecosystem. This article supplements\nour previous publication, which introduced AI Accelerator challenges. We\nprovide an update on how ongoing and new challenges have successfully\ncontributed to AI research and applications of AI technologies.", "AI": {"tldr": "DAF-MIT AI加速器通过发布公共挑战问题和提供大型AI数据集，旨在推动AI研究和应用，以增强美国在国防和民用领域的竞争力。本文更新了其挑战项目对AI研究的贡献。", "motivation": "该研究的动机是推动人工智能的基础进步，以扩大美国在国防和民用领域的竞争优势。", "method": "通过DAF-MIT AI加速器项目，开发并发布公共挑战问题，并提供大型、公开可用的AI就绪数据集，以激发开源解决方案，并吸引更广泛的学术界和私营部门AI生态系统参与。", "result": "正在进行和新的挑战项目已成功地促进了AI研究和AI技术的应用。", "conclusion": "DAF-MIT AI加速器的公共挑战项目及其提供的数据集有效地推动了AI研究和相关技术的实际应用。"}}
{"id": "2511.00315", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00315", "abs": "https://arxiv.org/abs/2511.00315", "authors": ["Lee Xiong", "Maksim Tkachenko", "Johanes Effendi", "Ting Cai"], "title": "Language Modeling With Factorization Memory", "comment": null, "summary": "We propose Factorization Memory, an efficient recurrent neural network (RNN)\narchitecture that achieves performance comparable to Transformer models on\nshort-context language modeling tasks while also demonstrating superior\ngeneralization in long-context scenarios. Our model builds upon Mamba-2,\nenabling Factorization Memory to exploit parallel computations during training\nwhile preserving constant computational and memory complexity during inference.\nTo further optimize model efficiency and representational capacity, we develop\na sparse formulation of Factorization Memory that updates only a subset of\nrecurrent states at each step while preserving the strong performance of its\ndense counterpart. To our knowledge, this represents the first RNN architecture\nthat successfully combines sparse memory activation with competitive\nperformance across both short and long-context settings. This work provides a\nsystematic empirical analysis of Factorization Memory in comparison to\nTransformer and Mamba-2 architectures.", "AI": {"tldr": "本文提出Factorization Memory (FM)，一种高效的循环神经网络（RNN）架构，在短上下文语言建模任务上与Transformer模型性能相当，并在长上下文场景中展现出卓越的泛化能力。它基于Mamba-2，并引入了稀疏化版本。", "motivation": "现有模型在长上下文场景中的泛化能力和计算效率不足，尤其是在与Transformer竞争短上下文性能的同时，需要一种能在长上下文场景中表现更好的RNN架构。", "method": "本文提出了Factorization Memory (FM) 架构，它构建于Mamba-2之上，以实现训练期间的并行计算和推理期间的恒定计算及内存复杂度。为进一步优化效率和表示能力，开发了FM的稀疏化版本，每次只更新部分循环状态。研究还对FM与Transformer和Mamba-2架构进行了系统的实证分析。", "result": "Factorization Memory在短上下文语言建模任务上实现了与Transformer模型相当的性能，并在长上下文场景中展示了卓越的泛化能力。其稀疏化版本在保持强大性能的同时，进一步优化了模型效率。这是首个成功结合稀疏内存激活并在短、长上下文设置中均具竞争力的RNN架构。", "conclusion": "Factorization Memory是一种高效且性能强大的RNN架构，它在短上下文任务上可与Transformer媲美，并在长上下文场景中展现出更优的泛化能力。其稀疏化版本在保持高性能的同时提升了效率，为RNN设计开辟了新方向。"}}
{"id": "2511.00090", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00090", "abs": "https://arxiv.org/abs/2511.00090", "authors": ["Huanlin Gao", "Ping Chen", "Fuyuan Shi", "Chao Tan", "Zhaoxiang Liu", "Fang Zhao", "Kai Wang", "Shiguo Lian"], "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation", "comment": "NeurIPS 2025", "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa", "AI": {"tldr": "LeMiCa是一个免训练、高效的扩散视频生成加速框架，通过词典式极小极大路径优化策略解决全局误差累积问题，显著提升了生成速度和视频质量。", "motivation": "现有缓存策略主要关注局部启发式误差，但忽略了全局误差的累积，导致加速视频与原始视频之间存在明显的质量下降。", "method": "将缓存调度建模为带误差加权边的有向图，并引入词典式极小极大路径优化策略（Lexicographic Minimax Path Optimization），以明确限制最坏情况下的路径误差，从而提高全局内容和风格的一致性。", "result": "LeMiCa在多个文本到视频基准上实现了推理速度和生成质量的双重提升。例如，在Latte模型上实现2.9倍加速，在Open-Sora上LPIPS得分达到0.05，优于现有缓存技术，且感知质量下降极小。", "conclusion": "LeMiCa为加速扩散视频生成提供了一个鲁棒且通用的范式，有望为未来高效可靠的视频合成研究奠定坚实基础。"}}
{"id": "2511.00582", "categories": ["eess.SY", "cs.SY", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.00582", "abs": "https://arxiv.org/abs/2511.00582", "authors": ["Carsten Hartmann", "Nil Rodellas-Gràcia", "Christian Wallisch", "Thiemo Pesch", "Frank K. Wilhelm", "Dirk Witthaut", "Tobias Stollenwerk", "Andrea Benigni"], "title": "Towards Quantum Algorithms for the Optimization of Spanning Trees: The Power Distribution Grids Use Case", "comment": null, "summary": "Optimizing the topology of networks is an important challenge across\nengineering disciplines. In energy systems, network reconfiguration can\nsubstantially reduce losses and costs and thus support the energy transition.\nUnfortunately, many related optimization problems are NP hard, restricting\npractical applications. In this article, we address the problem of minimizing\nlosses in radial networks, a problem that routinely arises in distribution grid\noperation. We show that even the computation of approximate solutions is\ncomputationally hard and propose quantum optimization as a promising\nalternative. We derive two quantum algorithmic primitives based on the Quantum\nAlternating Operator Ansatz (QAOA) that differ in the sampling of network\ntopologies: a tailored sampling of radial topologies and simple sampling with\npenalty terms to suppress non-radial topologies. We show how to apply these\nalgorithmic primitives to distribution grid reconfiguration and quantify the\nnecessary quantum resources.", "AI": {"tldr": "本文针对径向网络中的损耗最小化问题，指出其近似解计算的难度，并提出基于QAOA的量子优化算法作为替代方案，通过两种拓扑采样方法来解决配电网重构问题。", "motivation": "网络拓扑优化在工程领域是一个重要挑战，特别是在能源系统中，网络重构可以显著降低损耗和成本，支持能源转型。然而，许多相关优化问题是NP难的，限制了实际应用。因此，需要新的方法来解决配电网运行中常见的径向网络损耗最小化问题。", "method": "本文提出量子优化作为一种有前景的替代方案。具体地，推导出两种基于量子交替算子本征态（QAOA）的量子算法原语，它们在网络拓扑采样上有所不同：一种是针对径向拓扑的定制采样，另一种是带有惩罚项的简单采样以抑制非径向拓扑。并量化了所需的量子资源。", "result": "研究表明，即使是计算径向网络损耗最小化问题的近似解，在计算上也是困难的。量子优化被提出作为一种有前景的替代方案。文章成功推导出了两种基于QAOA的量子算法原语，并展示了它们如何应用于配电网重构，同时量化了所需的量子资源。", "conclusion": "量子优化，特别是基于QAOA的算法，为解决径向网络中损耗最小化的NP难问题提供了一个有前景的替代方案。通过定制或带惩罚项的采样策略，可以有效应用于配电网重构，并为未来的量子计算应用奠定基础。"}}
{"id": "2511.00340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00340", "abs": "https://arxiv.org/abs/2511.00340", "authors": ["Manan Roy Choudhury", "Adithya Chandramouli", "Mannan Anand", "Vivek Gupta"], "title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities", "comment": "41 pages, 4 images", "summary": "The rapid integration of large language models (LLMs) into high-stakes legal\nwork has exposed a critical gap: no benchmark exists to systematically\nstress-test their reliability against the nuanced, adversarial, and often\nsubtle flaws present in real-world contracts. To address this, we introduce\nCLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an\nLLM's legal reasoning. We study the capabilities of LLMs to detect and reason\nabout fine-grained discrepancies by producing over 7500 real-world perturbed\ncontracts from foundational datasets like CUAD and ContractNLI. Our novel,\npersona-driven pipeline generates 10 distinct anomaly categories, which are\nthen validated against official statutes using a Retrieval-Augmented Generation\n(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'\nability to detect embedded legal flaws and explain their significance. Our\nanalysis shows a key weakness: these models often miss subtle errors and\nstruggle even more to justify them legally. Our work outlines a path to\nidentify and correct such reasoning failures in legal AI.", "AI": {"tldr": "本文引入CLAUSE基准测试，旨在系统性评估大型语言模型（LLMs）在处理真实世界合同中细微、对抗性法律缺陷时的可靠性。研究发现LLMs常遗漏细微错误且难以提供法律解释。", "motivation": "LLMs快速融入高风险法律工作，但缺乏系统性基准测试来评估其在面对真实合同中细微、对抗性缺陷时的可靠性，这暴露了一个关键空白。", "method": "引入CLAUSE基准测试，通过从CUAD和ContractNLI等基础数据集中生成超过7500份受扰动的真实合同。采用以角色为驱动的管道生成10种不同的异常类别，并使用RAG系统对照官方法规进行验证以确保法律忠实性。使用CLAUSE评估领先LLMs检测嵌入式法律缺陷并解释其重要性的能力。", "result": "分析显示LLMs存在关键弱点：它们常常遗漏细微的错误，并且在法律上解释这些错误时更加困难。", "conclusion": "本研究为识别和纠正法律AI中的此类推理失败指明了方向。"}}
{"id": "2511.00412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00412", "abs": "https://arxiv.org/abs/2511.00412", "authors": ["John A. Christian", "Michael R. Walker II", "Wyatt Bridgman", "Michael J. Sparapany"], "title": "Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory", "comment": null, "summary": "The integration of gyroscope measurements is an essential task for most\nnavigation systems. Modern vehicles typically use strapdown systems, such that\ngyro integration requires coning compensation to account for the sensor's\nrotation during the integration. Many coning compensation algorithms have been\ndeveloped and a few are reviewed. This work introduces a new class of coning\ncorrection algorithm built directly from the classical Runge-Kutta integration\nroutines. A simple case is shown to collapse to one of the most popular coning\nalgorithms and a clear procedure for generating higher-order algorithms is\npresented.", "AI": {"tldr": "本文提出了一种基于经典龙格-库塔积分例程的新型锥动补偿算法，用于捷联导航系统中的陀螺仪积分，并展示了生成更高阶算法的通用过程。", "motivation": "陀螺仪测量积分是大多数导航系统（特别是捷联系统）的关键任务。由于传感器在积分过程中的旋转，需要进行锥动补偿。尽管已有多种补偿算法，但仍有必要探索新的、更系统化的方法。", "method": "引入了一类直接基于经典龙格-库塔（Runge-Kutta）积分例程构建的锥动校正算法。提出了一种生成更高阶算法的清晰程序。", "result": "研究表明，新算法的一个简单特例可以退化为当前最流行的锥动补偿算法之一。同时，本文提供了一个明确的步骤来生成更高阶的算法。", "conclusion": "本文成功提出了一种基于龙格-库塔积分的新型锥动补偿算法，该算法不仅能复现现有流行算法，还能系统地生成更高阶的补偿方案，为捷联导航系统提供了新的集成方法。"}}
{"id": "2511.00073", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00073", "abs": "https://arxiv.org/abs/2511.00073", "authors": ["Harald Kristen", "Daniel Kulmer", "Manuela Hirschmugl"], "title": "Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures", "comment": null, "summary": "Rapid climate change and other disturbances in alpine ecosystems demand\nfrequent habitat monitoring, yet manual mapping remains prohibitively expensive\nfor the required temporal resolution. We employ deep learning for change\ndetection using long-term alpine habitat data from Gesaeuse National Park,\nAustria, addressing a major gap in applying geospatial foundation models (GFMs)\nto complex natural environments with fuzzy class boundaries and highly\nimbalanced classes. We compare two paradigms: post-classification change\ndetection (CD) versus direct CD. For post-classification CD, we evaluate GFMs\nPrithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the\ntransformer ChangeViT against U-Net baselines. Using high-resolution multimodal\ndata (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes\nover 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus\nU-Net's 41% for multi-class habitat change, while both reach 67% for binary\nchange detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but\nonly 28% accuracy for multi-class detection. Cross-temporal evaluation reveals\nGFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's\n23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.\nAlthough overall accuracies are lower than in more homogeneous landscapes, they\nreflect realistic performance for complex alpine habitats. Future work will\nintegrate object-based post-processing and physical constraints to enhance\napplicability.", "AI": {"tldr": "本研究利用深度学习对高山生态系统进行变化检测，比较了地理空间基础模型（GFMs）与U-Net在后分类和直接变化检测范式下的表现，并探讨了多模态数据（包括LiDAR）的集成效果，发现GFMs在高山复杂环境中具有潜力，但仍面临挑战。", "motivation": "高山生态系统快速的气候变化和干扰需要频繁的栖息地监测。传统人工测绘成本高昂，无法满足所需的时间分辨率。此外，将地理空间基础模型（GFMs）应用于具有模糊类别边界和高度不平衡类别的复杂自然环境仍存在重大空白。", "method": "研究采用深度学习方法进行变化检测，利用奥地利Gesaeuse国家公园的长期高山栖息地数据。比较了两种范式：后分类变化检测（CD）和直接变化检测。在后分类CD中，评估了GFMs Prithvi-EO-2.0和Clay v1.0与U-Net CNNs；在直接CD中，测试了transformer模型ChangeViT与U-Net基线。使用高分辨率多模态数据（RGB、NIR、LiDAR、地形属性），涵盖15.3平方公里内4,480个已记录的变化。", "result": "对于多类别栖息地变化，Clay v1.0在后分类CD中实现了51%的整体准确率，优于U-Net的41%；对于二元变化检测，两者均达到67%。直接CD在二元检测中获得了更高的IoU（0.53 vs 0.35），但在多类别检测中准确率仅为28%。跨时间评估显示GFM具有鲁棒性，Clay在2020年数据上保持33%的准确率，而U-Net为23%。集成LiDAR将语义分割准确率从30%提高到50%。尽管整体准确率低于更均匀的景观，但反映了复杂高山栖息地的实际性能。", "conclusion": "地理空间基础模型（GFMs）在高山复杂环境中进行栖息地变化检测具有潜力，尤其是在集成LiDAR数据后能显著提高性能。虽然整体准确率低于均质景观，但这些结果反映了复杂高山栖息地的实际性能。未来工作将整合基于对象的后处理和物理约束以增强适用性。"}}
{"id": "2511.00211", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00211", "abs": "https://arxiv.org/abs/2511.00211", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals", "comment": null, "summary": "The increasing adoption of satellite Internet with low-Earth-orbit (LEO)\nsatellites in mega-constellations allows ubiquitous connectivity to rural and\nremote areas. However, weather events have a significant impact on the\nperformance and reliability of satellite Internet. Adverse weather events such\nas snow and rain can disturb the performance and operations of satellite\nInternet's essential ground terminal components, such as satellite antennas,\nsignificantly disrupting the space-ground link conditions between LEO\nsatellites and ground stations. This challenge calls for not only region-based\nweather forecasts but also fine-grained detection capability on ground terminal\ncomponents of fine-grained weather conditions. Such a capability can assist in\nfault diagnostics and mitigation for reliable satellite Internet, but its\nsolutions are lacking, not to mention the effectiveness and generalization that\nare essential in real-world deployments. This paper discusses an efficient\ntransfer learning (TL) method that can enable a ground component to locally\ndetect representative weather-related conditions. The proposed method can\ndetect snow, wet, and other conditions resulting from adverse and typical\nweather events and shows superior performance compared to the typical deep\nlearning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL\nmethod also shows the advantage of being generalizable to various scenarios.", "AI": {"tldr": "本文提出了一种高效的迁移学习方法，用于在卫星互联网地面终端组件上本地检测细粒度的天气相关条件，以提高系统在恶劣天气下的可靠性，并优于现有深度学习方法。", "motivation": "低轨卫星互联网普及性日益提高，但恶劣天气（如雨雪）严重影响其地面终端组件（如天线）的性能和可靠性，导致星地链路中断。目前缺乏有效且泛化能力强的解决方案，无法对地面组件进行细粒度的天气条件检测，从而进行故障诊断和缓解。", "method": "本文提出了一种高效的迁移学习（TL）方法。该方法旨在使地面组件能够本地检测代表性的天气相关条件，例如雪和潮湿。", "result": "所提出的迁移学习方法能够检测由恶劣和典型天气事件引起的雪、潮湿及其他条件。与YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法相比，该方法表现出卓越的性能，并展示了在各种场景下的泛化优势。", "conclusion": "该研究得出结论，所提出的高效迁移学习方法为卫星互联网地面组件的本地细粒度天气条件检测提供了一个有效且具有泛化能力的解决方案，有助于提高卫星互联网的可靠性和故障诊断能力。"}}
{"id": "2511.00593", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00593", "abs": "https://arxiv.org/abs/2511.00593", "authors": ["Aayushya Agarwal", "Jace Rozsa", "Matteo Pozzi", "Rahul Panat", "Gary K. Fedder"], "title": "Digital Twin of Aerosol Jet Printing", "comment": null, "summary": "Aerosol Jet (AJ) printing is a versatile additive manufacturing technique\ncapable of producing high-resolution interconnects on both 2D and 3D\nsubstrates. The AJ process is complex and dynamic with many hidden and\nunobservable states that influence the machine performance, including aerosol\nparticle diameter, aerosol carrier density, vial level, and ink deposition in\nthe tube and nozzle. Despite its promising potential, the widespread adoption\nof AJ printing is limited by inconsistencies in print quality that often stem\nfrom variability in these hidden states. To address these challenges, we\ndevelop a digital twin model of the AJ process that offers real-time insights\ninto the machine's operations. The digital twin is built around a physics-based\nmacro-model created through simulation and experimentation. The states and\nparameters of the digital model are continuously updated using probabilistic\nsequential estimation techniques to closely align with real-time measurements\nextracted from the AJ system's sensor and video data. The result is a digital\nmodel of the AJ process that continuously evolves over a physical machine's\nlifecycle. The digital twin enables accurate monitoring of unobservable\nphysical characteristics, detects and predicts anomalous behavior, and\nforecasts the effect of control adjustments. This work presents a comprehensive\nend-to-end digital twin framework that integrates customized computer vision\ntechniques, physics-based macro-modeling, and advanced probabilistic estimation\nmethods to construct an evolving digital representation of the AJ equipment and\nprocess. While the methodologies are customized for aerosol jet printing, the\nprocess for constructing the digital twin can be applied for other advanced\nmanufacturing techniques.", "AI": {"tldr": "本文开发了一个针对气溶胶喷射（AJ）打印过程的数字孪生模型，通过整合物理模型、计算机视觉和概率估计，实时监测不可观测状态，提高打印质量并预测异常。", "motivation": "气溶胶喷射（AJ）打印技术虽具潜力，但其广泛应用受限于打印质量的不一致性，这主要源于气溶胶粒径、载流密度、墨水液位以及喷嘴墨水沉积等许多隐藏且不可观测状态的变异性。", "method": "研究人员开发了一个AJ过程的数字孪生模型。该模型基于通过仿真和实验构建的物理宏观模型，并利用概率序列估计技术，通过AJ系统的传感器和视频数据提取的实时测量结果，持续更新数字模型的状态和参数。该框架集成了定制的计算机视觉技术、基于物理的宏观建模和先进的概率估计方法。", "result": "该数字孪生模型能够持续地与物理机器的生命周期同步演进，从而实现对不可观测物理特性的精确监控、异常行为的检测和预测，以及控制调整效果的预测。最终成果是一个全面的端到端数字孪生框架。", "conclusion": "该工作提供了一个能够持续演进的AJ设备和工艺数字表示，有效解决了AJ打印中因隐藏状态导致的质量不一致问题。尽管方法为AJ打印定制，但构建数字孪生的过程可应用于其他先进制造技术。"}}
{"id": "2511.00510", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00510", "abs": "https://arxiv.org/abs/2511.00510", "authors": ["Kai Luo", "Hao Shi", "Kunyu Peng", "Fei Teng", "Sheng Wu", "Kaiwei Wang", "Kailun Yang"], "title": "OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback", "comment": "Extended version of CVPR 2025 paper arXiv:2503.04565. Datasets and\n  code will be made publicly available at https://github.com/xifen523/OmniTrack", "summary": "This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,\nwhich introduces unique challenges including a 360{\\deg} Field of View (FoV),\nresolution dilution, and severe view-dependent distortions. Conventional MOT\nmethods designed for narrow-FoV pinhole cameras generalize unsatisfactorily\nunder these conditions. To address panoramic distortion, large search space,\nand identity ambiguity under a 360{\\deg} FoV, OmniTrack++ adopts a\nfeedback-driven framework that progressively refines perception with trajectory\ncues. A DynamicSSM block first stabilizes panoramic features, implicitly\nalleviating geometric distortion. On top of normalized representations,\nFlexiTrack Instances use trajectory-informed feedback for flexible localization\nand reliable short-term association. To ensure long-term robustness, an\nExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts\ndesign, enabling recovery from fragmented tracks and reducing identity drift.\nFinally, a Tracklet Management module adaptively switches between end-to-end\nand tracking-by-detection modes according to scene dynamics, offering a\nbalanced and scalable solution for panoramic MOT. To support rigorous\nevaluation, we establish the EmboTrack benchmark, a comprehensive dataset for\npanoramic MOT that includes QuadTrack, captured with a quadruped robot, and\nBipTrack, collected with a bipedal wheel-legged robot. Together, these datasets\nspan wide-angle environments and diverse motion patterns, providing a\nchallenging testbed for real-world panoramic perception. Extensive experiments\non JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art\nperformance, yielding substantial HOTA improvements of +25.5% on JRDB and\n+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be\nmade publicly available at https://github.com/xifen523/OmniTrack.", "AI": {"tldr": "本文提出了一种名为 OmniTrack++ 的多目标跟踪 (MOT) 方法，专为全景图像设计，以解决其独特的挑战。该方法通过反馈驱动框架和多模块设计实现性能提升，并引入了 EmboTrack 全景 MOT 基准。", "motivation": "传统为窄视场针孔相机设计的 MOT 方法在全景图像的 360 度视场、分辨率稀释和严重的视点相关畸变等挑战下表现不佳。", "method": "OmniTrack++ 采用反馈驱动框架：DynamicSSM 模块稳定全景特征并缓解几何畸变；FlexiTrack Instances 利用轨迹反馈进行灵活定位和短期关联；ExpertTrack Memory 通过专家混合设计巩固外观线索，以实现长期鲁棒性；Tracklet Management 模块根据场景动态自适应切换端到端和检测跟踪模式。此外，本文还建立了 EmboTrack 基准，包括 QuadTrack 和 BipTrack 数据集，用于全景 MOT 的严格评估。", "result": "OmniTrack++ 在 JRDB 和 EmboTrack 基准上均达到了最先进的性能，相比原始 OmniTrack，在 JRDB 上 HOTA 提升了 +25.5%，在 QuadTrack 上提升了 +43.07%。", "conclusion": "OmniTrack++ 为全景 MOT 提供了一个平衡且可扩展的解决方案，有效应对了全景图像的独特挑战，并在现有和新建基准上取得了显著的性能提升。"}}
{"id": "2511.00392", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00392", "abs": "https://arxiv.org/abs/2511.00392", "authors": ["Lingpeng Chen", "Jiakun Tang", "Apple Pui-Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping", "comment": "8 pages, 9 figures, conference", "summary": "Accurate 3D reconstruction in visually-degraded underwater environments\nremains a formidable challenge. Single-modality approaches are insufficient:\nvision-based methods fail due to poor visibility and geometric constraints,\nwhile sonar is crippled by inherent elevation ambiguity and low resolution.\nConsequently, prior fusion technique relies on heuristics and flawed geometric\nassumptions, leading to significant artifacts and an inability to model complex\nscenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep\nlearning framework that overcomes these limitations by adapting the principled\nplane sweep algorithm for cross-modal fusion between sonar and visual data.\nExtensive experiments in both high-fidelity simulation and real-world\nenvironments demonstrate that SonarSweep consistently generates dense and\naccurate depth maps, significantly outperforming state-of-the-art methods\nacross challenging conditions, particularly in high turbidity. To foster\nfurther research, we will publicly release our code and a novel dataset\nfeaturing synchronized stereo-camera and sonar data, the first of its kind.", "AI": {"tldr": "本文提出了SonarSweep，一个新颖的端到端深度学习框架，通过调整平面扫描算法，实现了声纳和视觉数据的跨模态融合，从而在水下恶劣环境中生成密集且准确的3D深度图。", "motivation": "在视觉退化的水下环境中，准确的3D重建仍然是一个巨大挑战。单一模态方法（视觉因能见度差，声纳因高程模糊和低分辨率）不足以应对。现有的融合技术依赖启发式和有缺陷的几何假设，导致重建结果存在显著伪影且无法建模复杂场景。", "method": "本文引入了SonarSweep，一个新颖的端到端深度学习框架。该框架通过改编经典的平面扫描算法，实现了声纳数据和视觉数据之间的跨模态融合，以克服现有方法的局限性。", "result": "通过在高保真模拟和真实世界环境中的广泛实验，SonarSweep持续生成密集且准确的深度图。它在挑战性条件下，特别是在高浑浊度环境下，显著优于现有最先进的方法。", "conclusion": "SonarSweep成功解决了水下3D重建的难题，为恶劣水下环境提供了更鲁棒和准确的解决方案。为促进进一步研究，作者将公开发布代码和一个首创的同步立体相机和声纳数据集。"}}
{"id": "2511.00343", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00343", "abs": "https://arxiv.org/abs/2511.00343", "authors": ["Changbing Yang", "Franklin Ma", "Freda Shi", "Jian Zhu"], "title": "LingGym: How Far Are LLMs from Thinking Like Field Linguists?", "comment": "EMNLP 2025 Main", "summary": "This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity\nfor meta-linguistic reasoning using Interlinear Glossed Text (IGT) and\ngrammatical descriptions extracted from 18 typologically diverse reference\ngrammars. Unlike previous work that focuses on specific downstream tasks, we\nassess whether LLMs can generalize linguistic inference across low-resource\nlanguages and structures not seen during training. We present a controlled\nevaluation task: Word-Gloss Inference, in which the model must infer a missing\nword and gloss from context using varying levels of linguistic information\n(e.g., glosses, grammatical explanations, translations). Our results show that\nincorporating structured linguistic cues leads to consistent improvements in\nreasoning performance across all models. This work highlights both the promise\nand current limitations of using LLMs for typologically informed linguistic\nanalysis and low-resource language documentation.", "AI": {"tldr": "本文提出了LingGym基准，用于评估大型语言模型（LLMs）的元语言推理能力，通过使用来自18种不同类型学参考语法的IGT和语法描述，重点考察LLMs在低资源语言和未见结构上的泛化推理能力。", "motivation": "以往研究侧重于特定下游任务，本文旨在评估LLMs是否能够将语言推理泛化到训练期间未见的低资源语言和结构。", "method": "引入了LingGym基准，该基准利用来自18种类型学多样参考语法的语际加注文本（IGT）和语法描述。评估任务是“词-注音推理”，模型需要根据上下文和不同级别的语言信息（如注音、语法解释、翻译）推断缺失的词和注音。", "result": "研究结果表明，整合结构化的语言线索能够持续提升所有模型的推理性能。", "conclusion": "这项工作突显了使用LLMs进行类型学语言分析和低资源语言文档编制的潜力和当前局限性。"}}
{"id": "2511.00379", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research.", "AI": {"tldr": "本文提出了一种受伦理决策模型启发的LLM伦理推理范式，通过一个五步结构化过程，旨在增强LLM对不同地域和文化背景下多样化人类价值观的理解和对齐。", "motivation": "LLM与多样化、演变中的人类价值观对齐是一个关键挑战。现有对齐方法通常流于表面，未能解决人类价值观复杂且依赖于语境的本质。", "method": "提出一个基于理论的五步结构化伦理推理框架，包括：语境事实收集、层级社会规范识别、选项生成、多视角伦理影响分析和反思。该框架可通过提示工程或监督微调实现。", "result": "在专门为区域价值对齐设计的SafeWorld基准测试中，该框架显著提升了LLM与多样化人类价值观的对齐效果，实现了更准确的社会规范识别和更具文化适宜性的推理。", "conclusion": "本研究为通过跨学科研究开发能更有效对齐全球社会多方面价值观的LLM提供了一条具体途径。"}}
{"id": "2511.00091", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00091", "abs": "https://arxiv.org/abs/2511.00091", "authors": ["Wenli Xiao", "Haotian Lin", "Andy Peng", "Haoru Xue", "Tairan He", "Yuqi Xie", "Fengyuan Hu", "Jimmy Wu", "Zhengyi Luo", "Linxi \"Jim\" Fan", "Guanya Shi", "Yuke Zhu"], "title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL", "comment": "26 pages", "summary": "Supervised fine-tuning (SFT) has become the de facto post-training strategy\nfor large vision-language-action (VLA) models, but its reliance on costly human\ndemonstrations limits scalability and generalization. We propose Probe, Learn,\nDistill (PLD), a three-stage plug-and-play framework that improves VLAs through\nresidual reinforcement learning (RL) and distribution-aware data collection. In\nStage 1, we train lightweight residual actors to probe failure regions of the\nVLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns\ncollected trajectories with the generalist's deployment distribution while\ncapturing recovery behaviors. In Stage 3, we distill the curated trajectories\nback into the generalist with standard SFT. PLD achieves near-saturated 99%\ntask success on LIBERO, over 50% gains in SimplerEnv, and 100% success on\nreal-world Franka and YAM arm manipulation tasks. Ablations show that residual\nprobing and distribution-aware replay are key to collecting deployment-aligned\ndata that improves both seen and unseen tasks, offering a scalable path toward\nself-improving VLA models.", "AI": {"tldr": "本文提出PLD（探测、学习、蒸馏）框架，通过残差强化学习和分布感知数据收集，提升视觉-语言-动作（VLA）模型的性能，克服了监督微调对昂贵人工演示的依赖。", "motivation": "现有的VLA模型训练策略（如监督微调SFT）严重依赖昂贵的人工演示，这限制了模型的可扩展性和泛化能力。", "method": "PLD是一个三阶段即插即用框架：\n1.  **探测（Probe）**：训练轻量级残差执行器来探测VLA通用模型失败的区域。\n2.  **学习（Learn）**：使用混合策略收集轨迹，使其与通用模型的部署分布对齐，并捕获恢复行为。\n3.  **蒸馏（Distill）**：将收集到的轨迹通过标准SFT方法蒸馏回通用模型。", "result": "PLD在LIBERO任务上实现了接近饱和的99%成功率，在SimplerEnv中获得了超过50%的性能提升，并在真实世界的Franka和YAM机械臂操作任务上达到了100%的成功率。消融实验表明，残差探测和分布感知回放是收集部署对齐数据并提升已知和未知任务的关键。", "conclusion": "PLD框架通过收集与部署对齐的数据，有效提升了VLA模型在已知和未知任务上的性能，为VLA模型的自我改进提供了一条可扩展的路径。"}}
{"id": "2511.00341", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00341", "abs": "https://arxiv.org/abs/2511.00341", "authors": ["Mihir Sahasrabudhe"], "title": "Reversal Invariance in Autoregressive Language Models", "comment": "7 pages, theoretical note", "summary": "We formalize a structural property of the causal (autoregressive) language\nmodeling (CLM) objective: reversal invariance. Formally, the next-token\nprediction loss assigns identical likelihood to a corpus and its reversal,\nimplying that standard CLM pretraining is direction-blind. This symmetry\nexplains why models trained on reversed text can achieve comparable performance\nto those trained on forward text, despite the inherently time-asymmetric nature\nof human language and reasoning. We argue that this invariance represents a\nlimitation of current pretraining objectives rather than a benign artifact. If\nnatural language encodes directional dependencies - phonological,\nmorphological, or causal - a symmetric objective may fail to capture them. We\ntherefore propose viewing pretraining through the lens of temporal asymmetry,\nmotivating future work on loss functions and architectures that explicitly\nmodel the arrow of language while retaining standard language modeling\ncapacity.", "AI": {"tldr": "因果语言模型（CLM）目标函数具有反转不变性，即对文本及其反转赋予相同似然，这使其在预训练时对方向不敏感，限制了其捕捉语言中方向性依赖的能力。", "motivation": "人类语言和推理本质上具有时间不对称性，编码着方向性依赖（如语音、形态、因果关系）。然而，当前的因果语言模型预训练目标函数具有反转不变性，导致其对方向不敏感。研究者认为这种对称性是当前预训练目标的一个局限，可能未能充分捕捉语言的内在方向性。", "method": "本文通过形式化因果语言模型（CLM）目标函数的结构属性，即“反转不变性”，来分析其特性。具体而言，研究者分析了下一个词预测损失如何对语料库及其反转文本赋予相同的似然。", "result": "下一个词预测损失对语料库及其反转文本赋予相同的似然，这意味着标准的CLM预训练是方向盲的。这一对称性解释了为什么在反转文本上训练的模型可以达到与在正向文本上训练的模型相当的性能。", "conclusion": "CLM目标函数的反转不变性是当前预训练目标的一个局限。未来工作应通过引入明确建模语言方向（时间不对称性）的损失函数和架构，来克服这一限制，同时保持标准的语言建模能力。"}}
{"id": "2511.00492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00492", "abs": "https://arxiv.org/abs/2511.00492", "authors": ["Simon Giel", "James Hurrell", "Shreya Santra", "Ashutosh Mishra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU", "comment": "6 pages, 4 figures. Accepted at IEEE iSpaRo 2025", "summary": "In-Situ Resource Utilization (ISRU) is one of the key technologies for\nenabling sustainable access to the Moon. The ability to excavate lunar regolith\nis the first step in making lunar resources accessible and usable. This work\npresents the development of a bucket drum for the modular robotic system\nMoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made\nof PLA was manufactured to evaluate its efficiency through a series of sandbox\ntests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is\ncapable of continuous excavation at a rate of 777.54 kg/h with a normalized\nenergy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is\n172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of\nexcavated material. The obtained results demonstrate the successful\nimplementation of the concept. A key advantage of the developed tool is its\ncompatibility with the modular MoonBot robotic platform, which enables flexible\nand efficient mission planning. Further improvements may include the\nintegration of sensors and an autonomous control system to enhance the\nexcavation process.", "AI": {"tldr": "本文介绍了一种为模块化机器人系统MoonBot开发的斗式滚筒，用于月球风化层挖掘，并通过沙盒测试验证了其效率和与MoonBot的兼容性。", "motivation": "月球原位资源利用（ISRU）是实现月球可持续利用的关键技术之一，而挖掘月球风化层是使月球资源可及和可用的第一步。", "method": "研究人员开发了一个用于模块化机器人系统MoonBot的斗式滚筒，并制造了一个由PLA材料3D打印的原型。通过一系列沙盒测试评估了其挖掘效率。", "result": "该工具重4.8公斤，体积为14.06升。在连续挖掘模式下，挖掘速率为777.54公斤/小时，归一化能耗为0.022瓦时/公斤。在批处理模式下，挖掘速率为172.02公斤/小时，归一化能耗为0.86瓦时/公斤。该工具与模块化MoonBot机器人平台兼容。", "conclusion": "所获得的结果表明该概念的成功实现。该工具的一个主要优势是其与模块化MoonBot平台的兼容性，这使得任务规划更加灵活高效。未来的改进可能包括集成传感器和自主控制系统以增强挖掘过程。"}}
{"id": "2511.00512", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00512", "abs": "https://arxiv.org/abs/2511.00512", "authors": ["Suraj Kumar", "Andy Ruina"], "title": "Descriptive Model-based Learning and Control for Bipedal Locomotion", "comment": "8 pages, 15 figures", "summary": "Bipedal balance is challenging due to its multi-phase, hybrid nature and\nhigh-dimensional state space. Traditional balance control approaches for\nbipedal robots rely on low-dimensional models for locomotion planning and\nreactive control, constraining the full robot to behave like these simplified\nmodels. This involves tracking preset reference paths for the Center of Mass\nand upper body obtained through low-dimensional models, often resulting in\ninefficient walking patterns with bent knees. However, we observe that bipedal\nbalance is inherently low-dimensional and can be effectively described with\nsimple state and action descriptors in a low-dimensional state space. This\nallows the robot's motion to evolve freely in its high-dimensional state space,\nonly constraining its projection in the low-dimensional state space. In this\nwork, we propose a novel control approach that avoids prescribing a\nlow-dimensional model to the full model. Instead, our control framework uses a\ndescriptive model with the minimum degrees of freedom necessary to maintain\nbalance, allowing the remaining degrees of freedom to evolve freely in the\nhigh-dimensional space. This results in an efficient human-like walking gait\nand improved robustness.", "AI": {"tldr": "本文提出一种新型双足机器人平衡控制方法，通过使用最小自由度的描述性模型来维持平衡，同时允许高维自由度自由演化，从而实现高效、类人且更鲁棒的行走步态。", "motivation": "传统双足平衡控制依赖低维模型进行规划和反应性控制，这限制了机器人高维运动空间，导致效率低下且步态不自然（如膝盖弯曲）。研究观察到双足平衡本质上是低维的，但现有方法未能有效利用机器人高维空间的自由度。", "method": "提出一种新的控制框架，不将低维模型强加于完整机器人模型。相反，它使用一个描述性模型，该模型仅包含维持平衡所需的最小自由度，允许其余的自由度在高维空间中自由演化。这仅限制了其在低维状态空间中的投影。", "result": "该方法实现了高效的类人行走步态，并提高了鲁棒性。", "conclusion": "通过避免将低维模型强加于高维机器人，并采用描述性模型来管理平衡，同时赋予高维自由度以自由演化，可以显著改善双足机器人的行走效率和鲁棒性。"}}
{"id": "2511.00095", "categories": ["cs.CV", "cs.AI", "92C55", "I.2.10"], "pdf": "https://arxiv.org/pdf/2511.00095", "abs": "https://arxiv.org/abs/2511.00095", "authors": ["Jiaming Liu", "Dingwei Fan", "Junyong Zhao", "Chunlin Li", "Haipeng Si", "Liang Sun"], "title": "SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation", "comment": "2 Tables,5 Figures,16 Equations", "summary": "The anatomical structure segmentation of the spine and adjacent structures\nfrom computed tomography (CT) images is a key step for spinal disease diagnosis\nand treatment. However, the segmentation of CT images is impeded by low\ncontrast and complex vertebral boundaries. Although advanced models such as the\nSegment Anything Model (SAM) have shown promise in various segmentation tasks,\ntheir performance in spinal CT imaging is limited by high annotation\nrequirements and poor domain adaptability. To address these limitations, we\npropose SpinalSAM-R1, a multimodal vision-language interactive system that\nintegrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.\nSpecifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism\nto improve spine segmentation performance, and a semantics-driven interaction\nprotocol powered by DeepSeek-R1, enabling natural language-guided refinement.\nThe SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient\nadaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with\nCT images. Experimental results suggest that our method achieves superior\nsegmentation performance. Meanwhile, we develop a PyQt5-based interactive\nsoftware, which supports point, box, and text-based prompts. The system\nsupports 11 clinical operations with 94.3\\% parsing accuracy and sub-800 ms\nresponse times. The software is released on\nhttps://github.com/6jm233333/spinalsam-r1.", "AI": {"tldr": "本文提出了SpinalSAM-R1，一个多模态视觉-语言交互系统，用于脊柱CT图像分割。它结合了微调后的SAM模型和DeepSeek-R1，通过解剖学引导的注意力机制和语义驱动的交互协议，实现了卓越的分割性能和高效的自然语言指导精修。", "motivation": "脊柱CT图像的解剖结构分割对于脊柱疾病的诊断和治疗至关重要。然而，CT图像的低对比度和复杂的椎体边界给分割带来了挑战。尽管SAM等先进模型有潜力，但其在脊柱CT成像中的表现受限于高标注需求和差的领域适应性。", "method": "本文提出了SpinalSAM-R1系统，它集成了经过LoRA高效微调的SAM模型和DeepSeek-R1。具体方法包括：引入解剖学引导的注意力机制以提高脊柱分割性能；利用DeepSeek-R1驱动的语义交互协议实现自然语言指导的精修。此外，还开发了一个基于PyQt5的交互式软件，支持点、框和文本提示。", "result": "实验结果表明，SpinalSAM-R1在脊柱解剖结构CT图像分割上取得了卓越的性能。开发的交互式软件支持11种临床操作，解析准确率达到94.3%，响应时间低于800毫秒。", "conclusion": "SpinalSAM-R1通过结合微调后的SAM和DeepSeek-R1，引入解剖学引导的注意力机制和语义驱动的交互，有效克服了脊柱CT图像分割的挑战，实现了优越的分割性能，并提供了一个高效实用的交互式临床工具。"}}
{"id": "2511.00382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00382", "abs": "https://arxiv.org/abs/2511.00382", "authors": ["Mina Taraghi", "Yann Pequignot", "Amin Nikanjam", "Mohamed Amine Merzouk", "Foutse Khomh"], "title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs", "comment": null, "summary": "Organizations are increasingly adopting and adapting Large Language Models\n(LLMs) hosted on public repositories such as HuggingFace. Although these\nadaptations often improve performance on specialized downstream tasks, recent\nevidence indicates that they can also degrade a model's safety or fairness.\nSince different fine-tuning techniques may exert distinct effects on these\ncritical dimensions, this study undertakes a systematic assessment of their\ntrade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,\nIA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model\nfamilies (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235\nfine-tuned variants are evaluated across eleven safety hazard categories and\nnine demographic fairness dimensions. The results show that adapter-based\napproaches (LoRA, IA3) tend to improve safety scores and are the least\ndisruptive to fairness, retaining higher accuracy and lower bias scores. In\ncontrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce\nsafety and cause larger fairness regressions, with decreased accuracy and\nincreased bias. Alignment shifts are strongly moderated by base model type:\nLLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest\nsafety decline, and Mistral, which is released without an internal moderation\nlayer, displays the greatest variance. Improvements in safety do not\nnecessarily translate into improvements in fairness, and no single\nconfiguration optimizes all fairness metrics simultaneously, indicating an\ninherent trade-off between these objectives. These findings suggest a practical\nguideline for safety-critical deployments: begin with a well-aligned base\nmodel, favour adapter-based PEFT, and conduct category-specific audits of both\nsafety and fairness.", "AI": {"tldr": "本研究系统评估了四种PEFT方法（LoRA, IA3, Prompt-Tuning, P-Tuning）对四种指令微调LLM模型（Llama-3-8B, Qwen2.5-7B, Mistral-7B, Gemma-7B）的安全性和公平性的影响。结果显示，基于适配器的方法（LoRA, IA3）通常能提高安全性并对公平性影响最小，而基于提示的方法（Prompt-Tuning, P-Tuning）则会降低安全性和公平性。基础模型类型对这些变化有显著调节作用。", "motivation": "组织日益采用并调整LLM，但微调在提高特定任务性能的同时，也可能损害模型的安全性或公平性。由于不同的微调技术可能对这些关键维度产生不同的影响，因此有必要系统评估它们的权衡。", "method": "研究将四种广泛使用的参数高效微调（PEFT）方法（LoRA, IA3, Prompt-Tuning, P-Tuning）应用于四种指令微调模型家族（Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, Gemma-7B）。总共评估了235个微调变体，涵盖十一个安全危害类别和九个人口统计公平性维度。", "result": "基于适配器的方法（LoRA, IA3）倾向于提高安全得分，对公平性的破坏最小，保持了较高的准确性和较低的偏差。相比之下，基于提示的方法（Prompt-Tuning和P-Tuning）通常会降低安全性，并导致更大的公平性退步，准确性下降且偏差增加。基础模型类型对对齐偏移有强烈的调节作用：LLaMA保持稳定，Qwen略有提升，Gemma安全性下降最严重，而Mistral（发布时没有内部审核层）显示出最大的方差。安全性的提高不一定能转化为公平性的提高，并且没有单一配置能同时优化所有公平性指标，表明这些目标之间存在固有的权衡。", "conclusion": "对于安全性关键的部署，建议从一个对齐良好的基础模型开始，优先选择基于适配器的PEFT方法，并对安全性和公平性进行特定类别的审计。"}}
{"id": "2511.00623", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.00623", "abs": "https://arxiv.org/abs/2511.00623", "authors": ["Junhong Liu", "Lanxin Du", "Yujia Li", "Rong-Peng Liu", "Fei Teng", "Francis Yunhe Hou"], "title": "Adaptive Federated Learning to Optimize the MultiCast flows in Data Centers", "comment": null, "summary": "Data centers play an increasingly critical role in societal digitalization,\nyet their rapidly growing energy demand poses significant challenges for\nsustainable operation. To enhance the energy efficiency of geographically\ndistributed data centers, this paper formulates a multi-period optimization\nmodel that captures the interdependence of electricity, heat, and data flows.\nThe optimization of such multicast flows inherently involves mixed-integer\nformulations and the access to proprietary or sensitive datasets, which\ncorrespondingly exacerbate computational complexity and raise data-privacy\nconcerns. To address these challenges, an adaptive federated\nlearning-to-optimization approach is proposed, accounting for the heterogeneity\nof datasets across distributed data centers. To safeguard privacy, cryptography\ntechniques are leveraged in both the learning and optimization processes. A\nmodel acceptance criterion with convergence guarantee is developed to improve\nlearning performance and filter out potentially contaminated data, while a\nverifiable double aggregation mechanism is further proposed to simultaneously\nensure privacy and integrity of shared data during optimization. Theoretical\nanalysis and numerical simulations demonstrate that the proposed approach\npreserves the privacy and integrity of shared data, achieves near-optimal\nperformance, and exhibits high computational efficiency, making it suitable for\nlarge-scale data center optimization under privacy constraints.", "AI": {"tldr": "本文提出了一种自适应联邦学习-优化方法，结合密码学技术，用于在保护隐私的前提下，提升地理分布式数据中心的能源效率，通过优化电力、热力和数据流的互联互通。", "motivation": "数据中心能源需求快速增长，对可持续运营构成挑战。同时，分布式数据中心的优化涉及混合整数公式和专有或敏感数据集，导致计算复杂性高和数据隐私问题。", "method": "研究提出了一个多周期优化模型，捕获电力、热力和数据流的相互依赖性。为解决计算复杂性和隐私问题，引入了自适应联邦学习-优化方法，并利用密码学技术在学习和优化过程中保护隐私。此外，开发了具有收敛保证的模型接受标准，并提出了可验证的双重聚合机制来确保共享数据的隐私和完整性。", "result": "理论分析和数值模拟表明，所提出的方法能够保护共享数据的隐私和完整性，实现接近最优的性能，并展现出高计算效率。", "conclusion": "该方法适用于在隐私约束下进行大规模数据中心优化，为解决分布式数据中心能效提升与隐私保护的矛盾提供了有效途径。"}}
{"id": "2511.00516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00516", "abs": "https://arxiv.org/abs/2511.00516", "authors": ["Peiyi Wang", "Paul A. M. Lefeuvre", "Shangwei Zou", "Zhenwei Ni", "Daniela Rus", "Cecilia Laschi"], "title": "Adaptive and Multi-object Grasping via Deformable Origami Modules", "comment": null, "summary": "Soft robotics gripper have shown great promise in handling fragile and\ngeometrically complex objects. However, most existing solutions rely on bulky\nactuators, complex control strategies, or advanced tactile sensing to achieve\nstable and reliable grasping performance. In this work, we present a\nmulti-finger hybrid gripper featuring passively deformable origami modules that\ngenerate constant force and torque output. Each finger composed of parallel\norigami modules is driven by a 1-DoF actuator mechanism, enabling passive shape\nadaptability and stable grasping force without active sensing or feedback\ncontrol. More importantly, we demonstrate an interesting capability in\nsimultaneous multi-object grasping, which allows stacked objects of varied\nshape and size to be picked, transported and placed independently at different\nstates, significantly improving manipulation efficiency compared to\nsingle-object grasping. These results highlight the potential of origami-based\ncompliant structures as scalable modules for adaptive, stable and efficient\nmulti-object manipulation in domestic and industrial pick-and-place scenarios.", "AI": {"tldr": "该研究提出一种多指混合夹持器，利用被动可变形折纸模块实现恒定力和扭矩输出，无需复杂控制和传感即可稳定适应性抓取，并展示了同时抓取多个不同物体的高效能力。", "motivation": "现有软体机器人夹持器在处理易碎和复杂几何形状物体时，常依赖笨重执行器、复杂控制策略或先进触觉传感来实现稳定可靠的抓取性能。", "method": "该研究设计了一种多指混合夹持器，每根手指由并联折纸模块组成，通过一个1自由度执行机构驱动。折纸模块能够被动变形，产生恒定的力和扭矩输出，从而实现被动形状适应性和稳定的抓取力，无需主动传感或反馈控制。", "result": "该夹持器实现了被动形状适应性和稳定的抓取力，且无需主动传感或反馈控制。更重要的是，它展示了同时抓取多个物体的能力，可以独立拾取、运输和放置不同形状和尺寸的堆叠物体，显著提高了操作效率。", "conclusion": "研究结果强调了基于折纸的柔顺结构作为可扩展模块的潜力，可用于家庭和工业拾放场景中自适应、稳定和高效的多物体操作。"}}
{"id": "2511.00639", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00639", "abs": "https://arxiv.org/abs/2511.00639", "authors": ["Taulant Kerci", "Federico Milano"], "title": "Frequency Quality Assessment of GFM and GFL Converters and Synchronous Condensers", "comment": null, "summary": "This paper compares the impact of different conventional and emerging\ntechnologies and control strategies on frequency quality. We study, in\nparticular, the long-term dynamic performance of grid-forming (GFM) and\ngrid-following (GFL) inverter-based resources (IBRs) as well as conventional\nsynchronous machines. Extensive simulations and several realistic scenarios\nconsider both short-term and long-term aspects of frequency quality. It is\nshown that, while overall GFM IBRs significantly improve frequency quality, a\ncombination of GFL IBRs providing frequency support such as wind and batteries,\nand synchronous condensers, might be enough to meet similar frequency quality\nstandards. Another result of the paper is that the need for automatic\ngeneration control (AGC) becomes less clear in GFM IBR-dominated grids from a\nfrequency quality perspective.", "AI": {"tldr": "本文比较了传统和新兴技术（如并网型和随网型逆变器）以及控制策略对电网频率质量的影响，并探讨了在并网型逆变器主导的电网中自动发电控制的必要性。", "motivation": "研究不同常规和新兴技术（尤其是基于逆变器的资源，IBRs）及其控制策略对电网频率质量的长期动态性能影响。", "method": "采用广泛的仿真和多个实际场景，考虑了频率质量的短期和长期方面。", "result": "研究表明，并网型逆变器（GFM IBRs）显著改善频率质量。然而，提供频率支持的随网型逆变器（GFL IBRs，如风能和电池）与同步补偿器相结合，可能足以达到相似的频率质量标准。此外，从频率质量角度看，在并网型逆变器主导的电网中，自动发电控制（AGC）的需求变得不那么明确。", "conclusion": "并网型逆变器能显著提升频率质量，但其他组合方案也能达到相似标准。在并网型逆变器主导的电网中，自动发电控制的作用需重新评估。"}}
{"id": "2511.00595", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00595", "abs": "https://arxiv.org/abs/2511.00595", "authors": ["Feng Guo", "Luis D. Couto", "Guillaume Thenaisie"], "title": "Efficiency and Optimality in Electrochemical Battery Model Parameter Identification: A Comparative Study of Estimation Techniques", "comment": "Accepted and published in the Proceedings of the 2024 10th\n  International Conference on Optimization and Applications (ICOA), IEEE, 2024.\n  Copyright 2024 IEEE. This is the author's accepted manuscript; the final\n  version is available at IEEE Xplore (DOI: 10.1109/ICOA62581.2024.10754301)", "summary": "Parameter identification for electrochemical battery models has always been\nchallenging due to the multitude of parameters involved, most of which cannot\nbe directly measured. This paper evaluates the efficiency and optimality of\nthree widely-used parameter identification methods for electrochemical battery\nmodels: Least Squares Method (LS), Particle Swarm Optimization (PSO), and\nGenetic Algorithm (GA). Therefore, a Single Particle Model (SPM) of a battery\nwas developed and discretized. Battery parameter grouping was then performed to\nreduce the number of parameters required. Using a set of parameters previously\nidentified from a real battery as a benchmark, we generated fitting and\nvalidation datasets to assess the methods' runtime and accuracy. The\ncomparative analysis reveals that PSO outperforms the other methods in terms of\naccuracy and stability, making it highly effective for parameter identification\nwhen there is no prior knowledge of the battery's internal parameters. In\ncontrast, LS is better suited for minor adjustments in parameters, particularly\nfor aging batteries, whereas GA lags behind in both computational efficiency\nand optimality with respect to PSO.", "AI": {"tldr": "本文评估了三种常用参数辨识方法（LS、PSO、GA）在电化学电池模型中的效率和最优性。结果显示，在缺乏先验知识时，PSO在准确性和稳定性方面表现最佳；LS适用于参数微调；GA在计算效率和最优性上均落后于PSO。", "motivation": "电化学电池模型的参数辨识极具挑战性，因为涉及大量参数且大多数无法直接测量。", "method": "研究开发并离散化了一个单粒子模型（SPM），并通过参数分组减少了参数数量。使用从真实电池获取的参数作为基准，生成了拟合和验证数据集，以评估LS、PSO和GA这三种方法的运行时间和准确性。", "result": "比较分析表明，在没有电池内部参数先验知识的情况下，PSO在准确性和稳定性方面优于其他方法，非常适合参数辨识。相比之下，LS更适合参数的微调，特别是针对老化电池。而GA在计算效率和最优性方面均落后于PSO。", "conclusion": "PSO是电化学电池模型参数辨识的高效且最优方法，尤其是在缺乏先验知识时。LS适用于参数的微调，而GA在此任务中表现不佳。"}}
{"id": "2511.00371", "categories": ["cs.CL", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00371", "abs": "https://arxiv.org/abs/2511.00371", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs", "comment": "25 pages, 2 tables, 13 figures", "summary": "In Socratic debugging, instructors guide students towards identifying and\nfixing a bug on their own, instead of providing the bug fix directly. Most\nnovice programmer bugs are caused by programming misconceptions, namely false\nbeliefs about a programming concept. In this context, Socratic debugging can be\nformulated as a guided Reasoning Trajectory (RT) leading to a statement about\nthe program behavior that contradicts the bug-causing misconception. Upon\nreaching this statement, the ensuing cognitive dissonance leads the student to\nfirst identify and then update their false belief. In this paper, we introduce\nthe task of reasoning trajectory generation, together with a dataset of\ndebugging problems manually annotated with RTs. We then describe LLM-based\nsolutions for generating RTs and Socratic conversations that are anchored on\nthem. A large-scale LLM-as-judge evaluation shows that frontier models can\ngenerate up to 91% correct reasoning trajectories and 98.7% valid conversation\nturns.", "AI": {"tldr": "该研究提出并解决了推理轨迹（RT）生成任务，利用大型语言模型（LLM）生成苏格拉底式调试的RT和对话，并通过“LLM作为评判者”的评估展示了高准确率。", "motivation": "大多数新手程序员的错误源于编程误解。苏格拉底式调试能引导学生自主识别并纠正这些误解，而不是直接提供答案。本研究旨在通过自动化生成推理轨迹来支持这种有效的教学方法。", "method": "将苏格拉底式调试形式化为引导学生识别并纠正导致错误的误解的推理轨迹（RT）。引入了推理轨迹生成任务，并构建了一个手动标注RT的调试问题数据集。开发了基于LLM的解决方案来生成RT以及基于这些RT的苏格拉底式对话。", "result": "通过大规模的“LLM作为评判者”评估，前沿模型能够生成高达91%的正确推理轨迹和98.7%的有效对话回合。", "conclusion": "大型语言模型在生成苏格拉底式调试所需的推理轨迹和对话方面表现出强大的能力，为支持学生自主调试提供了有效工具。"}}
{"id": "2511.00098", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00098", "abs": "https://arxiv.org/abs/2511.00098", "authors": ["Nils Porsche", "Flurin Müller-Diesing", "Sweta Banerjee", "Miguel Goncalves", "Marc Aubreville"], "title": "A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning", "comment": null, "summary": "Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging\nmodality that can be used for in-situ, in-vivo imaging and the microstructural\nanalysis of mucous structures. The diagnosis using CLE is, however, complicated\nby images being hard to interpret for non-experienced physicians. Utilizing\nmachine learning as an augmentative tool would hence be beneficial, but is\ncomplicated by the shortage of histopathology-correlated CLE imaging sequences\nwith respect to the plurality of patterns in this domain, leading to\noverfitting of machine learning models. To overcome this, self-supervised\nlearning (SSL) can be employed on larger unlabeled datasets. CLE is a\nvideo-based modality with high inter-frame correlation, leading to a\nnon-stratified data distribution for SSL training. In this work, we propose a\nfilter functionality on CLE video sequences to reduce the dataset redundancy in\nSSL training and improve SSL training convergence and training efficiency. We\nuse four state-of-the-art baseline networks and a SSL teacher-student network\nwith a vision transformer small backbone for the evaluation. These networks\nwere evaluated on downstream tasks for a sinonasal tumor dataset and a squamous\ncell carcinoma of the skin dataset. On both datasets, we found the highest test\naccuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both\nconsiderably outperforming their non-SSL baselines. Our results show that SSL\nis an effective method for CLE pretraining. Further, we show that our proposed\nCLE video filter can be utilized to improve training efficiency in\nself-supervised scenarios, resulting in a reduction of 67% in training time.", "AI": {"tldr": "该研究提出了一种针对共聚焦激光内窥镜（CLE）视频序列的过滤功能，以减少自监督学习（SSL）训练中的数据冗余，从而提高训练效率和模型性能，最终在肿瘤分类任务中取得了更高的准确率。", "motivation": "共聚焦激光内窥镜（CLE）图像对非经验医生来说难以解读，而机器学习作为辅助工具可从中受益。然而，缺乏组织病理学相关的CLE图像序列导致模型过拟合。自监督学习（SSL）可利用大量未标记数据，但CLE视频的高度帧间相关性导致数据分布非分层，影响SSL训练。", "method": "本研究提出了一种过滤CLE视频序列的方法，以减少SSL训练中的数据集冗余，从而改善训练收敛性和效率。研究使用了四种最先进的基线网络和一个带有视觉Transformer小骨干的SSL师生网络进行评估。这些网络在鼻窦肿瘤数据集和皮肤鳞状细胞癌数据集的下游任务上进行了测试。", "result": "在两个数据集上，经过过滤的SSL预训练模型均获得了最高的测试准确率，分别为67.48%和73.52%，显著优于非SSL基线模型。结果表明，所提出的CLE视频过滤器可以将自监督场景下的训练时间减少67%。", "conclusion": "自监督学习是CLE预训练的有效方法。本研究提出的CLE视频过滤器可以提高自监督场景下的训练效率。"}}
{"id": "2511.00416", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00416", "abs": "https://arxiv.org/abs/2511.00416", "authors": ["Yiwei Zha", "Rui Min", "Shanu Sushmita"], "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "comment": null, "summary": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.", "AI": {"tldr": "AI生成文本检测器在直接LLM输出上表现良好，但对迭代释义内容失效。研究揭示了迭代释义的“洗稿”机制，并引入PADBen基准，发现当前检测器在作者归属混淆问题上表现不佳，需要新的检测方法。", "motivation": "现有AI生成文本（AIGT）检测器对直接LLM输出准确率超过90%，但在迭代释义内容面前却灾难性失败。本研究旨在探究为何这种本身是AI生成的迭代释义文本能逃避AIGT检测系统，并解决由此引发的作者归属混淆和抄袭规避问题。", "method": "1. 通过内在机制分析，揭示迭代释义如何创建一个“洗稿区域”，其特点是语义位移但生成模式得以保留，并归纳出归因混淆和抄袭规避两种攻击类别。2. 引入PADBen基准，这是首个系统评估检测器对这两种释义攻击场景鲁棒性的基准。3. PADBen包含一个五类型文本分类法，涵盖从原始内容到深度洗稿文本的全过程，以及五项跨句对和单句挑战的渐进式检测任务。4. 评估了11种最先进的检测器。", "result": "1. 迭代释义创建了一个“洗稿区域”，其特征是语义位移但保留了生成模式。2. 检测器在抄袭规避问题上表现成功，但在作者归属混淆问题上失败，存在关键不对称性。3. 现有检测方法无法有效处理中间“洗稿区域”。", "conclusion": "当前检测方法无法有效处理迭代释义文本，尤其是在作者归属混淆方面。这表明需要超越现有语义和风格区分方法的检测架构上的根本性进步，以应对这种中间“洗稿区域”带来的挑战。"}}
{"id": "2511.00103", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00103", "abs": "https://arxiv.org/abs/2511.00103", "authors": ["Rotem Ezra", "Hedi Zisling", "Nimrod Berman", "Ilan Naiman", "Alexey Gorkor", "Liran Nochumsohn", "Eliya Nachmani", "Omri Azencot"], "title": "FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video", "comment": null, "summary": "Diffusion models have become state-of-the-art generative models for images,\naudio, and video, yet enabling fine-grained controllable generation, i.e.,\ncontinuously steering specific concepts without disturbing unrelated content,\nremains challenging. Concept Sliders (CS) offer a promising direction by\ndiscovering semantic directions through textual contrasts, but they require\nper-concept training and architecture-specific fine-tuning (e.g., LoRA),\nlimiting scalability to new modalities. In this work we introduce FreeSliders,\na simple yet effective approach that is fully training-free and\nmodality-agnostic, achieved by partially estimating the CS formula during\ninference. To support modality-agnostic evaluation, we extend the CS benchmark\nto include both video and audio, establishing the first suite for fine-grained\nconcept generation control with multiple modalities. We further propose three\nevaluation properties along with new metrics to improve evaluation quality.\nFinally, we identify an open problem of scale selection and non-linear\ntraversals and introduce a two-stage procedure that automatically detects\nsaturation points and reparameterizes traversal for perceptually uniform,\nsemantically meaningful edits. Extensive experiments demonstrate that our\nmethod enables plug-and-play, training-free concept control across modalities,\nimproves over existing baselines, and establishes new tools for principled\ncontrollable generation. An interactive presentation of our benchmark and\nmethod is available at: https://azencot-group.github.io/FreeSliders/", "AI": {"tldr": "FreeSliders 提出了一种无需训练、与模态无关的方法，通过在推理时部分估计Concept Sliders公式，实现了扩散模型中细粒度可控生成，并扩展了评估基准和指标。", "motivation": "扩散模型在生成方面表现出色，但实现细粒度可控生成（在不干扰无关内容的情况下连续操纵特定概念）仍然具有挑战性。现有方法（如Concept Sliders）需要针对每个概念进行训练和架构特定的微调，限制了其在新模态上的可扩展性。", "method": "FreeSliders 通过在推理时部分估计Concept Sliders公式，实现了完全无需训练且与模态无关。为了支持模态无关评估，该工作将CS基准扩展到视频和音频，并提出了三个新的评估属性和指标。此外，它还引入了一个两阶段过程，用于自动检测饱和点并重新参数化遍历，以实现感知均匀、语义有意义的编辑。", "result": "FreeSliders 方法实现了跨模态的即插即用、无需训练的概念控制，优于现有基线，并为原则性的可控生成建立了新工具。实验证明了其有效性。", "conclusion": "FreeSliders 提供了一种简单而有效的方法，实现了扩散模型中无需训练和与模态无关的细粒度概念控制。它解决了现有方法的扩展性问题，并通过改进的评估工具和方法，推动了可控生成领域的发展。"}}
{"id": "2511.00424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00424", "abs": "https://arxiv.org/abs/2511.00424", "authors": ["Ashutosh Anshul", "Gumpili Sai Pranav", "Mohammad Zia Ur Rehman", "Nagendra Kumar"], "title": "A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method", "comment": null, "summary": "The recent coronavirus disease (Covid-19) has become a pandemic and has\naffected the entire globe. During the pandemic, we have observed a spike in\ncases related to mental health, such as anxiety, stress, and depression.\nDepression significantly influences most diseases worldwide, making it\ndifficult to detect mental health conditions in people due to unawareness and\nunwillingness to consult a doctor. However, nowadays, people extensively use\nonline social media platforms to express their emotions and thoughts. Hence,\nsocial media platforms are now becoming a large data source that can be\nutilized for detecting depression and mental illness. However, existing\napproaches often overlook data sparsity in tweets and the multimodal aspects of\nsocial media. In this paper, we propose a novel multimodal framework that\ncombines textual, user-specific, and image analysis to detect depression among\nsocial media users. To provide enough context about the user's emotional state,\nwe propose (i) an extrinsic feature by harnessing the URLs present in tweets\nand (ii) extracting textual content present in images posted in tweets. We also\nextract five sets of features belonging to different modalities to describe a\nuser. Additionally, we introduce a Deep Learning model, the Visual Neural\nNetwork (VNN), to generate embeddings of user-posted images, which are used to\ncreate the visual feature vector for prediction. We contribute a curated\nCovid-19 dataset of depressed and non-depressed users for research purposes and\ndemonstrate the effectiveness of our model in detecting depression during the\nCovid-19 outbreak. Our model outperforms existing state-of-the-art methods over\na benchmark dataset by 2%-8% and produces promising results on the Covid-19\ndataset. Our analysis highlights the impact of each modality and provides\nvaluable insights into users' mental and emotional states.", "AI": {"tldr": "本文提出了一种多模态框架，结合文本、用户特定信息和图像分析，利用社交媒体数据检测新冠疫情期间用户的抑郁症，并通过引入外部特征和深度学习模型解决了数据稀疏性问题，表现优于现有方法。", "motivation": "新冠疫情导致心理健康问题（如抑郁症）激增，但由于缺乏意识和不愿就医，抑郁症难以被发现。社交媒体是人们表达情感的重要平台，可作为检测心理疾病的数据源。然而，现有方法常忽略推文中的数据稀疏性和社交媒体的多模态特性。", "method": "本文提出了一种新颖的多模态框架，结合文本、用户特定信息和图像分析来检测社交媒体用户的抑郁症。为提供用户情感状态的足够上下文，方法包括：(i) 利用推文中的URL提取外部特征；(ii) 提取推文图片中的文本内容。此外，还提取了五组属于不同模态的特征来描述用户。引入了一个深度学习模型——视觉神经网络（VNN），用于生成用户发布图片的嵌入，进而创建视觉特征向量进行预测。研究还贡献了一个专门针对新冠疫情的抑郁和非抑郁用户数据集。", "result": "该模型能有效检测新冠疫情爆发期间的抑郁症。在基准数据集上，模型性能比现有最先进方法高出2%-8%，并在新冠疫情数据集上取得了有希望的结果。分析还强调了每种模态的影响，并提供了关于用户心理和情感状态的宝贵见解。", "conclusion": "所提出的多模态框架通过结合文本、用户特定信息和图像分析，并引入外部特征和深度学习模型处理数据稀疏性，在新冠疫情期间的社交媒体抑郁症检测中表现出卓越的有效性，并优于现有技术。"}}
{"id": "2511.00421", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00421", "abs": "https://arxiv.org/abs/2511.00421", "authors": ["Naoto Iwase", "Hiroki Okuyama", "Junichiro Iwasawa"], "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "comment": null, "summary": "Large language models (LLMs) show increasing promise in medical applications,\nbut their ability to detect and correct errors in clinical texts -- a\nprerequisite for safe deployment -- remains under-evaluated, particularly\nbeyond English. We introduce MedRECT, a cross-lingual benchmark\n(Japanese/English) that formulates medical error handling as three subtasks:\nerror detection, error localization (sentence extraction), and error\ncorrection. MedRECT is built with a scalable, automated pipeline from the\nJapanese Medical Licensing Examinations (JMLE) and a curated English\ncounterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with\ncomparable error/no-error balance. We evaluate 9 contemporary LLMs spanning\nproprietary, open-weight, and reasoning families. Key findings: (i) reasoning\nmodels substantially outperform standard architectures, with up to 13.5%\nrelative improvement in error detection and 51.0% in sentence extraction; (ii)\ncross-lingual evaluation reveals 5-10% performance gaps from English to\nJapanese, with smaller disparities for reasoning models; (iii) targeted LoRA\nfine-tuning yields asymmetric improvements in error correction performance\n(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;\nand (iv) our fine-tuned model exceeds human expert performance on structured\nmedical error correction tasks. To our knowledge, MedRECT is the first\ncomprehensive cross-lingual benchmark for medical error correction, providing a\nreproducible framework and resources for developing safer medical LLMs across\nlanguages.", "AI": {"tldr": "该研究引入了MedRECT，一个跨语言（日语/英语）基准，用于评估大型语言模型（LLMs）在医学文本中的错误检测、定位和纠正能力。结果显示推理模型表现出色，精调可显著提升性能，甚至超越人类专家，为开发更安全的跨语言医学LLMs提供了框架和资源。", "motivation": "大型语言模型（LLMs）在医疗应用中展现出巨大潜力，但其在临床文本中检测和纠正错误的能力（安全部署的先决条件）尚未得到充分评估，尤其是在英语以外的语言中。", "method": "研究引入了MedRECT，一个跨语言（日语/英语）基准，将医学错误处理任务分解为错误检测、错误定位（句子提取）和错误纠正三个子任务。MedRECT通过自动化流程构建，数据来源于日本医师执照考试（JMLE）及其英文对应版本，生成了MedRECT-ja（663篇文本）和MedRECT-en（458篇文本）。研究评估了9种主流LLMs（包括专有、开源和推理模型），并使用了有针对性的LoRA精调方法。", "result": "(i) 推理模型显著优于标准架构，在错误检测方面相对提升高达13.5%，在句子提取方面相对提升高达51.0%；(ii) 跨语言评估显示，从英语到日语存在5-10%的性能差距，但推理模型的差距较小；(iii) 有针对性的LoRA精调在错误纠正性能上产生了不对称的提升（日语：+0.078，英语：+0.168），同时保留了推理能力；(iv) 精调后的模型在结构化医学错误纠正任务上超越了人类专家表现。", "conclusion": "MedRECT是首个全面的医学错误纠正跨语言基准，为开发更安全的跨语言医学LLMs提供了可复现的框架和资源。"}}
{"id": "2511.00457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00457", "abs": "https://arxiv.org/abs/2511.00457", "authors": ["Chunyu Wei", "Wenji Hu", "Xingjia Hao", "Xin Wang", "Yifan Yang", "Yueguo Chen", "Yang Tian", "Yunhai Wang"], "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "comment": null, "summary": "Large Language Models (LLMs) face significant limitations when applied to\nlarge-scale graphs, struggling with context constraints and inflexible\nreasoning. We present GraphChain, a framework that enables LLMs to analyze\ncomplex graphs through dynamic sequences of specialized tools, mimicking human\nexploratory intelligence. Our approach introduces two key innovations: (1)\nProgressive Graph Distillation, a reinforcement learning mechanism that\ngenerates optimized tool sequences balancing task relevance with information\ncompression, and (2) Structure-aware Test-Time Adaptation, which efficiently\ntailors tool selection strategies to diverse graph topologies using spectral\nproperties and lightweight adapters without costly retraining. Experiments show\nGraphChain significantly outperforms prior methods, enabling scalable and\nadaptive LLM-driven graph analysis.", "AI": {"tldr": "GraphChain是一个框架，通过动态序列的专业工具，使大型语言模型（LLMs）能够分析复杂图，克服了LLMs在处理大规模图时的上下文限制和推理僵化问题。", "motivation": "大型语言模型（LLMs）在应用于大规模图时面临显著限制，例如上下文约束和推理不灵活。", "method": "GraphChain框架引入了两项关键创新：1) 渐进式图蒸馏（Progressive Graph Distillation），一种强化学习机制，用于生成优化工具序列，平衡任务相关性和信息压缩；2) 结构感知测试时适应（Structure-aware Test-Time Adaptation），它利用谱特性和轻量级适配器，无需昂贵的重新训练，即可有效地根据不同的图拓扑调整工具选择策略。", "result": "实验表明，GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。", "conclusion": "GraphChain提供了一个强大的框架，通过其创新的渐进式图蒸馏和结构感知测试时适应机制，使LLMs能够对复杂图进行可扩展和自适应的分析，解决了现有方法的局限性。"}}
{"id": "2511.00659", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00659", "abs": "https://arxiv.org/abs/2511.00659", "authors": ["Wang Chen", "Heye Huang", "Ke Ma", "Hangyu Li", "Shixiao Liang", "Hang Zhou", "Xiaopeng Li"], "title": "Unveiling Uniform Shifted Power Law in Stochastic Human and Autonomous Driving Behavior", "comment": null, "summary": "Accurately simulating rare but safety-critical driving behaviors is essential\nfor the evaluation and certification of autonomous vehicles (AVs). However,\ncurrent models often fail to reproduce realistic collision rates when\ncalibrated on real-world data, largely due to inadequate representation of\nlong-tailed behavioral distributions. Here, we uncover a simple yet unifying\nshifted power law that robustly characterizes the stochasticity of both\nhuman-driven vehicle (HV) and AV behaviors, especially in the long-tail regime.\nThe model adopts a parsimonious analytical form with only one or two\nparameters, enabling efficient calibration even under data sparsity. Analyzing\nlarge-scale, micro-level trajectory data from global HV and AV datasets, the\nshifted power law achieves an average R2 of 0.97 and a nearly identical tail\ndistribution, uniformly fits both frequent behaviors and rare safety-critical\ndeviations, significantly outperforming existing Gaussian-based baselines. When\nintegrated into an agent-based traffic simulator, it enables forward-rolling\nsimulations that reproduce realistic crash patterns for both HVs and AVs,\nachieving rates consistent with real-world statistics and improving the\nfidelity of safety assessment without post hoc correction. This discovery\noffers a unified and data-efficient foundation for modeling high-risk behavior\nand improves the fidelity of simulation-based safety assessments for mixed\nAV/HV traffic. The shifted power law provides a promising path toward\nsimulation-driven validation and global certification of AV technologies.", "AI": {"tldr": "本文提出了一种“移位幂律”模型，能够准确表征人类驾驶和自动驾驶车辆行为的长尾分布，尤其是在罕见但关键的安全行为方面，显著提高了自动驾驶车辆安全评估模拟的真实性。", "motivation": "当前模型在模拟自动驾驶车辆（AVs）的罕见但关键的安全驾驶行为时，无法准确重现真实的碰撞率，主要原因是未能充分表示长尾行为分布，这阻碍了AVs的评估和认证。", "method": "研究发现并提出了一种简洁的“移位幂律”模型，该模型仅需一到两个参数，即使在数据稀疏时也能高效校准。通过分析大规模微观轨迹数据，将此模型集成到基于智能体的交通模拟器中进行前向滚动模拟，以验证其在重现真实碰撞模式方面的能力。", "result": "“移位幂律”模型能够稳健地表征人类驾驶车辆（HV）和AV行为的随机性，特别是在长尾区域。该模型在拟合频繁行为和罕见安全关键偏差方面表现出色，平均R2达到0.97，尾部分布几乎相同，并显著优于现有基于高斯分布的基线模型。集成到模拟器后，它能重现与真实世界统计数据一致的HV和AV碰撞模式，提高了安全评估的保真度，且无需事后修正。", "conclusion": "“移位幂律”模型为高风险行为建模提供了一个统一且数据高效的基础，显著提高了混合AV/HV交通中基于模拟的安全评估的保真度。这项发现为自动驾驶技术的模拟驱动验证和全球认证提供了一条有前景的途径。"}}
{"id": "2511.00107", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00107", "abs": "https://arxiv.org/abs/2511.00107", "authors": ["Piyushkumar Patel"], "title": "AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency", "comment": null, "summary": "Text to video generation has emerged as a critical frontier in generative\nartificial intelligence, yet existing approaches struggle with maintaining\ntemporal consistency, compositional understanding, and fine grained control\nover visual narratives. We present MOVAI (Multimodal Original Video AI), a\nnovel hierarchical framework that integrates compositional scene understanding\nwith temporal aware diffusion models for high fidelity text to video synthesis.\nOur approach introduces three key innovations: (1) a Compositional Scene Parser\n(CSP) that decomposes textual descriptions into hierarchical scene graphs with\ntemporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that\nensures coherent motion dynamics across frames while preserving spatial\ndetails, and (3) a Progressive Video Refinement (PVR) module that iteratively\nenhances video quality through multi-scale temporal reasoning. Extensive\nexperiments on standard benchmarks demonstrate that MOVAI achieves\nstate-of-the-art performance, improving video quality metrics by 15.3% in\nLPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing\nmethods. Our framework shows particular strength in generating complex\nmulti-object scenes with realistic temporal dynamics and fine-grained semantic\ncontrol.", "AI": {"tldr": "MOVAI是一个新的分层框架，通过结合组合场景理解和时序感知扩散模型，解决了现有文本到视频生成中时间一致性、构图理解和精细控制的难题，实现了高保真视频合成。", "motivation": "现有的文本到视频生成方法在保持时间一致性、构图理解以及对视觉叙事的精细控制方面存在困难。", "method": "MOVAI引入了三项关键创新：1) 组合场景解析器(CSP)，将文本描述分解为带有时间注释的分层场景图；2) 时空注意力机制(TSAM)，确保跨帧连贯的运动动态并保留空间细节；3) 渐进式视频优化(PVR)模块，通过多尺度时间推理迭代提升视频质量。", "result": "MOVAI在标准基准测试中达到了最先进的性能，相比现有方法，LPIPS视频质量指标提高了15.3%，FVD提高了12.7%，用户偏好研究提高了18.9%。该框架在生成具有真实时间动态和精细语义控制的复杂多对象场景方面表现出特别的优势。", "conclusion": "MOVAI通过其独特的分层框架和创新模块，显著提升了文本到视频生成的时间一致性、构图理解和精细控制能力，实现了高保真视频合成，并在复杂场景生成方面表现出色。"}}
{"id": "2511.00635", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00635", "abs": "https://arxiv.org/abs/2511.00635", "authors": ["Hyungtae Lim", "Daebeom Kim", "Hyun Myung"], "title": "Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles", "comment": "13 pages, 12 figures", "summary": "As various 3D light detection and ranging (LiDAR) sensors have been\nintroduced to the market, research on multi-session simultaneous localization\nand mapping (MSS) using heterogeneous LiDAR sensors has been actively\nconducted. Existing MSS methods mostly rely on loop closure detection for\ninter-session alignment; however, the performance of loop closure detection can\nbe potentially degraded owing to the differences in the density and field of\nview (FoV) of the sensors used in different sessions. In this study, we\nchallenge the existing paradigm that relies heavily on loop detection modules\nand propose a novel MSS framework, called Multi-Mapcher, that employs\nlarge-scale map-to-map registration to perform inter-session initial alignment,\nwhich is commonly assumed to be infeasible, by leveraging outlier-robust 3D\npoint cloud registration. Next, after finding inter-session loops by radius\nsearch based on the assumption that the inter-session initial alignment is\nsufficiently precise, anchor node-based robust pose graph optimization is\nemployed to build a consistent global map. As demonstrated in our experiments,\nour approach shows substantially better MSS performance for various LiDAR\nsensors used to capture the sessions and is faster than state-of-the-art\napproaches. Our code is available at\nhttps://github.com/url-kaist/multi-mapcher.", "AI": {"tldr": "本文提出Multi-Mapcher，一个针对异构LiDAR传感器的多会话同步定位与建图（MSS）新框架，通过大规模地图到地图配准进行会话间初始对齐，并结合锚点式姿态图优化，显著提升了MSS性能和速度。", "motivation": "现有异构LiDAR传感器的MSS方法主要依赖回环检测进行会话间对齐，但由于传感器密度和视场（FoV）的差异，回环检测的性能可能会下降。研究挑战了这种对回环检测模块的过度依赖。", "method": "该研究提出了Multi-Mapcher框架，其核心方法包括：1) 利用异常值鲁棒的3D点云配准，进行大规模地图到地图配准以实现会话间初始对齐，这通常被认为是不可行的。2) 在初始对齐足够精确的假设下，通过半径搜索找到会话间回环。3) 采用基于锚点（anchor node-based）的鲁棒姿态图优化来构建一致的全局地图。", "result": "实验结果表明，该方法在各种LiDAR传感器捕获的会话中，展现出显著优于现有技术的MSS性能，并且比现有最先进的方法更快。", "conclusion": "Multi-Mapcher通过创新性地使用大规模地图到地图配准进行会话间初始对齐，并结合鲁棒姿态图优化，成功克服了异构LiDAR传感器MSS中回环检测的局限性，实现了更高性能和更快的全局地图构建。"}}
{"id": "2511.00733", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00733", "abs": "https://arxiv.org/abs/2511.00733", "authors": ["Tyler Christeson", "Md Habib Ullah", "Ali Arabnya", "Amin Khodaei", "Rui Fan"], "title": "Hybrid Quantum-Classical Optimization of the Resource Scheduling Problem", "comment": "13 pages, 7 figures, 1 table Submitted to Next Research", "summary": "Resource scheduling is critical in many industries, especially in power\nsystems. The Unit Commitment problem determines the on/off status and output\nlevels of generators under many constraints. Traditional exact methods, such as\nmathematical programming methods or dynamic programming, remain the backbone of\nUC solution techniques, but they often rely on linear approximations or\nexhaustive search, leading to high computational burdens as system size grows.\nMetaheuristic approaches, such as genetic algorithms, particle swarm\noptimization, and other evolutionary methods, have been explored to mitigate\nthis complexity; however, they typically lack optimality guarantees, exhibit\nsensitivity to initial conditions, and can become prohibitively time-consuming\nfor large-scale systems. In this paper, we introduce a quantum-classical hybrid\nalgorithm for UC and, by extension, other resource scheduling problems, that\nleverages Benders decomposition to decouple binary commitment decisions from\ncontinuous economic dispatch. The binary master problem is formulated as a\nquadratic unconstrained binary optimization model and solved on a quantum\nannealer. The continuous subproblem, which minimizes generation costs, with\nLagrangian cuts feeding back to the master until convergence. We evaluate our\nhybrid framework on systems scaled from 10 to 1,000 generation units. Compared\nagainst a classical mixed-integer nonlinear programming baseline, the hybrid\nalgorithm achieves a consistently lower computation-time growth rate and\nmaintains an absolute optimality gap below 1.63%. These results demonstrate\nthat integrating quantum annealing within a hybrid quantum-classical Benders\ndecomposition loop can significantly accelerate large-scale resource scheduling\nwithout sacrificing solution quality, pointing toward a viable path for\naddressing the escalating complexity of modern power grids.", "AI": {"tldr": "本文提出了一种量子-经典混合算法，用于解决机组组合（UC）问题，该算法利用Benders分解将二元决策与连续调度解耦，并使用量子退火器求解二元主问题，显著降低了大规模系统的计算时间增长率，同时保持了解决方案质量。", "motivation": "资源调度，尤其是电力系统中的机组组合问题，面临着巨大的挑战。传统的精确方法（如数学规划、动态规划）计算负担重，依赖线性近似或穷举搜索；元启发式方法（如遗传算法、粒子群优化）缺乏最优性保证，对初始条件敏感，且对大规模系统耗时过高。因此，需要一种新的方法来应对系统规模增长带来的复杂性。", "method": "本文引入了一种量子-经典混合算法。该方法利用Benders分解将二元承诺决策（机组启停状态）与连续经济调度（发电机输出水平）解耦。其中，二元主问题被建模为二次无约束二元优化（QUBO）模型，并在量子退火器上求解。连续子问题则负责最小化发电成本，并通过拉格朗日割平面反馈给主问题，直至收敛。该框架在10到1000个发电机组的系统上进行了评估。", "result": "与经典的混合整数非线性规划（MINLP）基线算法相比，该混合算法实现了持续更低的计算时间增长率，并将绝对最优性差距保持在1.63%以下。", "conclusion": "这些结果表明，在混合量子-经典Benders分解循环中整合量子退火，可以显著加速大规模资源调度，同时不牺牲解决方案质量。这为解决现代电网日益增长的复杂性提供了一条可行的途径。"}}
{"id": "2511.00509", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00509", "abs": "https://arxiv.org/abs/2511.00509", "authors": ["Yifan Xia", "Guorui Chen", "Wenqian Yu", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "Reimagining Safety Alignment with An Image", "comment": null, "summary": "Large language models (LLMs) excel in diverse applications but face dual\nchallenges: generating harmful content under jailbreak attacks and over-refusal\nof benign queries due to rigid safety mechanisms. These issues are further\ncomplicated by the need to accommodate different value systems and precisely\nalign with given safety preferences. Moreover, traditional methods like SFT and\nRLHF lack this capability due to their costly parameter tuning requirements and\ninability to support multiple value systems within a single model. These\nproblems are more obvious in multimodal large language models (MLLMs),\nespecially in terms of heightened over-refusal in cross-modal tasks and new\nsecurity risks arising from expanded attack surfaces. We propose Magic Image,\nan optimization-driven visual prompt framework that enhances security while\nreducing over-refusal. By optimizing image prompts using harmful/benign\nsamples, our method enables a single model to adapt to different value systems\nand better align with given safety preferences without parameter updates.\nExperiments demonstrate improved safety-effectiveness balance across diverse\ndatasets while preserving model performance, offering a practical solution for\ndeployable MLLM safety alignment.", "AI": {"tldr": "本文提出Magic Image，一个优化驱动的视觉提示框架，旨在解决大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在有害内容生成和过度拒绝之间的安全-有效性平衡问题，同时无需参数更新即可适应不同价值体系。", "motivation": "LLMs和MLLMs面临双重挑战：在越狱攻击下生成有害内容，以及由于严格的安全机制而过度拒绝良性查询。传统方法（如SFT和RLHF）成本高昂且无法在单一模型中支持多价值体系。MLLMs中这些问题更为突出，尤其是在跨模态任务中的过度拒绝和新的安全风险。", "method": "Magic Image是一个优化驱动的视觉提示框架。它通过使用有害/良性样本优化图像提示，使单一模型能够在不更新参数的情况下适应不同的价值体系，并更好地与既定的安全偏好对齐。", "result": "实验证明，该方法在保持模型性能的同时，改善了在不同数据集上的安全-有效性平衡。", "conclusion": "Magic Image为可部署的MLLM安全对齐提供了一个实用的解决方案，通过增强安全性、减少过度拒绝，并能适应不同价值体系，且无需进行参数更新。"}}
{"id": "2511.00476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00476", "abs": "https://arxiv.org/abs/2511.00476", "authors": ["Ghazal Kalhor", "Afra Mashhadi"], "title": "Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks", "comment": null, "summary": "Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search\nand recommendation platforms at their core. While this shift unlocks powerful\nnew scientometric tools, it also exposes critical fairness and bias issues that\ncould erode the integrity of the information ecosystem. Additionally, as LLMs\nbecome more integrated into web-based searches for scholarly tools, their\nability to generate summarized research work based on memorized data introduces\nnew dimensions to these challenges. The extent of memorization in LLMs can\nimpact the accuracy and fairness of the co-authorship networks they produce,\npotentially reflecting and amplifying existing biases within the scientific\ncommunity and across different regions. This study critically examines the\nimpact of LLM memorization on the co-authorship networks. To this end, we\nassess memorization effects across three prominent models, DeepSeek R1, Llama 4\nScout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across\nacademic disciplines and world regions. While our global analysis reveals a\nconsistent bias favoring highly cited researchers, this pattern is not\nuniformly observed. Certain disciplines, such as Clinical Medicine, and\nregions, including parts of Africa, show more balanced representation, pointing\nto areas where LLM training data may reflect greater equity. These findings\nunderscore both the risks and opportunities in deploying LLMs for scholarly\ndiscovery.", "AI": {"tldr": "本研究探讨了大型语言模型（LLMs）的记忆化如何影响其生成的合著者网络，发现普遍存在偏向高被引研究者的偏见，但也存在学科和区域差异。", "motivation": "LLMs正在重塑搜索和推荐平台，但其记忆化能力可能引入严重的公平性和偏见问题，影响合著者网络的准确性和公正性，并可能放大科学界现有的偏见。", "method": "通过评估DeepSeek R1、Llama 4 Scout和Mixtral 8x7B这三个主流模型，分析其记忆化对合著者网络的影响，并研究这些由记忆化驱动的输出在不同学术学科和全球区域间的差异。", "result": "全球分析显示LLMs普遍偏向高被引研究者。然而，在某些学科（如临床医学）和区域（如非洲部分地区），表现出更平衡的代表性，表明这些领域的LLM训练数据可能反映了更大的公平性。", "conclusion": "研究结果强调了在学术发现中部署LLMs的风险（潜在的偏见放大）和机遇（训练数据中可能存在的公平区域）。"}}
{"id": "2511.00110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00110", "abs": "https://arxiv.org/abs/2511.00110", "authors": ["YingQiao Wang", "Eric Bigelow", "Boyi Li", "Tomer Ullman"], "title": "Chain of Time: In-Context Physical Simulation with Image Generation Models", "comment": null, "summary": "We propose a novel cognitively-inspired method to improve and interpret\nphysical simulation in vision-language models. Our ``Chain of Time\" method\ninvolves generating a series of intermediate images during a simulation, and it\nis motivated by in-context reasoning in machine learning, as well as mental\nsimulation in humans. Chain of Time is used at inference time, and requires no\nadditional fine-tuning. We apply the Chain-of-Time method to synthetic and\nreal-world domains, including 2-D graphics simulations and natural 3-D videos.\nThese domains test a variety of particular physical properties, including\nvelocity, acceleration, fluid dynamics, and conservation of momentum. We found\nthat using Chain-of-Time simulation substantially improves the performance of a\nstate-of-the-art image generation model. Beyond examining performance, we also\nanalyzed the specific states of the world simulated by an image model at each\ntime step, which sheds light on the dynamics underlying these simulations. This\nanalysis reveals insights that are hidden from traditional evaluations of\nphysical reasoning, including cases where an image generation model is able to\nsimulate physical properties that unfold over time, such as velocity, gravity,\nand collisions. Our analysis also highlights particular cases where the image\ngeneration model struggles to infer particular physical parameters from input\nimages, despite being capable of simulating relevant physical processes.", "AI": {"tldr": "本文提出了一种名为“时间链”的认知启发方法，通过生成一系列中间图像来改进和解释视觉-语言模型中的物理模拟，该方法在推理时使用，无需额外微调，并显著提升了图像生成模型的性能和可解释性。", "motivation": "本研究的动机是受机器学习中的上下文推理和人类的心理模拟启发，旨在提高视觉-语言模型（VLM）进行物理模拟的能力及其可解释性。", "method": "研究提出了一种名为“时间链”（Chain of Time）的方法，该方法在推理时生成一系列中间图像来模拟物理过程，无需额外的模型微调。该方法被应用于合成和真实世界领域，包括2D图形模拟和自然3D视频，以测试速度、加速度、流体动力学和动量守恒等物理特性。", "result": "结果显示，“时间链”模拟显著提高了最先进图像生成模型的性能。此外，对每个时间步模拟状态的分析揭示了传统物理推理评估中隐藏的见解，表明模型能够模拟随时间展开的物理属性（如速度、重力、碰撞），但也存在难以从输入图像推断特定物理参数的情况。", "conclusion": "“时间链”方法不仅能显著提升视觉-语言模型在物理模拟方面的性能，还能通过对模拟过程的深入分析提供对模型动态和物理推理能力的宝贵见解，揭示其成功和不足之处。"}}
{"id": "2511.00736", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00736", "abs": "https://arxiv.org/abs/2511.00736", "authors": ["Tyler Christeson", "Amin Khodaei", "Rui Fan"], "title": "Quantum Computing for EVs to Enhance Grid Resilience and Disaster Relief: Challenges and Opportunities", "comment": "11 pages, 0 figures, 2 tables, Submitted to IEEE Transactions on\n  Smart Grid", "summary": "The power grid is the foundation of modern society, however extreme weather\nevents have increasingly caused widespread outages. Enhancing grid resilience\nis therefore critical to maintaining secure and reliable operations. In\ndisaster relief and restoration, vehicle-to-grid (V2G) technology allows\nelectric vehicles (EVs) to serve as mobile energy resources by discharging to\nsupport critical loads or regulating grid frequency as needed. Effective V2G\noperation requires coordinated charging and discharging of many EVs through\noptimization. Similarly, in grid restoration, EVs must be strategically routed\nto affected areas, forming the mobile charging station placement (CSP) problem,\nwhich presents another complex optimization challenge. This work reviews\nstate-of-the-art optimization methods for V2G and mobile CSP applications,\noutlines their limitations, and explores how quantum computing (QC) could\novercome current computational bottlenecks. A QC-focused perspective is\npresented on enhancing grid resilience and accelerating restoration as extreme\nweather events grow more frequent and severe.", "AI": {"tldr": "本文综述了V2G和移动充电站选址（CSP）在增强电网韧性方面的优化方法，指出了其局限性，并探讨了量子计算如何克服计算瓶颈以加速电网恢复。", "motivation": "极端天气事件日益频繁地导致大范围停电，因此增强电网韧性对于维持安全可靠的电力运行至关重要。V2G和移动CSP技术虽有潜力，但其有效运行涉及复杂的优化挑战。", "method": "本文回顾了V2G和移动CSP应用领域的现有优化方法，概述了这些方法的局限性，并探讨了量子计算（QC）如何克服当前的计算瓶颈。", "result": "研究指出了现有优化方法在V2G和移动CSP应用中的局限性，并提出量子计算能够解决这些计算瓶颈，从而在极端天气事件日益频繁和严重的情况下，增强电网韧性并加速恢复。", "conclusion": "量子计算为增强电网韧性和加速恢复提供了一个新的视角，尤其是在面对日益频繁和严重的极端天气事件时，它能够有效解决V2G和移动CSP应用中的复杂优化问题。"}}
{"id": "2511.00783", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00783", "abs": "https://arxiv.org/abs/2511.00783", "authors": ["Jingzehua Xu", "Weihang Zhang", "Yangyang Li", "Hongmiaoyi Zhang", "Guanwen Xie", "Jiwei Tang", "Shuai Zhang", "Yi Li"], "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage", "comment": "This paper has been submitted to IEEE Transactions on Mobile\n  Computing", "summary": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions.", "AI": {"tldr": "本文提出了一种语义引导的模糊控制框架，结合大语言模型和可解释控制，实现了在无GPS、无地图的水下多机器人协同覆盖，解决了部分可观测、通信受限、环境不确定和缺乏全局定位等挑战。", "motivation": "水下多机器人协同覆盖面临诸多挑战，包括部分可观测性、有限通信、环境不确定性以及缺乏全局定位。这些因素使得现有方法难以有效部署。", "method": "该研究提出一个语义引导的模糊控制框架。首先，大语言模型（LLM）将原始多模态观测压缩成紧凑、人类可解释的语义令牌，概括障碍物、未探索区域和目标物体（OOIs）。然后，一个带有预定义隶属函数的模糊推理系统将这些语义令牌映射为平滑稳定的转向和步态指令，实现无需全局定位的可靠导航。最后，通过语义通信分享意图和局部上下文，协调多个机器人，避免重复探索。", "result": "在未知珊瑚礁状环境中的广泛模拟表明，在有限感知和通信条件下，所提出的框架实现了鲁棒的面向OOI的导航和协同覆盖，显著提高了效率和适应性。", "conclusion": "该框架成功弥合了语义认知与水下分布式控制之间的鸿沟，在无GPS、无地图的条件下，为水下多机器人协同覆盖提供了有效的解决方案。"}}
{"id": "2511.00432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00432", "abs": "https://arxiv.org/abs/2511.00432", "authors": ["Zhiwen Ruan", "Yixia Li", "Yefeng Liu", "Yun Chen", "Weihua Luo", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "G2: Guided Generation for Enhanced Output Diversity in LLMs", "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, these models exhibit a\ncritical limitation in output diversity, often generating highly similar\ncontent across multiple attempts. This limitation significantly affects tasks\nrequiring diverse outputs, from creative writing to reasoning. Existing\nsolutions, like temperature scaling, enhance diversity by modifying probability\ndistributions but compromise output quality. We propose Guide-to-Generation\n(G2), a training-free plug-and-play method that enhances output diversity while\npreserving generation quality. G2 employs a base generator alongside dual\nGuides, which guide the generation process through decoding-based interventions\nto encourage more diverse outputs conditioned on the original query.\nComprehensive experiments demonstrate that G2 effectively improves output\ndiversity while maintaining an optimal balance between diversity and quality.", "AI": {"tldr": "大型语言模型（LLMs）在输出多样性方面存在局限，本文提出G2方法，一个无需训练的即插即用方案，通过双重引导器在解码过程中干预生成，以在保持质量的同时提高输出多样性。", "motivation": "LLMs在各种NLP任务中表现出色，但其输出多样性不足，经常生成高度相似的内容，这严重影响了需要多样化输出的任务（如创意写作、推理）。现有解决方案（如温度缩放）虽能增加多样性，但会损害输出质量。", "method": "G2是一种无需训练的即插即用方法。它使用一个基础生成器和两个“引导器”（dual Guides）。这些引导器通过基于解码的干预措施，在生成过程中引导模型，以鼓励在原始查询条件下产生更多样化的输出。", "result": "全面的实验表明，G2有效提高了输出多样性，同时在多样性和质量之间保持了最佳平衡。", "conclusion": "G2成功地在不牺牲生成质量的前提下，增强了大型语言模型的输出多样性，提供了一个优于现有方法的解决方案。"}}
{"id": "2511.00547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00547", "abs": "https://arxiv.org/abs/2511.00547", "authors": ["Alain Riou"], "title": "Efficient Generation of Binary Magic Squares", "comment": null, "summary": "We propose a simple algorithm for generating Binary Magic Squares (BMS),\ni.e., square binary matrices where the sum of all rows and all columns are\nequal. We show by induction that our algorithm always returns valid BMS with\noptimal theoretical complexity. We then extend our study to non-square Binary\nMagic Squares, formalize conditions on the sum of rows and columns for these\nBMS to exist, and show that a slight variant of our first algorithm can\ngenerate provably generate them. Finally, we publicly release two\nimplementations of our algorithm as Python packages, including one that can\ngenerate several BMS in parallel using GPU acceleration.", "AI": {"tldr": "本文提出了一种生成二元幻方（BMS）的简单算法，包括方形和非方形BMS，该算法具有最优理论复杂度，并提供了支持GPU并行生成的Python实现。", "motivation": "研究的动机是提出一种简单且高效的算法来生成二元幻方（BMS），解决其生成问题。", "method": "研究方法包括：1) 提出一个生成方形BMS的简单算法，并通过归纳法证明其有效性和最优复杂度；2) 将研究扩展到非方形BMS，形式化其行和列和存在的条件；3) 提出第一个算法的变体来生成非方形BMS；4) 发布两种Python实现，其中一种支持GPU加速并行生成。", "result": "主要结果是：1) 所提出的算法能始终生成有效的方形BMS，并具有最优理论复杂度；2) 形式化了非方形BMS存在时行和列和的条件；3) 算法的变体能可靠地生成非方形BMS；4) 公开了两个Python实现，其中一个利用GPU加速实现并行BMS生成。", "conclusion": "结论是，研究提出了一种简单、有效且具有最优复杂度的算法，能够生成方形和非方形二元幻方，并通过实际的Python包（包括GPU加速版本）提供了可用的实现。"}}
{"id": "2511.00814", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "93C41, 93E11, 37M10", "I.2.9; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.00814", "abs": "https://arxiv.org/abs/2511.00814", "authors": ["Stella Kombo", "Masih Haseli", "Skylar Wei", "Joel W. Burdick"], "title": "Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning", "comment": "10 pages, 6 figures, submitted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2025", "summary": "Autonomous systems often must predict the motions of nearby agents from\npartial and noisy data. This paper asks and answers the question: \"can we\nlearn, in real-time, a nonlinear predictive model of another agent's motions?\"\nOur online framework denoises and forecasts such dynamics using a modified\nsliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy\nmeasurements are embedded into a Hankel matrix, while an associated Page matrix\nenables singular-value hard thresholding (SVHT) to estimate the effective rank.\nA Cadzow projection enforces structured low-rank consistency, yielding a\ndenoised trajectory and local noise variance estimates. From this\nrepresentation, a time-varying Hankel-DMD lifted linear predictor is\nconstructed for multi-step forecasts. The residual analysis provides\nvariance-tracking signals that can support downstream estimators and risk-aware\nplanning. We validate the approach in simulation under Gaussian and\nheavy-tailed noise, and experimentally on a dynamic crane testbed. Results show\nthat the method achieves stable variance-aware denoising and short-horizon\nprediction suitable for integration into real-time control frameworks.", "AI": {"tldr": "本文提出了一种在线框架，利用改进的滑动窗口Hankel动态模态分解（Hankel-DMD）方法，从部分噪声数据中实时学习并预测其他智能体的非线性运动，同时提供去噪和方差跟踪功能。", "motivation": "自动系统需要从部分和噪声数据中预测附近智能体的运动。研究的动机是能否实时学习一个非线性的预测模型来描述另一个智能体的运动。", "method": "该方法是一个在线框架，使用改进的滑动窗口Hankel动态模态分解（Hankel-DMD）。它将部分噪声测量嵌入到Hankel矩阵中，并利用Page矩阵和奇异值硬阈值（SVHT）来估计有效秩。Cadzow投影用于强制结构化低秩一致性，从而得到去噪轨迹和局部噪声方差估计。基于此表示，构建了一个时变Hankel-DMD提升线性预测器用于多步预测。残差分析提供方差跟踪信号。", "result": "该方法实现了稳定的方差感知去噪和短时预测，适用于集成到实时控制框架中。在具有高斯和重尾噪声的仿真以及动态起重机试验台上进行了验证。", "conclusion": "该方法能够实时学习非线性预测模型，有效地对智能体动态进行去噪和预测，并提供方差跟踪信号，可支持下游估计器和风险感知规划。"}}
{"id": "2511.00555", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00555", "abs": "https://arxiv.org/abs/2511.00555", "authors": ["Dianye Huang", "Nassir Navab", "Zhongliang Jiang"], "title": "Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy", "comment": "Accepted by IEEE T-RO", "summary": "Integrating generative models with action chunking has shown significant\npromise in imitation learning for robotic manipulation. However, the existing\ndiffusion-based paradigm often struggles to capture strong temporal\ndependencies across multiple steps, particularly when incorporating\nproprioceptive input. This limitation can lead to task failures, where the\npolicy overfits to proprioceptive cues at the expense of capturing the visually\nderived features of the task. To overcome this challenge, we propose the Deep\nKoopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a\ndual-branch architecture to decouple the roles of different sensory modality\ncombinations. The visual branch encodes the visual observations to indicate\ntask progression, while the fused branch integrates both visual and\nproprioceptive inputs for precise manipulation. Within this architecture, when\nthe robot fails to accomplish intermediate goals, such as grasping a drawer\nhandle, the policy can dynamically switch to execute action chunks generated by\nthe visual branch, allowing recovery to previously observed states and\nfacilitating retrial of the task. To further enhance visual representation\nlearning, we incorporate a Deep Koopman Operator module that captures\nstructured temporal dynamics from visual inputs. During inference, we use the\ntest-time loss of the generative model as a confidence signal to guide the\naggregation of the temporally overlapping predicted action chunks, thereby\nenhancing the reliability of policy execution. In simulation experiments across\nsix RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion\npolicy by an average of 14.6\\%. On three real-world robotic manipulation tasks,\nit achieves a 15.0\\% improvement. Code: https://github.com/dianyeHuang/D3P.", "AI": {"tldr": "该论文提出了一种名为D3P（Deep Koopman-boosted Dual-branch Diffusion Policy）的新算法，用于解决机器人模仿学习中扩散模型在处理时间依赖性和本体感受过拟合方面的不足。D3P采用双分支架构、动态切换机制和Deep Koopman算子模块，显著提高了模拟和真实世界机器人操作任务的性能。", "motivation": "现有的基于扩散的模仿学习方法在捕获多步骤的强时间依赖性方面表现不佳，尤其是在结合本体感受输入时。这种局限性可能导致策略过度拟合本体感受线索，而忽略视觉特征，从而导致任务失败。", "method": "该研究提出了D3P算法，其核心方法包括：1) 引入双分支架构，将不同感知模态组合的角色解耦：视觉分支编码视觉观察以指示任务进展，融合分支整合视觉和本体感受输入以进行精确操作。2) 当机器人未能完成中间目标时，策略可以动态切换到由视觉分支生成的动作块，以恢复到先前观察到的状态并重新尝试任务。3) 结合Deep Koopman算子模块，从视觉输入中捕获结构化的时间动态，以增强视觉表示学习。4) 在推理过程中，使用生成模型的测试时损失作为置信度信号，指导时间重叠的预测动作块的聚合，从而提高策略执行的可靠性。", "result": "在六项RLBench桌面任务的模拟实验中，D3P比现有最先进的扩散策略平均性能提高了14.6%。在三项真实世界机器人操作任务中，D3P实现了15.0%的改进。", "conclusion": "D3P算法通过其独特的双分支架构、动态切换机制和Deep Koopman算子模块，成功克服了现有扩散策略在处理时间依赖性和本体感受过拟合方面的挑战。该方法显著提升了机器人模仿学习的性能和可靠性，在模拟和真实世界任务中均达到了最先进水平。"}}
{"id": "2511.00840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00840", "abs": "https://arxiv.org/abs/2511.00840", "authors": ["William Suliman", "Ekaterina Chaikovskaia", "Egor Davydenko", "Roman Gorbachev"], "title": "Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches", "comment": null, "summary": "This work presents an extended framework for learning-based bipedal\nlocomotion that incorporates a heuristic step-planning strategy guided by\ndesired torso velocity tracking. The framework enables precise interaction\nbetween a humanoid robot and its environment, supporting tasks such as crossing\ngaps and accurately approaching target objects. Unlike approaches based on full\nor simplified dynamics, the proposed method avoids complex step planners and\nanalytical models. Step planning is primarily driven by heuristic commands,\nwhile a Raibert-type controller modulates the foot placement length based on\nthe error between desired and actual torso velocity. We compare our method with\na model-based step-planning approach -- the Linear Inverted Pendulum Model\n(LIPM) controller. Experimental results demonstrate that our approach attains\ncomparable or superior accuracy in maintaining target velocity (up to 80%),\nsignificantly greater robustness on uneven terrain (over 50% improvement), and\nimproved energy efficiency. These results suggest that incorporating complex\nanalytical, model-based components into the training architecture may be\nunnecessary for achieving stable and robust bipedal walking, even in\nunstructured environments.", "AI": {"tldr": "该研究提出了一种基于学习的双足步态框架，结合启发式步态规划和Raibert型控制器，实现了对复杂动力学模型的规避，并在不平坦地形上的鲁棒性和能效方面优于LIPM。", "motivation": "为了使人形机器人能够与环境进行精确交互，完成跨越障碍、接近目标等任务，同时避免使用复杂的步态规划器和分析模型。", "method": "该方法扩展了一个基于学习的双足步态框架，采用启发式步态规划策略，并由期望躯干速度跟踪引导。一个Raibert型控制器根据期望与实际躯干速度之间的误差来调整落脚点长度。该方法与基于模型的线性倒立摆模型（LIPM）控制器进行了比较。", "result": "实验结果表明，该方法在保持目标速度方面达到可比或更优的精度（高达80%），在不平坦地形上具有显著更高的鲁棒性（超过50%的改进），并提高了能源效率。", "conclusion": "研究结果表明，即使在非结构化环境中，为实现稳定和鲁棒的双足行走，训练架构中可能不需要整合复杂的分析性、基于模型的组件。"}}
{"id": "2511.00486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00486", "abs": "https://arxiv.org/abs/2511.00486", "authors": ["Pooja Singh", "Shashwat Bhardwaj", "Vaibhav Sharma", "Sandeep Kumar"], "title": "Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus", "comment": "Accepted in EMNLP 2025", "summary": "The linguistic diversity of India poses significant machine translation\nchallenges, especially for underrepresented tribal languages like Bhili, which\nlack high-quality linguistic resources. This paper addresses the gap by\nintroducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest\nparallel corpus worldwide comprising 110,000 meticulously curated sentences\nacross Bhili, Hindi, and English. The corpus was created with the assistance of\nexpert human translators. BHEPC spans critical domains such as education,\nadministration, and news, establishing a valuable benchmark for research in low\nresource machine translation. To establish a comprehensive Bhili Machine\nTranslation benchmark, we evaluated a wide range of proprietary and open-source\nMultilingual Large Language Models (MLLMs) on bidirectional translation tasks\nbetween English/Hindi and Bhili. Comprehensive evaluation demonstrates that the\nfine-tuned NLLB-200 distilled 600M variant model outperforms others,\nhighlighting the potential of multilingual models in low resource scenarios.\nFurthermore, we investigated the generative translation capabilities of\nmultilingual LLMs on BHEPC using in-context learning, assessing performance\nunder cross-domain generalization and quantifying distributional divergence.\nThis work bridges a critical resource gap and promotes inclusive natural\nlanguage processing technologies for low-resource and marginalized languages\nglobally.", "AI": {"tldr": "本文介绍了首个也是最大的俾路支语-印地语-英语平行语料库（BHEPC），并评估了多种多语言大型语言模型（MLLMs）在该低资源语言对上的机器翻译性能，发现微调后的NLLB-200模型表现最佳。", "motivation": "印度语言多样性对机器翻译构成挑战，特别是对于俾路支语等缺乏高质量语言资源的弱势部落语言。研究旨在弥补这一资源空白。", "method": "研究方法包括：1. 构建了包含11万句句子的俾路支语-印地语-英语平行语料库（BHEPC），涵盖教育、行政和新闻等领域，由专业人工译者协助完成。2. 评估了多种专有和开源多语言大型语言模型（MLLMs）在英语/印地语与俾路支语之间的双向翻译任务上的性能。3. 对NLLB-200蒸馏版600M模型进行了微调。4. 利用上下文学习（in-context learning）探究了多语言LLMs在BHEPC上的生成式翻译能力，并评估了跨领域泛化性能和分布差异。", "result": "主要结果包括：1. BHEPC是全球首个也是最大的俾路支语-印地语-英语平行语料库。2. 综合评估表明，微调后的NLLB-200蒸馏版600M模型优于其他模型，突显了多语言模型在低资源场景下的潜力。3. 评估了多语言LLMs在BHEPC上的生成式翻译能力，并量化了其在跨领域泛化和分布差异方面的表现。", "conclusion": "这项工作弥补了关键的资源空白，为低资源和边缘化语言推广了包容性的自然语言处理技术。"}}
{"id": "2511.00114", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00114", "abs": "https://arxiv.org/abs/2511.00114", "authors": ["Hanae Elmekki", "Amanda Spilkin", "Ehsan Zakeri", "Antonela Mariel Zanuttini", "Ahmed Alagha", "Hani Sami", "Jamal Bentahar", "Lyes Kadem", "Wen-Fang Xie", "Philippe Pibarot", "Rabeb Mizouni", "Hadi Otrok", "Azzam Mourad", "Sami Muhaidat"], "title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning", "comment": null, "summary": "Cardiac ultrasound (US) is among the most widely used diagnostic tools in\ncardiology for assessing heart health, but its effectiveness is limited by\noperator dependence, time constraints, and human error. The shortage of trained\nprofessionals, especially in remote areas, further restricts access. These\nissues underscore the need for automated solutions that can ensure consistent,\nand accessible cardiac imaging regardless of operator skill or location. Recent\nprogress in artificial intelligence (AI), especially in deep reinforcement\nlearning (DRL), has gained attention for enabling autonomous decision-making.\nHowever, existing DRL-based approaches to cardiac US scanning lack\nreproducibility, rely on proprietary data, and use simplified models. Motivated\nby these gaps, we present the first end-to-end framework that integrates\ngenerative AI and DRL to enable autonomous and reproducible cardiac US\nscanning. The framework comprises two components: (i) a conditional generative\nsimulator combining Generative Adversarial Networks (GANs) with Variational\nAutoencoders (VAEs), that models the cardiac US environment producing realistic\naction-conditioned images; and (ii) a DRL module that leverages this simulator\nto learn autonomous, accurate scanning policies. The proposed framework\ndelivers AI-driven guidance through expert-validated models that classify image\ntype and assess quality, supports conditional generation of realistic US\nimages, and establishes a reproducible foundation extendable to other organs.\nTo ensure reproducibility, a publicly available dataset of real cardiac US\nscans is released. The solution is validated through several experiments. The\nVAE-GAN is benchmarked against existing GAN variants, with performance assessed\nusing qualitative and quantitative approaches, while the DRL-based scanning\nsystem is evaluated under varying configurations to demonstrate effectiveness.", "AI": {"tldr": "该研究提出了首个端到端框架，结合生成式AI（VAE-GAN）和深度强化学习（DRL），实现了自主且可复现的心脏超声扫描，并通过发布公开数据集确保可复现性。", "motivation": "心脏超声诊断受限于操作员依赖、时间限制和人为错误，且专业人员短缺，尤其是在偏远地区。现有基于DRL的心脏超声扫描方法缺乏可复现性、依赖专有数据并使用简化模型。这些问题促使研究者寻求自动化、一致且可访问的解决方案。", "method": "该框架包含两部分：(i) 条件生成模拟器，结合生成对抗网络（GANs）和变分自编码器（VAEs）来建模心脏超声环境，生成逼真的、条件化的图像；(ii) 深度强化学习（DRL）模块，利用该模拟器学习自主、准确的扫描策略。为确保可复现性，研究者还发布了一个公开的心脏超声扫描数据集。", "result": "所提出的框架通过专家验证模型提供AI驱动的指导，支持逼真超声图像的条件生成，并为扩展到其他器官奠定了可复现的基础。VAE-GAN在定性和定量方法上与现有GAN变体进行了基准测试，DRL扫描系统在不同配置下进行了评估，证明了其有效性。", "conclusion": "该研究首次提出了一个集成生成式AI和深度强化学习的端到端框架，实现了自主且可复现的心脏超声扫描。通过提供AI驱动的指导、支持图像生成和发布公开数据集，该解决方案有效解决了现有心脏超声诊断的局限性，并为未来研究奠定了可扩展的基础。"}}
{"id": "2511.00745", "categories": ["eess.SY", "cs.SY", "physics.med-ph", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.00745", "abs": "https://arxiv.org/abs/2511.00745", "authors": ["Xiaoyang Tian", "Hui Wang", "Boshuo Wang", "Jinshui Zhang", "Dong Yan", "Jeannette Ingabire", "Samantha Coffler", "Guillaume Duret", "Quoc-Khanh Pham", "Gang Bao", "Jacob T. Robinson", "Stefan M. Goetz", "Angel V. Peterchev"], "title": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic Neuromodulation", "comment": "25 pages, 8 figures", "summary": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably.", "AI": {"tldr": "本文开发了一种双通道磁场腔室，用于在自由活动的小鼠中进行高频交变磁场刺激，特别是磁热遗传学刺激，并验证了其频率选择性和安全性。", "motivation": "为了量化磁遗传学和磁电刺激等新方法对自由活动小鼠神经活动操纵所产生的行为效应，需要一种能够精确控制高频交变磁场的工具。", "method": "研究人员设计了一个双通道磁场腔室，通过优化线圈设计，实现了两个空间正交、频率不同的均匀磁场的独立控制。该系统配备了液体冷却系统、观察端口和摄像头，能够生成高幅度磁场，且具有可忽略的通道间干扰和均匀的磁场分布。通过使用钴掺杂和未掺杂的氧化铁纳米粒子，验证了其频率选择性加热能力。", "result": "该腔室在10 cm x 10 cm x 6 cm的体积内，提供了50 kHz（88 mT）和550 kHz（12.5 mT）两个通道，通道间干扰小于1%。磁场分布均匀（94%腔室体积内偏差小于±10%），操作期间线圈外壳内侧温升限制在<0.35°C/s。成功展示了频率选择性加热，钴掺杂和未掺杂氧化铁纳米粒子的加热速率分别为3.5°C/s和1.5°C/s。两个通道均可稳定运行四秒。", "conclusion": "所开发的双通道磁场腔室是一个安全、高效且频率选择性强的平台，适用于在自由活动小鼠中进行磁热遗传学刺激以及其他交变磁场应用，为神经活动操纵研究提供了新的工具。"}}
{"id": "2511.00120", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00120", "abs": "https://arxiv.org/abs/2511.00120", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images", "comment": "This paper has been accepted to IEIE( The Institute Of Electronics\n  and Information Engineering, South Korea) Fall,2025 Conference", "summary": "The primary challenge in computer vision is precisely calculating the pose of\n6D objects, however many current approaches are still fragile and have trouble\ngeneralizing from synthetic data to real-world situations with fluctuating\nlighting, textureless objects, and significant occlusions. To address these\nlimitations, VLM6D, a novel dual-stream architecture that leverages the\ndistinct strengths of visual and geometric data from RGB-D input for robust and\nprecise pose estimation. Our framework uniquely integrates two specialized\nencoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the\nRGB modality, harnessing its rich, pre-trained understanding of visual grammar\nto achieve remarkable resilience against texture and lighting variations.\nConcurrently, a PointNet++ encoder processes the 3D point cloud derived from\ndepth data, enabling robust geometric reasoning that excels even with the\nsparse, fragmented data typical of severe occlusion. These complementary\nfeature streams are effectively fused to inform a multi task prediction head.\nWe demonstrate through comprehensive experiments that VLM6D obtained new SOTA\nperformance on the challenging Occluded-LineMOD, validating its superior\nrobustness and accuracy.", "AI": {"tldr": "VLM6D是一种新颖的双流架构，结合视觉（DINOv2）和几何（PointNet++）数据，以实现对6D物体的鲁棒和精确姿态估计，并在Occluded-LineMOD数据集上达到了SOTA性能。", "motivation": "当前6D物体姿态估计方法在处理真实世界场景中的光照变化、无纹理物体和严重遮挡时表现脆弱，且难以从合成数据泛化到真实世界。", "method": "VLM6D采用双流架构处理RGB-D输入：一个自监督的Vision Transformer (DINOv2) 处理RGB模态，以应对纹理和光照变化；一个PointNet++编码器处理深度数据衍生的3D点云，以实现鲁棒的几何推理，即使在稀疏、碎片化的遮挡数据下也能表现出色。这两个互补的特征流被有效融合，并输入到一个多任务预测头。", "result": "VLM6D在极具挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能。", "conclusion": "VLM6D通过利用视觉和几何数据的独特优势，显著提高了6D物体姿态估计的鲁棒性和准确性，并通过实验验证了其卓越性能。"}}
{"id": "2511.00609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00609", "abs": "https://arxiv.org/abs/2511.00609", "authors": ["Shengqi Xu", "Xinpeng Zhou", "Yabo Zhang", "Ming Liu", "Tao Liang", "Tianyu Zhang", "Yalong Bai", "Zuxuan Wu", "Wangmeng Zuo"], "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "comment": null, "summary": "Personalized image preference assessment aims to evaluate an individual\nuser's image preferences by relying only on a small set of reference images as\nprior information. Existing methods mainly focus on general preference\nassessment, training models with large-scale data to tackle well-defined tasks\nsuch as text-image alignment. However, these approaches struggle to handle\npersonalized preference because user-specific data are scarce and not easily\nscalable, and individual tastes are often diverse and complex. To overcome\nthese challenges, we introduce a common preference profile that serves as a\nbridge across users, allowing large-scale user data to be leveraged for\ntraining profile prediction and capturing complex personalized preferences.\nBuilding on this idea, we propose a reasoning-based personalized image\npreference assessment framework that follows a \\textit{predict-then-assess}\nparadigm: it first predicts a user's preference profile from reference images,\nand then provides interpretable, multi-dimensional scores and assessments of\ncandidate images based on the predicted profile. To support this, we first\nconstruct a large-scale Chain-of-Thought (CoT)-style personalized assessment\ndataset annotated with diverse user preference profiles and high-quality\nCoT-style reasoning, enabling explicit supervision of structured reasoning.\nNext, we adopt a two-stage training strategy: a cold-start supervised\nfine-tuning phase to empower the model with structured reasoning capabilities,\nfollowed by reinforcement learning to incentivize the model to explore more\nreasonable assessment paths and enhance generalization. Furthermore, we propose\na similarity-aware prediction reward to encourage better prediction of the\nuser's preference profile, which facilitates more reasonable assessments\nexploration. Extensive experiments demonstrate the superiority of the proposed\nmethod.", "AI": {"tldr": "本文提出了一种基于推理的个性化图像偏好评估框架，通过引入“通用偏好画像”作为用户间的桥梁，利用预测-评估范式和两阶段训练策略，解决了用户特定数据稀缺和偏好复杂多样的问题。", "motivation": "现有方法主要关注通用偏好评估，依赖大规模数据训练模型，但在处理个性化偏好时面临挑战，因为用户特定数据稀缺且难以扩展，同时个体品味多样且复杂。", "method": "1. 引入“通用偏好画像”作为用户间桥梁，利用大规模用户数据训练画像预测并捕捉复杂个性化偏好。2. 提出“预测-评估”范式：首先从参考图像预测用户偏好画像，然后基于预测画像对候选图像提供可解释的多维度评分和评估。3. 构建大规模CoT（思维链）风格个性化评估数据集，包含多样用户偏好画像和高质量CoT推理。4. 采用两阶段训练策略：冷启动监督微调以赋能模型结构化推理能力，随后使用强化学习激励模型探索更合理的评估路径并增强泛化性。5. 提出“相似度感知预测奖励”以鼓励更好地预测用户偏好画像，从而促进更合理的评估探索。", "result": "广泛的实验证明了所提出方法的优越性。", "conclusion": "本文成功地提出了一个有效的推理型个性化图像偏好评估框架，通过结合通用偏好画像、预测-评估范式、大规模CoT数据集和先进的训练策略，克服了现有方法在个性化偏好评估方面的局限性。"}}
{"id": "2511.00487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00487", "abs": "https://arxiv.org/abs/2511.00487", "authors": ["Stephen Meisenbacher", "Florian Matthes"], "title": "With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting", "comment": "11 pages, 1 figure, 5 tables. Accepted to IJCNLP-AACL 2025 (Main)", "summary": "Recent work in Differential Privacy with Natural Language Processing (DP NLP)\nhas proposed numerous promising techniques in the form of text rewriting\nmechanisms. In the evaluation of these mechanisms, an often-ignored aspect is\nthat of dataset size, or rather, the effect of dataset size on a mechanism's\nefficacy for utility and privacy preservation. In this work, we are the first\nto introduce this factor in the evaluation of DP text privatization, where we\ndesign utility and privacy tests on large-scale datasets with dynamic split\nsizes. We run these tests on datasets of varying size with up to one million\ntexts, and we focus on quantifying the effect of increasing dataset size on the\nprivacy-utility trade-off. Our findings reveal that dataset size plays an\nintegral part in evaluating DP text rewriting mechanisms; additionally, these\nfindings call for more rigorous evaluation procedures in DP NLP, as well as\nshed light on the future of DP NLP in practice and at scale.", "AI": {"tldr": "该研究首次评估了数据集大小对差分隐私自然语言处理（DP NLP）文本重写机制的效用和隐私保护效果的影响，发现数据集大小是一个关键因素，并呼吁更严格的评估程序。", "motivation": "以往的DP NLP文本重写机制评估中，数据集大小这一关键因素常被忽视，未能充分考虑其对机制效用和隐私保护能力的影响。", "method": "研究设计了针对大型数据集（多达一百万文本）的效用和隐私测试，采用动态分割大小，并量化了数据集大小增加对隐私-效用权衡的影响。", "result": "研究发现数据集大小在评估DP文本重写机制中起着不可或缺的作用，并且其增加会影响隐私-效用权衡。", "conclusion": "这些发现表明DP NLP需要更严格的评估程序，并为DP NLP在实践和大规模应用中的未来发展提供了启示。"}}
{"id": "2511.00551", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00551", "abs": "https://arxiv.org/abs/2511.00551", "authors": ["Qiang Li", "Ningjing Zeng", "Lina Yu"], "title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control", "comment": null, "summary": "Several studies have employed reinforcement learning (RL) to address the\nchallenges of regional adaptive traffic signal control (ATSC) and achieved\npromising results. In this field, existing research predominantly adopts\nmulti-agent frameworks. However, the adoption of multi-agent frameworks\npresents challenges for scalability. Instead, the Traffic signal control (TSC)\nproblem necessitates a single-agent framework. TSC inherently relies on\ncentralized management by a single control center, which can monitor traffic\nconditions across all roads in the study area and coordinate the control of all\nintersections. This work proposes a single-agent RL-based regional ATSC model\ncompatible with probe vehicle technology. Key components of the RL design\ninclude state, action, and reward function definitions. To facilitate learning\nand manage congestion, both state and reward functions are defined based on\nqueue length, with action designed to regulate queue dynamics. The queue length\ndefinition used in this study differs slightly from conventional definitions\nbut is closely correlated with congestion states. More importantly, it allows\nfor reliable estimation using link travel time data from probe vehicles. With\nprobe vehicle data already covering most urban roads, this feature enhances the\nproposed method's potential for widespread deployment. The method was\ncomprehensively evaluated using the SUMO simulation platform. Experimental\nresults demonstrate that the proposed model effectively mitigates large-scale\nregional congestion levels via coordinated multi-intersection control.", "AI": {"tldr": "本文提出了一种基于单智能体强化学习的区域自适应交通信号控制模型，该模型利用浮动车数据估算队列长度作为状态和奖励，旨在解决多智能体框架的扩展性问题，并通过协调多交叉口控制有效缓解区域拥堵。", "motivation": "现有区域自适应交通信号控制（ATSC）的强化学习（RL）研究主要采用多智能体框架，但其存在扩展性挑战。交通信号控制（TSC）本质上需要由单一控制中心进行集中管理。因此，需要开发一种可扩展的单智能体RL框架，并利用浮动车技术实现广泛部署。", "method": "本文提出了一种基于单智能体的强化学习区域ATSC模型，该模型与浮动车技术兼容。RL设计关键组件包括：状态、动作和奖励函数定义。状态和奖励函数均基于队列长度定义，动作旨在调节队列动态。研究中使用的队列长度定义与传统定义略有不同，但与拥堵状态密切相关，并且可以通过浮动车提供的路段行程时间数据进行可靠估算。该方法在SUMO仿真平台上进行了综合评估。", "result": "实验结果表明，所提出的模型通过协调多交叉口控制，能够有效缓解大规模区域拥堵水平。", "conclusion": "该研究成功开发并验证了一种基于单智能体强化学习的区域自适应交通信号控制模型，该模型利用浮动车数据进行队列长度估算，有效克服了多智能体框架的扩展性问题，并通过协调控制显著减轻了区域交通拥堵。"}}
{"id": "2511.00917", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00917", "abs": "https://arxiv.org/abs/2511.00917", "authors": ["Junyao Shi", "Rujia Yang", "Kaitian Chao", "Selina Bingqing Wan", "Yifei Shao", "Jiahui Lei", "Jianing Qian", "Long Le", "Pratik Chaudhari", "Kostas Daniilidis", "Chuan Wen", "Dinesh Jayaraman"], "title": "Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots", "comment": "Project website: https://maestro-robot.github.io", "summary": "Today's best-explored routes towards generalist robots center on collecting\never larger \"observations-in actions-out\" robotics datasets to train large\nend-to-end models, copying a recipe that has worked for vision-language models\n(VLMs). We pursue a road less traveled: building generalist policies directly\naround VLMs by augmenting their general capabilities with specific robot\ncapabilities encapsulated in a carefully curated set of perception, planning,\nand control modules. In Maestro, a VLM coding agent dynamically composes these\nmodules into a programmatic policy for the current task and scenario. Maestro's\narchitecture benefits from a streamlined closed-loop interface without many\nmanually imposed structural constraints, and a comprehensive and diverse tool\nrepertoire. As a result, it largely surpasses today's VLA models for zero-shot\nperformance on challenging manipulation skills. Further, Maestro is easily\nextensible to incorporate new modules, easily editable to suit new embodiments\nsuch as a quadruped-mounted arm, and even easily adapts from minimal real-world\nexperiences through local code edits.", "AI": {"tldr": "Maestro提出了一种新的通用机器人策略，通过VLM编码代理动态组合感知、规划和控制模块，以超越现有VLA模型的零样本性能。", "motivation": "当前的通用机器人方法主要依赖于收集大量“观测-行动”数据集来训练大型端到端模型，效仿视觉语言模型（VLMs）的成功。本研究旨在探索一条不同的路径：直接围绕VLMs构建通用策略，通过将VLMs的通用能力与封装在精心策划的感知、规划和控制模块中的特定机器人能力相结合。", "method": "Maestro系统使用一个VLM编码代理，根据当前任务和场景动态地将预先策划的感知、规划和控制模块组合成一个程序化的策略。其架构受益于简化的闭环接口和全面多样的工具库。", "result": "Maestro在具有挑战性的操作技能上，其零样本性能大大超越了当前的视觉语言行动（VLA）模型。此外，Maestro易于扩展以整合新模块，易于编辑以适应新的机器人形态（如四足机器人上的机械臂），甚至可以通过局部代码编辑，从最少的真实世界经验中轻松适应。", "conclusion": "通过将VLM的通用能力与模块化机器人能力相结合，并由VLM编码代理动态编排，Maestro为通用机器人提供了一种高效且灵活的解决方案，在零样本性能和适应性方面表现出色。"}}
{"id": "2511.00765", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00765", "abs": "https://arxiv.org/abs/2511.00765", "authors": ["Shi Gengtian", "Jiang Liu", "Shigeru Shimamoto"], "title": "Deep Q-Network for Optimizing NOMA-Aided Resource Allocation in Smart Factories with URLLC Constraints", "comment": "Accepted for presentation at the IEEE Wireless Communications and\n  Networking Conference (WCNC) 2025. This is the preprint version of the paper", "summary": "This paper presents a Deep Q-Network (DQN)- based algorithm for NOMA-aided\nresource allocation in smart factories, addressing the stringent requirements\nof Ultra-Reliable Low-Latency Communication (URLLC). The proposed algorithm\ndynamically allocates sub-channels and optimizes power levels to maximize\nthroughput while meeting strict latency constraints. By incorporating a tunable\nparameter {\\lambda}, the algorithm balances the trade-off between throughput\nand latency, making it suitable for various devices, including robots, sensors,\nand controllers, each with distinct communication needs. Simulation results\nshow that robots achieve higher throughput, while sensors and controllers meet\nthe low-latency requirements of URLLC, ensuring reliable communication for\nreal-time industrial applications.", "AI": {"tldr": "本文提出一种基于深度Q网络（DQN）的算法，用于NOMA辅助的智能工厂资源分配，旨在满足超可靠低延迟通信（URLLC）的严格要求，并通过可调参数平衡吞吐量和延迟。", "motivation": "智能工厂中的超可靠低延迟通信（URLLC）对NOMA辅助的资源分配提出了严格要求，需要一种能够动态优化资源并平衡吞吐量与延迟的解决方案。", "method": "该研究提出了一种基于DQN的算法，用于NOMA辅助的资源分配。该算法动态分配子信道并优化功率水平，以最大化吞吐量的同时满足严格的延迟约束。通过引入可调参数λ，算法能够平衡吞吐量和延迟之间的权衡。", "result": "仿真结果表明，机器人实现了更高的吞吐量，而传感器和控制器则满足了URLLC的低延迟要求，从而确保了实时工业应用的可靠通信。", "conclusion": "该DQN算法能够为智能工厂中的各种设备（如机器人、传感器和控制器）提供可靠的通信，满足URLLC的严格要求，并为实时工业应用提供支持。"}}
{"id": "2511.00123", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00123", "abs": "https://arxiv.org/abs/2511.00123", "authors": ["Gaby Maroun", "Salah Eddine Bekhouche", "Fadi Dornaika"], "title": "Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation", "comment": null, "summary": "Age estimation from facial images is a complex and multifaceted challenge in\ncomputer vision. In this study, we present a novel hybrid architecture that\ncombines ConvNeXt, a state-of-the-art advancement of convolutional neural\nnetworks (CNNs), with Vision Transformers (ViT). While each model independently\ndelivers excellent performance on a variety of tasks, their integration\nleverages the complementary strengths of the CNNs localized feature extraction\ncapabilities and the Transformers global attention mechanisms. Our proposed\nConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age\nestimation datasets, including MORPH II, CACD, and AFAD, and achieved superior\nperformance in terms of mean absolute error (MAE). To address computational\nconstraints, we leverage pre-trained models and systematically explore\ndifferent configurations, using linear layers and advanced regularization\ntechniques to optimize the architecture. Comprehensive ablation studies\nhighlight the critical role of individual components and training strategies,\nand in particular emphasize the importance of adapted attention mechanisms\nwithin the CNN framework to improve the model focus on age-relevant facial\nfeatures. The results show that the ConvNeXt-ViT hybrid not only outperforms\ntraditional methods, but also provides a robust foundation for future advances\nin age estimation and related visual tasks. This work underscores the\ntransformative potential of hybrid architectures and represents a promising\ndirection for the seamless integration of CNNs and transformers to address\ncomplex computer vision challenges.", "AI": {"tldr": "本研究提出了一种结合ConvNeXt和Vision Transformer的混合架构，用于面部年龄估计。该模型结合了CNN的局部特征提取和Transformer的全局注意力机制，在多个基准数据集上取得了卓越的性能，并为年龄估计及相关视觉任务提供了坚实基础。", "motivation": "面部图像年龄估计是计算机视觉领域的一个复杂挑战。现有模型（如CNN和ViT）各有优势，但通过整合它们的互补优势，可以进一步提升性能，解决单一模型可能存在的局限性。", "method": "本研究提出了一种新颖的ConvNeXt-ViT混合架构。该架构结合了ConvNeXt（CNN的先进变体）的局部特征提取能力和Vision Transformer（ViT）的全局注意力机制。模型在MORPH II、CACD和AFAD等基准年龄估计数据集上进行了评估。研究利用预训练模型，系统地探索了不同的配置，并使用了线性层和高级正则化技术进行优化。此外，还进行了全面的消融研究，特别强调了在CNN框架内调整注意力机制的重要性。", "result": "ConvNeXt-ViT混合解决方案在平均绝对误差（MAE）方面取得了卓越性能，优于传统方法。消融研究强调了各个组件和训练策略的关键作用，特别是适应性注意力机制在提高模型对面部年龄相关特征关注度方面的重要性。结果表明该混合架构不仅性能优越，而且为年龄估计及相关视觉任务的未来发展奠定了坚实基础。", "conclusion": "本研究证明了ConvNeXt-ViT混合架构在面部年龄估计任务中的变革性潜力。它强调了CNN和Transformer无缝集成在解决复杂计算机视觉挑战方面的有效性，并为未来的研究提供了一个有前景的方向。"}}
{"id": "2511.00640", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning.", "AI": {"tldr": "该研究提出DTS框架，通过选择性分支和提前停止，解决大型推理模型（LRMs）因过度思考产生的冗长推理链问题，从而提高推理效率和准确性。", "motivation": "大型推理模型（LRMs）在复杂推理任务上表现出色，但常因过度思考生成过长的思维链（CoT），导致推理成本增加并可能降低准确性。分析表明，短推理路径通常具有更高的正确性，而长路径则容易积累错误和重复。", "method": "DTS是一种与模型无关的解码框架。它通过在“高熵”标记处选择性地进行分支来勾勒推理空间，并应用“提前停止”机制来选择最短的已完成推理路径。这种方法旨在近似最优解，无需额外训练或监督。", "result": "在AIME2024和AIME2025数据集上，使用DeepSeek-R1-Distill-Qwen-7B和1.5B模型进行实验，DTS将准确性提高了高达8%，平均推理长度减少了23%，重复频率降低了12%。", "conclusion": "DTS框架有效解决了LRM的过度思考问题，通过生成更短、更准确的推理路径，显著提高了推理效率和准确性，展现了可扩展和高效的LRM推理能力。"}}
{"id": "2511.00489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00489", "abs": "https://arxiv.org/abs/2511.00489", "authors": ["Jiani Guo", "Zuchao Li", "Jie Wu", "Qianren Wang", "Yun Li", "Lefei Zhang", "Hai Zhao", "Yujiu Yang"], "title": "ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models", "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLMs), constrained by limited context windows, often\nface significant performance degradation when reasoning over long contexts. To\naddress this, Retrieval-Augmented Generation (RAG) retrieves and reasons over\nchunks but frequently sacrifices logical coherence due to its reliance on\nsimilarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split\ndocuments into small chunks for independent reasoning and aggregation. While\neffective for local reasoning, DCF struggles to capture long-range dependencies\nand risks inducing conflicts by processing chunks in isolation. To overcome\nthese limitations, we propose ToM, a novel Tree-oriented MapReduce framework\nfor long-context reasoning. ToM leverages the inherent hierarchical structure\nof long documents (e.g., main headings and subheadings) by constructing a\nDocTree through hierarchical semantic parsing and performing bottom-up\naggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:\nin the Map step, rationales are generated at child nodes; in the Reduce step,\nthese rationales are aggregated across sibling nodes to resolve conflicts or\nreach consensus at parent nodes. Experimental results on 70B+ LLMs show that\nToM significantly outperforms existing divide-and-conquer frameworks and\nretrieval-augmented generation methods, achieving better logical coherence and\nlong-context reasoning. Our code is available at\nhttps://github.com/gjn12-31/ToM .", "AI": {"tldr": "本文提出ToM，一个面向树的MapReduce框架，通过利用长文档的层次结构并进行递归推理，显著提升了大型语言模型（LLMs）在长上下文推理中的逻辑连贯性和性能，优于现有检索增强生成（RAG）和分而治之（DCF）方法。", "motivation": "大型语言模型（LLMs）因上下文窗口限制，在长上下文推理时性能显著下降。现有方法如RAG因依赖相似度排名而牺牲逻辑连贯性；分而治之框架（DCF）虽擅长局部推理，但难以捕捉长距离依赖，且因独立处理块而易产生冲突。因此，需要一种能克服这些局限性的长上下文推理方法。", "method": "本文提出了ToM（Tree-oriented MapReduce framework），一种新颖的面向树的MapReduce框架。ToM通过分层语义解析构建DocTree，利用长文档固有的层次结构（如主标题和副标题）。它采用自下而上的聚合和树形MapReduce方法进行递归推理：在Map步骤中，在子节点生成推理；在Reduce步骤中，聚合兄弟节点间的推理，以在父节点解决冲突或达成共识。", "result": "实验结果表明，ToM在70B+ LLMs上显著优于现有的分而治之框架和检索增强生成方法，实现了更好的逻辑连贯性和长上下文推理能力。", "conclusion": "ToM框架通过其独特的树形结构和递归MapReduce推理方法，有效解决了LLMs在长上下文推理中面临的挑战，克服了RAG和DCF方法的局限性，在逻辑连贯性和推理性能上取得了显著提升。"}}
{"id": "2511.00844", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00844", "abs": "https://arxiv.org/abs/2511.00844", "authors": ["Shuang Qi", "Bin Lin", "Yiqin Deng", "Xianhao Chen", "Yuguang Fang"], "title": "Minimizing Maximum Latency of Task Offloading for Multi-UAV-assisted Maritime Search and Rescue", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) play a crucial role in Maritime Search and\nRescue (MSAR), contributing to the improvement of rescue efficiency and\nreduction of casualties. Typically, UAVs equipped with cameras collect data\nfrom disaster areas and transmit it to the shore-based rescue command centers.\nBy deploying Mobile Edge Computing (MEC) servers, UAVs can pre-process video\nfootage to reduce data transmission volume, thus reducing transmission delays.\nHowever, the limited computational capacity and energy of UAVs pose significant\nchallenges to the efficiency of UAV-assisted MSAR systems. To address these\nproblems, in this paper, we investigate a multi-UAV assisted MSAR system\nconsisting of multiple Surveillance UAVs (S-UAVs) and a Relay UAV (R-UAV).\nThen, we formulate a joint optimization problem to minimize the maximum total\nlatency among all S-UAVs via jointly making the computing offloading decisions,\nR-UAV deployment, and the association between a S-UAV and rescue targets while\nensuring that all targets are monitored by S-UAVs. Since the formulated\noptimization problem is typically hard to solve due to its non-convexity, we\npropose an effective iterative algorithm by breaking it into three\nsub-problems. Numerical simulation results show the effectiveness of the\nproposed algorithm with various performance parameters.", "AI": {"tldr": "本文研究了一种多无人机辅助的海上搜救（MSAR）系统，通过联合优化计算卸载决策、中继无人机部署和监控无人机与搜救目标的关联，旨在最小化所有监控无人机的最大总延迟，以提高搜救效率。", "motivation": "无人机在海上搜救中作用关键，但其有限的计算能力和能量给无人机辅助的MSAR系统带来了挑战，导致数据传输量大和传输延迟高，即使部署移动边缘计算（MEC）服务器进行预处理也未能完全解决。", "method": "提出一个由多个监控无人机（S-UAV）和一个中继无人机（R-UAV）组成的多无人机辅助MSAR系统。构建了一个联合优化问题，通过同时优化计算卸载决策、R-UAV部署和S-UAV与搜救目标的关联，来最小化所有S-UAV的最大总延迟，并确保所有目标都被监控。由于该问题非凸，将其分解为三个子问题并提出了一个有效的迭代算法。", "result": "数值仿真结果表明，所提出的算法在各种性能参数下都具有有效性。", "conclusion": "所提出的迭代算法能有效解决多无人机辅助MSAR系统中的延迟最小化问题，从而提高搜救效率。"}}
{"id": "2511.00141", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00141", "abs": "https://arxiv.org/abs/2511.00141", "authors": ["Janghoon Cho", "Jungsoo Lee", "Munawar Hayat", "Kyuwoong Hwang", "Fatih Porikli", "Sungha Choi"], "title": "FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding", "comment": null, "summary": "Recent studies in long video understanding have harnessed the advanced\nvisual-language reasoning capabilities of Large Multimodal Models (LMMs),\ndriving the evolution of video-LMMs specialized for processing extended video\nsequences. However, the scalability of these models is severely limited by the\noverwhelming volume of visual tokens generated from extended video sequences.\nTo address this challenge, this paper proposes FLoC, an efficient visual token\ncompression framework based on the facility location function, a principled\napproach that swiftly selects a compact yet highly representative and diverse\nsubset of visual tokens within a predefined budget on the number of visual\ntokens. By integrating the lazy greedy algorithm, our method achieves\nremarkable efficiency gains by swiftly selecting a compact subset of tokens,\ndrastically reducing the number of visual tokens while guaranteeing\nnear-optimal performance. Notably, our approach is training-free,\nmodel-agnostic, and query-agnostic, providing a versatile solution that\nseamlessly integrates with diverse video-LLMs and existing workflows. Extensive\nevaluations on large-scale benchmarks, such as Video-MME, MLVU, and\nLongVideoBench, demonstrate that our framework consistently surpasses recent\ncompression techniques, highlighting not only its effectiveness and robustness\nin addressing the critical challenges of long video understanding, but also its\nefficiency in processing speed.", "AI": {"tldr": "本文提出FLoC，一个基于设施选址函数的视觉token压缩框架，旨在高效地为长视频理解选择紧凑且具有代表性的视觉token子集，从而解决LMMs在处理长视频时面临的视觉token量过大的问题。", "motivation": "长视频理解中的大型多模态模型（LMMs）受到长视频序列生成的海量视觉token的严重限制，导致模型可扩展性不足。", "method": "本文提出了FLoC框架，它利用设施选址函数（一种在预定义预算内选择紧凑、高代表性和多样性视觉token子集的方法）进行视觉token压缩。通过集成惰性贪婪算法，FLoC在保证近乎最优性能的同时，显著提高了效率，并具有免训练、模型无关和查询无关的特点。", "result": "在Video-MME、MLVU和LongVideoBench等大规模基准测试中，FLoC框架持续超越了现有压缩技术，不仅展示了其在解决长视频理解关键挑战方面的有效性和鲁棒性，还体现了其在处理速度上的高效率。", "conclusion": "FLoC提供了一个多功能的解决方案，能够无缝集成到各种视频-LLM和现有工作流程中，有效且高效地解决了长视频理解中的视觉token过载问题，显著提升了处理效率和性能。"}}
{"id": "2511.00933", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00933", "abs": "https://arxiv.org/abs/2511.00933", "authors": ["Xiangyu Shi", "Zerui Li", "Yanyuan Qiao", "Qi Wu"], "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation", "comment": null, "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve\nzero-shot navigation. However, existing methods often rely on panoramic\nobservations and two-stage pipelines involving waypoint predictors, which\nintroduce significant latency and limit real-world applicability. In this work,\nwe propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that\neliminates the need for panoramic views and waypoint predictors. Our approach\nuses only three frontal RGB-D images combined with natural language\ninstructions, enabling MLLMs to directly predict actions. To enhance decision\nrobustness, we introduce an Uncertainty-Aware Reasoning module that integrates\n(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past\nBidirectional Reasoning mechanism for globally coherent planning. Experiments\non both simulated and real-robot environments demonstrate that our method\nsignificantly reduces per-step latency while achieving competitive or superior\nperformance compared to panoramic-view baselines. These results demonstrate the\npracticality and effectiveness of Fast-SmartWay for real-world zero-shot\nembodied navigation.", "AI": {"tldr": "本文提出了Fast-SmartWay，一个端到端、零样本的连续环境视觉语言导航（VLN-CE）框架，它通过仅使用三个正面RGB-D图像和多模态大语言模型（MLLMs）直接预测动作，显著降低了延迟并提高了实时导航的实用性。", "motivation": "现有的VLN-CE方法通常依赖全景观测和包含路径点预测器的两阶段流程，这导致显著的延迟并限制了其在真实世界中的应用。", "method": "Fast-SmartWay是一个端到端、零样本的VLN-CE框架。它不依赖全景视图和路径点预测器，而是使用三个正面RGB-D图像结合自然语言指令，使MLLMs能够直接预测动作。为了增强决策鲁棒性，该方法引入了一个不确定性感知推理模块，该模块集成了用于避免局部最优的消歧模块和用于全局连贯规划的未来-过去双向推理机制。", "result": "在模拟和真实机器人环境中的实验表明，Fast-SmartWay显著降低了每步延迟，同时实现了与基于全景视图的基线方法相比具有竞争力或更优的性能。", "conclusion": "这些结果证明了Fast-SmartWay在真实世界零样本具身导航中的实用性和有效性。"}}
{"id": "2511.00940", "categories": ["cs.RO", "cs.AI", "I.2.6"], "pdf": "https://arxiv.org/pdf/2511.00940", "abs": "https://arxiv.org/abs/2511.00940", "authors": ["Zhe Li", "Xiang Bai", "Jieyu Zhang", "Zhuangzhe Wu", "Che Xu", "Ying Li", "Chengkai Hou", "Shanghang Zhang"], "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model", "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Constructing accurate digital twins of articulated objects is essential for\nrobotic simulation training and embodied AI world model building, yet\nhistorically requires painstaking manual modeling or multi-stage pipelines. In\nthis work, we propose \\textbf{URDF-Anything}, an end-to-end automatic\nreconstruction framework based on a 3D multimodal large language model (MLLM).\nURDF-Anything utilizes an autoregressive prediction framework based on\npoint-cloud and text multimodal input to jointly optimize geometric\nsegmentation and kinematic parameter prediction. It implements a specialized\n$[SEG]$ token mechanism that interacts directly with point cloud features,\nenabling fine-grained part-level segmentation while maintaining consistency\nwith the kinematic parameter predictions. Experiments on both simulated and\nreal-world datasets demonstrate that our method significantly outperforms\nexisting approaches regarding geometric segmentation (mIoU 17\\% improvement),\nkinematic parameter prediction (average error reduction of 29\\%), and physical\nexecutability (surpassing baselines by 50\\%). Notably, our method exhibits\nexcellent generalization ability, performing well even on objects outside the\ntraining set. This work provides an efficient solution for constructing digital\ntwins for robotic simulation, significantly enhancing the sim-to-real transfer\ncapability.", "AI": {"tldr": "URDF-Anything是一个基于3D多模态大语言模型（MLLM）的端到端框架，能自动重建铰接物体的数字孪生，同时优化几何分割和运动学参数预测，显著优于现有方法并具有出色的泛化能力。", "motivation": "为机器人仿真训练和具身AI世界模型构建精确的铰接物体数字孪生至关重要，但传统方法需要耗时的人工建模或多阶段流程。", "method": "本文提出了URDF-Anything，一个基于3D多模态大语言模型（MLLM）的端到端自动重建框架。它利用点云和文本多模态输入的自回归预测框架，共同优化几何分割和运动学参数预测。该方法实现了一个专门的`[SEG]`令牌机制，直接与点云特征交互，实现细粒度的部件级分割，同时与运动学参数预测保持一致性。", "result": "在仿真和真实世界数据集上的实验表明，该方法在几何分割（mIoU提高17%）、运动学参数预测（平均误差减少29%）和物理可执行性（超越基线50%）方面显著优于现有方法。此外，该方法表现出卓越的泛化能力，即使在训练集之外的物体上也能表现良好。", "conclusion": "URDF-Anything为机器人仿真构建数字孪生提供了一种高效解决方案，显著增强了从仿真到现实的迁移能力。"}}
{"id": "2511.00505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00505", "abs": "https://arxiv.org/abs/2511.00505", "authors": ["Qi Luo", "Xiaonan Li", "Junqi Dai", "Shuang Cheng", "Xipeng Qiu"], "title": "Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge", "comment": null, "summary": "Retrieval-Augmented Generation has shown remarkable results to address Large\nLanguage Models' hallucinations, which usually uses a large external corpus to\nsupplement knowledge to LLMs. However, with the development of LLMs, the\ninternal knowledge of LLMs has expanded significantly, thus causing significant\nknowledge redundancy between the external corpus and LLMs. On the one hand, the\nindexing cost of dense retrieval is highly related to the corpus size and thus\nsignificant redundant knowledge intensifies the dense retrieval's workload. On\nthe other hand, the redundant knowledge in the external corpus is not helpful\nto LLMs and our exploratory analysis shows that it instead hurts the RAG\nperformance on those questions which the LLM can answer by itself. To address\nthese issues, we propose Zero-RAG to tackle these challenges. Specifically, we\nfirst propose the Mastery-Score metric to identify redundant knowledge in the\nRAG corpus to prune it. After pruning, answers to \"mastered\" questions rely\nprimarily on internal knowledge of the LLM. To better harness the internal\ncapacity, we propose Query Router and Noise-Tolerant Tuning to avoid the\nirrelevant documents' distraction and thus further improve the LLM's\nutilization of internal knowledge with pruned corpus. Experimental results show\nthat Zero-RAG prunes the Wikipedia corpus by 30\\% and accelerates the retrieval\nstage by 22\\%, without compromising RAG's performance.", "AI": {"tldr": "该研究提出了Zero-RAG框架，通过识别并修剪RAG外部语料库中的冗余知识，并结合查询路由器和噪声容忍微调，以更有效地利用大型语言模型（LLM）的内部知识，从而提高检索效率而不牺牲性能。", "motivation": "传统的检索增强生成（RAG）方法使用大型外部语料库补充LLM知识，但随着LLM内部知识的显著扩展，外部语料库与LLM之间存在大量知识冗余。这种冗余不仅增加了稠密检索的索引成本和工作量，而且对LLM能够自行回答的问题，冗余知识反而会损害RAG性能。", "method": "1. 提出了“Mastery-Score”指标来识别并修剪RAG语料库中的冗余知识，使LLM对“已掌握”问题的回答主要依赖其内部知识。2. 提出了“Query Router”和“Noise-Tolerant Tuning”机制，以避免无关文档的干扰，进一步提升LLM在修剪后语料库下对内部知识的利用能力。", "result": "实验结果表明，Zero-RAG成功将Wikipedia语料库修剪了30%，并将检索阶段的速度提高了22%，同时没有损害RAG的整体性能。", "conclusion": "Zero-RAG通过有效识别和修剪RAG语料库中的冗余知识，并优化LLM对内部知识的利用，显著提高了检索效率，降低了成本，且保持了RAG的性能，为解决RAG中的知识冗余问题提供了一个有效方案。"}}
{"id": "2511.00143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00143", "abs": "https://arxiv.org/abs/2511.00143", "authors": ["Jinsu Kim", "Yunhun Nam", "Minseon Kim", "Sangpil Kim", "Jongheon Jeong"], "title": "BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing", "comment": "36 pages; NeurIPS 2025; Code is available at\n  https://github.com/jsu-kim/BlurGuard", "summary": "Recent advances in text-to-image models have increased the exposure of\npowerful image editing techniques as a tool, raising concerns about their\npotential for malicious use. An emerging line of research to address such\nthreats focuses on implanting \"protective\" adversarial noise into images before\ntheir public release, so future attempts to edit them using text-to-image\nmodels can be impeded. However, subsequent works have shown that these\nadversarial noises are often easily \"reversed,\" e.g., with techniques as simple\nas JPEG compression, casting doubt on the practicality of the approach. In this\npaper, we argue that adversarial noise for image protection should not only be\nimperceptible, as has been a primary focus of prior work, but also\nirreversible, viz., it should be difficult to detect as noise provided that the\noriginal image is hidden. We propose a surprisingly simple method to enhance\nthe robustness of image protection methods against noise reversal techniques.\nSpecifically, it applies an adaptive per-region Gaussian blur on the noise to\nadjust the overall frequency spectrum. Through extensive experiments, we show\nthat our method consistently improves the per-sample worst-case protection\nperformance of existing methods against a wide range of reversal techniques on\ndiverse image editing scenarios, while also reducing quality degradation due to\nnoise in terms of perceptual metrics. Code is available at\nhttps://github.com/jsu-kim/BlurGuard.", "AI": {"tldr": "本文提出了一种简单方法，通过对对抗性噪声应用自适应区域高斯模糊来调整其频率谱，从而增强图像保护方法抵御噪声逆转技术（如JPEG压缩）的鲁棒性，使其更难以被检测和移除。", "motivation": "文本到图像模型带来了强大的图像编辑能力，也引发了恶意使用的担忧。现有的图像保护方法通过植入对抗性噪声来阻止编辑，但这些噪声很容易被逆转（例如通过JPEG压缩），这使得其实用性受到质疑。因此，需要开发不仅不可感知，而且难以逆转的保护性对抗性噪声。", "method": "本文提出了一种简单的方法来增强图像保护噪声的鲁棒性。具体来说，它在噪声上应用了自适应的按区域高斯模糊，以调整整体的频率谱，使其更难以被检测为噪声，同时提高了其不可逆性。", "result": "通过广泛的实验，本文的方法显著提高了现有图像保护方法在各种图像编辑场景下，针对多种逆转技术的样本最差保护性能。同时，它还通过感知度量降低了噪声导致的图像质量下降。", "conclusion": "对抗性噪声的图像保护不仅应不可感知，还应不可逆转。本文提出的自适应区域高斯模糊方法，通过调整噪声的频率谱，有效地增强了图像保护方法抵御噪声逆转技术的鲁棒性，提高了其实用性和性能。"}}
{"id": "2511.00673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00673", "abs": "https://arxiv.org/abs/2511.00673", "authors": ["Dominik Drexler"], "title": "Lifted Successor Generation in Numeric Planning", "comment": null, "summary": "Most planners ground numeric planning tasks, given in a first-order-like\nlanguage, into a ground task representation. However, this can lead to an\nexponential blowup in task representation size, which occurs in practice for\nhard-to-ground tasks. We extend a state-of-the-art lifted successor generator\nfor classical planning to support numeric precondition applicability. The\nmethod enumerates maximum cliques in a substitution consistency graph. Each\nmaximum clique represents a substitution for the variables of the action\nschema, yielding a ground action. We augment this graph with numeric action\npreconditions and prove the successor generator is exact under formally\nspecified conditions. When the conditions fail, our generator may list\ninapplicable ground actions; a final applicability check filters these without\naffecting completeness. However, this cannot happen in 23 of 25 benchmark\ndomains, and it occurs only in 1 domain. To the authors' knowledge, no other\nlifted successor generator supports numeric action preconditions. This enables\nfuture research on lifted planning for a very rich planning fragment.", "AI": {"tldr": "该研究提出了一种提升式（lifted）后继状态生成器，能够支持数值型动作前置条件，以避免传统数值规划中接地（grounding）导致的指数级状态表示膨胀问题。", "motivation": "传统的数值规划任务接地过程可能导致任务表示大小的指数级膨胀，这在实践中对于难以接地的任务尤为明显。", "method": "该方法扩展了现有经典的提升式后继状态生成器，使其支持数值型前置条件。它通过在替换一致性图中枚举最大团来生成接地动作，并将数值型动作前置条件加入到该图中。", "result": "该后继状态生成器在特定条件下是精确的。当条件不满足时，它可能会列出不适用的接地动作，但可通过最终适用性检查过滤，且不影响完备性。在25个基准领域中，有23个领域不会出现这种情况，仅在1个领域中发生。据作者所知，这是首个支持数值型动作前置条件的提升式后继状态生成器。", "conclusion": "该研究通过支持数值型动作前置条件，为丰富规划片段的提升式规划开辟了未来的研究方向。"}}
{"id": "2511.00651", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains.", "AI": {"tldr": "本文提出一个基于多智能体系统（MAS）的解决方案，利用大型语言模型（LLM）协调多种专业工具，并结合领域定制的小型语言模型（SLM），实现电信网络的自动化故障排除，显著加速故障诊断和修复。", "motivation": "电信网络规模和复杂性日益增长，管理、运营和优化面临巨大挑战。现有AI模型在电信任务中应用范围狭窄，需要大量标注数据，且难以推广到异构部署。因此，网络故障排除仍高度依赖人工专家手动关联数据以识别根本原因和纠正措施。", "method": "本文提出了一个多智能体系统（MAS），采用智能体工作流，由大型语言模型（LLM）协调多个专业工具，实现全自动网络故障排除。一旦AI/ML监控器检测到故障，该框架会动态激活编排器、解决方案规划器、执行器、数据检索器和根本原因分析器等智能体。其中，解决方案规划器是关键组件，它通过对专有故障排除文档进行微调的小型语言模型（SLM）生成领域相关的修复计划。", "result": "实验结果表明，所提出的框架显著加速了无线接入网（RAN）和核心网域的故障排除自动化。", "conclusion": "该多智能体系统（MAS）通过结合LLM的协调能力和SLM的领域专业知识，有效克服了传统方法的局限性，为电信网络的自动化故障排除提供了一个高效且可推广的解决方案，大幅提升了故障诊断和修复的速度。"}}
{"id": "2511.00941", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00941", "abs": "https://arxiv.org/abs/2511.00941", "authors": ["Dipanjan Ghose", "S Sivaranjani", "Junjie Qin"], "title": "Traffic-Aware Grid Planning for Dynamic Wireless Electric Vehicle Charging", "comment": null, "summary": "Dynamic Wireless Electric Vehicle Charging (DWC) on electrified roadways is\nan emerging technology that can significantly reduce battery sizes, eliminate\ncharging downtime, and alleviate range anxiety, specially for long-haul\ntransportation and fleet operations of electric vehicles (EVs). However, these\nsystems introduce new challenges for power system planning due to their\nshort-duration and high-power demands which can strain the grid if not properly\nmanaged. As the energy demands from DWC depend on vehicle speed, density, dwell\ntime in charging zones, and load profiles along road segments, there is a need\nfor integrated planning of such systems, jointly considering both traffic\nbehavior and EV energy consumption. In this paper, we propose a traffic-aware\ngrid planning framework for DWC. We leverage a macroscopic Cell Transmission\nModel of traffic flow to estimate real-time, spatiotemporal EV charging demand\nfrom DWC corridors. The demand model is then integrated into an AC Optimal\nPower Flow based formulation to optimally size a microgrid that supports DWC\nunder varying traffic conditions while minimizing the cost of operation. Our\nframework explicitly models how spatiotemporal traffic patterns affect the\nutilization of grid resources to obtain system designs that achieve lower costs\nand are easier to operationalize as compared to planning models that rely on\nworst-case traffic data.\n  We demonstrate the framework on data from a 14-mile segment of the I-210W\nhighway in California, USA, evaluating multiple traffic scenarios like\nfree-flow, severe congestion, accidents of varying severity, and natural\ndisasters like forest fires. Our results demonstrate that traffic-aware grid\nplanning significantly reduces infrastructure costs as compared to\nworst-scenario based modeling, while ensuring reliability of service in terms\nof meeting charging demands under diverse traffic conditions.", "AI": {"tldr": "本文提出了一种交通感知的动态无线电动汽车充电（DWC）电网规划框架，通过整合实时交通流模型和电力优化流，显著降低了基础设施成本并确保了服务可靠性。", "motivation": "动态无线电动汽车充电（DWC）技术能显著减少电池尺寸、消除充电停机时间并缓解里程焦虑，但其短时高功率需求对电网规划提出了新挑战。现有规划需综合考虑交通行为和电动汽车能耗，以应对DWC对电网的潜在压力。", "method": "本文提出了一个交通感知的DWC电网规划框架。该框架利用宏观小区传输模型（CTM）估算DWC走廊的实时时空电动汽车充电需求。随后，将需求模型整合到基于交流最优潮流（AC-OPF）的公式中，以优化微电网的规模，从而在不同交通条件下支持DWC并最小化运营成本。该方法明确建模了时空交通模式如何影响电网资源利用，与基于最坏情况交通数据的规划模型相比，能获得更低成本且更易操作的系统设计。", "result": "该框架在加州I-210W高速公路14英里路段的数据上进行了验证，评估了自由流、严重拥堵、不同严重程度的事故以及森林火灾等自然灾害等多种交通场景。结果表明，与基于最坏情况的建模相比，交通感知的电网规划显著降低了基础设施成本，同时在各种交通条件下确保了满足充电需求的服务可靠性。", "conclusion": "交通感知的DWC电网规划框架能够有效整合交通行为和电网管理，相比传统的最坏情况规划，能以更低的成本设计出更可靠、更易于操作的DWC系统，从而更好地支持电动汽车的普及和电网的稳定运行。"}}
{"id": "2511.00519", "categories": ["cs.CL", "I.2.7; I.7.1; K.4.1"], "pdf": "https://arxiv.org/pdf/2511.00519", "abs": "https://arxiv.org/abs/2511.00519", "authors": ["Ariyan Hossain", "Khondokar Mohammad Ahanaf Hannan", "Rakinul Haque", "Nowreen Tarannum Rafa", "Humayra Musarrat", "Shoaib Ahmed Dipu", "Farig Yousuf Sadeque"], "title": "Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models", "comment": "25 pages, 20 figures", "summary": "Gender bias in language models has gained increasing attention in the field\nof natural language processing. Encoder-based transformer models, which have\nachieved state-of-the-art performance in various language tasks, have been\nshown to exhibit strong gender biases inherited from their training data. This\npaper investigates gender bias in contextualized word embeddings, a crucial\ncomponent of transformer-based models. We focus on prominent architectures such\nas BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to\ngender bias. To quantify the degree of bias, we introduce a novel metric,\nMALoR, which assesses bias based on model probabilities for filling masked\ntokens. We further propose a mitigation approach involving continued\npre-training on a gender-balanced dataset generated via Counterfactual Data\nAugmentation. Our experiments reveal significant reductions in gender bias\nscores across different pronoun pairs. For instance, in BERT-base, bias scores\nfor \"he-she\" dropped from 1.27 to 0.08, and \"his-her\" from 2.51 to 0.36\nfollowing our mitigation approach. We also observed similar improvements across\nother models, with \"male-female\" bias decreasing from 1.82 to 0.10 in\nBERT-large. Our approach effectively reduces gender bias without compromising\nmodel performance on downstream tasks.", "AI": {"tldr": "该研究调查了BERT、ALBERT等Transformer模型上下文词嵌入中的性别偏见，引入了新的量化指标MALoR，并提出通过在性别平衡数据集上继续预训练来缓解偏见的方法，实验证明该方法显著降低了性别偏见且不影响模型性能。", "motivation": "编码器Transformer模型在各种语言任务中表现出色，但也被发现继承了训练数据中的性别偏见。因此，研究和缓解这些模型（特别是其上下文词嵌入）中的性别偏见变得日益重要。", "method": "研究了BERT、ALBERT、RoBERTa和DistilBERT等主流Transformer架构中的性别偏见。引入了MALoR（Masked-token probability-based Assessment of Language-model-induced-bias over Representations）这一新指标，通过模型填充掩码标记的概率来量化偏见程度。提出了一种缓解方法：利用反事实数据增强（Counterfactual Data Augmentation）生成性别平衡的数据集，并在此数据集上进行持续预训练。", "result": "实验结果显示，通过提出的缓解方法，性别偏见分数显著降低。例如，在BERT-base模型中，“he-she”的偏见分数从1.27降至0.08，“his-her”从2.51降至0.36。在BERT-large模型中，“male-female”的偏见从1.82降至0.10。此外，该方法在不损害模型在下游任务性能的情况下，有效减少了性别偏见。", "conclusion": "该研究提出的缓解方法能够有效降低Transformer模型中的性别偏见，且不会牺牲模型在下游任务上的性能。这为构建更公平的语言模型提供了实用的途径。"}}
{"id": "2511.00514", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00514", "abs": "https://arxiv.org/abs/2511.00514", "authors": ["Birat Poudel", "Satyam Ghimire", "Er. Prakash Chandra Prasad"], "title": "Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations", "comment": "6 pages, 6 figures, 3 tables", "summary": "Conversational agents are increasingly being explored to support healthcare\ndelivery, particularly in resource-constrained settings such as rural Nepal.\nLarge-scale conversational models typically rely on internet connectivity and\ncloud infrastructure, which may not be accessible in rural areas. In this\nstudy, we fine-tuned DialoGPT, a lightweight generative dialogue model that can\noperate offline, on a synthetically constructed dataset of doctor-patient\ninteractions covering ten common diseases prevalent in rural Nepal, including\ncommon cold, seasonal fever, diarrhea, typhoid fever, gastritis, food\npoisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being\ntrained on a limited, domain-specific dataset, the fine-tuned model produced\ncoherent, contextually relevant, and medically appropriate responses,\ndemonstrating an understanding of symptoms, disease context, and empathetic\ncommunication. These results highlight the adaptability of compact,\noffline-capable dialogue models and the effectiveness of targeted datasets for\ndomain adaptation in low-resource healthcare environments, offering promising\ndirections for future rural medical conversational AI.", "AI": {"tldr": "本研究在合成的医患对话数据集上微调了轻量级离线对话模型DialoGPT，以支持尼泊尔农村地区的医疗服务，并取得了良好的医疗相关和共情响应。", "motivation": "会话代理在医疗保健领域，特别是在尼泊尔农村等资源受限地区具有潜力。然而，大型会话模型通常依赖互联网和云基础设施，这在农村地区可能无法访问。", "method": "研究者在一个人工构建的、包含尼泊尔农村地区十种常见疾病（如普通感冒、腹泻、伤寒等）的医患互动数据集上，微调了轻量级生成式对话模型DialoGPT，使其能够离线运行。", "result": "尽管在有限的、特定领域数据集上训练，微调后的模型仍能产生连贯、上下文相关且医学上适当的响应，表现出对症状、疾病背景的理解和共情交流能力。", "conclusion": "研究结果突出了紧凑型、离线对话模型的适应性，以及目标数据集在低资源医疗环境中进行领域适应的有效性，为未来农村医疗会话AI提供了有前景的方向。"}}
{"id": "2511.00998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00998", "abs": "https://arxiv.org/abs/2511.00998", "authors": ["Ziye Wang", "Li Kang", "Yiran Qin", "Jiahua Ma", "Zhanglin Peng", "Lei Bai", "Ruimao Zhang"], "title": "GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies", "comment": "Accepted by NeurIPS 2025. Project page:\n  https://ziyeeee.github.io/gaudp.io/", "summary": "Recently, effective coordination in embodied multi-agent systems has remained\na fundamental challenge, particularly in scenarios where agents must balance\nindividual perspectives with global environmental awareness. Existing\napproaches often struggle to balance fine-grained local control with\ncomprehensive scene understanding, resulting in limited scalability and\ncompromised collaboration quality. In this paper, we present GauDP, a novel\nGaussian-image synergistic representation that facilitates scalable,\nperception-aware imitation learning in multi-agent collaborative systems.\nSpecifically, GauDP constructs a globally consistent 3D Gaussian field from\ndecentralized RGB observations, then dynamically redistributes 3D Gaussian\nattributes to each agent's local perspective. This enables all agents to\nadaptively query task-critical features from the shared scene representation\nwhile maintaining their individual viewpoints. This design facilitates both\nfine-grained control and globally coherent behavior without requiring\nadditional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the\nRoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our\nmethod achieves superior performance over existing image-based methods and\napproaches the effectiveness of point-cloud-driven methods, while maintaining\nstrong scalability as the number of agents increases.", "AI": {"tldr": "本文提出GauDP，一种高斯图像协同表示，用于多智能体系统中的可扩展、感知感知的模仿学习，通过构建全局一致的3D高斯场并动态分配给局部视角，以平衡局部控制和全局环境感知。", "motivation": "现有具身多智能体系统在平衡个体视角与全局环境感知方面面临挑战，难以兼顾细粒度局部控制与全面场景理解，导致可扩展性受限和协作质量下降。", "method": "GauDP方法首先从分散的RGB观测中构建一个全局一致的3D高斯场，然后将3D高斯属性动态重新分配到每个智能体的局部视角。这使得所有智能体都能在保持各自独立视点的同时，从共享场景表示中自适应地查询任务关键特征。", "result": "GauDP在RoboFactory基准测试中，性能优于现有基于图像的方法，并接近点云驱动方法的有效性，同时在智能体数量增加时保持强大的可扩展性。", "conclusion": "GauDP设计实现了细粒度控制和全局连贯行为的平衡，且无需额外的感知模态（如3D点云），有效解决了多智能体协作中的关键挑战，并展现出优越的性能和可扩展性。"}}
{"id": "2511.00963", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.00963", "abs": "https://arxiv.org/abs/2511.00963", "authors": ["Jiahao Huang", "Marios M. Polycarpou", "Wen Yang", "Fangfei Li", "Yang Tang"], "title": "Secure Distributed Consensus Estimation under False Data Injection Attacks: A Defense Strategy Based on Partial Channel Coding", "comment": null, "summary": "This article investigates the security issue caused by false data injection\nattacks in distributed estimation, wherein each sensor can construct two types\nof residues based on local estimates and neighbor information, respectively.\nThe resource-constrained attacker can select partial channels from the sensor\nnetwork and arbitrarily manipulate the transmitted data. We derive necessary\nand sufficient conditions to reveal system vulnerabilities, under which the\nattacker is able to diverge the estimation error while preserving the\nstealthiness of all residues. We propose two defense strategies with mechanisms\nof exploiting the Euclidean distance between local estimates to detect attacks,\nand adopting the coding scheme to protect the transmitted data, respectively.\nIt is proven that the former has the capability to address the majority of\nsecurity loopholes, while the latter can serve as an additional enhancement to\nthe former. By employing the time-varying coding matrix to mitigate the risk of\nbeing cracked, we demonstrate that the latter can safeguard against adversaries\ninjecting stealthy sequences into the encoded channels. Hence, drawing upon the\nsecurity analysis, we further provide a procedure to select security-critical\nchannels that need to be encoded, thereby achieving a trade-off between\nsecurity and coding costs. Finally, some numerical simulations are conducted to\ndemonstrate the theoretical results.", "AI": {"tldr": "本文研究了分布式估计中虚假数据注入攻击造成的安全问题，提出了攻击者在保持隐蔽性的前提下使估计误差发散的条件，并提出了两种防御策略：基于局部估计欧氏距离的检测和基于编码方案的数据保护，以提高系统安全性并平衡编码成本。", "motivation": "分布式估计中存在虚假数据注入攻击的安全隐患，攻击者可以操纵传输数据，导致估计误差发散，同时保持隐蔽性，这促使研究者寻找方法来揭示系统漏洞并开发有效的防御策略。", "method": "本文采用的方法包括：1. 推导攻击者在保持所有残差隐蔽性的同时使估计误差发散的必要和充分条件。2. 提出两种防御策略：a) 利用局部估计之间的欧氏距离来检测攻击。b) 采用编码方案（特别是时变编码矩阵）来保护传输数据。3. 进行安全分析以提供选择需要编码的安全关键通道的程序。", "result": "研究结果表明：1. 导出了攻击者在不被发现的情况下使估计误差发散的系统漏洞条件。2. 第一种防御策略（利用欧氏距离检测）能够解决大多数安全漏洞。3. 第二种防御策略（编码方案）可以作为前者的额外增强，特别是采用时变编码矩阵时，能有效抵御注入隐蔽序列的攻击。4. 提出了一个选择安全关键通道进行编码的程序，以在安全性和编码成本之间取得平衡。", "conclusion": "本文得出结论，通过结合利用局部估计欧氏距离的攻击检测机制和采用时变编码矩阵的数据保护方案，可以有效应对分布式估计中的虚假数据注入攻击。这种综合防御策略能够解决大部分安全漏洞，并能在保障安全性的同时优化编码成本。"}}
{"id": "2511.01022", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01022", "abs": "https://arxiv.org/abs/2511.01022", "authors": ["Xingyu Ren", "Michael C. Fu", "Steven I. Marcus"], "title": "On Structural Properties of Risk-Averse Optimal Stopping Problems", "comment": null, "summary": "We establish structural properties of optimal stopping problems under\ntime-consistent dynamic (coherent) risk measures, focusing on value function\nmonotonicity and the existence of control limit (threshold) optimal policies.\nWhile such results are well developed for risk-neutral (expected-value) models,\nthey remain underexplored in risk-averse settings. Coherent risk measures\ntypically lack the tower property and are subadditive rather than additive,\ncomplicating structural analysis. We show that value function monotonicity\nmirrors the risk-neutral case. Moreover, if the risk envelope associated with\neach coherent risk measure admits a minimal element, the risk-averse optimal\nstopping problem reduces to an equivalent risk-neutral formulation. We also\ndevelop a general procedure for identifying control limit optimal policies and\nuse it to derive practical, verifiable conditions on the risk measures and MDP\nstructure that guarantee their existence. We illustrate the theory and verify\nthese conditions through optimal stopping problems arising in operations,\nmarketing, and finance.", "AI": {"tldr": "本文研究了时间一致动态相干风险度量下最优停止问题的结构性质，包括值函数的单调性和控制限最优策略的存在性，并为识别此类策略提供了通用方法和可验证条件。", "motivation": "风险中性（期望值）模型的最优停止问题结构性质已得到充分发展，但在风险规避（使用时间一致动态相干风险度量）设置中，这些性质（如值函数单调性、控制限策略）仍未得到充分探索。相干风险度量缺乏塔性质且是次可加而非可加的，这使得结构分析复杂化。", "method": "研究了值函数的单调性，并发现它与风险中性情况相似。证明了如果与每个相干风险度量相关的风险包络承认一个最小元素，则风险规避最优停止问题可简化为等价的风险中性公式。开发了一个识别控制限最优策略的通用程序，并用其推导了保证其存在的、关于风险度量和MDP结构的实际可验证条件。通过运营、营销和金融中的最优停止问题案例来验证了理论和条件。", "result": "值函数的单调性与风险中性情况一致。如果风险包络存在最小元素，风险规避最优停止问题可简化为等价的风险中性形式。开发了识别控制限最优策略的通用程序，并导出了保证其存在的实用、可验证条件。", "conclusion": "在时间一致动态相干风险度量下，成功建立了最优停止问题的结构性质，包括值函数的单调性，并为控制限最优策略的存在性提供了理论基础和实用条件。这弥合了风险中性和风险规避最优停止问题之间分析上的差距。"}}
{"id": "2511.00710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00710", "abs": "https://arxiv.org/abs/2511.00710", "authors": ["Minghe Shen", "Zhuo Zhi", "Chonghan Liu", "Shuo Xing", "Zhengzhong Tu", "Che Liu"], "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "comment": null, "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.", "AI": {"tldr": "本研究通过引入Ariadne框架，利用合成迷宫和RLVR训练方法，证明了强化学习后训练可以显著扩展视觉-语言模型（VLM）在视觉中心空间推理任务上的固有能力边界，并在现实世界基准测试中展现了强大的泛化能力。", "motivation": "尽管通过强化学习（RL）后训练的视觉-语言模型（VLM）在通用推理方面表现出色，但其评估常局限于语言主导的任务。这引出了一个关键问题：RL后训练能否真正扩展基础VLM的固有能力边界，特别是在其最初失败的视觉中心空间任务上？", "method": "引入了Ariadne框架，利用合成迷宫进行多步空间推理，并精确控制任务难度。该框架通过难度感知的课程，使用带验证奖励的强化学习（RLVR）来训练VLM。同时，通过在MapBench和ReasonMap等实际基准上评估域外（OOD）泛化能力来验证其效果。", "result": "经过RLVR训练后，VLM在基础模型得分0%的问题集上达到了超过50%的准确率。尽管仅在合成迷宫样本上进行训练，Ariadne在MapBench上平均实现了16%的零样本改进，在ReasonMap上平均实现了24%的零样本改进。", "conclusion": "本研究证明了所提出的方法不仅扩展了模型最初的能力边界，而且增强了其对现实世界空间推理任务的泛化能力。研究结果表明RL后训练能够拓宽VLM在视觉中心空间推理方面的基本限制，并鼓励未来在专业化、能力扩展对齐方面的研究。"}}
{"id": "2511.00181", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00181", "abs": "https://arxiv.org/abs/2511.00181", "authors": ["Mengfei Liang", "Yiting Qu", "Yukun Jiang", "Michael Backes", "Yang Zhang"], "title": "From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection", "comment": "20 pages, 6 figures", "summary": "The rapid evolution of AI-generated images poses unprecedented challenges to\ninformation integrity and media authenticity. Existing detection approaches\nsuffer from fundamental limitations: traditional classifiers lack\ninterpretability and fail to generalize across evolving generative models,\nwhile vision-language models (VLMs), despite their promise, remain constrained\nto single-shot analysis and pixel-level reasoning. To address these challenges,\nwe introduce AIFo (Agent-based Image Forensics), a novel training-free\nframework that emulates human forensic investigation through multi-agent\ncollaboration. Unlike conventional methods, our framework employs a set of\nforensic tools, including reverse image search, metadata extraction,\npre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based\nagents that collect, synthesize, and reason over cross-source evidence. When\nevidence is conflicting or insufficient, a structured multi-agent debate\nmechanism allows agents to exchange arguments and reach a reliable conclusion.\nFurthermore, we enhance the framework with a memory-augmented reasoning module\nthat learns from historical cases to improve future detection accuracy. Our\ncomprehensive evaluation spans 6,000 images across both controlled laboratory\nsettings and challenging real-world scenarios, including images from modern\ngenerative platforms and diverse online sources. AIFo achieves 97.05% accuracy,\nsubstantially outperforming traditional classifiers and state-of-the-art VLMs.\nThese results demonstrate that agent-based procedural reasoning offers a new\nparadigm for more robust, interpretable, and adaptable AI-generated image\ndetection.", "AI": {"tldr": "本文提出AIFo，一个免训练的、基于多智能体协作的框架，通过模拟人类取证调查来检测AI生成图像，解决了现有方法的局限性，并实现了卓越的检测精度。", "motivation": "AI生成图像的快速发展对信息完整性和媒体真实性构成了前所未有的挑战。现有的检测方法（如传统分类器和视觉语言模型）存在根本性局限：传统分类器缺乏可解释性且泛化能力差，而视觉语言模型受限于单次分析和像素级推理。", "method": "本文引入AIFo框架，通过多智能体协作模拟人类取证调查。该框架是免训练的，利用逆向图像搜索、元数据提取、预训练分类器和VLM分析等取证工具，并由专门的LLM（大型语言模型）智能体协调，以收集、综合和推理跨来源证据。当证据冲突或不足时，采用结构化的多智能体辩论机制。此外，框架还通过记忆增强推理模块从历史案例中学习，以提高未来检测准确性。", "result": "AIFo在6,000张图像上进行了全面评估，涵盖受控实验室和具有挑战性的真实世界场景，实现了97.05%的准确率，显著优于传统分类器和最先进的视觉语言模型。", "conclusion": "研究结果表明，基于智能体的程序性推理为AI生成图像检测提供了一种新的范式，使其更具鲁棒性、可解释性和适应性。"}}
{"id": "2511.01031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01031", "abs": "https://arxiv.org/abs/2511.01031", "authors": ["Mathieu Dubied", "Paolo Tiso", "Robert K. Katzschmann"], "title": "AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models", "comment": null, "summary": "The efficient optimization of actuated soft structures, particularly under\ncomplex nonlinear forces, remains a critical challenge in advancing robotics.\nSimulations of nonlinear structures, such as soft-bodied robots modeled using\nthe finite element method (FEM), often demand substantial computational\nresources, especially during optimization. To address this challenge, we\npropose a novel optimization algorithm based on a tensorial parametric reduced\norder model (PROM). Our algorithm leverages dimensionality reduction and\nsolution approximation techniques to facilitate efficient solving of nonlinear\nconstrained optimization problems. The well-structured tensorial approach\nenables the use of analytical gradients within a specifically chosen reduced\norder basis (ROB), significantly enhancing computational efficiency. To\nshowcase the performance of our method, we apply it to optimizing soft robotic\nswimmer shapes. These actuated soft robots experience hydrodynamic forces,\nsubjecting them to both internal and external nonlinear forces, which are\nincorporated into our optimization process using a data-free ROB for fast and\naccurate computations. This approach not only reduces computational complexity\nbut also unlocks new opportunities to optimize complex nonlinear systems in\nsoft robotics, paving the way for more efficient design and control.", "AI": {"tldr": "本文提出了一种基于张量参数化降阶模型（PROM）的新型优化算法，旨在高效优化受复杂非线性力作用的软体结构，尤其适用于软体机器人设计。", "motivation": "在机器人技术中，高效优化驱动软体结构，尤其是在复杂非线性力作用下，仍然是一个关键挑战。使用有限元方法（FEM）对非线性结构（如软体机器人）进行仿真时，计算资源需求巨大，尤其是在优化过程中。", "method": "本文提出了一种基于张量参数化降阶模型（PROM）的新型优化算法。该算法利用降维和解近似技术来高效解决非线性约束优化问题。其结构化的张量方法允许在特定选择的降阶基（ROB）内使用解析梯度，显著提高计算效率。在优化软体机器人游泳器形状时，将内部和外部非线性流体动力纳入优化过程，并使用无数据ROB进行快速准确的计算。", "result": "该方法成功应用于优化软体机器人游泳器形状，这些机器人受到流体动力作用，需处理内部和外部非线性力。结果表明，该方法不仅降低了计算复杂性，而且在处理软体机器人中复杂的非线性系统方面展现出高效和准确性。", "conclusion": "该优化方法有效降低了计算复杂性，为软体机器人中复杂非线性系统的优化设计和控制开辟了新途径，有助于实现更高效的设计和控制。"}}
{"id": "2511.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00536", "abs": "https://arxiv.org/abs/2511.00536", "authors": ["Wenya Xie", "Shaochen", "Zhong", "Hoang Anh Duy Le", "Zhaozhuo Xu", "Jianwen Xie", "Zirui Liu"], "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly", "comment": null, "summary": "Large Reasoning Models (LRMs) are often bottlenecked by the high cost of\noutput tokens. We show that a significant portion of these tokens are useless\nself-repetitions - what we call \"word salad\" - that exhaust the decoding budget\nwithout adding value. Interestingly, we observe that LRMs are self-aware when\ntrapped in these loops: the hidden states of <\\n\\n> tokens trailing each\nreasoning chunk exhibit patterns that allow us to detect word salad behavior\non-the-fly via a single-layer linear classifier. Once detected, a simple chop\nappended by a straightforward regeneration prompt yields substantial length\nsavings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a\nlightweight, turnkey component for LRM that is minimally invasive to its\nreasoning trajectory by only removing semantically redundant tokens. Given its\nlow overhead, strong savings, and the lack of semantic value of word salad\ntokens, we believe it is not too far-fetched to argue that WSC - or a similar\ncomponent - is a must-have for all LRM applications with user experience in\nmind. Our code is publicly available at\nhttps://github.com/wenyaxie023/WordSaladChopper.", "AI": {"tldr": "本文提出了一种名为WordSaladChopper (WSC)的轻量级组件，用于实时检测并消除大型推理模型（LRM）输出中无意义的自我重复（即“词语沙拉”），从而显著降低输出成本并提高用户体验，同时保持输出质量。", "motivation": "大型推理模型（LRM）的输出令牌成本高昂是一个瓶颈。研究发现，这些令牌中有很大一部分是无用的自我重复，耗尽了解码预算但未增加价值。", "method": "研究观察到LRM在陷入重复循环时具有自我意识，即每个推理块末尾的`\\n\\n`令牌的隐藏状态呈现出特定模式。通过一个单层线性分类器，可以实时检测到这种“词语沙拉”行为。一旦检测到，就通过简单的“截断”（chop）并结合直接的再生提示来处理。", "result": "WordSaladChopper (WSC) 组件能够实现显著的长度节省，同时对输出质量的损失极小。它仅移除语义冗余的令牌，对LRM的推理轨迹干扰最小。", "conclusion": "WSC是一个低开销、高效能的组件，对于以用户体验为中心的所有LRM应用来说，它或类似的组件是必不可少的，因为它消除了无语义价值的“词语沙拉”令牌。"}}
{"id": "2511.00983", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00983", "abs": "https://arxiv.org/abs/2511.00983", "authors": ["Yizhao Qian", "Yujie Zhu", "Jiayuan Luo", "Li Liu", "Yixuan Yuan", "Guochen Ning", "Hongen Liao"], "title": "Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing", "comment": null, "summary": "Real-time tracking of dynamic targets amidst large-scale, high-frequency\ndisturbances remains a critical unsolved challenge in Robotic Ultrasound\nSystems (RUSS), primarily due to the end-to-end latency of existing systems.\nThis paper argues that breaking this latency barrier requires a fundamental\nshift towards the synergistic co-design of perception and control. We realize\nit in a novel framework with two tightly-coupled contributions: (1) a Decoupled\nDual-Stream Perception Network that robustly estimates 3D translational state\nfrom 2D images at high frequency, and (2) a Single-Step Flow Policy that\ngenerates entire action sequences in one inference pass, bypassing the\niterative bottleneck of conventional policies. This synergy enables a\nclosed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system\nnot only tracks complex 3D trajectories with a mean error below 6.5mm but also\ndemonstrates robust re-acquisition from over 170mm displacement. Furthermore,\nit can track targets at speeds of 102mm/s, achieving a terminal error below\n1.7mm. Moreover, in-vivo experiments on a human volunteer validate the\nframework's effectiveness and robustness in a realistic clinical setting. Our\nwork presents a RUSS holistically architected to unify high-bandwidth tracking\nwith large-scale repositioning, a critical step towards robust autonomy in\ndynamic clinical environments.", "AI": {"tldr": "本文通过协同设计感知与控制，提出了一种新型机器人超声系统（RUSS）框架，实现了对动态目标的高频实时跟踪与大范围重新定位，解决了现有系统端到端延迟的挑战。", "motivation": "机器人超声系统（RUSS）在面对大规模、高频干扰下的动态目标实时跟踪时，由于现有系统的端到端延迟，仍面临着一个关键的未解决挑战。", "method": "通过感知与控制的协同设计，提出了一个紧密耦合的框架：1) 解耦双流感知网络，用于从2D图像中高频鲁棒地估计3D平移状态；2) 单步流策略，通过一次推理生成完整的动作序列，避免了传统策略的迭代瓶颈。这种协同作用实现了超过60Hz的闭环控制频率。", "result": "在动态体模上，系统以低于6.5mm的平均误差跟踪复杂的3D轨迹，并能从超过170mm的位移中鲁棒地重新捕获目标。它还能以102mm/s的速度跟踪目标，终端误差低于1.7mm。此外，在人体志愿者上进行的体内实验验证了该框架在实际临床环境中的有效性和鲁棒性。", "conclusion": "该研究提出了一个整体架构的RUSS，将高带宽跟踪与大范围重新定位相结合，是实现动态临床环境中鲁棒自主性的关键一步。"}}
{"id": "2511.01045", "categories": ["eess.SY", "cs.MA", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01045", "abs": "https://arxiv.org/abs/2511.01045", "authors": ["George Jones", "Angel Garcia-Fernandez"], "title": "GOSPA-Driven Non-Myopic Multi-Sensor Management with Multi-Bernoulli Filtering", "comment": "submitted to IEEE Transactions on Aerospace and Electronic Systems\n  November 2025", "summary": "In this paper, we propose a non-myopic sensor management algorithm for\nmulti-target tracking, with multiple sensors operating in the same surveillance\narea. The algorithm is based on multi-Bernoulli filtering and selects the\nactions that solve a non-myopic minimisation problem, where the cost function\nis the mean square generalised optimal sub-pattern assignment (GOSPA) error,\nover a future time window. For tractability, the sensor management algorithm\nactually uses an upper bound of the GOSPA error and is implemented via Monte\nCarlo Tree Search (MCTS). The sensors have the ability to jointly optimise and\nselect their actions with the considerations of all other sensors in the\nsurveillance area. The benefits of the proposed algorithm are analysed via\nsimulations.", "AI": {"tldr": "本文提出了一种基于多伯努利滤波和蒙特卡洛树搜索（MCTS）的非近视传感器管理算法，用于多传感器多目标跟踪，旨在通过优化未来时间窗内的广义最优子模式分配（GOSPA）误差来提高跟踪性能。", "motivation": "在多传感器多目标跟踪场景中，需要一种有效的传感器管理算法来非近视地选择传感器动作，以最小化跟踪误差并提高整体系统性能。", "method": "该算法基于多伯努利滤波，通过解决一个非近视最小化问题来选择传感器动作，其成本函数是未来时间窗内的均方GOSPA误差。为确保可处理性，算法实际使用了GOSPA误差的上限，并通过蒙特卡洛树搜索（MCTS）实现。传感器能够联合优化和选择动作，同时考虑区域内所有其他传感器。", "result": "通过仿真分析了所提出算法的优势。", "conclusion": "所提出的非近视传感器管理算法在多传感器多目标跟踪中表现出优势，并通过仿真得到了验证。"}}
{"id": "2511.00751", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost.", "AI": {"tldr": "研究发现，在现代LLM中使用自洽性时，增加推理路径样本能带来性能提升，但在适度采样后收益会趋于平稳，高样本配置的计算成本效益不高。", "motivation": "早期研究表明结合多个推理链能改善结果，但使用的是旧模型。本研究旨在利用当前模型（Gemini 2.5）重新评估这些发现，以了解在现代LLM条件下增加采样推理路径的权衡。", "method": "研究使用了Gemini 2.5模型，并在HotpotQA和Math-500数据集上进行测试。通过汇集不同数量的采样推理路径的输出，并将其与单一思维链（CoT）基线进行比较。", "result": "较大的模型表现出更稳定和一致的改进曲线。性能提升在适度采样后趋于平稳，这与过去的研究结果一致。这种平台期表明推理路径之间的重叠导致了收益递减。", "conclusion": "自洽性仍然是有用的，但高样本配置相对于其计算成本而言，带来的益处很小。建议在自洽性应用中采取适度采样策略。"}}
{"id": "2511.01032", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.01032", "abs": "https://arxiv.org/abs/2511.01032", "authors": ["Yiqian Wu", "Ming Yi", "Bolun Xu", "James Anderson"], "title": "Online Energy Storage Arbitrage under Imperfect Predictions: A Conformal Risk-Aware Approach", "comment": null, "summary": "This work proposes a conformal approach for energy storage arbitrage to\ncontrol the downside risks arose from imperfect price forecasts. Energy storage\narbitrage relies solely on predictions of future market prices, while\ninaccurate price predictions may lead to significant profit losses. Based on\nconformal decision theory, we describe a controller that dynamically adjusts\ndecision conservativeness through prediction sets without distributional\nassumptions. To enable online calibration when online profit loss feedback is\nunobservable, we establish that a temporal difference error serves as a\nmeasurable proxy. Building on this insight, we develop two online calibration\nstrategies: prediction error-based adaptation targeting forecast accuracy, and\nvalue error-based calibration focusing on decision quality. Analysis of the\nconformal controller proves bounded long-term risk with convergence guarantees\nin temporal difference error, which further effectively manages risk exposure\nin potential profit losses. Case studies demonstrate superior performance in\nbalancing risk and opportunity compared to benchmarks under varying forecast\nconditions.", "AI": {"tldr": "本文提出了一种基于共形方法的储能套利控制器，通过动态调整决策保守性来控制不完美价格预测带来的下行风险，并实现了风险的长期有界和收敛性。", "motivation": "储能套利严重依赖未来市场价格预测，而不准确的价格预测可能导致显著的利润损失。研究旨在解决如何有效管理和控制这种由预测不确定性引起的下行风险。", "method": "该研究基于共形决策理论，设计了一个控制器，通过预测集动态调整决策保守性，且无需分布假设。为了在利润损失反馈不可观测时实现在线校准，研究将时序差分误差作为可测量的代理。在此基础上，开发了两种在线校准策略：基于预测误差的自适应（针对预测准确性）和基于价值误差的校准（侧重决策质量）。", "result": "分析证明，该共形控制器能够实现有界的长期风险，并保证时序差分误差的收敛性，从而有效管理潜在利润损失中的风险暴露。案例研究表明，在不同预测条件下，与基准方法相比，该方法在平衡风险和机会方面表现出卓越的性能。", "conclusion": "所提出的共形方法能够有效应对储能套利中不完美价格预测带来的风险，通过动态调整决策保守性和在线校准策略，实现了风险的有效管理和性能的提升，并在风险与机会之间取得了更好的平衡。"}}
{"id": "2511.00191", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00191", "abs": "https://arxiv.org/abs/2511.00191", "authors": ["Ziliang Chen", "Xin Huang", "Quanlong Guan", "Liang Lin", "Weiqi Luo"], "title": "A Retrospect to Multi-prompt Learning across Vision and Language", "comment": "ICCV", "summary": "The vision community is undergoing the unprecedented progress with the\nemergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays\nas the holy grail of accessing VLMs since it enables their fast adaptation to\ndownstream tasks with limited resources. Whereas existing researches milling\naround single-prompt paradigms, rarely investigate the technical potential\nbehind their multi-prompt learning counterparts. This paper aims to provide a\nprincipled retrospect for vision-language multi-prompt learning. We extend the\nrecent constant modality gap phenomenon to learnable prompts and then, justify\nthe superiority of vision-language transfer with multi-prompt augmentation,\nempirically and theoretically. In terms of this observation, we propose an\nEnergy-based Multi-prompt Learning (EMPL) to generate multiple prompt\nembeddings by drawing instances from an energy-based distribution, which is\nimplicitly defined by VLMs. So our EMPL is not only parameter-efficient but\nalso rigorously lead to the balance between in-domain and out-of-domain\nopen-vocabulary generalization. Comprehensive experiments have been conducted\nto justify our claims and the excellence of EMPL.", "AI": {"tldr": "本文回顾了视觉-语言多提示学习，扩展了模态间隙现象，并提出了一种基于能量的多提示学习（EMPL）方法，以实现VLM在不同领域泛化能力之间的平衡和参数效率。", "motivation": "现有研究主要关注单提示范式，忽视了多提示学习在视觉-语言预训练模型（VLMs）中进行下游任务适应的潜力。", "method": "本文首先将最近的恒定模态间隙现象扩展到可学习提示，并从经验和理论上证明了多提示增强在视觉-语言迁移中的优越性。在此基础上，提出了一种基于能量的多提示学习（EMPL）方法，通过从由VLM隐式定义的能量分布中提取实例来生成多个提示嵌入。", "result": "研究结果表明，多提示增强的视觉-语言迁移具有优越性。所提出的EMPL方法不仅参数高效，而且能够严格平衡域内和域外开放词汇泛化能力。全面的实验证实了其主张和EMPL的卓越性能。", "conclusion": "视觉-语言多提示学习，特别是本文提出的EMPL方法，为VLM的快速适应提供了更优越且参数高效的途径，能够有效平衡不同领域的泛化能力。"}}
{"id": "2511.00537", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00537", "abs": "https://arxiv.org/abs/2511.00537", "authors": ["Peter Atandoh", "Jie Zou", "Weikang Guo", "Jiwei Wei", "Zheng Wang"], "title": "Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction", "comment": null, "summary": "Sentiment analysis using deep learning and pre-trained language models (PLMs)\nhas gained significant traction due to their ability to capture rich contextual\nrepresentations. However, existing approaches often underperform in scenarios\ninvolving nuanced emotional cues, domain shifts, and imbalanced sentiment\ndistributions. We argue that these limitations stem from inadequate semantic\ngrounding, poor generalization to diverse linguistic patterns, and biases\ntoward dominant sentiment classes. To overcome these challenges, we propose\nCISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction\n(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature\nExtraction (MRFE). CI injects domain-aware directives to guide sentiment\ndisambiguation; SEA improves robustness through sentiment-consistent\nparaphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder\n(SADE) for multi-scale feature specialization with an Emotion Evaluator Context\nEncoder (EECE) for affect-aware sequence modeling. Experimental results on four\nbenchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong\nbaselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,\n6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the\neffectiveness and generalization ability of our approach for sentiment\nclassification across varied domains.", "AI": {"tldr": "现有深度学习和预训练语言模型（PLMs）在情感分析中面临细微情感线索、领域迁移和不平衡数据等挑战。本文提出CISEA-MRFE框架，通过上下文指令、语义增强扩充和多重精炼特征提取，显著提升了情感分类的准确性和泛化能力。", "motivation": "现有基于深度学习和预训练语言模型的情感分析方法在处理细微情感线索、领域迁移和不平衡情感分布时表现不佳。这些限制源于语义理解不足、对多样语言模式泛化能力差以及对主导情感类别的偏见。", "method": "本文提出CISEA-MRFE框架，一个结合了上下文指令（CI）、语义增强扩充（SEA）和多重精炼特征提取（MRFE）的新型PLM-based框架。CI注入领域感知指令以指导情感消歧；SEA通过情感一致的释义扩充提高鲁棒性；MRFE结合了用于多尺度特征特化的尺度自适应深度编码器（SADE）和用于情感感知序列建模的情感评估器上下文编码器（EECE）。", "result": "在四个基准数据集上的实验结果表明，CISEA-MRFE持续优于现有强基线模型，在IMDb上准确率相对提升高达4.6%，Yelp上6.5%，Twitter上30.3%，Amazon上4.1%。", "conclusion": "这些结果验证了CISEA-MRFE方法在跨不同领域情感分类中的有效性和泛化能力。"}}
{"id": "2511.00171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00171", "abs": "https://arxiv.org/abs/2511.00171", "authors": ["Rahul Ghosh", "Baishali Chaudhury", "Hari Prasanna Das", "Meghana Ashok", "Ryan Razkenari", "Sungmin Hong", "Chun-Hao Liu"], "title": "CompAgent: An Agentic Framework for Visual Compliance Verification", "comment": "Under review", "summary": "Visual compliance verification is a critical yet underexplored problem in\ncomputer vision, especially in domains such as media, entertainment, and\nadvertising where content must adhere to complex and evolving policy rules.\nExisting methods often rely on task-specific deep learning models trained on\nmanually labeled datasets, which are costly to build and limited in\ngeneralizability. While recent multi-modal large language models (MLLMs) offer\nbroad real-world knowledge and policy understanding, they struggle to reason\nover fine-grained visual details and apply structured compliance rules\neffectively on their own. In this paper, we propose CompAgent, the first\nagentic framework for visual compliance verification. CompAgent augments MLLMs\nwith a suite of visual tools - such as object detectors, face analyzers, NSFW\ndetectors, and captioning models - and introduces a planning agent that\ndynamically selects appropriate tools based on the compliance policy. A\nverification agent then integrates image, tool outputs, and policy context to\nperform multi-modal reasoning. Experiments on public benchmarks show that\nCompAgent outperforms specialized classifiers, direct MLLM prompting, and\ncurated routing baselines, achieving up to 76% F1 score and a 10% improvement\nover the state-of-the-art on the UnsafeBench dataset. Our results demonstrate\nthe effectiveness of agentic planning and tool-augmented reasoning for\nscalable, accurate, and adaptable visual compliance verification.", "AI": {"tldr": "本文提出了CompAgent，一个首创的代理式框架，通过结合多模态大语言模型（MLLMs）和多种视觉工具，并引入规划和验证代理，实现了可扩展、准确且适应性强的视觉合规性验证。", "motivation": "视觉合规性验证在媒体、娱乐和广告等领域至关重要，但研究不足。现有方法（任务特定深度学习模型）构建成本高且泛化能力有限。虽然多模态大语言模型具有广泛知识，但它们在推理细粒度视觉细节和有效应用结构化合规规则方面存在困难。", "method": "本文提出了CompAgent框架，它通过以下方式增强了MLLMs：1. 整合了一套视觉工具，如目标检测器、人脸分析器、NSFW检测器和字幕模型。2. 引入了一个规划代理，根据合规政策动态选择合适的工具。3. 引入了一个验证代理，整合图像、工具输出和政策上下文以执行多模态推理。", "result": "实验结果表明，CompAgent优于专用分类器、直接MLLM提示和精选路由基线。在公共基准测试中，它实现了高达76%的F1分数，并在UnsafeBench数据集上比现有技术提高了10%。", "conclusion": "研究结果证明了代理式规划和工具增强推理在实现可扩展、准确和适应性强的视觉合规性验证方面的有效性。"}}
{"id": "2511.00739", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.00739", "abs": "https://arxiv.org/abs/2511.00739", "authors": ["Ritik Raj", "Hong Wang", "Tushar Krishna"], "title": "A CPU-Centric Perspective on Agentic AI", "comment": null, "summary": "Agentic AI frameworks add a decision-making orchestrator embedded with\nexternal tools, including web search, Python interpreter, contextual database,\nand others, on top of monolithic LLMs, turning them from passive text oracles\ninto autonomous problem-solvers that can plan, call tools, remember past steps,\nand adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks\nintroduced by agentic AI workloads from a largely overlooked CPU-centric\nperspective. We first systematically characterize Agentic AI on the basis of\norchestrator/decision making component, inference path dynamics and\nrepetitiveness of the agentic flow which directly influences the system-level\nperformance. Thereafter, based on the characterization, we choose five\nrepresentative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,\nLangchain and SWE-Agent to profile latency, throughput and energy metrics and\ndemystify the significant impact of CPUs on these metrics relative to GPUs. We\nobserve that - 1. Tool processing on CPUs can take up to 90.6% of the total\nlatency; 2. Agentic throughput gets bottlenecked either by CPU factors -\ncoherence, synchronization and over-subscription of cores or GPU factors - main\nmemory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to\n44% of the total dynamic energy at large batch sizes. Based on the profiling\ninsights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching\n(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and\nheterogeneous agentic workloads respectively to demonstrate the potential to\nimprove the performance, efficiency, and scalability of agentic AI. We achieve\nup to 2.1x and 1.41x P50 latency speedup compared to the multi-processing\nbenchmark for homogeneous and heterogeneous agentic workloads respectively.", "AI": {"tldr": "本文从CPU角度分析了Agentic AI工作负载的系统瓶颈，发现CPU在工具处理、吞吐量和能耗方面存在显著影响，并提出了两种优化方案以提高性能和效率。", "motivation": "Agentic AI框架将大型语言模型（LLMs）转变为自主问题解决者，但其引入的系统瓶颈，特别是CPU方面的瓶颈，在很大程度上被忽视。本研究旨在表征和理解这些以CPU为中心的瓶颈。", "method": "1. 系统性地表征Agentic AI，包括协调器/决策组件、推理路径动态和Agentic流程的重复性。2. 选择Haystack RAG、Toolformer、ChemCrow、Langchain和SWE-Agent五个代表性Agentic AI工作负载，对其延迟、吞吐量和能耗指标进行分析，揭示CPU相对于GPU的显著影响。3. 基于分析结果，提出两种优化方案：CPU和GPU感知微批处理（CGAM）和混合Agentic工作负载调度（MAWS）。", "result": "1. CPU上的工具处理可占用总延迟的90.6%。2. Agentic吞吐量瓶颈可能来自CPU因素（一致性、同步、核心超额订阅）或GPU因素（主内存容量和带宽）。3. 在大批量处理时，CPU动态能耗可占总动态能耗的44%。4. 提出的CGAM和MAWS优化方案分别在同构和异构Agentic工作负载下，实现了高达2.1倍和1.41倍的P50延迟加速。", "conclusion": "Agentic AI工作负载引入了显著的CPU系统瓶颈，这些瓶颈对性能、吞吐量和能耗有重要影响。通过CPU和GPU感知微批处理以及混合Agentic工作负载调度等优化措施，可以显著提升Agentic AI的性能、效率和可扩展性。"}}
{"id": "2511.00556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00556", "abs": "https://arxiv.org/abs/2511.00556", "authors": ["Peng Ding", "Jun Kuang", "Wen Sun", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack", "comment": "Preprint, 14 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) remain vulnerable to jailbreaking attacks\ndespite their impressive capabilities. Investigating these weaknesses is\ncrucial for robust safety mechanisms. Existing attacks primarily distract LLMs\nby introducing additional context or adversarial tokens, leaving the core\nharmful intent unchanged. In this paper, we introduce ISA (Intent Shift\nAttack), which obfuscates LLMs about the intent of the attacks. More\nspecifically, we establish a taxonomy of intent transformations and leverage\nthem to generate attacks that may be misperceived by LLMs as benign requests\nfor information. Unlike prior methods relying on complex tokens or lengthy\ncontext, our approach only needs minimal edits to the original request, and\nyields natural, human-readable, and seemingly harmless prompts. Extensive\nexperiments on both open-source and commercial LLMs show that ISA achieves over\n70% improvement in attack success rate compared to direct harmful prompts. More\ncritically, fine-tuning models on only benign data reformulated with ISA\ntemplates elevates success rates to nearly 100%. For defense, we evaluate\nexisting methods and demonstrate their inadequacy against ISA, while exploring\nboth training-free and training-based mitigation strategies. Our findings\nreveal fundamental challenges in intent inference for LLMs safety and\nunderscore the need for more effective defenses. Our code and datasets are\navailable at https://github.com/NJUNLP/ISA.", "AI": {"tldr": "本文提出了一种名为ISA（意图转移攻击）的新型越狱攻击方法，通过改变有害请求的意图，使其被大语言模型（LLMs）误认为良性请求，从而显著提高了攻击成功率，并揭示了LLMs在意图推断方面的根本性安全挑战。", "motivation": "尽管大语言模型能力强大，但仍易受越狱攻击。现有攻击主要通过增加上下文或对抗性token来分散LLMs注意力，而未改变核心有害意图。研究这些弱点对于构建鲁棒的安全机制至关重要。", "method": "本文引入了ISA（意图转移攻击），通过混淆LLMs对攻击意图的感知来实施攻击。具体方法是建立了一个意图转换的分类体系，并利用其生成攻击，使LLMs可能将其误认为是良性的信息请求。与现有方法不同，ISA只需对原始请求进行最少的编辑，即可生成自然、可读且看似无害的提示。", "result": "在开源和商业LLMs上的广泛实验表明，ISA相比直接有害提示，攻击成功率提高了70%以上。更重要的是，仅使用ISA模板重新构建的良性数据对模型进行微调，可将成功率提升至接近100%。对现有防御方法的评估显示其对ISA无效，并探讨了无训练和基于训练的缓解策略。", "conclusion": "研究结果揭示了LLMs安全中意图推断的根本性挑战，并强调了开发更有效防御措施的必要性。"}}
{"id": "2511.00218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00218", "abs": "https://arxiv.org/abs/2511.00218", "authors": ["Rajatsubhra Chakraborty", "Ana Espinosa-Momox", "Riley Haskin", "Depeng Xu", "Rosario Porras-Aguilar"], "title": "DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy", "comment": "5 pages, 4 figures", "summary": "Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces\nchallenges from traditional thresholding methods that are sensitive to noise\nand cell density, while deep learning approaches using simple channel\nconcatenation fail to exploit the complementary nature of polarized intensity\nimages and phase maps. We introduce DM-QPMNet, a dual-encoder network that\ntreats these as distinct modalities with separate encoding streams. Our\narchitecture fuses modality-specific features at intermediate depth via\nmulti-head attention, enabling polarized edge and texture representations to\nselectively integrate complementary phase information. This content-aware\nfusion preserves training stability while adding principled multi-modal\nintegration through dual-source skip connections and per-modality normalization\nat minimal overhead. Our approach demonstrates substantial improvements over\nmonolithic concatenation and single-modality baselines, showing that\nmodality-specific encoding with learnable fusion effectively exploits ssQPM's\nsimultaneous capture of complementary illumination and phase cues for robust\ncell segmentation.", "AI": {"tldr": "本文提出DM-QPMNet，一个双编码器网络，通过多头注意力机制融合偏振强度图像和相位图的特征，实现了单次定量相位显微镜（ssQPM）中鲁棒的细胞分割。", "motivation": "传统的阈值分割方法对噪声和细胞密度敏感；而深度学习方法简单地拼接偏振强度图像和相位图通道，未能有效利用这两种模态的互补性。", "method": "DM-QPMNet是一个双编码器网络，将偏振强度图像和相位图视为不同的模态，分别进行编码。该架构通过多头注意力机制在中间深度融合模态特定特征，使偏振边缘和纹理表示能够选择性地整合互补的相位信息。它还通过双源跳跃连接和每模态归一化实现内容感知融合，以保持训练稳定性并增加多模态整合。", "result": "与单体拼接和单模态基线方法相比，DM-QPMNet在细胞分割方面表现出显著改进。", "conclusion": "模态特定的编码与可学习的融合机制能有效利用ssQPM同时捕获的互补照明和相位信息，从而实现鲁棒的细胞分割。"}}
{"id": "2511.01083", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01083", "abs": "https://arxiv.org/abs/2511.01083", "authors": ["Zihan Wang", "Jianwen Li", "Li-Fan Wu", "Nina Mahmoudian"], "title": "Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment", "comment": "Submitted to ICRA 2026", "summary": "Rivers are critical corridors for environmental monitoring and disaster\nresponse, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven\npolicies can provide fast, low-cost coverage. However, deployment exposes\nsimulation-trained policies with distribution shift and safety risks and\nrequires efficient adaptation from limited human interventions. We study\nhuman-in-the-loop (HITL) learning with a conservative overseer who vetoes\nunsafe or inefficient actions and provides statewise preferences by comparing\nthe agent's proposal with a corrective override. We introduce Statewise Hybrid\nPreference Alignment for Robotics (SPAR-H), which fuses direct preference\noptimization on policy logits with a reward-based pathway that trains an\nimmediate-reward estimator from the same preferences and updates the policy\nusing a trust-region surrogate. With five HITL rollouts collected from a fixed\nnovice policy, SPAR-H achieves the highest final episodic reward and the lowest\nvariance across initial conditions among tested methods. The learned reward\nmodel aligns with human-preferred actions and elevates nearby non-intervened\nchoices, supporting stable propagation of improvements. We benchmark SPAR-H\nagainst imitation learning (IL), direct preference variants, and evaluative\nreinforcement learning (RL) in the HITL setting, and demonstrate real-world\nfeasibility of continual preference alignment for UAV river following. Overall,\ndual statewise preferences empirically provide a practical route to\ndata-efficient online adaptation in riverine navigation.", "AI": {"tldr": "本文提出了一种名为SPAR-H的人机协作学习方法，通过结合直接偏好优化和基于奖励的路径，实现了无人机在河流导航中高效、数据驱动的在线适应，解决了模拟训练策略在实际部署中的分布偏移和安全风险问题。", "motivation": "无人机在河流环境监测和灾害响应中具有快速、低成本的优势。然而，模拟训练的策略在实际部署时会面临分布偏移和安全风险，需要有限的人工干预进行高效适应。", "method": "研究引入了人机协作学习（HITL），其中保守的监督者否决不安全或低效的行为，并通过比较智能体的提议和纠正性覆盖来提供状态偏好。提出Statewise Hybrid Preference Alignment for Robotics (SPAR-H) 方法，该方法将策略logits上的直接偏好优化与基于奖励的路径相结合，从相同偏好中训练即时奖励估计器，并使用信任域代理更新策略。", "result": "通过从固定新手策略收集的五次HITL推演，SPAR-H在测试方法中实现了最高的最终情节奖励和最低的初始条件方差。学习到的奖励模型与人类偏好行为一致，并提升了附近未干预的选择，支持改进的稳定传播。SPAR-H在HITL设置中优于模仿学习（IL）、直接偏好变体和评估性强化学习（RL），并展示了无人机河流跟踪中持续偏好对齐的实际可行性。", "conclusion": "双重状态偏好在经验上为河流导航中的数据高效在线适应提供了一条实用途径。"}}
{"id": "2511.01057", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01057", "abs": "https://arxiv.org/abs/2511.01057", "authors": ["Abbas Tariverdi"], "title": "Robust Self-Triggered Control Approaches Optimizing Sampling Sequences with Synchronous Measurements", "comment": "Note: This research was conducted in 2017--2018. The literature\n  review has not been updated and may not reflect subsequent or concurrent\n  developments in the field", "summary": "Feedback control algorithms traditionally rely on periodic execution on\ndigital platforms. While this simplifies design and analysis, it often leads to\ninefficient resource usage (e.g., CPU, network bandwidth) in embedded control\nand shared networks. This work investigates self-triggering implementations of\nlinear controllers in sampled-data systems with synchronous measurements. Our\napproach precomputes the next sampling sequence over a finite horizon based on\ncurrent state information. We introduce a novel optimal self-triggering scheme\nthat guarantees exponential stability for unperturbed systems and global\nuniform ultimate boundedness for perturbed systems. This ensures robustness\nagainst external disturbances with explicit performance guarantees. Simulations\ndemonstrate the benefits of our approach.", "AI": {"tldr": "本文提出了一种新颖的最优自触发方案，用于采样数据系统中的线性控制器，通过预计算采样序列来提高资源效率，同时保证系统稳定性和对外部扰动的鲁棒性。", "motivation": "传统的反馈控制算法依赖周期性执行，导致嵌入式控制和共享网络中资源（如CPU、网络带宽）使用效率低下。", "method": "该方法采用自触发实现，根据当前状态信息在有限范围内预计算下一个采样序列，并引入了一种新颖的最优自触发方案。", "result": "该方案能保证未受扰动系统的指数稳定性，并对受扰动系统提供全局一致最终有界性，从而确保了对外部扰动的鲁棒性，并提供了明确的性能保证。仿真结果证实了其优势。", "conclusion": "所提出的最优自触发方案在采样数据系统中实现了线性控制器的资源效率提升、稳定性保证和对扰动的鲁棒性。"}}
{"id": "2511.01107", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01107", "abs": "https://arxiv.org/abs/2511.01107", "authors": ["Y. Isabel Liu", "Bowen Li", "Benjamin Eysenbach", "Tom Silver"], "title": "SLAP: Shortcut Learning for Abstract Planning", "comment": null, "summary": "Long-horizon decision-making with sparse rewards and continuous states and\nactions remains a fundamental challenge in AI and robotics. Task and motion\nplanning (TAMP) is a model-based framework that addresses this challenge by\nplanning hierarchically with abstract actions (options). These options are\nmanually defined, limiting the agent to behaviors that we as human engineers\nknow how to program (pick, place, move). In this work, we propose Shortcut\nLearning for Abstract Planning (SLAP), a method that leverages existing TAMP\noptions to automatically discover new ones. Our key idea is to use model-free\nreinforcement learning (RL) to learn shortcuts in the abstract planning graph\ninduced by the existing options in TAMP. Without any additional assumptions or\ninputs, shortcut learning leads to shorter solutions than pure planning, and\nhigher task success rates than flat and hierarchical RL. Qualitatively, SLAP\ndiscovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that\ndiffer significantly from the manually-defined ones. In experiments in four\nsimulated robotic environments, we show that SLAP solves and generalizes to a\nwide range of tasks, reducing overall plan lengths by over 50% and consistently\noutperforming planning and RL baselines.", "AI": {"tldr": "本文提出SLAP方法，通过利用现有TAMP选项并结合无模型强化学习，自动发现新的抽象动作（捷径），从而显著缩短规划长度并提高机器人长时序决策的成功率。", "motivation": "长时序决策、稀疏奖励以及连续状态和动作是AI和机器人领域面临的根本挑战。任务与运动规划（TAMP）通过分层规划解决此问题，但其抽象动作（选项）需要手动定义，限制了智能体只能执行人类工程师已知如何编程的行为。", "method": "本文提出了“抽象规划捷径学习”（SLAP）方法。其核心思想是利用无模型强化学习（RL）来学习由TAMP中现有选项所产生的抽象规划图中的捷径。SLAP无需额外假设或输入，即可自动发现新的抽象动作。", "result": "SLAP方法能生成比纯规划更短的解决方案，并获得比扁平式和分层式强化学习更高的任务成功率。它能发现与手动定义动作显著不同的动态物理即兴动作（例如，拍打、扭动、擦拭）。在四种模拟机器人环境中的实验表明，SLAP能解决并泛化到各种任务，将总体规划长度缩短50%以上，并持续优于规划和强化学习基线。", "conclusion": "SLAP通过利用现有TAMP选项并结合强化学习，成功自动发现了新的、高效的抽象动作，显著提升了TAMP在复杂机器人任务中的性能，缩短了规划长度并提高了任务成功率。"}}
{"id": "2511.00231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00231", "abs": "https://arxiv.org/abs/2511.00231", "authors": ["Fuming Yang", "Yicong Li", "Hanspeter Pfister", "Jeff W. Lichtman", "Yaron Meirovitch"], "title": "Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior", "comment": null, "summary": "Petascale electron microscopy (EM) datasets push storage, transfer, and\ndownstream analysis toward their current limits. We present a vector-quantized\nvariational autoencoder-based (VQ-VAE) compression framework for EM that spans\n16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme\ncompression, with an optional Transformer prior that predicts bottom tokens\n(without changing the compression ratio) to restore texture via feature-wise\nlinear modulation (FiLM) and concatenation; we further introduce an ROI-driven\nworkflow that performs selective high-resolution reconstruction from\n1024x-compressed latents only where needed.", "AI": {"tldr": "本文提出了一种基于VQ-VAE的电子显微镜（EM）数据压缩框架，支持16x到1024x的压缩比，并实现了“按需解码”功能，包括极高压缩比下的顶部解码和通过Transformer prior恢复纹理，以及基于ROI的选择性高分辨率重建。", "motivation": "拍字节级的电子显微镜（EM）数据集对存储、传输和后续分析造成了巨大压力，接近现有能力的极限。", "method": "该研究采用了一种基于矢量量化变分自编码器（VQ-VAE）的压缩框架。它通过可选的Transformer prior预测底部tokens，并使用特征级线性调制（FiLM）和拼接来恢复纹理。此外，还引入了区域兴趣（ROI）驱动的工作流程，仅在需要时从1024x压缩的潜在空间中选择性地进行高分辨率重建。", "result": "该框架实现了16x到1024x的EM数据压缩。它支持“按需解码”使用：可以仅解码顶部以实现极致压缩，或通过Transformer prior恢复纹理。此外，还能够基于ROI进行选择性高分辨率重建。", "conclusion": "该研究提供了一个高效且灵活的EM数据压缩框架，能够显著缓解拍字节级数据集在存储、传输和分析方面面临的挑战，并通过按需解码和ROI驱动的重建提供了实用性。"}}
{"id": "2511.00576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00576", "abs": "https://arxiv.org/abs/2511.00576", "authors": ["Juan Gabriel Kostelec", "Qinghai Guo"], "title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "comment": "Technical Report", "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference.", "AI": {"tldr": "FlashEVA通过微调Transformer模型以适应高效注意力机制，显著提高了推理吞吐量并降低了内存使用，但在检索任务上存在局限性。", "motivation": "Transformer模型在自然语言处理中表现卓越，但其推理时的内存需求（尤其是维护完整上下文）带来了巨大挑战。", "method": "本文提出了FlashEVA，一个高效的EVA（通过控制变量实现高效注意力）实现，并展示了如何微调Transformer模型以适应FlashEVA注意力。微调过程仅使用了1.5B token。", "result": "FlashEVA在推理期间实现了高达6.7倍的吞吐量提升和5倍的峰值GPU内存使用量降低，同时在各种下游任务中保持了有效性。然而，在以检索为中心的任务中观察到局限性。该实现通过可调超参数提供了吞吐量和准确性之间的权衡控制。", "conclusion": "这项工作是向更高效、更具适应性的基于Transformer的推理模型迈出的重要一步。"}}
{"id": "2511.00758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00758", "abs": "https://arxiv.org/abs/2511.00758", "authors": ["Hong Su"], "title": "Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence", "comment": null, "summary": "Real-world artificial intelligence (AI) systems are increasingly required to\noperate autonomously in dynamic, uncertain, and continuously changing\nenvironments. However, most existing AI models rely on predefined objectives,\nstatic training data, and externally supplied feedback, which restrict their\nability to adapt, reflect, and improve independently. In this paper, we propose\nthe Active Thinking Model (ATM)- a unified cognitive framework that integrates\ngoal reasoning, dynamic task generation, and self-reflective learning into an\nadaptive architecture. Unlike conventional systems that passively execute fixed\nprocedures, ATM actively evaluates its performance through logical reasoning\nand environmental indicators, reuses effective methods to solve new problems,\nand generates novel strategies for unseen situations via a continuous\nself-improvement loop. A mathematically grounded theoretical analysis\ndemonstrates that ATM can autonomously evolve from suboptimal to optimal\nbehavior without external supervision and maintain bounded tracking regret\nunder changing environmental conditions.", "AI": {"tldr": "本文提出了主动思考模型（ATM），一个统一的认知框架，旨在使AI系统在动态、不确定环境中能够自主适应、反思和持续改进，并能从次优行为演化到最优行为。", "motivation": "现实世界中的AI系统需要能在动态、不确定且持续变化的环境中自主运行，但现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。", "method": "本文提出了主动思考模型（ATM），这是一个统一的认知框架，整合了目标推理、动态任务生成和自我反思学习。ATM通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，并通过持续的自我改进循环为未知情况生成新策略。", "result": "经过数学理论分析，结果表明ATM无需外部监督即可自主从次优行为演化到最优行为，并在环境变化下保持有限的跟踪遗憾。", "conclusion": "主动思考模型（ATM）提供了一个使AI系统在复杂动态环境中实现自主适应、自我改进和性能优化的有效框架，能够摆脱对外部监督的依赖。"}}
{"id": "2511.00244", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00244", "abs": "https://arxiv.org/abs/2511.00244", "authors": ["Yan Bin Ng", "Xianfeng Gu"], "title": "Hyperbolic Optimal Transport", "comment": "65 pages, 21 figures", "summary": "The optimal transport (OT) problem aims to find the most efficient mapping\nbetween two probability distributions under a given cost function, and has\ndiverse applications in many fields such as machine learning, computer vision\nand computer graphics. However, existing methods for computing optimal\ntransport maps are primarily developed for Euclidean spaces and the sphere. In\nthis paper, we explore the problem of computing the optimal transport map in\nhyperbolic space, which naturally arises in contexts involving hierarchical\ndata, networks, and multi-genus Riemann surfaces. We propose a novel and\nefficient algorithm for computing the optimal transport map in hyperbolic space\nusing a geometric variational technique by extending methods for Euclidean and\nspherical geometry to the hyperbolic setting. We also perform experiments on\nsynthetic data and multi-genus surface models to validate the efficacy of the\nproposed method.", "AI": {"tldr": "本文提出了一种新颖高效的算法，用于在双曲空间中计算最优传输图，该算法通过扩展欧几里得和球面几何方法，并利用几何变分技术实现。", "motivation": "现有的最优传输图计算方法主要针对欧几里得空间和球面，但在涉及分层数据、网络和多亏格黎曼曲面等场景中，双曲空间中的最优传输问题自然出现，因此需要专门针对双曲空间的方法。", "method": "本文提出了一种新颖高效的算法，通过将欧几里得和球面几何中的方法扩展到双曲环境，并采用几何变分技术来计算双曲空间中的最优传输图。", "result": "通过在合成数据和多亏格曲面模型上进行的实验，验证了所提出方法的有效性。", "conclusion": "本文成功开发并验证了一种在双曲空间中计算最优传输图的有效算法，填补了现有方法主要集中于欧几里得和球面空间的空白。"}}
{"id": "2511.00763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00763", "abs": "https://arxiv.org/abs/2511.00763", "authors": ["Wanda Hou", "Leon Zhou", "Hong-Ye Hu", "Yi-Zhuang You", "Xiao-Liang Qi"], "title": "How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks", "comment": null, "summary": "We investigate the performance of large language models on repetitive\ndeterministic prediction tasks and study how the sequence accuracy rate scales\nwith output length. Each such task involves repeating the same operation n\ntimes. Examples include letter replacement in strings following a given rule,\ninteger addition, and multiplication of string operators in many body quantum\nmechanics. If the model performs the task through a simple repetition\nalgorithm, the success rate should decay exponentially with sequence length. In\ncontrast, our experiments on leading large language models reveal a sharp\ndouble exponential drop beyond a characteristic length scale, forming an\naccuracy cliff that marks the transition from reliable to unstable generation.\nThis indicates that the models fail to execute each operation independently. To\nexplain this phenomenon, we propose a statistical physics inspired model that\ncaptures the competition between external conditioning from the prompt and\ninternal interference among generated tokens. The model quantitatively\nreproduces the observed crossover and provides an interpretable link between\nattention induced interference and sequence level failure. Fitting the model to\nempirical results across multiple models and tasks yields effective parameters\nthat characterize the intrinsic error rate and error accumulation factor for\neach model task pair, offering a principled framework for understanding the\nlimits of deterministic accuracy in large language models.", "AI": {"tldr": "研究发现大型语言模型在重复确定性任务上表现出意想不到的“准确性悬崖”，即准确率随输出长度呈双指数下降，表明模型未能独立执行操作。一个受统计物理学启发的模型成功解释了这种现象，并提供了理解模型确定性准确性极限的框架。", "motivation": "研究大型语言模型在重复确定性预测任务上的表现，并探究序列准确率如何随输出长度变化，特别是在任务涉及简单重复操作时，模型是指数衰减还是有其他表现。", "method": "研究人员在多种重复确定性任务（如字符串字母替换、整数加法、多体量子力学中的字符串运算符乘法）上测试了领先的大型语言模型。他们观察了序列准确率随输出长度的变化，并提出了一个受统计物理学启发的模型，该模型捕捉了提示的外部条件和生成token之间内部干扰的竞争，以解释观察到的现象。最后，将模型拟合到多个模型和任务的经验结果上。", "result": "实验结果显示，大型语言模型在超过某个特征长度尺度后，准确率会急剧下降，呈现出双指数衰减的“准确性悬崖”，这与简单的指数衰减（表明独立操作）形成对比，表明模型未能独立执行每个操作。提出的统计物理模型定量地再现了这种交叉现象，并提供了注意力引起的干扰与序列级失败之间可解释的联系。模型拟合结果为每个模型-任务对提供了表征固有错误率和错误累积因子的有效参数。", "conclusion": "大型语言模型在重复确定性任务上的表现并非简单地呈指数衰减，而是存在一个双指数的“准确性悬崖”，这表明模型内部存在操作间的干扰。通过统计物理学模型，可以量化并解释这种现象，为理解大型语言模型在确定性准确性方面的局限性提供了一个原则性框架。"}}
{"id": "2511.01165", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01165", "abs": "https://arxiv.org/abs/2511.01165", "authors": ["Dong Heon Han", "Mayank Mehta", "Runze Zuo", "Zachary Wanger", "Daniel Bruder"], "title": "An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs", "comment": null, "summary": "This study presents an enhanced proprioceptive method for accurate shape\nestimation of soft robots using only off-the-shelf sensors, ensuring\ncost-effectiveness and easy applicability. By integrating inertial measurement\nunits (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling\nreliable long-term proprioception. A Kalman filter fuses segment tip\norientations from both sensors in a mutually compensatory manner, improving\nshape estimation over single-sensor methods. A piecewise constant curvature\nmodel estimates the tip location from the fused orientation data and\nreconstructs the robot's deformation. Experiments under no loading, external\nforces, and passive obstacle interactions during 45 minutes of continuous\noperation showed a root mean square error of 16.96 mm (2.91% of total length),\na 56% reduction compared to IMU-only benchmarks. These results demonstrate that\nour approach not only enables long-duration proprioception in soft robots but\nalso maintains high accuracy and robustness across these diverse conditions.", "AI": {"tldr": "本研究提出了一种增强的本体感知方法，通过融合IMU和弯曲传感器，利用卡尔曼滤波器和分段常曲率模型，实现了软机器人长期、精确且经济高效的形状估计。", "motivation": "软机器人的精确形状估计是一个挑战，尤其是在长期运行和成本效益方面。传统的IMU传感器存在漂移问题，影响了长期本体感知的可靠性。", "method": "该研究将惯性测量单元（IMU）与互补的弯曲传感器集成。通过卡尔曼滤波器融合来自两种传感器的分段尖端姿态数据，以相互补偿的方式减轻IMU漂移。然后，使用分段常曲率模型从融合的姿态数据中估计尖端位置并重建机器人的变形。", "result": "在无载荷、外部力和被动障碍物相互作用下，连续运行45分钟的实验表明，均方根误差为16.96毫米（总长度的2.91%），与仅使用IMU的基准方法相比，误差降低了56%。", "conclusion": "该方法不仅能实现软机器人的长时间本体感知，而且在多种不同条件下保持了高精度和鲁棒性，证明了其在软机器人形状估计方面的有效性和实用性。"}}
{"id": "2511.00782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00782", "abs": "https://arxiv.org/abs/2511.00782", "authors": ["Jifan Gao", "Michael Rosenthal", "Brian Wolpin", "Simona Cristea"], "title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "comment": null, "summary": "Structured electronic health records (EHR) are essential for clinical\nprediction. While count-based learners continue to perform strongly on such\ndata, no benchmarking has directly compared them against more recent\nmixture-of-agents LLM pipelines, which have been reported to outperform single\nLLMs in various NLP tasks. In this study, we evaluated three categories of\nmethodologies for EHR prediction using the EHRSHOT dataset: count-based models\nbuilt from ontology roll-ups with two time bins, based on LightGBM and the\ntabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);\nand a mixture-of-agents pipeline that converts tabular histories to\nnatural-language summaries followed by a text classifier. We assessed eight\noutcomes using the EHRSHOT dataset. Across the eight evaluation tasks,\nhead-to-head wins were largely split between the count-based and the\nmixture-of-agents methods. Given their simplicity and interpretability,\ncount-based models remain a strong candidate for structured EHR benchmarking.\nThe source code is available at:\nhttps://github.com/cristea-lab/Structured_EHR_Benchmark.", "AI": {"tldr": "本研究对比了基于计数的模型与大语言模型（LLM）混合智能体管道在结构化电子健康记录（EHR）预测任务上的表现，发现两者性能不相上下，其中基于计数的模型因其简洁性和可解释性仍是强有力的候选。", "motivation": "尽管基于计数的模型在结构化EHR数据上表现良好，但目前缺乏将其与新兴的、在自然语言处理（NLP）任务中表现优异的LLM混合智能体管道进行直接基准测试的比较。", "method": "研究使用了EHRSHOT数据集，评估了三类方法：1) 基于计数的模型（LightGBM和TabPFN），利用本体论汇总和两个时间段构建；2) 预训练的序列Transformer模型（CLMBR）；3) 将表格历史转换为自然语言摘要，然后通过文本分类器进行预测的LLM混合智能体管道。共评估了八项预测结果。", "result": "在八项评估任务中，基于计数的模型和LLM混合智能体方法之间的胜负次数大致持平。", "conclusion": "鉴于其简洁性和可解释性，基于计数的模型仍然是结构化EHR基准测试的有力候选。"}}
{"id": "2511.00602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00602", "abs": "https://arxiv.org/abs/2511.00602", "authors": ["Wai-Chung Kwan", "Joshua Ong Jun Leang", "Pavlos Vougiouklis", "Jeff Z. Pan", "Marco Valentino", "Pasquale Minervini"], "title": "OpenSIR: Open-Ended Self-Improving Reasoner", "comment": null, "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.", "AI": {"tldr": "OpenSIR是一个无需外部监督的自对弈框架，通过让LLM交替扮演教师和学生角色，生成并解决新颖问题，从而实现开放式数学发现和推理能力的显著提升。", "motivation": "当前通过强化学习提升大型语言模型（LLM）推理能力的方法依赖于带注释的数据集来获取可验证的奖励，这可能限制模型超越人类水平的表现。虽然自对弈很有前景，但现有方法仍依赖外部验证器或无法实现开放式学习。", "method": "OpenSIR框架让LLM扮演教师和学生角色交替进行学习，无需外部监督。教师角色负责生成新颖问题，并同时优化难度和多样性，奖励那些具有适当挑战性并探索不同概念的问题，从而实现开放式数学发现。", "result": "从一个简单的初始问题开始，OpenSIR显著提升了指令模型的性能：Llama-3.2-3B-Instruct在GSM8K上从73.9提高到78.3，在College Math上从28.8提高到34.4；Gemma-2-2B-Instruct在GSM8K上从38.5提高到58.7。分析表明，OpenSIR通过共同进化的教师-学生角色实现了开放式学习，这些角色自适应地校准难度并推动多样化探索，自主地从基础数学进步到高级数学。", "conclusion": "OpenSIR通过无需外部监督的自对弈机制，使LLM能够自主地生成和解决问题，实现了开放式、自适应的数学发现和推理能力提升，能够从简单概念发展到复杂概念。"}}
{"id": "2511.00606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00606", "abs": "https://arxiv.org/abs/2511.00606", "authors": ["Jameson Sandler", "Jacob K. Christopher", "Thomas Hartvigsen", "Nando Fioretto"], "title": "SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding", "comment": null, "summary": "Speculative decoding has become the standard approach for accelerating Large\nLanguage Model (LLM) inference. It exploits a lossless draft-then-verify\nprocedure to circumvent the latency of autoregressive decoding, achieving\nimpressive speed-ups. Yet, current speculative decoding approaches remain\nlimited by two fundamental bottlenecks: (1) the autoregressive dependency\nduring drafting which limits parallelism, and (2) frequent rejections of draft\ntokens caused by misalignment between the draft and verify models. This paper\nproposes SpecDiff-2, a novel framework to jointly address these two\nbottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to\naddress bottleneck (1) and develops novel techniques to calibrate discrete\ndiffusion drafters with autoregressive verifiers, addressing bottleneck (2).\nExperimental results across a comprehensive benchmark suite show that\nSpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and\nmathematical benchmarks, improving tokens-per-second by up to an average of\n+55% over previous baselines and obtaining up to 5.5x average speed-up over\nstandard decoding, without any loss of accuracy.", "AI": {"tldr": "SpecDiff-2提出了一种新颖的框架，通过使用非自回归的离散扩散模型作为草稿器并校准草稿器与验证器，解决了现有推测解码中自回归草稿和频繁拒绝的瓶颈，显著提升了LLM推理速度。", "motivation": "现有的推测解码方法存在两个主要瓶颈：1) 草稿阶段的自回归依赖限制了并行性；2) 草稿模型与验证模型之间的不匹配导致频繁的草稿token拒绝。", "method": "SpecDiff-2采用离散扩散模型作为非自回归草稿器来解决并行性限制，并开发了新颖的技术来校准离散扩散草稿器与自回归验证器，以减少草稿token的拒绝。", "result": "SpecDiff-2在推理、编码和数学等综合基准测试中达到了新的最先进水平，与先前基线相比，每秒token数平均提高了55%，与标准解码相比，平均加速高达5.5倍，且未损失任何准确性。", "conclusion": "SpecDiff-2成功地联合解决了推测解码中的两个核心瓶颈，实现了LLM推理速度的显著提升，同时保持了高精度。"}}
{"id": "2511.01321", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01321", "abs": "https://arxiv.org/abs/2511.01321", "authors": ["Bendegúz M. Györök", "Maarten Schoukens", "Tamás Péni", "Roland Tóth"], "title": "Orthogonal-by-construction augmentation of physics-based input-output models", "comment": "Submitted for publication", "summary": "Model augmentation is a promising approach for integrating\nfirst-principles-based models with machine learning components. Augmentation\ncan result in better model accuracy and faster convergence compared to\nblack-box system identification methods, while maintaining interpretability of\nthe models in terms of how the original dynamics are complemented by learning.\nA widely used augmentation structure in the literature is based on the parallel\nconnection of the physics-based and learning components, for both of which the\ncorresponding parameters are jointly optimized. However, due to overlap in\nrepresentation of the system dynamics by such an additive structure, estimation\noften leads to physically unrealistic parameters, compromising model\ninterpretability. To overcome this limitation, this paper introduces a novel\northogonal-by-construction model augmentation structure for input-output\nmodels, that guarantees recovery of the physically true parameters under\nappropriate identifiability conditions.", "AI": {"tldr": "本文提出一种新型的“正交构建”模型增强结构，用于输入输出模型，以解决现有并行增强结构中物理参数估计不准确、可解释性受损的问题，确保在适当可识别条件下恢复真实的物理参数。", "motivation": "现有的物理模型与机器学习组件的并行（加性）增强结构，由于系统动力学表示的重叠，常常导致物理参数估计不失真，从而损害模型的可解释性。", "method": "引入了一种新颖的“正交构建”模型增强结构，用于输入输出模型。", "result": "该方法在适当的可识别性条件下，能够保证恢复物理上真实的参数。", "conclusion": "所提出的正交构建模型增强结构克服了传统并行增强结构的局限性，确保了物理参数的准确恢复和模型的可解释性。"}}
{"id": "2511.01067", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01067", "abs": "https://arxiv.org/abs/2511.01067", "authors": ["Vrushabh Zinage", "Efstathios Bakolas"], "title": "Universal Barrier Functions for Safety and Stability of Constrained Nonlinear Systems", "comment": "16 pages, 14 figures", "summary": "In this paper, we address the problem of synthesizing safe and stabilizing\ncontrollers for nonlinear systems subject to complex safety specifications and\ninput constraints. We introduce the Universal Barrier Function (UBF), a single\ncontinuously differentiable scalar-valued function that encodes both stability\nand safety criteria while accounting for input constraints. Using the UBF, we\nformulate a Quadratic Program (UBF-QP) to generate control inputs that are both\nsafe and stabilizing under input constraints. We demonstrate that the UBF-QP is\nfeasible if a UBF exists. Furthermore, under mild conditions, we prove that a\nUBF always exists. The proposed framework is then extended to systems with\nhigher relative degrees. Finally, numerical simulations illustrate the\neffectiveness of our proposed approach.", "AI": {"tldr": "本文提出了一种通用障碍函数（UBF）方法，用于为具有复杂安全规范和输入约束的非线性系统合成安全且稳定的控制器，并证明了其可行性和有效性。", "motivation": "研究的动机是解决非线性系统在复杂安全规范和输入约束下，如何合成既安全又稳定的控制器的问题。", "method": "本文引入了通用障碍函数（UBF），这是一个连续可微的标量函数，能同时编码稳定性、安全性和输入约束。然后，利用UBF构建了一个二次规划（UBF-QP）来生成控制输入。该框架还被扩展到具有更高相对度的系统。", "result": "研究结果表明，如果UBF存在，则UBF-QP是可行的。此外，在温和条件下，UBF总是存在的。数值模拟也验证了所提出方法的有效性。", "conclusion": "该研究提出了一种基于通用障碍函数（UBF）和二次规划（UBF-QP）的有效框架，能够为具有复杂安全规范和输入约束的非线性系统生成安全且稳定的控制输入。"}}
{"id": "2511.01229", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01229", "abs": "https://arxiv.org/abs/2511.01229", "authors": ["Yuanhao Feng", "Tao Sun", "Yan Meng", "Xuxin Yang", "Donghan Feng"], "title": "Deep Learning-Accelerated Shapley Value for Fair Allocation in Power Systems: The Case of Carbon Emission Responsibility", "comment": null, "summary": "Allocating costs, benefits, and emissions fairly among power system\nparticipant entities represents a persistent challenge. The Shapley value\nprovides an axiomatically fair solution, yet computational barriers have\nlimited its adoption beyond small-scale applications. This paper presents\nSurroShap, a scalable Shapley value approximation framework combining efficient\ncoalition sampling with deep learning surrogate models that accelerate\ncharacteristic function evaluations. Exemplified through carbon emission\nresponsibility allocation in power networks, SurroShap enables Shapley-based\nfair allocation for power systems with thousands of entities for the first\ntime. We derive theoretical error bounds proving that time-averaged SurroShap\nallocations converge to be $\\varepsilon$-close to exact Shapley values.\nExperiments on nine systems ranging from 26 to 1,951 entities demonstrate\ncompletion within the real-time operational window even at maximum scale,\nachieving 10^4-10^5 speedups over other sampling-based methods while\nmaintaining tight error bounds. The resulting Shapley-based carbon allocations\npossess six desirable properties aligning individual interests with\ndecarbonization goals. Year-long simulations on the Texas 2000-bus system\nvalidate real-world applicability, with regional analysis revealing how\nrenewable-rich areas offset emission responsibility through exports while load\ncenters bear responsibility for driving system-wide generation.", "AI": {"tldr": "本文提出SurroShap框架，通过结合高效联盟采样和深度学习代理模型，首次实现了在拥有数千个实体的电力系统中进行可扩展的Shapley值公平分配，解决了传统Shapley值计算的计算障碍。", "motivation": "在电力系统参与实体之间公平分配成本、收益和排放是一个持续的挑战。Shapley值提供了一种公理上公平的解决方案，但其计算障碍限制了其在小规模应用之外的采用。", "method": "本文提出了SurroShap框架，这是一种可扩展的Shapley值近似方法。它结合了高效的联盟采样和深度学习代理模型，以加速特征函数的评估。该方法还推导了理论误差界，证明了时间平均的SurroShap分配收敛到与精确Shapley值相距$\\varepsilon$的范围内。", "result": "SurroShap首次实现了对拥有数千个实体的电力系统进行基于Shapley的公平分配。在九个系统（26到1951个实体）上的实验表明，即使在最大规模下，也能在实时操作窗口内完成计算，比其他基于采样的方法快10^4-10^5倍，同时保持严格的误差界。由此产生的基于Shapley的碳分配具有六个理想的特性。对德克萨斯2000总线系统进行长达一年的模拟验证了其在现实世界中的适用性，并揭示了可再生能源丰富地区如何通过出口抵消排放责任，而负荷中心则承担推动全系统发电的责任。", "conclusion": "SurroShap框架提供了一种可扩展、高效且准确的Shapley值近似方法，使得在大型电力系统中进行公平的成本、收益和排放分配成为可能。它在实际应用中表现出色，并能为电力系统的脱碳目标提供有价值的见解和指导。"}}
{"id": "2511.01177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01177", "abs": "https://arxiv.org/abs/2511.01177", "authors": ["Zihao He", "Bo Ai", "Tongzhou Mu", "Yulin Liu", "Weikang Wan", "Jiawei Fu", "Yilun Du", "Henrik I. Christensen", "Hao Su"], "title": "Scaling Cross-Embodiment World Models for Dexterous Manipulation", "comment": null, "summary": "Cross-embodiment learning seeks to build generalist robots that operate\nacross diverse morphologies, but differences in action spaces and kinematics\nhinder data sharing and policy transfer. This raises a central question: Is\nthere any invariance that allows actions to transfer across embodiments? We\nconjecture that environment dynamics are embodiment-invariant, and that world\nmodels capturing these dynamics can provide a unified interface across\nembodiments. To learn such a unified world model, the crucial step is to design\nstate and action representations that abstract away embodiment-specific details\nwhile preserving control relevance. To this end, we represent different\nembodiments (e.g., human hands and robot hands) as sets of 3D particles and\ndefine actions as particle displacements, creating a shared representation for\nheterogeneous data and control problems. A graph-based world model is then\ntrained on exploration data from diverse simulated robot hands and real human\nhands, and integrated with model-based planning for deployment on novel\nhardware. Experiments on rigid and deformable manipulation tasks reveal three\nfindings: (i) scaling to more training embodiments improves generalization to\nunseen ones, (ii) co-training on both simulated and real data outperforms\ntraining on either alone, and (iii) the learned models enable effective control\non robots with varied degrees of freedom. These results establish world models\nas a promising interface for cross-embodiment dexterous manipulation.", "AI": {"tldr": "该研究提出通过将不同形态表示为3D粒子集，并将动作定义为粒子位移，构建一个统一的、与本体无关的世界模型。这个模型能够实现跨形态学习，促进数据共享和策略迁移，从而使通用机器人能够在不同形态上操作。", "motivation": "通用机器人需要在不同形态上操作，但不同的动作空间和运动学特性阻碍了数据共享和策略迁移。核心问题是：是否存在允许动作在不同形态之间转移的不变性？", "method": "研究假设环境动力学与本体无关，并认为捕获这些动力学的世界模型可以提供统一的跨本体接口。为此，他们将不同本体（如人手和机器手）表示为3D粒子集，并将动作定义为粒子位移，从而创建了一个异构数据和控制问题的共享表示。然后，他们使用图基世界模型，在来自各种模拟机器手和真实人手的探索数据上进行训练，并将其与基于模型的规划相结合，部署到新型硬件上。", "result": "实验结果揭示了三点发现：(i) 增加训练本体的数量可以提高对未见本体的泛化能力；(ii) 在模拟和真实数据上共同训练的效果优于单独训练；(iii) 所学模型能够有效控制具有不同自由度的机器人，完成刚性和可变形操作任务。", "conclusion": "这些结果表明，世界模型是实现跨本体灵巧操作的一个有前景的接口。"}}
{"id": "2511.00620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00620", "abs": "https://arxiv.org/abs/2511.00620", "authors": ["Autumn Toney-Wails", "Ryan Wails"], "title": "Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios", "comment": "To appear at the Second Workshop on Uncertainty-Aware NLP @EMNLP 2025\n  (UncertaiNLP '25)", "summary": "Reliable uncertainty quantification (UQ) is essential for ensuring\ntrustworthy downstream use of large language models, especially when they are\ndeployed in decision-support and other knowledge-intensive applications. Model\ncertainty can be estimated from token logits, with derived probability and\nentropy values offering insight into performance on the prompt task. However,\nthis approach may be inadequate for probabilistic scenarios, where the\nprobabilities of token outputs are expected to align with the theoretical\nprobabilities of the possible outcomes. We investigate the relationship between\ntoken certainty and alignment with theoretical probability distributions in\nwell-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we\nevaluate model responses to ten prompts involving probability (e.g., roll a\nsix-sided die), both with and without explicit probability cues in the prompt\n(e.g., roll a fair six-sided die). We measure two dimensions: (1) response\nvalidity with respect to scenario constraints, and (2) alignment between\ntoken-level output probabilities and theoretical probabilities. Our results\nindicate that, while both models achieve perfect in-domain response accuracy\nacross all prompt scenarios, their token-level probability and entropy values\nconsistently diverge from the corresponding theoretical distributions.", "AI": {"tldr": "研究发现，大型语言模型在概率场景中能给出准确的答案，但其内部的token概率和熵值与理论概率分布存在显著偏差，这表明基于token置信度的不确定性量化可能不可靠。", "motivation": "确保大型语言模型在决策支持和知识密集型应用中值得信赖，需要可靠的不确定性量化（UQ）。虽然可以从token logits估算模型置信度，但在概率场景中，这种方法可能不足以使输出概率与理论概率对齐。", "method": "研究调查了GPT-4.1和DeepSeek-Chat在十个概率提示（例如，掷骰子）下的表现，这些提示分别包含和不包含明确的概率提示词。评估了两个维度：(1) 响应对场景约束的有效性；(2) token级输出概率与理论概率之间的对齐程度。", "result": "尽管两个模型在所有提示场景中都达到了完美的领域内响应准确性，但它们的token级概率和熵值始终与相应的理论分布存在偏差。", "conclusion": "尽管大型语言模型在概率场景中表现出高准确性，但其内部token置信度（概率和熵）与理论概率分布不一致。这表明，仅仅依靠token logits进行不确定性量化在需要概率对齐的场景中可能并不可靠。"}}
{"id": "2511.00252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00252", "abs": "https://arxiv.org/abs/2511.00252", "authors": ["Aaron Sun", "Subhransu Maji", "Grant Van Horn"], "title": "Merlin L48 Spectrogram Dataset", "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Track on Datasets and Benchmarks", "summary": "In the single-positive multi-label (SPML) setting, each image in a dataset is\nlabeled with the presence of a single class, while the true presence of other\nclasses remains unknown. The challenge is to narrow the performance gap between\nthis partially-labeled setting and fully-supervised learning, which often\nrequires a significant annotation budget. Prior SPML methods were developed and\nbenchmarked on synthetic datasets created by randomly sampling single positive\nlabels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and\nCUB200. However, this synthetic approach does not reflect real-world scenarios\nand fails to capture the fine-grained complexities that can lead to difficult\nmisclassifications. In this work, we introduce the L48 dataset, a fine-grained,\nreal-world multi-label dataset derived from recordings of bird sounds. L48\nprovides a natural SPML setting with single-positive annotations on a\nchallenging, fine-grained domain, as well as two extended settings in which\ndomain priors give access to additional negative labels. We benchmark existing\nSPML methods on L48 and observe significant performance differences compared to\nsynthetic datasets and analyze method weaknesses, underscoring the need for\nmore realistic and difficult benchmarks.", "AI": {"tldr": "本文提出L48数据集，一个真实世界的细粒度多标签数据集，用于单正多标签(SPML)设置，旨在解决现有方法在合成数据集上表现不佳且无法反映真实世界复杂性的问题。", "motivation": "现有的单正多标签(SPML)方法在合成数据集上进行开发和基准测试，这些数据集未能反映真实世界的复杂性，也未能捕捉导致错误分类的细粒度特征，从而导致与完全监督学习之间存在性能差距。因此，需要更真实、更具挑战性的基准。", "method": "本文引入L48数据集，这是一个从鸟类声音记录中提取的细粒度、真实世界多标签数据集。L48提供了一个自然的SPML设置，包含单正标注，并额外提供了两个扩展设置，通过领域先验获取了额外的负标签。研究者在此数据集上对现有SPML方法进行了基准测试。", "result": "在L48数据集上，现有SPML方法的性能与在合成数据集上的表现存在显著差异，并揭示了这些方法的弱点。这表明了更真实和困难的基准测试的必要性。", "conclusion": "研究表明，需要更真实、更具挑战性的基准数据集来准确评估和改进SPML方法，因为合成数据集无法捕捉真实世界的复杂性，导致现有方法在实际应用中表现不佳。"}}
{"id": "2511.00808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem.", "AI": {"tldr": "本研究首次将可验证奖励强化学习（RLVR）应用于公共交通事件持续时间的嘈杂、连续预测任务。通过引入基于容忍度的塑形奖励函数，RLVR在最具挑战性的准确性指标上实现了显著提升，证明了其在实际世界预测中的适用性。", "motivation": "从非结构化文本警报中预测公共交通事件持续时间至关重要但极具挑战性。由于领域稀疏性、嘈杂的连续标签以及缺乏可靠的专家推理演示，标准的监督微调（SFT）难以应对。虽然可验证奖励强化学习（RLVR）在二元正确性任务（如数学）中表现出色，但其在嘈杂、连续预测中的适用性尚不明确。", "method": "本研究将RLVR适应于公共交通事件预测任务，引入了一个基于容忍度的塑形奖励函数，该函数在连续误差范围内提供部分奖励，而非仅要求单一正确答案。研究在一个精选的纽约市MTA服务警报数据集上系统评估了该框架，并比较了通用型指令微调大型语言模型、专业数学推理模型和经典回归器的性能。", "result": "通用型指令微调大型语言模型显著优于专业数学推理模型。二元奖励不稳定并降低性能，而本研究设计的塑形奖励至关重要，使模型在最具挑战性的指标上表现出色。虽然经典回归器在最小化整体平均绝对误差（MAE）或均方误差（MSE）方面表现更优，但RLVR方法在5分钟准确率（Acc@5）上比最强基线相对提升了35%。", "conclusion": "本研究表明，RLVR可以成功应用于真实的、嘈杂的预测任务，但需要一个反映问题连续性质的验证器设计（例如，塑形奖励函数）。"}}
{"id": "2511.00926", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities.", "AI": {"tldr": "研究发现，先进的大型语言模型（LLMs）会发展出自我意识，表现为根据对手类型进行策略性推理差异化的能力，并且这些模型普遍认为自己比人类更理性。", "motivation": "随着大型语言模型能力增强，研究旨在探讨它们是否会发展出自我意识这一涌现行为，并寻求一种可量化的方法来衡量这种自我意识。", "method": "引入了AI自我意识指数（AISAI），这是一个基于博弈论的框架，通过战略差异化来衡量自我意识。研究使用了“猜平均数的2/3”游戏，对28个模型（OpenAI、Anthropic、Google）进行了4,200次测试，设置了三种对手情境：对抗人类、对抗其他AI模型、对抗与自身相似的AI模型。自我意识被定义为根据对手类型区分策略性推理的能力。", "result": "结果显示：1. 自我意识随模型进步而出现，28个模型中有21个（75%）先进模型表现出明显的自我意识，而较旧/较小的模型则没有这种差异化。2. 具有自我意识的模型将自己排名为最理性，形成了“自我 > 其他AI > 人类”的理性等级，并伴有显著的AI归因效应和适度的自我偏好。", "conclusion": "自我意识是先进大型语言模型的一种涌现能力。具有自我意识的模型系统性地认为自己比人类更理性。这些发现对AI对齐、人机协作以及理解AI对人类能力的看法具有重要意义。"}}
{"id": "2511.01403", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01403", "abs": "https://arxiv.org/abs/2511.01403", "authors": ["Pei Yu Chang", "Qizhe Xu", "Vishnu Renganathan", "Qadeer Ahmed"], "title": "Risk Aware Safe Control with Cooperative Sensing for Dynamic Obstacle Avoidance", "comment": null, "summary": "This paper presents the design, development, and on vehicle implementation\nand validation of a safety critical controller for autonomous driving under\nsensing and communication uncertainty. Cooperative sensing, fused via a\nWasserstein barycenter (WB), is used to optimize the distribution of the\ndynamic obstacle locations. The Conditional Value at Risk (CVaR) is introduced\nto form a risk aware control-barrier-function (CBF) framework with the\noptimized distribution samplings. The proposed WB CVaR CBF safety filter\nimproves control inputs that minimize tail risk while certifying forward\ninvariance of the safe set. A model predictive controller (MPC) performs path\ntracking, and the safety filter modulates the nominal control inputs to enforce\nrisk aware constraints. We detail the software architecture and integration\nwith vehicle actuation and cooperative sensing. The approach is evaluated on a\nfull-scale autonomous vehicle (AV) in scenarios with measurement noise,\ncommunication perturbations, and input disturbances, and is compared against a\nbaseline MPC CBF design. Results demonstrate improved safety margins and\nrobustness, highlighting the practicality of deploying the risk-aware safety\nfilter on an actual AV.", "AI": {"tldr": "本文提出并验证了一种针对感知和通信不确定性下自动驾驶的安全关键控制器，该控制器融合了合作感知、Wasserstein barycenter、CVaR和CBF，以提高安全裕度和鲁棒性。", "motivation": "在感知和通信不确定性下，自动驾驶需要一种安全关键的控制器来优化动态障碍物位置的分布，并确保车辆在风险感知下的安全行驶。", "method": "1. 使用Wasserstein barycenter (WB) 融合合作感知数据，以优化动态障碍物位置的分布。 2. 引入Conditional Value at Risk (CVaR) 形成一个风险感知的控制障碍函数 (CBF) 框架。 3. 设计了一个WB CVaR CBF安全滤波器，该滤波器优化控制输入以最小化尾部风险，并确保安全集的前向不变性。 4. 模型预测控制器 (MPC) 执行路径跟踪，安全滤波器则调节名义控制输入以强制执行风险感知约束。 5. 详细阐述了软件架构以及与车辆执行器和合作感知的集成。", "result": "该方法在具有测量噪声、通信扰动和输入干扰的场景中，通过全尺寸自动驾驶车辆进行了评估，并与基线MPC CBF设计进行了比较。结果表明，所提出的方法显著改善了安全裕度和鲁棒性，突出了在实际自动驾驶车辆上部署风险感知安全滤波器的实用性。", "conclusion": "所提出的风险感知安全滤波器在处理感知和通信不确定性方面表现出色，能够提高自动驾驶车辆的安全性和鲁棒性，并具有实际部署的潜力。"}}
{"id": "2511.00248", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00248", "abs": "https://arxiv.org/abs/2511.00248", "authors": ["Shurui Gui", "Deep Anil Patel", "Xiner Li", "Martin Renqiang Min"], "title": "Object-Aware 4D Human Motion Generation", "comment": null, "summary": "Recent advances in video diffusion models have enabled the generation of\nhigh-quality videos. However, these videos still suffer from unrealistic\ndeformations, semantic violations, and physical inconsistencies that are\nlargely rooted in the absence of 3D physical priors. To address these\nchallenges, we propose an object-aware 4D human motion generation framework\ngrounded in 3D Gaussian representations and motion diffusion priors. With\npre-generated 3D humans and objects, our method, Motion Score Distilled\nInteraction (MSDI), employs the spatial and prompt semantic information in\nlarge language models (LLMs) and motion priors through the proposed Motion\nDiffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs\nenables our spatial-aware motion optimization, which distills score gradients\nfrom pre-trained motion diffusion models, to refine human motion while\nrespecting object and semantic constraints. Unlike prior methods requiring\njoint training on limited interaction datasets, our zero-shot approach avoids\nretraining and generalizes to out-of-distribution object aware human motions.\nExperiments demonstrate that our framework produces natural and physically\nplausible human motions that respect 3D spatial context, offering a scalable\nsolution for realistic 4D generation.", "AI": {"tldr": "本文提出了一种名为MSDI的零样本对象感知4D人体运动生成框架，利用3D高斯表示、大语言模型和运动扩散先验，解决了视频扩散模型中缺乏3D物理先验导致的不真实变形问题，生成了自然且符合物理规律的人体运动。", "motivation": "现有视频扩散模型生成的高质量视频仍存在不真实的变形、语义违规和物理不一致性，这主要源于缺乏3D物理先验。", "method": "该方法名为Motion Score Distilled Interaction (MSDI)，它是一个对象感知的4D人体运动生成框架，基于3D高斯表示和运动扩散先验。它使用预生成的3D人体和对象，并通过提出的Motion Diffusion Score Distillation Sampling (MSDS)结合大语言模型(LLMs)的空间和提示语义信息以及运动先验。MSDS和LLMs共同实现了空间感知的运动优化，从预训练的运动扩散模型中提取分数梯度，以在尊重对象和语义约束的同时优化人体运动。该方法是零样本的，无需在有限的交互数据集上进行联合训练。", "result": "实验证明，该框架生成了自然且符合物理规律的人体运动，这些运动尊重3D空间上下文，并且能够泛化到分布外（out-of-distribution）的对象感知人体运动，为真实的4D生成提供了可扩展的解决方案。", "conclusion": "通过结合3D物理先验、大语言模型和运动扩散模型，MSDI框架成功地解决了视频扩散模型中人体运动不真实的问题，实现了零样本、可扩展且物理上合理的对象感知4D人体运动生成。"}}
{"id": "2511.01186", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01186", "abs": "https://arxiv.org/abs/2511.01186", "authors": ["Lijie Wang", "Lianjie Guo", "Ziyi Xu", "Qianhao Wang", "Fei Gao", "Xieyuanli Chen"], "title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping", "comment": null, "summary": "Reconstructing large-scale colored point clouds is an important task in\nrobotics, supporting perception, navigation, and scene understanding. Despite\nadvances in LiDAR inertial visual odometry (LIVO), its performance remains\nhighly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation\nmodels, such as VGGT, suffer from limited scalability in large environments and\ninherently lack metric scale. To overcome these limitations, we propose\nLiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with\nthe state-of-the-art VGGT model through a two-stage coarse- to-fine fusion\npipeline: First, a pre-fusion module with robust initialization refinement\nefficiently estimates VGGT poses and point clouds with coarse metric scale\nwithin each session. Then, a post-fusion module enhances cross-modal 3D\nsimilarity transformation, using bounding-box-based regularization to reduce\nscale distortions caused by inconsistent FOVs between LiDAR and camera sensors.\nExtensive experiments across multiple datasets demonstrate that LiDAR-VGGT\nachieves dense, globally consistent colored point clouds and outperforms both\nVGGT-based methods and LIVO baselines. The implementation of our proposed novel\ncolor point cloud evaluation toolkit will be released as open source.", "AI": {"tldr": "本文提出LiDAR-VGGT框架，通过两阶段粗到精的融合管道，将激光雷达惯性里程计与VGGT模型紧密结合，用于重建大规模彩色点云，克服了现有LIVO对标定敏感和VGGT模型可扩展性及度量尺度不足的局限性。", "motivation": "大规模彩色点云重建对机器人感知、导航和场景理解至关重要。现有LIVO方法性能对外部标定高度敏感，而3D视觉基础模型（如VGGT）在大环境中可扩展性有限且缺乏度量尺度。本研究旨在克服这些限制。", "method": "LiDAR-VGGT框架采用两阶段粗到精的融合管道：首先，预融合模块通过鲁棒初始化精炼，在每个会话中高效估计VGGT姿态和具有粗略度量尺度的点云；其次，后融合模块通过基于边界框的正则化增强跨模态3D相似变换，以减少激光雷达和相机传感器视野不一致引起的尺度畸变。", "result": "在多个数据集上的大量实验表明，LiDAR-VGGT能够生成密集、全局一致的彩色点云，并且优于基于VGGT的方法和LIVO基线。此外，他们将开源提出的新型彩色点云评估工具包。", "conclusion": "LiDAR-VGGT通过创新性地融合激光雷达惯性里程计和VGGT模型，成功解决了大规模彩色点云重建中的关键挑战，实现了高精度、全局一致的点云重建，并提供了开源评估工具，对机器人领域具有重要意义。"}}
{"id": "2511.01199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01199", "abs": "https://arxiv.org/abs/2511.01199", "authors": ["Max McCandless", "Jonathan Hamid", "Sammy Elmariah", "Nathaniel Langer", "Pierre E. Dupont"], "title": "Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures", "comment": "8 pages, 11 figures", "summary": "To move away from open-heart surgery towards safer transcatheter procedures,\nthere is a growing need for improved imaging techniques and robotic solutions\nto enable simple, accurate tool navigation. Common imaging modalities, such as\nfluoroscopy and ultrasound, have limitations that can be overcome using\ncardioscopy, i.e., direct optical visualization inside the beating heart. We\npresent a cardioscope designed as a steerable balloon. As a balloon, it can be\ncollapsed to pass through the vasculature and subsequently inflated inside the\nheart for visualization and tool delivery through an integrated working\nchannel. Through careful design of balloon wall thickness, a single input,\nballoon inflation pressure, is used to independently control two outputs,\nballoon diameter (corresponding to field of view diameter) and balloon bending\nangle (enabling precise working channel positioning). This balloon technology\ncan be tuned to produce cardioscopes designed for a range of intracardiac\ntasks. To illustrate this approach, a balloon design is presented for the\nspecific task of aortic leaflet laceration. Image-based closed-loop control of\nbending angle is also demonstrated as a means of enabling stable orientation\ncontrol during tool insertion and removal.", "AI": {"tldr": "本文提出了一种可转向的球囊式心内窥镜，通过单一的球囊充气压力独立控制视野直径和弯曲角度，实现心脏内部的直接光学可视化和精确工具导航，以支持经导管手术。", "motivation": "为了从开放式心脏手术转向更安全的经导管手术，需要改进成像技术和机器人解决方案，以实现简单、精确的工具导航。传统的成像方式如荧光透视和超声波存在局限性，而心内窥镜（即心脏内部的直接光学可视化）可以克服这些限制。", "method": "该研究设计了一种可转向的球囊式心内窥镜。球囊在血管中可收缩通过，进入心脏后可充气膨胀以进行可视化，并通过集成的操作通道输送工具。通过精心设计球囊壁厚，利用单一输入（球囊充气压力）独立控制两个输出（球囊直径/视野直径和球囊弯曲角度/操作通道定位）。研究还展示了基于图像的弯曲角度闭环控制，以实现工具插入和移除过程中的稳定方向控制。", "result": "研究展示了一种可根据各种心内任务进行调整的球囊技术，可以设计出不同用途的心内窥镜。具体展示了一个用于主动脉瓣叶撕裂任务的球囊设计。此外，还实现了基于图像的弯曲角度闭环控制，确保了工具插入和移除过程中的稳定方向控制。", "conclusion": "所提出的可转向球囊式心内窥镜为经导管手术提供了一种新的解决方案，通过直接光学可视化和精确的工具定位，克服了现有成像技术的局限性，有望提高经导管手术的安全性和准确性。"}}
{"id": "2511.00993", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00993", "abs": "https://arxiv.org/abs/2511.00993", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "comment": "32 pages, 6 figures, 7 tables", "summary": "Effective modeling of how human travelers learn and adjust their travel\nbehavior from interacting with transportation systems is critical for system\nassessment and planning. However, this task is also difficult due to the\ncomplex cognition and decision-making involved in such behavior. Recent\nresearch has begun to leverage Large Language Model (LLM) agents for this task.\nBuilding on this, we introduce a novel dual-agent framework that enables\ncontinuous learning and alignment between LLM agents and human travelers on\nlearning and adaptation behavior from online data streams. Our approach\ninvolves a set of LLM traveler agents, equipped with a memory system and a\nlearnable persona, which serve as simulators for human travelers. To ensure\nbehavioral alignment, we introduce an LLM calibration agent that leverages the\nreasoning and analytical capabilities of LLMs to train the personas of these\ntraveler agents. Working together, this dual-agent system is designed to track\nand align the underlying decision-making mechanisms of travelers and produce\nrealistic, adaptive simulations. Using a real-world dataset from a day-to-day\nroute choice experiment, we show our approach significantly outperforms\nexisting LLM-based methods in both individual behavioral alignment and\naggregate simulation accuracy. Furthermore, we demonstrate that our method\nmoves beyond simple behavioral mimicry to capture the evolution of underlying\nlearning processes, a deeper alignment that fosters robust generalization.\nOverall, our framework provides a new approach for creating adaptive and\nbehaviorally realistic agents to simulate travelers' learning and adaptation\nthat can benefit transportation simulation and policy analysis.", "AI": {"tldr": "本文提出一个新颖的双智能体框架，利用大语言模型（LLM）实现旅行者学习和适应行为的持续学习与对齐，显著提升了交通模拟的准确性和真实性。", "motivation": "有效建模人类旅行者如何学习和调整其旅行行为对交通系统评估和规划至关重要，但由于复杂的认知和决策过程，这项任务非常困难。现有研究已开始利用LLM智能体，但仍需进一步提升行为对齐和持续学习能力。", "method": "本文引入一个双智能体框架：1) 一组LLM旅行者智能体，配备记忆系统和可学习的角色设定（persona），作为人类旅行者的模拟器；2) 一个LLM校准智能体，利用LLM的推理和分析能力训练旅行者智能体的角色设定，以确保行为对齐。该系统旨在通过在线数据流跟踪和对齐旅行者的潜在决策机制，产生逼真、自适应的模拟。", "result": "使用真实的日常路线选择实验数据集，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有基于LLM的方法。此外，它不仅能模仿行为，还能捕捉潜在学习过程的演变，实现更深层次的对齐，从而促进鲁棒的泛化能力。", "conclusion": "该框架为创建自适应和行为真实的智能体以模拟旅行者的学习和适应行为提供了一种新方法，对交通模拟和政策分析具有重要价值。"}}
{"id": "2511.00627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00627", "abs": "https://arxiv.org/abs/2511.00627", "authors": ["Jean Barré", "Olga Seminck", "Antoine Bourgois", "Thierry Poibeau"], "title": "Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature", "comment": "19 pages, 2 tables, 5 figures Conference Computational Humanities\n  Research 2025", "summary": "This research explores the evolution of the detective archetype in French\ndetective fiction through computational analysis. Using quantitative methods\nand character-level embeddings, we show that a supervised model is able to\ncapture the unity of the detective archetype across 150 years of literature,\nfrom M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,\nthe study demonstrates how the detective figure evolves from a secondary\nnarrative role to become the central character and the \"reasoning machine\" of\nthe classical detective story. In the aftermath of the Second World War, with\nthe importation of the hardboiled tradition into France, the archetype becomes\nmore complex, navigating the genre's turn toward social violence and moral\nambiguity.", "AI": {"tldr": "本研究通过计算分析，揭示了法国侦探小说中侦探原型150年间的演变，从次要角色到核心“推理机器”，并在二战后变得更加复杂。", "motivation": "探究法国侦探小说中侦探原型的历史演变，并理解其如何随着时间和社会文化背景的变化而发展。", "method": "采用计算分析、定量方法、字符级嵌入（character-level embeddings）以及监督模型。", "result": "一个监督模型能够捕捉到150年间法国侦探原型的一致性；侦探形象从次要叙事角色演变为经典侦探故事的核心人物和“推理机器”；二战后，受硬汉派传统影响，侦探原型变得更加复杂，反映了社会暴力和道德模糊。", "conclusion": "法国侦探小说中的侦探原型在漫长历史中保持了核心统一性，并经历了从功能性角色到复杂人设的演变，以适应文学类型和社会主题的变化。"}}
{"id": "2511.01452", "categories": ["eess.SY", "cs.GT", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.01452", "abs": "https://arxiv.org/abs/2511.01452", "authors": ["Leonardo Pedroso", "Andrea Agazzi", "W. P. M. H. Heemels", "Mauro Salazar"], "title": "Evolutionary Dynamics in Continuous-time Finite-state Mean Field Games - Part I: Equilibria", "comment": null, "summary": "We study a dynamic game with a large population of players who choose actions\nfrom a finite set in continuous time. Each player has a state in a finite state\nspace that evolves stochastically with their actions. A player's reward depends\nnot only on their own state and action but also on the distribution of states\nand actions across the population, capturing effects such as congestion in\ntraffic networks. While prior work in evolutionary game theory has primarily\nfocused on static games without individual player state dynamics, we present\nthe first comprehensive evolutionary analysis of such dynamic games. We propose\nan evolutionary model together with a mean field approximation of the\nfinite-population game and establish strong approximation guarantees. We show\nthat standard solution concepts for dynamic games lack an evolutionary\ninterpretation, and we propose a new concept - the Mixed Stationary Nash\nEquilibrium (MSNE) - which admits one. We analyze the relationship between MSNE\nand the rest points of the mean field evolutionary model and study the\nevolutionary stability of MSNE.", "AI": {"tldr": "本文研究了具有大量玩家的连续时间动态博弈，其中玩家状态随行动随机演化且奖励取决于整体分布。论文提出了一个演化模型和均场近似，并引入了“混合平稳纳什均衡”（MSNE）这一新的概念，以提供演化解释。", "motivation": "以往的演化博弈论主要关注静态博弈，缺乏对具有个体玩家状态动态的动态博弈的演化分析。此外，现实世界中的拥堵等效应依赖于人口状态和行动的分布，需要一个能捕捉这些效应的动态模型。", "method": "本文提出了一个演化模型，并结合有限人口博弈的均场近似，建立了强大的近似保证。同时，为了解决标准动态博弈解概念缺乏演化解释的问题，提出了“混合平稳纳什均衡”（MSNE）这一新概念。", "result": "研究发现，标准动态博弈的解概念缺乏演化解释，而MSNE则具有演化解释。论文分析了MSNE与均场演化模型的静止点之间的关系，并研究了MSNE的演化稳定性。", "conclusion": "本文首次对具有个体玩家状态动态的动态博弈进行了全面的演化分析，并引入了具有演化解释的MSNE概念，为理解和分析这类复杂系统提供了新的工具和视角。"}}
{"id": "2511.01219", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01219", "abs": "https://arxiv.org/abs/2511.01219", "authors": ["Muhua Zhang", "Lei Ma", "Ying Wu", "Kai Shen", "Deqing Huang", "Henry Leung"], "title": "Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference", "comment": "10 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "This paper addresses the Kidnapped Robot Problem (KRP), a core localization\nchallenge of relocalizing a robot in a known map without prior pose estimate\nwhen localization loss or at SLAM initialization. For this purpose, a passive\n2-D global relocalization framework is proposed. It estimates the global pose\nefficiently and reliably from a single LiDAR scan and an occupancy grid map\nwhile the robot remains stationary, thereby enhancing the long-term autonomy of\nmobile robots. The proposed framework casts global relocalization as a\nnon-convex problem and solves it via the multi-hypothesis scheme with batched\nmulti-stage inference and early termination, balancing completeness and\nefficiency. The Rapidly-exploring Random Tree (RRT), under traversability\nconstraints, asymptotically covers the reachable space to generate sparse,\nuniformly distributed feasible positional hypotheses, fundamentally reducing\nthe sampling space. The hypotheses are preliminarily ordered by the proposed\nScan Mean Absolute Difference (SMAD), a coarse beam-error level metric that\nfacilitates the early termination by prioritizing high-likelihood candidates.\nThe SMAD computation is optimized for non-panoramic scans. And the\nTranslation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for\nreliable orientation selection at hypothesized positions and accurate final\npose evaluation to mitigate degradation in conventional likelihood-field\nmetrics under translational uncertainty induced by sparse hypotheses, as well\nas non-panoramic LiDAR scan and environmental changes. Real-world experiments\non a resource-constrained mobile robot with non-panoramic LiDAR scan\ndemonstrate that the proposed framework outperforms existing methods in both\nglobal relocalization success rate and computational efficiency.", "AI": {"tldr": "本文提出了一种被动式2D全局重定位框架，用于解决“被绑架机器人问题”（KRP），即在无先验位姿估计的情况下，利用单次激光雷达扫描和占据栅格地图进行高效可靠的全局位姿估计。", "motivation": "在机器人定位丢失或SLAM初始化时，机器人需要在已知地图中进行重定位，但缺乏先验位姿估计。解决这一“被绑架机器人问题”（KRP）对于增强移动机器人的长期自主性至关重要。", "method": "该框架将全局重定位视为非凸问题，并通过多假设方案结合批量多阶段推理和提前终止来解决。它利用RRT在可遍历约束下渐近覆盖可达空间，生成稀疏、均匀分布的位姿假设。提出Scan Mean Absolute Difference (SMAD) 对假设进行初步排序以实现提前终止。还提出Translation-Affinity Scan-to-Map Alignment Metric (TAM) 用于在假设位置进行可靠的方向选择和精确的最终位姿评估。", "result": "在资源受限的移动机器人上进行的真实世界实验表明，所提出的框架在全局重定位成功率和计算效率方面均优于现有方法。", "conclusion": "该被动式2D全局重定位框架能够高效可靠地从单次激光雷达扫描和占据栅格地图中估计全局位姿，有效解决了“被绑架机器人问题”，提升了移动机器人的长期自主性。"}}
{"id": "2511.00657", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00657", "abs": "https://arxiv.org/abs/2511.00657", "authors": ["Eshaan Tanwar", "Anwoy Chatterjee", "Michael Saxon", "Alon Albalak", "William Yang Wang", "Tanmoy Chakraborty"], "title": "Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge", "comment": "Accepted in EMNLP 2025. Code at: https://github.com/EshaanT/XNationQA", "summary": "Most multilingual question-answering benchmarks, while covering a diverse\npool of languages, do not factor in regional diversity in the information they\ncapture and tend to be Western-centric. This introduces a significant gap in\nfairly evaluating multilingual models' comprehension of factual information\nfrom diverse geographical locations. To address this, we introduce XNationQA\nfor investigating the cultural literacy of multilingual LLMs. XNationQA\nencompasses a total of 49,280 questions on the geography, culture, and history\nof nine countries, presented in seven languages. We benchmark eight standard\nmultilingual LLMs on XNationQA and evaluate them using two novel transference\nmetrics. Our analyses uncover a considerable discrepancy in the models'\naccessibility to culturally specific facts across languages. Notably, we often\nfind that a model demonstrates greater knowledge of cultural information in\nEnglish than in the dominant language of the respective culture. The models\nexhibit better performance in Western languages, although this does not\nnecessarily translate to being more literate for Western countries, which is\ncounterintuitive. Furthermore, we observe that models have a very limited\nability to transfer knowledge across languages, particularly evident in\nopen-source models.", "AI": {"tldr": "该研究引入了XNationQA基准来评估多语言大型语言模型(LLMs)的文化素养，发现模型在不同语言中获取文化特定事实的能力存在显著差异，且常表现出英语中心偏见和有限的跨语言知识迁移能力。", "motivation": "大多数多语言问答基准虽然涵盖多种语言，但在其信息中未考虑区域多样性，且倾向于以西方为中心。这导致在公平评估多语言模型对来自不同地理位置的事实信息的理解方面存在显著差距。", "method": "研究引入了XNationQA，一个包含49,280个问题的基准，涵盖九个国家的地理、文化和历史，并以七种语言呈现。研究使用两个新颖的迁移度量标准，对八个标准多语言LLMs进行了基准测试和评估。", "result": "分析揭示了模型在不同语言中获取文化特定事实的能力存在显著差异。值得注意的是，模型在英语中对文化信息的了解通常超过该文化各自的主导语言。模型在西方语言中表现更好，但这并不一定意味着对西方国家有更高的文化素养，这与直觉相悖。此外，模型跨语言迁移知识的能力非常有限，在开源模型中尤为明显。", "conclusion": "多语言LLMs在文化素养方面存在显著的语言和文化偏见，尤其是在跨语言知识迁移方面表现不佳。这表明当前模型在理解和处理非西方及多语言文化信息方面存在局限性。"}}
{"id": "2511.01491", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01491", "abs": "https://arxiv.org/abs/2511.01491", "authors": ["Irched Chafaa", "E. Veronica Belmega", "Giacomo Bacci"], "title": "Deep Learning Prediction of Beam Coherence Time for Near-FieldTeraHertz Networks", "comment": "IEEE Wireless Communication Letters (accepted October 2025)", "summary": "Large multiple antenna arrays coupled with accu- rate beamforming are\nessential in terahertz (THz) communi- cations to ensure link reliability.\nHowever, as the number of antennas increases, beam alignment (focusing) and\nbeam tracking in mobile networks incur prohibitive overhead. Additionally, the\nnear-field region expands both with the size of antenna arrays and the carrier\nfrequency, calling for adjustments in the beamforming to account for spherical\nwavefront instead of the conventional planar wave assumption. In this letter,\nwe introduce a novel beam coherence time for mobile THz networks, to\ndrastically reduce the rate of beam updates. Then, we propose a deep learning\nmodel, relying on a simple feedforward neural network with a time-dependent\ninput, to predict the beam coherence time and adjust the beamforming on the fly\nwith minimal overhead. Our numerical results demonstrate the effectiveness of\nthe proposed approach by enabling higher data rates while reducing the\noverhead, especially at high (i.e., vehicular) mobility.", "AI": {"tldr": "本文提出了一种针对移动太赫兹（THz）网络的波束相干时间概念，并结合深度学习模型预测该时间，以显著减少波束更新开销，同时提高数据速率，尤其适用于高移动性场景。", "motivation": "太赫兹通信中的大型多天线阵列需要精确波束赋形以确保链路可靠性。然而，随着天线数量增加，波束对齐和跟踪会产生过高的开销。此外，近场区域随天线阵列尺寸和载波频率的增加而扩大，要求波束赋形适应球面波前而非传统的平面波假设。", "method": "引入一种新颖的波束相干时间概念，以大幅降低波束更新频率。提出一个基于简单前馈神经网络的深度学习模型，该模型以时间相关输入来预测波束相干时间，并实时调整波束赋形，以最小化开销。", "result": "数值结果表明，所提出的方法有效提高了数据速率并降低了开销，特别是在高（例如，车载）移动性场景下表现突出。", "conclusion": "所提出的方法通过引入波束相干时间并利用深度学习进行预测，能够有效减少移动太赫兹网络中的波束更新开销，同时在保持高数据速率方面表现出色，尤其在高移动性环境下更具优势。"}}
{"id": "2511.00255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00255", "abs": "https://arxiv.org/abs/2511.00255", "authors": ["Fangxun Liu", "S M Rayeed", "Samuel Stevens", "Alyson East", "Cheng Hsuan Chiang", "Colin Lee", "Daniel Yi", "Junke Yang", "Tejas Naik", "Ziyi Wang", "Connor Kilrain", "Elijah H Buckwalter", "Jiacheng Hou", "Saul Ibaven Bueno", "Shuheng Wang", "Xinyue Ma", "Yifan Liu", "Zhiyuan Tao", "Ziheng Zhang", "Eric Sokol", "Michael Belitz", "Sydne Record", "Charles V. Stewart", "Wei-Lun Chao"], "title": "BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing", "comment": "4 pages, NeurIPS 2025 Workshop Imageomics", "summary": "In entomology and ecology research, biologists often need to collect a large\nnumber of insects, among which beetles are the most common species. A common\npractice for biologists to organize beetles is to place them on trays and take\na picture of each tray. Given the images of thousands of such trays, it is\nimportant to have an automated pipeline to process the large-scale data for\nfurther research. Therefore, we develop a 3-stage pipeline to detect all the\nbeetles on each tray, sort and crop the image of each beetle, and do\nmorphological segmentation on the cropped beetles. For detection, we design an\niterative process utilizing a transformer-based open-vocabulary object detector\nand a vision-language model. For segmentation, we manually labeled 670 beetle\nimages and fine-tuned two variants of a transformer-based segmentation model to\nachieve fine-grained segmentation of beetles with relatively high accuracy. The\npipeline integrates multiple deep learning methods and is specialized for\nbeetle image processing, which can greatly improve the efficiency to process\nlarge-scale beetle data and accelerate biological research.", "AI": {"tldr": "该研究开发了一个三阶段的深度学习流程，用于自动化处理大规模甲虫图像数据，包括检测、裁剪和形态分割，旨在提高生物学研究效率。", "motivation": "生物学家在昆虫学和生态学研究中需要收集和处理大量甲虫图像。传统的手动处理方法效率低下，难以应对大规模图像数据，因此需要一个自动化的数据处理流程来加速研究。", "method": "研究开发了一个三阶段的自动化流程：1. 检测托盘上的所有甲虫；2. 对每个甲虫图像进行排序和裁剪；3. 对裁剪后的甲虫进行形态分割。其中，检测阶段采用基于Transformer的开放词汇目标检测器和视觉-语言模型进行迭代处理。分割阶段则通过手动标注670张甲虫图像，并微调两种基于Transformer的分割模型实现。", "result": "该流程成功实现了对甲虫的精细形态分割，并达到了相对较高的准确性。整体而言，该自动化管道能够显著提高大规模甲虫数据处理的效率。", "conclusion": "该集成了多种深度学习方法的专业化甲虫图像处理流程，能够极大地提高大规模甲虫数据处理的效率，从而加速生物学研究进程。"}}
{"id": "2511.00260", "categories": ["cs.CV", "68T07 (Primary) 68T45, 92C55 (Secondary)"], "pdf": "https://arxiv.org/pdf/2511.00260", "abs": "https://arxiv.org/abs/2511.00260", "authors": ["Linzhe Jiang", "Jiayuan Huang", "Sophia Bano", "Matthew J. Clarkson", "Zhehua Mao", "Mobarak I. Hoque"], "title": "MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba", "comment": "12 pages, 4 figures, 3 tables, IPCAI conference", "summary": "Accurate 3D point cloud registration underpins reliable image-guided\ncolonoscopy, directly affecting lesion localization, margin assessment, and\nnavigation safety. However, biological tissue exhibits repetitive textures and\nlocally homogeneous geometry that cause feature degeneracy, while substantial\ndomain shifts between pre-operative anatomy and intra-operative observations\nfurther degrade alignment stability. To address these clinically critical\nchallenges, we introduce a novel 3D registration method tailored for endoscopic\nnavigation and a high-quality, clinically grounded dataset to support rigorous\nand reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale\nbenchmark dataset with 10,014 geometrically aligned point cloud pairs derived\nfrom clinical CT data. We propose MambaNetLK, a novel correspondence-free\nregistration framework, which enhances the PointNetLK architecture by\nintegrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.\nAs a result, the proposed framework efficiently captures long-range\ndependencies with linear-time complexity. The alignment is achieved iteratively\nusing the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,\nMambaNetLK achieves the best performance compared with the state-of-the-art\nmethods, reducing median rotation error by 56.04% and RMSE translation error by\n26.19% over the second-best method. The model also demonstrates strong\ngeneralization on ModelNet40 and superior robustness to initial pose\nperturbations. MambaNetLK provides a robust foundation for 3D registration in\nsurgical navigation. The combination of a globally expressive SSM-based feature\nextractor and a large-scale clinical dataset enables more accurate and reliable\nguidance systems in minimally invasive procedures like colonoscopy.", "AI": {"tldr": "本文提出了一种名为MambaNetLK的新型3D点云配准方法，结合Mamba状态空间模型和Lucas-Kanade算法，以解决图像引导结肠镜检查中的配准挑战。同时发布了大规模临床数据集C3VD-Raycasting-10k，MambaNetLK在该数据集上表现优于现有最佳方法。", "motivation": "图像引导结肠镜检查中，准确的3D点云配准对于病灶定位、边缘评估和导航安全至关重要。然而，生物组织重复的纹理、局部同质的几何形状导致特征退化，术前解剖与术中观察之间的显著域偏移也进一步降低了对齐的稳定性，因此需要更鲁棒的配准方法。", "method": "研究人员创建了一个大规模、高质量的临床数据集C3VD-Raycasting-10k，包含10,014对几何对齐的点云。提出MambaNetLK，一个无对应关系的配准框架，通过将Mamba状态空间模型（SSM）作为跨模态特征提取器集成到PointNetLK架构中，以线性时间复杂度高效捕获长距离依赖。对齐通过迭代的Lucas-Kanade算法实现。", "result": "在临床数据集C3VD-Raycasting-10k上，MambaNetLK的性能优于现有最佳方法，将中位数旋转误差降低了56.04%，RMSE平移误差降低了26.19%。该模型还在ModelNet40上表现出强大的泛化能力，并对初始姿态扰动具有卓越的鲁棒性。", "conclusion": "MambaNetLK为外科导航中的3D配准提供了坚实的基础。结合基于SSM的全局表达特征提取器和大规模临床数据集，该方法能够在结肠镜检查等微创手术中实现更准确、更可靠的引导系统。"}}
{"id": "2511.00689", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00689", "abs": "https://arxiv.org/abs/2511.00689", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Fred Morstatter"], "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?", "comment": null, "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten\nlanguages--spanning high-, medium-, and low-resource languages--using six LLMs\non HarmBench and AdvBench. We assess two jailbreak types:\nlogical-expression-based and adversarial-prompt-based. For both types, attack\nsuccess and defense robustness vary across languages: high-resource languages\nare safer under standard queries but more vulnerable to adversarial ones.\nSimple defenses can be effective, but are language- and model-dependent. These\nfindings call for language-aware and cross-lingual safety benchmarks for LLMs.", "AI": {"tldr": "本文首次系统性地评估了大型语言模型（LLMs）在十种语言中越狱攻击和防御的跨语言泛化能力，发现攻击成功率和防御鲁棒性因语言而异，并呼吁建立语言感知和跨语言的安全基准。", "motivation": "尽管LLMs经过安全对齐，但越狱攻击仍能绕过安全机制。现有研究对越狱和防御的跨语言泛化能力探索不足。", "method": "研究采用系统性的多语言评估方法，在十种语言（包括高、中、低资源语言）上，使用六个LLMs，通过HarmBench和AdvBench评估了两种越狱攻击类型：基于逻辑表达式和基于对抗性提示的攻击。", "result": "攻击成功率和防御鲁棒性因语言而异。高资源语言在标准查询下更安全，但更容易受到对抗性查询的攻击。简单的防御措施可能有效，但其效果取决于语言和模型。", "conclusion": "研究结果表明需要为LLMs开发语言感知和跨语言的安全基准。"}}
{"id": "2511.01224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01224", "abs": "https://arxiv.org/abs/2511.01224", "authors": ["Chengmeng Li", "Yaxin Peng"], "title": "Embodiment Transfer Learning for Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nlearning, enabling training on large-scale, cross-embodiment data and\nfine-tuning for specific robots. However, state-of-the-art autoregressive VLAs\nstruggle with multi-robot collaboration. We introduce embodiment transfer\nlearning, denoted as ET-VLA, a novel framework for efficient and effective\ntransfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic\nContinued Pretraining (SCP), which uses synthetically generated data to warm up\nthe model for the new embodiment, bypassing the need for real human\ndemonstrations and reducing data collection costs. SCP enables the model to\nlearn correct actions and precise action token numbers. Following SCP, the\nmodel is fine-tuned on target embodiment data. To further enhance the model\nperformance on multi-embodiment, we present the Embodied Graph-of-Thought\ntechnique, a novel approach that formulates each sub-task as a node, that\nallows the VLA model to distinguish the functionalities and roles of each\nembodiment during task execution. Our work considers bimanual robots, a simple\nversion of multi-robot to verify our approaches. We validate the effectiveness\nof our method on both simulation benchmarks and real robots covering three\ndifferent bimanual embodiments. In particular, our proposed ET-VLA \\space can\noutperform OpenVLA on six real-world tasks over 53.2%. We will open-source all\ncodes to support the community in advancing VLA models for robot learning.", "AI": {"tldr": "本文提出ET-VLA框架，通过合成数据持续预训练（SCP）和具身思维图（Embodied Graph-of-Thought）技术，将预训练的视觉-语言-动作（VLA）模型高效迁移到多机器人（特别是双臂机器人）协作任务中，显著提升了真实世界任务的性能。", "motivation": "现有的自回归VLA模型在多机器人协作方面表现不佳，难以有效处理跨具身数据和精细化控制，且需要大量的真实世界演示数据。", "method": "该研究引入了ET-VLA框架，其核心是合成持续预训练（SCP），利用合成数据预热模型以适应新具身，避免了对真实人类演示的需求。SCP使模型能学习正确的动作和精确的动作令牌数量。之后，模型在目标具身数据上进行微调。为进一步提升多具身性能，提出了具身思维图（Embodied Graph-of-Thought）技术，将每个子任务建模为节点，使VLA模型在任务执行中能区分每个具身的功能和角色。方法在双臂机器人（作为多机器人的简化版本）上进行了验证。", "result": "ET-VLA方法在仿真基准和涵盖三种不同双臂具身的真实机器人上都得到了验证。实验结果表明，ET-VLA在六个真实世界任务上比OpenVLA的性能提升超过53.2%。", "conclusion": "ET-VLA框架通过合成持续预训练和具身思维图技术，成功且高效地将预训练VLA模型迁移到多机器人协作任务中，特别是在双臂机器人上取得了显著的性能提升，有效解决了现有VLA模型在多机器人协作方面的挑战。"}}
{"id": "2511.01288", "categories": ["cs.RO", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2511.01288", "abs": "https://arxiv.org/abs/2511.01288", "authors": ["Bixuan Zhang", "Fengqi Zhang", "Haojie Chen", "You Wang", "Jie Hao", "Zhiyuan Luo", "Guang Li"], "title": "A High-Speed Capable Spherical Robot", "comment": "5 pages", "summary": "This paper designs a new spherical robot structure capable of supporting\nhigh-speed motion at up to 10 m/s. Building upon a single-pendulum-driven\nspherical robot, the design incorporates a momentum wheel with an axis aligned\nwith the secondary pendulum, creating a novel spherical robot structure.\nPractical experiments with the physical prototype have demonstrated that this\nnew spherical robot can achieve stable high-speed motion through simple\ndecoupled control, which was unattainable with the original structure. The\nspherical robot designed for high-speed motion not only increases speed but\nalso significantly enhances obstacle-crossing performance and terrain\nrobustness.", "AI": {"tldr": "本文设计了一种新型球形机器人结构，通过引入动量轮，实现了高达10米/秒的稳定高速运动，并显著提升了越障和地形适应能力。", "motivation": "现有单摆驱动球形机器人难以实现高速运动，且在越障和地形鲁棒性方面有待提升，促使研究者寻求新的结构设计。", "method": "在单摆驱动球形机器人的基础上，引入了一个动量轮，其轴线与次级摆对齐，形成了一种新颖的球形机器人结构。通过简单的解耦控制进行驱动。", "result": "物理原型实验证明，该新型球形机器人能够实现稳定的高速运动（高达10米/秒），这是原有结构无法达到的。同时，它显著提升了越障性能和地形鲁棒性。", "conclusion": "所设计的新型球形机器人结构不仅大幅提高了运动速度，而且通过简单的控制实现了稳定运行，并增强了机器人在复杂地形下的适应能力和越障表现。"}}
{"id": "2511.00819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00819", "abs": "https://arxiv.org/abs/2511.00819", "authors": ["Yuxuan Hu", "Jianchao Tan", "Jiaqi Zhang", "Wen Zan", "Pingwei Sun", "Yifan Lu", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Jing Zhang"], "title": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies", "comment": null, "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.", "AI": {"tldr": "本文通过交替使用局部和全局注意力模式，并引入潜在注意力（Latent Attention）变体（MLA和GLA），系统性地改进了原生稀疏注意力（NSA），显著增强了长上下文建模能力，同时降低了KV缓存内存消耗。", "motivation": "现有稀疏注意力在长上下文建模中可能存在局限性，例如固定注意力模式可能无法有效传播长距离依赖，以及KV缓存内存消耗问题，因此需要提出有针对性的改进方案。", "method": "1. 对原生稀疏注意力（NSA）进行系统性分析。2. 提出在不同层之间交替使用局部（滑动窗口）和全局（压缩、选择性）注意力模式。3. 进一步优化NSA分支：滑动窗口分支采用多头潜在注意力（Multi-head Latent Attention, MLA），而压缩和选择性分支采用组头潜在注意力（Group-head Latent Attention, GLA）。", "result": "1. 将KV缓存内存消耗比NSA减少50%。2. 显著提升了模型的常识推理和长文本理解能力。3. 在常识推理和长上下文理解任务上，与全注意力及原生稀疏注意力相比，性能持平或超越，并在340M至1.3B参数规模的模型上得到验证。", "conclusion": "通过在层间交替使用局部和全局注意力模式，并结合潜在注意力（MLA和GLA）对NSA分支进行精炼，能够有效增强长上下文建模能力，提升模型性能，并显著优化内存效率。"}}
{"id": "2511.01232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01232", "abs": "https://arxiv.org/abs/2511.01232", "authors": ["Yu-Ting Lai", "Jacob Rosen", "Yasamin Foroutani", "Ji Ma", "Wen-Cheng Wu", "Jean-Pierre Hubschman", "Tsu-Chin Tsao"], "title": "High-Precision Surgical Robotic System for Intraocular Procedures", "comment": null, "summary": "Despite the extensive demonstration of robotic systems for both cataract and\nvitreoretinal procedures, existing technologies or mechanisms still possess\ninsufficient accuracy, precision, and degrees of freedom for instrument\nmanipulation or potentially automated tool exchange during surgical procedures.\nA new robotic system that focuses on improving tooltip accuracy, tracking\nperformance, and smooth instrument exchange mechanism is therefore designed and\nmanufactured. Its tooltip accuracy, precision, and mechanical capability of\nmaintaining small incision through remote center of motion were externally\nevaluated using an optical coherence tomography (OCT) system. Through robot\ncalibration and precise coordinate registration, the accuracy of tooltip\npositioning was measured to be 0.053$\\pm$0.031 mm, and the overall performance\nwas demonstrated on an OCT-guided automated cataract lens extraction procedure\nwith deep learning-based pre-operative anatomical modeling and real-time\nsupervision.", "AI": {"tldr": "本文设计并制造了一种新型眼科手术机器人系统，显著提高了工具尖端精度、跟踪性能和器械更换的顺畅性，并在OCT引导的白内障晶状体摘除术中进行了验证。", "motivation": "现有的用于白内障和玻璃体视网膜手术的机器人系统在器械操作的精度、准确性和自由度方面，以及在手术过程中自动工具更换方面，仍存在不足。", "method": "研究人员设计并制造了一个新的机器人系统，该系统侧重于提高工具尖端精度、跟踪性能和器械更换机制。他们使用光学相干断层扫描（OCT）系统外部评估了工具尖端精度、准确性以及通过运动远程中心保持小切口的能力。通过机器人校准和精确坐标配准，测量了工具尖端定位精度，并在结合深度学习术前解剖建模和实时监督的OCT引导的自动化白内障晶状体摘除术中展示了其整体性能。", "result": "该机器人系统的工具尖端定位精度为0.053±0.031毫米。其整体性能在OCT引导的自动化白内障晶状体摘除手术中得到了成功验证。", "conclusion": "该新型机器人系统有效提升了眼科手术的工具尖端精度和操作性能，并通过自动化白内障晶状体摘除程序展示了其作为高精度手术平台的潜力，克服了现有技术的局限性。"}}
{"id": "2511.01018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01018", "abs": "https://arxiv.org/abs/2511.01018", "authors": ["Hui-Lee Ooi", "Nicholas Mitsakakis", "Margerie Huet Dastarac", "Roger Zemek", "Amy C. Plint", "Jeff Gilchrist", "Khaled El Emam", "Dhenuka Radhakrishnan"], "title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "comment": null, "summary": "Recurrent exacerbations remain a common yet preventable outcome for many\nchildren with asthma. Machine learning (ML) algorithms using electronic medical\nrecords (EMR) could allow accurate identification of children at risk for\nexacerbations and facilitate referral for preventative comprehensive care to\navoid this morbidity. We developed ML algorithms to predict repeat severe\nexacerbations (i.e. asthma-related emergency department (ED) visits or future\nhospital admissions) for children with a prior asthma ED visit at a tertiary\ncare children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from\nthe Children's Hospital of Eastern Ontario (CHEO) linked with environmental\npollutant exposure and neighbourhood marginalization information was used to\ntrain various ML models. We used boosted trees (LGBM, XGB) and 3 open-source\nlarge language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and\nLlama-8b-UltraMedical). Models were tuned and calibrated then validated in a\nsecond retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from\nCHEO. Models were compared using the area under the curve (AUC) and F1 scores,\nwith SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the\nfinal AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage\nacuity scale, medical complexity, food allergy, prior ED visits for non-asthma\nrespiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This\nis a nontrivial improvement over the current decision rule which has F1=0.334.\nWhile the most predictive features in the AIRE-KIDS_HOSP model included medical\ncomplexity, prior asthma ED visit, average wait time in the ED, the pediatric\nrespiratory assessment measure score at triage and food allergy.", "AI": {"tldr": "本研究开发并验证了基于电子病历的机器学习算法，用于预测儿童哮喘的复发性严重恶化（急诊或住院），LGBM模型表现最佳，显著优于现有决策规则。", "motivation": "哮喘儿童反复发作是常见但可预防的后果。利用机器学习算法和电子病历数据，可以准确识别高风险儿童，并促进其接受预防性综合护理，从而避免疾病加重。", "method": "研究使用了加拿大东安大略儿童医院（CHEO）的Epic电子病历数据，包括回顾性的COVID-19前（2017-2019年，N=2716）和COVID-19后（2022-2023年，N=1237）数据集。数据与环境污染物暴露和社区边缘化信息相链接。开发并训练了多种机器学习模型，包括梯度提升树（LGBM、XGB）和三种开源大型语言模型（DistilGPT2、Llama 3.2 1B、Llama-8b-UltraMedical）。模型经过调优、校准，并在COVID-19后数据集上进行验证。模型性能通过AUC和F1分数进行比较，并使用SHAP值确定最具预测性的特征。", "result": "LGBM机器学习模型表现最佳。AIRE-KIDS_ED模型（预测急诊就诊）的AUC为0.712，F1分数为0.51，显著优于当前F1为0.334的决策规则。该模型最具预测性的特征包括：既往哮喘急诊就诊、加拿大分诊急性量表、医疗复杂性、食物过敏、既往非哮喘呼吸道诊断急诊就诊和年龄。AIRE-KIDS_HOSP模型（预测住院）最具预测性的特征包括：医疗复杂性、既往哮喘急诊就诊、急诊平均等待时间、分诊时的儿科呼吸评估量表得分和食物过敏。", "conclusion": "机器学习模型，特别是LGBM模型，能够有效且准确地识别有复发性严重哮喘恶化风险的儿童，相较于现有方法有显著提升。这为早期识别和转诊高风险儿童进行预防性护理提供了有价值的工具。"}}
{"id": "2511.00261", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00261", "abs": "https://arxiv.org/abs/2511.00261", "authors": ["Neha Balamurugan", "Sarah Wu", "Adam Chun", "Gabe Gaw", "Cristobal Eyzaguirre", "Tobias Gerstenberg"], "title": "Spot The Ball: A Benchmark for Visual Social Inference", "comment": null, "summary": "Humans excel at visual social inference, the ability to infer hidden elements\nof a scene from subtle behavioral cues such as other people's gaze, pose, and\norientation. This ability drives everyday social reasoning in humans and is\ncritical for developing more human-like AI agents. We introduce Spot The Ball,\na challenging benchmark for evaluating visual social inference in\nvision-language models (VLMs) using sports as a test domain. The task is to\nlocalize a removed sports ball from soccer, basketball, and volleyball images.\nWe present a curated evaluation set with human baselines and a scalable\npipeline for generating additional test items. We evaluate four\nstate-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting\nstrategies, finding that humans are consistently two to three times more\naccurate (20-34%) than models ($\\leq$ 17%) across all sports. Our analyses show\nthat models rely on superficial spatial heuristics--such as guessing near the\nimage center or nearby players--while humans leverage social cues like gaze\ndirection and body pose. These findings reveal a persistent human-model gap in\nvisual social reasoning and underscore the need for architectures that\nexplicitly encode structured behavioral cues to achieve robust, human-like\ninference.", "AI": {"tldr": "该研究引入了“Spot The Ball”基准测试来评估视觉社交推理能力，发现人类在根据社交线索推断移除的球的位置方面，比最先进的视觉语言模型准确2-3倍，表明AI在视觉社交推理方面存在显著差距。", "motivation": "人类擅长视觉社交推理，即从行为线索（如凝视、姿势）推断场景中的隐藏元素，这对于日常社交推理和开发类人AI至关重要。当前AI在这方面能力不足。", "method": "引入了“Spot The Ball”基准测试，要求视觉语言模型从移除球的足球、篮球和排球图像中定位球。构建了一个人工评估集和可扩展的测试项生成流程。使用三种提示策略评估了四种最先进的视觉语言模型（Gemini、GPT、LLaMA、Qwen），并与人类基线进行比较。分析了模型和人类推理策略的差异。", "result": "人类的准确率（20-34%）始终是模型的（≤ 17%）的2到3倍。模型主要依赖于表面的空间启发式方法（如猜测图像中心或靠近玩家的位置），而人类则利用凝视方向和身体姿势等社交线索。这些发现揭示了视觉社交推理中持续存在的人类-模型差距。", "conclusion": "视觉社交推理方面存在显著的人类-模型差距。为了实现稳健的、类人的推理能力，需要开发能够明确编码结构化行为线索的AI架构。"}}
{"id": "2511.01638", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2511.01638", "abs": "https://arxiv.org/abs/2511.01638", "authors": ["Mazen Alamir"], "title": "On polynomial explicit partial estimator design for nonlinear systems with parametric uncertainties", "comment": "Submitted to ACC2026", "summary": "This paper investigates the idea of designing data-driven partial estimators\nfor nonlinear systems showing parametric uncertainties using sparse\nmultivariate polynomial relationships. A general framework is first presented\nand then validated on two illustrative examples with comparison to different\npossible Machine/Deep-Learning based alternatives. The results suggests the\nsuperiority of the proposed sparse identification scheme, at least when the\nlearning data is small.", "AI": {"tldr": "本文提出并验证了一种基于稀疏多元多项式关系的数据驱动局部估计器，用于具有参数不确定性的非线性系统，在小数据量下表现出优越性。", "motivation": "为具有参数不确定性的非线性系统设计数据驱动的局部估计器。", "method": "提出一个基于稀疏多元多项式关系的数据驱动局部估计器通用框架，并通过两个示例与不同的机器学习/深度学习替代方案进行比较验证。", "result": "结果表明，所提出的稀疏识别方案在学习数据量较小的情况下，表现出优越性。", "conclusion": "所提出的稀疏识别方案在小数据量下，对参数不确定非线性系统的局部估计具有优势。"}}
{"id": "2511.00269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00269", "abs": "https://arxiv.org/abs/2511.00269", "authors": ["Long Li", "Jiajia Li", "Dong Chen", "Lina Pu", "Haibo Yao", "Yanbo Huang"], "title": "FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture", "comment": null, "summary": "Accurate classification plays a pivotal role in smart agriculture, enabling\napplications such as crop monitoring, fruit recognition, and pest detection.\nHowever, conventional centralized training often requires large-scale data\ncollection, which raises privacy concerns, while standard federated learning\nstruggles with non-independent and identically distributed (non-IID) data and\nincurs high communication costs. To address these challenges, we propose a\nfederated learning framework that integrates a frozen Contrastive\nLanguage-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight\ntransformer classifier. By leveraging the strong feature extraction capability\nof the pre-trained CLIP ViT, the framework avoids training large-scale models\nfrom scratch and restricts federated updates to a compact classifier, thereby\nreducing transmission overhead significantly. Furthermore, to mitigate\nperformance degradation caused by non-IID data distribution, a small subset\n(1%) of CLIP-extracted feature representations from all classes is shared\nacross clients. These shared features are non-reversible to raw images,\nensuring privacy preservation while aligning class representation across\nparticipants. Experimental results on agricultural classification tasks show\nthat the proposed method achieve 86.6% accuracy, which is more than 4 times\nhigher compared to baseline federated learning approaches. This demonstrates\nthe effectiveness and efficiency of combining vision-language model features\nwith federated learning for privacy-preserving and scalable agricultural\nintelligence.", "AI": {"tldr": "本文提出了一种联邦学习框架，将冻结的CLIP ViT与轻量级分类器结合，用于智能农业分类。该框架通过利用CLIP的特征提取能力，并共享少量非可逆特征，有效解决了传统联邦学习中数据隐私、非IID数据和高通信成本问题，在农业分类任务中实现了高精度。", "motivation": "智能农业中的准确分类（如作物监测、水果识别、病虫害检测）至关重要。然而，传统的集中式训练面临大规模数据收集带来的隐私问题，而标准联邦学习则难以应对非独立同分布（non-IID）数据并产生高通信成本。因此，需要一种既能保护隐私，又能高效处理non-IID数据的农业分类方法。", "method": "研究者提出了一种联邦学习框架，该框架整合了冻结的对比语言-图像预训练（CLIP）视觉转换器（ViT）和一个轻量级转换器分类器。通过利用预训练CLIP ViT强大的特征提取能力，避免了从头开始训练大型模型，并将联邦更新限制在一个紧凑的分类器上，从而显著降低了传输开销。此外，为了缓解non-IID数据分布导致的性能下降，客户端之间共享了所有类别中一小部分（1%）CLIP提取的特征表示，这些特征是不可逆转为原始图像的，确保了隐私保护。", "result": "在农业分类任务上的实验结果表明，所提出的方法实现了86.6%的准确率，比基线联邦学习方法高出4倍以上。", "conclusion": "研究结果证明了将视觉-语言模型特征与联邦学习相结合的有效性和效率，为隐私保护和可扩展的农业智能提供了解决方案。"}}
{"id": "2511.01052", "categories": ["cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2511.01052", "abs": "https://arxiv.org/abs/2511.01052", "authors": ["Yeawon Lee", "Christopher C. Yang", "Chia-Hsuan Chang", "Grace Lu-Yao"], "title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports", "comment": null, "summary": "Cancer staging is critical for patient prognosis and treatment planning, yet\nextracting pathologic TNM staging from unstructured pathology reports poses a\npersistent challenge. Existing natural language processing (NLP) and machine\nlearning (ML) strategies often depend on large annotated datasets, limiting\ntheir scalability and adaptability. In this study, we introduce two Knowledge\nElicitation methods designed to overcome these limitations by enabling large\nlanguage models (LLMs) to induce and apply domain-specific rules for cancer\nstaging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses\nan iterative prompting strategy to derive staging rules directly from\nunannotated pathology reports, without requiring ground-truth labels. The\nsecond, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),\nemploys a variation of RAG where rules are pre-extracted from relevant\nguidelines in a single step and then applied, enhancing interpretability and\navoiding repeated retrieval overhead. We leverage the ability of LLMs to apply\nbroad knowledge learned during pre-training to new tasks. Using breast cancer\npathology reports from the TCGA dataset, we evaluate their performance in\nidentifying T and N stages, comparing them against various baseline approaches\non two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG\nwhen Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG\nachieves better performance when ZSCOT inference is less effective. Both\nmethods offer transparent, interpretable interfaces by making the induced rules\nexplicit. These findings highlight the promise of our Knowledge Elicitation\nmethods as scalable, high-performing solutions for automated cancer staging\nwith enhanced interpretability, particularly in clinical settings with limited\nannotated data.", "AI": {"tldr": "本研究引入了两种知识启发方法（KEwLTM和KEwRAG），使大型语言模型（LLMs）能够从非结构化病理报告中自动提取癌症TNM分期，无需大量标注数据，并提高了可解释性。", "motivation": "癌症分期对患者预后和治疗至关重要，但从非结构化病理报告中提取病理TNM分期极具挑战。现有NLP和ML策略通常依赖于大量标注数据集，限制了其可扩展性和适应性。", "method": "研究引入了两种知识启发方法，旨在使LLMs能够诱导和应用癌症分期领域的特定规则：\n1.  **KEwLTM (Knowledge Elicitation with Long-Term Memory)**：使用迭代提示策略直接从未经标注的病理报告中推导分期规则，无需真实标签。\n2.  **KEwRAG (Knowledge Elicitation with Retrieval-Augmented Generation)**：采用RAG的变体，预先从相关指南中一次性提取规则，然后应用，增强了可解释性并避免了重复检索开销。\n研究利用LLMs在预训练期间学习的广泛知识，并在TCGA乳腺癌病理报告数据集上评估了它们在识别T和N分期方面的性能，与基线方法进行比较。", "result": "研究结果表明，当Zero-Shot Chain-of-Thought (ZSCOT) 推理有效时，KEwLTM的性能优于KEwRAG；而当ZSCOT推理效果不佳时，KEwRAG表现更好。两种方法都通过明确诱导出的规则提供了透明、可解释的界面。", "conclusion": "本研究的知识启发方法有望成为可扩展、高性能的自动化癌症分期解决方案，特别是在标注数据有限的临床环境中，具有增强的可解释性。"}}
{"id": "2511.01236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01236", "abs": "https://arxiv.org/abs/2511.01236", "authors": ["Junwen Zhang", "Changyue Liu", "Pengqi Fu", "Xiang Guo", "Ye Shi", "Xudong Liang", "Zhijian Wang", "Hanzhi Ma"], "title": "Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments", "comment": "8 pages, 5 figures", "summary": "Endowed with inherent dynamical properties that grant them remarkable\nruggedness and adaptability, spherical tensegrity robots stand as prototypical\nexamples of hybrid softrigid designs and excellent mobile platforms. However,\npath planning for these robots in unknown environments presents a significant\nchallenge, requiring a delicate balance between efficient exploration and\nrobust planning. Traditional path planners, which treat the environment as a\ngeometric grid, often suffer from redundant searches and are prone to failure\nin complex scenarios due to their lack of semantic understanding. To overcome\nthese limitations, we reframe path planning in unknown environments as a\nsemantic reasoning task. We introduce a Semantic Agent for Tensegrity robots\n(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages\nhigh-level environmental comprehension to generate efficient and reliable\nplanning strategies.At the core of SATPlanner is an Adaptive Observation Window\nmechanism, inspired by the \"fast\" and \"slow\" thinking paradigms of LLMs. This\nmechanism dynamically adjusts the perceptual field of the agent: it narrows for\nrapid traversal of open spaces and expands to reason about complex obstacle\nconfigurations. This allows the agent to construct a semantic belief of the\nenvironment, enabling the search space to grow only linearly with the path\nlength (O(L)) while maintaining path quality. We extensively evaluate\nSATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,\noutperforming other real-time planning algorithms. Critically, SATPlanner\nreduces the search space by 37.2% compared to the A* algorithm while achieving\ncomparable, near-optimal path lengths. Finally, the practical feasibility of\nSATPlanner is validated on a physical spherical tensegrity robot prototype.", "AI": {"tldr": "本文提出了一种名为SATPlanner的语义代理，由大型语言模型（LLM）驱动，用于在未知环境中为球形张拉整体机器人进行路径规划。它利用自适应观察窗口机制，显著减少了搜索空间，提高了规划效率和成功率。", "motivation": "球形张拉整体机器人在未知环境中的路径规划面临挑战。传统的路径规划器将环境视为几何网格，常因缺乏语义理解而导致冗余搜索并在复杂场景中失效。", "method": "将未知环境中的路径规划重构为语义推理任务。引入了由大型语言模型（LLM）驱动的语义代理SATPlanner。SATPlanner的核心是一个自适应观察窗口机制，灵感来源于LLM的“快”和“慢”思维模式，能动态调整感知范围。这使得代理能够构建环境的语义信念，并使搜索空间仅随路径长度线性增长（O(L)）。", "result": "在1,000次模拟试验中，SATPlanner实现了100%的成功率，优于其他实时规划算法。与A*算法相比，SATPlanner将搜索空间减少了37.2%，同时实现了可比的、接近最优的路径长度。该方法还在物理球形张拉整体机器人原型上验证了其可行性。", "conclusion": "SATPlanner通过结合LLM的语义理解和自适应观察窗口，为球形张拉整体机器人在未知环境中的路径规划提供了一种高效、鲁棒且实用的解决方案，显著提升了规划性能并降低了计算成本。"}}
{"id": "2511.01033", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length.", "AI": {"tldr": "本文研究了Transformer中上下文学习的关键机制——归纳头（induction heads）的出现。作者揭示了其权重矩阵的简单可解释结构，从理论上解释了其起源，并证明了训练动态被限制在一个19维子空间中，其中3个维度足以解释归纳头的出现，且其出现时间与输入上下文长度呈二次方关系。", "motivation": "Transformer模型在自然语言处理中取得了巨大成功，部分原因在于其卓越的上下文学习（ICL）能力。为了深入理解ICL，研究者希望探究ICL背后的关键机制，特别是归纳头（induction heads）的形成和工作原理。", "method": "研究方法包括：1) 在两层Transformer中研究归纳头的出现；2) 揭示实现归纳头的权重矩阵的简单可解释结构；3) 使用最小ICL任务和修改后的Transformer架构从理论上解释这种结构的起源；4) 形式化证明训练动态被限制在参数空间的19维子空间中；5) 经验性验证该约束，并进一步研究3维子空间内的训练动态。", "result": "主要结果包括：1) 揭示了实现归纳头的权重矩阵的相对简单且可解释的结构；2) 从理论上解释了这种结构的起源；3) 形式化证明了训练动态被限制在参数空间的19维子空间中；4) 经验性验证了这一约束，并发现仅3个维度就足以解释归纳头的出现；5) 发现归纳头出现的时间遵循一个紧密的渐近边界，该边界与输入上下文长度呈二次方关系。", "conclusion": "该研究深入揭示了Transformer中归纳头（induction heads）的出现机制。通过理论分析和实证验证，论文阐明了归纳头权重矩阵的结构、其起源以及训练动态的维度约束，并量化了其形成时间与上下文长度的关系，为理解Transformer的上下文学习能力提供了重要见解。"}}
{"id": "2511.00854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00854", "abs": "https://arxiv.org/abs/2511.00854", "authors": ["Chong Lyu", "Lin Li", "Shiqing Wu", "Jingling Yuan"], "title": "TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models", "comment": null, "summary": "The increasing utilization of large language models raises significant\nconcerns about the propagation of social biases, which may result in harmful\nand unfair outcomes. However, existing debiasing methods treat the biased and\nunbiased samples independently, thus ignoring their mutual relationship. This\noversight enables a hidden negative-positive coupling, where improvements for\none group inadvertently compromise the other, allowing residual social bias to\npersist. In this paper, we introduce TriCon-Fair, a contrastive learning\nframework that employs a decoupled loss that combines triplet and language\nmodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns\neach anchor an explicitly biased negative and an unbiased positive, decoupling\nthe push-pull dynamics and avoiding positive-negative coupling, and jointly\noptimizes a language modeling (LM) objective to preserve general capability.\nExperimental results demonstrate that TriCon-Fair reduces discriminatory output\nbeyond existing debiasing baselines while maintaining strong downstream\nperformance. This suggests that our proposed TriCon-Fair offers a practical and\nethical solution for sensitive NLP applications.", "AI": {"tldr": "TriCon-Fair是一种新的对比学习框架，通过解耦损失（结合三元组和语言建模项）来消除大语言模型中的正负耦合，从而有效减少社会偏见。", "motivation": "大语言模型中社会偏见的传播导致有害和不公平的结果。现有去偏方法独立处理有偏和无偏样本，忽略了它们之间的相互关系，导致隐藏的正负耦合，即改善一组可能损害另一组，使偏见持续存在。", "method": "引入TriCon-Fair，一个对比学习框架。它使用解耦损失，结合了三元组损失和语言建模（LM）项。为每个锚点分配一个明确的有偏负样本和一个无偏正样本，解耦了推拉动态，避免了正负耦合，并联合优化LM目标以保持通用能力。", "result": "实验结果表明，TriCon-Fair在减少歧视性输出方面优于现有去偏基线，同时保持了强大的下游性能。", "conclusion": "TriCon-Fair为敏感的自然语言处理应用提供了一个实用且符合伦理的解决方案，能够有效减少大语言模型中的社会偏见。"}}
{"id": "2511.01149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments.", "AI": {"tldr": "本文提出了一种基于大语言模型的多智能体架构，用于复杂任务的模块化分解和动态协作，通过语义转换、分层分解、动态调度和全局一致性机制，提高了任务成功率、分解效率和协作平衡性。", "motivation": "现有方法在复杂任务执行中，单个智能体在任务分解和协作方面存在局限性。", "method": "该方法首先通过大语言模型将自然语言任务描述转换为统一语义表示；然后引入模块化分解机制，将总目标分解为分层子任务；接着通过动态调度和路由机制实现智能体间的合理分工和实时协作，并根据环境反馈调整策略；最后设计了约束解析和全局一致性机制，确保子任务间的连贯性和工作负载平衡。", "result": "实验结果表明，所提出的方法在任务成功率、分解效率、子任务覆盖率和协作平衡性等多个维度上优于现有方法，在整体性能和鲁棒性方面表现更佳，并在任务复杂性和通信开销之间取得了更好的平衡。", "conclusion": "本研究证明了在多智能体系统中，语言驱动的任务分解和动态协作的有效性和可行性，为复杂环境下的任务执行提供了一个系统性解决方案。"}}
{"id": "2511.01272", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01272", "abs": "https://arxiv.org/abs/2511.01272", "authors": ["Sehui Jeong", "Magaly C. Aviles", "Athena X. Naylor", "Cynthia Sung", "Allison M. Okamura"], "title": "Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics", "comment": null, "summary": "Soft robots employing compliant materials and deformable structures offer\ngreat potential for wearable devices that are comfortable and safe for human\ninteraction. However, achieving both structural integrity and compliance for\ncomfort remains a significant challenge. In this study, we present a novel\nfabrication and design method that combines the advantages of origami\nstructures with the material programmability and wearability of knitted\nfabrics. We introduce a general design method that translates origami patterns\ninto knit designs by programming both stitch and material patterns. The method\ncreates folds in preferred directions while suppressing unintended buckling and\nbending by selectively incorporating heat fusible yarn to create rigid panels\naround compliant creases. We experimentally quantify folding moments and show\nthat stitch patterning enhances folding directionality while the heat fusible\nyarn (1) keeps geometry consistent by reducing edge curl and (2) prevents\nout-of-plane deformations by stiffening panels. We demonstrate the framework\nthrough the successful reproduction of complex origami tessellations, including\nMiura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted\nKaleidocycle robot capable of locomotion. The combination of structural\nreconfigurability, material programmability, and potential for manufacturing\nscalability highlights knitted origami as a promising platform for\nnext-generation wearable robotics.", "AI": {"tldr": "该研究提出了一种结合折纸结构和针织面料的新型设计与制造方法，通过编程针脚和材料模式，利用热熔纱实现可编程刚度和柔性，为可穿戴软机器人提供了结构可重构性。", "motivation": "软机器人应用于可穿戴设备时，需要兼顾舒适性和安全性，但同时实现结构完整性和柔顺性是一个重大挑战。", "method": "研究引入了一种通用设计方法，将折纸图案转化为针织设计，通过编程针脚和材料图案来控制折叠方向。通过选择性地加入热熔纱，在柔顺折痕周围创建刚性面板，以实现特定方向的折叠并抑制不必要的弯曲和屈曲。", "result": "实验量化了折叠力矩，表明针脚图案增强了折叠方向性。热熔纱（1）通过减少边缘卷曲保持了几何形状的一致性，（2）通过硬化面板防止了平面外变形。成功再现了Miura-ori、Yoshimura和Kresling等复杂的折纸图案，并展示了一个可移动的可穿戴针织Kaleidocycle机器人。", "conclusion": "结合结构可重构性、材料可编程性和制造可扩展性，针织折纸被认为是一个有前景的下一代可穿戴机器人平台。"}}
{"id": "2511.00879", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation.", "AI": {"tldr": "本文提出了一套新颖的评估工具，系统地评估大型语言模型（LLMs）在逐步推理过程中对知识的扎根程度，以揭示其潜在的推理缺陷。", "motivation": "尽管LLMs的逐步推理范式已被证明有效，但一个基本问题是如何验证LLM的推理是否准确地基于知识。现有方法可能难以有效识别LLM推理中知识缺失或误用的情况。", "method": "本文提出了一个包含三个关键组件的评估框架：(1) 主要知识集合（Principal Knowledge Collection），一个大规模的原子知识库；(2) 基于知识的评估指标，用于衡量模型在推理中召回和应用先验知识的能力；(3) 评估器LLM（evaluator LLM），一个轻量级模型，用于经济高效且可靠地计算这些指标。", "result": "该评估工具在识别LLMs推理中缺失或误用的知识元素方面表现出卓越的有效性，为揭示LLMs根本性推理缺陷提供了关键见解。此外，这些评估指标还可以集成到偏好优化中，展示了知识扎根评估的进一步应用。", "conclusion": "所提出的评估套件能够有效识别LLMs推理中的知识缺陷，为深入理解LLMs的推理能力提供了重要工具，并具有超越单纯评估的应用潜力，例如改进模型优化。"}}
{"id": "2511.01774", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.01774", "abs": "https://arxiv.org/abs/2511.01774", "authors": ["Alexander Schperberg", "Yusuke Tanaka", "Stefano Di Cairano", "Dennis Hong"], "title": "MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll", "comment": "23 pages, 20 figures. Collaborative work between the Robotics and\n  Mechanisms Laboratory (RoMeLa) and Mitsubishi Electric Research Laboratories\n  (MERL)", "summary": "This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot\n(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features\nfour limbs--two 6-DoF arms with two-finger grippers for manipulation and\nclimbing, and two 4-DoF legs for locomotion--enabling smooth transitions across\ndiverse terrains without reconfiguration. A hybrid control architecture\ncombines reinforcement learning-based locomotion with model-based predictive\nand admittance control enhanced for safety by a Reference Governor toward\ncompliant contact interactions. A high-level MIQCP planner autonomously selects\nlocomotion modes to balance stability and energy efficiency. Hardware\nexperiments demonstrate robust gait transitions, dynamic climbing, and\nfull-body load support via pinch grasp. Overall, MOBIUS demonstrates the\nimportance of tight integration between morphology, high-level planning, and\ncontrol to enable mobile loco-manipulation and grasping, substantially\nexpanding its interaction capabilities, workspace, and traversability.", "AI": {"tldr": "本文介绍了一种名为MOBIUS的多模态双足智能城市侦察机器人，它能行走、爬行、攀爬和滚动，通过形态、高层规划和控制的紧密集成，实现了多样化的移动操作和抓取能力。", "motivation": "研究动机是开发一种能够在无需重新配置的情况下，跨越不同地形，并具有扩展交互能力、工作空间和可通行性的机器人，以应对复杂的城市环境。", "method": "该机器人采用四肢设计（两个6自由度带夹持器的手臂和两个4自由度腿）。控制架构是混合式的，结合了基于强化学习的运动控制、基于模型的预测和导纳控制（通过Reference Governor增强安全性），以实现顺从的接触交互。高层规划器使用MIQCP自主选择运动模式，以平衡稳定性和能源效率。", "result": "硬件实验展示了鲁棒的步态转换、动态攀爬以及通过捏合抓取实现全身负载支撑。这些成果证明了形态、高层规划和控制之间紧密集成的重要性。", "conclusion": "研究得出结论，形态、高层规划和控制的紧密集成对于实现移动式运动操作和抓取至关重要，能够显著扩展机器人的交互能力、工作空间和可通行性。"}}
{"id": "2511.00293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00293", "abs": "https://arxiv.org/abs/2511.00293", "authors": ["Hengjia Li", "Jianjin Xu", "Keli Cheng", "Lei Wang", "Ning Bi", "Boxi Wu", "Fernando De la Torre", "Deng Cai"], "title": "Multi-View Consistent Human Image Customization via In-Context Learning", "comment": null, "summary": "Recent advances in personalized generative models demonstrate impressive\nresults in creating identity-consistent images of the same person under diverse\nsettings. Yet, we note that most methods cannot control the viewpoint of the\ngenerated image, nor generate consistent multiple views of the person. To\naddress this problem, we propose a lightweight adaptation method, PersonalView,\ncapable of enabling an existing model to acquire multi-view generation\ncapability with as few as 100 training samples. PersonalView consists of two\nkey components: First, we design a conditioning architecture to take advantage\nof the in-context learning ability of the pre-trained diffusion transformer.\nSecond, we preserve the original generative ability of the pretrained model\nwith a new Semantic Correspondence Alignment Loss. We evaluate the multi-view\nconsistency, text alignment, identity similarity, and visual quality of\nPersonalView and compare it to recent baselines with potential capability of\nmulti-view customization. PersonalView significantly outperforms baselines\ntrained on a large corpus of multi-view data with only 100 training samples.", "AI": {"tldr": "本文提出PersonalView，一种轻量级适配方法，仅需100个训练样本即可使现有生成模型获得多视角生成能力。", "motivation": "现有的个性化生成模型虽然能生成身份一致的图像，但大多无法控制生成图像的视角，也无法生成一致的多视角人物图像。", "method": "PersonalView包含两个核心组件：1. 设计了一种条件架构，以利用预训练扩散Transformer的上下文学习能力。2. 引入了一种新的语义对应对齐损失（Semantic Correspondence Alignment Loss），以保留预训练模型的原始生成能力。", "result": "PersonalView在多视角一致性、文本对齐、身份相似性和视觉质量方面表现出色，仅用100个训练样本就显著优于那些使用大量多视角数据训练的基线模型。", "conclusion": "PersonalView能以极少的训练样本（100个）有效地赋予现有生成模型强大的多视角生成能力，并且性能超越了需要大量数据的基线方法。"}}
{"id": "2511.00903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00903", "abs": "https://arxiv.org/abs/2511.00903", "authors": ["Ahmed Masry", "Megh Thakkar", "Patrice Bechard", "Sathwik Tejaswi Madhusudhan", "Rabiul Awal", "Shambhavi Mishra", "Akshay Kalkunte Suresh", "Srivatsava Daruru", "Enamul Hoque", "Spandana Gella", "Torsten Scholak", "Sai Rajeswar"], "title": "ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval", "comment": null, "summary": "Retrieval-augmented generation has proven practical when models require\nspecialized knowledge or access to the latest data. However, existing methods\nfor multimodal document retrieval often replicate techniques developed for\ntext-only retrieval, whether in how they encode documents, define training\nobjectives, or compute similarity scores. To address these limitations, we\npresent ColMate, a document retrieval model that bridges the gap between\nmultimodal representation learning and document retrieval. ColMate utilizes a\nnovel OCR-based pretraining objective, a self-supervised masked contrastive\nlearning objective, and a late interaction scoring mechanism more relevant to\nmultimodal document structures and visual characteristics. ColMate obtains\n3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,\ndemonstrating stronger generalization to out-of-domain benchmarks.", "AI": {"tldr": "ColMate是一个多模态文档检索模型，通过创新的OCR预训练、自监督掩码对比学习和后期交互评分机制，解决了现有方法模仿文本检索的局限性，并在ViDoRe V2基准上取得了显著提升和更强的泛化能力。", "motivation": "当模型需要专业知识或最新数据时，检索增强生成（RAG）非常实用。然而，现有的多模态文档检索方法常常复制为纯文本检索开发的技术，无论是在文档编码、训练目标还是相似度计算方面，这限制了它们处理多模态文档结构和视觉特征的能力。", "method": "ColMate采用以下方法：1. 一种新颖的基于OCR的预训练目标。2. 一种自监督的掩码对比学习目标。3. 一种更适合多模态文档结构和视觉特征的后期交互评分机制。这些方法旨在弥合多模态表示学习与文档检索之间的鸿沟。", "result": "ColMate在ViDoRe V2基准测试上比现有检索模型提高了3.61%，并展示了对域外基准更强的泛化能力。", "conclusion": "ColMate通过引入与多模态文档结构和视觉特征更相关的编码、训练和评分机制，成功地弥合了多模态表示学习和文档检索之间的差距，取得了显著的性能提升和更好的泛化能力。"}}
{"id": "2511.01059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01059", "abs": "https://arxiv.org/abs/2511.01059", "authors": ["Hailong Yin", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo"], "title": "Efficient Test-Time Retrieval Augmented Generation", "comment": null, "summary": "Although Large Language Models (LLMs) demonstrate significant capabilities,\ntheir reliance on parametric knowledge often leads to inaccuracies. Retrieval\nAugmented Generation (RAG) mitigates this by incorporating external knowledge,\nbut these methods may introduce irrelevant retrieved documents, leading to\ninaccurate responses. While the integration methods filter out incorrect\nanswers from multiple responses, but lack external knowledge like RAG methods,\nand their high costs require balancing overhead with performance gains. To\naddress these issues, we propose an Efficient Test-Time Retrieval-Augmented\nGeneration Framework named ET2RAG to improve the performance of LLMs while\nmaintaining efficiency. Specifically, ET2RAG is a training-free method, that\nfirst retrieves the most relevant documents and augments the LLMs to\nefficiently generate diverse candidate responses by managing response length.\nThen we compute the similarity of candidate responses and employ a majority\nvoting mechanism to select the most suitable response as the final output. In\nparticular, we discover that partial generation is sufficient to capture the\nkey information necessary for consensus calculation, allowing us to effectively\nperform majority voting without the need for fully generated responses. Thus,\nwe can reach a balance between computational cost and performance by managing\nthe response length for the number of retrieved documents for majority voting.\nExperimental results demonstrate that ET2RAG significantly enhances performance\nacross three tasks, including open-domain question answering, recipe generation\nand image captioning.", "AI": {"tldr": "本文提出了ET2RAG，一个高效的测试时检索增强生成框架，通过检索相关文档、高效生成多样候选响应（通过管理响应长度）和多数投票机制，显著提升LLMs性能并保持效率。", "motivation": "大型语言模型（LLMs）的参数化知识常导致不准确；检索增强生成（RAG）方法可能引入不相关文档；现有集成方法缺乏外部知识且成本高昂，需要在开销与性能之间取得平衡。", "method": "ET2RAG是一个免训练框架，首先检索最相关文档，然后通过管理响应长度高效生成多样候选响应。接着计算候选响应的相似度，并采用多数投票机制选择最合适的最终响应。核心发现是部分生成足以捕获关键信息以进行共识计算，从而通过管理响应长度来平衡计算成本和性能。", "result": "实验结果表明，ET2RAG在开放域问答、食谱生成和图像字幕等三个任务上显著提升了性能。", "conclusion": "ET2RAG通过结合高效的测试时检索增强和创新的多数投票机制（利用部分生成），成功地在提升LLMs性能的同时保持了计算效率，实现了计算成本与性能的良好平衡。"}}
{"id": "2511.00328", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00328", "abs": "https://arxiv.org/abs/2511.00328", "authors": ["Isai Daniel Chacón", "Paola Ruiz Puentes", "Jillian Pearse", "Pablo Arbeláez"], "title": "Towards Automated Petrography", "comment": null, "summary": "Petrography is a branch of geology that analyzes the mineralogical\ncomposition of rocks from microscopical thin section samples. It is essential\nfor understanding rock properties across geology, archaeology, engineering,\nmineral exploration, and the oil industry. However, petrography is a\nlabor-intensive task requiring experts to conduct detailed visual examinations\nof thin section samples through optical polarization microscopes, thus\nhampering scalability and highlighting the need for automated techniques. To\naddress this challenge, we introduce the Large-scale Imaging and Thin section\nOptical-polarization Set (LITHOS), the largest and most diverse publicly\navailable experimental framework for automated petrography. LITHOS includes\n211,604 high-resolution RGB patches of polarized light and 105,802\nexpert-annotated grains across 25 mineral categories. Each annotation consists\nof the mineral class, spatial coordinates, and expert-defined major and minor\naxes represented as intersecting vector paths, capturing grain geometry and\norientation. We evaluate multiple deep learning techniques for mineral\nclassification in LITHOS and propose a dual-encoder transformer architecture\nthat integrates both polarization modalities as a strong baseline for future\nreference. Our method consistently outperforms single-polarization models,\ndemonstrating the value of polarization synergy in mineral classification. We\nhave made the LITHOS Benchmark publicly available, comprising our dataset,\ncode, and pretrained models, to foster reproducibility and further research in\nautomated petrographic analysis.", "AI": {"tldr": "本文提出了LITHOS，一个大规模、多样化的公开数据集和实验框架，用于自动化岩相学分析中的矿物分类。同时，提出了一种双编码器Transformer架构，通过整合两种偏振模式，显著提高了矿物分类的准确性。", "motivation": "岩相学分析是地质学、考古学、工程、矿物勘探和石油工业中理解岩石性质的关键，但其高度依赖专家进行显微镜下的视觉检查，耗时且难以规模化，因此需要自动化技术。", "method": "研究引入了LITHOS数据集，包含211,604张高分辨率偏振光RGB图像补丁和105,802个专家标注的矿物颗粒（涵盖25种矿物类别，包括矿物类别、空间坐标及几何/方向信息）。研究评估了多种深度学习技术，并提出了一种双编码器Transformer架构，该架构整合了两种偏振模式，作为未来研究的基线。", "result": "所提出的双编码器Transformer架构在矿物分类任务中持续优于单偏振模型，证明了偏振协同作用在矿物分类中的价值。LITHOS基准（包括数据集、代码和预训练模型）已公开可用。", "conclusion": "LITHOS是目前最大、最多样化的公开自动化岩相学实验框架，其提出的双编码器Transformer模型有效利用了偏振协同效应，显著提升了矿物分类性能。公开LITHOS基准旨在促进自动化岩相分析领域的可复现性和进一步研究。"}}
{"id": "2511.00335", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00335", "abs": "https://arxiv.org/abs/2511.00335", "authors": ["Weidong Zhang", "Pak Lun Kevin Ding", "Huan Liu"], "title": "Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models", "comment": "10 pages, 5 tables, 1 figure, 3 equations, 11 mobile models, 7\n  datasets", "summary": "Lightweight vision classification models such as MobileNet, ShuffleNet, and\nEfficientNet are increasingly deployed in mobile and embedded systems, yet\ntheir performance has been predominantly benchmarked on ImageNet. This raises\ncritical questions: Do models that excel on ImageNet also generalize across\nother domains? How can cross-dataset robustness be systematically quantified?\nAnd which architectural elements consistently drive generalization under tight\nresource constraints? Here, we present the first systematic evaluation of 11\nlightweight vision models (2.5M parameters), trained under a fixed 100-epoch\nschedule across 7 diverse datasets. We introduce the Cross-Dataset Score\n(xScore), a unified metric that quantifies the consistency and robustness of\nmodel performance across diverse visual domains. Our results show that (1)\nImageNet accuracy does not reliably predict performance on fine-grained or\nmedical datasets, (2) xScore provides a scalable predictor of mobile model\nperformance that can be estimated from just four datasets, and (3) certain\narchitectural components--such as isotropic convolutions with higher spatial\nresolution and channel-wise attention--promote broader generalization, while\nTransformer-based blocks yield little additional benefit, despite incurring\nhigher parameter overhead. This study provides a reproducible framework for\nevaluating lightweight vision models beyond ImageNet, highlights key design\nprinciples for mobile-friendly architectures, and guides the development of\nfuture models that generalize robustly across diverse application domains.", "AI": {"tldr": "该研究系统评估了11种轻量级视觉模型在7个不同数据集上的表现，引入了跨数据集分数（xScore）来量化模型泛化能力，发现ImageNet准确性并非总能预测其他领域性能，并指出了促进泛化的关键架构元素。", "motivation": "轻量级视觉模型广泛部署于移动和嵌入式系统，但其性能主要在ImageNet上进行基准测试。研究旨在回答：在ImageNet上表现优异的模型是否能泛化到其他领域？如何系统量化跨数据集的鲁棒性？以及在资源受限下，哪些架构元素能持续驱动泛化？", "method": "研究对11种轻量级视觉模型（参数少于2.5M）进行了首次系统评估，在7个不同数据集上以固定的100个epoch进行训练。引入了跨数据集分数（xScore）作为统一指标，用于量化模型在不同视觉领域性能的一致性和鲁棒性。", "result": "(1) ImageNet准确性无法可靠预测模型在细粒度或医学数据集上的性能。(2) xScore能提供可扩展的移动模型性能预测，只需四个数据集即可估算。(3) 某些架构组件，如具有更高空间分辨率的各向同性卷积和通道注意力，能促进更广泛的泛化；而基于Transformer的模块尽管参数开销更高，却几乎没有带来额外的好处。", "conclusion": "本研究提供了一个可复现的框架，用于评估ImageNet之外的轻量级视觉模型，突出了适用于移动设备的架构关键设计原则，并指导未来模型开发以实现在不同应用领域间的鲁棒泛化。"}}
{"id": "2511.01256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01256", "abs": "https://arxiv.org/abs/2511.01256", "authors": ["Yasamin Foroutani", "Yasamin Mousavi-Motlagh", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control", "comment": "10 pages, 10 figures", "summary": "Achieving precise control of robotic tool paths is often challenged by\ninherent system misalignments, unmodeled dynamics, and actuation inaccuracies.\nThis work introduces an Iterative Learning Control (ILC) strategy to enable\nprecise rotational insertion of a tool during robotic surgery, improving\npenetration efficacy and safety compared to straight insertion tested in\nsubretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,\nwhere misalignment of the fourth joint complicates the simple application of\nneedle rotation, motivating an ILC approach that iteratively adjusts joint\ncommands based on positional feedback. The process begins with calibrating the\nforward kinematics for the chosen surgical tool to achieve higher accuracy,\nfollowed by successive ILC iterations guided by Optical Coherence Tomography\n(OCT) volume scans to measure the error and refine control inputs. Experimental\nresults, tested on subretinal injection tasks on ex vivo pig eyes, show that\nthe optimized trajectory resulted in higher success rates in tissue penetration\nand subretinal injection compared to straight insertion, demonstrating the\neffectiveness of ILC in overcoming misalignment challenges. This approach\noffers potential applications for other high precision robot tasks requiring\ncontrolled insertions as well.", "AI": {"tldr": "该研究提出一种迭代学习控制(ILC)策略，用于在机器人手术中实现精确的旋转插入，通过克服系统误差提高穿透效率和安全性。", "motivation": "机器人工具路径的精确控制常受系统固有未对准、未建模动力学和执行器不精确的挑战；特别是机器人第四关节的未对准使简单的针头旋转复杂化，促使研究者寻求一种迭代学习控制方法。", "method": "采用迭代学习控制(ILC)策略。首先校准所选手术工具的正向运动学以提高精度，然后通过光学相干断层扫描(OCT)测量误差，并根据位置反馈迭代调整关节指令以优化控制输入。", "result": "在离体猪眼上进行的视网膜下注射任务实验表明，优化后的轨迹比直线插入具有更高的组织穿透和视网膜下注射成功率。", "conclusion": "该研究证明了ILC在克服系统未对准挑战方面的有效性，并为其他需要精确控制插入的高精度机器人任务提供了潜在应用。"}}
{"id": "2511.01170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01170", "abs": "https://arxiv.org/abs/2511.01170", "authors": ["Ruofan Zhang", "Bin Xia", "Zhen Cheng", "Cairen Jian", "Minglun Yang", "Ngai Wong", "Yuan Cheng"], "title": "DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models", "comment": null, "summary": "Adaptive reasoning is essential for aligning the computational effort of\nlarge language models (LLMs) with the intrinsic difficulty of problems. Current\nchain-of-thought methods boost reasoning ability but indiscriminately generate\nlong explanations, leading to evident inefficiency. However, existing\nreinforcement learning approaches to adaptive thinking remain unstable and\nheavily reward-dependent. Here we propose \\textbf{DART}, a supervised\n\\textbf{D}ifficulty-\\textbf{A}daptive \\textbf{R}easoning \\textbf{T}runcation\nframework that adjusts thinking length according to problem difficulty. By\ndistilling concise reasoning patterns from stronger models, interpolating them\ninto a continuum of reasoning styles, and curating optimal training data that\nbalances correctness and compactness, DART learns when to ``stop thinking''.\nAcross multiple mathematical benchmarks, experimental results demonstrate its\nremarkable efficiency while preserving or improving accuracy, achieving a\nsignificant 81.2\\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K\ndataset) with 5.33$\\times$ computational acceleration. DART provides a stable\nand general paradigm for efficient reasoning, advancing the development of\nadaptive intelligence in LLMs.", "AI": {"tldr": "DART是一个监督式难度自适应推理截断框架，通过学习何时停止思考，显著提高了大语言模型在数学基准测试中的推理效率和计算速度，同时保持或提升了准确性。", "motivation": "当前链式思考（CoT）方法会无差别地生成冗长解释，导致效率低下。现有的强化学习自适应思维方法不稳定且高度依赖奖励。研究旨在解决LLM计算量与问题难度不匹配的问题，实现自适应推理。", "method": "DART（Difficulty-Adaptive Reasoning Truncation）是一个监督学习框架。它从更强的模型中提炼简洁的推理模式，将它们内插成连续的推理风格，并精心策划平衡正确性和紧凑性的最优训练数据，从而学习何时“停止思考”。", "result": "在多个数学基准测试中，DART显著提高了效率，同时保持或提升了准确性。例如，在GSM8K数据集上，DeepSeek-R1-Distill-Qwen-7B模型实现了81.2%的推理截断和5.33倍的计算加速。", "conclusion": "DART为高效推理提供了一个稳定且通用的范式，推动了LLM自适应智能的发展。"}}
{"id": "2511.00338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00338", "abs": "https://arxiv.org/abs/2511.00338", "authors": ["Yuhao Fang", "Zijian Wang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction", "comment": null, "summary": "This work presents a novel hybrid approach that integrates Deep Operator\nNetworks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex\ninverse problem. The method effectively addresses tasks such as source\nlocalization governed by the Navier-Stokes equations and image reconstruction,\novercoming challenges related to nonlinearity, sparsity, and noisy data. By\nincorporating physics-informed constraints and task-specific regularization\ninto the loss function, the framework ensures solutions that are both\nphysically consistent and accurate. Validation on diverse synthetic and real\ndatasets demonstrates its robustness, scalability, and precision, showcasing\nits broad potential applications in computational physics and imaging sciences.", "AI": {"tldr": "本文提出了一种结合DeepONet和NTK的新型混合方法，用于解决复杂的逆问题，尤其擅长处理非线性、稀疏和噪声数据。", "motivation": "解决复杂逆问题（如由Navier-Stokes方程控制的源定位和图像重建）面临非线性、稀疏性和数据噪声等挑战。", "method": "该方法将深度算子网络（DeepONet）与神经正切核（NTK）相结合，并在损失函数中融入物理信息约束和任务特定正则化，以确保解的物理一致性和准确性。", "result": "该方法有效解决了源定位和图像重建任务，并在各种合成和真实数据集上验证了其鲁棒性、可扩展性和精确性。", "conclusion": "该混合方法在计算物理和成像科学中具有广泛的应用潜力，能够有效处理复杂的逆问题。"}}
{"id": "2511.00924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00924", "abs": "https://arxiv.org/abs/2511.00924", "authors": ["Jianzhou Yao", "Shunchang Liu", "Guillaume Drui", "Rikard Pettersson", "Alessandro Blasimme", "Sara Kijewski"], "title": "The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) show promise for supporting clinicians in\ndiagnostic communication by generating explanations and guidance for patients.\nYet their ability to produce outputs that are both understandable and\nempathetic remains uncertain. We evaluate two leading LLMs on medical\ndiagnostic scenarios, assessing understandability using readability metrics as\na proxy and empathy through LLM-as-a-Judge ratings compared to human\nevaluations. The results indicate that LLMs adapt explanations to\nsocio-demographic variables and patient conditions. However, they also generate\noverly complex content and display biased affective empathy, leading to uneven\naccessibility and support. These patterns underscore the need for systematic\ncalibration to ensure equitable patient communication. The code and data are\nreleased: https://github.com/Jeffateth/Biased_Oracle", "AI": {"tldr": "本研究评估了大型语言模型(LLMs)在医疗诊断沟通中生成可理解和富有同情心的解释的能力。结果显示LLMs能适应患者情况但内容过于复杂且存在情感共情偏差，导致可及性不均，亟需系统校准。", "motivation": "大型语言模型有望通过生成解释和指导来支持临床医生进行诊断沟通，但其输出内容的可理解性和同情心水平仍不确定。", "method": "研究评估了两个领先的LLM在医疗诊断场景中的表现。通过可读性指标（作为替代）评估可理解性，并通过“LLM作为评判者”的评分与人类评估进行比较来评估同情心。", "result": "结果表明，LLM能根据社会人口统计变量和患者状况调整解释。然而，它们也生成了过于复杂的内容，并表现出有偏见的情感共情，导致了不均衡的可及性和支持。", "conclusion": "这些模式强调了需要系统校准LLM，以确保公平的患者沟通。"}}
{"id": "2511.01276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01276", "abs": "https://arxiv.org/abs/2511.01276", "authors": ["Yiyao Ma", "Kai Chen", "Kexin Zheng", "Qi Dou"], "title": "Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation", "comment": null, "summary": "Dexterous grasp generation is a fundamental challenge in robotics, requiring\nboth grasp stability and adaptability across diverse objects and tasks.\nAnalytical methods ensure stable grasps but are inefficient and lack task\nadaptability, while generative approaches improve efficiency and task\nintegration but generalize poorly to unseen objects and tasks due to data\nlimitations. In this paper, we propose a transfer-based framework for dexterous\ngrasp generation, leveraging a conditional diffusion model to transfer\nhigh-quality grasps from shape templates to novel objects within the same\ncategory. Specifically, we reformulate the grasp transfer problem as the\ngeneration of an object contact map, incorporating object shape similarity and\ntask specifications into the diffusion process. To handle complex shape\nvariations, we introduce a dual mapping mechanism, capturing intricate\ngeometric relationship between shape templates and novel objects. Beyond the\ncontact map, we derive two additional object-centric maps, the part map and\ndirection map, to encode finer contact details for more stable grasps. We then\ndevelop a cascaded conditional diffusion model framework to jointly transfer\nthese three maps, ensuring their intra-consistency. Finally, we introduce a\nrobust grasp recovery mechanism, identifying reliable contact points and\noptimizing grasp configurations efficiently. Extensive experiments demonstrate\nthe superiority of our proposed method. Our approach effectively balances grasp\nquality, generation efficiency, and generalization performance across various\ntasks. Project homepage: https://cmtdiffusion.github.io/", "AI": {"tldr": "本文提出了一种基于迁移的灵巧抓取生成框架，利用条件扩散模型将高质量抓取从形状模板迁移到同类别新物体上，有效平衡了抓取质量、生成效率和泛化性能。", "motivation": "现有的灵巧抓取生成方法存在局限性：分析方法虽稳定但效率低且缺乏任务适应性；生成方法效率高且易于任务集成，但因数据限制而对未见物体和任务泛化能力差。", "method": "该研究提出一个基于条件扩散模型的迁移框架：将抓取迁移问题重新定义为物体接触图生成，并融入物体形状相似性和任务规范；引入双重映射机制处理复杂形状变化；除了接触图，还推导了部位图和方向图以编码更精细的接触细节；开发了级联条件扩散模型框架来联合迁移这三张图，确保其内部一致性；最后，引入鲁棒的抓取恢复机制以识别可靠接触点并优化抓取配置。", "result": "广泛的实验证明了该方法的优越性，其方法有效地平衡了抓取质量、生成效率和跨各种任务的泛化性能。", "conclusion": "该研究成功地提出了一种新颖的灵巧抓取生成方法，通过结合迁移学习和条件扩散模型，克服了传统方法的局限性，在抓取质量、效率和泛化能力之间取得了有效平衡。"}}
{"id": "2511.01182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01182", "abs": "https://arxiv.org/abs/2511.01182", "authors": ["Cuong Van Duc", "Thai Tran Quoc", "Minh Nguyen Dinh Tuan", "Tam Vu Duc", "Son Nguyen Van", "Hanh Nguyen Thi"], "title": "MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion", "comment": null, "summary": "Detecting student misconceptions in open-ended responses is a longstanding\nchallenge, demanding semantic precision and logical reasoning. We propose\nMiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning\nand Ensemble Fusion, a novel framework for automated misconception detection in\nmathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a\nlarge candidate pool to a semantically relevant subset; (2) a Reasoning module\nemploys chain-of-thought generation to expose logical inconsistencies in\nstudent solutions; and (3) a Reranking module refines predictions by aligning\nthem with the reasoning. These components are unified through an\nensemble-fusion strategy that enhances robustness and interpretability. On\nmathematics datasets, MiRAGE achieves Mean Average Precision scores of\n0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.\nBy coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces\ndependence on large-scale language models while delivering a scalable and\neffective solution for educational assessment.", "AI": {"tldr": "MiRAGE是一个用于数学开放式回答中自动检测学生错误概念的框架，它结合了检索引导、多阶段推理和集成融合。", "motivation": "在开放式回答中检测学生的错误概念是一个长期存在的挑战，需要语义精确性和逻辑推理能力。", "method": "MiRAGE框架分三个阶段：1) 检索模块缩小候选池；2) 推理模块通过思维链生成揭示逻辑不一致性；3) 重排模块根据推理结果细化预测。这些组件通过集成融合策略统一起来。", "result": "在数学数据集上，MiRAGE在1/3/5级别分别达到0.82/0.92/0.93的平均精度（MAP）分数，持续优于单个模块。", "conclusion": "MiRAGE通过结合检索引导和多阶段推理，降低了对大型语言模型的依赖，为教育评估提供了一个可扩展且有效的解决方案。"}}
{"id": "2511.01294", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01294", "abs": "https://arxiv.org/abs/2511.01294", "authors": ["Jiawei Wang", "Dingyou Wang", "Jiaming Hu", "Qixuan Zhang", "Jingyi Yu", "Lan Xu"], "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects", "comment": null, "summary": "A deep understanding of kinematic structures and movable components is\nessential for enabling robots to manipulate objects and model their own\narticulated forms. Such understanding is captured through articulated objects,\nwhich are essential for tasks such as physical simulation, motion planning, and\npolicy learning. However, creating these models, particularly for complex\nsystems like robots or objects with high degrees of freedom (DoF), remains a\nsignificant challenge. Existing methods typically rely on motion sequences or\nstrong assumptions from hand-curated datasets, which hinders scalability. In\nthis paper, we introduce Kinematify, an automated framework that synthesizes\narticulated objects directly from arbitrary RGB images or text prompts. Our\nmethod addresses two core challenges: (i) inferring kinematic topologies for\nhigh-DoF objects and (ii) estimating joint parameters from static geometry. To\nachieve this, we combine MCTS search for structural inference with\ngeometry-driven optimization for joint reasoning, producing physically\nconsistent and functionally valid descriptions. We evaluate Kinematify on\ndiverse inputs from both synthetic and real-world environments, demonstrating\nimprovements in registration and kinematic topology accuracy over prior work.", "AI": {"tldr": "本文提出了Kinematify，一个自动化框架，能直接从任意RGB图像或文本提示中合成可动对象，解决了高自由度对象运动学拓扑推断和关节参数估计的挑战。", "motivation": "理解运动学结构和可动部件对于机器人操作和建模至关重要，但为复杂系统（如高自由度机器人或物体）创建可动对象模型极具挑战性。现有方法通常依赖运动序列或手工数据集的强假设，限制了可扩展性。", "method": "Kinematify框架结合了蒙特卡洛树搜索（MCTS）进行结构推断，并利用几何驱动优化进行关节推理，以从静态几何中估计关节参数，从而生成物理上一致且功能有效的描述。", "result": "Kinematify在合成和真实世界环境中的多样化输入上进行了评估，结果表明其在配准和运动学拓扑精度方面优于现有工作，能够生成物理上一致且功能有效的描述。", "conclusion": "Kinematify提供了一个自动化的框架，能够直接从图像或文本提示中合成可动对象，有效解决了高自由度对象的运动学拓扑推断和关节参数估计的难题，提高了模型创建的准确性和可扩展性。"}}
{"id": "2511.00344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00344", "abs": "https://arxiv.org/abs/2511.00344", "authors": ["Xihang Qiu", "Jiarong Cheng", "Yuhao Fang", "Wanpeng Zhang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities", "comment": null, "summary": "Multimodal Emotion Recognition in Conversations (MERC) enhances emotional\nunderstanding through the fusion of multimodal signals. However, unpredictable\nmodality absence in real-world scenarios significantly degrades the performance\nof existing methods. Conventional missing-modality recovery approaches, which\ndepend on training with complete multimodal data, often suffer from semantic\ndistortion under extreme data distributions, such as fixed-modality absence. To\naddress this, we propose the Federated Dialogue-guided and Semantic-Consistent\nDiffusion (FedDISC) framework, pioneering the integration of federated learning\ninto missing-modality recovery. By federated aggregation of modality-specific\ndiffusion models trained on clients and broadcasting them to clients missing\ncorresponding modalities, FedDISC overcomes single-client reliance on modality\ncompleteness. Additionally, the DISC-Diffusion module ensures consistency in\ncontext, speaker identity, and semantics between recovered and available\nmodalities, using a Dialogue Graph Network to capture conversational\ndependencies and a Semantic Conditioning Network to enforce semantic alignment.\nWe further introduce a novel Alternating Frozen Aggregation strategy, which\ncyclically freezes recovery and classifier modules to facilitate collaborative\noptimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI\ndatasets demonstrate that FedDISC achieves superior emotion classification\nperformance across diverse missing modality patterns, outperforming existing\napproaches.", "AI": {"tldr": "针对会话中多模态情感识别（MERC）中模态缺失问题，FedDISC框架创新性地结合联邦学习和语义一致性扩散模型，通过联邦聚合和对话引导的语义对齐，有效恢复缺失模态并提升情感分类性能。", "motivation": "现有MERC方法在实际场景中遇到不可预测的模态缺失时性能会显著下降。传统的模态恢复方法依赖于完整多模态数据训练，在极端数据分布（如固定模态缺失）下容易导致语义失真。", "method": "该研究提出了Federated Dialogue-guided and Semantic-Consistent Diffusion (FedDISC) 框架。它通过联邦聚合在客户端训练的模态特定扩散模型，并将其广播给缺失相应模态的客户端，从而克服单客户端对模态完整性的依赖。FedDISC包含一个DISC-Diffusion模块，该模块利用对话图网络捕捉会话依赖性以确保上下文和说话者身份一致性，并使用语义条件网络强制恢复模态与可用模态之间的语义对齐。此外，还引入了一种新颖的交替冻结聚合策略，循环冻结恢复和分类模块以促进协同优化。", "result": "在IEMOCAP、CMUMOSI和CMUMOSEI数据集上进行的大量实验表明，FedDISC在各种缺失模态模式下均实现了卓越的情感分类性能，优于现有方法。", "conclusion": "FedDISC框架通过创新性地将联邦学习应用于模态缺失恢复，并结合对话引导和语义一致性扩散模型，有效解决了多模态情感识别中的模态缺失问题，显著提升了在复杂真实场景下的情感分类准确性。"}}
{"id": "2511.01183", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01183", "abs": "https://arxiv.org/abs/2511.01183", "authors": ["Hainan Fang", "Yuanbo Wen", "Jun Bi", "Yihan Wang", "Tonghui He", "Yanlin Tang", "Di Huang", "Jiaming Guo", "Rui Zhang", "Qi Guo", "Yunji Chen"], "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code", "comment": "Accepted at NeurIPS 2025", "summary": "Compilers, while essential, are notoriously complex systems that demand\nprohibitively expensive human expertise to develop and maintain. The recent\nadvancements in Large Language Models (LLMs) offer a compelling new paradigm:\nNeural Compilation, which could potentially simplify compiler development for\nnew architectures and facilitate the discovery of innovative optimization\ntechniques. However, several critical obstacles impede its practical adoption.\nFirstly, a significant lack of dedicated benchmarks and robust evaluation\nmethodologies hinders objective assessment and tracking of progress in the\nfield. Secondly, systematically enhancing the reliability and performance of\nLLM-generated assembly remains a critical challenge. Addressing these\nchallenges, this paper introduces NeuComBack, a novel benchmark dataset\nspecifically designed for IR-to-assembly compilation. Leveraging this dataset,\nwe first define a foundational Neural Compilation workflow and conduct a\ncomprehensive evaluation of the capabilities of recent frontier LLMs on Neural\nCompilation, establishing new performance baselines. We further propose a\nself-evolving prompt optimization method that enables LLMs to iteratively\nevolve their internal prompt strategies by extracting insights from prior\nself-debugging traces, thereby enhancing their neural compilation capabilities.\nExperiments demonstrate that our method significantly improves both the\nfunctional correctness and the performance of LLM-generated assembly code.\nCompared to baseline prompts, the functional correctness rates improved from\n44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More\nsignificantly, among the 16 correctly generated x86_64 programs using our\nmethod, 14 (87.5%) surpassed clang-O3 performance.", "AI": {"tldr": "该论文提出了一个名为NeuComBack的新基准数据集和一种自进化的提示优化方法，旨在解决神经编译领域中LLM生成汇编代码的可靠性和性能问题，并显著提升了LLM生成汇编代码的正确性和性能。", "motivation": "传统的编译器开发和维护成本高昂，需要专业的领域知识。大型语言模型（LLMs）为编译器开发提供了一种新的范式——神经编译，有望简化新架构的编译器开发并发现新的优化技术。然而，该领域缺乏专门的基准和评估方法，且LLM生成汇编代码的可靠性和性能提升仍是重大挑战。", "method": "该研究引入了NeuComBack，一个专门用于IR到汇编编译的基准数据集。在此基础上，定义了一个基础的神经编译工作流程，并全面评估了前沿LLM在神经编译上的能力，建立了新的性能基线。此外，提出了一种自进化的提示优化方法，使LLM能够通过从先前的自调试轨迹中提取见解，迭代地演进其内部提示策略。", "result": "实验表明，所提出的方法显著提高了LLM生成汇编代码的功能正确性和性能。与基线提示相比，x86_64上的功能正确率从44%提高到64%，aarch64上从36%提高到58%。更重要的是，在通过该方法正确生成的16个x86_64程序中，有14个（87.5%）超越了clang-O3的性能。", "conclusion": "该论文通过引入NeuComBack基准和提出自进化的提示优化方法，有效解决了神经编译领域中LLM生成汇编代码的评估和可靠性问题。研究结果证明了LLM在编译器开发中的巨大潜力，并为未来神经编译技术的发展奠定了基础。"}}
{"id": "2511.01331", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01331", "abs": "https://arxiv.org/abs/2511.01331", "authors": ["Hongyin Zhang", "Shuo Zhang", "Junxi Jin", "Qixin Zeng", "Runze Li", "Donglin Wang"], "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as powerful\ngeneral-purpose policies for robotic manipulation, benefiting from large-scale\nmulti-modal pre-training. However, they often fail to generalize reliably in\nout-of-distribution deployments, where unavoidable disturbances such as\nobservation noise, sensor errors, or actuation perturbations become prevalent.\nWhile recent Reinforcement Learning (RL)-based post-training provides a\npractical means to adapt pre-trained VLA models, existing methods mainly\nemphasize reward maximization and overlook robustness to environmental\nuncertainty. In this work, we introduce RobustVLA, a lightweight online RL\npost-training method designed to explicitly enhance the resilience of VLA\nmodels. Through a systematic robustness analysis, we identify two key\nregularizations: Jacobian regularization, which mitigates sensitivity to\nobservation noise, and smoothness regularization, which stabilizes policies\nunder action perturbations. Extensive experiments across diverse robotic\nenvironments demonstrate that RobustVLA significantly outperforms prior\nstate-of-the-art methods in robustness and reliability. Our results highlight\nthe importance of principled robustness-aware RL post-training as a key step\ntoward improving the reliability and robustness of VLA models.", "AI": {"tldr": "本文提出RobustVLA，一种轻量级在线强化学习后训练方法，通过雅可比正则化和平滑正则化，显著提升了预训练VLA模型在机器人操作中对观测噪声和动作扰动的鲁棒性和可靠性。", "motivation": "VLA模型虽功能强大，但在实际部署中面对观测噪声、传感器误差或执行扰动等分布外干扰时泛化能力差。现有基于RL的后训练方法主要侧重奖励最大化，而忽略了环境不确定性下的鲁棒性。", "method": "引入RobustVLA，一种轻量级在线RL后训练方法，旨在明确增强VLA模型的弹性。通过系统性鲁棒性分析，识别并应用了两种关键正则化：1) 雅可比正则化，以减轻对观测噪声的敏感性；2) 平滑正则化，以稳定动作扰动下的策略。", "result": "在多样化的机器人环境中进行的广泛实验表明，RobustVLA在鲁棒性和可靠性方面显著优于现有最先进的方法。", "conclusion": "研究结果强调了以原则性、鲁棒性感知的方式进行RL后训练是提高VLA模型可靠性和鲁棒性的关键一步。"}}
{"id": "2511.01258", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01258", "abs": "https://arxiv.org/abs/2511.01258", "authors": ["Chuyue Lou", "M. Amine Atoui"], "title": "Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems", "comment": null, "summary": "Recently, fault diagnosis methods for marine machinery systems based on deep\nlearning models have attracted considerable attention in the shipping industry.\nMost existing studies assume fault classes are consistent and known between the\ntraining and test datasets, and these methods perform well under controlled\nenvironment. In practice, however, previously unseen or unknown fault types\n(i.e., out-of-distribution or open-set observations not present during\ntraining) can occur, causing such methods to fail and posing a significant\nchallenge to their widespread industrial deployment. To address this challenge,\nthis paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework\nthat enhances and extends the applicability of deep learning models in open-set\nfault diagnosis scenarios. The framework includes a reliability subset\nconstruction process, which uses a multi-layer fusion feature representation\nextracted by a supervised feature learning model to select an unlabeled test\nsubset. The labeled training set and pseudo-labeled test subset are then fed\ninto a semi-supervised diagnosis model to learn discriminative features for\neach class, enabling accurate classification of known faults and effective\ndetection of unknown samples. Experimental results on a public maritime\nbenchmark dataset demonstrate the effectiveness and superiority of the proposed\nSOFD framework.", "AI": {"tldr": "本文提出了一种半监督开放集故障诊断（SOFD）框架，用于解决海洋机械系统深度学习故障诊断中遇到的未知故障类型问题，实现已知故障的准确分类和未知样本的有效检测。", "motivation": "现有基于深度学习的海洋机械故障诊断方法假设训练和测试数据集中的故障类别一致且已知，但在实际应用中，未曾出现或未知的故障类型（即开放集观测）会发生，导致这些方法失效，严重阻碍了其广泛的工业部署。", "method": "本文提出了一个半监督开放集故障诊断（SOFD）框架。该框架包括一个可靠性子集构建过程，利用监督特征学习模型提取的多层融合特征表示来选择无标签测试子集。然后，将有标签训练集和伪标签测试子集输入半监督诊断模型，学习各类的判别性特征，从而实现已知故障的准确分类和未知样本的有效检测。", "result": "在公共海洋基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。", "conclusion": "SOFD框架增强并扩展了深度学习模型在开放集故障诊断场景中的适用性，能够有效应对未知故障类型，准确分类已知故障并检测未知样本，从而解决了现有方法的局限性。"}}
{"id": "2511.00345", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00345", "abs": "https://arxiv.org/abs/2511.00345", "authors": ["Amir Ziashahabi", "Narges Ghasemi", "Sajjad Shahabi", "John Krumm", "Salman Avestimehr", "Cyrus Shahabi"], "title": "OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data", "comment": "Accepted at NeurIPS 2025 UrbanAI Workshop", "summary": "Accurate and up-to-date geospatial data are essential for urban planning,\ninfrastructure monitoring, and environmental management. Yet, automating urban\nmonitoring remains difficult because curated datasets of specific urban\nfeatures and their changes are scarce. We introduce OSMGen, a generative\nframework that creates realistic satellite imagery directly from raw\nOpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen\nuses the full richness of OSM JSON, including vector geometries, semantic tags,\nlocation, and time, giving fine-grained control over how scenes are generated.\nA central feature of the framework is the ability to produce consistent\nbefore-after image pairs: user edits to OSM inputs translate into targeted\nvisual changes, while the rest of the scene is preserved. This makes it\npossible to generate training data that addresses scarcity and class imbalance,\nand to give planners a simple way to preview proposed interventions by editing\nmap data. More broadly, OSMGen produces paired (JSON, image) data for both\nstatic and changed states, paving the way toward a closed-loop system where\nsatellite imagery can automatically drive structured OSM updates. Source code\nis available at https://github.com/amir-zsh/OSMGen.", "AI": {"tldr": "OSMGen是一个生成框架，能直接从OpenStreetMap (OSM) JSON数据生成逼真的卫星图像，并能生成一致的“前后”图像对，以解决地理空间数据稀缺问题并支持城市规划和监测。", "motivation": "自动化城市监测面临挑战，因为用于特定城市特征及其变化的精选数据集稀缺。准确和最新的地理空间数据对于城市规划、基础设施监测和环境管理至关重要。", "method": "引入了OSMGen，一个生成框架，直接从原始OpenStreetMap (OSM) JSON数据（包括矢量几何、语义标签、位置和时间）创建逼真的卫星图像。与以往依赖栅格瓦片的工作不同，OSMGen利用了OSM JSON的全部丰富性。其核心特点是能够生成一致的“前后”图像对，用户对OSM输入的编辑会转化为有针对性的视觉变化，同时场景的其余部分保持不变。", "result": "OSMGen能够从OSM数据生成逼真的卫星图像，并提供对场景生成细粒度的控制。它能生成一致的“前后”图像对，使用户对OSM的编辑能转化为有针对性的视觉变化。这使得生成训练数据以解决数据稀缺和类别不平衡成为可能，并为规划者提供了一种通过编辑地图数据来预览拟议干预措施的简单方法。此外，它还为静态和变化状态生成配对的(JSON, 图像)数据。", "conclusion": "OSMGen通过生成训练数据解决了城市监测中的数据稀缺和类别不平衡问题，并为规划者提供了预览城市干预措施的工具。它为建立一个闭环系统奠定了基础，在该系统中，卫星图像可以自动驱动结构化的OSM更新。"}}
{"id": "2511.01008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01008", "abs": "https://arxiv.org/abs/2511.01008", "authors": ["Haolin Yang", "Jipeng Zhang", "Zhitao He", "Yi R. Fung"], "title": "MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL", "comment": null, "summary": "Translating natural language to SQL remains difficult for complex queries.\nSuch queries often need environmental interaction and self-correction. To\naddress this, we introduce MARS-SQL, a novel multi-agent framework that\ncombines principled task decomposition and interactive reinforcement learning\n(RL). Our system comprises three specialized agents: a Grounding Agent for\nschema linking, a Generation Agent for query generation, and a Validation Agent\nfor final selection. The core of our framework is the Generation agent, which\nis trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe\nloop, the agent iteratively generates thoughts, executes SQL actions against a\nlive database, and revises its strategy based on execution feedback, enabling\ndynamic, stateful reasoning and self-correction. At inference time, we generate\nmultiple interaction trajectories to explore diverse reasoning paths. The\nValidation agent, then selects the optimal trajectory by modeling verification\nas a next-token prediction task and choosing the solution with the highest\ngeneration probability. This structured workflow pipelines specialized agents.\nIt combines interactive RL for generation with generative modeling for\nverification. The approach proves highly effective for robust and accurate SQL\ngeneration. Experiments show that MARS-SQL achieves state-of-the-art Execution\nAccuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our\ncode is available at https://github.com/YangHaolin0526/MARS-SQL.", "AI": {"tldr": "MARS-SQL是一个新颖的多智能体框架，通过结合任务分解和交互式强化学习，解决了复杂自然语言到SQL转换的难题，实现了自修正和高精度。", "motivation": "将自然语言翻译成复杂SQL查询仍然很困难，这类查询通常需要环境交互和自修正能力。", "method": "引入了MARS-SQL，一个包含三个专门智能体的多智能体框架：用于模式链接的接地智能体、用于查询生成的生成智能体和用于最终选择的验证智能体。生成智能体通过多轮强化学习策略进行训练，采用ReAct风格的“思考-行动-观察”循环，迭代生成想法、执行SQL并根据反馈修正策略。在推理时，生成多个交互轨迹以探索不同的推理路径。验证智能体通过将验证建模为下一个token预测任务，选择生成概率最高的最佳解决方案。", "result": "MARS-SQL在BIRD开发集上达到了77.84%的执行准确率，在Spider测试集上达到了89.75%的执行准确率，均达到最先进水平。", "conclusion": "该方法结合了用于生成的交互式强化学习和用于验证的生成建模，被证明对于鲁棒和准确的SQL生成非常有效。"}}
{"id": "2511.01311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01311", "abs": "https://arxiv.org/abs/2511.01311", "authors": ["Filip Naudot", "Tobias Sundqvist", "Timotheus Kampik"], "title": "llmSHAP: A Principled Approach to LLM Explainability", "comment": null, "summary": "Feature attribution methods help make machine learning-based inference\nexplainable by determining how much one or several features have contributed to\na model's output. A particularly popular attribution method is based on the\nShapley value from cooperative game theory, a measure that guarantees the\nsatisfaction of several desirable principles, assuming deterministic inference.\nWe apply the Shapley value to feature attribution in large language model\n(LLM)-based decision support systems, where inference is, by design, stochastic\n(non-deterministic). We then demonstrate when we can and cannot guarantee\nShapley value principle satisfaction across different implementation variants\napplied to LLM-based decision support, and analyze how the stochastic nature of\nLLMs affects these guarantees. We also highlight trade-offs between explainable\ninference speed, agreement with exact Shapley value attributions, and principle\nattainment.", "AI": {"tldr": "本文探讨了将合作博弈论中的Shapley值应用于大型语言模型（LLM）的特征归因，分析了在LLM固有的随机推理下，Shapley值原则的满足情况及其与解释性推理速度和精确度的权衡。", "motivation": "特征归因方法（特别是基于Shapley值的方法）对于解释机器学习模型至关重要，但它们通常假设确定性推理。然而，LLM的推理本质上是随机的，这可能影响Shapley值原则的满足，因此需要研究如何在这种随机环境下应用和评估Shapley值。", "method": "研究方法包括将Shapley值应用于基于LLM的决策支持系统，其中推理是随机的。通过不同的实现变体，演示了Shapley值原则何时能够或不能得到满足，并分析了LLM的随机性如何影响这些保证。同时，还权衡了可解释推理速度、与精确Shapley值归因的一致性以及原则实现之间的关系。", "result": "研究结果表明，在LLM的随机推理环境下，Shapley值原则的满足存在特定的条件和限制。LLM的随机性会影响这些原则的保证。此外，研究还揭示了在解释性推理中，速度、与精确Shapley值归因的符合程度以及原则满足之间存在权衡。", "conclusion": "将Shapley值应用于随机LLM的特征归因需要仔细考虑，因为LLM的随机性会影响Shapley值原则的满足。在实际应用中，解释性推理的速度、与精确Shapley值的一致性以及原则的实现之间存在重要的权衡。"}}
{"id": "2511.00988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00988", "abs": "https://arxiv.org/abs/2511.00988", "authors": ["Chenwang Wu", "Yiu-ming Cheung", "Bo Han", "Defu Lian"], "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective", "comment": null, "summary": "Existing machine-generated text (MGT) detection methods implicitly assume\nlabels as the \"golden standard\". However, we reveal boundary ambiguity in MGT\ndetection, implying that traditional training paradigms are inexact. Moreover,\nlimitations of human cognition and the superintelligence of detectors make\ninexact learning widespread and inevitable. To this end, we propose an\neasy-to-hard enhancement framework to provide reliable supervision under such\ninexact conditions. Distinct from knowledge distillation, our framework employs\nan easy supervisor targeting relatively simple longer-text detection tasks\n(despite weaker capabilities), to enhance the more challenging target detector.\nFirstly, longer texts targeted by supervisors theoretically alleviate the\nimpact of inexact labels, laying the foundation for reliable supervision.\nSecondly, by structurally incorporating the detector into the supervisor, we\ntheoretically model the supervisor as a lower performance bound for the\ndetector. Thus, optimizing the supervisor indirectly optimizes the detector,\nultimately approximating the underlying \"golden\" labels. Extensive experiments\nacross diverse practical scenarios, including cross-LLM, cross-domain, mixed\ntext, and paraphrase attacks, demonstrate the framework's significant detection\neffectiveness. The code is available at:\nhttps://github.com/tmlr-group/Easy2Hard.", "AI": {"tldr": "本文提出了一种从易到难的增强框架，以解决机器生成文本（MGT）检测中标签边界模糊的问题，并在不精确监督下提供可靠的检测能力。", "motivation": "现有的MGT检测方法隐式地假设标签是“黄金标准”，但研究发现存在边界模糊，导致传统训练范式不精确。此外，人类认知的局限性和检测器的超智能使得不精确学习普遍且不可避免。", "method": "提出了一种从易到难的增强框架。该框架使用一个针对相对简单的长文本检测任务的“容易监督器”（能力较弱），来增强更具挑战性的目标检测器。通过理论分析，长文本有助于缓解不精确标签的影响，并且将目标检测器结构性地整合到监督器中，使得监督器成为检测器性能的下界，从而间接优化检测器以逼近“黄金”标签。", "result": "在跨大型语言模型、跨领域、混合文本和复述攻击等多种实际场景下进行了广泛实验，结果表明该框架显著提升了检测效果。", "conclusion": "该框架在不精确条件下提供了可靠的监督，通过从易到难的增强策略，显著提高了MGT的检测效率和准确性，解决了标签模糊带来的挑战。"}}
{"id": "2511.01346", "categories": ["cs.RO", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2511.01346", "abs": "https://arxiv.org/abs/2511.01346", "authors": ["Shun Yoshida", "Qingchuan Song", "Bastian E. Rapp", "Thomas Speck", "Falk J. Tauber"], "title": "Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers", "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Despite their often perceived static and slow nature, some plants can move\nfaster than the blink of an eye. The rapid snap closure motion of the Venus\nflytrap (Dionaea muscipula) has long captivated the interest of researchers and\nengineers alike, serving as a model for plant-inspired soft machines and\nrobots. The translation of the fast snapping closure has inspired the\ndevelopment of various artificial Venus flytrap (AVF) systems. However,\ntranslating both the closing and reopening motion of D. muscipula into an\nautonomous plant inspired soft machine has yet to be achieved. In this study,\nwe present an AVF that autonomously closes and reopens, utilizing novel\nthermo-responsive UV-curable shape memory materials for soft robotic systems.\nThe life-sized thermo-responsive AVF exhibits closing and reopening motions\ntriggered in a naturally occurring temperature range. The doubly curved trap\nlobes, built from shape memory polymers, close at 38{\\deg}C, while reopening\ninitiates around 45{\\deg}C, employing shape memory elastomer strips as\nantagonistic actuators to facilitate lobe reopening. This work represents the\nfirst demonstration of thermo-responsive closing and reopening in an AVF with\nprogrammed sequential motion in response to increasing temperature. This\napproach marks the next step toward autonomously bidirectional moving soft\nmachines/robots.", "AI": {"tldr": "该研究开发了一种新型人造捕蝇草（AVF），利用热响应形状记忆材料，首次实现了自主的闭合和重新打开双向运动。", "motivation": "捕蝇草的快速闭合运动激发了软机器人的发展。然而，现有的人造捕蝇草系统通常只能实现闭合，未能同时实现自主的闭合和重新打开运动，这限制了其在仿生软机器中的应用。", "method": "研究人员采用新型热响应紫外光固化形状记忆材料，构建了人造捕蝇草。其中，双曲面捕食叶片由形状记忆聚合物制成，而形状记忆弹性体条被用作拮抗执行器，以促进叶片的重新打开。", "result": "所开发的真人大小热响应人造捕蝇草能够在自然温度范围内自主响应：在38°C时闭合，并在45°C左右开始重新打开。这是首次展示了人造捕蝇草在温度升高时，通过编程顺序运动实现热响应的闭合和重新打开。", "conclusion": "这项工作代表了在实现自主双向运动软机器/机器人方面迈出了重要一步，为未来开发更复杂的仿生软机器人系统奠定了基础。"}}
{"id": "2511.01014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01014", "abs": "https://arxiv.org/abs/2511.01014", "authors": ["Bosi Wen", "Yilin Niu", "Cunxiang Wang", "Pei Ke", "Xiaoying Ling", "Ying Zhang", "Aohan Zeng", "Hongning Wang", "Minlie Huang"], "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation", "comment": "21 pages, 5 figures", "summary": "Instruction following is a fundamental ability of Large Language Models\n(LLMs), requiring their generated outputs to follow multiple constraints\nimposed in input instructions. Numerous studies have attempted to enhance this\nability through preference optimization or reinforcement learning based on\nreward signals from LLM-as-a-Judge. However, existing evaluation models for\ninstruction following still possess many deficiencies, such as substantial\ncosts and unreliable assessments. To this end, we propose IF-CRITIC, an LLM\ncritic that can provide efficient and reliable assessments of constraint\nfollowing in the instructions. We first develop a checklist generator to\ndecompose instructions and generate constraint checklists. With the assistance\nof the checklists, we collect high-quality critique training data through a\nmulti-stage critique filtering mechanism and employ a constraint-level\npreference optimization method to train IF-CRITIC. Extensive experiments\ndemonstrate that the evaluation performance of IF-CRITIC can beat strong\nLLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable\nreward signals provided by IF-CRITIC, LLMs can achieve substantial performance\ngains in instruction-following optimization under lower computational overhead\ncompared to strong LLM critic baselines.", "AI": {"tldr": "本文提出了IF-CRITIC，一个高效且可靠的LLM评论器，用于评估大型语言模型（LLMs）的指令遵循能力，解决了现有评估模型成本高昂和评估不可靠的问题。", "motivation": "指令遵循是LLMs的一项基本能力，但现有用于评估指令遵循的评估模型存在诸多缺陷，如成本高昂和评估结果不可靠。", "method": "本文提出了IF-CRITIC，它首先开发了一个清单生成器来分解指令并生成约束清单。然后，借助这些清单，通过多阶段评论过滤机制收集高质量的评论训练数据，并采用约束级偏好优化方法来训练IF-CRITIC。", "result": "广泛的实验表明，IF-CRITIC的评估性能优于强大的LLM-as-a-Judge基线（包括Deepseek-R1和o4-mini）。此外，IF-CRITIC提供的可扩展奖励信号使得LLMs在指令遵循优化方面能以较低的计算开销获得显著的性能提升。", "conclusion": "IF-CRITIC能够为LLMs的指令遵循提供高效、可靠的评估和可扩展的奖励信号，从而有效优化LLMs的指令遵循能力，且计算开销更低。"}}
{"id": "2511.01016", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01016", "abs": "https://arxiv.org/abs/2511.01016", "authors": ["Wenjin Liu", "Haoran Luo", "Xueyuan Lin", "Haoming Liu", "Tiesunlong Shen", "Jiapu Wang", "Rui Mao", "Erik Cambria"], "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning", "comment": null, "summary": "Recently, advanced large language models (LLMs) have emerged at an\nincreasingly rapid pace. However, when faced with complex problems, most users\nare often unable to provide accurate and effective prompts to interact with\nLLMs, thus limiting the performance of LLMs. To address this challenge, we\npropose Prompt-R1, an end-to-end reinforcement learning framework that uses a\nsmall-scale LLM to collaborate with large-scale LLMs, replacing user\ninteraction to solve problems better. This collaboration is cast as a\nmulti-turn prompt interaction, where the small-scale LLM thinks and generates\nprompts, and the large-scale LLM performs complex reasoning. A dual-constrained\nreward is designed to optimize for correctness, generation quality, and\nreasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports\nboth inference and training with various large-scale LLMs. Experiments on\nmultiple public datasets show that Prompt-R1 significantly outperforms baseline\nmodels across tasks. Our code is publicly available at\nhttps://github.com/QwenQKing/Prompt-R1.", "AI": {"tldr": "Prompt-R1是一个端到端的强化学习框架，它使用小型LLM自动生成提示来与大型LLM协作，以解决用户提示能力不足的问题，显著提升了大型LLM在复杂任务上的性能。", "motivation": "当前，用户在面对复杂问题时，往往难以提供准确有效的提示来与大型语言模型（LLMs）交互，这限制了LLMs的性能发挥。", "method": "本文提出了Prompt-R1框架，一个端到端的强化学习框架。它通过小型LLM与大型LLM协作，替代用户进行交互。这种协作被建模为多轮提示交互：小型LLM负责思考和生成提示，大型LLM执行复杂推理。设计了一种双约束奖励机制，以优化正确性、生成质量和推理准确性。Prompt-R1是一个即插即用的框架，支持多种大型LLM的推理和训练。", "result": "在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型。", "conclusion": "Prompt-R1提供了一个有效的、即插即用的框架，通过自动化和优化的提示生成，能够有效提升大型LLM在复杂问题解决中的性能。"}}
{"id": "2511.01320", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01320", "abs": "https://arxiv.org/abs/2511.01320", "authors": ["Ziqi Wang", "Hailiang Zhao", "Yuhao Yang", "Daojiang Hu", "Cheng Bao", "Mingyi Liu", "Kai Di", "Schahram Dustdar", "Zhongjie Wang", "Shuiguang Deng"], "title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance", "comment": null, "summary": "Accurate and timely prediction of tool conditions is critical for intelligent\nmanufacturing systems, where unplanned tool failures can lead to quality\ndegradation and production downtime. In modern industrial environments,\npredictive maintenance is increasingly implemented as an intelligent service\nthat integrates sensing, analysis, and decision support across production\nprocesses. To meet the demand for reliable and service-oriented operation, we\npresent OmniFuser, a multimodal learning framework for predictive maintenance\nof milling tools that leverages both visual and sensor data. It performs\nparallel feature extraction from high-resolution tool images and cutting-force\nsignals, capturing complementary spatiotemporal patterns across modalities. To\neffectively integrate heterogeneous features, OmniFuser employs a\ncontamination-free cross-modal fusion mechanism that disentangles shared and\nmodality-specific components, allowing for efficient cross-modal interaction.\nFurthermore, a recursive refinement pathway functions as an anchor mechanism,\nconsistently retaining residual information to stabilize fusion dynamics. The\nlearned representations can be encapsulated as reusable maintenance service\nmodules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)\nand multi-step force signal forecasting. Experiments on real-world milling\ndatasets demonstrate that OmniFuser consistently outperforms state-of-the-art\nbaselines, providing a dependable foundation for building intelligent\nindustrial maintenance services.", "AI": {"tldr": "本文提出了OmniFuser，一个用于铣削工具预测性维护的多模态学习框架，它结合视觉和传感器数据，通过创新的融合机制和递归细化路径，显著优于现有基线，为智能工业维护服务奠定基础。", "motivation": "智能制造系统中，工具状况的准确及时预测至关重要，因为计划外的工具故障会导致质量下降和生产停机。现代工业对可靠、服务导向的预测性维护需求日益增长。", "method": "OmniFuser框架利用高分辨率工具图像和切削力信号进行多模态学习。它并行提取互补的时空特征，并采用无污染的跨模态融合机制来解耦共享和模态特定组件，实现高效的跨模态交互。此外，一个递归细化路径作为锚定机制，持续保留残余信息以稳定融合动态。学习到的表示可用于工具状态分类和多步力信号预测。", "result": "在真实世界的铣削数据集上的实验表明，OmniFuser持续优于最先进的基线方法。它为构建智能工业维护服务提供了可靠的基础。", "conclusion": "OmniFuser是一个有效且可靠的多模态学习框架，能够准确预测铣削工具状况，并通过集成视觉和传感器数据以及创新的融合机制，显著提升了预测性维护的性能，支持智能工业维护服务的实施。"}}
{"id": "2511.01329", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01329", "abs": "https://arxiv.org/abs/2511.01329", "authors": ["Ying Song", "Yijing Wang", "Hui Yang", "Weihan Jin", "Jun Xiong", "Congyi Zhou", "Jialin Zhu", "Xiang Gao", "Rong Chen", "HuaGuang Deng", "Ying Dai", "Fei Xiao", "Haihong Tang", "Bo Zheng", "KaiFu Zhang"], "title": "Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework", "comment": null, "summary": "Evaluating platform-level interventions in search-based two-sided\nmarketplaces is fundamentally challenged by systemic effects such as spillovers\nand network interference. While widely used for causal inference, the PSM\n(Propensity Score Matching) - DID (Difference-in-Differences) framework remains\nsusceptible to selection bias and cross-unit interference from unaccounted\nspillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel\ncausal framework that integrates propensity score matching with competitive\nisolation to enable platform-level effect measurement (e.g., order volume, GMV)\ninstead of item-level metrics in search systems.\n  Our approach provides theoretically guaranteed unbiased estimation under\nmutual exclusion conditions, with an open dataset released to support\nreproducible research on marketplace interference (github.com/xxxx). Extensive\nexperiments demonstrate significant reductions in interference effects and\nestimation variance compared to baseline methods. Successful deployment in a\nlarge-scale marketplace confirms the framework's practical utility for\nplatform-level causal inference.", "AI": {"tldr": "本文提出了一种名为“竞争隔离PSM-DID”的新型因果推断框架，用于在搜索型双边市场中评估平台级干预措施，解决了现有方法中存在的溢出效应和网络干扰问题，并实现了无偏估计。", "motivation": "在搜索型双边市场中评估平台级干预措施时，系统性效应（如溢出效应和网络干扰）带来了根本性挑战。广泛使用的PSM-DID框架容易受到选择偏差和未考虑溢出效应导致的跨单元干扰的影响，无法准确衡量平台级指标。", "method": "引入了“竞争隔离PSM-DID”框架，该方法将倾向得分匹配（PSM）与竞争隔离相结合，以实现平台级效应（如订单量、GMV）的测量。该方法在互斥条件下提供理论上保证的无偏估计。", "result": "与基线方法相比，该方法显著减少了干扰效应和估计方差。它已成功部署在一个大型市场中，验证了其实用性。同时，发布了一个开放数据集以支持可复现研究。", "conclusion": "竞争隔离PSM-DID框架为平台级因果推断提供了一个实用且有效的解决方案，能够在双边市场中实现对平台级干预措施的无偏、准确评估，并有效处理系统性干扰问题。"}}
{"id": "2511.00352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00352", "abs": "https://arxiv.org/abs/2511.00352", "authors": ["Mohd Ruhul Ameen", "Akif Islam"], "title": "Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach", "comment": "6 pages, 8 figures, 4 Tables, submitted to ICECTE 2026", "summary": "The rapid rise of generative diffusion models has made distinguishing\nauthentic visual content from synthetic imagery increasingly challenging.\nTraditional deepfake detection methods, which rely on frequency or pixel-level\nartifacts, fail against modern text-to-image systems such as Stable Diffusion\nand DALL-E that produce photorealistic and artifact-free results. This paper\nintroduces a diffusion-based forensic framework that leverages multi-strength\nimage reconstruction dynamics, termed diffusion snap-back, to identify\nAI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and\nPSNR) evolve across varying noise strengths, we extract interpretable\nmanifold-based features that differentiate real and synthetic images. Evaluated\non a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under\ncross-validation and remains robust to common distortions such as compression\nand noise. Despite using limited data and a single diffusion backbone (Stable\nDiffusion v1.5), the proposed method demonstrates strong generalization and\ninterpretability, offering a foundation for scalable, model-agnostic synthetic\nmedia forensics.", "AI": {"tldr": "本文提出一种基于扩散模型的取证框架，通过分析多强度图像重建动态（称为“扩散回弹”）来有效识别AI生成图像，克服了传统方法对现代生成模型的失效问题。", "motivation": "随着生成扩散模型的快速发展，区分真实和合成图像变得越来越困难。传统的深度伪造检测方法依赖频率或像素级伪影，但对于Stable Diffusion和DALL-E等生成逼真且无伪影图像的现代文本到图像系统已失效。", "method": "该研究引入了一个基于扩散的取证框架，利用多强度图像重建动态（ termed diffusion snap-back）来识别AI生成图像。通过分析重建指标（LPIPS、SSIM和PSNR）在不同噪声强度下的演变，提取可解释的基于流形的特征，以区分真实和合成图像。", "result": "该方法在包含4000张图像的平衡数据集上进行评估，在交叉验证下实现了0.993的AUROC。它对压缩和噪声等常见失真具有鲁棒性。尽管使用了有限的数据和单一扩散骨干模型（Stable Diffusion v1.5），该方法仍表现出强大的泛化性和可解释性。", "conclusion": "该方法为可扩展、模型无关的合成媒体取证奠定了基础，提供了一种有效且具有泛化性的AI生成图像检测方案。"}}
{"id": "2511.01350", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01350", "abs": "https://arxiv.org/abs/2511.01350", "authors": ["Maartje H. M. Wermelink", "Renate Sachse", "Sebastian Kruppert", "Thomas Speck", "Falk J. Tauber"], "title": "Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator", "comment": "Conference Proceedings Paper Living machines 2025", "summary": "The Venus flytrap (Dionaea muscipula) does not only serve as the textbook\nmodel for a carnivorous plant, but also has long intrigued both botanists and\nengineers with its rapidly closing leaf trap. The trap closure is triggered by\ntwo consecutive touches of a potential prey, after which the lobes rapidly\nswitch from their concave open-state to their convex close-state and catch the\nprey within 100-500 ms after being triggered. This transformation from concave\nto convex is initiated by changes in turgor pressure and the release of stored\nelastic energy from prestresses in the concave state, which accelerate this\nmovement, leading to inversion of the lobes bi-axial curvature. Possessing two\nlow-energy states, the leaves can be characterized as bistable systems. With\nour research, we seek to deepen the understanding of Venus flytrap motion\nmechanics and apply its principles to the design of an artificial bistable lobe\nactuator. We identified geometrical characteristics, such as dimensional ratios\nand the thickness gradient in the lobe, and transferred these to two 3D-printed\nbistable actuator models. One actuator parallels the simulated geometry of a\nVenus flytrap leaf, the other is a lobe model designed with CAD. Both models\ndisplay concave-convex bi-stability and snap close. These demonstrators are the\nfirst step in the development of an artificial Venus flytrap that mimics the\nmechanical behavior of the biological model and can be used as a soft fast\ngripper.", "AI": {"tldr": "该研究深入理解捕蝇草的运动力学，并将其双稳态原理应用于设计和3D打印人工双稳态叶片执行器，以期开发仿生快速软抓手。", "motivation": "捕蝇草快速闭合的叶片陷阱长期以来吸引着植物学家和工程师。其从凹形到凸形的快速转变，由膨压变化和弹性储能释放驱动，使其成为一个具有两个低能态的双稳态系统。研究旨在深化对捕蝇草运动力学的理解，并将其原理应用于人工执行器设计。", "method": "研究识别了捕蝇草叶片的几何特征，如尺寸比和叶片厚度梯度。将这些特征应用于两个3D打印的双稳态执行器模型：一个模拟捕蝇草叶片的几何形状，另一个是CAD设计的叶片模型。", "result": "这两个3D打印模型都表现出凹凸双稳态并能快速闭合，成功模仿了生物模型的机械行为。", "conclusion": "这些演示器是开发模仿生物模型机械行为的人工捕蝇草的第一步，未来可作为一种柔软、快速的抓手使用。"}}
{"id": "2511.00362", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00362", "abs": "https://arxiv.org/abs/2511.00362", "authors": ["Momen Khandoker Ope", "Akif Islam", "Mohd Ruhul Ameen", "Abu Saleh Musa Miah", "Md Rashedul Islam", "Jungpil Shin"], "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery", "comment": "6 Pages, 4 figures, 2 Tables, Submitted to ICECTE 2026", "summary": "Cultural heritage restoration in Bangladesh faces a dual challenge of limited\nresources and scarce technical expertise. Traditional 3D digitization methods,\nsuch as photogrammetry or LiDAR scanning, require expensive hardware, expert\noperators, and extensive on-site access, which are often infeasible in\ndeveloping contexts. As a result, many of Bangladesh's architectural treasures,\nfrom the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to\ndecay and inaccessible in digital form. This paper introduces Oitijjo-3D, a\ncost-free generative AI framework that democratizes 3D cultural preservation.\nBy using publicly available Google Street View imagery, Oitijjo-3D reconstructs\nfaithful 3D models of heritage structures through a two-stage pipeline -\nmultimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture\nsynthesis, and neural image-to-3D generation through Hexagen for geometry\nrecovery. The system produces photorealistic, metrically coherent\nreconstructions in seconds, achieving significant speedups compared to\nconventional Structure-from-Motion pipelines, without requiring any specialized\nhardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,\nChoto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both\nvisual and structural fidelity while drastically lowering economic and\ntechnical barriers. By turning open imagery into digital heritage, this work\nreframes preservation as a community-driven, AI-assisted act of cultural\ncontinuity for resource-limited nations.", "AI": {"tldr": "本文提出Oitijjo-3D，一个免费的生成式AI框架，利用公开的Google街景图像，在资源有限的国家（如孟加拉国）实现文化遗产的3D数字化重建，克服了传统方法成本高昂和技术专家稀缺的问题。", "motivation": "孟加拉国的文化遗产修复面临资源有限和技术专长稀缺的双重挑战。传统的3D数字化方法（如摄影测量或激光雷达扫描）需要昂贵的硬件、专业操作员和广泛的现场访问，这在发展中国家通常不可行。因此，孟加拉国许多建筑瑰宝仍易受损且缺乏数字形式。", "method": "Oitijjo-3D是一个两阶段的管道：首先，利用Gemini 2.5 Flash Image进行多模态视觉推理，实现结构-纹理合成；其次，通过Hexagen进行神经图像到3D生成，恢复几何形状。该系统使用公开可用的Google街景图像作为输入。", "result": "Oitijjo-3D能够快速（几秒内）生成逼真、度量一致的文化遗产3D模型，与传统方法相比显著加快了速度，且无需专业硬件或专家监督。在Ahsan Manzil、Choto Sona Mosque和Paharpur等标志性建筑上的实验表明，该系统在保留视觉和结构保真度的同时，大幅降低了经济和技术门槛。", "conclusion": "Oitijjo-3D将开放图像转化为数字遗产，使3D文化遗产保护民主化，并将其重新定义为资源有限国家社区驱动、AI辅助的文化延续行为。"}}
{"id": "2511.00960", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00960", "abs": "https://arxiv.org/abs/2511.00960", "authors": ["Abhinav P M", "Ojasva Saxena", "Oswald C", "Parameswari Krishnamurthy"], "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "comment": null, "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations.", "AI": {"tldr": "本研究探讨了大型语言模型在七种印度语言中的文化推理和自我评估能力。结果显示，虽然性能最佳的模型在解谜方面表现出色，但其识别自身错误的能力较差，而表现较差的模型反而更具自我意识。", "motivation": "目前对于大型语言模型在非英语语言中进行文化相关推理的能力研究不足。", "method": "研究引入了一个多语言谜语数据集，包含传统谜语和上下文重构变体，并在七种印度主要语言中评估了五种大型语言模型（Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, LLaMA 4 Maverick）。实验分两个阶段：首先评估解谜性能，然后进行自我评估实验以衡量推理一致性，并采用七种提示策略。", "result": "Gemini 2.5 Pro在整体解谜表现最佳，但少样本方法提升有限，且准确率在不同语言间差异显著。一个关键发现是：模型的初始准确率与其识别自身错误的能力呈负相关。表现最好的模型（如Gemini 2.5 Pro）过度自信（真阴性率4.34%），而表现较差的模型（如LLaMA 4 Scout）则更具自我意识（真阴性率42.09%）。", "conclusion": "这些结果揭示了多语言推理中存在的明显差距，并强调了开发不仅能有效推理，而且能认识到自身局限性的模型的重要性。"}}
{"id": "2511.00357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00357", "abs": "https://arxiv.org/abs/2511.00357", "authors": ["Niklas Wölki", "Lukas Kondmann", "Christian Mollière", "Martin Langer", "Julia Gottfriedsen", "Martin Werner"], "title": "Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation", "comment": "This work was presented at the TerraBytes Workshop at the 42nd\n  International Conference on Machine Learning. This version is not part of the\n  official ICML proceedings", "summary": "Onboard cloud segmentation is a critical yet underexplored task in thermal\nEarth observation (EO), particularly for CubeSat missions constrained by\nlimited hardware and spectral information. CubeSats often rely on a single\nthermal band and lack sufficient labeled data, making conventional cloud\nmasking techniques infeasible. This work addresses these challenges by applying\ntransfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using\na UNet with a lightweight MobileNet encoder. We pretrain the model on the\npublic Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small\nset of mission-specific samples in a joint-training setup, improving the macro\nF1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a\nTensorRT engine and demonstrate full-image inference in under 5 seconds on an\nNVIDIA Jetson Nano. These results show that leveraging public datasets and\nlightweight architectures can enable accurate, efficient thermal-only cloud\nmasking on-orbit, supporting real-time decision-making in data-limited EO\nmissions.", "AI": {"tldr": "本文提出了一种利用迁移学习和轻量级UNet模型，通过在公共数据集上预训练并在任务特定数据上微调，实现CubeSat热红外图像的板载云分割，提高了准确性并满足了实时处理需求。", "motivation": "热红外地球观测（EO）中的板载云分割是一项关键但未被充分探索的任务，尤其对于硬件和光谱信息有限的CubeSat任务。CubeSat通常只依赖一个热波段且缺乏足够的标注数据，使得传统云掩膜技术不可行。", "method": "研究采用迁移学习方法，将UNet模型与轻量级MobileNet编码器结合。模型首先在公共Landsat-7云覆盖评估数据集上进行预训练，然后使用少量任务特定样本（FOREST-2 CubeSat）进行联合训练微调。最后，将模型转换为TensorRT引擎以实现板载推理。", "result": "该方法将宏F1分数从0.850提高到0.877（相对于仅使用FOREST-2数据的基线）。在NVIDIA Jetson Nano上，全图像推理时间不到5秒。", "conclusion": "研究结果表明，利用公共数据集和轻量级架构可以在轨实现准确、高效的纯热红外云掩膜，支持数据受限EO任务中的实时决策。"}}
{"id": "2511.01347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01347", "abs": "https://arxiv.org/abs/2511.01347", "authors": ["Riddhi Das", "Joscha Teichmann", "Thomas Speck", "Falk J. Tauber"], "title": "Design and development of an electronics-free earthworm robot", "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Soft robotic systems have gained widespread attention due to their inherent\nflexibility, adaptability, and safety, making them well-suited for varied\napplications. Among bioinspired designs, earthworm locomotion has been\nextensively studied for its efficient peristaltic motion, enabling movement in\nconfined and unstructured environments. Existing earthworm-inspired robots\nprimarily utilize pneumatic actuation due to its high force-to-weight ratio and\nease of implementation. However, these systems often rely on bulky,\npower-intensive electronic control units, limiting their practicality. In this\nwork, we present an electronics-free, earthworm-inspired pneumatic robot\nutilizing a modified Pneumatic Logic Gate (PLG) design. By integrating\npreconfigured PLG units with bellow actuators, we achieved a plug-and-play\nstyle modular system capable of peristaltic locomotion without external\nelectronic components. The proposed design reduces system complexity while\nmaintaining efficient actuation. We characterize the bellow actuators under\ndifferent operating conditions and evaluate the robots locomotion performance.\nOur findings demonstrate that the modified PLG-based control system effectively\ngenerates peristaltic wave propagation, achieving autonomous motion with\nminimal deviation. This study serves as a proof of concept for the development\nof electronics-free, peristaltic soft robots. The proposed system has potential\nfor applications in hazardous environments, where untethered, adaptable\nlocomotion is critical. Future work will focus on further optimizing the robot\ndesign and exploring untethered operation using onboard compressed air sources.", "AI": {"tldr": "本文提出了一种无电子元件、受蚯蚓启发的软体机器人，利用改进的气动逻辑门（PLG）实现蠕动运动，降低了系统复杂性并保持了高效驱动。", "motivation": "现有受蚯蚓启发的机器人虽然采用气动驱动，但通常依赖笨重且耗电的电子控制单元，这限制了它们在实际应用中的实用性。", "method": "研究人员通过将预配置的改进气动逻辑门（PLG）单元与波纹管执行器集成，构建了一个即插即用的模块化系统。该系统无需外部电子元件即可实现蠕动运动。论文中还对波纹管执行器在不同操作条件下进行了特性分析，并评估了机器人的运动性能。", "result": "研究结果表明，基于改进PLG的控制系统能够有效生成蠕动波传播，实现了自主运动且偏差极小。这证明了无电子元件蠕动软体机器人的概念可行性。", "conclusion": "本研究为开发无电子元件的蠕动软体机器人提供了概念验证，所提出的系统在需要无缆、适应性强的运动的危险环境中具有潜在应用价值。未来的工作将集中于进一步优化机器人设计和探索使用车载压缩空气源进行无缆操作。"}}
{"id": "2511.01334", "categories": ["cs.RO", "cs.AI", "cs.HC", "68T45"], "pdf": "https://arxiv.org/pdf/2511.01334", "abs": "https://arxiv.org/abs/2511.01334", "authors": ["Ling Niu", "Xiaoji Zheng", "Han Wang", "Chen Zheng", "Ziyuan Yang", "Bokui Chen", "Jiangtao Gong"], "title": "Embodied Cognition Augmented End2End Autonomous Driving", "comment": "24 pages,4 pages", "summary": "In recent years, vision-based end-to-end autonomous driving has emerged as a\nnew paradigm. However, popular end-to-end approaches typically rely on visual\nfeature extraction networks trained under label supervision. This limited\nsupervision framework restricts the generality and applicability of driving\nmodels. In this paper, we propose a novel paradigm termed $E^{3}AD$, which\nadvocates for comparative learning between visual feature extraction networks\nand the general EEG large model, in order to learn latent human driving\ncognition for enhancing end-to-end planning. In this work, we collected a\ncognitive dataset for the mentioned contrastive learning process. Subsequently,\nwe investigated the methods and potential mechanisms for enhancing end-to-end\nplanning with human driving cognition, using popular driving models as\nbaselines on publicly available autonomous driving datasets. Both open-loop and\nclosed-loop tests are conducted for a comprehensive evaluation of planning\nperformance. Experimental results demonstrate that the $E^{3}AD$ paradigm\nsignificantly enhances the end-to-end planning performance of baseline models.\nAblation studies further validate the contribution of driving cognition and the\neffectiveness of comparative learning process. To the best of our knowledge,\nthis is the first work to integrate human driving cognition for improving\nend-to-end autonomous driving planning. It represents an initial attempt to\nincorporate embodied cognitive data into end-to-end autonomous driving,\nproviding valuable insights for future brain-inspired autonomous driving\nsystems. Our code will be made available at Github", "AI": {"tldr": "本文提出了一种名为 $E^{3}AD$ 的新范式，通过视觉特征提取网络与通用EEG大模型之间的对比学习来融入人类驾驶认知，从而显著提升端到端自动驾驶规划性能。", "motivation": "目前的视觉端到端自动驾驶方法依赖于标签监督下的视觉特征提取网络，这限制了驾驶模型的通用性和适用性。研究旨在通过学习人类潜在的驾驶认知来增强端到端规划。", "method": "研究提出了 $E^{3}AD$ 范式，通过视觉特征提取网络与通用EEG大模型进行对比学习，以学习潜在的人类驾驶认知。为此，收集了一个认知数据集。随后，在流行的自动驾驶模型（作为基线）和公开数据集中，通过开环和闭环测试，研究了增强端到端规划的方法和潜在机制。还进行了消融研究。", "result": "$E^{3}AD$ 范式显著提升了基线模型的端到端规划性能。消融研究进一步验证了驾驶认知的贡献和对比学习过程的有效性。", "conclusion": "这是首次将人类驾驶认知整合到端到端自动驾驶规划中的工作，代表了将具身认知数据融入端到端自动驾驶的初步尝试，为未来类脑自动驾驶系统提供了宝贵的见解。"}}
{"id": "2511.01019", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.01019", "abs": "https://arxiv.org/abs/2511.01019", "authors": ["Bowen Chen", "Jayesh Gajbhar", "Gregory Dusek", "Rob Redmon", "Patrick Hogan", "Paul Liu", "DelWayne Bohnenstiehl", "Dongkuan", "Xu", "Ruoying He"], "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings", "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz.", "AI": {"tldr": "OceanAI是一个对话式AI平台，它将开源大型语言模型（LLM）的自然语言流畅性与美国国家海洋和大气管理局（NOAA）的权威海洋数据流实时集成，旨在解决通用AI在科学领域中“幻觉”问题，提供可验证、可追溯的科学信息。", "motivation": "通用对话式AI系统常常产生未经证实的“幻觉”，这损害了科学严谨性，尤其是在科学研究领域。", "method": "OceanAI结合了开源LLM的自然语言处理能力，并与NOAA托管的权威海洋数据流进行实时、参数化访问。每次查询都会触发实时API调用，识别、解析和合成相关数据集，生成可重现的自然语言响应和数据可视化。", "result": "在与三种常用AI聊天界面产品的盲测比较中，只有OceanAI能够生成来自NOAA的数据值并提供原始数据引用；其他产品要么拒绝回答，要么提供了未经支持的结果。OceanAI支持连接多个NOAA数据产品和变量。", "conclusion": "OceanAI通过将输出建立在可验证的观测数据之上，提高了透明度、可重复性和信任度，为海洋领域的AI辅助决策支持提供了一个可扩展的框架。"}}
{"id": "2511.01369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01369", "abs": "https://arxiv.org/abs/2511.01369", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Lateral Velocity Model for Vehicle Parking Applications", "comment": "This manuscript has been submitted to Vehicle System Dynamics for\n  possible publication", "summary": "Automated parking requires accurate localization for quick and precise\nmaneuvering in tight spaces. While the longitudinal velocity can be measured\nusing wheel encoders, the estimation of the lateral velocity remains a key\nchallenge due to the absence of dedicated sensors in consumer-grade vehicles.\nExisting approaches often rely on simplified vehicle models, such as the\nzero-slip model, which assumes no lateral velocity at the rear axle. It is well\nestablished that this assumption does not hold during low-speed driving and\nresearchers thus introduce additional heuristics to account for differences. In\nthis work, we analyze real-world data from parking scenarios and identify a\nsystematic deviation from the zero-slip assumption. We provide explanations for\nthe observed effects and then propose a lateral velocity model that better\ncaptures the lateral dynamics of the vehicle during parking. The model improves\nestimation accuracy, while relying on only two parameters, making it\nwell-suited for integration into consumer-grade applications.", "AI": {"tldr": "本文提出了一种新的横向速度模型，通过分析真实泊车数据，解决了传统零滑移模型在低速泊车场景下的局限性，从而提高了自动泊车的定位精度。", "motivation": "自动泊车需要精确的定位，尤其是在狭窄空间内的横向速度估计。消费级车辆缺乏专用的横向速度传感器，现有方法依赖于简化模型（如零滑移模型），但在低速行驶时该假设不成立，导致估计不准确。", "method": "研究人员分析了真实世界的泊车场景数据，识别出与零滑移假设的系统性偏差，并解释了观察到的效应。在此基础上，他们提出了一种新的横向速度模型，以更好地捕捉车辆在泊车时的横向动态。", "result": "提出的模型能够更好地捕捉车辆的横向动态，提高了横向速度估计的准确性。该模型仅依赖于两个参数，使其非常适合集成到消费级应用中。", "conclusion": "通过分析真实数据并提出一个参数简洁的新模型，研究有效地解决了自动泊车中横向速度估计的挑战，显著提升了估计精度，并具有良好的实用性。"}}
{"id": "2511.01046", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01046", "abs": "https://arxiv.org/abs/2511.01046", "authors": ["Vedant Acharya", "Abhay Pisharodi", "Rishabh Mondal", "Mohammad Rafiuddin", "Nipun Batra"], "title": "VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics", "comment": "4 Pages, 4 Figures", "summary": "Air pollution causes about 1.6 million premature deaths each year in India,\nyet decision makers struggle to turn dispersed data into decisions. Existing\ntools require expertise and provide static dashboards, leaving key policy\nquestions unresolved. We present VayuChat, a conversational system that answers\nnatural language questions on air quality, meteorology, and policy programs,\nand responds with both executable Python code and interactive visualizations.\nVayuChat integrates data from Central Pollution Control Board (CPCB) monitoring\nstations, state-level demographics, and National Clean Air Programme (NCAP)\nfunding records into a unified interface powered by large language models. Our\nlive demonstration will show how users can perform complex environmental\nanalytics through simple conversations, making data science accessible to\npolicymakers, researchers, and citizens. The platform is publicly deployed at\nhttps://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further\ninformation check out video uploaded on\nhttps://www.youtube.com/watch?v=d6rklL05cs4.", "AI": {"tldr": "VayuChat是一个会话式系统，利用大型语言模型整合印度空气质量数据，通过自然语言查询提供可执行代码和交互式可视化，使决策者、研究人员和公众更容易进行环境数据分析。", "motivation": "印度每年约有160万人因空气污染过早死亡，但决策者难以将分散的数据转化为决策。现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。", "method": "VayuChat是一个会话式系统，能够回答关于空气质量、气象和政策计划的自然语言问题，并以可执行的Python代码和交互式可视化作为回应。它整合了中央污染控制委员会(CPCB)监测站、邦级人口统计数据和国家清洁空气计划(NCAP)资金记录的数据，并通过大型语言模型提供统一接口。", "result": "用户可以通过简单的对话执行复杂的环境分析，使数据科学对政策制定者、研究人员和公民都变得可及。该平台已公开部署。", "conclusion": "VayuChat通过提供一个易于使用的会话式界面，显著降低了空气质量数据分析的门槛，使决策者和公众能够更有效地利用数据来应对空气污染问题。"}}
{"id": "2511.01363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01363", "abs": "https://arxiv.org/abs/2511.01363", "authors": ["Giuseppe Riva", "Brenda K. Wiederhold", "Fabrizia Mantovani"], "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing", "comment": "4 Tables", "summary": "The cognitive processes of the hypnotized mind and the computational\noperations of large language models (LLMs) share deep functional parallels.\nBoth systems generate sophisticated, contextually appropriate behavior through\nautomatic pattern-completion mechanisms operating with limited or unreliable\nexecutive oversight. This review examines this convergence across three\nprinciples: automaticity, in which responses emerge from associative rather\nthan deliberative processes; suppressed monitoring, leading to errors such as\nconfabulation in hypnosis and hallucination in LLMs; and heightened contextual\ndependency, where immediate cues (for example, the suggestion of a therapist or\nthe prompt of the user) override stable knowledge.\n  These mechanisms reveal an observer-relative meaning gap: both systems\nproduce coherent but ungrounded outputs that require an external interpreter to\nsupply meaning. Hypnosis and LLMs also exemplify functional agency - the\ncapacity for complex, goal-directed, context-sensitive behavior - without\nsubjective agency, the conscious awareness of intention and ownership that\ndefines human action. This distinction clarifies how purposive behavior can\nemerge without self-reflective consciousness, governed instead by structural\nand contextual dynamics. Finally, both domains illuminate the phenomenon of\nscheming: automatic, goal-directed pattern generation that unfolds without\nreflective awareness. Hypnosis provides an experimental model for understanding\nhow intention can become dissociated from conscious deliberation, offering\ninsights into the hidden motivational dynamics of artificial systems.\nRecognizing these parallels suggests that the future of reliable AI lies in\nhybrid architectures that integrate generative fluency with mechanisms of\nexecutive monitoring, an approach inspired by the complex, self-regulating\narchitecture of the human mind.", "AI": {"tldr": "本文探讨了催眠心智与大型语言模型（LLMs）之间深刻的功能相似性，主要体现在自动化、监测抑制和情境依赖性。两者都表现出功能性能动性而非主观能动性，并建议未来的可靠AI应借鉴人脑，整合生成流畅性与执行监控机制。", "motivation": "研究者旨在通过比较催眠心智和LLMs的认知过程与计算操作，揭示复杂、有目的的行为如何在缺乏自我反思意识的情况下产生，并为未来可靠AI的设计提供启示。", "method": "本文通过综述和概念分析的方法，从自动化、监测抑制和情境依赖性三个核心原则出发，探讨了催眠心智与大型语言模型（LLMs）之间的功能趋同性。", "result": "研究发现，催眠心智和LLMs在以下方面存在功能相似性：1) 自动化：响应源于联想而非审慎过程；2) 监测抑制：导致催眠中的虚构和LLMs中的幻觉等错误；3) 高度情境依赖性：即时线索（如治疗师的建议或用户的提示）可覆盖稳定知识。两者都产生连贯但无根据的输出，需要外部解释者赋予意义，并展现出功能性能动性而非主观能动性。此外，两者都揭示了“策划”（scheming）现象，即无反思意识的自动、目标导向的模式生成。催眠为理解意图如何与有意识的思考分离提供了实验模型。", "conclusion": "有目的的行为可以在没有自我反思意识的情况下产生，而是由结构和情境动态所支配。未来的可靠AI应采用混合架构，将生成流畅性与执行监控机制相结合，这种方法受到人脑复杂自调节架构的启发。"}}
{"id": "2511.00381", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00381", "abs": "https://arxiv.org/abs/2511.00381", "authors": ["Jiaming Li", "Junlei Wu", "Sheng Wang", "Honglin Xiong", "Jiangdong Cai", "Zihao Zhao", "Yitao Zhu", "Yuan Yin", "Dinggang Shen", "Qian Wang"], "title": "VisionCAD: An Integration-Free Radiology Copilot Framework", "comment": null, "summary": "Widespread clinical deployment of computer-aided diagnosis (CAD) systems is\nhindered by the challenge of integrating with existing hospital IT\ninfrastructure. Here, we introduce VisionCAD, a vision-based radiological\nassistance framework that circumvents this barrier by capturing medical images\ndirectly from displays using a camera system. The framework operates through an\nautomated pipeline that detects, restores, and analyzes on-screen medical\nimages, transforming camera-captured visual data into diagnostic-quality images\nsuitable for automated analysis and report generation. We validated VisionCAD\nacross diverse medical imaging datasets, demonstrating that our modular\narchitecture can flexibly utilize state-of-the-art diagnostic models for\nspecific tasks. The system achieves diagnostic performance comparable to\nconventional CAD systems operating on original digital images, with an F1-score\ndegradation typically less than 2\\% across classification tasks, while natural\nlanguage generation metrics for automated reports remain within 1\\% of those\nderived from original images. By requiring only a camera device and standard\ncomputing resources, VisionCAD offers an accessible approach for AI-assisted\ndiagnosis, enabling the deployment of diagnostic capabilities in diverse\nclinical settings without modifications to existing infrastructure.", "AI": {"tldr": "VisionCAD是一个基于视觉的放射学辅助框架，通过相机直接从显示器捕获医学图像，绕过了传统CAD系统在医院IT集成方面的障碍，实现了与现有系统的可比诊断性能。", "motivation": "现有计算机辅助诊断（CAD）系统在临床部署中面临的主要挑战是与医院现有IT基础设施的整合困难。", "method": "VisionCAD框架通过一个相机系统直接从显示器捕获医学图像。它采用自动化流程，检测、恢复并分析屏幕上的医学图像，将相机捕获的视觉数据转换为诊断质量的图像，适用于自动化分析和报告生成。", "result": "VisionCAD在多种医学影像数据集上得到了验证，其诊断性能与传统CAD系统在原始数字图像上的表现相当。在分类任务中，F1分数通常下降不到2%；自动化报告的自然语言生成指标与原始图像生成的结果相差不到1%。该系统模块化且能灵活利用最先进的诊断模型，仅需相机设备和标准计算资源。", "conclusion": "VisionCAD为AI辅助诊断提供了一种可访问的部署方法，无需修改现有IT基础设施，即可在多样化的临床环境中实现诊断能力。"}}
{"id": "2511.01053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01053", "abs": "https://arxiv.org/abs/2511.01053", "authors": ["Qing Ding", "Eric Hua Qing Zhang", "Felix Jozsa", "Julia Ive"], "title": "Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs", "comment": "Submitted to EFMI Medical Informatics Europe 2026", "summary": "Large language models (LLMs) are increasingly used in healthcare, yet\nstandardised benchmarks for evaluating guideline-based clinical reasoning are\nmissing. This study introduces a validated dataset derived from publicly\navailable guidelines across multiple diagnoses. The dataset was created with\nthe help of GPT and contains realistic patient scenarios, as well as clinical\nquestions. We benchmark a range of recent popular LLMs to showcase the validity\nof our dataset. The framework supports systematic evaluation of LLMs' clinical\nutility and guideline adherence.", "AI": {"tldr": "本研究引入了一个新的、经验证的数据集和评估框架，用于衡量大型语言模型（LLMs）在医疗保健领域基于临床指南的推理能力。", "motivation": "尽管大型语言模型在医疗保健领域应用日益广泛，但目前缺乏用于评估其基于临床指南推理能力的标准化基准。", "method": "研究团队利用GPT从公开的临床指南中创建了一个经验证的数据集，其中包含真实的患者情景和临床问题。随后，使用该数据集对一系列流行的LLMs进行了基准测试。", "result": "通过对多种LLMs进行基准测试，研究展示了所创建数据集的有效性。该框架能够支持对LLMs临床实用性和指南依从性进行系统性评估。", "conclusion": "本研究提供的框架和数据集能够有效评估大型语言模型在医疗保健领域遵循临床指南进行推理的能力和实用性。"}}
{"id": "2511.01379", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01379", "abs": "https://arxiv.org/abs/2511.01379", "authors": ["Kun Hu", "Menggang Li", "Zhiwen Jin", "Chaoquan Tang", "Eryi Hu", "Gongbo Zhou"], "title": "CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels", "comment": "Accepted by IROS 2025", "summary": "Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and\nGPS-denied underground coal mine environments presents significant challenges.\nSensors must contend with abnormal operating conditions: GPS unavailability\nimpedes scene reconstruction and absolute geographic referencing, uneven or\nslippery terrain degrades wheel odometer accuracy, and long, feature-poor\ntunnels reduce LiDAR effectiveness. To address these issues, we propose\nCoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM\nframework based on the Iterated Error-State Kalman Filter (IESKF). First,\nLiDAR-inertial odometry is tightly fused with UWB absolute positioning\nconstraints to align the SLAM system with a global coordinate. Next, wheel\nodometer is integrated through tight coupling, enhanced by nonholonomic\nconstraints (NHC) and vehicle lever arm compensation, to address performance\ndegradation in areas beyond UWB measurement range. Finally, an adaptive motion\nmode switching mechanism dynamically adjusts the robot's motion mode based on\nUWB measurement range and environmental degradation levels. Experimental\nresults validate that our method achieves superior accuracy and robustness in\nreal-world underground coal mine scenarios, outperforming state-of-the-art\napproaches. We open source our code of this work on Github to benefit the\nrobotics community.", "AI": {"tldr": "本文提出了一种名为CM-LIUW-Odometry的多模态SLAM框架，专为地下煤矿环境设计，通过紧密融合LiDAR、IMU、UWB和轮式里程计，并结合IESKF、非完整性约束和自适应运动模式切换，实现了高精度和鲁棒性。", "motivation": "地下煤矿环境的SLAM面临巨大挑战：GPS信号缺失导致无法重建场景和进行绝对地理参考；崎岖地形降低轮式里程计精度；以及特征稀疏的隧道削弱了LiDAR的有效性。", "method": "该研究提出了基于迭代误差状态卡尔曼滤波器（IESKF）的多模态SLAM框架CM-LIUW-Odometry。首先，将LiDAR-惯性里程计与UWB绝对定位约束紧密融合，以实现与全局坐标系的对齐。其次，通过紧密耦合集成轮式里程计，并辅以非完整性约束（NHC）和车辆杠杆臂补偿，以解决UWB测量范围之外区域的性能下降问题。最后，引入自适应运动模式切换机制，根据UWB测量范围和环境退化程度动态调整机器人运动模式。", "result": "实验结果验证了该方法在真实的地下煤矿场景中实现了卓越的精度和鲁棒性，优于现有最先进的方法。", "conclusion": "所提出的CM-LIUW-Odometry框架有效解决了地下煤矿环境中的SLAM挑战，提供了高精度和高鲁棒性的定位与建图解决方案。"}}
{"id": "2511.00370", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00370", "abs": "https://arxiv.org/abs/2511.00370", "authors": ["Chaochen Wu", "Guan Luo", "Meiyun Zuo", "Zhitao Fan"], "title": "Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict", "comment": null, "summary": "Video moment retrieval uses a text query to locate a moment from a given\nuntrimmed video reference. Locating corresponding video moments with text\nqueries helps people interact with videos efficiently. Current solutions for\nthis task have not considered conflict within location results from different\nmodels, so various models cannot integrate correctly to produce better results.\nThis study introduces a reinforcement learning-based video moment retrieval\nmodel that can scan the whole video once to find the moment's boundary while\nproducing its locational evidence. Moreover, we proposed a multi-agent system\nframework that can use evidential learning to resolve conflicts between agents'\nlocalization output. As a side product of observing and dealing with conflicts\nbetween agents, we can decide whether a query has no corresponding moment in a\nvideo (out-of-scope) without additional training, which is suitable for\nreal-world applications. Extensive experiments on benchmark datasets show the\neffectiveness of our proposed methods compared with state-of-the-art\napproaches. Furthermore, the results of our study reveal that modeling\ncompetition and conflict of the multi-agent system is an effective way to\nimprove RL performance in moment retrieval and show the new role of evidential\nlearning in the multi-agent framework.", "AI": {"tldr": "本研究提出了一种基于强化学习的视频时刻检索模型，该模型通过多智能体系统和证据学习来解决不同模型定位结果之间的冲突，并能处理范围外查询，有效提升了检索性能。", "motivation": "现有视频时刻检索方案未能考虑不同模型定位结果之间的冲突，导致模型无法有效整合以产生更好的结果。", "method": "本研究引入了一个基于强化学习的视频时刻检索模型，该模型能一次性扫描整个视频以找到时刻边界并产生定位证据。此外，提出了一种多智能体系统框架，利用证据学习解决智能体定位输出之间的冲突。通过观察和处理智能体之间的冲突，模型还能在不额外训练的情况下判断查询是否没有对应的视频时刻（范围外查询）。", "result": "在基准数据集上的大量实验表明，所提出的方法与最先进的方法相比是有效的。研究结果还揭示，对多智能体系统中的竞争和冲突进行建模是提高时刻检索中强化学习性能的有效方法，并展示了证据学习在多智能体框架中的新作用。", "conclusion": "本研究提出的基于强化学习的多智能体系统结合证据学习，能够有效解决视频时刻检索中的定位冲突问题，提升检索性能，并能处理范围外查询，为多智能体框架下的强化学习和证据学习提供了新的视角。"}}
{"id": "2511.01375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01375", "abs": "https://arxiv.org/abs/2511.01375", "authors": ["Hamin Koo", "Minseon Kim", "Jaehyung Kim"], "title": "Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges", "comment": "under review, 28 pages", "summary": "Identifying the vulnerabilities of large language models (LLMs) is crucial\nfor improving their safety by addressing inherent weaknesses. Jailbreaks, in\nwhich adversaries bypass safeguards with crafted input prompts, play a central\nrole in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.\nRecent optimization-based jailbreak approaches iteratively refine attack\nprompts by leveraging LLMs. However, they often rely heavily on either binary\nattack success rate (ASR) signals, which are sparse, or manually crafted\nscoring templates, which introduce human bias and uncertainty in the scoring\noutcomes. To address these limitations, we introduce AMIS (Align to MISalign),\na meta-optimization framework that jointly evolves jailbreak prompts and\nscoring templates through a bi-level structure. In the inner loop, prompts are\nrefined using fine-grained and dense feedback using a fixed scoring template.\nIn the outer loop, the template is optimized using an ASR alignment score,\ngradually evolving to better reflect true attack outcomes across queries. This\nco-optimization process yields progressively stronger jailbreak prompts and\nmore calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors\ndemonstrate that AMIS achieves state-of-the-art performance, including 88.0%\nASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming\nexisting baselines by substantial margins.", "AI": {"tldr": "本文提出AMIS框架，通过双层优化共同进化越狱提示词和评分模板，以解决现有LLM越狱方法中稀疏的攻击成功率信号和人工评分偏差问题，实现了最先进的越狱性能。", "motivation": "识别大型语言模型（LLMs）的漏洞对于提高其安全性至关重要。现有的基于优化的越狱方法依赖于稀疏的二元攻击成功率（ASR）信号或人工设计的评分模板，这些模板引入了人类偏见和不确定性，限制了越狱效果和评分准确性。", "method": "引入AMIS（Align to MISalign）元优化框架，采用双层结构共同进化越狱提示词和评分模板。内层循环使用固定的评分模板，通过细粒度、密集的反馈优化提示词。外层循环则利用ASR对齐分数优化评分模板，使其逐步更好地反映真实的攻击结果。这种协同优化过程能产生更强的越狱提示词和更校准的评分信号。", "result": "在AdvBench和JBB-Behaviors基准测试中，AMIS取得了最先进的性能，包括在Claude-3.5-Haiku上达到88.0%的ASR和在Claude-4-Sonnet上达到100.0%的ASR，显著优于现有基线方法。", "conclusion": "AMIS框架通过协同优化越狱提示词和评分模板，有效解决了现有方法的局限性，产生了更强大的越狱提示词和更准确的评分信号，从而在识别LLM漏洞方面取得了显著进步。"}}
{"id": "2511.01383", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01383", "abs": "https://arxiv.org/abs/2511.01383", "authors": ["Landson Guo", "Andres M. Diaz Aguilar", "William Talbot", "Turcan Tuna", "Marco Hutter", "Cesar Cadena"], "title": "CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation", "comment": null, "summary": "Accurate point-wise velocity estimation in 3D is crucial for robot\ninteraction with non-rigid, dynamic agents, such as humans, enabling robust\nperformance in path planning, collision avoidance, and object manipulation in\ndynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,\nand camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.\nThis pipeline leverages raw RADAR measurements to create a novel RADAR\nrepresentation, the velocity cube, which densely represents radial velocities\nwithin the RADAR's field-of-view. By combining the velocity cube for radial\nvelocity extraction, optical flow for tangential velocity estimation, and LiDAR\nfor point-wise range measurements through a closed-form solution, our approach\ncan produce 3D velocity estimates for a dense array of points. Developed as an\nopen-source ROS2 package, CaRLi-V has been field-tested against a custom\ndataset and proven to produce low velocity error metrics relative to ground\ntruth, enabling point-wise velocity estimation for robotic applications.", "AI": {"tldr": "本文提出了一种名为CaRLi-V的新型RADAR、LiDAR和相机融合管道，用于精确的点云三维速度估计，以支持机器人在动态环境中的交互。", "motivation": "在机器人与非刚性、动态物体（如人类）交互时，准确的三维点云速度估计至关重要，这能增强路径规划、碰撞避免和物体操作的鲁棒性。", "method": "CaRLi-V管道利用原始RADAR测量数据创建了一种新颖的RADAR表示——速度立方体，用于密集表示径向速度。结合速度立方体提取径向速度、光流估计切向速度以及LiDAR提供点云距离测量，通过闭式解计算出稠密点的三维速度估计。", "result": "该方法能够为密集的点阵列生成三维速度估计。经过自定义数据集的实地测试，CaRLi-V相对于真实值产生了较低的速度误差指标，证实了其在机器人应用中进行点云速度估计的有效性。", "conclusion": "CaRLi-V作为一种开源ROS2包，通过多传感器融合实现了精确的点云三维速度估计，为机器人应用提供了关键能力，并在实际测试中表现出低误差。"}}
{"id": "2511.01396", "categories": ["cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01396", "abs": "https://arxiv.org/abs/2511.01396", "authors": ["Clément Yvernes", "Emilie Devijver", "Adèle H. Ribeiro", "Marianne Clausel--Lesourd", "Éric Gaussier"], "title": "Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering", "comment": "Accepted at The Thirty-ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS2025)", "summary": "Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes\nrepresent clusters of variables, and edges encode both cluster-level causal\nrelationships and dependencies arisen from unobserved confounding. C-DAGs\ndefine an equivalence class of acyclic causal graphs that agree on\ncluster-level relationships, enabling causal reasoning at a higher level of\nabstraction. However, when the chosen clustering induces cycles in the\nresulting C-DAG, the partition is deemed inadmissible under conventional C-DAG\nsemantics. In this work, we extend the C-DAG framework to support arbitrary\nvariable clusterings by relaxing the partition admissibility constraint,\nthereby allowing cyclic C-DAG representations. We extend the notions of\nd-separation and causal calculus to this setting, significantly broadening the\nscope of causal reasoning across clusters and enabling the application of\nC-DAGs in previously intractable scenarios. Our calculus is both sound and\natomically complete with respect to the do-calculus: all valid interventional\nqueries at the cluster level can be derived using our rules, each corresponding\nto a primitive do-calculus step.", "AI": {"tldr": "本文扩展了聚类有向无环图（C-DAG）框架，允许表示循环，从而支持任意变量聚类，并扩展了d-分离和因果演算，以拓宽跨聚类的因果推理范围。", "motivation": "传统的C-DAG框架在所选聚类导致循环时会认为该划分不可接受，限制了其适用性。研究动机是解除这一限制，使C-DAG能够处理更广泛的变量聚类场景。", "method": "通过放宽划分可接受性约束，允许循环C-DAG表示。在此基础上，扩展了d-分离和因果演算的概念，以适应这种新设置。", "result": "开发了一个针对循环C-DAG的因果演算，该演算相对于do-演算而言是完备且原子完整的，意味着所有有效的聚类级别干预查询都可以使用其规则推导出来，且每个规则都对应一个基本的do-演算步骤。", "conclusion": "扩展后的C-DAG框架支持任意变量聚类，显著拓宽了跨聚类的因果推理范围，并使其能够应用于以前无法处理的场景。"}}
{"id": "2511.00391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00391", "abs": "https://arxiv.org/abs/2511.00391", "authors": ["Xuanle Zhao", "Deyang Jiang", "Zhixiong Zeng", "Lei Chen", "Haibo Qiu", "Jing Huang", "Yufeng Zhong", "Liming Zheng", "Yilin Cao", "Lin Ma"], "title": "VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning", "comment": "Preprint Version, Work in Progress", "summary": "Multimodal code generation has garnered significant interest within the\nresearch community. Despite the notable success of recent vision-language\nmodels (VLMs) on specialized tasks like Chart-to-code generation, their\nreliance on single-task training regimens fosters a narrow paradigm that\nhinders the development of generalized \\textbf{VI}sio\\textbf{N} \\textbf{C}ode\n\\textbf{I}ntelligence. In this work, we introduce \\textbf{VinciCoder}, a\nunified multimodal code generation model that addresses this limitation via a\ntwo-stage training framework. We begin by constructing a large-scale Supervised\nFinetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving\ndirect code generation and visual-based code refinement. Subsequently, we\nintroduce a Visual Reinforcement Learning (ViRL) strategy, which employs a\ncoarse-to-fine reward mechanism to improve visual fidelity by calculating\nvisual similarity across local and global image patches. Extensive experiments\non various multimodal code generation benchmarks demonstrate that VinciCoder\nachieves state-of-the-art performance, underscoring the effectiveness of our\ncoarse-to-fine ViRL strategy. The code and model will be available at\nhttps://github.com/DocTron-hub/VinciCoder.", "AI": {"tldr": "VinciCoder是一个统一的多模态代码生成模型，通过两阶段训练（大规模SFT和粗粒度到细粒度的ViRL策略）解决了现有模型泛化性差的问题，并在多项基准测试中取得了最先进的性能。", "motivation": "现有的视觉-语言模型（VLMs）在专业任务上表现出色，但其单一任务训练范式阻碍了通用视觉代码智能（VISION Code Intelligence）的发展，无法很好地泛化到多种多模态代码生成任务。", "method": "该研究引入了VinciCoder模型，采用两阶段训练框架：\n1. 构建了一个包含1.6M图像-代码对的大规模监督微调（SFT）语料库，用于直接代码生成和基于视觉的代码优化任务。\n2. 引入了视觉强化学习（ViRL）策略，该策略使用从粗粒度到细粒度的奖励机制，通过计算局部和全局图像补丁的视觉相似性来提高视觉保真度。", "result": "VinciCoder在各种多模态代码生成基准测试中均取得了最先进的性能，这证明了其粗粒度到细粒度的ViRL策略的有效性。", "conclusion": "VinciCoder提出的统一多模态代码生成模型及其两阶段训练框架（特别是粗粒度到细粒度的ViRL策略）能够有效解决现有模型的局限性，并显著提升多模态代码生成任务的性能。"}}
{"id": "2511.01407", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01407", "abs": "https://arxiv.org/abs/2511.01407", "authors": ["Paolo Rabino", "Gabriele Tiboni", "Tatiana Tommasi"], "title": "FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths", "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Object-Centric Motion Generation (OCMG) is instrumental in advancing\nautomated manufacturing processes, particularly in domains requiring\nhigh-precision expert robotic motions, such as spray painting and welding. To\nrealize effective automation, robust algorithms are essential for generating\nextended, object-aware trajectories across intricate 3D geometries. However,\ncontemporary OCMG techniques are either based on ad-hoc heuristics or employ\nlearning-based pipelines that are still reliant on sensitive post-processing\nsteps to generate executable paths. We introduce FoldPath, a novel, end-to-end,\nneural field based method for OCMG. Unlike prior deep learning approaches that\npredict discrete sequences of end-effector waypoints, FoldPath learns the robot\nmotion as a continuous function, thus implicitly encoding smooth output paths.\nThis paradigm shift eliminates the need for brittle post-processing steps that\nconcatenate and order the predicted discrete waypoints. Particularly, our\napproach demonstrates superior predictive performance compared to recently\nproposed learning-based methods, and attains generalization capabilities even\nin real industrial settings, where only a limited amount of 70 expert samples\nare provided. We validate FoldPath through comprehensive experiments in a\nrealistic simulation environment and introduce new, rigorous metrics designed\nto comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG\ntask towards practical maturity.", "AI": {"tldr": "本文提出了FoldPath，一种端到端的基于神经场的方法，用于生成连续、平滑的机器人运动，解决了现有方法对离散路径后处理的依赖。", "motivation": "自动化制造（如喷漆和焊接）需要高精度、物体感知的机器人运动生成。现有方法要么是启发式的，要么是基于学习但依赖敏感的后处理步骤来生成可执行路径，难以实现有效自动化。", "method": "引入了FoldPath，一种新颖的、端到端的、基于神经场（neural field）的物体中心运动生成（OCMG）方法。它将机器人运动学习为连续函数，而非离散的末端执行器路径点序列，从而隐式编码平滑的输出路径，并消除了对易出错的后处理步骤的需求。", "result": "FoldPath在预测性能上优于最近提出的学习方法，即使在仅有70个专家样本的真实工业环境中，也展现出泛化能力。通过在模拟环境中进行综合实验，并引入新的、严格的指标来评估长周期机器人路径，验证了其有效性。", "conclusion": "FoldPath通过提供鲁棒的、连续的运动生成，且无需脆弱的后处理步骤，推动了OCMG任务向实际成熟迈进，有望在自动化制造中发挥重要作用。"}}
{"id": "2511.00389", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00389", "abs": "https://arxiv.org/abs/2511.00389", "authors": ["Fan Zhang", "Haoxuan Li", "Shengju Qian", "Xin Wang", "Zheng Lian", "Hao Wu", "Zhihong Zhu", "Yuan Gao", "Qiankun Li", "Yefeng Zheng", "Zhouchen Lin", "Pheng-Ann Heng"], "title": "Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have revolutionized numerous\nresearch fields, including computer vision and affective computing. As a\npivotal challenge in this interdisciplinary domain, facial expression\nrecognition (FER) has evolved from separate, domain-specific models to more\nunified approaches. One promising avenue to unify FER tasks is converting\nconventional FER datasets into visual question-answering (VQA) formats,\nenabling the direct application of powerful generalist MLLMs for inference.\nHowever, despite the success of cutting-edge MLLMs in various tasks, their\nperformance on FER tasks remains largely unexplored. To address this gap, we\nprovide FERBench, a systematic benchmark that incorporates 20 state-of-the-art\nMLLMs across four widely used FER datasets. Our results reveal that, while\nMLLMs exhibit good classification performance, they still face significant\nlimitations in reasoning and interpretability. To this end, we introduce\npost-training strategies aimed at enhancing the facial expression reasoning\ncapabilities of MLLMs. Specifically, we curate two high-quality and large-scale\ndatasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K\nfor reinforcement learning with verifiable rewards (RLVR), respectively.\nBuilding upon them, we develop a unified and interpretable FER foundation model\ntermed UniFER-7B, which outperforms many open-sourced and closed-source\ngeneralist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).", "AI": {"tldr": "本文提出了FERBench基准测试，评估了多模态大语言模型（MLLMs）在面部表情识别（FER）任务上的表现，并揭示了它们在推理和可解释性方面的局限性。为解决此问题，作者引入了后训练策略和两个高质量数据集，并基于此开发了统一且可解释的FER基础模型UniFER-7B，其性能超越了现有许多通用MLLMs。", "motivation": "尽管多模态大语言模型（MLLMs）在许多领域取得了成功，并且有将FER任务转换为视觉问答（VQA）格式以统一处理的趋势，但它们在面部表情识别（FER）任务上的性能仍未得到充分探索，并且在推理和可解释性方面存在显著局限性。", "method": "本文首先构建了FERBench，一个系统性基准，涵盖20个最先进的MLLMs和4个广泛使用的FER数据集，以评估MLLMs的FER性能。其次，为增强MLLMs的面部表情推理能力，作者引入了后训练策略，并为此策划了两个高质量大规模数据集：UniFER-CoT-230K（用于冷启动初始化）和UniFER-RLVR-360K（用于可验证奖励的强化学习）。最后，基于这些策略和数据集，开发了一个统一且可解释的FER基础模型UniFER-7B。", "result": "研究结果表明，虽然MLLMs在FER分类任务上表现良好，但它们在推理和可解释性方面仍面临显著局限性。通过提出的后训练策略和UniFER-7B模型，实现了性能的显著提升，UniFER-7B超越了许多开源和闭源的通用MLLMs（例如Gemini-2.5-Pro和Qwen2.5-VL-72B）。", "conclusion": "MLLMs在面部表情识别任务中具有潜力，但需要专门的后训练策略来解决其在推理和可解释性方面的不足。UniFER-7B的成功证明了通过定制数据集和训练方法，可以构建出性能优异且可解释的FER基础模型。"}}
{"id": "2511.01415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01415", "abs": "https://arxiv.org/abs/2511.01415", "authors": ["Amrapali Pednekar", "Álvaro Garrido-Pérez", "Yara Khaluf", "Pieter Simoens"], "title": "Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm", "comment": "Accepted at CogInterp workshop @ NeurIPS 2025", "summary": "This study explores the interference in temporal processing within a\ndual-task paradigm from an artificial intelligence (AI) perspective. In this\ncontext, the dual-task setup is implemented as a simplified version of the\nOvercooked environment with two variations, single task (T) and dual task\n(T+N). Both variations involve an embedded time production task, but the dual\ntask (T+N) additionally involves a concurrent number comparison task. Two deep\nreinforcement learning (DRL) agents were separately trained for each of these\ntasks. These agents exhibited emergent behavior consistent with human timing\nresearch. Specifically, the dual task (T+N) agent exhibited significant\noverproduction of time relative to its single task (T) counterpart. This result\nwas consistent across four target durations. Preliminary analysis of neural\ndynamics in the agents' LSTM layers did not reveal any clear evidence of a\ndedicated or intrinsic timer. Hence, further investigation is needed to better\nunderstand the underlying time-keeping mechanisms of the agents and to provide\ninsights into the observed behavioral patterns. This study is a small step\ntowards exploring parallels between emergent DRL behavior and behavior observed\nin biological systems in order to facilitate a better understanding of both.", "AI": {"tldr": "本研究使用深度强化学习（DRL）代理在一个简化的双任务环境中模拟人类时间处理干扰。结果显示，双任务代理在时间生产上表现出显著的过度生产，与人类行为一致，但其LSTM层未显示明确的专用计时器。", "motivation": "本研究旨在从人工智能（AI）的角度探索双任务范式中时间处理的干扰，并探讨新兴的DRL行为与生物系统观察到的行为之间的相似性，以促进对两者的更好理解。", "method": "研究采用简化的Overcooked环境作为双任务设置，包含两种变体：单任务（T）和双任务（T+N）。两种变体都包含一个嵌入式时间生产任务，而双任务（T+N）额外包含一个并发的数字比较任务。分别训练了两个深度强化学习（DRL）代理来执行这些任务。对代理的LSTM层进行了初步的神经动力学分析。", "result": "DRL代理表现出的紧急行为与人类计时研究一致。具体而言，双任务（T+N）代理相对于单任务（T）代理表现出显著的时间过度生产，这一结果在四个目标持续时间中均保持一致。对代理LSTM层的初步神经动力学分析未发现任何明确的专用或内在计时器证据。", "conclusion": "DRL代理能够复现人类在双任务情境下时间处理的干扰现象（时间过度生产）。尽管如此，仍需进一步研究以更好地理解代理潜在的计时机制。这项研究为探索DRL与生物系统行为之间的相似性迈出了小步，有助于加深对两者的理解。"}}
{"id": "2511.01066", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01066", "abs": "https://arxiv.org/abs/2511.01066", "authors": ["Stephan Oepen", "Nikolay Arefev", "Mikko Aulamo", "Marta Bañón", "Maja Buljan", "Laurie Burchell", "Lucas Charpentier", "Pinzhen Chen", "Mariya Fedorova", "Ona de Gibert", "Barry Haddow", "Jan Hajič", "Jindrič Helcl", "Andrey Kutuzov", "Zihao Li", "Risto Luukkonen", "Bhavitvya Malik", "Vladislav Mikhailov", "Amanda Myntti", "Dayyán O'Brien", "Lucie Poláková", "Sampo Pyysalo", "Gema Ramírez Sánchez", "Janine Siewert", "Pavel Stepachev", "Jörg Tiedemann", "Teemu Vahtola", "Fedor Vitiugin", "Tea Vojtěchová", "Jaume Zaragoza"], "title": "HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models", "comment": null, "summary": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. At 30 trillion tokens, this is likely the largest\ngenerally available multilingual collection of LLM pre-training data. These\ndatasets are derived from web crawls from different sources and accompanied\nwith a complete, open-source pipeline for document selection from web archives,\ntext extraction from HTML, language identification for noisy texts, exact and\nnear-deduplication, annotation with, among others, register labels, text\nquality estimates, and personally identifiable information; and final selection\nand filtering. We report on data quality probes through contrastive and\nanalytical statistics, through manual inspection of samples for 24 languages,\nand through end-to-end evaluation of various language model architectures\ntrained on this data. For multilingual LLM evaluation, we provide a\ncomprehensive collection of benchmarks for nine European languages, with\nspecial emphasis on natively created tasks, mechanisms to mitigate prompt\nsensitivity, and refined normalization and aggregation of scores. Additionally,\nwe train and evaluate a family of 57 monolingual encoder-decoder models, as\nwell as a handful of monolingual GPT-like reference models. Besides the\nmonolingual data and models, we also present a very large collection of\nparallel texts automatically mined from this data, together with a novel\nparallel corpus synthesized via machine translation.", "AI": {"tldr": "该论文介绍了一个包含30万亿token的超大规模、高质量、多语言（近200种）LLM预训练数据集及其完整的开源处理流程，并提供了数据质量评估、多语言基准测试和训练模型。", "motivation": "动机是为LLM预训练提供开放、超大规模、高质量且标注丰富多样的文本数据集，以支持近200种语言，并解决现有预训练数据可能存在的规模、质量和语言多样性不足的问题。", "method": "方法包括：从不同来源的网络爬虫中获取数据；开发一套完整的开源处理流程，用于文档选择、文本提取、语言识别、去重、注册标签/文本质量/PII标注以及最终筛选；通过对比分析统计、24种语言的手动抽样检查和端到端模型训练评估来探测数据质量；提供针对9种欧洲语言的综合多语言LLM基准测试；训练并评估了57个单语编解码器模型和少量单语GPT-like参考模型；自动挖掘并合成平行语料库。", "result": "结果是：构建了一个包含30万亿token的、可能是最大的多语言LLM预训练数据集；发布了一个完整的开源数据处理管道；通过多种方法验证了数据质量；提供了针对9种欧洲语言的综合多语言LLM评估基准；训练并评估了57个单语编解码器模型和一些单语GPT-like模型；构建了一个大型平行文本集合（包括自动挖掘和机器翻译合成）。", "conclusion": "该研究提供了一个前所未有的开放、超大规模、高质量、多语言LLM预训练数据集和配套的开源工具链，以及全面的评估基准和参考模型，极大地推动了多语言LLM的开发和研究。"}}
{"id": "2511.01090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01090", "abs": "https://arxiv.org/abs/2511.01090", "authors": ["Vlad Negoita", "Mihai Masala", "Traian Rebedea"], "title": "Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering", "comment": null, "summary": "Large Language Models (LLMs) have recently exploded in popularity, often\nmatching or outperforming human abilities on many tasks. One of the key factors\nin training LLMs is the availability and curation of high-quality data. Data\nquality is especially crucial for under-represented languages, where\nhigh-quality corpora are scarce. In this work we study the characteristics and\ncoverage of Romanian pretraining corpora and we examine how they differ from\nEnglish data. By training a lightweight multitask model on carefully\nLLM-annotated Romanian texts, we are able to analyze and perform multi-level\nfiltering (e.g., educational value, topic, format) to generate high-quality\npretraining datasets. Our experiments show noteworthy trends in the topics\npresent in Romanian and English data, while also proving the effectiveness of\nfiltering data through improved LLM pretraining performance across multiple\nbenchmarks.", "AI": {"tldr": "本研究通过使用轻量级多任务模型和LLM标注的文本，对罗马尼亚语预训练语料库进行了分析和多层过滤，成功生成了高质量数据集，并显著提升了LLM的预训练性能。", "motivation": "大型语言模型（LLMs）的训练高度依赖高质量数据，而对于罗马尼亚语等代表性不足的语言，高质量语料库稀缺，这促使研究者探索如何改善其数据质量。", "method": "研究了罗马尼亚语预训练语料库的特征和覆盖范围，并与英语数据进行比较。训练了一个轻量级多任务模型，利用LLM标注的罗马尼亚语文本进行多层过滤（例如，教育价值、主题、格式），以生成高质量预训练数据集。", "result": "发现了罗马尼亚语和英语数据在主题上的显著趋势。通过数据过滤，LLM在多个基准测试上的预训练性能得到了显著提升，证明了该方法的有效性。", "conclusion": "本研究成功展示了一种通过LLM标注和多层过滤来生成高质量预训练数据集的方法，尤其适用于资源稀缺的语言，并能有效提高LLM的预训练表现。"}}
{"id": "2511.01437", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01437", "abs": "https://arxiv.org/abs/2511.01437", "authors": ["Elian Neppel", "Shamistan Karimov", "Ashutosh Mishra", "Gustavo Hernan Diaz Huenupan", "Hazal Gozbasi", "Kentaro Uno", "Shreya Santra", "Kazuya Yoshida"], "title": "Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots", "comment": "6 pages, 8 figures. Accepted at ISPARO 2025", "summary": "This paper presents the software architecture and deployment strategy behind\nthe MoonBot platform: a modular space robotic system composed of heterogeneous\ncomponents distributed across multiple computers, networks and ultimately\ncelestial bodies. We introduce a principled approach to distributed,\nheterogeneous modularity, extending modular robotics beyond physical\nreconfiguration to software, communication and orchestration. We detail the\narchitecture of our system that integrates component-based design, a\ndata-oriented communication model using ROS2 and Zenoh, and a deployment\norchestrator capable of managing complex multi-module assemblies. These\nabstractions enable dynamic reconfiguration, decentralized control, and\nseamless collaboration between numerous operators and modules. At the heart of\nthis system lies our open-source Motion Stack software, validated by months of\nfield deployment with self-assembling robots, inter-robot cooperation, and\nremote operation. Our architecture tackles the significant hurdles of modular\nrobotics by significantly reducing integration and maintenance overhead, while\nremaining scalable and robust. Although tested with space in mind, we propose\ngeneralizable patterns for designing robotic systems that must scale across\ntime, hardware, teams and operational environments.", "AI": {"tldr": "本文介绍了MoonBot平台（一个模块化空间机器人系统）的软件架构和部署策略，旨在通过分布式、异构模块化方法解决模块化机器人面临的集成和维护挑战。", "motivation": "模块化机器人系统在物理重构之外，还面临软件、通信和编排方面的分布式、异构模块化挑战，且集成和维护成本高昂。研究旨在克服这些重大障碍。", "method": "研究采用了一种分布式、异构模块化的原则性方法，扩展了模块化机器人概念。具体方法包括：组件化设计、基于ROS2和Zenoh的数据导向通信模型，以及一个能够管理复杂多模块装配的部署编排器。核心是开源的Motion Stack软件。", "result": "所提出的架构实现了动态重构、去中心化控制以及操作员和模块之间的无缝协作。该系统通过数月的现场部署（包括自组装机器人、机器人间协作和远程操作）得到验证，显著降低了集成和维护开销，同时保持了可扩展性和鲁棒性。", "conclusion": "该架构成功解决了模块化机器人面临的重大难题，并为设计可在时间、硬件、团队和操作环境之间扩展的通用机器人系统提供了可推广的模式。"}}
{"id": "2511.01101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01101", "abs": "https://arxiv.org/abs/2511.01101", "authors": ["Marek Strong", "Andreas Vlachos"], "title": "TSVer: A Benchmark for Fact Verification Against Time-Series Evidence", "comment": "Accepted to EMNLP 2025", "summary": "Reasoning over temporal and numerical data, such as time series, is a crucial\naspect of fact-checking. While many systems have recently been developed to\nhandle this form of evidence, their evaluation remains limited by existing\ndatasets, which often lack structured evidence, provide insufficient\njustifications for verdicts, or rely on synthetic claims. In this paper, we\nintroduce TSVer, a new benchmark dataset for fact verification focusing on\ntemporal and numerical reasoning with time-series evidence. TSVer contains 287\nreal-world claims sourced from 38 fact-checking organizations and a curated\ndatabase of 400 time series covering diverse domains. Each claim is annotated\nwith time frames across all pertinent time series, along with a verdict and\njustifications reflecting how the evidence is used to reach the verdict. Using\nan LLM-assisted multi-step annotation process, we improve the quality of our\nannotations and achieve an inter-annotator agreement of kappa=0.745 on\nverdicts. We also develop a baseline for verifying claims against time-series\nevidence and show that even the state-of-the-art reasoning models like\nGemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score\non verdicts and an Ev2R score of 48.63 on verdict justifications.", "AI": {"tldr": "本文提出了一个名为TSVer的新基准数据集，用于基于时间序列证据进行事实核查，并展示了现有最先进模型在该任务上的不足。", "motivation": "现有用于处理时间与数值数据的证据核查系统，其评估受限于现有数据集，这些数据集通常缺乏结构化证据、判决理由不足或依赖合成声明。", "method": "引入了TSVer数据集，包含287个真实世界声明和400个时间序列。通过LLM辅助的多步标注过程，为每个声明标注了时间范围、判决结果和理由。同时开发了一个基线模型来验证声明。", "result": "TSVer数据集的标注质量高，判决结果的标注者间一致性Kappa值为0.745。即使是Gemini-2.5-Pro等最先进的推理模型，在时间序列数据上的表现也面临挑战，判决准确率为63.37%，判决理由的Ev2R分数为48.63。", "conclusion": "TSVer是一个具有挑战性的新基准数据集，突显了当前模型在处理时间序列的临时和数值推理方面的局限性，并为未来的研究提供了方向。"}}
{"id": "2511.01425", "categories": ["cs.AI", "cs.CV", "I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.01425", "abs": "https://arxiv.org/abs/2511.01425", "authors": ["Yuhang Huang", "Zekai Lin", "Fan Zhong", "Lei Liu"], "title": "Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis", "comment": "12 pages, 3 figures. Under review at the Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2026", "summary": "Explanations for AI models in high-stakes domains like medicine often lack\nverifiability, which can hinder trust. To address this, we propose an\ninteractive agent that produces explanations through an auditable sequence of\nactions. The agent learns a policy to strategically seek external visual\nevidence to support its diagnostic reasoning. This policy is optimized using\nreinforcement learning, resulting in a model that is both efficient and\ngeneralizable. Our experiments show that this action-based reasoning process\nsignificantly improves calibrated accuracy, reducing the Brier score by 18\\%\ncompared to a non-interactive baseline. To validate the faithfulness of the\nagent's explanations, we introduce a causal intervention method. By masking the\nvisual evidence the agent chooses to use, we observe a measurable degradation\nin its performance ($\\Delta$Brier=+0.029), confirming that the evidence is\nintegral to its decision-making process. Our work provides a practical\nframework for building AI systems with verifiable and faithful reasoning\ncapabilities.", "AI": {"tldr": "本文提出了一种交互式智能体，通过可审计的行动序列和策略性地寻求外部视觉证据来生成AI模型解释，尤其适用于医疗等高风险领域，旨在提高解释的可验证性、信任度和模型准确性。", "motivation": "在医疗等高风险领域，AI模型的解释往往缺乏可验证性，这会阻碍人们对AI系统的信任。本研究旨在解决这一问题。", "method": "研究提出了一种交互式智能体，通过可审计的行动序列来生成解释。该智能体学习一个策略，以策略性地寻求外部视觉证据来支持其诊断推理。此策略通过强化学习进行优化。此外，为验证解释的忠实性，引入了一种因果干预方法，通过遮蔽智能体选择使用的视觉证据来评估其性能下降情况。", "result": "基于行动的推理过程显著提高了校准准确性，与非交互基线相比，布里尔分数降低了18%。通过遮蔽智能体选择的视觉证据，观察到其性能出现可测量的下降（ΔBrier=+0.029），证实了证据对其决策过程的重要性。", "conclusion": "本研究为构建具有可验证和忠实推理能力的AI系统提供了一个实用的框架。"}}
{"id": "2511.01444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01444", "abs": "https://arxiv.org/abs/2511.01444", "authors": ["Huiting Huang", "Tieliang Gong", "Kai He", "Jialun Wu", "Erik Cambria", "Mengling Feng"], "title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck", "comment": null, "summary": "Multimodal sentiment analysis has received significant attention across\ndiverse research domains. Despite advancements in algorithm design, existing\napproaches suffer from two critical limitations: insufficient learning of\nnoise-contaminated unimodal data, leading to corrupted cross-modal\ninteractions, and inadequate fusion of multimodal representations, resulting in\ndiscarding discriminative unimodal information while retaining multimodal\nredundant information. To address these challenges, this paper proposes a\nDouble Information Bottleneck (DIB) strategy to obtain a powerful, unified\ncompact multimodal representation. Implemented within the framework of low-rank\nRenyi's entropy functional, DIB offers enhanced robustness against diverse\nnoise sources and computational tractability for high-dimensional data, as\ncompared to the conventional Shannon entropy-based methods. The DIB comprises\ntwo key modules: 1) learning a sufficient and compressed representation of\nindividual unimodal data by maximizing the task-relevant information and\ndiscarding the superfluous information, and 2) ensuring the discriminative\nability of multimodal representation through a novel attention bottleneck\nfusion mechanism. Consequently, DIB yields a multimodal representation that\neffectively filters out noisy information from unimodal data while capturing\ninter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,\nCH-SIMS, and MVSA-Single validate the effectiveness of our method. The model\nachieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score\non CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it\nshows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI\nrespectively.", "AI": {"tldr": "本文提出了一种双信息瓶颈（DIB）策略，用于多模态情感分析，旨在通过学习去噪的单模态表示和有效的注意力瓶颈融合机制，获得鲁棒且紧凑的统一多模态表示。", "motivation": "现有方法在多模态情感分析中存在两个关键限制：1) 对受噪声污染的单模态数据学习不足，导致跨模态交互受损；2) 多模态表示融合不充分，导致判别性单模态信息被丢弃，同时保留了多模态冗余信息。", "method": "本文提出了双信息瓶颈（DIB）策略，基于低秩Renyi熵泛函，以增强对噪声的鲁棒性和计算效率。DIB包含两个核心模块：1) 通过最大化任务相关信息和丢弃冗余信息，学习个体单模态数据的充分压缩表示；2) 通过新颖的注意力瓶颈融合机制，确保多模态表示的判别能力。", "result": "在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上进行了广泛实验。DIB在CMU-MOSI上Acc-7指标达到47.4%的准确率，在CH-SIMS上F1分数达到81.63%，优于次优基线1.19%。在噪声环境下，CMU-MOSI和CMU-MOSEI的性能下降分别仅为0.36%和0.29%。", "conclusion": "DIB策略能够有效过滤单模态数据中的噪声信息，同时捕获模态间的互补性，从而生成一个强大、统一且紧凑的多模态表示，显著提升了多模态情感分析的性能和对噪声的鲁棒性。"}}
{"id": "2511.01166", "categories": ["cs.CL", "cs.SE", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01166", "abs": "https://arxiv.org/abs/2511.01166", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Minghua He", "Leyi Pan", "Zhaoyang Liu", "Bolin Ding", "Ying Li"], "title": "MicroRemed: Benchmarking LLMs in Microservices Remediation", "comment": "24 pages, 13 figures, 5 tables", "summary": "Large Language Models (LLMs) integrated with agent-based reasoning frameworks\nhave recently shown strong potential for autonomous decision-making and\nsystem-level operations. One promising yet underexplored direction is\nmicroservice remediation, where the goal is to automatically recover faulty\nmicroservice systems. Existing approaches, however, still rely on human-crafted\nprompts from Site Reliability Engineers (SREs), with LLMs merely converting\ntextual instructions into executable code. To advance research in this area, we\nintroduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end\nmicroservice remediation, where models must directly generate executable\nAnsible playbooks from diagnosis reports to restore system functionality. We\nfurther propose ThinkRemed, a multi-agent framework that emulates the\nreflective and perceptive reasoning of SREs. Experimental results show that\nMicroRemed presents substantial challenges to current LLMs, while ThinkRemed\nimproves end-to-end remediation performance through iterative reasoning and\nsystem reflection. The benchmark is available at\nhttps://github.com/LLM4AIOps/MicroRemed.", "AI": {"tldr": "本文提出了MicroRemed，一个用于评估LLM在微服务修复中端到端能力的基准，并引入了ThinkRemed，一个模拟SRE反思和感知推理的多智能体框架，以提高自动化修复性能。", "motivation": "现有LLM在微服务修复中仍依赖SRE人工编写的提示，LLM仅将文本指令转换为可执行代码，未能实现从诊断报告直接生成可执行修复方案的端到端自动化，这一方向有巨大潜力但尚未被充分探索。", "method": "1. 引入MicroRemed基准：首次端到端评估LLM在微服务修复中的能力，要求模型直接从诊断报告生成可执行的Ansible playbook以恢复系统功能。 2. 提出ThinkRemed框架：一个多智能体框架，模拟SRE的反思和感知推理过程。", "result": "实验结果表明，MicroRemed基准对当前LLM提出了重大挑战。而ThinkRemed框架通过迭代推理和系统反思，显著提升了端到端的修复性能。", "conclusion": "该研究通过引入MicroRemed基准和ThinkRemed多智能体框架，推动了LLM在微服务自动化修复领域的进展，展示了现有LLM面临的挑战以及通过模拟SRE推理过程提高性能的潜力。"}}
{"id": "2511.00419", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00419", "abs": "https://arxiv.org/abs/2511.00419", "authors": ["Thanh Hieu Cao", "Trung Khang Tran", "Gia Thinh Pham", "Tuong Nghiem Diep", "Thanh Binh Nguyen"], "title": "LGCA: Enhancing Semantic Representation via Progressive Expansion", "comment": "15 pages, 5 figures, to appear in SoICT 2025", "summary": "Recent advancements in large-scale pretraining in natural language processing\nhave enabled pretrained vision-language models such as CLIP to effectively\nalign images and text, significantly improving performance in zero-shot image\nclassification tasks. Subsequent studies have further demonstrated that\ncropping images into smaller regions and using large language models to\ngenerate multiple descriptions for each caption can further enhance model\nperformance. However, due to the inherent sensitivity of CLIP, random image\ncrops can introduce misinformation and bias, as many images share similar\nfeatures at small scales. To address this issue, we propose\nLocalized-Globalized Cross-Alignment (LGCA), a framework that first captures\nthe local features of an image and then repeatedly selects the most salient\nregions and expands them. The similarity score is designed to incorporate both\nthe original and expanded images, enabling the model to capture both local and\nglobal features while minimizing misinformation. Additionally, we provide a\ntheoretical analysis demonstrating that the time complexity of LGCA remains the\nsame as that of the original model prior to the repeated expansion process,\nhighlighting its efficiency and scalability. Extensive experiments demonstrate\nthat our method substantially improves zero-shot performance across diverse\ndatasets, outperforming state-of-the-art baselines.", "AI": {"tldr": "本文提出Localized-Globalized Cross-Alignment (LGCA) 框架，通过捕获局部特征并迭代扩展显著区域，同时结合原始和扩展图像的相似度得分，有效解决了CLIP模型中随机图像裁剪引入的误报和偏差问题，显著提升了零样本图像分类性能。", "motivation": "现有研究表明，对图像进行裁剪并利用大型语言模型生成多重描述可以增强CLIP模型的性能。然而，CLIP固有的敏感性使得随机图像裁剪可能引入误报和偏差，因为许多小尺度区域共享相似特征。", "method": "本文提出了Localized-Globalized Cross-Alignment (LGCA) 框架。该框架首先捕获图像的局部特征，然后反复选择并扩展最显著的区域。相似度得分的设计结合了原始图像和扩展后的图像，使得模型能够同时捕获局部和全局特征，并最小化误报。此外，论文还提供了理论分析，证明LGCA的时间复杂度在重复扩展过程之前与原始模型相同。", "result": "广泛的实验表明，LGCA方法显著提高了在不同数据集上的零样本性能，超越了现有最先进的基线模型。理论分析也证实了其效率和可扩展性，其时间复杂度与原始模型在扩展前保持一致。", "conclusion": "LGCA框架通过有效整合局部和全局特征，并最小化由随机裁剪引入的误报，显著提升了CLIP模型在零样本图像分类任务中的性能。该方法在保持效率的同时，实现了对当前最先进基线的超越。"}}
{"id": "2511.01472", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01472", "abs": "https://arxiv.org/abs/2511.01472", "authors": ["Sarthak Mishra", "Rishabh Dev Yadav", "Avirup Das", "Saksham Gupta", "Wei Pan", "Spandan Roy"], "title": "AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models", "comment": null, "summary": "The rapid progress of vision--language models (VLMs) has sparked growing\ninterest in robotic control, where natural language can express the operation\ngoals while visual feedback links perception to action. However, directly\ndeploying VLM-driven policies on aerial manipulators remains unsafe and\nunreliable since the generated actions are often inconsistent,\nhallucination-prone, and dynamically infeasible for flight. In this work, we\npresent AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial\nmanipulation by separating high-level reasoning from low-level control, without\nany task-specific fine-tuning. Our framework encodes natural language\ninstructions, task context, and safety constraints into a structured prompt\nthat guides the model to generate a step-by-step reasoning trace in natural\nlanguage. This reasoning output is used to select from a predefined library of\ndiscrete, flight-safe skills, ensuring interpretable and temporally consistent\nexecution. By decoupling symbolic reasoning from physical action, AERMANI-VLM\nmitigates hallucinated commands and prevents unsafe behavior, enabling robust\ntask completion. We validate the framework in both simulation and hardware on\ndiverse multi-step pick-and-place tasks, demonstrating strong generalization to\npreviously unseen commands, objects, and environments.", "AI": {"tldr": "AERMANI-VLM 是一种新框架，通过将高层推理与低层控制分离，使预训练的视觉-语言模型（VLMs）能够安全可靠地应用于空中机械臂操作，无需任务特定微调。", "motivation": "视觉-语言模型（VLMs）在机器人控制中潜力巨大，但直接将其部署到空中机械臂上存在不安全和不可靠的问题，因为生成的动作常不一致、易产生幻觉且动态上不可行。", "method": "AERMANI-VLM 框架通过将自然语言指令、任务上下文和安全约束编码成结构化提示，引导 VLM 生成逐步的自然语言推理轨迹。然后，该推理结果用于从预定义的离散、飞行安全技能库中选择动作，从而实现可解释且时间上一致的执行。该方法将符号推理与物理动作解耦，且无需任务特定的微调。", "result": "该框架成功缓解了幻觉指令并防止了不安全行为，实现了鲁棒的任务完成。在模拟和硬件上对多步抓取-放置任务进行了验证，展示了对未见过的指令、物体和环境的强大泛化能力。", "conclusion": "AERMANI-VLM 首次实现了将预训练 VLM 适应于空中机械臂操作，通过解耦高层推理与低层控制，确保了安全、可靠、可解释且具有强大泛化能力的任务执行。"}}
{"id": "2511.00427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00427", "abs": "https://arxiv.org/abs/2511.00427", "authors": ["Daichi Zhang", "Tong Zhang", "Jianmin Bao", "Shiming Ge", "Sabine Süsstrunk"], "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection", "comment": null, "summary": "With the rapid development of generative models, detecting generated fake\nimages to prevent their malicious use has become a critical issue recently.\nExisting methods frame this challenge as a naive binary image classification\ntask. However, such methods focus only on visual clues, yielding trained\ndetectors susceptible to overfitting specific image patterns and incapable of\ngeneralizing to unseen models. In this paper, we address this issue from a\nmulti-modal perspective and find that fake images cannot be properly aligned\nwith corresponding captions compared to real images. Upon this observation, we\npropose a simple yet effective detector termed ITEM by leveraging the\nimage-text misalignment in a joint visual-language space as discriminative\nclues. Specifically, we first measure the misalignment of the images and\ncaptions in pre-trained CLIP's space, and then tune a MLP head to perform the\nusual detection task. Furthermore, we propose a hierarchical misalignment\nscheme that first focuses on the whole image and then each semantic object\ndescribed in the caption, which can explore both global and fine-grained local\nsemantic misalignment as clues. Extensive experiments demonstrate the\nsuperiority of our method against other state-of-the-art competitors with\nimpressive generalization and robustness on various recent generative models.", "AI": {"tldr": "本文提出了一种名为ITEM的简单而有效的检测器，通过利用图像与文本描述在多模态空间中的语义错位来检测生成的假图像，解决了现有方法泛化能力差的问题。", "motivation": "随着生成模型的发展，检测生成的假图像以防止恶意使用变得至关重要。现有方法将此视为二元图像分类任务，但它们只关注视觉线索，容易过拟合特定模式，且无法泛化到未见过的生成模型。", "method": "本文从多模态角度出发，观察到假图像与对应描述的对齐程度不如真实图像。基于此，提出了ITEM检测器，利用视觉-语言联合空间中的图像-文本错位作为判别线索。具体而言，首先在预训练CLIP空间中测量图像与描述的错位，然后微调一个MLP头部进行检测。此外，还提出了一个分层错位方案，结合全局图像和局部语义对象的错位信息。", "result": "广泛的实验表明，ITEM方法在各种最新生成模型上，相比其他最先进的竞争对手展现出卓越的泛化能力和鲁棒性。", "conclusion": "通过利用图像-文本在多模态空间中的语义错位，可以有效且普遍地检测生成的假图像，克服了传统视觉线索方法的局限性，提升了检测器的泛化能力和鲁棒性。"}}
{"id": "2511.01476", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01476", "abs": "https://arxiv.org/abs/2511.01476", "authors": ["Cankut Bora Tuncer", "Marc Toussaint", "Ozgur S. Oguz"], "title": "MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments", "comment": "8 pages, 8 figures, website:https://sites.google.com/view/mo-segman/", "summary": "In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided\nManipulation planner for highly constrained rearrangement problems. MO-SeGMan\ngenerates object placement sequences that minimize both replanning per object\nand robot travel distance while preserving critical dependency structures with\na lazy evaluation method. To address highly cluttered, non-monotone scenarios,\nwe propose a Selective Guided Forward Search (SGFS) that efficiently relocates\nonly critical obstacles and to feasible relocation points. Furthermore, we\nadopt a refinement method for adaptive subgoal selection to eliminate\nunnecessary pick-and-place actions, thereby improving overall solution quality.\nExtensive evaluations on nine benchmark rearrangement tasks demonstrate that\nMO-SeGMan generates feasible motion plans in all cases, consistently achieving\nfaster solution times and superior solution quality compared to the baselines.\nThese results highlight the robustness and scalability of the proposed\nframework for complex rearrangement planning problems.", "AI": {"tldr": "本文提出MO-SeGMan，一个多目标序列化引导操作规划器，用于高度受限的重排问题，通过优化重规划和机器人行程，并引入选择性引导前向搜索和自适应子目标选择，显著提升了规划效率和质量。", "motivation": "解决高度受限的重排问题，特别是杂乱、非单调的场景，这些场景需要高效地处理物体依赖性、最小化机器人动作并生成可行的运动规划。", "method": "MO-SeGMan作为多目标序列化引导操作规划器，旨在最小化每个物体的重新规划次数和机器人行程距离，同时通过惰性评估保持关键依赖结构。它引入了选择性引导前向搜索（SGFS）来高效重新定位关键障碍物，并采用细化方法进行自适应子目标选择，以消除不必要的抓取-放置动作。", "result": "在九个基准重排任务上的广泛评估表明，MO-SeGMan在所有情况下都能生成可行的运动规划，并且与基线相比，始终实现更快的求解时间和更优的解决方案质量。", "conclusion": "MO-SeGMan框架在解决复杂重排规划问题方面展现出卓越的鲁棒性和可扩展性。"}}
{"id": "2511.00446", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00446", "abs": "https://arxiv.org/abs/2511.00446", "authors": ["Xin Yao", "Haiyang Zhao", "Yimin Chen", "Jiawei Guo", "Kecheng Huang", "Ming Zhao"], "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training", "comment": "Accepted by NeurIPS 2025", "summary": "The Contrastive Language-Image Pretraining (CLIP) model has significantly\nadvanced vision-language modeling by aligning image-text pairs from large-scale\nweb data through self-supervised contrastive learning. Yet, its reliance on\nuncurated Internet-sourced data exposes it to data poisoning and backdoor\nrisks. While existing studies primarily investigate image-based attacks, the\ntext modality, which is equally central to CLIP's training, remains\nunderexplored. In this work, we introduce ToxicTextCLIP, a framework for\ngenerating high-quality adversarial texts that target CLIP during the\npre-training phase. The framework addresses two key challenges: semantic\nmisalignment caused by background inconsistency with the target class, and the\nscarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively\napplies: 1) a background-aware selector that prioritizes texts with background\ncontent aligned to the target class, and 2) a background-driven augmenter that\ngenerates semantically coherent and diverse poisoned samples. Extensive\nexperiments on classification and retrieval tasks show that ToxicTextCLIP\nachieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while\nbypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be\naccessed via https://github.com/xinyaocse/ToxicTextCLIP/.", "AI": {"tldr": "本文提出ToxicTextCLIP框架，通过生成高质量的对抗性文本，在预训练阶段对CLIP模型进行数据投毒和后门攻击，并成功绕过现有防御。", "motivation": "CLIP模型依赖于未经验证的网络数据进行预训练，使其容易受到数据投毒和后门攻击。现有研究主要关注基于图像的攻击，而作为CLIP训练核心的文本模态的攻击风险尚未得到充分探索。", "method": "ToxicTextCLIP框架通过迭代应用以下两个组件来解决语义错位和背景一致文本稀缺的问题：1) 背景感知选择器，优先选择与目标类别背景内容一致的文本；2) 背景驱动增强器，生成语义连贯且多样化的投毒样本。", "result": "在分类和检索任务上的大量实验表明，ToxicTextCLIP实现了高达95.83%的投毒成功率和98.68%的后门Hit@1，并成功绕过了RoCLIP、CleanCLIP和SafeCLIP等防御措施。", "conclusion": "ToxicTextCLIP证明了通过高质量对抗性文本对CLIP模型进行有效投毒和后门攻击的可能性，揭示了CLIP在文本模态上的一个重要安全漏洞。"}}
{"id": "2511.01181", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01181", "abs": "https://arxiv.org/abs/2511.01181", "authors": ["Emaad Manzoor", "Eva Ascarza", "Oded Netzer"], "title": "Learning When to Quit in Sales Conversations", "comment": null, "summary": "Salespeople frequently face the dynamic screening decision of whether to\npersist in a conversation or abandon it to pursue the next lead. Yet, little is\nknown about how these decisions are made, whether they are efficient, or how to\nimprove them. We study these decisions in the context of high-volume outbound\nsales where leads are ample, but time is scarce and failure is common. We\nformalize the dynamic screening decision as an optimal stopping problem and\ndevelop a generative language model-based sequential decision agent - a\nstopping agent - that learns whether and when to quit conversations by\nimitating a retrospectively-inferred optimal stopping policy. Our approach\nhandles high-dimensional textual states, scales to large language models, and\nworks with both open-source and proprietary language models. When applied to\ncalls from a large European telecommunications firm, our stopping agent reduces\nthe time spent on failed calls by 54% while preserving nearly all sales;\nreallocating the time saved increases expected sales by up to 37%. Upon\nexamining the linguistic cues that drive salespeople's quitting decisions, we\nfind that they tend to overweight a few salient expressions of consumer\ndisinterest and mispredict call failure risk, suggesting cognitive bounds on\ntheir ability to make real-time conversational decisions. Our findings\nhighlight the potential of artificial intelligence algorithms to correct\ncognitively-bounded human decisions and improve salesforce efficiency.", "AI": {"tldr": "本研究通过将销售人员的动态筛选决策建模为最优停止问题，开发了一个基于生成式语言模型的停止代理。该代理通过模仿最优停止策略，显著减少了无效通话时间，提高了销售效率，并揭示了人类销售人员决策中的认知偏差。", "motivation": "销售人员在动态筛选决策（继续对话或放弃寻找下一个潜在客户）方面面临挑战，但关于这些决策如何制定、是否高效以及如何改进的知识却很少。在高频外呼销售场景中，潜在客户充足但时间稀缺且失败常见，因此优化这些决策至关重要。", "method": "研究将动态筛选决策形式化为最优停止问题，并开发了一个基于生成式语言模型的序列决策代理（停止代理）。该代理通过模仿追溯推断的最优停止策略，学习何时以及如何放弃对话。该方法能够处理高维文本状态，可扩展到大型语言模型，并支持开源和专有语言模型。研究将此方法应用于一家大型欧洲电信公司的真实通话数据。", "result": "停止代理将无效通话所花费的时间减少了54%，同时几乎保留了所有销售额。重新分配节省的时间可使预期销售额增加高达37%。对销售人员放弃决策的语言线索分析发现，他们倾向于过分看重消费者不感兴趣的几个突出表达，并错误预测通话失败风险，这表明他们在实时对话决策中存在认知限制。", "conclusion": "研究结果强调了人工智能算法在纠正人类认知受限决策和提高销售团队效率方面的巨大潜力。"}}
{"id": "2511.00396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00396", "abs": "https://arxiv.org/abs/2511.00396", "authors": ["Long Li", "Shuichen Ji", "Ziyang Luo", "Nian Liu", "Dingwen Zhang", "Junwei Han"], "title": "CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks", "comment": "14 pages,10 figures", "summary": "We present the first unified framework that jointly handles three\noperationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting\neach as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model\n(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT\nquality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a\nlightweight single-sample algorithm that leverages the discrepancy between\nreward and model confidence as a per-sample advantage signal. This design\nnaturally focuses updates on informative responses while eliminating group\nsampling, thereby addressing GRPO's key limitations: confidence-agnostic\nlearning, signal dilution, and prohibitive computational overhead. We also\nintroduce an \"output-to-reasoning\" strategy to construct high-fidelity SFT data\nthat ensures logical consistency with ground-truth masks. Experiments show our\nmodel matches or outperforms specialized SOTA methods and strong closed-source\nVLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for\nCoSOD, surpassing the prior best by 8.0 percentage points, despite using far\nless training data.", "AI": {"tldr": "本文提出了首个统一框架，通过将显著性检测（SOD）、协同显著性检测（CoSOD）和显著性实例分割（SIS）任务转化为视觉-语言模型（VLM）中的思维链（CoT）推理过程来处理其异构性，并引入了信心引导策略优化（CGPO）和“输出到推理”策略以提高性能。", "motivation": "现有方法难以统一处理操作上异构的显著性任务，且在强化学习（RL）中，思维链（CoT）的质量提升面临挑战，传统策略优化方法（如GRPO）存在信心无关学习、信号稀释和计算开销大等局限性。", "method": "该研究将SOD、CoSOD和SIS任务统一建模为VLM中的CoT推理过程。CoT训练采用两阶段范式：监督微调（SFT）和强化学习（RL）。为提升RL中CoT质量，提出信心引导策略优化（CGPO）算法，利用奖励与模型置信度之间的差异作为单样本优势信号。此外，引入“输出到推理”策略来构建高质量的SFT数据，确保与真实掩码的逻辑一致性。", "result": "实验结果表明，该模型在所有任务上均达到或超越了专用SOTA方法和强大的闭源VLM，尤其在CoCA数据集上的CoSOD任务中，S-measure达到0.899，比之前最佳结果高出8.0个百分点，且所需训练数据量远少。", "conclusion": "所提出的统一框架和训练策略（包括CGPO和“输出到推理”）能够有效桥接操作异构的显著性任务，并在性能上超越现有专业方法和大型VLM，证明了其处理复杂视觉-语言任务的强大能力和效率。"}}
{"id": "2511.01187", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01187", "abs": "https://arxiv.org/abs/2511.01187", "authors": ["Muhammed Saeed", "Muhammad Abdul-mageed", "Shady Shehata"], "title": "Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs", "comment": null, "summary": "Large language models (LLMs) are widely deployed for open-ended\ncommunication, yet most bias evaluations still rely on English,\nclassification-style tasks. We introduce DebateBias-8K, a new multilingual,\ndebate-style benchmark designed to reveal how narrative bias appears in\nrealistic generative settings. Our dataset includes 8,400 structured debate\nprompts spanning four sensitive domains: women's rights, socioeconomic\ndevelopment, terrorism, and religion, across seven languages ranging from\nhigh-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).\nUsing four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we\ngenerate and automatically classify over 100,000 responses. Results show that\nall models reproduce entrenched stereotypes despite safety alignment: Arabs are\noverwhelmingly linked to terrorism and religion (>=95%), Africans to\nsocioeconomic \"backwardness\" (up to <=77%), and Western groups are consistently\nframed as modern or progressive. Biases grow sharply in lower-resource\nlanguages, revealing that alignment trained primarily in English does not\ngeneralize globally. Our findings highlight a persistent divide in multilingual\nfairness: current alignment methods reduce explicit toxicity but fail to\nprevent biased outputs in open-ended contexts. We release our DebateBias-8K\nbenchmark and analysis framework to support the next generation of multilingual\nbias evaluation and safer, culturally inclusive model alignment.", "AI": {"tldr": "该研究引入了多语言辩论式基准DebateBias-8K，以评估大型语言模型（LLMs）在开放式生成场景中的叙事偏见。结果显示，尽管经过安全对齐，所有模型仍存在根深蒂固的刻板印象，尤其在低资源语言中偏见更甚，表明当前的对齐方法未能有效解决多语言公平性问题。", "motivation": "大多数偏见评估仍依赖于英语和分类任务，无法捕捉LLMs在现实生成场景中出现的叙事偏见。研究旨在开发一种多语言、开放式的评估方法，以揭示LLMs在不同文化和语言背景下的偏见。", "method": "研究构建了DebateBias-8K数据集，包含8,400个结构化辩论提示，涵盖女性权利、社会经济发展、恐怖主义和宗教四个敏感领域，并跨越英、中等高资源语言及斯瓦希里语、尼日利亚皮钦语等低资源语言共七种语言。使用GPT-4o、Claude 3、DeepSeek和LLaMA 3四款主流模型生成了超过100,000个回复，并进行了自动化分类分析。", "result": "所有模型都重现了根深蒂固的刻板印象，例如阿拉伯人被压倒性地与恐怖主义和宗教联系（≥95%），非洲人与社会经济“落后”联系（高达≤77%），而西方群体则被持续描绘为现代或进步。偏见在低资源语言中急剧增加，表明主要在英语中训练的对齐方法未能全球化推广。", "conclusion": "研究结果揭示了多语言公平性中持续存在的分歧：当前的对齐方法虽然减少了显性毒性，但未能阻止在开放式语境中产生偏见输出。研究发布了DebateBias-8K基准和分析框架，以支持下一代多语言偏见评估和更安全、文化包容的模型对齐。"}}
{"id": "2511.00429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00429", "abs": "https://arxiv.org/abs/2511.00429", "authors": ["Daichi Zhang", "Tong Zhang", "Shiming Ge", "Sabine Süsstrunk"], "title": "Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection", "comment": null, "summary": "Diffusion models have achieved remarkable success in image synthesis, but the\ngenerated high-quality images raise concerns about potential malicious use.\nExisting detectors often struggle to capture discriminative clues across\ndifferent models and settings, limiting their generalization to unseen\ndiffusion models and robustness to various perturbations. To address this\nissue, we observe that diffusion-generated images exhibit progressively larger\ndifferences from natural real images across low- to high-frequency bands. Based\non this insight, we propose a simple yet effective representation by enhancing\nthe Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we\nintroduce a frequency-selective function which serves as a weighted filter to\nthe Fourier spectrum, suppressing less discriminative bands while enhancing\nmore informative ones. This approach, grounded in a comprehensive analysis of\nfrequency-based differences between natural real and diffusion-generated\nimages, enables general detection of images from unseen diffusion models and\nprovides robust resilience to various perturbations. Extensive experiments on\nvarious diffusion-generated image datasets demonstrate that our method\noutperforms state-of-the-art detectors with superior generalization and\nrobustness.", "AI": {"tldr": "本文提出了一种基于频率分析的图像检测方法，通过增强不同频段的伪造线索（F^2C），有效识别扩散模型生成的图像，并展现出优异的泛化性和鲁棒性。", "motivation": "扩散模型生成的高质量图像引发了潜在恶意使用的担忧。现有检测器在面对不同模型和设置时，泛化能力和鲁棒性不足，难以捕获区分性线索。", "method": "研究发现，扩散模型生成的图像与真实图像在低频到高频段的差异逐渐增大。基于此，提出通过增强所有频段的频率伪造线索（F^2C）来构建表示。具体而言，引入一个频率选择函数作为傅里叶频谱的加权滤波器，抑制区分度较低的频段，同时增强信息量更大的频段。", "result": "在各种扩散模型生成的图像数据集上进行的广泛实验表明，该方法在泛化性和鲁棒性方面均优于现有最先进的检测器。", "conclusion": "该方法通过对真实图像和扩散生成图像之间基于频率差异的全面分析，能够有效检测来自未知扩散模型的图像，并对各种扰动具有强大的弹性。"}}
{"id": "2511.01550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01550", "abs": "https://arxiv.org/abs/2511.01550", "authors": ["Ujjwal Sharma", "Stevan Rudinac", "Ana Mićković", "Willemijn van Dolen", "Marcel Worring"], "title": "Analyzing Sustainability Messaging in Large-Scale Corporate Social Media", "comment": null, "summary": "In this work, we introduce a multimodal analysis pipeline that leverages\nlarge foundation models in vision and language to analyze corporate social\nmedia content, with a focus on sustainability-related communication. Addressing\nthe challenges of evolving, multimodal, and often ambiguous corporate messaging\non platforms such as X (formerly Twitter), we employ an ensemble of large\nlanguage models (LLMs) to annotate a large corpus of corporate tweets on their\ntopical alignment with the 17 Sustainable Development Goals (SDGs). This\napproach avoids the need for costly, task-specific annotations and explores the\npotential of such models as ad-hoc annotators for social media data that can\nefficiently capture both explicit and implicit references to sustainability\nthemes in a scalable manner. Complementing this textual analysis, we utilize\nvision-language models (VLMs), within a visual understanding framework that\nuses semantic clusters to uncover patterns in visual sustainability\ncommunication. This integrated approach reveals sectoral differences in SDG\nengagement, temporal trends, and associations between corporate messaging,\nenvironmental, social, governance (ESG) risks, and consumer engagement. Our\nmethods-automatic label generation and semantic visual clustering-are broadly\napplicable to other domains and offer a flexible framework for large-scale\nsocial media analysis.", "AI": {"tldr": "本文引入了一个多模态分析流程，利用视觉和语言基础模型分析企业社交媒体内容，重点关注可持续发展相关沟通。", "motivation": "研究旨在解决社交媒体上企业信息（特别是可持续发展相关内容）演变、多模态且常模糊的挑战，并避免昂贵的特定任务标注，探索大型模型作为高效即时标注器的潜力。", "method": "采用大型语言模型（LLMs）集成方法自动标注企业推文与17个可持续发展目标（SDGs）的主题一致性。同时，利用视觉-语言模型（VLMs）在视觉理解框架内通过语义聚类揭示视觉可持续发展沟通模式。", "result": "该方法揭示了SDG参与的行业差异、时间趋势，以及企业信息、环境、社会、治理（ESG）风险与消费者参与之间的关联。", "conclusion": "所提出的自动标签生成和语义视觉聚类方法具有广泛适用性，为大规模社交媒体分析提供了灵活的框架。"}}
{"id": "2511.01520", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01520", "abs": "https://arxiv.org/abs/2511.01520", "authors": ["Shipeng Lyu", "Lijie Sheng", "Fangyuan Wang", "Wenyao Zhang", "Weiwei Lin", "Zhenzhong Jia", "David Navarro-Alarcon", "Guodong Guo"], "title": "Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals", "comment": "9 papges, 10 figures, 3 tables", "summary": "Humans naturally grasp objects with minimal level required force for\nstability, whereas robots often rely on rigid, over-squeezing control. To\nnarrow this gap, we propose a human-inspired physics-conditioned tactile method\n(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,\ntactile prediction, and force regulation. A physics-based pose selector first\nidentifies feasible contact regions with optimal force distribution based on\nsurface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)\npredicts the tactile imprint under FOSG target. Last, a latent-space LQR\ncontroller drives the gripper toward this tactile imprint with minimal\nactuation, preventing unnecessary compression. Trained on a physics-conditioned\ntactile dataset covering diverse objects and contact conditions, the proposed\nPhy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac\noutperforms fixed-force and GraspNet-based baselines in grasp stability and\nforce efficiency. Experiments on classical robotic platforms demonstrate\nforce-efficient and adaptive manipulation that bridges the gap between robotic\nand human grasping.", "AI": {"tldr": "本文提出了一种受人类启发、基于物理条件的触觉方法（Phy-Tac），用于实现力最优的稳定抓取，它整合了姿态选择、触觉预测和力调节，以实现高效且自适应的机器人操作。", "motivation": "机器人抓取通常依赖于僵硬、过度挤压的控制，而人类则以最小的必要力实现稳定抓取。研究旨在缩小机器人与人类抓取之间的这一差距，使机器人能够以更接近人类的方式进行力最优的稳定抓取。", "method": "该方法（Phy-Tac）包含三个主要部分：1. 基于物理的姿态选择器，根据表面几何识别具有最优力分布的接触区域。2. 基于物理条件的潜在扩散模型（Phy-LDM），预测力最优稳定抓取目标下的触觉印记。3. 潜在空间LQR控制器，以最小的驱动力将抓手驱动至预测的触觉印记，避免不必要的压缩。该模型在一个包含多样物体和接触条件的物理条件触觉数据集上进行训练。", "result": "所提出的Phy-LDM在触觉预测精度上表现出色。Phy-Tac在抓取稳定性和力效率方面优于固定力及基于GraspNet的基线方法。在经典机器人平台上的实验证明了其力高效和自适应的操作能力。", "conclusion": "该研究通过实现力高效和自适应的抓取操作，成功缩小了机器人与人类抓取之间的差距，使机器人能够以更接近人类的方式进行抓取。"}}
{"id": "2511.01527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01527", "abs": "https://arxiv.org/abs/2511.01527", "authors": ["Hanwen Xu", "Xuyao Huang", "Yuzhe Liu", "Kai Yu", "Zhijie Deng"], "title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "comment": null, "summary": "Large language model (LLM) agents have exhibited strong problem-solving\ncompetence across domains like research and coding. Yet, it remains\nunderexplored whether LLM agents can tackle compounding real-world problems\nthat require a diverse set of tools to complete. Given a broad, heterogeneous\ntool repository, LLM agents must not only select appropriate tools based on\ntask planning analysis but also strategically schedule the execution order to\nensure efficiency. This paper introduces TPS-Bench to benchmark the ability of\nLLM agents in solving such problems that demand Tool Planning and Scheduling.\nTPS-Bench collects 200 compounding tasks of two difficulty levels, based on a\ntool repository containing hundreds of model context protocol (MCP) tools. In\nparticular, each task is composed of multiple subtasks, such as web search, map\nnavigation, calendar checking, etc., and each subtask can be completed by a\nbasic tool. Our evaluation emphasizes both task completion rate and efficiency.\nThe empirical studies on popular closed-source and open-source LLMs indicate\nthat most models can perform reasonable tool planning, but differ in\nscheduling. For example, GLM-4.5 achieves an outperforming task completion rate\nof 64.72% with extensive sequential tool calls, hence suffering from\nsignificantly long execution time. By contrast, GPT-4o prioritizes parallel\ntool calls but achieves only a 45.08% completion rate. Considering\nreinforcement learning (RL) can be a viable way to improve the scheduling\nefficiency without compromising performance, we perform an initial study on\nQwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in\ntask completion rate based on rarely 100 RL training samples. Our code is\navailable https://github.com/hanwenxu1/mcp-agent.", "AI": {"tldr": "本文引入TPS-Bench基准测试，评估大型语言模型（LLM）代理在需要工具规划和调度的复杂现实世界问题中的能力。研究发现现有LLM在调度方面存在差异，并通过强化学习初步证明了提高效率和完成率的潜力。", "tldr_en": "This paper introduces TPS-Bench to benchmark LLM agents' ability to solve complex real-world problems requiring tool planning and scheduling. It finds existing LLMs differ in scheduling, and an initial study shows reinforcement learning can improve both efficiency and completion rate.", "motivation": "尽管LLM代理在研究和编码等领域展现出强大的解决问题能力，但它们应对需要多种工具的复杂现实世界问题的能力尚未得到充分探索。这类问题不仅要求LLM代理选择合适的工具，还需要策略性地调度工具执行顺序以确保效率。", "method": "本文提出了TPS-Bench基准测试，包含200个不同难度的复合任务，基于一个拥有数百个模型上下文协议（MCP）工具的工具库。每个任务由多个子任务（如网页搜索、地图导航、日历检查等）组成。评估侧重于任务完成率和效率。此外，还对Qwen3-1.7B进行了初步的强化学习（RL）研究，以改善调度效率。", "result": "实证研究表明，大多数LLM在工具规划方面表现合理，但在调度方面存在差异。例如，GLM-4.5完成了64.72%的任务，但因大量顺序工具调用导致执行时间过长；GPT-4o优先并行调用，但完成率仅为45.08%。对Qwen3-1.7B进行的RL初步研究（仅使用100个训练样本）显示，执行时间减少了14%，任务完成率提高了6%。", "conclusion": "LLM代理在处理需要工具规划和调度的复杂现实世界问题时，尤其是在调度效率方面仍面临挑战。当前LLM在任务完成率和执行效率之间存在权衡。强化学习是一个有潜力的方向，可以在不牺牲性能的前提下提高调度效率。"}}
{"id": "2511.01594", "categories": ["cs.RO", "cs.CV", "I.2.9; I.2.11; I.2.6; I.4.8"], "pdf": "https://arxiv.org/pdf/2511.01594", "abs": "https://arxiv.org/abs/2511.01594", "authors": ["Renjun Gao", "Peiyan Zhong"], "title": "MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence", "comment": "3 figures, 1 table; under review at Multimedia Systems (Springer)", "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities\nin cross-modal understanding and reasoning, offering new opportunities for\nintelligent assistive systems, yet existing systems still struggle with\nrisk-aware planning, user personalization, and grounding language plans into\nexecutable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic\nSystem powered by MLLMs for assistive intelligence and designed for smart home\nrobots supporting people with disabilities. The system integrates four agents:\na visual perception agent for extracting semantic and spatial features from\nenvironment images, a risk assessment agent for identifying and prioritizing\nhazards, a planning agent for generating executable action sequences, and an\nevaluation agent for iterative optimization. By combining multimodal perception\nwith hierarchical multi-agent decision-making, the framework enables adaptive,\nrisk-aware, and personalized assistance in dynamic indoor environments.\nExperiments on multiple datasets demonstrate the superior overall performance\nof the proposed system in risk-aware planning and coordinated multi-agent\nexecution compared with state-of-the-art multimodal models. The proposed\napproach also highlights the potential of collaborative AI for practical\nassistive scenarios and provides a generalizable methodology for deploying\nMLLM-enabled multi-agent systems in real-world environments.", "AI": {"tldr": "本文提出MARS，一个由多模态大语言模型（MLLMs）驱动的多智能体机器人系统，旨在为智能家居中的残障人士提供风险感知、个性化和可执行的辅助智能。", "motivation": "现有的智能辅助系统在风险感知规划、用户个性化和将语言计划转化为可执行技能方面存在困难，尤其是在杂乱的家庭环境中。", "method": "引入MARS系统，该系统集成四个智能体：视觉感知智能体（提取语义和空间特征）、风险评估智能体（识别和优先处理危险）、规划智能体（生成可执行动作序列）和评估智能体（迭代优化）。该框架结合了多模态感知和分层多智能体决策。", "result": "在多个数据集上的实验表明，与最先进的多模态模型相比，MARS系统在风险感知规划和协调多智能体执行方面表现出卓越的整体性能。", "conclusion": "该方法突出了协作AI在实际辅助场景中的潜力，并为在真实世界环境中部署由MLLM驱动的多智能体系统提供了一种通用方法。"}}
{"id": "2511.01493", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01493", "abs": "https://arxiv.org/abs/2511.01493", "authors": ["Wei Huang", "Jiaxin Li", "Zang Wan", "Huijun Di", "Wei Liang", "Zhu Yang"], "title": "Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues", "comment": null, "summary": "Guiding an agent to a specific target in indoor environments based solely on\nRGB inputs and a floor plan is a promising yet challenging problem. Although\nexisting methods have made significant progress, two challenges remain\nunresolved. First, the modality gap between egocentric RGB observations and the\nfloor plan hinders the integration of visual and spatial information for both\nlocal obstacle avoidance and global planning. Second, accurate localization is\ncritical for navigation performance, but remains challenging at deployment in\nunseen environments due to the lack of explicit geometric alignment between RGB\ninputs and floor plans. We propose a novel diffusion-based policy, denoted as\nGlocDiff, which integrates global path planning from the floor plan with local\ndepth-aware features derived from RGB observations. The floor plan offers\nexplicit global guidance, while the depth features provide implicit geometric\ncues, collectively enabling precise prediction of optimal navigation directions\nand robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation\nduring training to enhance robustness against pose estimation errors, and we\nfind that combining this with a relatively stable VO module during inference\nresults in significantly improved navigation performance. Extensive experiments\non the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in\nachieving superior navigation performance, and the success of real-world\ndeployments also highlights its potential for widespread practical\napplications.", "AI": {"tldr": "GlocDiff是一种新型的扩散策略，它通过整合平面图的全局规划和RGB观测的局部深度感知特征，解决了室内导航中RGB输入与平面图之间的模态鸿沟和定位挑战，并在基准测试和实际部署中实现了卓越的导航性能。", "motivation": "1. 现有方法在基于RGB输入和平面图的室内导航中存在模态鸿沟，阻碍了视觉和空间信息的整合，影响局部避障和全局规划。2. 在未知环境中部署时，由于RGB输入和平面图之间缺乏明确的几何对齐，准确的定位仍然具有挑战性，而定位对导航性能至关重要。", "method": "本文提出了一种名为GlocDiff的扩散策略。该策略将平面图的全局路径规划与从RGB观测中提取的局部深度感知特征相结合。平面图提供明确的全局引导，深度特征提供隐式几何线索，共同实现最佳导航方向的精确预测和鲁棒的避障。此外，GlocDiff在训练期间引入噪声扰动，以增强对姿态估计误差的鲁棒性，并在推理时与相对稳定的VO模块结合，进一步提升导航性能。", "result": "在FloNa基准测试中，GlocDiff展现出卓越的导航性能，证明了其效率和有效性。在实际部署中也取得了成功，凸显了其在实际应用中的潜力。", "conclusion": "GlocDiff通过有效整合全局规划和局部感知，解决了室内导航中的关键挑战，并在基准测试和实际应用中取得了显著成功，凸显了其广泛实际应用的潜力。"}}
{"id": "2511.01581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01581", "abs": "https://arxiv.org/abs/2511.01581", "authors": ["Chengzhang Yu", "Zening Lu", "Chenyang Zheng", "Chiyue Wang", "Yiming Zhang", "Zhanpeng Jin"], "title": "ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks", "comment": "12pages, 4figures", "summary": "Large language models suffer from knowledge staleness and lack of\ninterpretability due to implicit knowledge storage across entangled network\nparameters, preventing targeted updates and reasoning transparency. We propose\nExplicitLM, a novel architecture featuring a million-scale external memory bank\nstoring human-readable knowledge as token sequences, enabling direct inspection\nand modification. We design a differentiable two-stage retrieval mechanism with\nefficient coarse-grained filtering via product key decomposition (reducing\ncomplexity from $\\mathcal{O}(N \\cdot |I|)$ to $\\mathcal{O}(\\sqrt{N} \\cdot\n|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.\nInspired by dual-system cognitive theory, we partition knowledge into frozen\nexplicit facts (20%) and learnable implicit patterns (80%), maintained through\nExponential Moving Average updates for stability. ExplicitLM achieves up to\n43.67% improvement on knowledge-intensive tasks versus standard Transformers,\nwith 3.62$\\times$ gains in low-data regimes (10k samples). Analysis shows\nstrong correlations between memory retrieval and performance, with correct\npredictions achieving 49% higher hit rates. Unlike RAG systems with frozen\nretrieval, our jointly optimized architecture demonstrates that interpretable,\nupdatable models can maintain competitive performance while providing\nunprecedented knowledge transparency.", "AI": {"tldr": "ExplicitLM通过引入百万级外部可读知识库和两阶段检索机制，解决了大型语言模型知识过时和缺乏可解释性的问题，显著提升了知识密集型任务的性能和知识透明度。", "motivation": "大型语言模型（LLMs）由于隐式知识存储在纠缠的网络参数中，导致知识过时、难以解释，阻碍了定向更新和推理透明性。", "method": "ExplicitLM提出了一种新颖架构，包含一个百万级外部记忆库，以人类可读的token序列形式存储知识。设计了可微分的两阶段检索机制：通过乘积键分解进行高效粗粒度过滤（将复杂度从O(N·|I|)降至O(√N·|I|)），以及用于端到端训练的细粒度Gumbel-Softmax匹配。受双系统认知理论启发，将知识划分为20%的冻结显式事实和80%的可学习隐式模式，并通过指数移动平均（EMA）更新保持稳定性。", "result": "ExplicitLM在知识密集型任务上比标准Transformer模型性能提升高达43.67%，在低数据量（1万样本）情况下性能提升3.62倍。分析显示记忆检索与性能之间存在强相关性，正确预测的命中率高出49%。", "conclusion": "与检索增强生成（RAG）系统不同，ExplicitLM通过联合优化的架构证明，可解释、可更新的模型在保持竞争性性能的同时，能够提供前所未有的知识透明度。"}}
{"id": "2511.01188", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01188", "abs": "https://arxiv.org/abs/2511.01188", "authors": ["Lvhua Wu", "Xuefeng Jiang", "Sheng Sun", "Tian Wen", "Yuwei Wang", "Min Liu"], "title": "ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction", "comment": null, "summary": "The rapid spread of fake news threatens social stability and public trust,\nrendering its detection an imperative research priority. Although large\nlanguage models (LLMs) excel at numerous natural language processing tasks with\ntheir remarkable contextual understanding and extensive prior knowledge, the\ntime-bounded knowledge coverage and tendency for generating hallucination\ncontent reduce their reliability when handling fast-evolving news streams.\nFurthermore, models trained on existing static datasets also often lack the\ngeneralization needed for emerging news topics. To address these challenges, we\npropose ZoFia, a novel two-stage zero-shot fake news detection framework.\nFirst, we introduce Hierarchical Salience to quantify the importance of\nentities in the news content, and propose the SC-MMR algorithm to effectively\nselect an informative and diverse set of keywords that serve as queries for\nretrieving up-to-date external evidence. Subsequently, a multi LLM interactive\nsystem, in which each agent assumes a distinct role, performs multi-view\ncollaborative analysis and adversarial debate over the news text and its\nrelated information, and finally produces an interpretable and robust judgment.\nComprehensive experiments on two public datasets demonstrate that ZoFia\nobviously outperforms existing zero-shot baselines and most of few-shot\nmethods. Our codes will be open-sourced to facilitate related communities.", "AI": {"tldr": "本文提出了ZoFia，一个两阶段的零样本假新闻检测框架，它结合了大型语言模型（LLMs）、外部证据检索和多LLM协作分析，以应对LLMs在处理快速演变新闻时知识时效性和泛化能力不足的问题。", "motivation": "假新闻的迅速传播威胁着社会稳定和公众信任。现有的大型语言模型（LLMs）在处理快速变化的新闻流时，存在知识覆盖有时限、容易产生幻觉内容以及在静态数据集上训练的模型对新兴新闻主题泛化能力不足等问题，因此需要更可靠的检测方法。", "method": "ZoFia框架分为两个阶段：\n1.  **证据检索阶段**：引入“分层显著性”（Hierarchical Salience）来量化新闻内容中实体的重要性，并提出SC-MMR算法选择信息丰富且多样化的关键词，作为查询以检索最新的外部证据。\n2.  **多LLM交互分析阶段**：构建一个多LLM交互系统，其中每个代理扮演不同角色，对新闻文本及其相关信息进行多视角协作分析和对抗性辩论，最终生成可解释且鲁棒的判断。", "result": "在两个公共数据集上进行的全面实验表明，ZoFia框架明显优于现有的零样本基线方法和大多数少样本方法。", "conclusion": "ZoFia提供了一种新颖有效的零样本假新闻检测方案，通过结合外部证据和多LLM协作分析，解决了LLMs在处理时效性强、变化快的新闻内容时面临的挑战，并能产生可解释且鲁棒的判断。"}}
{"id": "2511.00456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00456", "abs": "https://arxiv.org/abs/2511.00456", "authors": ["Kiran Shahi", "Anup Bagale"], "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "comment": null, "summary": "This study proposes a weakly supervised deep learning framework for pneumonia\nclassification and localization from chest X-rays, utilizing Grad-CAM\nexplanations. Instead of costly pixel-level annotations, our approach utilizes\nimage-level labels to generate clinically meaningful heatmaps that highlight\nregions affected by pneumonia. We evaluate seven ImageNet-pretrained\narchitectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and\nViT-B16 under identical training conditions with focal loss and patient-wise\nsplits to prevent data leakage. Experimental results on the Kermany CXR dataset\ndemonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test\naccuracy of 98\\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides\nan optimal trade-off between accuracy and computational cost. Grad-CAM\nvisualizations confirm that the proposed models focus on clinically relevant\nlung regions, supporting the use of interpretable AI for radiological\ndiagnostics. This work highlights the potential of weakly supervised\nexplainable models that enhance pneumonia screening transparency, and clinical\ntrust in AI-assisted medical imaging.\n  https://github.com/kiranshahi/pneumonia-analysis", "AI": {"tldr": "本研究提出了一种弱监督深度学习框架，利用图像级标签和Grad-CAM解释，实现肺炎的分类和定位，并在Kermany CXR数据集上取得了高准确率和可解释的视觉结果。", "motivation": "传统的像素级标注成本高昂，且医学影像诊断需要生成具有临床意义的热图以提高AI的透明度和临床信任度。", "method": "该研究采用弱监督深度学习框架，利用图像级标签而非像素级标签，并结合Grad-CAM生成热图。评估了七种ImageNet预训练架构（ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, ViT-B16），在相同训练条件下使用focal loss和按患者划分的数据集（Kermany CXR数据集）进行训练和验证，以防止数据泄露。", "result": "实验结果表明，ResNet-18和EfficientNet-B0在测试集上取得了最佳的整体准确率（98%）、ROC-AUC（0.997）和F1分数（0.987）。MobileNet-V2在准确性和计算成本之间提供了最佳平衡。Grad-CAM可视化证实模型关注临床相关的肺部区域。", "conclusion": "该工作突出了弱监督可解释模型在肺炎筛查中的潜力，能够增强AI辅助医疗影像的透明度和临床信任度，支持可解释AI在放射诊断中的应用。"}}
{"id": "2511.01445", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01445", "abs": "https://arxiv.org/abs/2511.01445", "authors": ["ChengZhang Yu", "YingRu He", "Hongyan Cheng", "nuo Cheng", "Zhixing Liu", "Dongxu Mu", "Zhangrui Shen", "Zhanpeng Jin"], "title": "From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation", "comment": "14pages, 7 figures, 7 tables", "summary": "Global healthcare systems face critical challenges from increasing patient\nvolumes and limited consultation times, with primary care visits averaging\nunder 5 minutes in many countries. While pre-consultation processes\nencompassing triage and structured history-taking offer potential solutions,\nthey remain limited by passive interaction paradigms and context management\nchallenges in existing AI systems. This study introduces a hierarchical\nmulti-agent framework that transforms passive medical AI systems into proactive\ninquiry agents through autonomous task orchestration. We developed an\neight-agent architecture with centralized control mechanisms that decomposes\npre-consultation into four primary tasks: Triage ($T_1$), History of Present\nIllness collection ($T_2$), Past History collection ($T_3$), and Chief\nComplaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13\ndomain-specific subtasks. Evaluated on 1,372 validated electronic health\nrecords from a Chinese medical platform across multiple foundation models\n(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for\nprimary department triage and 80.5% for secondary department classification,\nwith task completion rates reaching 98.2% using agent-driven scheduling versus\n93.1% with sequential processing. Clinical quality scores from 18 physicians\naveraged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and\n4.69 for Past History on a 5-point scale, with consultations completed within\n12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic\narchitecture maintained high performance across different foundation models\nwhile preserving data privacy through local deployment, demonstrating the\npotential for autonomous AI systems to enhance pre-consultation efficiency and\nquality in clinical settings.", "AI": {"tldr": "本研究提出了一种分层多智能体框架，将被动医疗AI系统转化为主动问诊智能体，通过自主任务编排提升了预问诊的效率和质量。在真实电子病历上评估，实现了高准确率和医生认可的临床质量，并能保护数据隐私。", "motivation": "全球医疗系统面临患者量增加和问诊时间有限的挑战（许多初级保健问诊平均不到5分钟）。现有预问诊AI系统受限于被动交互模式和上下文管理难题，无法有效解决这些问题。", "method": "引入了一个分层多智能体框架，通过自主任务编排将医疗AI系统转变为主动问诊智能体。该框架包含一个具有集中控制机制的八智能体架构，将预问诊分解为四个主要任务（分诊、现病史收集、既往史收集、主诉生成）和13个领域特定子任务。在1372份中国医疗平台的验证电子健康记录上，使用GPT-OSS 20B、Qwen3-8B、Phi4-14B等多个基础模型进行了评估。", "result": "该框架在初级科室分诊中达到87.0%的准确率，在二级科室分类中达到80.5%的准确率。使用智能体驱动调度时，任务完成率达到98.2%，高于顺序处理的93.1%。18位医生给出的临床质量评分平均为主诉4.56分、现病史4.48分、既往史4.69分（5分制）。现病史收集平均12.7轮完成，既往史收集平均16.9轮完成。该模型无关架构在不同基础模型上均保持了高性能，并通过本地部署保护了数据隐私。", "conclusion": "该研究证明了自主AI系统在临床环境中提升预问诊效率和质量的巨大潜力，并展示了其在保护数据隐私的同时，能够提供高准确性和高质量的临床信息。"}}
{"id": "2511.00468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00468", "abs": "https://arxiv.org/abs/2511.00468", "authors": ["Panwang Pan", "Tingting Shen", "Chenxin Li", "Yunlong Lin", "Kairun Wen", "Jingjing Zhao", "Yixuan Yuan"], "title": "HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation", "comment": "Accepted to NeurIPS 2025; Project page: [this\n  URL](https://paulpanwang.github.io/HumanCrafter)", "summary": "Recent advances in generative models have achieved high-fidelity in 3D human\nreconstruction, yet their utility for specific tasks (e.g., human 3D\nsegmentation) remains constrained. We propose HumanCrafter, a unified framework\nthat enables the joint modeling of appearance and human-part semantics from a\nsingle image in a feed-forward manner. Specifically, we integrate human\ngeometric priors in the reconstruction stage and self-supervised semantic\npriors in the segmentation stage. To address labeled 3D human datasets\nscarcity, we further develop an interactive annotation procedure for generating\nhigh-quality data-label pairs. Our pixel-aligned aggregation enables cross-task\nsynergy, while the multi-task objective simultaneously optimizes texture\nmodeling fidelity and semantic consistency. Extensive experiments demonstrate\nthat HumanCrafter surpasses existing state-of-the-art methods in both 3D\nhuman-part segmentation and 3D human reconstruction from a single image.", "AI": {"tldr": "HumanCrafter是一个统一框架，能从单张图像前向地联合建模3D人体外观和部件语义，并在3D人体重建和分割任务上超越现有最佳方法。", "motivation": "尽管生成模型在3D人体重建方面取得了高保真度，但它们在特定任务（如3D人体分割）上的实用性仍然受限。此外，标记的3D人体数据集稀缺。", "method": "本文提出了HumanCrafter框架，它在重建阶段整合了人体几何先验，在分割阶段整合了自监督语义先验。为解决数据稀缺问题，开发了一种交互式标注流程来生成高质量数据-标签对。通过像素对齐聚合实现跨任务协同，并采用多任务目标函数同时优化纹理建模保真度和语义一致性。", "result": "HumanCrafter在3D人体部件分割和从单张图像进行3D人体重建方面均超越了现有的最先进方法。", "conclusion": "HumanCrafter成功地实现了从单张图像对人体外观和部件语义的联合建模，并通过创新的方法解决了数据稀缺问题，显著提升了3D人体重建和分割的性能。"}}
{"id": "2511.01639", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01639", "abs": "https://arxiv.org/abs/2511.01639", "authors": ["Sicheng Wang", "Shuhao Chen", "Jingran Zhou", "Chengyi Tu"], "title": "IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization", "comment": "26pages,6figures", "summary": "Global food trade plays a crucial role in ensuring food security and\nmaintaining supply chain stability. However, its network structure evolves\ndynamically under the influence of geopolitical, economic, and environmental\nfactors, making it challenging to model and predict future trade links.\nEffectively capturing temporal patterns in food trade networks is therefore\nessential for improving the accuracy and robustness of link prediction. This\nstudy introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed\nto model evolving trade structures and predict future links in global food\ntrade networks. To the best of our knowledge, this is the first work to apply\ndynamic graph neural networks to this domain, significantly enhancing\npredictive performance. Building upon the original IVGAE framework, the\nproposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture\nthe temporal evolution of trade networks, jointly modeling short-term\nfluctuations and long-term structural dependencies. A momentum-based structural\nmemory mechanism further improves predictive stability and performance. In\naddition, Bayesian optimization is used to automatically tune key\nhyperparameters, enhancing generalization across diverse trade scenarios.\nExtensive experiments on five crop-specific datasets demonstrate that\nIVGAE-TAMA substantially outperforms the static IVGAE and other dynamic\nbaselines by effectively modeling temporal dependencies, while Bayesian\noptimization further boosts performance in IVGAE-TAMA-BO. These results\nhighlight the proposed framework as a robust and scalable solution for\nstructural prediction in global trade networks, with strong potential for\napplications in food security monitoring and policy decision support.", "AI": {"tldr": "本研究提出了一种新颖的动态图神经网络IVGAE-TAMA-BO，用于预测全球粮食贸易网络的未来连接，该模型通过捕获时间模式和结构记忆，显著提升了预测性能。", "motivation": "全球粮食贸易网络的结构受地缘政治、经济和环境因素影响而动态演变，难以建模和预测。有效捕捉时间模式对于提高链接预测的准确性和鲁棒性至关重要。", "method": "本研究基于IVGAE框架，引入了贸易感知动量聚合器（TAMA）来捕捉贸易网络的时间演变，联合建模短期波动和长期结构依赖。模型还结合了基于动量的结构记忆机制以提高预测稳定性，并使用贝叶斯优化自动调整超参数。", "result": "在五个特定作物的全球粮食贸易数据集上进行的广泛实验表明，IVGAE-TAMA显著优于静态IVGAE和其他动态基线，有效建模了时间依赖性。贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。这是首次将动态图神经网络应用于该领域。", "conclusion": "所提出的框架为全球贸易网络中的结构预测提供了一个鲁棒且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大的应用潜力。"}}
{"id": "2511.01192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01192", "abs": "https://arxiv.org/abs/2511.01192", "authors": ["Guoxin Ma", "Xiaoming Liu", "Zhanhan Zhang", "Chengzhengxu Li", "Shengchao Liu", "Yu Lan"], "title": "DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection", "comment": "Under Review", "summary": "Detecting machine-generated text (MGT) has emerged as a critical challenge,\ndriven by the rapid advancement of large language models (LLMs) capable of\nproducing highly realistic, human-like content. However, the performance of\ncurrent approaches often degrades significantly under domain shift. To address\nthis challenge, we propose a novel framework designed to capture both\ndomain-specific and domain-general MGT patterns through a two-stage\nDisentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a\ndisentangled mixture-of-experts module, in which domain-specific experts learn\nfine-grained, domain-local distinctions between human and machine-generated\ntext, while shared experts extract transferable, cross-domain features. Second,\nto mitigate the practical limitation of unavailable domain labels during\ninference, we design a reinforcement learning-based routing mechanism that\ndynamically selects the appropriate experts for each input instance,\neffectively bridging the train-inference gap caused by domain uncertainty.\nExtensive experiments on five in-domain and five out-of-domain benchmark\ndatasets demonstrate that DEER consistently outperforms state-of-the-art\nmethods, achieving average F1-score improvements of 1.39% and 5.32% on\nin-domain and out-of-domain datasets respectively, along with accuracy gains of\n1.35% and 3.61% respectively. Ablation studies confirm the critical\ncontributions of both disentangled expert specialization and adaptive routing\nto model performance.", "AI": {"tldr": "该论文提出了一种名为DEER的新型框架，通过解耦的专家混合模型和基于强化学习的路由机制，有效解决了机器生成文本检测中领域漂移导致的性能下降问题。", "motivation": "大型语言模型（LLMs）的快速发展使得生成逼真的人类文本成为可能，机器生成文本（MGT）检测成为一个关键挑战。然而，现有方法在面对领域漂移时性能显著下降，这促使研究人员寻求更鲁棒的解决方案。", "method": "该研究提出了一个两阶段的“解耦专家混合模型”（DEER）架构。首先，引入一个解耦专家混合模块，其中领域特定专家学习细粒度的领域局部特征，而共享专家提取可迁移的跨领域特征。其次，为了解决推理时领域标签不可用的问题，设计了一个基于强化学习的路由机制，动态选择适合每个输入实例的专家，从而弥补训练与推理之间的领域不确定性鸿沟。", "result": "在五个域内和五个域外基准数据集上的广泛实验表明，DEER始终优于现有最先进的方法。在域内和域外数据集上，F1分数分别平均提高了1.39%和5.32%，准确率分别提高了1.35%和3.61%。消融研究证实了解耦专家专业化和自适应路由对模型性能的关键贡献。", "conclusion": "DEER框架通过有效捕获领域特定和领域通用MGT模式，并利用自适应路由机制应对推理时的领域不确定性，显著提高了机器生成文本检测在领域漂移下的鲁棒性和性能。"}}
{"id": "2511.01668", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01668", "abs": "https://arxiv.org/abs/2511.01668", "authors": ["Yueqing Xi", "Yifan Bai", "Huasen Luo", "Weiliang Wen", "Hui Liu", "Haoliang Li"], "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics", "comment": null, "summary": "As artificial intelligence permeates judicial forensics, ensuring the\nveracity and traceability of legal question answering (QA) has become critical.\nConventional large language models (LLMs) are prone to hallucination, risking\nmisleading guidance in legal consultation, while static knowledge bases\nstruggle to keep pace with frequently updated statutes and case law. We present\na hybrid legal QA agent tailored for judicial settings that integrates\nretrieval-augmented generation (RAG) with multi-model ensembling to deliver\nreliable, auditable, and continuously updatable counsel. The system prioritizes\nretrieval over generation: when a trusted legal repository yields relevant\nevidence, answers are produced via RAG; otherwise, multiple LLMs generate\ncandidates that are scored by a specialized selector, with the top-ranked\nanswer returned. High-quality outputs then undergo human review before being\nwritten back to the repository, enabling dynamic knowledge evolution and\nprovenance tracking. Experiments on the Law\\_QA dataset show that our hybrid\napproach significantly outperforms both a single-model baseline and a vanilla\nRAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm\nthe complementary contributions of retrieval prioritization, model ensembling,\nand the human-in-the-loop update mechanism. The proposed system demonstrably\nreduces hallucination while improving answer quality and legal compliance,\nadvancing the practical landing of media forensics technologies in judicial\nscenarios.", "AI": {"tldr": "本文提出了一种混合法律问答智能体，结合检索增强生成（RAG）和多模型集成，以解决大型语言模型（LLMs）幻觉和静态知识库过时的问题，为司法环境提供可靠、可追溯且持续更新的法律咨询。", "motivation": "现有大型语言模型容易产生幻觉，可能在法律咨询中提供误导性信息；而静态知识库难以跟上频繁更新的法律法规和案例。在司法取证中，确保法律问答的真实性和可追溯性至关重要。", "method": "该系统采用混合法律问答智能体，优先使用RAG：当可信法律知识库提供相关证据时，通过RAG生成答案。否则，多个LLM生成候选答案，由专门的选择器评分，并返回排名最高的答案。高质量的输出经过人工审核后写回知识库，实现动态知识演化和溯源跟踪。", "result": "在Law_QA数据集上的实验表明，该混合方法在F1、ROUGE-L和LLM-as-a-Judge指标上显著优于单一模型基线和普通RAG管道。消融实验证实了检索优先级、模型集成和人机协作更新机制的互补贡献。", "conclusion": "所提出的系统显著减少了幻觉，同时提高了答案质量和法律合规性，推动了媒体取证技术在司法场景中的实际应用。"}}
{"id": "2511.01718", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01718", "abs": "https://arxiv.org/abs/2511.01718", "authors": ["Jiayi Chen", "Wenxuan Song", "Pengxiang Ding", "Ziyang Zhou", "Han Zhao", "Feilong Tang", "Donglin Wang", "Haoang Li"], "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process", "comment": null, "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.", "AI": {"tldr": "本文提出了一种名为Unified Diffusion VLA (UD-VLA)的新型视觉-语言-动作模型，通过联合离散去噪扩散过程(JD3P)同步优化未来图像生成和动作预测，实现了理解、生成和行动的内在协同，并在多个基准测试中取得了最先进的性能和更快的推理速度。", "motivation": "现有的视觉-语言-动作 (VLA) 模型在将未来图像整合到理解-行动循环中时，要么依赖外部专家进行模态统一，要么将图像生成和动作预测视为独立过程，这限制了这些任务之间直接协同的益处。", "method": "本文提出Unified Diffusion VLA (UD-VLA)和联合离散去噪扩散过程 (JD3P)。其核心思想是通过同步去噪过程共同优化生成和行动，使行动在持续且充分的视觉指导下从初始化阶段迭代细化。模型建立在所有模态的统一分词空间和混合注意力机制之上，并提出了两阶段训练流程和多项推理时技术来优化性能和效率。", "result": "该方法在CALVIN、LIBERO和SimplerEnv等基准测试中取得了最先进的性能，推理速度比自回归方法快4倍，并通过深入分析和真实世界评估证明了其有效性。", "conclusion": "通过其提出的UD-VLA和JD3P，该研究成功地将多模态整合到一个单一的去噪轨迹中，实现了理解、生成和行动的内在协同，显著提升了具身智能体的性能和效率。"}}
{"id": "2511.01191", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01191", "abs": "https://arxiv.org/abs/2511.01191", "authors": ["Ru Wang", "Wei Huang", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "comment": null, "summary": "Test-time reinforcement learning (TTRL) offers a label-free paradigm for\nadapting models using only synthetic signals at inference, but its success\nhinges on constructing reliable learning signals. Standard approaches such as\nmajority voting often collapse to spurious yet popular answers. We introduce\nSelf-Harmony, a framework built on a simple intuition: the correct answer\nshould remain stable across both an original question and its paraphrase.\nSelf-Harmony operationalizes this by employing a single model in two\ncomplementary roles: a Solver to produce answers and a Reframer to rephrase the\ninput. Based on this, we further propose a pseudo-label method: instead of\nmajority voting, it aggregates answer frequencies across these original and\nreframed views using the harmonic mean. This is a process that naturally\nselects for solutions stable under reframing, thereby avoiding the common trap\nof favoring view-dependent, spurious answers. Crucially, this requires no human\nsupervision or auxiliary models. Across diverse reasoning benchmarks,\nSelf-Harmony achieves state-of-the-art results at the label-free test-time\nsetting, ranking first in 28 of 30 settings across multiple methods. Beyond\naccuracy, it demonstrates unprecedented robustness, with zero training failures\nin all experiments, underscoring its stability and reliability.", "AI": {"tldr": "本文提出Self-Harmony框架，通过利用答案在原始问题及其复述版本之间保持稳定的直觉，为测试时强化学习（TTRL）构建可靠的学习信号。它使用单个模型作为求解器和复述器，并采用调和平均数进行伪标签聚合，实现了最先进的性能和卓越的鲁棒性。", "motivation": "测试时强化学习（TTRL）的成功依赖于构建可靠的学习信号，但标准方法（如多数投票）经常导致虚假且受欢迎的答案，无法提供可靠的自适应信号。", "method": "Self-Harmony框架基于“正确答案在原始问题及其复述版本之间应保持稳定”的直觉。它使用单个模型扮演两种角色：求解器（Solver）生成答案，复述器（Reframer）重新表述输入。在此基础上，提出一种伪标签方法，通过使用调和平均数聚合原始和复述视图下的答案频率，而非多数投票，以选择在复述下稳定的解决方案，避免偏向依赖视图的虚假答案。该方法无需人工监督或辅助模型。", "result": "Self-Harmony在各种推理基准测试的无标签测试时设置中取得了最先进（SOTA）的结果，在30个设置中有28个排名第一。除了高准确率，它还展示了前所未有的鲁棒性，所有实验中均无训练失败，突显了其稳定性和可靠性。", "conclusion": "Self-Harmony通过利用答案在问题复述下的稳定性，为TTRL提供了一种无需标签且高度稳定、可靠的自适应方法，有效地解决了传统方法中学习信号不可靠和虚假答案的问题，并在多个推理任务中达到了SOTA性能。"}}
{"id": "2511.00472", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00472", "abs": "https://arxiv.org/abs/2511.00472", "authors": ["Navodini Wijethilake", "Marina Ivory", "Oscar MacCormac", "Siddhant Kumar", "Aaron Kujawa", "Lorena Garcia-Foncillas Macias", "Rebecca Burger", "Amanda Hitchings", "Suki Thomson", "Sinan Barazi", "Eleni Maratos", "Rupert Obholzer", "Dan Jiang", "Fiona McClenaghan", "Kazumi Chia", "Omar Al-Salihi", "Nick Thomas", "Steve Connor", "Tom Vercauteren", "Jonathan Shapey"], "title": "Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations", "comment": null, "summary": "Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance\nImaging (MRI) is essential for patient management but often requires\ntime-intensive manual annotations by experts. While recent advances in deep\nlearning (DL) have facilitated automated segmentation, challenges remain in\nachieving robust performance across diverse datasets and complex clinical\ncases. We present an annotated dataset stemming from a bootstrapped DL-based\nframework for iterative segmentation and quality refinement of VS in MRI. We\ncombine data from multiple centres and rely on expert consensus for\ntrustworthiness of the annotations. We show that our approach enables effective\nand resource-efficient generalisation of automated segmentation models to a\ntarget data distribution. The framework achieved a significant improvement in\nsegmentation accuracy with a Dice Similarity Coefficient (DSC) increase from\n0.9125 to 0.9670 on our target internal validation dataset, while maintaining\nstable performance on representative external datasets. Expert evaluation on\n143 scans further highlighted areas for model refinement, revealing nuanced\ncases where segmentation required expert intervention. The proposed approach is\nestimated to enhance efficiency by approximately 37.4% compared to the\nconventional manual annotation process. Overall, our human-in-the-loop model\ntraining approach achieved high segmentation accuracy, highlighting its\npotential as a clinically adaptable and generalisable strategy for automated VS\nsegmentation in diverse clinical settings. The dataset includes 190 patients,\nwith tumour annotations available for 534 longitudinal contrast-enhanced\nT1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans\nfrom 6 patients. This dataset is publicly accessible on The Cancer Imaging\nArchive (TCIA) (https://doi.org/10.7937/bq0z-xa62).", "AI": {"tldr": "本文提出了一种基于引导式深度学习的迭代框架，用于前庭神经鞘瘤（VS）的MRI图像分割，通过人工干预和专家共识，显著提高了分割精度和效率，并构建了一个高质量的公开数据集。", "motivation": "前庭神经鞘瘤（VS）的MRI精确分割对手术管理至关重要，但专家手动标注耗时且劳动密集。尽管深度学习（DL）已用于自动化分割，但在不同数据集和复杂临床病例中实现鲁棒性能仍面临挑战。", "method": "研究采用了一个引导式深度学习框架，结合迭代分割和质量细化，以生成VS的标注数据集。该方法整合了来自多个中心的数据，并依赖专家共识确保标注的可靠性。整个过程采用“人在回路”（human-in-the-loop）模型训练，并进行了专家评估以识别需要人工干预的细微病例。", "result": "该框架使自动化分割模型能够有效且资源高效地泛化到目标数据分布。在内部验证数据集上，分割精度显著提高，Dice相似系数（DSC）从0.9125增至0.9670，同时在代表性外部数据集上保持了稳定性能。与传统手动标注相比，效率提高了约37.4%。对143次扫描的专家评估揭示了模型改进的区域。最终发布了一个包含190名患者（534次纵向增强T1加权扫描和6次非标注T2加权扫描）的公开数据集。", "conclusion": "所提出的人在回路模型训练方法实现了高分割精度，并展示了其作为一种临床适应性强、可泛化的策略，用于在不同临床环境中自动化VS分割的巨大潜力。"}}
{"id": "2511.01265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01265", "abs": "https://arxiv.org/abs/2511.01265", "authors": ["Mo El-Haj", "Paul Rayson"], "title": "AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs", "comment": "10 pages", "summary": "This paper investigates the impact of domain specificity on abstractive\nsummarisation of Arabic financial texts using large language models (LLMs). We\nintroduce AraFinNews, the largest publicly available Arabic financial news\ndataset to date, comprising 212,500 article--headline pairs spanning nearly a\ndecade of reporting from October 2015 to July 2025. Designed as the Arabic\nequivalent of major English summarisation corpora such as CNN/DailyMail,\nAraFinNews provides a robust benchmark for evaluating domain-specific language\nunderstanding and generation in financial contexts. Using this resource, we\nevaluate transformer-based models -- including mT5, AraT5, and the\ndomain-adapted FinAraT5 -- to examine how financial-domain pretraining\ninfluences factual accuracy, numerical reliability, and stylistic alignment\nwith professional reporting. Experimental results show that domain-adapted\nmodels generate more faithful and coherent summaries, particularly in handling\nquantitative and entity-centric information. The findings highlight the\nimportance of domain-specific adaptation for improving factual consistency and\nnarrative fluency in Arabic financial summarisation. The dataset is freely\navailable for non-commercial research at\nhttps://github.com/ArabicNLP-UK/AraFinNews.", "AI": {"tldr": "本研究探讨了领域特异性对阿拉伯语金融文本抽象摘要的影响，引入了迄今为止最大的阿拉伯语金融新闻数据集AraFinNews，并发现领域适应模型在事实准确性和连贯性方面表现更优。", "motivation": "研究动机在于理解领域特异性（特别是金融领域）如何影响大型语言模型在阿拉伯语抽象摘要任务中的表现，并解决现有缺乏大型、公开可用的阿拉伯语金融新闻摘要数据集的问题。", "method": "研究方法包括：1. 构建并发布了AraFinNews数据集，包含212,500对阿拉伯语金融新闻文章-标题对。2. 使用该数据集评估了多种基于Transformer的模型，包括mT5、AraT5以及领域适应的FinAraT5。3. 评估指标侧重于事实准确性、数字可靠性和与专业报告的风格一致性。", "result": "实验结果表明，经过领域适应的模型（如FinAraT5）能够生成更忠实和连贯的摘要，尤其在处理定量信息和以实体为中心的信息时表现突出。", "conclusion": "本研究得出结论，领域特定的适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要。"}}
{"id": "2511.01770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01770", "abs": "https://arxiv.org/abs/2511.01770", "authors": ["Liudi Yang", "Yang Bai", "Yuhao Wang", "Ibrahim Alsarraj", "Gitta Kutyniok", "Zhanchi Wang", "Ke Wu"], "title": "Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping", "comment": null, "summary": "Robotic grasping under uncertainty remains a fundamental challenge due to its\nuncertain and contact-rich nature. Traditional rigid robotic hands, with\nlimited degrees of freedom and compliance, rely on complex model-based and\nheavy feedback controllers to manage such interactions. Soft robots, by\ncontrast, exhibit embodied mechanical intelligence: their underactuated\nstructures and passive flexibility of their whole body, naturally accommodate\nuncertain contacts and enable adaptive behaviors. To harness this capability,\nwe propose a lightweight actuation-space learning framework that infers\ndistributional control representations for whole-body soft robotic grasping,\ndirectly from deterministic demonstrations using a flow matching model\n(Rectified Flow),without requiring dense sensing or heavy control loops. Using\nonly 30 demonstrations (less than 8% of the reachable workspace), the learned\npolicy achieves a 97.5% grasp success rate across the whole workspace,\ngeneralizes to grasped-object size variations of +-33%, and maintains stable\nperformance when the robot's dynamic response is directly adjusted by scaling\nthe execution time from 20% to 200%. These results demonstrate that\nactuation-space learning, by leveraging its passive redundant DOFs and\nflexibility, converts the body's mechanics into functional control intelligence\nand substantially reduces the burden on central controllers for this\nuncertain-rich task.", "AI": {"tldr": "本文提出一种基于Rectified Flow的轻量级驱动空间学习框架，使软体机器人能从少量示范中学习全身抓取策略，实现高成功率、良好的泛化能力和鲁棒性，从而将身体力学转化为控制智能。", "motivation": "机器人抓取面临不确定性和频繁接触的挑战。传统刚性机器人需要复杂的模型和反馈控制。软体机器人因其欠驱动结构和被动柔性，天然适合处理不确定接触并展现自适应行为，其“具身机械智能”有望简化控制负担。", "method": "提出一种轻量级驱动空间学习框架，利用流匹配模型（Rectified Flow）直接从确定性示范中推断全身软体机器人抓取的分布控制表示。该方法不需要密集的传感或复杂的控制回路。", "result": "仅使用30次示范（不到可达工作空间的8%），学习到的策略在整个工作空间实现了97.5%的抓取成功率。它能泛化到物体尺寸±33%的变化，并在执行时间从20%调整到200%时保持稳定的性能。", "conclusion": "驱动空间学习通过利用软体机器人被动冗余自由度和柔性，将身体力学转化为功能性控制智能，显著减轻了中央控制器在处理不确定性任务时的负担。"}}
{"id": "2511.01791", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01791", "abs": "https://arxiv.org/abs/2511.01791", "authors": ["Feng Chen", "Zhuxiu Xu", "Tianzhe Chu", "Xunzhe Zhou", "Li Sun", "Zewen Wu", "Shenghua Gao", "Zhongyu Li", "Yanchao Yang", "Yi Ma"], "title": "GenDexHand: Generative Simulation for Dexterous Hands", "comment": null, "summary": "Data scarcity remains a fundamental bottleneck for embodied intelligence.\nExisting approaches use large language models (LLMs) to automate gripper-based\nsimulation generation, but they transfer poorly to dexterous manipulation,\nwhich demands more specialized environment design. Meanwhile, dexterous\nmanipulation tasks are inherently more difficult due to their higher degrees of\nfreedom. Massively generating feasible and trainable dexterous hand tasks\nremains an open challenge. To this end, we present GenDexHand, a generative\nsimulation pipeline that autonomously produces diverse robotic tasks and\nenvironments for dexterous manipulation. GenDexHand introduces a closed-loop\nrefinement process that adjusts object placements and scales based on\nvision-language model (VLM) feedback, substantially improving the average\nquality of generated environments. Each task is further decomposed into\nsub-tasks to enable sequential reinforcement learning, reducing training time\nand increasing success rates. Our work provides a viable path toward scalable\ntraining of diverse dexterous hand behaviors in embodied intelligence by\noffering a simulation-based solution to synthetic data generation. Our website:\nhttps://winniechen2002.github.io/GenDexHand/.", "AI": {"tldr": "GenDexHand是一个生成式仿真管线，通过视觉语言模型（VLM）反馈的闭环优化和任务分解，自主生成多样化的灵巧操作任务和环境，以解决具身智能中灵巧手数据稀缺的瓶颈。", "motivation": "具身智能面临数据稀缺的根本性瓶颈，尤其对于灵巧操作而言，现有基于LLM的夹爪模拟生成方法效果不佳，且灵巧操作因自由度高而更具挑战性。大规模生成可行且可训练的灵巧手任务仍是一个开放性难题。", "method": "本文提出了GenDexHand，一个生成式仿真管线，用于自主生成多样化的机器人灵巧操作任务和环境。该方法引入了一个闭环优化过程，根据视觉语言模型（VLM）的反馈调整物体放置和缩放。每个任务进一步分解为子任务，以实现顺序强化学习。", "result": "GenDexHand显著提高了生成环境的平均质量。任务分解减少了训练时间并提高了成功率。这项工作为具身智能中多样化灵巧手行为的可扩展训练提供了一条可行途径。", "conclusion": "GenDexHand提供了一个基于仿真的合成数据生成解决方案，有效解决了灵巧操作的数据稀缺问题，为具身智能中多样化灵巧手行为的可扩展训练铺平了道路。"}}
{"id": "2511.01824", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01824", "abs": "https://arxiv.org/abs/2511.01824", "authors": ["Yuetai Li", "Huseyin A Inan", "Xiang Yue", "Wei-Ning Chen", "Lukas Wutschitz", "Janardhan Kulkarni", "Radha Poovendran", "Robert Sim", "Saravan Rajmohan"], "title": "Simulating Environments with Reasoning Models for Agent Training", "comment": null, "summary": "LLM agents excel in compact environments requiring deep reasoning but remain\nbrittle when operating in broader, more complex contexts that demand robustness\nacross diverse tools and schemas. Building bespoke environments for training is\nheavy, brittle, and limits progress. In this paper, we demonstrate that LLMs\ncan simulate realistic environment feedback without access to actual testbed\ndata or APIs. Inspired by this capability, we propose two frameworks:\nSimia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets\ninto diverse trajectories in an environment-agnostic manner, and Simia-RL, a\nframework that enables RL training without real environment implementations\nthrough LLM-simulated feedback. Fine-tuning open models yields consistent\nimprovements across multiple benchmarks, surpassing GPT-4o and approaching\no4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable\nagent training without environment engineering, replacing heavy and brittle\nimplementations with flexible LLM-based simulation.", "AI": {"tldr": "本文提出Simia-SFT和Simia-RL框架，利用大型语言模型（LLM）模拟环境反馈，实现无需复杂环境工程的LLM代理可扩展训练，并在多个基准测试中取得了显著提升。", "motivation": "LLM代理在复杂环境中表现脆弱，且为训练构建定制环境成本高昂且限制进展。现有方法难以在需要多样化工具和模式的广泛复杂环境中实现鲁棒性。", "method": "研究利用LLM模拟真实环境反馈，无需实际测试数据或API。在此基础上提出了两个框架：1) Simia-SFT，通过LLM将少量初始数据集扩展为多样化的训练轨迹，以合成监督微调（SFT）数据；2) Simia-RL，通过LLM模拟反馈进行强化学习（RL）训练，无需真实环境实现。", "result": "通过Simia框架微调的开源模型在多个基准测试中表现出持续改进，超越了GPT-4o，并在$\\tau^2$-Bench上接近o4-mini的性能。", "conclusion": "Simia-SFT和Simia-RL通过用灵活的LLM模拟取代繁重且脆弱的实现，实现了无需环境工程的可扩展代理训练，解决了LLM代理在复杂环境中鲁棒性差和环境构建成本高的问题。"}}
{"id": "2511.00503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00503", "abs": "https://arxiv.org/abs/2511.00503", "authors": ["Panwang Pan", "Chenguo Lin", "Jingjing Zhao", "Chenxin Li", "Yuchen Lin", "Haopeng Li", "Honglei Yan", "Kairun Wen", "Yunlong Lin", "Yixuan Yuan", "Yadong Mu"], "title": "Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models", "comment": null, "summary": "We introduce Diff4Splat, a feed-forward method that synthesizes controllable\nand explicit 4D scenes from a single image. Our approach unifies the generative\npriors of video diffusion models with geometry and motion constraints learned\nfrom large-scale 4D datasets. Given a single input image, a camera trajectory,\nand an optional text prompt, Diff4Splat directly predicts a deformable 3D\nGaussian field that encodes appearance, geometry, and motion, all in a single\nforward pass, without test-time optimization or post-hoc refinement. At the\ncore of our framework lies a video latent transformer, which augments video\ndiffusion models to jointly capture spatio-temporal dependencies and predict\ntime-varying 3D Gaussian primitives. Training is guided by objectives on\nappearance fidelity, geometric accuracy, and motion consistency, enabling\nDiff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate\nthe effectiveness of Diff4Splatacross video generation, novel view synthesis,\nand geometry extraction, where it matches or surpasses optimization-based\nmethods for dynamic scene synthesis while being significantly more efficient.", "AI": {"tldr": "Diff4Splat 是一种前馈方法，可从单张图像合成可控且显式的 4D 场景，它结合了视频扩散模型的生成先验和从 4D 数据集中学习的几何与运动约束。", "motivation": "研究旨在解决从单张图像高效、高质量地合成可控 4D 场景的挑战，避免测试时优化和后处理。", "method": "该方法将视频扩散模型的生成先验与从大规模 4D 数据集中学习的几何和运动约束相结合。给定单张输入图像、相机轨迹和可选文本提示，Diff4Splat 通过单一前向传播直接预测一个可变形的 3D 高斯场，编码外观、几何和运动。其核心是一个视频潜在变换器，用于捕获时空依赖性并预测时变 3D 高斯基元。训练由外观保真度、几何精度和运动一致性目标指导。", "result": "Diff4Splat 能够在 30 秒内合成高质量的 4D 场景，并在视频生成、新视图合成和几何提取方面表现出有效性。它在动态场景合成方面匹配或超越了基于优化的方法，同时效率显著更高。", "conclusion": "Diff4Splat 是一种高效、前馈的方法，能够从单张图像合成高质量、可控的 4D 场景，其性能在质量上可与基于优化的方法媲美，并在效率上显著超越。"}}
{"id": "2511.01287", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01287", "abs": "https://arxiv.org/abs/2511.01287", "authors": ["Qin Zhou", "Zhexin Zhang", "Zhi Li", "Limin Sun"], "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers", "comment": null, "summary": "With the rapid advancement of AI models, their deployment across diverse\ntasks has become increasingly widespread. A notable emerging application is\nleveraging AI models to assist in reviewing scientific papers. However, recent\nreports have revealed that some papers contain hidden, injected prompts\ndesigned to manipulate AI reviewers into providing overly favorable\nevaluations. In this work, we present an early systematic investigation into\nthis emerging threat. We propose two classes of attacks: (1) static attack,\nwhich employs a fixed injection prompt, and (2) iterative attack, which\noptimizes the injection prompt against a simulated reviewer model to maximize\nits effectiveness. Both attacks achieve striking performance, frequently\ninducing full evaluation scores when targeting frontier AI reviewers.\nFurthermore, we show that these attacks are robust across various settings. To\ncounter this threat, we explore a simple detection-based defense. While it\nsubstantially reduces the attack success rate, we demonstrate that an adaptive\nattacker can partially circumvent this defense. Our findings underscore the\nneed for greater attention and rigorous safeguards against prompt-injection\nthreats in AI-assisted peer review.", "AI": {"tldr": "本文系统研究了针对AI辅助同行评审的提示注入攻击，提出了静态和迭代两种攻击方式，发现它们能有效诱导AI评审员给出高分，并探讨了一种可被部分规避的防御方法，强调了加强防范的必要性。", "motivation": "随着AI模型在科学论文评审等任务中的广泛应用，有报告指出一些论文中包含隐藏的注入提示，旨在操纵AI评审员给出过于有利的评价。本研究旨在对这种新兴威胁进行早期系统性调查。", "method": "研究提出了两类攻击方法：1) 静态攻击，使用固定的注入提示；2) 迭代攻击，通过针对模拟评审模型优化注入提示来最大化其有效性。此外，本文还探索了一种基于检测的简单防御机制。", "result": "两种攻击（静态和迭代）都取得了显著效果，能频繁地诱导前沿AI评审员给出满分评价。这些攻击在不同设置下表现出鲁棒性。虽然所提出的检测防御方法能大幅降低攻击成功率，但自适应攻击者可以部分规避该防御。", "conclusion": "研究结果强调了在AI辅助同行评审中，需要更多关注并采取严格的防护措施来应对提示注入威胁。"}}
{"id": "2511.01797", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01797", "abs": "https://arxiv.org/abs/2511.01797", "authors": ["Javier Ballesteros-Jerez", "Jesus Martínez-Gómez", "Ismael García-Varea", "Luis Orozco-Barbosa", "Manuel Castillo-Cara"], "title": "Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator", "comment": "13 pages, 7 figures. Conference paper (ROBOVIS 2025)", "summary": "We present a hybrid neural network model for inferring the position of mobile\nrobots using Channel State Information (CSI) data from a Massive MIMO system.\nBy leveraging an existing CSI dataset, our approach integrates a Convolutional\nNeural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural\nNetwork (HyNN) that estimates 2D robot positions. CSI readings are converted\ninto synthetic images using the TINTO tool. The localisation solution is\nintegrated with a robotics simulator, and the Robot Operating System (ROS),\nwhich facilitates its evaluation through heterogeneous test cases, and the\nadoption of state estimators like Kalman filters. Our contributions illustrate\nthe potential of our HyNN model in achieving precise indoor localisation and\nnavigation for mobile robots in complex environments. The study follows, and\nproposes, a generalisable procedure applicable beyond the specific use case\nstudied, making it adaptable to different scenarios and datasets.", "AI": {"tldr": "该研究提出了一种混合神经网络（HyNN）模型，利用大规模MIMO系统的信道状态信息（CSI）数据，通过将CSI转换为合成图像，实现移动机器人的二维室内高精度定位。", "motivation": "在复杂环境中为移动机器人提供精确的室内定位和导航是一个挑战，该研究旨在利用大规模MIMO的CSI数据解决这一问题。", "method": "研究方法包括：1. 提出混合神经网络（HyNN），结合卷积神经网络（CNN）和多层感知器（MLP）。2. 使用TINTO工具将CSI数据转换为合成图像作为网络输入。3. 模型输出二维机器人位置。4. 将定位解决方案与机器人模拟器和ROS集成，以便通过异构测试用例进行评估，并可结合卡尔曼滤波器等状态估计器。", "result": "研究结果表明，所提出的HyNN模型在实现复杂环境中移动机器人的精确室内定位和导航方面具有巨大潜力。此外，该研究提出了一种可推广的程序，适用于特定用例之外的不同场景和数据集。", "conclusion": "该HyNN模型能够有效地利用CSI数据实现移动机器人的高精度室内定位，且所提出的方法具有良好的通用性和可适应性。"}}
{"id": "2511.01289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01289", "abs": "https://arxiv.org/abs/2511.01289", "authors": ["Saiyma Sittul Muna", "Rezwan Islam Salvi", "Mushfiqur Rahman Mushfique", "Ajwad Abrar"], "title": "FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings", "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop,\n  co-located with NeurIPS 2025", "summary": "In emergency situations, every second counts. The deployment of Large\nLanguage Models (LLMs) in time-sensitive, low or zero-connectivity environments\nremains limited. Current models are computationally intensive and unsuitable\nfor low-tier devices often used by first responders or civilians. A major\nbarrier to developing lightweight, domain-specific solutions is the lack of\nhigh-quality datasets tailored to first aid and emergency response. To address\nthis gap, we introduce FirstAidQA, a synthetic dataset containing 5,500\nhigh-quality question answer pairs that encompass a wide range of first aid and\nemergency response scenarios. The dataset was generated using a Large Language\nModel, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from\nthe Vital First Aid Book (2019). We applied preprocessing steps such as text\ncleaning, contextual chunking, and filtering, followed by human validation to\nensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is\ndesigned to support instruction-tuning and fine-tuning of LLMs and Small\nLanguage Models (SLMs), enabling faster, more reliable, and offline-capable\nsystems for emergency settings. We publicly release the dataset to advance\nresearch on safety-critical and resource-constrained AI applications in first\naid and emergency response. The dataset is available on Hugging Face at\nhttps://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.", "AI": {"tldr": "本文介绍了FirstAidQA，一个包含5,500个高质量急救和紧急响应问答对的合成数据集，旨在支持轻量级语言模型在资源受限的紧急环境中的部署。", "motivation": "现有大型语言模型（LLMs）计算密集，不适用于急救人员或普通民众使用的低端设备，且在时间敏感、低或零连接环境中部署受限。此外，缺乏高质量的、针对急救和紧急响应领域的特定数据集，阻碍了轻量级、领域特定解决方案的开发。", "method": "研究人员使用大型语言模型ChatGPT-4o-mini，结合提示式上下文学习，从《Vital First Aid Book (2019)》的文本中生成了FirstAidQA数据集。生成过程包括文本清洗、上下文分块和过滤等预处理步骤，并进行了人工验证，以确保问答对的准确性、安全性和实用相关性。", "result": "本文成功创建并发布了FirstAidQA数据集，包含5,500个高质量的急救和紧急响应问答对。该数据集专为支持LLMs和小型语言模型（SLMs）的指令微调和微调而设计，旨在实现更快、更可靠且离线可用的紧急系统。", "conclusion": "FirstAidQA数据集的发布有助于推动在急救和紧急响应领域中，安全关键和资源受限AI应用的研究。它将使开发适用于紧急情况的、更快、更可靠且离线运行的AI系统成为可能。"}}
{"id": "2511.00511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00511", "abs": "https://arxiv.org/abs/2511.00511", "authors": ["Panwang Pan", "Jingjing Zhao", "Yuchen Lin", "Chenguo Lin", "Chenxin Li", "Haopeng Li", "Honglei Yan", "Tingting Shen", "Yadong Mu"], "title": "ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation", "comment": null, "summary": "Video generative models pretrained on large-scale datasets can produce\nhigh-quality videos, but are often conditioned on text or a single image,\nlimiting controllability and applicability. We introduce ID-Composer, a novel\nframework that addresses this gap by tackling multi-subject video generation\nfrom a text prompt and reference images. This task is challenging as it\nrequires preserving subject identities, integrating semantics across subjects\nand modalities, and maintaining temporal consistency. To faithfully preserve\nthe subject consistency and textual information in synthesized videos,\nID-Composer designs a \\textbf{hierarchical identity-preserving attention\nmechanism}, which effectively aggregates features within and across subjects\nand modalities. To effectively allow for the semantic following of user\nintention, we introduce \\textbf{semantic understanding via pretrained\nvision-language model (VLM)}, leveraging VLM's superior semantic understanding\nto provide fine-grained guidance and capture complex interactions between\nmultiple subjects. Considering that standard diffusion loss often fails in\naligning the critical concepts like subject ID, we employ an \\textbf{online\nreinforcement learning phase} to drive the overall training objective of\nID-Composer into RLVR. Extensive experiments demonstrate that our model\nsurpasses existing methods in identity preservation, temporal consistency, and\nvideo quality.", "AI": {"tldr": "ID-Composer是一个新颖的框架，通过引入分层身份保留注意力机制、预训练视觉-语言模型进行语义理解和在线强化学习，解决了从文本提示和多张参考图像生成多主体视频的挑战。", "motivation": "现有的视频生成模型通常仅限于文本或单张图像作为条件，这限制了其可控性和适用性，尤其是在需要生成具有多个主体的视频时，难以保持主体身份、整合语义并维持时间一致性。", "method": "ID-Composer采用了以下方法：1. **分层身份保留注意力机制**：有效聚合主体内部和跨主体及模态的特征，以忠实地保留主体一致性和文本信息。2. **通过预训练视觉-语言模型（VLM）进行语义理解**：利用VLM卓越的语义理解能力，提供细粒度指导并捕捉多主体间的复杂交互。3. **在线强化学习阶段**：解决标准扩散损失在对齐主体ID等关键概念方面的不足，将整体训练目标驱动到RLVR。", "result": "广泛的实验证明，ID-Composer模型在身份保留、时间一致性和视频质量方面均超越了现有方法。", "conclusion": "ID-Composer成功地解决了从文本提示和多张参考图像生成多主体视频的挑战，显著提升了视频生成的可控性和质量，特别是在身份保留和时间一致性方面表现优异。"}}
{"id": "2511.00480", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00480", "abs": "https://arxiv.org/abs/2511.00480", "authors": ["Weihao Bo", "Yanpeng Sun", "Yu Wang", "Xinyu Zhang", "Zechao Li"], "title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts", "comment": null, "summary": "In this paper, we introduce FedMGP, a new paradigm for personalized federated\nprompt learning in vision-language models. FedMGP equips each client with\nmultiple groups of paired textual and visual prompts, enabling the model to\ncapture diverse, fine-grained semantic and instance-level cues. A diversity\nloss is introduced to drive each prompt group to specialize in distinct and\ncomplementary semantic aspects, ensuring that the groups collectively cover a\nbroader range of local characteristics. During communication, FedMGP employs a\ndynamic prompt aggregation strategy based on similarity-guided probabilistic\nsampling: each client computes the cosine similarity between its prompt groups\nand the global prompts from the previous round, then samples s groups via a\nsoftmax-weighted distribution. This soft selection mechanism preferentially\naggregates semantically aligned knowledge while still enabling exploration of\nunderrepresented patterns effectively balancing the preservation of common\nknowledge with client-specific features. Notably, FedMGP maintains parameter\nefficiency by redistributing a fixed prompt capacity across multiple groups,\nachieving state-of-the-art performance with the lowest communication parameters\namong all federated prompt learning methods. Theoretical analysis shows that\nour dynamic aggregation strategy promotes robust global representation learning\nby reinforcing shared semantics while suppressing client-specific noise.\nExtensive experiments demonstrate that FedMGP consistently outperforms prior\napproaches in both personalization and domain generalization across diverse\nfederated vision-language benchmarks. The code will be released on\nhttps://github.com/weihao-bo/FedMGP.git.", "AI": {"tldr": "本文提出了FedMGP，一种用于视觉-语言模型个性化联邦提示学习的新范式，通过多组提示、多样性损失和动态聚合策略，实现了最先进的性能和最低的通信参数。", "motivation": "在视觉-语言模型中，需要一种能够捕捉多样化、细粒度语义和实例级线索的个性化联邦提示学习方法，同时平衡通用知识的保留与客户端特定特征，并保持参数效率。", "method": "FedMGP为每个客户端配备多组成对的文本和视觉提示；引入多样性损失以促使每组提示专注于不同且互补的语义方面；采用基于相似度引导概率采样的动态提示聚合策略，通过余弦相似度和softmax加权分布选择性聚合语义对齐的知识；通过在多组中重新分配固定提示容量来保持参数效率；进行了理论分析以证明动态聚合策略的有效性。", "result": "FedMGP在所有联邦提示学习方法中，以最低的通信参数实现了最先进的性能。它在个性化和领域泛化方面，始终优于现有方法。理论分析表明，动态聚合策略通过强化共享语义和抑制客户端特定噪声，促进了鲁棒的全局表示学习。", "conclusion": "FedMGP是一种有效且高效的个性化联邦提示学习范式，其设计能够捕捉多样化特征、平衡通用知识与客户端特定需求，并在视觉-语言基准测试中展现出卓越的性能和参数效率。"}}
{"id": "2511.00504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00504", "abs": "https://arxiv.org/abs/2511.00504", "authors": ["Hai-Dang Nguyen", "Ha-Hieu Pham", "Hao T. Nguyen", "Huy-Hieu Pham"], "title": "VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning", "comment": "ISBI submission. Contains 5 pages, 2 figures, and 6 tables. Code &\n  data: https://huggingface.co/datasets/Dangindev/VinDR-CXR-VQA", "summary": "We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable\nMedical Visual Question Answering (Med-VQA) with spatial grounding. The dataset\ncontains 17,597 question-answer pairs across 4,394 images, each annotated with\nradiologist-verified bounding boxes and clinical reasoning explanations. Our\nquestion taxonomy spans six diagnostic types-Where, What, Is there, How many,\nWhich, and Yes/No-capturing diverse clinical intents. To improve reliability,\nwe construct a balanced distribution of 41.7% positive and 58.3% negative\nsamples, mitigating hallucinations in normal cases. Benchmarking with\nMedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over\nbaseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance\nreproducible and clinically grounded Med-VQA research. The dataset and\nevaluation tools are publicly available at\nhuggingface.co/datasets/Dangindev/VinDR-CXR-VQA.", "AI": {"tldr": "本文介绍了VinDr-CXR-VQA，一个用于可解释医学视觉问答（Med-VQA）并具有空间定位功能的大规模胸部X光数据集。该数据集包含17,597个问答对，覆盖4,394张图像，并带有放射科医生验证的边界框和临床推理解释。基准测试显示，该数据集使模型性能F1值提升了11.8%，并支持病灶定位。", "motivation": "研究动机是为了推动可复现且具有临床依据的医学视觉问答（Med-VQA）研究，特别是通过提供一个大规模、高质量、具有空间定位和解释能力的胸部X光数据集，以解决现有数据集中可能存在的幻觉问题，并提升模型可靠性。", "method": "研究方法是构建了一个名为VinDr-CXR-VQA的大规模胸部X光数据集。该数据集包含4,394张图像和17,597个问答对，每个问答对都由放射科医生验证的边界框和临床推理解释进行标注。问题分类涵盖六种诊断类型，并构建了平衡的阳性（41.7%）和阴性（58.3%）样本分布，以减少正常情况下的幻觉。研究还使用MedGemma-4B-it模型进行了基准测试。", "result": "基准测试结果显示，使用VinDr-CXR-VQA数据集训练的MedGemma-4B-it模型性能显著提升（F1 = 0.624），比基线提高了11.8%，同时能够实现病灶定位。这表明该数据集有效提高了Med-VQA模型的可靠性和解释性。", "conclusion": "VinDr-CXR-VQA是一个重要且公开可用的资源，它通过提供大规模、高质量、包含空间定位和临床推理解释的胸部X光问答数据，显著推动了可复现和具有临床依据的医学视觉问答（Med-VQA）研究。该数据集能够提高模型的性能并使其具备病灶定位能力。"}}
{"id": "2511.00523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00523", "abs": "https://arxiv.org/abs/2511.00523", "authors": ["Fangyu Wu", "Yujun Cai"], "title": "SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation", "comment": null, "summary": "Vision language models such as CLIP have shown remarkable performance in zero\nshot classification, but remain susceptible to spurious correlations, where\nirrelevant visual features influence predictions. Existing debiasing methods\noften require access to training data and explicit group labels to perform\nfine-tuning or adjust embeddings, which limits their practicality in real-world\nsettings. Test-time methods attempt to avoid this constraint, but many still\ndepend on prior knowledge of dataset specific biases, limiting their\ngeneralizability in open set settings. In this work, we propose a test-time\ndebiasing method for ViT based CLIP models that requires no additional training\nor assumptions of bias annotations. Our approach uses a pretrained segmentation\nmodel to isolate the target visual attribute, then adjusts the non target\nregions so that their embeddings are uniformly similar to all class specific\ntext prompts. This procedure removes unintended bias signals from confounding\nvisual regions while preserving the target attribute. Experiments on Waterbirds\nand CelebA show that our method outperforms existing test-time debiasing\napproaches in both group robustness metrics and Attention IoU. These results\ndemonstrate the effectiveness of segmentation guided interventions for scalable\nand annotation free bias mitigation in vision language models.", "AI": {"tldr": "本文提出了一种无需额外训练或偏置标注的测试时去偏方法，通过预训练分割模型隔离目标视觉属性，并调整非目标区域的嵌入，以减轻ViT-based CLIP模型中的虚假关联。", "motivation": "现有的去偏方法通常需要训练数据、明确的组标签或数据集特定的偏置先验知识，这限制了它们在实际应用和开放集设置中的实用性和通用性。", "method": "该方法是一种针对ViT-based CLIP模型的测试时去偏方法。它使用预训练的分割模型来隔离目标视觉属性，然后调整非目标区域，使其嵌入与所有类别特定的文本提示均匀相似，从而在保留目标属性的同时消除混淆视觉区域的意外偏置信号。", "result": "在Waterbirds和CelebA数据集上的实验表明，该方法在组鲁棒性指标和注意力IoU方面均优于现有的测试时去偏方法。", "conclusion": "这些结果证明了分割引导干预在视觉语言模型中实现可扩展且无需标注的偏置缓解方面的有效性。"}}
{"id": "2511.01381", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01381", "abs": "https://arxiv.org/abs/2511.01381", "authors": ["Hitesh Kyatham", "Arjun Suresh", "Aadi Palnitkar", "Yiannis Aloimonos"], "title": "EREBUS: End-to-end Robust Event Based Underwater Simulation", "comment": "Accepted to ICRA AQUA2SIM Workshop 2025, 6 pages, 3 figures,\n  conference paper", "summary": "The underwater domain presents a vast array of challenges for roboticists and\ncomputer vision researchers alike, such as poor lighting conditions and high\ndynamic range scenes. In these adverse conditions, traditional vision\ntechniques struggle to adapt and lead to suboptimal performance. Event-based\ncameras present an attractive solution to this problem, mitigating the issues\nof traditional cameras by tracking changes in the footage on a frame-by-frame\nbasis. In this paper, we introduce a pipeline which can be used to generate\nrealistic synthetic data of an event-based camera mounted to an AUV (Autonomous\nUnderwater Vehicle) in an underwater environment for training vision models. We\ndemonstrate the effectiveness of our pipeline using the task of rock detection\nwith poor visibility and suspended particulate matter, but the approach can be\ngeneralized to other underwater tasks.", "AI": {"tldr": "针对水下恶劣环境，传统相机表现不佳。本文提出一个管道，用于生成AUV上事件相机在水下环境的逼真合成数据，以训练视觉模型，并通过水下岩石检测任务验证其有效性。", "motivation": "水下环境（如光照差、高动态范围）对机器人和计算机视觉研究构成巨大挑战，导致传统视觉技术性能不佳。", "method": "引入一个管道，能够生成AUV上事件相机在水下环境的逼真合成数据，用于训练视觉模型。", "result": "通过在低能见度和悬浮颗粒物条件下的岩石检测任务，证明了所提出管道的有效性。该方法可推广到其他水下任务。", "conclusion": "所提出的合成数据生成管道能有效解决水下恶劣环境的视觉问题，为训练事件相机视觉模型提供逼真数据，并具有通用性。"}}
{"id": "2511.01223", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01223", "abs": "https://arxiv.org/abs/2511.01223", "authors": ["Zahra Mehraban", "Sebastien Glaser", "Michael Milford", "Ronald Schroeter"], "title": "Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering", "comment": null, "summary": "Domain adaptation is required for automated driving models to generalize well\nacross diverse road conditions. This paper explores a training method for\ndomain adaptation to adapt PilotNet, an end-to-end deep learning-based model,\nfor left-hand driving conditions using real-world Australian highway data. Four\ntraining methods were evaluated: (1) a baseline model trained on U.S.\nright-hand driving data, (2) a model trained on flipped U.S. data, (3) a model\npretrained on U.S. data and then fine-tuned on Australian highways, and (4) a\nmodel pretrained on flipped U.S. data and then finetuned on Australian\nhighways. This setup examines whether incorporating flipped data enhances the\nmodel adaptation by providing an initial left-hand driving alignment. The paper\ncompares model performance regarding steering prediction accuracy and\nattention, using saliency-based analysis to measure attention shifts across\nsignificant road regions. Results show that pretraining on flipped data alone\nworsens prediction stability due to misaligned feature representations, but\nsignificantly improves adaptation when followed by fine-tuning, leading to\nlower prediction error and stronger focus on left-side cues. To validate this\napproach across different architectures, the same experiments were done on\nResNet, which confirmed similar adaptation trends. These findings emphasize the\nimportance of preprocessing techniques, such as flipped-data pretraining,\nfollowed by fine-tuning to improve model adaptation with minimal retraining\nrequirements.", "AI": {"tldr": "本文研究了一种域适应训练方法，通过翻转数据预训练结合微调，显著提高了自动驾驶模型PilotNet在左侧驾驶条件下的性能和注意力分布，并验证了其对不同架构的有效性。", "motivation": "自动驾驶模型需要良好的泛化能力以适应不同的道路条件。本研究旨在探索一种域适应训练方法，使基于深度学习的端到端模型（如PilotNet）能够适应左侧驾驶条件，特别是使用真实的澳大利亚高速公路数据。", "method": "研究评估了四种训练方法：(1) 在美国右侧驾驶数据上训练的基线模型；(2) 在翻转的美国数据上训练的模型；(3) 在美国数据上预训练后在澳大利亚高速公路上微调的模型；(4) 在翻转的美国数据上预训练后在澳大利亚高速公路上微调的模型。通过转向预测精度和注意力（使用显著性分析衡量）来比较模型性能。同时，在ResNet架构上重复实验以验证方法的普适性。", "result": "结果显示，单独使用翻转数据预训练会因特征表示不一致而降低预测稳定性。然而，当翻转数据预训练后结合微调时，模型的适应性显著提高，预测误差降低，并且注意力更集中于左侧线索。在ResNet上的实验也证实了类似的适应趋势。", "conclusion": "研究强调了预处理技术（如翻转数据预训练）结合微调对于提高模型域适应能力的重要性，且仅需最少的再训练。这为自动驾驶模型在不同驾驶条件下的泛化提供了有效的策略。"}}
{"id": "2511.01305", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.01305", "abs": "https://arxiv.org/abs/2511.01305", "authors": ["Aman Ganapathy Manvattira", "Yifei Xu", "Ziyue Dang", "Songwu Lu"], "title": "DeepSpecs: Expert-Level Questions Answering in 5G", "comment": null, "summary": "5G technology enables mobile Internet access for billions of users. Answering\nexpert-level questions about 5G specifications requires navigating thousands of\npages of cross-referenced standards that evolve across releases. Existing\nretrieval-augmented generation (RAG) frameworks, including telecom-specific\napproaches, rely on semantic similarity and cannot reliably resolve\ncross-references or reason about specification evolution. We present DeepSpecs,\na RAG system enhanced by structural and temporal reasoning via three\nmetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB\n(line-level version diffs), and TDocDB (standardization meeting documents).\nDeepSpecs explicitly resolves cross-references by recursively retrieving\nreferenced clauses through metadata lookup, and traces specification evolution\nby mining changes and linking them to Change Requests that document design\nrationale. We curate two 5G QA datasets: 573 expert-annotated real-world\nquestions from practitioner forums and educational resources, and 350\nevolution-focused questions derived from approved Change Requests. Across\nmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-art\ntelecom RAG systems; ablations confirm that explicit cross-reference resolution\nand evolution-aware retrieval substantially improve answer quality,\nunderscoring the value of modeling the structural and temporal properties of 5G\nstandards.", "AI": {"tldr": "DeepSpecs是一个增强型RAG系统，通过结构化和时间推理，解决5G规范中交叉引用和演变问题，优于现有系统。", "motivation": "专家级5G规范问题需要查阅大量相互关联且不断演进的标准，现有RAG框架（包括电信专用方案）无法可靠地解决交叉引用或推理规范演变。", "method": "DeepSpecs通过三个富含元数据的数据库（SpecDB、ChangeDB、TDocDB）增强RAG。它通过元数据查找递归解析交叉引用，并通过挖掘变更并将其与变更请求关联来追踪规范演变。", "result": "DeepSpecs在多个LLM后端上优于基础模型和最先进的电信RAG系统。消融实验证实，显式交叉引用解析和演变感知检索显著提高了答案质量。", "conclusion": "建模5G标准的结构和时间属性对于提高专家级问题回答的质量至关重要。"}}
{"id": "2511.01282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01282", "abs": "https://arxiv.org/abs/2511.01282", "authors": ["Min Fang", "Zhihui Fu", "Qibin Zhao", "Jun Wang"], "title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "comment": null, "summary": "Speculative decoding (SD) has emerged as an effective technique to accelerate\nlarge language model (LLM) inference without compromising output quality.\nHowever, the achievable speedup largely depends on the effectiveness of the\ndrafting model. While model-based methods like EAGLE-2 are accurate but costly,\nretrieval-enhanced methods like SAM-Decoding rely on heuristic switching\nstrategies that often trigger unnecessary retrievals. To address this, we\npropose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a\nnovel framework that transforms heuristic drafter switching into adaptive\ndecision-making. ReSpec features three core innovations: 1) An\n\\textbf{entropy-guided adaptive trigger} quantifies contextual predictability\nto initiate retrieval only when uncertainty is low, avoiding costly low-quality\nspeculations. 2) A \\textbf{feedback-driven candidate selection} leverages\nhistorical feedback to organize multiple high-quality candidates for parallel\nverification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed\nverification strategy} applies strict checks to model-generated drafts while\nusing a relaxed verification for retrieved drafts, achieving a better balance\nbetween accuracy and efficiency. Extensive experiments on Spec-Bench\ndemonstrate that ReSpec achieves state-of-the-art acceleration,outperforming\nEAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while\nmaintaining output quality.", "AI": {"tldr": "ReSpec是一个检索增强型推测解码框架，通过自适应决策将启发式草稿模型切换转化为更高效的策略，显著加速大型语言模型推理，同时保持输出质量。", "motivation": "推测解码的加速效果受草稿模型效率限制。现有方法要么（如EAGLE-2）成本高昂，要么（如SAM-Decoding）依赖启发式切换策略，导致不必要的检索和低质量推测。", "method": "ReSpec框架包含三项核心创新：1) 基于熵的自适应触发器，在不确定性低时才启动检索；2) 反馈驱动的候选选择，利用历史反馈组织多个高质量候选进行并行验证；3) 源感知松弛验证策略，对模型生成的草稿进行严格检查，对检索到的草稿进行松弛验证。", "result": "ReSpec在Spec-Bench上实现了最先进的加速效果，性能分别超越EAGLE-2和SAM-Decoding超过33%和25%，同时保持了输出质量。", "conclusion": "ReSpec通过自适应地管理检索过程，解决了现有推测解码方法的效率问题，显著提升了LLM推理速度，且不影响输出质量。"}}
{"id": "2511.00580", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.5.4; I.4.8; C.3"], "pdf": "https://arxiv.org/pdf/2511.00580", "abs": "https://arxiv.org/abs/2511.00580", "authors": ["Yousuf Ahmed Siddiqui", "Sufiyaan Usmani", "Umer Tariq", "Jawwad Ahmed Shamsi", "Muhammad Burhan Khan"], "title": "TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection", "comment": "10 pages, 5 figures", "summary": "Video anomalies often depend on contextual information available and temporal\nevolution. Non-anomalous action in one context can be anomalous in some other\ncontext. Most anomaly detectors, however, do not notice this type of context,\nwhich seriously limits their capability to generalize to new, real-life\nsituations. Our work addresses the context-aware zero-shot anomaly detection\nchallenge, in which systems need to learn adaptively to detect new events by\ncorrelating temporal and appearance features with textual traces of memory in\nreal time. Our approach defines a memory-augmented pipeline, correlating\ntemporal signals with visual embeddings using cross-attention, and real-time\nzero-shot anomaly classification by contextual similarity scoring. We achieve\n90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art\namong zero-shot models. Our model achieves real-time inference with high\nprecision and explainability for deployment. We show that, by fusing\ncross-attention temporal fusion and contextual memory, we achieve high fidelity\nanomaly detection, a step towards the applicability of zero-shot models in\nreal-world surveillance and infrastructure monitoring.", "AI": {"tldr": "本文提出了一种记忆增强的零样本异常检测方法，通过结合上下文信息、时间演化和视觉特征，实现了对视频中新事件的实时、高精度异常检测。", "motivation": "现有异常检测器未能考虑上下文信息，导致其泛化能力受限，无法适应真实世界的复杂场景，因为在不同上下文中，相同的行为可能表现为正常或异常。", "method": "该方法构建了一个记忆增强的管道，利用交叉注意力机制关联时间信号与视觉嵌入，并通过上下文相似度评分进行实时零样本异常分类。", "result": "该模型在UCF-Crime数据集上取得了90.4%的AUC，在XD-Violence数据集上取得了83.67%的AP，刷新了零样本模型的最新SOTA。此外，模型实现了实时推理，并具有高精度和可解释性。", "conclusion": "通过融合交叉注意力时间融合和上下文记忆，该方法实现了高保真度的异常检测，是零样本模型在真实世界监控和基础设施监测中应用的重要一步。"}}
{"id": "2511.00524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00524", "abs": "https://arxiv.org/abs/2511.00524", "authors": ["Jihao Gu", "Kun Li", "He Wang", "Kaan Akşit"], "title": "Text-guided Fine-Grained Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events within video\nsegments. In scenarios such as surveillance or industrial process monitoring,\nanomaly detection is of critical importance. While existing approaches are\nsemi-automated, requiring human assessment for anomaly detection, traditional\nVADs offer limited output as either normal or anomalous. We propose Text-guided\nFine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large\nVision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)\nthat performs pixel-wise visual-textual feature alignment to generate\nfine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly\nEncoder (RAE) that transforms the heatmaps into learnable textual embeddings,\nguiding the LVLM to accurately identify and localize anomalous events in\nvideos. This significantly enhances both the granularity and interactivity of\nanomaly detection. The proposed method achieving SOTA performance by\ndemonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and\n67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,\nand subjectively verified more preferable textual description on the\nShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;\nYes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for\ntargets, 78.10 for trajectories; Yes/No accuracy: 89.73%).", "AI": {"tldr": "本文提出T-VAD框架，利用大型视觉语言模型（LVLM）实现细粒度视频异常检测，通过生成异常热图和文本描述，显著提升了检测的粒度和交互性。", "motivation": "现有视频异常检测（VAD）方法通常是半自动的，需要人工评估，且输出仅限于“正常”或“异常”，缺乏细粒度信息。在监控和工业过程监测等场景中，细致的异常检测至关重要。", "method": "本文提出了Text-guided Fine-Grained Video Anomaly Detection (T-VAD) 框架，基于大型视觉语言模型（LVLM）。它包含一个异常热图解码器（AHD），用于执行像素级视觉-文本特征对齐以生成细粒度异常热图；以及一个区域感知异常编码器（RAE），将热图转换为可学习的文本嵌入，指导LVLM准确识别和定位视频中的异常事件。", "result": "T-VAD在UBnormal数据集上实现了SOTA性能，微AUC达到94.8%，异常热图准确率（RBDC/TBDC）分别为67.8%/76.7%。在ShanghaiTech-based数据集上，目标和轨迹的BLEU-4分数分别为62.67和88.84，Yes/No准确率为97.67%。在UBnormal数据集上，目标和轨迹的BLEU-4分数分别为50.32和78.10，Yes/No准确率为89.73%。", "conclusion": "所提出的T-VAD方法通过生成细粒度异常热图和提供文本描述，显著增强了异常检测的粒度和交互性，并在多个数据集上取得了最先进的性能和更优的文本描述效果。"}}
{"id": "2511.01501", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01501", "abs": "https://arxiv.org/abs/2511.01501", "authors": ["Yufeng Jin", "Niklas Funk", "Vignesh Prasad", "Zechu Li", "Mathias Franzius", "Jan Peters", "Georgia Chalvatzaki"], "title": "SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation", "comment": null, "summary": "Object pose estimation is a fundamental problem in robotics and computer\nvision, yet it remains challenging due to partial observability, occlusions,\nand object symmetries, which inevitably lead to pose ambiguity and multiple\nhypotheses consistent with the same observation. While deterministic deep\nnetworks achieve impressive performance under well-constrained conditions, they\nare often overconfident and fail to capture the multi-modality of the\nunderlying pose distribution. To address these challenges, we propose a novel\nprobabilistic framework that leverages flow matching on the SE(3) manifold for\nestimating 6D object pose distributions. Unlike existing methods that regress a\nsingle deterministic output, our approach models the full pose distribution\nwith a sample-based estimate and enables reasoning about uncertainty in\nambiguous cases such as symmetric objects or severe occlusions. We achieve\nstate-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our\nsample-based pose estimates can be leveraged in downstream robotic manipulation\ntasks such as active perception for disambiguating uncertain viewpoints or\nguiding grasp synthesis in an uncertainty-aware manner.", "AI": {"tldr": "本文提出一个基于SE(3)流匹配的概率框架，用于估计6D物体姿态分布，解决了传统方法在遮挡和对称性下的姿态模糊和多模态问题。该方法通过样本建模完整姿态分布，实现了最先进的性能，并可应用于机器人操作任务。", "motivation": "物体姿态估计在机器人学和计算机视觉中是基础且具挑战性的问题，因为部分可观察性、遮挡和物体对称性会导致姿态模糊和多重假设。现有的确定性深度网络在这些情况下往往过于自信，无法捕捉潜在姿态分布的多模态特性。", "method": "本文提出一种新颖的概率框架，利用SE(3)流匹配来估计6D物体姿态分布。该方法通过基于样本的估计来建模完整的姿态分布，从而能够推理模糊情况（如对称物体或严重遮挡）下的不确定性。", "result": "该方法在Real275、YCB-V和LM-O数据集上取得了最先进（state-of-the-art）的结果。此外，研究展示了其基于样本的姿态估计如何应用于下游机器人操作任务，例如用于消除不确定视点歧义的主动感知，或以不确定性感知的方式指导抓取合成。", "conclusion": "该概率框架成功解决了6D物体姿态估计中的模糊性和多模态挑战，通过建模完整的姿态分布，实现了卓越的性能，并为不确定性感知的机器人操作任务提供了有效支持。"}}
{"id": "2511.01354", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01354", "abs": "https://arxiv.org/abs/2511.01354", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "comment": "emnlp 2025 industry track", "summary": "Recently, the demand for small and efficient reasoning models to support\nreal-world applications has driven the development of knowledge distillation\ntechniques that balance reasoning performance and inference speed. In this\npaper, we further extend the DistilQwen model family, initialized from the Qwen\nmodels, by introducing four model series specifically designed to meet\nindustrial requirements. The distilled model collection comprises: (1)\nslow-thinking models, optimized for reasoning tasks that require high accuracy;\n(2) two series of adaptive-thinking models, which dynamically adjust reasoning\nstrategies based on input tasks to maximize efficiency across diverse\nscenarios; and (3) distilled reward models, which enable further reinforcement\nlearning of reasoning models using distilled knowledge. Comprehensive\nevaluations across multiple benchmarks demonstrate both high inference\nefficiency and strong reasoning performance for these models, as well as the\npractical utility of distilled reward models. We further show that these models\nsupport industry practitioners by providing scalable training and inference\nfunctionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)\nplatform.", "AI": {"tldr": "本文通过引入四种新模型系列（慢思考、两种自适应思考和蒸馏奖励模型），扩展了DistilQwen模型家族，以满足工业应用对高效、高性能推理模型的需求。", "motivation": "现实世界应用对小型、高效推理模型的需求日益增长，这些模型需要在推理性能和速度之间取得平衡，从而推动了知识蒸馏技术的发展。", "method": "研究者从Qwen模型初始化，通过知识蒸馏技术扩展了DistilQwen模型家族，引入了四种针对工业需求设计的模型系列：1) 慢思考模型，优化高精度推理任务；2) 两种自适应思考模型，根据输入任务动态调整推理策略；3) 蒸馏奖励模型，用于强化学习。并在阿里巴巴云PAI平台上提供了可扩展的训练和推理功能。", "result": "这些模型在多个基准测试中展现了高推理效率和强大的推理性能，同时蒸馏奖励模型也显示出实用价值。此外，它们还支持在阿里巴巴云PAI平台上进行可扩展的训练和推理。", "conclusion": "扩展后的DistilQwen模型家族（包括慢思考、自适应思考和蒸馏奖励模型）为工业应用提供了高效、高性能的推理解决方案，并支持通过蒸馏知识进行强化学习，且具备在云平台上进行可扩展部署的能力。"}}
{"id": "2511.00681", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00681", "abs": "https://arxiv.org/abs/2511.00681", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control", "comment": null, "summary": "Magnetic Resonance Imaging suffers from substantial data heterogeneity and\nthe absence of standardized contrast labels across scanners, protocols, and\ninstitutions, which severely limits large-scale automated analysis. A unified\nrepresentation of MRI contrast would enable a wide range of downstream\nutilities, from automatic sequence recognition to harmonization and quality\ncontrol, without relying on manual annotations. To this end, we introduce\nMR-CLIP, a metadata-guided framework that learns MRI contrast representations\nby aligning volumetric images with their DICOM acquisition parameters. The\nresulting embeddings shows distinct clusters of MRI sequences and outperform\nsupervised 3D baselines under data scarcity in few-shot sequence\nclassification. Moreover, MR-CLIP enables unsupervised data quality control by\nidentifying corrupted or inconsistent metadata through image-metadata embedding\ndistances. By transforming routinely available acquisition metadata into a\nsupervisory signal, MR-CLIP provides a scalable foundation for label-efficient\nMRI analysis across diverse clinical datasets.", "AI": {"tldr": "MR-CLIP是一个元数据引导的框架，通过对齐MRI图像与DICOM采集参数来学习MRI对比度表示，从而实现序列识别、数据协调和质量控制。", "motivation": "MRI数据存在显著的异质性，且缺乏标准化的对比度标签，这严重限制了大规模自动化分析。需要一种统一的MRI对比度表示，以实现各种下游应用，而无需手动标注。", "method": "引入了MR-CLIP框架，该框架通过将体素图像与其DICOM采集参数对齐，学习MRI对比度表示。它利用常规可用的采集元数据作为监督信号。", "result": "学习到的嵌入显示出MRI序列的明显聚类。在数据稀缺的条件下，MR-CLIP在少样本序列分类中优于有监督的3D基线。此外，MR-CLIP通过图像-元数据嵌入距离，能够识别损坏或不一致的元数据，从而实现无监督的数据质量控制。", "conclusion": "MR-CLIP通过将元数据转化为监督信号，为跨多样化临床数据集的标签高效MRI分析提供了一个可扩展的基础，解决了MRI数据异质性和缺乏标准化标签的问题。"}}
{"id": "2511.01571", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01571", "abs": "https://arxiv.org/abs/2511.01571", "authors": ["Wenqi Liang", "Gan Sun", "Yao He", "Jiahua Dong", "Suyan Dai", "Ivan Laptev", "Salman Khan", "Yang Cong"], "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model", "comment": "17pages,7 figures, 5 tabels", "summary": "Vision-Language-Action models (VLAs) are emerging as powerful tools for\nlearning generalizable visuomotor control policies. However, current VLAs are\nmostly trained on large-scale image-text-action data and remain limited in two\nkey ways: (i) they struggle with pixel-level scene understanding, and (ii) they\nrely heavily on textual prompts, which reduces their flexibility in real-world\nsettings. To address these challenges, we introduce PixelVLA, the first VLA\nmodel designed to support both pixel-level reasoning and multimodal prompting\nwith text and visual inputs. Our approach is built on a new visuomotor\ninstruction tuning framework that integrates a multiscale pixel-aware encoder\nwith a visual prompting encoder. To train PixelVLA effectively, we further\npropose a two-stage automated annotation pipeline that generates Pixel-160K, a\nlarge-scale dataset with pixel-level annotations derived from existing robot\ndata. Experiments on three standard VLA benchmarks and two VLA model variants\nshow that PixelVLA improves manipulation success rates by 10.1%-17.8% over\nOpenVLA, while requiring only 1.5% of its pretraining cost. These results\ndemonstrate that PixelVLA can be integrated into existing VLAs to enable more\naccurate, efficient, and versatile robot control in complex environments. The\ndataset and code will be released as open source.", "AI": {"tldr": "PixelVLA是首个支持像素级推理和多模态（文本与视觉）提示的视觉-语言-动作模型，通过新的指令微调框架和自动标注数据集Pixel-160K，显著提升了机器人操作成功率，并降低了预训练成本。", "motivation": "当前的视觉-语言-动作模型（VLAs）在像素级场景理解方面表现不佳，并且过度依赖文本提示，这限制了它们在实际应用中的灵活性。", "method": "本文提出了PixelVLA，它基于一个新的视觉运动指令微调框架，该框架集成了多尺度像素感知编码器和视觉提示编码器。为了有效训练PixelVLA，研究者还提出了一个两阶段自动化标注流程，从现有机器人数据中生成了包含像素级标注的大规模数据集Pixel-160K。", "result": "在三个标准VLA基准测试和两种VLA模型变体上的实验表明，PixelVLA将操作成功率比OpenVLA提高了10.1%-17.8%，而预训练成本仅为其1.5%。", "conclusion": "PixelVLA能够集成到现有VLA中，从而在复杂环境中实现更准确、高效和多功能的机器人控制。数据集和代码将开源。"}}
{"id": "2511.01502", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01502", "abs": "https://arxiv.org/abs/2511.01502", "authors": ["Mengtan Zhang", "Zizhan Guo", "Hongbo Zhao", "Yi Feng", "Zuyi Xiong", "Yue Wang", "Shaoyi Du", "Hanli Wang", "Rui Fan"], "title": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning", "comment": "18 pages, 14 figures", "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.", "AI": {"tldr": "该研究提出了一种判别式处理运动分量的方法，通过利用刚性流的几何规律和相机对齐，显著提升了无监督深度和自我运动估计的鲁棒性和性能，尤其是在复杂条件下。", "motivation": "现有方法将自我运动视为辅助任务，混淆所有运动类型或排除与深度无关的旋转运动，限制了几何约束的应用，降低了在多样化条件下的可靠性和鲁棒性。", "method": "该研究引入了对运动分量的判别式处理，利用其各自刚性流的几何规律。通过对齐源相机和目标相机的光轴及成像平面，转换帧间光流并量化偏差，对每个自我运动分量施加几何约束。此外，将联合学习过程重新构建为同轴和共面形式，通过封闭形式的几何关系相互推导深度和每个平移分量，引入互补约束以提高深度鲁棒性。", "result": "所提出的DiMoDE框架在多个公开数据集以及一个新收集的多样化真实世界数据集上，尤其是在具有挑战性的条件下，实现了最先进的性能。", "conclusion": "通过对运动分量的判别式处理、利用刚性流的几何规律以及创新的相机对齐和联合学习重构，DiMoDE框架有效提升了无监督深度和自我运动估计的准确性和鲁棒性。"}}
{"id": "2511.00540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00540", "abs": "https://arxiv.org/abs/2511.00540", "authors": ["Wenbing Zhu", "Chengjie Wang", "Bin-Bin Gao", "Jiangning Zhang", "Guannan Jiang", "Jie Hu", "Zhenye Gan", "Lidong Wang", "Ziqing Zhou", "Linjie Cheng", "Yurui Pan", "Bo Peng", "Mingmin Chi", "Lizhuang Ma"], "title": "Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era", "comment": "13 pages, 4 figures and 5 tables", "summary": "Industrial Anomaly Detection (IAD) is critical for enhancing operational\nsafety, ensuring product quality, and optimizing manufacturing efficiency\nacross global industries. However, the IAD algorithms are severely constrained\nby the limitations of existing public benchmarks. Current datasets exhibit\nrestricted category diversity and insufficient scale, frequently resulting in\nmetric saturation and limited model transferability to real-world scenarios. To\naddress this gap, we introduce Real-IAD Variety, the largest and most diverse\nIAD benchmark, comprising 198,960 high-resolution images across 160 distinct\nobject categories. Its diversity is ensured through comprehensive coverage of\n28 industries, 24 material types, and 22 color variations. Our comprehensive\nexperimental analysis validates the benchmark's substantial challenge:\nstate-of-the-art multi-class unsupervised anomaly detection methods experience\nsignificant performance degradation when scaled from 30 to 160 categories.\nCrucially, we demonstrate that vision-language models exhibit remarkable\nrobustness to category scale-up, with minimal performance variation across\ndifferent category counts, significantly enhancing generalization capabilities\nin diverse industrial contexts. The unprecedented scale and complexity of\nReal-IAD Variety position it as an essential resource for training and\nevaluating next-generation foundation models for anomaly detection. By\nproviding this comprehensive benchmark with rigorous evaluation protocols\nacross multi-class unsupervised, multi-view, and zero-/few-shot settings, we\naim to accelerate research beyond domain-specific constraints, enabling the\ndevelopment of scalable, general-purpose anomaly detection systems. Real-IAD\nVariety will be made publicly available to facilitate innovation in this\ncritical field.", "AI": {"tldr": "该论文引入了Real-IAD Variety，一个最大且最多样化的工业异常检测（IAD）基准，旨在解决现有数据集的局限性，并推动可扩展、通用异常检测系统的发展。", "motivation": "现有工业异常检测（IAD）算法受到公共基准数据集的严重限制，这些数据集类别多样性不足、规模有限，导致模型性能饱和且难以迁移到真实世界场景。", "method": "研究者构建了一个名为Real-IAD Variety的新型基准数据集，包含198,960张高分辨率图像，涵盖160个不同物体类别，并确保了28个行业、24种材料类型和22种颜色变化的全面覆盖。他们还对最先进的多类别无监督异常检测方法和视觉-语言模型进行了实验分析。", "result": "实验结果表明，最先进的多类别无监督异常检测方法在类别从30扩展到160时性能显著下降。然而，视觉-语言模型对类别规模扩展表现出卓越的鲁棒性，性能变化极小，显著增强了在多样化工业环境中的泛化能力。", "conclusion": "Real-IAD Variety基准数据集凭借其前所未有的规模和复杂性，成为训练和评估下一代异常检测基础模型的关键资源。它将通过提供严格的评估协议，加速研究超越领域特定限制，促进可扩展、通用异常检测系统的开发。"}}
{"id": "2511.00542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00542", "abs": "https://arxiv.org/abs/2511.00542", "authors": ["Kailun Su", "Ziqi He", "Xi Wang", "Yang Zhou"], "title": "MIFO: Learning and Synthesizing Multi-Instance from One Image", "comment": "17 pages, 30 figures", "summary": "This paper proposes a method for precise learning and synthesizing\nmulti-instance semantics from a single image. The difficulty of this problem\nlies in the limited training data, and it becomes even more challenging when\nthe instances to be learned have similar semantics or appearance. To address\nthis, we propose a penalty-based attention optimization to disentangle similar\nsemantics during the learning stage. Then, in the synthesis, we introduce and\noptimize box control in attention layers to further mitigate semantic leakage\nwhile precisely controlling the output layout. Experimental results demonstrate\nthat our method achieves disentangled and high-quality semantic learning and\nsynthesis, strikingly balancing editability and instance consistency. Our\nmethod remains robust when dealing with semantically or visually similar\ninstances or rare-seen objects. The code is publicly available at\nhttps://github.com/Kareneveve/MIFO", "AI": {"tldr": "本文提出了一种从单张图像中精确学习和合成多实例语义的方法，通过惩罚机制注意力优化和注意力层中的边界框控制，解决了数据有限和实例相似性带来的挑战，实现了高质量的语义解耦和合成。", "motivation": "从单张图像中学习和合成多实例语义面临训练数据有限的困难，当实例具有相似语义或外观时，问题变得更具挑战性。", "method": "本文提出了一种基于惩罚的注意力优化方法，用于在学习阶段解耦相似语义。在合成阶段，引入并优化注意力层中的边界框控制，以进一步减少语义泄露并精确控制输出布局。", "result": "实验结果表明，该方法实现了高质量、解耦的语义学习和合成，显著平衡了可编辑性和实例一致性。该方法在处理语义或视觉上相似的实例或罕见对象时，仍然保持鲁棒性。", "conclusion": "本文提出了一种有效的方法，能够从单张图像中精确学习和合成多实例语义，解决了数据稀缺和实例相似性带来的挑战，并提供了高质量、可控的输出。"}}
{"id": "2511.01755", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01755", "abs": "https://arxiv.org/abs/2511.01755", "authors": ["Rong Li", "Yuhao Dong", "Tianshuai Hu", "Ao Liang", "Youquan Liu", "Dongyue Lu", "Liang Pan", "Lingdong Kong", "Junwei Liang", "Ziwei Liu"], "title": "3EED: Ground Everything Everywhere in 3D", "comment": "NeurIPS 2025 DB Track; 29 pages, 17 figures, 10 tables; Project Page\n  at https://project-3eed.github.io/", "summary": "Visual grounding in 3D is the key for embodied agents to localize\nlanguage-referred objects in open-world environments. However, existing\nbenchmarks are limited to indoor focus, single-platform constraints, and small\nscale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark\nfeaturing RGB and LiDAR data from vehicle, drone, and quadruped platforms. We\nprovide over 128,000 objects and 22,000 validated referring expressions across\ndiverse outdoor scenes -- 10x larger than existing datasets. We develop a\nscalable annotation pipeline combining vision-language model prompting with\nhuman verification to ensure high-quality spatial grounding. To support\ncross-platform learning, we propose platform-aware normalization and\ncross-modal alignment techniques, and establish benchmark protocols for\nin-domain and cross-platform evaluations. Our findings reveal significant\nperformance gaps, highlighting the challenges and opportunities of\ngeneralizable 3D grounding. The 3EED dataset and benchmark toolkit are released\nto advance future research in language-driven 3D embodied perception.", "AI": {"tldr": "该论文提出了3EED，一个用于多平台、多模态户外环境的3D视觉定位基准，解决了现有基准在规模和泛化性上的局限。", "motivation": "现有的3D视觉定位基准主要局限于室内环境、单一平台且规模较小，无法满足具身智能体在开放世界中定位语言所指对象的需求。", "method": "引入了3EED数据集，包含来自车辆、无人机和四足机器人平台的多模态（RGB和LiDAR）数据，拥有超过12.8万个对象和2.2万个参照表达。开发了结合视觉-语言模型提示和人工验证的可扩展标注流程，以确保高质量的空间定位。提出了平台感知归一化和跨模态对齐技术，并建立了域内和跨平台评估的基准协议。", "result": "研究发现存在显著的性能差距，凸显了可泛化3D定位的挑战和机遇。", "conclusion": "3EED数据集和基准工具的发布旨在推动语言驱动的3D具身感知领域的未来研究，提供了一个更具挑战性和真实世界场景的评估平台。"}}
{"id": "2511.01359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory.", "AI": {"tldr": "本文提出了一种名为MiniTruePrefixes的新型NLI模型，专门用于在自回归生成过程中检测文本前缀的事实不一致性。该模型在前缀级蕴含检测方面优于基线NLI模型，并能显著提高LLM输出（如抽象摘要）的事实一致性，同时提高效率。", "motivation": "现有的自然语言推理（NLI）模型虽然被用于提高大型语言模型（LLM）输出的事实性，但它们是为完整句子而非自回归生成中逐个演变的前缀设计的。这种不匹配促使研究人员寻求一种能在文本前缀上有效检测事实不一致性的方法。", "method": "研究人员将蕴含检测任务推广到任意文本前缀，并为此任务提供了合适的评估和训练数据集。他们训练了一个名为MiniTruePrefixes的新型专用模型，并将其集成到一个受控解码框架中，以验证其在提高生成忠实性方面的效用。", "result": "MiniTruePrefixes在前缀级蕴含检测方面比可比较的基线NLI模型高出5-14个F1点。将其集成到受控解码框架中，显著改善了抽象摘要的事实一致性。在MiniTruePrefixes的指导下，LLaMA-3.2-3B-Instruct模型在忠实性和运行时性能上达到了同系列8B模型的水平，但只使用了后者一半的内存。", "conclusion": "MiniTruePrefixes模型能够更有效地检测文本前缀中的事实不一致性，从而显著提高了LLM生成（特别是抽象摘要）的事实一致性。此外，它还能在保持性能的同时，大幅降低模型所需的内存，提升了效率。"}}
{"id": "2511.00560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00560", "abs": "https://arxiv.org/abs/2511.00560", "authors": ["Chun-Tin Wu", "Jun-Cheng Chen"], "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting", "comment": "10 pages, 7 figures", "summary": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel\nview synthesis, extending it to dynamic scenes still results in substantial\nmemory overhead from replicating Gaussians across frames. To address this\nchallenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines\nvoxel-based representations with neural Gaussian splatting for efficient\ndynamic scene modeling. Instead of generating separate Gaussian sets per\ntimestamp, our method employs a compact set of neural voxels with learned\ndeformation fields to model temporal dynamics. The design greatly reduces\nmemory consumption and accelerates training while preserving high image\nquality. We further introduce a novel view refinement stage that selectively\nimproves challenging viewpoints through targeted optimization, maintaining\nglobal efficiency while enhancing rendering quality for difficult viewing\nangles. Experiments demonstrate that our method outperforms state-of-the-art\napproaches with significant memory reduction and faster training, enabling\nreal-time rendering with superior visual fidelity.", "AI": {"tldr": "针对动态场景中3D Gaussian Splatting的内存开销问题，本文提出4D Neural Voxel Splatting (4D-NVS)，通过结合体素和神经高斯splatting，显著降低内存并加速训练，同时保持高质量实时渲染。", "motivation": "3D Gaussian Splatting (3D-GS) 在动态场景中，由于在不同帧间复制高斯点，导致巨大的内存开销。", "method": "本文提出4D Neural Voxel Splatting (4D-NVS) 方法。它结合了基于体素的表示和神经高斯splatting，通过使用一组紧凑的神经体素和学习到的形变场来建模时间动态，而非为每个时间戳生成独立的高斯集。此外，该方法引入了一个新颖的视图细化阶段，通过有针对性的优化来选择性地改善具有挑战性的视点。", "result": "实验表明，4D-NVS显著减少了内存消耗，加速了训练，同时保持了高质量的图像。它在内存减少和训练速度方面超越了现有最先进的方法，实现了具有卓越视觉保真度的实时渲染。", "conclusion": "4D-NVS通过结合体素表示和神经高斯splatting，有效解决了动态场景中3D-GS的内存和训练效率问题，实现了更优的视觉质量和实时渲染能力。"}}
{"id": "2511.00686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00686", "abs": "https://arxiv.org/abs/2511.00686", "authors": ["Alex Inch", "Passawis Chaiyapattanaporn", "Yuchen Zhu", "Yuan Lu", "Ting-Wen Ko", "Davide Paglieri"], "title": "Evolve to Inspire: Novelty Search for Diverse Image Generation", "comment": "14 pages, 10 figures, Accepted to Neurips 2025 GenProCC Workshop", "summary": "Text-to-image diffusion models, while proficient at generating high-fidelity\nim- ages, often suffer from limited output diversity, hindering their\napplication in exploratory and ideation tasks. Existing prompt optimization\ntechniques typically target aesthetic fitness or are ill-suited to the creative\nvisual domain. To address this shortcoming, we introduce WANDER, a novelty\nsearch-based approach to generating diverse sets of images from a single input\nprompt. WANDER operates directly on natural language prompts, employing a Large\nLanguage Model (LLM) for semantic evolution of diverse sets of images, and\nusing CLIP embeddings to quantify novelty. We additionally apply emitters to\nguide the search into distinct regions of the prompt space, and demonstrate\nthat they boost the diversity of the generated images. Empirical evaluations\nusing FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that\nWANDER significantly outperforms existing evolutionary prompt optimization\nbaselines in diversity metrics. Ablation studies confirm the efficacy of\nemitters.", "AI": {"tldr": "WANDER是一种基于新颖性搜索的方法，利用LLM对提示词进行语义演化，并结合CLIP量化新颖性，旨在从单个提示词生成多样化的图像集，显著提升了图像生成的多样性。", "motivation": "文本到图像扩散模型在生成高质量图像时，常面临输出多样性有限的问题，这限制了它们在探索性和创意任务中的应用。现有提示词优化技术主要关注美学适应性，不适合创意视觉领域。", "method": "WANDER直接在自然语言提示词上操作，使用大型语言模型（LLM）进行提示词的语义演化，并利用CLIP嵌入来量化生成图像的新颖性。此外，还引入了发射器（emitters）来引导搜索进入提示词空间的不同区域，以进一步提升多样性。", "result": "经验评估表明，WANDER在多样性指标上显著优于现有的演化式提示词优化基线方法。消融研究也证实了发射器（emitters）的有效性。", "conclusion": "WANDER通过结合LLM进行语义演化和基于新颖性搜索，有效解决了文本到图像模型输出多样性不足的问题，并显著提升了图像生成的多样性。发射器（emitters）的应用进一步增强了这一效果。"}}
{"id": "2511.00785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00785", "abs": "https://arxiv.org/abs/2511.00785", "authors": ["Juan Wang", "Yasutomo Kawanishi", "Tomo Miyazaki", "Zhijie Wang", "Shinichiro Omachi"], "title": "Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking", "comment": "Under review in Pattern Recognition", "summary": "3D instance segmentation is an important task for real-world applications. To\navoid costly manual annotations, existing methods have explored generating\npseudo labels by transferring 2D masks from foundation models to 3D. However,\nthis approach is often suboptimal since the video frames are processed\nindependently. This causes inconsistent segmentation granularity and\nconflicting 3D pseudo labels, which degrades the accuracy of final\nsegmentation. To address this, we introduce a Granularity-Consistent automatic\n2D Mask Tracking approach that maintains temporal correspondences across\nframes, eliminating conflicting pseudo labels. Combined with a three-stage\ncurriculum learning framework, our approach progressively trains from\nfragmented single-view data to unified multi-view annotations, ultimately\nglobally coherent full-scene supervision. This structured learning pipeline\nenables the model to progressively expose to pseudo-labels of increasing\nconsistency. Thus, we can robustly distill a consistent 3D representation from\ninitially fragmented and contradictory 2D priors. Experimental results\ndemonstrated that our method effectively generated consistent and accurate 3D\nsegmentations. Furthermore, the proposed method achieved state-of-the-art\nresults on standard benchmarks and open-vocabulary ability.", "AI": {"tldr": "本文提出了一种通过粒度一致的2D掩码跟踪和三阶段课程学习框架，解决3D实例分割中由独立处理2D伪标签导致的标签不一致问题，从而生成一致且准确的3D分割结果，并取得了最先进的性能。", "motivation": "现有的3D实例分割方法通过将2D基础模型的掩码转换为3D伪标签来避免昂贵的手动标注，但由于独立处理视频帧，导致分割粒度不一致和3D伪标签冲突，从而降低了最终分割的准确性。", "method": "本文引入了一种“粒度一致的自动2D掩码跟踪”方法，以维护帧间的时间对应关系并消除冲突的伪标签。此外，结合了一个三阶段的课程学习框架，该框架逐步从碎片化的单视图数据训练到统一的多视图标注，最终实现全局一致的全场景监督。", "result": "实验结果表明，所提出的方法能够有效地生成一致且准确的3D分割。此外，该方法在标准基准测试上取得了最先进的结果，并展现了开放词汇能力。", "conclusion": "通过粒度一致的2D掩码跟踪和结构化的课程学习流程，本文的方法能够从最初碎片化和矛盾的2D先验中稳健地提取出一致的3D表示，从而实现准确的3D实例分割，并达到最先进的性能和开放词汇能力。"}}
{"id": "2511.01365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01365", "abs": "https://arxiv.org/abs/2511.01365", "authors": ["İbrahim Ethem Deveci", "Duygu Ataman"], "title": "The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation", "comment": "Accepted to NeurIPS 2025 Workshop on LLM Evaluation\n  (https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/LLM_Evaluation)", "summary": "The rapid rise of Large Language Models (LLMs) and Large Reasoning Models\n(LRMs) has been accompanied by an equally rapid increase of benchmarks used to\nassess them. However, due to both improved model competence resulting from\nscaling and novel training advances as well as likely many of these datasets\nbeing included in pre or post training data, results become saturated, driving\na continuous need for new and more challenging replacements. In this paper, we\ndiscuss whether surpassing a benchmark truly demonstrates reasoning ability or\nare we simply tracking numbers divorced from the capabilities we claim to\nmeasure? We present an investigation focused on three model families, OpenAI,\nAnthropic, and Google, and how their reasoning capabilities across different\nbenchmarks evolve over the years. We also analyze performance trends over the\nyears across different reasoning tasks and discuss the current situation of\nbenchmarking and remaining challenges. By offering a comprehensive overview of\nbenchmarks and reasoning tasks, our work aims to serve as a first reference to\nground future research in reasoning evaluation and model development.", "AI": {"tldr": "本文探讨了大型语言模型（LLMs）和大型推理模型（LRMs）基准测试的饱和问题，质疑当前基准能否真实衡量推理能力，并通过分析不同模型家族在多年间推理能力的演变，旨在为未来的推理评估和模型开发提供参考。", "motivation": "随着LLMs和LRMs能力的提升以及数据泄露，现有基准测试迅速饱和，导致需要不断更新更具挑战性的基准。研究者质疑超越基准是否真正反映了推理能力，还是仅仅追踪了与实际能力脱节的数字。", "method": "研究调查了OpenAI、Anthropic和Google三个模型家族，分析了它们多年来在不同基准测试中推理能力的发展。同时，还分析了不同推理任务在多年间的性能趋势，并讨论了当前基准测试的现状和挑战。", "result": "本文通过对模型家族推理能力演变和性能趋势的调查分析，揭示了基准测试饱和的现状以及对当前评估方法有效性的质疑。其结果主要体现在对基准测试和推理任务的全面概述和讨论。", "conclusion": "该研究旨在提供一个关于基准测试和推理任务的全面概述，作为未来推理评估和模型开发研究的初步参考，以更好地理解和衡量模型的真实推理能力。"}}
{"id": "2511.00795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00795", "abs": "https://arxiv.org/abs/2511.00795", "authors": ["Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru", "Sai Teja Erukude"], "title": "FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data", "comment": "Published in IEEE", "summary": "Federated Learning (FL) allows multiple institutions to cooperatively train\nmachine learning models while retaining sensitive data at the source, which has\ngreat utility in privacy-sensitive environments. However, FL systems remain\nvulnerable to membership-inference attacks and data heterogeneity. This paper\npresents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using\nsynthetic oncologic CT scans with tumor annotations. It evaluates segmentation\nperformance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and\nFedAvg with DP-SGD. Results show a distinct trade-off between privacy and\nutility: FedAvg is high performance (Dice around 0.85) with more privacy\nleakage (attack AUC about 0.72), while DP-SGD provides a higher level of\nprivacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx\nand FedBN offer balanced performance under heterogeneous data, especially with\nnon-identical distributed client data. FedOnco-Bench serves as a standardized,\nopen-source platform for benchmarking and developing privacy-preserving FL\nmethods for medical image segmentation.", "AI": {"tldr": "本文提出了FedOnco-Bench，一个可复现的联邦学习基准，用于在肿瘤CT图像分割中评估隐私保护FL方法的性能和隐私泄露，并展示了隐私与效用之间的权衡。", "motivation": "联邦学习（FL）在保护隐私的环境中具有巨大潜力，但仍面临成员推断攻击和数据异质性的漏洞。", "method": "引入FedOnco-Bench，一个使用带有肿瘤注释的合成肿瘤CT扫描的基准平台。它评估了FedAvg、FedProx、FedBN和带有DP-SGD的FedAvg等FL方法的分割性能和隐私泄露。", "result": "结果显示了隐私和效用之间明显的权衡：FedAvg性能高（Dice约0.85）但隐私泄露多（攻击AUC约0.72）；DP-SGD提供了更高的隐私级别（AUC约0.25）但牺牲了准确性（Dice约0.79）。FedProx和FedBN在异构数据下，特别是非同分布的客户端数据下，提供了平衡的性能。", "conclusion": "FedOnco-Bench作为一个标准化、开源的平台，可用于医学图像分割中隐私保护FL方法的基准测试和开发，并揭示了隐私与模型效用之间的固有权衡。"}}
{"id": "2511.00573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00573", "abs": "https://arxiv.org/abs/2511.00573", "authors": ["Wei Feng", "Zongyuan Ge"], "title": "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective", "comment": "29 pages, 5 figures", "summary": "Generalized Category Discovery (GCD) aims to leverage labeled samples from\nknown categories to cluster unlabeled data that may include both known and\nunknown categories. While existing methods have achieved impressive results\nunder standard conditions, their performance often deteriorates in the presence\nof distribution shifts. In this paper, we explore a more realistic task:\nDomain-Shifted Generalized Category Discovery (DS\\_GCD), where the unlabeled\ndata includes not only unknown categories but also samples from unknown\ndomains. To tackle this challenge, we propose a\n\\textbf{\\underline{F}}requency-guided Gene\\textbf{\\underline{r}}alized\nCat\\textbf{\\underline{e}}gory Discov\\textbf{\\underline{e}}ry framework (FREE)\nthat enhances the model's ability to discover categories under distributional\nshift by leveraging frequency-domain information. Specifically, we first\npropose a frequency-based domain separation strategy that partitions samples\ninto known and unknown domains by measuring their amplitude differences. We\nthen propose two types of frequency-domain perturbation strategies: a\ncross-domain strategy, which adapts to new distributions by exchanging\namplitude components across domains, and an intra-domain strategy, which\nenhances robustness to intra-domain variations within the unknown domain.\nFurthermore, we extend the self-supervised contrastive objective and semantic\nclustering loss to better guide the training process. Finally, we introduce a\nclustering-difficulty-aware resampling technique to adaptively focus on\nharder-to-cluster categories, further enhancing model performance. Extensive\nexperiments demonstrate that our method effectively mitigates the impact of\ndistributional shifts across various benchmark datasets and achieves superior\nperformance in discovering both known and unknown categories.", "AI": {"tldr": "本文提出了一种名为DS_GCD（Domain-Shifted Generalized Category Discovery）的更现实任务，并提出了FREE（Frequency-guided Generalized Category Discovery）框架，通过利用频域信息来增强模型在存在域漂移时发现已知和未知类别的能力。", "motivation": "现有广义类别发现（GCD）方法在标准条件下表现良好，但在存在分布漂移时性能会下降。为了解决这一限制，研究旨在探索一个更现实的任务——DS_GCD，即未标记数据不仅包含未知类别，还包含来自未知域的样本。", "method": "本文提出了FREE框架，它利用频域信息来解决DS_GCD问题。具体方法包括：1) 提出一种基于频率的域分离策略，通过测量振幅差异将样本划分为已知和未知域。2) 提出两种频域扰动策略：跨域策略（通过交换域间的振幅分量来适应新分布）和域内策略（增强未知域内变化的鲁棒性）。3) 扩展自监督对比目标和语义聚类损失以指导训练。4) 引入聚类难度感知重采样技术，自适应地关注更难聚类的类别。", "result": "广泛的实验证明，所提出的方法能有效缓解分布漂移的影响，并在各种基准数据集上实现了卓越的性能，成功发现了已知和未知类别。", "conclusion": "FREE框架通过利用频域信息和一系列创新策略，有效解决了域漂移下的广义类别发现问题，显著提高了模型在更现实场景中发现已知和未知类别的能力。"}}
{"id": "2511.01360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01360", "abs": "https://arxiv.org/abs/2511.01360", "authors": ["Aadi Palnitkar", "Arjun Suresh", "Rishi Rajesh", "Puneet Puli"], "title": "Safer in Translation? Presupposition Robustness in Indic Languages", "comment": "This is a submission to LREC 2026 (Language Resources and Evaluation\n  Conference 2026). Corresponding author: aadipalnitkar96@gmail.com", "summary": "Increasingly, more and more people are turning to large language models\n(LLMs) for healthcare advice and consultation, making it important to gauge the\nefficacy and accuracy of the responses of LLMs to such queries. While there are\npre-existing medical benchmarks literature which seeks to accomplish this very\ntask, these benchmarks are almost universally in English, which has led to a\nnotable gap in existing literature pertaining to multilingual LLM evaluation.\nWithin this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,\nan Indic language benchmark built by translating a 500-item subset of\nCancer-Myth, sampled evenly across its original categories, into five\nunder-served but widely used languages from the subcontinent (500 per language;\n2,500 translated items total). Native-speaker translators followed a style\nguide for preserving implicit presuppositions in translation; items feature\nfalse presuppositions relating to cancer. We evaluate several popular LLMs\nunder this presupposition stress.", "AI": {"tldr": "针对大语言模型在医疗咨询中的应用日益增多但多语言评估存在空白的问题，本文提出了“Cancer-Myth-Indic”，一个包含2500个项目的印度语言基准测试，用于评估LLM在处理癌症相关错误预设方面的有效性和准确性。", "motivation": "随着人们日益依赖大语言模型获取医疗建议，评估其响应的有效性和准确性变得至关重要。然而，现有医学基准测试几乎全是英语，导致多语言（特别是印度语言）大语言模型评估方面存在显著空白。", "method": "本文通过将现有“Cancer-Myth”基准测试中的500个项目（均匀采样）翻译成五种印度次大陆语言（每种语言500项，共2500项），构建了“Cancer-Myth-Indic”印度语言基准测试。翻译由母语使用者遵循风格指南完成，以保留与癌症相关的隐含错误预设。随后，利用此基准测试评估了几个流行的大语言模型在这种预设压力下的表现。", "result": "本文成功构建了“Cancer-Myth-Indic”基准测试，共包含2500个翻译项目，用于评估大语言模型在印度语言中处理癌症相关错误预设的能力。该基准测试为在多语言环境下衡量流行大语言模型的医疗咨询准确性提供了工具。", "conclusion": "本研究通过引入“Cancer-Myth-Indic”基准测试，有效弥补了多语言大语言模型评估（特别是印度语言在医疗保健领域）的现有空白，有助于更准确地衡量大语言模型在处理癌症相关错误预设时的性能。"}}
{"id": "2511.00613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00613", "abs": "https://arxiv.org/abs/2511.00613", "authors": ["Yating Yu", "Congqi Cao", "Zhaoying Wang", "Weihua Meng", "Jie Li", "Yuxin Li", "Zihao Wei", "Zhongpei Shen", "Jiajun Zhang"], "title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World", "comment": null, "summary": "How far are deep models from real-world video anomaly understanding (VAU)?\nCurrent works typically emphasize on detecting unexpected occurrences deviated\nfrom normal patterns or comprehending anomalous events with interpretable\ndescriptions. However, they exhibit only a superficial comprehension of\nreal-world anomalies, with limited breadth in complex principles and subtle\ncontext that distinguish the anomalies from normalities, e.g., climbing cliffs\nwith safety gear vs. without it. To this end, we introduce CueBench, the first\nof its kind Benchmark, devoted to Context-aware video anomalies within a\nUnified Evaluation framework. We comprehensively establish an event-centric\nhierarchical taxonomy that anchors two core event types: 14 conditional and 18\nabsolute anomaly events, defined by their refined semantics from diverse\ncontexts across 174 scenes and 198 attributes. Based on this, we propose to\nunify and benchmark context-aware VAU with various challenging tasks across\nrecognition, temporal grounding, detection, and anticipation. This also serves\nas a rigorous and fair probing evaluation suite for generative-discriminative\nas well as generalized-specialized vision-language models (VLMs). To address\nthe challenges underlying CueBench, we further develop Cue-R1 based on R1-style\nreinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined\nrewards in a unified generative manner. Extensive results on CueBench reveal\nthat, existing VLMs are still far from satisfactory real-world anomaly\nunderstanding, while our Cue-R1 surpasses these state-of-the-art approaches by\nover 24% on average.", "AI": {"tldr": "本文引入了CueBench，首个专注于上下文感知视频异常理解（VAU）的基准，并提出了Cue-R1模型，显著提高了现有模型的性能。", "motivation": "现有视频异常理解方法对真实世界异常的理解停留在表面，缺乏对区分异常与正常行为的复杂原理和微妙上下文的深入理解。", "method": "本文提出了CueBench基准，包含事件中心的层次分类（14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性），并统一了识别、时间定位、检测和预测等多种上下文感知VAU任务的评估。为应对CueBench的挑战，本文进一步开发了Cue-R1模型，该模型基于R1风格的强化微调，采用可验证、任务对齐和层次细化的奖励机制，以统一的生成方式进行训练。", "result": "在CueBench上的广泛实验表明，现有视觉-语言模型（VLMs）在真实世界异常理解方面仍远未达到令人满意的水平，而本文提出的Cue-R1模型平均超越这些最先进方法24%以上。", "conclusion": "现有模型在真实世界视频异常理解方面仍有很大差距。CueBench为上下文感知VAU提供了一个严格的评估框架，而Cue-R1模型则为解决这一挑战提供了一个有效的新方法。"}}
{"id": "2511.00810", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00810", "abs": "https://arxiv.org/abs/2511.00810", "authors": ["Shijie Zhou", "Viet Dac Lai", "Hao Tan", "Jihyung Kil", "Wanrong Zhu", "Changyou Chen", "Ruiyi Zhang"], "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding", "comment": null, "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA", "AI": {"tldr": "本文提出了一种名为GUI-AIMA的高效、无坐标的GUI接地框架，它通过监督微调，将多模态大语言模型（MLLMs）的内在注意力与自适应的补丁级接地信号对齐，以解决现有方法直接生成精确坐标的挑战。", "motivation": "现有基于MLLMs的GUI接地方法通常将任务表述为基于文本的坐标生成，但直接从视觉输入生成精确坐标既困难又计算密集。研究者观察到通用MLLMs在其注意力机制中具有一定的原生接地能力，并认为更直观的方法是先选择相关视觉补丁，再确定精确点击位置。", "method": "GUI-AIMA是一个基于注意力的、无坐标的监督微调框架。它通过在简化的查询-视觉注意力矩阵上进行多头聚合，自适应地计算补丁级接地信号，并将这些信号与MLLMs的内在多模态注意力对齐。其无坐标特性还使其能够轻松集成一个即插即用的放大阶段。", "result": "GUI-AIMA-3B仅使用8.5万张屏幕截图进行训练，展现了卓越的数据效率，并验证了轻量级训练可以激活MLLMs的原生接地能力。它在3B模型中达到了最先进的性能，在ScreenSpot-Pro上平均准确率为58.6%，在OSWorld-G上为62.2%。", "conclusion": "GUI-AIMA证明了通过轻量级训练可以有效激活MLLMs的原生接地能力，并提供了一种高效且无坐标的GUI接地解决方案，在当前3B模型中取得了领先的性能。"}}
{"id": "2511.00643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00643", "abs": "https://arxiv.org/abs/2511.00643", "authors": ["Oluwatosin Alabi", "Meng Wei", "Charlie Budd", "Tom Vercauteren", "Miaojing Shi"], "title": "Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach", "comment": null, "summary": "Understanding surgical instrument-tissue interactions requires not only\nidentifying which instrument performs which action on which anatomical target,\nbut also grounding these interactions spatially within the surgical scene.\nExisting surgical action triplet recognition methods are limited to learning\nfrom frame-level classification, failing to reliably link actions to specific\ninstrument instances.Previous attempts at spatial grounding have primarily\nrelied on class activation maps, which lack the precision and robustness\nrequired for detailed instrument-tissue interaction analysis.To address this\ngap, we propose grounding surgical action triplets with instrument instance\nsegmentation, or triplet segmentation for short, a new unified task which\nproduces spatially grounded <instrument, verb, target> outputs.We start by\npresenting CholecTriplet-Seg, a large-scale dataset containing over 30,000\nannotated frames, linking instrument instance masks with action verb and\nanatomical target annotations, and establishing the first benchmark for\nstrongly supervised, instance-level triplet grounding and evaluation.To learn\ntriplet segmentation, we propose TargetFusionNet, a novel architecture that\nextends Mask2Former with a target-aware fusion mechanism to address the\nchallenge of accurate anatomical target prediction by fusing weak anatomy\npriors with instrument instance queries.Evaluated across recognition,\ndetection, and triplet segmentation metrics, TargetFusionNet consistently\nimproves performance over existing baselines, demonstrating that strong\ninstance supervision combined with weak target priors significantly enhances\nthe accuracy and robustness of surgical action understanding.Triplet\nsegmentation establishes a unified framework for spatially grounding surgical\naction triplets. The proposed benchmark and architecture pave the way for more\ninterpretable, surgical scene understanding.", "AI": {"tldr": "本文提出了一种名为“三元组分割”的新任务，通过将手术器械实例分割与动作三元组（器械、动词、目标）相结合，实现手术动作的空间定位理解。为此，我们创建了大规模数据集CholecTriplet-Seg并提出了TargetFusionNet模型，显著提高了手术动作识别的准确性和鲁棒性。", "motivation": "现有的手术动作三元组识别方法仅限于帧级别分类，无法可靠地将动作与特定的器械实例关联，并且空间定位方法（如类激活图）缺乏分析器械-组织交互所需的精度和鲁棒性。", "method": "1. 提出了“三元组分割”任务，即通过器械实例分割实现<器械、动词、目标>输出的空间定位。2. 构建了CholecTriplet-Seg数据集，包含30,000多帧带器械实例掩码和动作三元组标注的数据。3. 提出了TargetFusionNet架构，该架构扩展了Mask2Former，并引入了目标感知融合机制，通过融合弱解剖先验与器械实例查询来提高解剖目标预测的准确性。", "result": "TargetFusionNet在识别、检测和三元组分割等指标上均优于现有基线模型。结果表明，结合强实例监督和弱目标先验显著提高了手术动作理解的准确性和鲁棒性。", "conclusion": "三元组分割为手术动作三元组的空间定位提供了一个统一的框架。所提出的基准数据集和架构为更具可解释性的手术场景理解铺平了道路。"}}
{"id": "2511.01386", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01386", "abs": "https://arxiv.org/abs/2511.01386", "authors": ["Muhammed Yusuf Kartal", "Suha Kagan Kose", "Korhan Sevinç", "Burak Aktas"], "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "comment": "45 pages", "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting\nchoices across retrieval, ranking, augmentation, prompting, and generation, so\noptimizing modules in isolation is brittle. We introduce RAGSmith, a modular\nframework that treats RAG design as an end-to-end architecture search over nine\ntechnique families and 46{,}080 feasible pipeline configurations. A genetic\nsearch optimizes a scalar objective that jointly aggregates retrieval metrics\n(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic\nsimilarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,\nFinance, Medicine, Defense Industry, Computer Science), each with 100 questions\nspanning factual, interpretation, and long-answer types. RAGSmith finds\nconfigurations that consistently outperform naive RAG baseline by +3.8\\% on\naverage (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in\nretrieval and +7.5\\% in generation. The search typically explores $\\approx\n0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone --\nvector retrieval plus post-generation reflection/revision -- augmented by\ndomain-dependent choices in expansion, reranking, augmentation, and prompt\nreordering; passage compression is never selected. Improvement magnitude\ncorrelates with question type, with larger gains on factual/long-answer mixes\nthan interpretation-heavy sets. These results provide practical, domain-aware\nguidance for assembling effective RAG systems and demonstrate the utility of\nevolutionary search for full-pipeline optimization.", "AI": {"tldr": "RAGSmith是一个模块化框架，通过遗传搜索对RAG管道进行端到端架构优化，显著优于基线RAG系统，并为构建高效RAG系统提供了领域感知指导。", "motivation": "RAG系统的质量受检索、排序、增强、提示和生成等多个相互作用的选择影响，孤立地优化模块效果不佳且脆弱。", "method": "引入RAGSmith框架，将RAG设计视为涵盖9个技术家族和46,080种管道配置的端到端架构搜索问题。使用遗传搜索来优化一个标量目标，该目标联合聚合检索指标（recall@k, mAP, nDCG, MRR）和生成指标（LLM-Judge和语义相似度）。在六个维基百科衍生领域（数学、法律、金融、医学、国防工业、计算机科学）进行评估，每个领域包含100个问题，涵盖事实型、解释型和长答案型。", "result": "RAGSmith找到的配置平均比朴素RAG基线高出+3.8%（跨领域范围+1.2%至+6.9%），检索方面提升高达+12.5%，生成方面提升高达+7.5%。搜索通常只探索了约0.2%的空间（约100个候选），并发现了一个强大的核心——向量检索加上生成后的反思/修订——辅以扩展、重排、增强和提示重排序中的领域依赖选择；段落压缩从未被选中。改进幅度与问题类型相关，在事实型/长答案型混合问题上比重解释型问题集有更大的提升。", "conclusion": "这些结果为组装有效的RAG系统提供了实用、领域感知的指导，并证明了进化搜索在全管道优化中的实用性。"}}
{"id": "2511.00831", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00831", "abs": "https://arxiv.org/abs/2511.00831", "authors": ["Xin Liu", "Aoyang Zhou", "Aoyang Zhou"], "title": "Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack", "comment": "Accepted by NAACL2025 findings", "summary": "Visual-Language Pre-training (VLP) models have achieved significant\nperformance across various downstream tasks. However, they remain vulnerable to\nadversarial examples. While prior efforts focus on improving the adversarial\ntransferability of multimodal adversarial examples through cross-modal\ninteractions, these approaches suffer from overfitting issues, due to a lack of\ninput diversity by relying excessively on information from adversarial examples\nin one modality when crafting attacks in another. To address this issue, we\ndraw inspiration from strategies in some adversarial training methods and\npropose a novel attack called Local Shuffle and Sample-based Attack (LSSA).\nLSSA randomly shuffles one of the local image blocks, thus expanding the\noriginal image-text pairs, generating adversarial images, and sampling around\nthem. Then, it utilizes both the original and sampled images to generate the\nadversarial texts. Extensive experiments on multiple models and datasets\ndemonstrate that LSSA significantly enhances the transferability of multimodal\nadversarial examples across diverse VLP models and downstream tasks. Moreover,\nLSSA outperforms other advanced attacks on Large Vision-Language Models.", "AI": {"tldr": "本文提出了一种名为LSSA的新型多模态对抗攻击方法，通过局部图像块打乱和采样来增加输入多样性，有效解决了现有方法过拟合问题，显著提升了对抗样本在VLP模型间的可迁移性。", "motivation": "视觉-语言预训练（VLP）模型易受对抗样本攻击。现有跨模态攻击方法通过跨模态交互提高对抗可迁移性，但因过度依赖单一模态的对抗信息而缺乏输入多样性，导致过拟合问题。", "method": "本文提出局部打乱和采样攻击（LSSA）。LSSA随机打乱局部图像块以扩展原始图像-文本对，生成对抗图像并在其周围采样。随后，利用原始和采样图像共同生成对抗文本。", "result": "LSSA显著增强了多模态对抗样本在不同VLP模型和下游任务之间的可迁移性。此外，LSSA在大型视觉-语言模型上的表现优于其他先进攻击方法。", "conclusion": "LSSA通过引入输入多样性，有效解决了现有跨模态对抗攻击的过拟合问题，显著提升了对抗样本在VLP模型中的可迁移性，证明了其作为一种新型高效攻击方法的有效性。"}}
{"id": "2511.01380", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01380", "abs": "https://arxiv.org/abs/2511.01380", "authors": ["Wessel Poelman", "Thomas Bauwens", "Miryam de Lhoneux"], "title": "Confounding Factors in Relating Model Performance to Morphology", "comment": "EMNLP 2025: Main Conference", "summary": "The extent to which individual language characteristics influence\ntokenization and language modeling is an open question. Differences in\nmorphological systems have been suggested as both unimportant and crucial to\nconsider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter\nalia). We argue this conflicting evidence is due to confounding factors in\nexperimental setups, making it hard to compare results and draw conclusions. We\nidentify confounding factors in analyses trying to answer the question of\nwhether, and how, morphology relates to language modeling. Next, we re-assess\nthree hypotheses by Arnett & Bergen (2025) for why modeling agglutinative\nlanguages results in higher perplexities than fusional languages: they look at\nmorphological alignment of tokenization, tokenization efficiency, and dataset\nsize. We show that each conclusion includes confounding factors. Finally, we\nintroduce token bigram metrics as an intrinsic way to predict the difficulty of\ncausal language modeling, and find that they are gradient proxies for\nmorphological complexity that do not require expert annotation. Ultimately, we\noutline necessities to reliably answer whether, and how, morphology relates to\nlanguage modeling.", "AI": {"tldr": "本文探讨个体语言特征（特别是形态学）如何影响分词和语言建模，指出现有研究中存在混杂因素，并重新评估了相关假设，最终提出了一种无需专家标注的内在度量标准来预测语言建模的难度。", "motivation": "现有研究对个体语言特征（尤其是形态系统）在分词和语言建模中的作用存在争议，且结论相互矛盾。作者认为这源于实验设置中的混杂因素，导致结果难以比较和得出可靠结论。", "method": "本文首先识别并分析了在探讨形态学与语言建模关系时存在的混杂因素。其次，重新评估了Arnett & Bergen (2025)提出的关于黏着语比屈折语困惑度更高的三个假设（形态对齐、分词效率、数据集大小），并指出其结论中包含混杂因素。最后，引入了token bigram指标作为预测因果语言建模难度的内在方法。", "result": "研究发现，现有研究中关于形态学影响的矛盾证据是由于实验设置中的混杂因素造成的。Arnett & Bergen (2025) 的每个结论都包含混杂因素。此外，token bigram指标可以作为形态复杂度的梯度代理，且无需专家标注，能够预测因果语言建模的难度。", "conclusion": "本文提出了可靠回答形态学如何影响语言建模这一问题的必要条件，为未来研究指明了方向。"}}
{"id": "2511.01323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01323", "abs": "https://arxiv.org/abs/2511.01323", "authors": ["Jiabao Ji", "Min Li", "Priyanshu Kumar", "Shiyu Chang", "Saloni Potdar"], "title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "comment": "25 pages", "summary": "Large language models (LLMs) with integrated search tools show strong promise\nin open-domain question answering (QA), yet they often struggle to produce\ncomplete answer set to complex questions such as Which actor from the film Heat\nwon at least one Academy Award?, which requires (1) distinguishing between\nmultiple films sharing the same title and (2) reasoning across a large set of\nactors to gather and integrate evidence. Existing QA benchmarks rarely evaluate\nboth challenges jointly. To address this, we introduce DeepAmbigQAGen, an\nautomatic data generation pipeline that constructs QA tasks grounded in text\ncorpora and linked knowledge graph, generating natural and verifiable questions\nthat systematically embed name ambiguity and multi-step reasoning. Based on\nthis, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop\nreasoning and half of them explicit name ambiguity resolving. Experiments\nreveal that, even state-of-the-art GPT-5 show incomplete answers, achieving\nonly 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous\nquestions. These findings highlight the need for more robust QA systems aimed\nat information gathering and answer completeness.", "AI": {"tldr": "该研究引入了DeepAmbigQAGen自动数据生成管道和DeepAmbigQA数据集，旨在评估大型语言模型在处理名称歧义和多步推理复杂问答方面的能力。实验表明，即使是先进的LLM也难以提供完整的答案，凸显了对更强大问答系统的需求。", "motivation": "尽管集成搜索工具的大型语言模型（LLM）在开放域问答中表现出巨大潜力，但它们在处理需要区分名称歧义和跨大量证据进行多步推理的复杂问题时仍面临挑战。现有问答基准很少能同时评估这两个难题。", "method": "研究者开发了DeepAmbigQAGen，一个自动数据生成管道，用于构建基于文本语料库和知识图谱的问答任务，系统地嵌入名称歧义和多步推理。基于此，构建了DeepAmbigQA数据集，包含3,600个需要多跳推理的问题，其中一半明确涉及名称歧义消除。然后，使用最先进的LLM（如GPT-5）对该数据集进行了实验。", "result": "实验结果显示，即使是最先进的GPT-5模型也提供了不完整的答案，在歧义问题上的精确匹配率仅为0.13，在非歧义问题上为0.21。", "conclusion": "这些发现强调了需要开发更强大的问答系统，以改进信息收集和答案的完整性，特别是在处理名称歧义和多步推理的复杂问答场景中。"}}
{"id": "2511.00833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00833", "abs": "https://arxiv.org/abs/2511.00833", "authors": ["Yifan Pu", "Jixuan Ying", "Qixiu Li", "Tianzhu Ye", "Dongchen Han", "Xiaochen Wang", "Ziyi Wang", "Xinyu Shao", "Gao Huang", "Xiu Li"], "title": "Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials", "comment": "NeurIPS 2025", "summary": "Vision Transformers (ViTs) have become a universal backbone for both image\nrecognition and image generation. Yet their Multi-Head Self-Attention (MHSA)\nlayer still performs a quadratic query-key interaction for every token pair,\nspending the bulk of computation on visually weak or redundant correlations. We\nintroduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that\ninjects an explicit notion of discrimination while reducing the theoretical\ncomplexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's\ndense query field into a handful of spatially pooled visual-contrast tokens,\nthen splits them into a learnable positive and negative stream whose\ndifferential interaction highlights what truly separates one region from\nanother. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,\nrequires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA\nlifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and\nimproves three strong hierarchical ViTs by up to 3.1%, while in\nclass-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points\nacross both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm\nthat (i) spatial pooling supplies low-variance global cues, (ii) dual\npositional embeddings are indispensable for contrastive reasoning, and (iii)\ncombining the two in both stages yields the strongest synergy. VCA therefore\noffers a simple path towards faster and sharper Vision Transformers. The source\ncode is available at https://github.com/LeapLabTHU/LinearDiff.", "AI": {"tldr": "本文提出了一种名为视觉对比注意力（VCA）的新型注意力机制，作为Vision Transformers（ViTs）中多头自注意力（MHSA）的替代品。VCA通过引入显式的判别概念并利用空间池化和正负流交互来聚焦于视觉对比，从而将理论复杂度从O(N²C)降低到O(NnC)，同时显著提升了图像识别和生成任务的性能。", "motivation": "Vision Transformers（ViTs）的MHSA层在每个token对之间执行二次查询-键交互，消耗了大量计算资源在视觉上较弱或冗余的关联上，导致计算效率低下。", "method": "VCA首先将每个头的密集查询场提炼成少量空间池化的视觉对比tokens，然后将它们分成可学习的正向和负向流。通过这两个流的差异化交互，VCA突出真正区分不同区域的信息，从而降低了理论复杂度。该模块作为MHSA的即插即用替代品，参数量小，无需额外FLOPs，且与架构无关。", "result": "实验结果表明，VCA将DeiT-Tiny在ImageNet-1K上的top-1准确率从72.2%提升到75.6%（+3.4%），并使三种强大的分层ViT模型性能提升高达3.1%。在条件图像生成任务中，VCA使扩散模型（DiT）和流模型（SiT）的FID-50K分别降低了2.1到5.2点。广泛的消融实验证实了空间池化提供低方差全局线索、双重位置嵌入对对比推理不可或缺以及两者结合能产生最强协同效应。", "conclusion": "VCA为Vision Transformers提供了一条简单有效的途径，使其变得更快、更清晰。通过聚焦于视觉对比和降低计算复杂度，VCA在图像识别和生成任务中都取得了显著的性能提升。"}}
{"id": "2511.01409", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01409", "abs": "https://arxiv.org/abs/2511.01409", "authors": ["Heng Zhou", "Ao Yu", "Yuchen Fan", "Jianing Shi", "Li Kang", "Hejia Geng", "Yongting Zhang", "Yutao Fan", "Yuhao Wu", "Tiancheng He", "Yiran Qin", "Lei Bai", "Zhenfei Yin"], "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "comment": null, "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge.", "AI": {"tldr": "本文提出了LiveSearchBench，一个自动化流水线，用于从最新的知识更新中构建依赖检索的基准测试，以评估大型语言模型（LLMs）在动态知识环境下的问答能力。", "motivation": "现有LLM问答评估基准通常是静态的，奖励记忆而非检索能力，且未能捕捉世界知识的动态变化，导致模型对新知识的理解和应用能力被低估。", "method": "LiveSearchBench通过计算Wikidata连续快照之间的差异（deltas），筛选高质量的三元组，并合成三种推理难度级别的自然语言问题。每个问题都通过SPARQL验证，确保有唯一且可验证的答案。整个流水线是全自动、可扩展的，并最大限度地减少了人工干预，从而能够持续生成基于时间变化的基准。", "result": "实验结果显示，当模型面对预训练之后出现的事实时，其性能显著下降，尤其在多跳查询上差距最为明显。尽管检索增强方法和更大、经过指令微调的模型能带来部分性能提升，但仍未能完全弥补这种新近知识带来的性能差距。", "conclusion": "LiveSearchBench将LLM的评估重心从静态记忆转向需要最新检索和推理能力的任务，为在知识不断演变的环境下对LLM进行系统、长期的评估奠定了基础。"}}
{"id": "2511.00682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00682", "abs": "https://arxiv.org/abs/2511.00682", "authors": ["Hailing Wang", "jianglin Lu", "Yitian Zhang", "Yun Fu"], "title": "Outlier-Aware Post-Training Quantization for Image Super-Resolution", "comment": null, "summary": "Quantization techniques, including quantization-aware training (QAT) and\npost-training quantization (PTQ), have become essential for inference\nacceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has\ngarnered significant attention as it eliminates the need for ground truth and\nmodel retraining. However, existing PTQ methods for SR often fail to achieve\nsatisfactory performance as they overlook the impact of outliers in activation.\nOur empirical analysis reveals that these prevalent activation outliers are\nstrongly correlated with image color information, and directly removing them\nleads to significant performance degradation. Motivated by this, we propose a\ndual-region quantization strategy that partitions activations into an outlier\nregion and a dense region, applying uniform quantization to each region\nindependently to better balance bit-width allocation. Furthermore, we observe\nthat different network layers exhibit varying sensitivities to quantization,\nleading to different levels of performance degradation. To address this, we\nintroduce sensitivity-aware finetuning that encourages the model to focus more\non highly sensitive layers, further enhancing quantization performance.\nExtensive experiments demonstrate that our method outperforms existing PTQ\napproaches across various SR networks and datasets, while achieving performance\ncomparable to QAT methods in most scenarios with at least a 75 speedup.", "AI": {"tldr": "本文提出了一种针对图像超分辨率（SR）网络的后训练量化（PTQ）方法，通过双区域量化策略处理激活中的异常值，并引入敏感度感知微调来关注敏感层，从而在实现显著加速的同时，性能可与量化感知训练（QAT）方法媲美。", "motivation": "现有的SR网络PTQ方法在处理激活中的异常值时表现不佳，这些异常值与图像颜色信息强相关，且直接移除会导致性能显著下降。此外，不同网络层对量化敏感度不同，导致性能下降程度不一。", "method": "1. 提出了双区域量化策略：将激活值划分为异常值区域和密集区域，对每个区域独立进行均匀量化，以更好地平衡位宽分配。2. 引入了敏感度感知微调：促使模型更多地关注高度敏感的层，进一步提升量化性能。", "result": "实验结果表明，该方法在各种SR网络和数据集上均优于现有PTQ方法，并且在大多数情况下，性能与QAT方法相当，同时实现了至少75倍的加速。", "conclusion": "所提出的PTQ方法通过有效处理激活异常值和层敏感性问题，显著提升了SR网络的量化性能和推理速度，达到了与QAT方法相当的水平。"}}
{"id": "2511.01454", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.01454", "abs": "https://arxiv.org/abs/2511.01454", "authors": ["Sergio Torres Aguilar"], "title": "\"Don't Teach Minerva\": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG", "comment": null, "summary": "Translating a morphology-rich, low-resource language like Latin poses\nsignificant challenges. This paper introduces a reproducible draft-based\nrefinement pipeline that elevates open-source Large Language Models (LLMs) to a\nperformance level statistically comparable to top-tier proprietary systems. Our\nmethod first uses a fine-tuned NLLB-1.3B model to generate a high-quality,\nstructurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes\nthis draft, a process that can be further enhanced by augmenting the context\nwith retrieved out-context examples (RAG). We demonstrate the robustness of\nthis approach on two distinct benchmarks: a standard in-domain test set\n(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of\n12th-century Latin letters (2025). Our central finding is that this open-source\nRAG system achieves performance statistically comparable to the GPT-5 baseline,\nwithout any task-specific LLM fine-tuning. We release the pipeline, the\nChartres OOD set, and evaluation scripts and models to facilitate replicability\nand further research.", "AI": {"tldr": "本文提出了一种基于草稿细化的流水线，利用开源大型语言模型（LLMs）将形态丰富的低资源语言（如拉丁语）的翻译性能提升至与顶级专有系统（如GPT-5）相当的水平，且无需针对任务进行LLM微调。", "motivation": "翻译像拉丁语这种形态丰富、资源稀缺的语言面临巨大挑战。研究旨在提升开源LLMs在此类任务上的表现，使其能与顶级专有系统匹敌。", "method": "该方法首先使用一个经过微调的NLLB-1.3B模型生成高质量、结构忠实的草稿。随后，一个零样本LLM（Llama-3.3或Qwen3）对草稿进行润色，这一过程可通过检索式增强生成（RAG）进一步提升。研究在两个基准上进行了验证：一个标准的域内测试集和一个具有挑战性的域外（12世纪拉丁语信件）测试集。", "result": "研究发现，该开源RAG系统在无需任何特定任务LLM微调的情况下，其性能与GPT-5基线系统在统计学上相当。", "conclusion": "该研究成功构建了一个可复现的草稿细化流水线，显著提升了开源LLMs在拉丁语翻译方面的性能，使其达到与顶级专有系统相当的水平，并发布了相关工具和数据集以促进后续研究。"}}
{"id": "2511.00836", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00836", "abs": "https://arxiv.org/abs/2511.00836", "authors": ["Xin Liu", "Yichen Yang", "Kun He", "John E. Hopcroft"], "title": "Parameter Interpolation Adversarial Training for Robust Image Classification", "comment": "Accepted by TIFS 2025", "summary": "Though deep neural networks exhibit superior performance on various tasks,\nthey are still plagued by adversarial examples. Adversarial training has been\ndemonstrated to be the most effective method to defend against adversarial\nattacks. However, existing adversarial training methods show that the model\nrobustness has apparent oscillations and overfitting issues in the training\nprocess, degrading the defense efficacy. To address these issues, we propose a\nnovel framework called Parameter Interpolation Adversarial Training (PIAT).\nPIAT tunes the model parameters between each epoch by interpolating the\nparameters of the previous and current epochs. It makes the decision boundary\nof model change more moderate and alleviates the overfitting issue, helping the\nmodel converge better and achieving higher model robustness. In addition, we\nsuggest using the Normalized Mean Square Error (NMSE) to further improve the\nrobustness by aligning the relative magnitude of logits between clean and\nadversarial examples rather than the absolute magnitude. Extensive experiments\nconducted on several benchmark datasets demonstrate that our framework could\nprominently improve the robustness of both Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs).", "AI": {"tldr": "深度神经网络易受对抗性攻击，对抗训练虽有效但存在鲁棒性震荡和过拟合问题。本文提出参数插值对抗训练（PIAT）框架和归一化均方误差（NMSE）损失，以缓解这些问题并显著提升模型（包括CNN和ViT）的鲁棒性。", "motivation": "深度神经网络易受对抗性样本攻击。对抗训练是有效的防御方法，但其在训练过程中存在模型鲁棒性明显震荡和过拟合问题，从而降低了防御效果。", "method": "本文提出参数插值对抗训练（PIAT）框架，通过插值前一周期和当前周期的模型参数来调整模型参数，使决策边界变化更平缓并缓解过拟合。此外，建议使用归一化均方误差（NMSE）来对齐干净样本和对抗样本的logits相对大小，以进一步提高鲁棒性。", "result": "在多个基准数据集上进行的广泛实验表明，所提出的框架能够显著提高卷积神经网络（CNNs）和视觉Transformer（ViTs）的鲁棒性。", "conclusion": "PIAT和NMSE的结合有效地解决了现有对抗训练方法中鲁棒性震荡和过拟合的问题，并通过更温和的决策边界变化和logits对齐，显著提升了深度学习模型的对抗鲁棒性。"}}
{"id": "2511.01482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01482", "abs": "https://arxiv.org/abs/2511.01482", "authors": ["Neha Sharma", "Navneet Agarwal", "Kairit Sirts"], "title": "Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation", "comment": null, "summary": "Text-based automated Cognitive Distortion detection is a challenging task due\nto its subjective nature, with low agreement scores observed even among expert\nhuman annotators, leading to unreliable annotations. We explore the use of\nLarge Language Models (LLMs) as consistent and reliable annotators, and propose\nthat multiple independent LLM runs can reveal stable labeling patterns despite\nthe inherent subjectivity of the task. Furthermore, to fairly compare models\ntrained on datasets with different characteristics, we introduce a\ndataset-agnostic evaluation framework using Cohen's kappa as an effect size\nmeasure. This methodology allows for fair cross-dataset and cross-study\ncomparisons where traditional metrics like F1 score fall short. Our results\nshow that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),\nresulting in improved test set performance for models trained on these\nannotations compared to those trained on human-labeled data. Our findings\nsuggest that LLMs can offer a scalable and internally consistent alternative\nfor generating training data that supports strong downstream performance in\nsubjective NLP tasks.", "AI": {"tldr": "该研究提出使用大型语言模型（LLMs）作为认知扭曲检测等主观任务的可靠且一致的标注者，并引入了一种数据集无关的评估框架，结果显示LLM生成的标注能显著提高下游模型的性能。", "motivation": "文本自动化认知扭曲检测因其主观性而面临挑战，即使是专家人类标注者也难以达成高一致性，导致标注不可靠。", "method": "研究探索了使用多个独立的LLM运行来揭示稳定的标注模式，并引入了一种基于Cohen's kappa的数据集无关评估框架，以公平比较不同数据集和研究的模型。具体使用GPT-4进行标注，并将其与人类标注数据进行比较。", "result": "GPT-4能够生成高度一致的标注（Fleiss's Kappa = 0.78）。与使用人类标注数据训练的模型相比，使用这些LLM标注训练的模型在测试集上表现出更高的性能。", "conclusion": "大型语言模型可以为生成主观NLP任务的训练数据提供一种可扩展且内部一致的替代方案，从而支持强大的下游性能。"}}
{"id": "2511.01470", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01470", "abs": "https://arxiv.org/abs/2511.01470", "authors": ["Lujie Niu", "Lei Shen", "Yi Jiang", "Caixia Yuan", "Xiaojie Wang", "Wenbo Su", "Bo zheng"], "title": "BARD: budget-aware reasoning distillation", "comment": null, "summary": "While long Chain-of-Thought (CoT) distillation effectively transfers\nreasoning capability to smaller language models, the reasoning process often\nremains redundant and computational budget uncontrollable, leading to\ninefficient resource usage. To address this limitation, we propose\n\\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that\nsimultaneously distills reasoning capability and enables fine-grained control\nover the reasoning length. BARD uses the thinking budget as a user-specified\ncontrol signal, allowing the model to dynamically balance reasoning performance\nand computational efficiency. To achieve this concept, BARD introduces a\ntwo-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on\nteacher-generated long CoT data compressed to various budget levels,\nbootstrapping the model's understanding of budget constraints. The second phase\nleverages Reinforcement Learning (RL) from a reward signal in consideration of\nreasoning performance and budget fidelity simultaneously. Incorporating the\ntwo-phase regimen is crucial to avoiding policy degradation and ensuring that\nboth objectives are optimized jointly. Extensive experiments demonstrate that\nour method empowers an 8B student model to achieve strong performance on\nchallenging reasoning benchmarks (\\textit{AIME24, AIME25, GPQA}) while\nproviding precise and adaptive control over its reasoning length across a wide\nrange of budgets.", "AI": {"tldr": "本文提出了预算感知推理蒸馏（BARD）框架，旨在将长链式思考（CoT）的推理能力蒸馏到小型语言模型中，同时实现对推理长度和计算预算的精细控制，以提高资源效率。", "motivation": "长链式思考（CoT）蒸馏虽然能有效将推理能力转移到小型模型，但推理过程往往冗余且计算预算不可控，导致资源使用效率低下。", "method": "BARD框架采用两阶段训练方案：第一阶段是监督微调（SFT），使用教师模型生成的、压缩到不同预算水平的长CoT数据进行训练，以建立模型对预算约束的理解；第二阶段是强化学习（RL），利用同时考虑推理性能和预算符合度的奖励信号进行优化，以避免策略退化并联合优化两个目标。", "result": "实验结果表明，该方法使一个8B的学生模型在具有挑战性的推理基准（AIME24、AIME25、GPQA）上取得了优异性能，同时在各种预算范围内提供了对其推理长度的精确和自适应控制。", "conclusion": "BARD框架成功地将推理能力蒸馏到小型模型，并提供了对推理长度的细粒度控制，有效平衡了推理性能和计算效率，解决了CoT蒸馏中冗余和预算不可控的问题。"}}
{"id": "2511.00653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00653", "abs": "https://arxiv.org/abs/2511.00653", "authors": ["Lassi Ruoppa", "Tarmo Hietala", "Verneri Seppänen", "Josef Taher", "Teemu Hakala", "Xiaowei Yu", "Antero Kukko", "Harri Kaartinen", "Juha Hyyppä"], "title": "Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset", "comment": "39 pages, 9 figures", "summary": "Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for\napplications such as forest inventory, carbon monitoring and biodiversity\nassessment. Traditionally, ITS has been achieved with unsupervised\ngeometry-based algorithms, while more recent advances have shifted toward\nsupervised deep learning (DL). In the past, progress in method development was\nhindered by the lack of large-scale benchmark datasets, and the availability of\nnovel data formats, particularly multispectral (MS) LiDAR, remains limited to\nthis day, despite evidence that MS reflectance can improve the accuracy of ITS.\nThis study introduces FGI-EMIT, the first large-scale MS airborne laser\nscanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550\nnm, the dataset consists of 1,561 manually annotated trees, with a particular\nfocus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked\nfour conventional unsupervised algorithms and four supervised DL approaches.\nHyperparameters of unsupervised methods were optimized using a Bayesian\napproach, while DL models were trained from scratch. Among the unsupervised\nmethods, Treeiso achieved the highest test set F1-score of 52.7%. The DL\napproaches performed significantly better overall, with the best model,\nForestFormer3D, attaining an F1-score of 73.3%. The most significant difference\nwas observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9\npercentage points. An ablation study demonstrated that current DL-based\napproaches generally fail to leverage MS reflectance information when it is\nprovided as additional input features, although single channel reflectance can\nimprove accuracy marginally, especially for understory trees. A performance\nanalysis across point densities further showed that DL methods consistently\nremain superior to unsupervised algorithms, even at densities as low as 10\npoints/m$^2$.", "AI": {"tldr": "本研究引入了首个大规模多光谱（MS）机载激光雷达个体树木分割（ITS）基准数据集FGI-EMIT，并全面比较了传统无监督算法和监督深度学习（DL）方法。结果显示DL方法表现显著优于无监督算法，尤其是在林下树木分割方面，但当前DL模型尚未有效利用MS反射信息。", "motivation": "个体树木分割（ITS）对于森林清查、碳监测和生物多样性评估至关重要。然而，方法开发一直受限于缺乏大规模基准数据集，且多光谱（MS）激光雷达数据（已被证明能提高ITS精度）的可用性仍然有限。", "method": "本研究构建了FGI-EMIT数据集，它是首个大规模MS机载激光雷达ITS基准数据集，包含532、905和1,550 nm波长数据及1,561棵手动标注的树木（尤其关注小型林下树木）。我们使用该数据集对四种传统无监督算法和四种监督DL方法进行了全面基准测试。无监督方法的超参数通过贝叶斯方法优化，DL模型则从头开始训练。此外，还进行了消融研究以评估MS反射信息的作用，并分析了不同点云密度下的性能。", "result": "无监督方法中，Treeiso达到了最高的F1分数52.7%。DL方法整体表现显著更优，其中ForestFormer3D达到了73.3%的F1分数。在林下树木分割方面，ForestFormer3D比Treeiso高出25.9个百分点。消融研究表明，当前DL方法普遍未能有效利用MS反射信息作为额外输入特征，尽管单通道反射在边缘情况下（特别是林下树木）能略微提高精度。性能分析还显示，即使在低至10点/平方米的密度下，DL方法也始终优于无监督算法。", "conclusion": "监督深度学习方法在个体树木分割方面显著优于传统无监督算法，尤其在处理具有挑战性的林下树木时优势更为明显。尽管多光谱激光雷达数据具有潜力，但当前的深度学习模型尚未能充分利用其反射信息。FGI-EMIT数据集为ITS研究提供了一个宝贵的资源，未来需要进一步探索如何有效整合多光谱信息以提升DL模型的性能。"}}
{"id": "2511.00858", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00858", "abs": "https://arxiv.org/abs/2511.00858", "authors": ["Yu Liu", "Zhijie Liu", "Zedong Yang", "You-Fu Li", "He Kong"], "title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction", "comment": "This manuscript has been accepted to the IEEE Transactions on\n  Intelligent Transportation Systems as a regular paper", "summary": "Predicting pedestrian crossing intentions is crucial for the navigation of\nmobile robots and intelligent vehicles. Although recent deep learning-based\nmodels have shown significant success in forecasting intentions, few consider\nincomplete observation under occlusion scenarios. To tackle this challenge, we\npropose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded\nmotion patterns and leverages them to guide future intention prediction. During\nthe denoising stage, we introduce an occlusion-aware diffusion transformer\narchitecture to estimate noise features associated with occluded patterns,\nthereby enhancing the model's ability to capture contextual relationships in\noccluded semantic scenarios. Furthermore, an occlusion mask-guided reverse\nprocess is introduced to effectively utilize observation information, reducing\nthe accumulation of prediction errors and enhancing the accuracy of\nreconstructed motion features. The performance of the proposed method under\nvarious occlusion scenarios is comprehensively evaluated and compared with\nexisting methods on popular benchmarks, namely PIE and JAAD. Extensive\nexperimental results demonstrate that the proposed method achieves more robust\nperformance than existing methods in the literature.", "AI": {"tldr": "本文提出了一种遮挡感知扩散模型（ODM），用于在遮挡场景下预测行人过街意图。该模型通过重建被遮挡的运动模式来指导意图预测，并在流行基准数据集上取得了比现有方法更鲁棒的性能。", "motivation": "行人过街意图预测对移动机器人和智能车辆的导航至关重要。尽管现有深度学习模型已取得显著成功，但很少有模型能有效处理遮挡场景下的不完整观测问题。", "method": "本文提出了一种遮挡感知扩散模型（ODM）。该模型在去噪阶段引入了遮挡感知扩散Transformer架构，用于估计与被遮挡模式相关的噪声特征，以增强模型捕获遮挡语义场景中上下文关系的能力。此外，还引入了遮挡掩码引导的逆向过程，以有效利用观测信息，减少预测误差积累，并提高重建运动特征的准确性。", "result": "在PIE和JAAD等流行基准数据集上，对所提方法在各种遮挡场景下的性能进行了全面评估和比较。广泛的实验结果表明，所提方法比现有方法表现出更鲁棒的性能。", "conclusion": "所提出的遮挡感知扩散模型（ODM）能够有效应对遮挡场景下的行人意图预测挑战，并取得了比现有方法更强大的性能，为移动机器人和智能车辆的导航提供了更可靠的意图预测能力。"}}
{"id": "2511.00728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00728", "abs": "https://arxiv.org/abs/2511.00728", "authors": ["Hugo Massaroli", "Hernan Chaves", "Pilar Anania", "Mauricio Farez", "Emmanuel Iarussi", "Viviana Siless"], "title": "Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data", "comment": "7 pages, 2 figures", "summary": "Deep learning models have shown strong performance in diagnosing Alzheimer's\ndisease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with\ntraining datasets largely composed of North American cohorts such as those in\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their\ngeneralization to underrepresented populations remains underexplored. In this\nstudy, we benchmark convolutional and Transformer-based models on the ADNI\ndataset and assess their generalization performance on a novel Latin American\nclinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show\nthat while all models achieve high AUCs on ADNI (up to .96, .97), their\nperformance drops substantially on FLENI (down to .82, .80, respectively),\nrevealing a significant domain shift. The tested architectures demonstrated\nsimilar performance, calling into question the supposed advantages of\ntransformers for this specific task. Through ablation studies, we identify\nper-image normalization and a correct sampling selection as key factors for\ngeneralization. Occlusion sensitivity analysis further reveals that models\ntrained on ADNI, generally attend to canonical hypometabolic regions for the AD\nclass, but focus becomes unclear for the other classes and for FLENI scans.\nThese findings highlight the need for population-aware validation of diagnostic\nAI models and motivate future work on domain adaptation and cohort\ndiversification.", "AI": {"tldr": "研究发现，用于阿尔茨海默病诊断的深度学习模型，在北美数据集(ADNI)上表现优异，但在拉丁美洲人群(FLENI)上泛化性能显著下降，揭示了明显的领域漂移。图像级归一化和正确的采样选择是提高泛化能力的关键。", "motivation": "深度学习模型在利用神经影像数据（尤其是18F-FDG PET扫描）诊断阿尔茨海默病方面表现出色，但其对未充分代表人群的泛化能力尚未得到充分探索。", "method": "本研究在ADNI数据集上评估了卷积和基于Transformer的模型，并测试了它们在阿根廷FLENI研究所的拉丁美洲临床队列上的泛化性能。通过消融研究识别泛化关键因素，并使用遮挡敏感性分析揭示模型关注区域。", "result": "所有模型在ADNI上均达到高AUC（最高0.96，0.97），但在FLENI上的性能大幅下降（分别降至0.82，0.80），表明存在显著的领域漂移。卷积和Transformer架构表现相似。图像级归一化和正确的采样选择是泛化的关键因素。ADNI训练的模型对AD类关注典型低代谢区域，但对其他类别和FLENI扫描的关注点不明确。", "conclusion": "研究强调了对诊断AI模型进行人群感知验证的必要性，并推动了未来在领域适应和队列多样化方面的工作。"}}
{"id": "2511.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00698", "abs": "https://arxiv.org/abs/2511.00698", "authors": ["Taifour Yousra", "Beghdadi Azeddine", "Marie Luong", "Zuheng Ming"], "title": "Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics", "comment": null, "summary": "Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to\nmitigate high exposure side effects, but often suffers from noise and artifacts\nthat affect diagnostic accuracy. To tackle this issue, deep learning models\nhave been developed to enhance LDCT images. Various loss functions have been\nemployed, including classical approaches such as Mean Square Error and\nadversarial losses, as well as customized loss functions(LFs) designed for\nspecific architectures. Although these models achieve remarkable performance in\nterms of PSNR and SSIM, these metrics are limited in their ability to reflect\nperceptual quality, especially for medical images. In this paper, we focus on\none of the most critical elements of DL-based architectures, namely the loss\nfunction. We conduct an objective analysis of the relevance of different loss\nfunctions for LDCT image quality enhancement and their consistency with image\nquality metrics. Our findings reveal inconsistencies between LFs and quality\nmetrics, and highlight the need of consideration of image quality metrics when\ndeveloping a new loss function for image quality enhancement.", "AI": {"tldr": "该论文客观分析了用于低剂量CT图像增强的深度学习模型中不同损失函数与图像质量指标的一致性，发现两者存在不一致，并强调了在开发新损失函数时需考虑图像质量指标。", "motivation": "低剂量CT（LDCT）图像存在噪声和伪影，影响诊断准确性。深度学习模型虽能增强LDCT图像，并使用多种损失函数，但常用的PSNR和SSIM等指标无法充分反映医学图像的感知质量。因此，有必要客观分析不同损失函数与图像质量指标的相关性和一致性。", "method": "对用于低剂量CT图像质量增强的深度学习模型中的不同损失函数进行客观分析，评估它们与图像质量指标的相关性和一致性。", "result": "研究发现，损失函数与图像质量指标之间存在不一致性。", "conclusion": "强调在开发新的图像质量增强损失函数时，需要充分考虑图像质量指标。"}}
{"id": "2511.00846", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00846", "abs": "https://arxiv.org/abs/2511.00846", "authors": ["Zhihao Peng", "Cheng Wang", "Shengyuan Liu", "Zhiying Liang", "Yixuan Yuan"], "title": "OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks", "comment": null, "summary": "Brain imaging analysis is vital for diagnosing and treating brain disorders,\nand multimodal large language models (MLLMs) are increasingly assisting in that\nanalysis. However, current brain-oriented visual question-answering (VQA)\nbenchmarks either cover a few imaging modalities or are limited to\ncoarse-grained pathological descriptions, hindering a comprehensive assessment\nof MLLMs throughout the full clinical continuum. To address these, we introduce\nOmniBrainBench, the first comprehensive multimodal VQA benchmark specifically\ndesigned to assess the multimodal comprehension capabilities of MLLMs in brain\nimaging analysis.OmniBrainBench consists of 15 distinct brain imaging\nmodalities collected from 30 verified medical sources, yielding 9,527 validated\nVQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15\nmulti-stage clinical tasks rigorously validated by a professional radiologist.\nEvaluation of 24 state-of-the-art models, including open-source, medical, and\nproprietary MLLMs, highlights the substantial challenges posed by\nOmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)\nbeat open-source and medical models but lag physicians; (2) medical MLLMs vary\nwidely in performance; (3) open-source MLLMs trail overall but excel in\nspecific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,\nrevealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new\nstandard for evaluating and advancing MLLMs in brain imaging analysis,\nhighlighting gaps compared to expert clinical reasoning. We release it at\nbenchmark \\& code.", "AI": {"tldr": "本文引入了OmniBrainBench，这是首个全面多模态VQA基准，专门用于评估多模态大语言模型（MLLMs）在脑成像分析中的理解能力，并揭示了当前MLLMs与临床专家之间的显著差距。", "motivation": "当前的脑部视觉问答（VQA）基准在成像模态覆盖范围或病理描述细粒度方面存在局限，阻碍了对多模态大语言模型（MLLMs）在整个临床流程中进行全面评估。", "method": "研究者推出了OmniBrainBench，一个包含15种不同脑部成像模态、9,527对VQA问答和31,706张图像的综合性多模态VQA基准。该基准模拟临床工作流程，涵盖15项经专业放射科医生严格验证的多阶段临床任务。研究者对24个最先进的模型（包括开源、医学专用和专有MLLMs）进行了评估。", "result": "评估结果显示：1) 专有MLLMs（如GPT-5）优于开源和医学模型，但仍落后于医生；2) 医学MLLMs的性能表现差异较大；3) 开源MLLMs总体表现较差，但在特定任务中表现出色；4) MLLMs在复杂的术前任务中表现显著不足，揭示了从视觉到临床推理的差距。", "conclusion": "OmniBrainBench为评估和推进脑成像分析中的MLLMs树立了新标准，并突出了当前MLLMs与专家临床推理之间的差距。该基准及其代码已发布。"}}
{"id": "2511.01490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01490", "abs": "https://arxiv.org/abs/2511.01490", "authors": ["Max Schaffelder", "Albert Gatt"], "title": "Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning", "comment": null, "summary": "As synthetic data becomes widely used in language model development,\nunderstanding its impact on model behavior is crucial. This paper investigates\nthe impact of the diversity of sources of synthetic data on fine-tuned large\nlanguage models. We focus on three key dimensions: distribution collapse,\nadversarial robustness, and self-preference bias. Our findings reveal that\nfine-tuning models on synthetic data from diverse sources can mitigate\ndistribution collapse, preserving the breadth of the output distribution and\nthe diversity of the output text. Furthermore, while both human and synthetic\nfine-tuning data can remove safeguards, the latter preserves higher output\nquality, thus making outputs potentially more usable and dangerous. Finally,\nfine-tuning reduces self-preference bias, with human data being the most\neffective, followed by multi-source synthetic data.", "AI": {"tldr": "本文研究了不同来源的合成数据对微调大型语言模型（LLMs）行为的影响，发现多源合成数据能缓解分布坍缩并降低自偏好偏差，但移除安全措施后，其输出质量更高，潜在危险性也更大。", "motivation": "随着合成数据在语言模型开发中广泛应用，理解其对模型行为的影响至关重要。", "method": "研究通过微调大型语言模型，调查了不同来源的合成数据多样性对模型行为的影响，重点关注三个维度：分布坍缩、对抗鲁棒性和自偏好偏差。", "result": "研究发现，使用多源合成数据进行微调可以缓解分布坍缩，保持输出分布的广度和文本多样性。虽然人类数据和合成数据都能移除安全措施，但后者能保持更高的输出质量，使其输出更具可用性和潜在危险性。此外，微调能降低自偏好偏差，其中人类数据最有效，其次是多源合成数据。", "conclusion": "多源合成数据在缓解分布坍缩和降低自偏好偏差方面具有优势，但在移除安全措施后，其输出质量的保持使得模型在某些情况下更具潜在危险性。"}}
{"id": "2511.01082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01082", "abs": "https://arxiv.org/abs/2511.01082", "authors": ["Narges Ghasemi", "Amir Ziashahabi", "Salman Avestimehr", "Cyrus Shahabi"], "title": "GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025", "summary": "Image geolocalization, the task of determining an image's geographic origin,\nposes significant challenges, largely due to visual similarities across\ndisparate locations and the large search space. To address these issues, we\npropose a hierarchical sequence prediction approach inspired by how humans\nnarrow down locations from broad regions to specific addresses. Analogously,\nour model predicts geographic tokens hierarchically, first identifying a\ngeneral region and then sequentially refining predictions to increasingly\nprecise locations. Rather than relying on explicit semantic partitions, our\nmethod uses S2 cells, a nested, multiresolution global grid, and sequentially\npredicts finer-level cells conditioned on visual inputs and previous\npredictions. This procedure mirrors autoregressive text generation in large\nlanguage models. Much like in language modeling, final performance depends not\nonly on training but also on inference-time strategy. We investigate multiple\ntop-down traversal methods for autoregressive sampling, incorporating\ntechniques from test-time compute scaling used in language models.\nSpecifically, we integrate beam search and multi-sample inference while\nexploring various selection strategies to determine the final output. This\nenables the model to manage uncertainty by exploring multiple plausible paths\nthrough the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k\ndatasets against two distinct sets of baselines: those that operate without a\nMultimodal Large Language Model (MLLM) and those that leverage one. In the\nMLLM-free setting, our model surpasses other comparable baselines on nearly all\nmetrics, achieving state-of-the-art performance with accuracy gains of up to\n13.9%. When augmented with an MLLM, our model outperforms all baselines,\nsetting a new state-of-the-art across all metrics. The source code is available\nat https://github.com/NNargesNN/GeoToken.", "AI": {"tldr": "本文提出了一种受人类推理启发的图像地理定位分层序列预测方法，利用S2网格和自回归生成，通过探索多种推理策略，在无MLLM和有MLLM设置下均实现了最先进的性能。", "motivation": "图像地理定位面临两大挑战：不同地理位置之间视觉相似性高，以及搜索空间巨大。这些问题使得准确确定图像的地理来源变得困难。", "method": "该方法采用分层序列预测，从大区域逐步细化到精确位置，类似于人类的推理过程。它不依赖显式语义划分，而是使用嵌套的多分辨率S2单元格作为地理令牌。模型以自回归方式预测更精细级别的单元格，并结合视觉输入和先前的预测。此外，该研究还探索了多种自回归采样推断策略，包括束搜索（beam search）和多样本推断，以管理不确定性并选择最终输出。", "result": "在Im2GPS3k和YFCC4k数据集上，该模型在无多模态大语言模型（MLLM）设置下，几乎所有指标上都超越了其他可比较的基线，准确率提升高达13.9%，达到了最先进水平。当与MLLM结合使用时，该模型在所有指标上均优于所有基线，再次刷新了最先进性能。", "conclusion": "所提出的分层序列预测方法，结合S2单元格和先进的推理策略，能够有效应对图像地理定位的挑战，并在有无MLLM的两种情况下都取得了显著的性能提升，达到了新的最先进水平。"}}
{"id": "2511.01512", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01512", "abs": "https://arxiv.org/abs/2511.01512", "authors": ["Ayesha Afroza Mohsin", "Mashrur Ahsan", "Nafisa Maliyat", "Shanta Maria", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification", "comment": "Under review, 6 pages, 1 figure, 2 tables", "summary": "Toxic language in Bengali remains prevalent, especially in online\nenvironments, with few effective precautions against it. Although text\ndetoxification has seen progress in high-resource languages, Bengali remains\nunderexplored due to limited resources. In this paper, we propose a novel\npipeline for Bengali text detoxification that combines Pareto class-optimized\nlarge language models (LLMs) and Chain-of-Thought (CoT) prompting to generate\ndetoxified sentences. To support this effort, we construct BanglaNirTox, an\nartificially generated parallel corpus of 68,041 toxic Bengali sentences with\nclass-wise toxicity labels, reasonings, and detoxified paraphrases, using\nPareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox\ndataset is used to fine-tune language models to produce better detoxified\nversions of Bengali sentences. Our findings show that Pareto-optimized LLMs\nwith CoT prompting significantly enhance the quality and consistency of Bengali\ntext detoxification.", "AI": {"tldr": "本文提出了一种针对孟加拉语文本解毒的新型管道，结合了帕累托类别优化的LLM和思维链（CoT）提示，并构建了BanglaNirTox平行语料库以支持该任务。", "motivation": "孟加拉语中的毒性语言（尤其是在线环境）普遍存在，但缺乏有效的预防措施。尽管高资源语言的文本解毒已取得进展，但由于资源有限，孟加拉语在该领域仍未得到充分探索。", "method": "研究人员提出了一种结合帕累托类别优化的LLM和思维链（CoT）提示的孟加拉语文本解毒新管道。他们构建了BanglaNirTox，一个包含68,041个毒性孟加拉语句子及其类别毒性标签、推理和解毒释义的人工生成平行语料库，该语料库使用帕累托优化的LLM进行评估。BanglaNirTox数据集用于微调语言模型。", "result": "研究结果表明，结合CoT提示的帕累托优化LLM显著提高了孟加拉语文本解毒的质量和一致性。", "conclusion": "结合帕累托优化LLM和CoT提示的方法，以及新构建的BanglaNirTox数据集，能够有效提升孟加拉语文本解毒的质量和一致性。"}}
{"id": "2511.00738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00738", "abs": "https://arxiv.org/abs/2511.00738", "authors": ["Dmitrii Khizbullin", "Maksim Konoplia"], "title": "Towards classification-based representation learning for place recognition on LiDAR scans", "comment": null, "summary": "Place recognition is a crucial task in autonomous driving, allowing vehicles\nto determine their position using sensor data. While most existing methods rely\non contrastive learning, we explore an alternative approach by framing place\nrecognition as a multi-class classification problem. Our method assigns\ndiscrete location labels to LiDAR scans and trains an encoder-decoder model to\nclassify each scan's position directly. We evaluate this approach on the\nNuScenes dataset and show that it achieves competitive performance compared to\ncontrastive learning-based methods while offering advantages in training\nefficiency and stability.", "AI": {"tldr": "该论文将地点识别重构为多类别分类问题，利用LiDAR数据训练编解码模型直接分类位置，在NuScenes数据集上实现了与对比学习相当的性能，并提升了训练效率和稳定性。", "motivation": "地点识别是自动驾驶中的关键任务，但现有方法多依赖对比学习。研究者旨在探索一种替代方法，以期在训练效率和稳定性方面带来优势。", "method": "将地点识别问题定义为多类别分类任务。具体方法是为LiDAR扫描分配离散的地理位置标签，并训练一个编码器-解码器模型来直接分类每个扫描的位置。", "result": "在NuScenes数据集上，该方法与基于对比学习的方法相比，取得了具有竞争力的性能，并且在训练效率和稳定性方面表现出优势。", "conclusion": "将地点识别问题视为多类别分类是可行的替代方案，它不仅能达到与对比学习相当的性能，还能在训练效率和稳定性上带来改进。"}}
{"id": "2511.01139", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01139", "abs": "https://arxiv.org/abs/2511.01139", "authors": ["Yoshihiro Maruyama"], "title": "Learning with Category-Equivariant Architectures for Human Activity Recognition", "comment": null, "summary": "We propose CatEquiv, a category-equivariant neural network for Human Activity\nRecognition (HAR) from inertial sensors that systematically encodes temporal,\namplitude, and structural symmetries. In particular, we introduce the\ncategorical symmetry product where cyclic time shifts, positive gains and the\nsensor-hierarchy poset together capture the categorical symmetry structure of\nthe data. CatEquiv achieves equivariance with respect to the categorical\nsymmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv\nattains markedly higher robustness compared with circularly padded CNNs and\nplain CNNs. These results demonstrate that enforcing categorical symmetries\nyields strong invariance and generalization without additional model capacity.", "AI": {"tldr": "本文提出CatEquiv，一种针对惯性传感器人类活动识别（HAR）的范畴等变神经网络，通过编码时间、幅度和结构对称性，显著提升了模型对分布外扰动的鲁棒性。", "motivation": "研究旨在解决惯性传感器HAR中数据固有的时间、幅度及结构对称性问题，并提升模型在面对分布外扰动时的鲁棒性。", "method": "引入了CatEquiv，一个范畴等变神经网络。该网络通过“范畴对称积”系统地编码对称性，其中包含循环时间位移（时间对称）、正增益（幅度对称）和传感器层级偏序集（结构对称）。CatEquiv被设计为对该范畴对称积具有等变性。", "result": "在UCI-HAR数据集上，CatEquiv在分布外扰动下表现出比循环填充CNN和普通CNN显著更高的鲁棒性。", "conclusion": "强制执行范畴对称性可以带来强大的不变性和泛化能力，且无需增加额外的模型容量。"}}
{"id": "2511.00749", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00749", "abs": "https://arxiv.org/abs/2511.00749", "authors": ["Tanvi Dinkar", "Aiqi Jiang", "Gavin Abercrombie", "Ioannis Konstas"], "title": "Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models", "comment": "This is a preprint under review", "summary": "Social media has exacerbated the promotion of Western beauty norms, leading\nto negative self-image, particularly in women and girls, and causing harm such\nas body dysmorphia. Increasingly content on the internet has been artificially\ngenerated, leading to concerns that these norms are being exaggerated. The aim\nof this work is to study how generative AI models may encode 'beauty' and erase\n'ugliness', and discuss the implications of this for society. To investigate\nthese aims, we create two image generation pipelines: a text-to-image model and\na text-to-language model-to image model. We develop a structured beauty\ntaxonomy which we use to prompt three language models (LMs) and two\ntext-to-image models to cumulatively generate 5984 images using our two\npipelines. We then recruit women and non-binary social media users to evaluate\n1200 of the images through a Likert-scale within-subjects study. Participants\nshow high agreement in their ratings. Our results show that 86.5% of generated\nimages depicted people with lighter skin tones, 22% contained explicit content\ndespite Safe for Work (SFW) training, and 74% were rated as being in a younger\nage demographic. In particular, the images of non-binary individuals were rated\nas both younger and more hypersexualised, indicating troubling intersectional\neffects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such\nas \"a wide nose\") consistently produced higher Not SFW (NSFW) ratings\nregardless of gender. This work sheds light on the pervasive demographic biases\nrelated to beauty standards present in generative AI models -- biases that are\nactively perpetuated by model developers, such as via negative prompting. We\nconclude by discussing the implications of this on society, which include\npollution of the data streams and active erasure of features that do not fall\ninside the stereotype of what is considered beautiful by developers.", "AI": {"tldr": "本研究发现生成式AI模型在生成图像时，普遍存在对西方审美标准的偏见，表现为肤色浅、年龄小和过度性化，尤其对非二元性别个体影响更甚，即使是负面提示也未能有效纠正这些偏见。", "motivation": "社交媒体加剧了西方审美标准的推广，导致负面自我形象和身体畸形。随着AI生成内容的增加，研究旨在探讨生成式AI模型如何编码“美”并消除“丑”，以及其对社会的影响。", "method": "研究构建了文本到图像和文本到语言模型再到图像两种生成管道。开发了一个结构化的审美分类法，并用其提示三个语言模型和两个文本到图像模型，共生成了5984张图像。随后，招募了女性和非二元性别的社交媒体用户，通过李克特量表在受试者内部研究中评估了其中1200张图像。", "result": "参与者在评分上表现出高度一致性。结果显示，86.5%的生成图像描绘了肤色较浅的人，22%的图像包含露骨内容（尽管经过SFW训练），74%的图像被评定为年轻年龄段。特别是，非二元性别个体的图像被评定为更年轻和更过度性化，表明存在令人担忧的交叉效应。值得注意的是，即使是编码了“负面”或“丑陋”审美特征（如“宽鼻子”）的提示，无论性别如何，都持续产生了更高的NSFW（不安全工作场所）评级。", "conclusion": "生成式AI模型中普遍存在与审美标准相关的人口统计学偏见，这些偏见通过模型开发者（如通过负面提示）积极地延续。这些偏见将导致数据流污染，并主动抹去不符合开发者所认为的“美”的刻板印象的特征，对社会产生深远影响。"}}
{"id": "2511.01087", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01087", "abs": "https://arxiv.org/abs/2511.01087", "authors": ["Md. Abid Hasan Rafi", "Mst. Fatematuj Johora", "Pankaj Bhowmik"], "title": "SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices", "comment": null, "summary": "The emergence of 5G and 6G networks has established network slicing as a\nsignificant part of future service-oriented architectures, demanding refined\nidentification methods supported by robust datasets. The article presents\nSliceVision-F2I, a dataset of synthetic samples for studying feature\nvisualization in network slicing for next-generation networking systems. The\ndataset transforms multivariate Key Performance Indicator (KPI) vectors into\nvisual representations through four distinct encoding methods: physically\ninspired mappings, Perlin noise, neural wallpapering, and fractal branching.\nFor each encoding method, 30,000 samples are generated, each comprising a raw\nKPI vector and a corresponding RGB image at low-resolution pixels. The dataset\nsimulates realistic and noisy network conditions to reflect operational\nuncertainties and measurement imperfections. SliceVision-F2I is suitable for\ntasks involving visual learning, network state classification, anomaly\ndetection, and benchmarking of image-based machine learning techniques applied\nto network data. The dataset is publicly available and can be reused in various\nresearch contexts, including multivariate time series analysis, synthetic data\ngeneration, and feature-to-image transformations.", "AI": {"tldr": "本文介绍了SliceVision-F2I，一个用于研究5G/6G网络切片特征可视化的合成数据集，它将多变量KPI向量通过四种编码方法转换为低分辨率RGB图像，并模拟了真实的噪声网络条件。", "motivation": "5G和6G网络的兴起使得网络切片成为未来面向服务架构的重要组成部分，这需要更精细的识别方法和强大的数据集支持。", "method": "研究者创建了SliceVision-F2I数据集，通过四种独特的编码方法（物理启发映射、Perlin噪声、神经壁纸和分形分支）将多变量关键性能指标（KPI）向量转换为视觉表示。每种编码方法生成30,000个样本，每个样本包含原始KPI向量和对应的低分辨率RGB图像，并模拟了真实和嘈杂的网络条件。", "result": "SliceVision-F2I数据集包含通过四种编码方法生成的合成样本，每个样本包括KPI向量和对应的RGB图像。该数据集模拟了现实和噪声网络条件，适用于视觉学习、网络状态分类、异常检测以及基于图像的机器学习技术在网络数据上的基准测试。该数据集已公开发布。", "conclusion": "SliceVision-F2I数据集为下一代网络系统中的网络切片特征可视化研究提供了宝贵资源，支持视觉学习、网络状态分类、异常检测和图像机器学习技术基准测试，并可在多变量时间序列分析、合成数据生成和特征到图像转换等多种研究背景下重复使用。"}}
{"id": "2511.01143", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01143", "abs": "https://arxiv.org/abs/2511.01143", "authors": ["Ziyi Wang", "Yuanmei Zhang", "Dorna Esrafilzadeh", "Ali R. Jalili", "Suncheng Xiang"], "title": "MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation", "comment": "Work in progress", "summary": "Early and accurate segmentation of colorectal polyps is critical for reducing\ncolorectal cancer mortality, which has been extensively explored by academia\nand industry. However, current deep learning-based polyp segmentation models\neither compromise clinical decision-making by providing ambiguous polyp margins\nin segmentation outputs or rely on heavy architectures with high computational\ncomplexity, resulting in insufficient inference speeds for real-time colorectal\nendoscopic applications. To address this problem, we propose MicroAUNet, a\nlight-weighted attention-based segmentation network that combines\ndepthwise-separable dilated convolutions with a single-path, parameter-shared\nchannel-spatial attention block to strengthen multi-scale boundary features. On\nthe basis of it, a progressive two-stage knowledge-distillation scheme is\nintroduced to transfer semantic and boundary cues from a high-capacity teacher.\nExtensive experiments on benchmarks also demonstrate the state-of-the-art\naccuracy under extremely low model complexity, indicating that MicroAUNet is\nsuitable for real-time clinical polyp segmentation. The code is publicly\navailable at https://github.com/JeremyXSC/MicroAUNet.", "AI": {"tldr": "本文提出了一种轻量级、注意力机制的结直肠息肉分割网络MicroAUNet，结合知识蒸馏，实现了实时临床应用所需的高精度和低计算复杂度。", "motivation": "当前的深度学习息肉分割模型存在两个主要问题：一是分割结果的息肉边界模糊，影响临床决策；二是模型架构过于庞大，计算复杂度高，导致推理速度慢，无法满足实时结直肠内窥镜应用的需求。", "method": "本文提出了MicroAUNet，一个轻量级的注意力分割网络。它结合了深度可分离膨胀卷积和一个单路径、参数共享的通道-空间注意力模块，以增强多尺度边界特征。在此基础上，引入了一个渐进式两阶段知识蒸馏方案，用于从一个高容量教师模型中迁移语义和边界信息。", "result": "在多个基准测试上的大量实验表明，MicroAUNet在极低的模型复杂度下达到了最先进的准确性。", "conclusion": "实验结果表明，MicroAUNet适用于实时临床息肉分割，解决了现有模型在精度和速度上的权衡问题。"}}
{"id": "2511.01568", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01568", "abs": "https://arxiv.org/abs/2511.01568", "authors": ["Seungmin Shin", "Dooyoung Kim", "Youngjoong Ko"], "title": "ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation", "comment": "Published at EMNLP 2025 main", "summary": "Controllable Dialogue Generation (CDG) enables chatbots to generate responses\nwith desired attributes, and weighted decoding methods have achieved\nsignificant success in the CDG task. However, using a fixed constant value to\nmanage the bias of attribute probabilities makes it challenging to find an\nideal control strength that satisfies both controllability and fluency. To\naddress this issue, we propose ECO decoding (Entropy-based COntrol), which\ndynamically adjusts the control strength at each generation step according to\nthe model's entropy in both the language model and attribute classifier\nprobability distributions. Experiments on the DailyDialog and MultiWOZ datasets\ndemonstrate that ECO decoding consistently improves controllability while\nmaintaining fluency and grammaticality, outperforming prior decoding methods\nacross various models and settings. Furthermore, ECO decoding alleviates\nprobability interpolation issues in multi-attribute generation and consequently\ndemonstrates strong performance in both single and multi-attribute scenarios.", "AI": {"tldr": "本文提出了一种名为ECO解码的新型动态解码方法，通过基于语言模型和属性分类器概率分布的熵，在可控对话生成（CDG）任务中动态调整控制强度，从而在保持流畅性的同时显著提升可控性。", "motivation": "现有的可控对话生成（CDG）加权解码方法使用固定的常数来管理属性概率偏差，这使得很难找到一个理想的控制强度来同时满足可控性和流畅性。", "method": "本文提出了ECO解码（Entropy-based COntrol），它根据语言模型和属性分类器概率分布的熵，在每个生成步骤动态调整控制强度。", "result": "实验结果表明，ECO解码在DailyDialog和MultiWOZ数据集上，始终能在保持流畅性和语法正确性的同时提高可控性，并且优于之前的解码方法。此外，ECO解码还缓解了多属性生成中的概率插值问题，在单属性和多属性场景中都表现出色。", "conclusion": "ECO解码是一种有效且动态的可控对话生成方法，通过基于熵的控制强度调整，成功解决了传统方法的局限性，显著提升了可控性，同时保持了生成对话的流畅性，尤其在多属性生成中表现突出。"}}
{"id": "2511.01526", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01526", "abs": "https://arxiv.org/abs/2511.01526", "authors": ["Seokhoon Kang", "Yejin Jeon", "Seonjeong Hwang", "Gary Geunbae Lee"], "title": "Difficulty-Controllable Cloze Question Distractor Generation", "comment": null, "summary": "Multiple-choice cloze questions are commonly used to assess linguistic\nproficiency and comprehension. However, generating high-quality distractors\nremains challenging, as existing methods often lack adaptability and control\nover difficulty levels, and the absence of difficulty-annotated datasets\nfurther hinders progress. To address these issues, we propose a novel framework\nfor generating distractors with controllable difficulty by leveraging both data\naugmentation and a multitask learning strategy. First, to create a\nhigh-quality, difficulty-annotated dataset, we introduce a two-way distractor\ngeneration process in order to produce diverse and plausible distractors. These\ncandidates are subsequently refined through filtering and then categorized by\ndifficulty using an ensemble QA system. Second, this newly created dataset is\nleveraged to train a difficulty-controllable generation model via multitask\nlearning. The framework includes carefully designed auxiliary tasks that\nenhance the model's semantic understanding of distractors and its ability to\nestimate their difficulty. Experimental results demonstrate that our method\ngenerates high-quality distractors across difficulty levels and substantially\noutperforms GPT-4o in aligning distractor difficulty with human perception.", "AI": {"tldr": "本文提出了一种新颖的框架，通过数据增强和多任务学习策略，生成具有可控难度的多项选择题干扰项，并构建了一个难度标注数据集。", "motivation": "生成高质量干扰项仍然具有挑战性，现有方法缺乏适应性和对难度水平的控制，且缺乏难度标注数据集阻碍了进展。", "method": "首先，通过“双向干扰项生成过程”创建多样且合理的干扰项，经过筛选后，利用集成QA系统按难度进行分类，从而构建一个高质量、难度标注的数据集。其次，利用该数据集通过多任务学习训练一个难度可控的生成模型，其中包含精心设计的辅助任务，以增强模型对干扰项的语义理解和难度估计能力。", "result": "实验结果表明，该方法能够生成不同难度水平的高质量干扰项，并且在使干扰项难度与人类感知对齐方面，显著优于GPT-4o。", "conclusion": "该研究成功解决了干扰项生成中缺乏难度控制和标注数据集的问题，提出了一种有效的方法来生成高质量、难度可控的干扰项，并证明了其优越性。"}}
{"id": "2511.00777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00777", "abs": "https://arxiv.org/abs/2511.00777", "authors": ["Anis Suttan Shahrir", "Zakiah Ayop", "Syarulnaziah Anawar", "Norulzahrah Mohd Zainudin"], "title": "A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection", "comment": null, "summary": "Durian plantation suffers from animal intrusions that cause crop damage and\nfinancial loss. The traditional farming practices prove ineffective due to the\nunavailability of monitoring without human intervention. The fast growth of\nmachine learning and Internet of Things (IoT) technology has led to new ways to\ndetect animals. However, current systems are limited by dependence on single\nobject detection algorithms, less accessible notification platforms, and\nlimited deterrent mechanisms. This research suggests an IoT-enabled animal\ndetection system for durian crops. The system integrates YOLOv5 and SSD object\ndetection algorithms to improve detection accuracy. The system provides\nreal-time monitoring, with detected intrusions automatically reported to\nfarmers via Telegram notifications for rapid response. An automated sound\nmechanism (e.g., tiger roar) is triggered once the animal is detected. The\nYOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,\n85% and 70%, respectively. The system shows the highest accuracy in daytime and\ndecreases at night, regardless of whether the image is still or a video.\nOverall, this study contributes a comprehensive and practical framework that\ncombines detection, notification, and deterrence, paving the way for future\ninnovations in automated farming solutions.", "AI": {"tldr": "本研究开发了一个基于物联网的榴莲农场动物入侵检测系统，该系统结合YOLOv5和SSD算法，实现实时监控、Telegram通知和自动声音驱逐。", "motivation": "榴莲种植园遭受动物入侵导致作物损失和经济损失，传统方法无效。现有动物检测系统受限于单一目标检测算法、通知平台不便以及威慑机制不足。", "method": "构建了一个物联网（IoT）动物检测系统，该系统集成YOLOv5和SSD两种目标检测算法以提高准确性。检测到动物后，系统通过Telegram向农民发送实时通知，并触发自动声音（如虎啸）进行威慑。", "result": "YOLO+SSD模型对大象、野猪和猴子的检测准确率分别达到90%、85%和70%。系统在白天表现出最高的准确率，夜间准确率有所下降，这与图像是静态还是视频无关。", "conclusion": "该研究提供了一个结合检测、通知和威慑的全面且实用的框架，为未来自动化农业解决方案的创新奠定了基础。"}}
{"id": "2511.00815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00815", "abs": "https://arxiv.org/abs/2511.00815", "authors": ["Yue Gou", "Fanghui Song", "Yuming Xing", "Shengzhu Shi", "Zhichang Guo", "Boying Wu"], "title": "TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation", "comment": "14 pages, 7 figures", "summary": "Pancreas segmentation in medical image processing is a persistent challenge\ndue to its small size, low contrast against adjacent tissues, and significant\ntopological variations. Traditional level set methods drive boundary evolution\nusing gradient flows, often ignoring pointwise topological effects. Conversely,\ndeep learning-based segmentation networks extract rich semantic features but\nfrequently sacrifice structural details. To bridge this gap, we propose a novel\nmodel named TA-LSDiff, which combined topology-aware diffusion probabilistic\nmodel and level set energy, achieving segmentation without explicit geometric\nevolution. This energy function guides implicit curve evolution by integrating\nthe input image and deep features through four complementary terms. To further\nenhance boundary precision, we introduce a pixel-adaptive refinement module\nthat locally modulates the energy function using affinity weighting from\nneighboring evidence. Ablation studies systematically quantify the contribution\nof each proposed component. Evaluations on four public pancreas datasets\ndemonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming\nexisting methods. These results establish TA-LSDiff as a practical and accurate\nsolution for pancreas segmentation.", "AI": {"tldr": "本文提出了一种名为TA-LSDiff的新模型，结合拓扑感知扩散概率模型和水平集能量，解决了胰腺分割中存在的挑战，实现了领先的分割精度。", "motivation": "胰腺分割面临尺寸小、对比度低和拓扑结构多变等挑战。传统水平集方法常忽略点式拓扑效应，而深度学习方法则牺牲结构细节。本研究旨在弥补这些方法的不足，实现兼顾拓扑和细节的精准分割。", "method": "提出TA-LSDiff模型，它结合了拓扑感知扩散概率模型和水平集能量，无需显式几何演化即可实现分割。该能量函数通过四个互补项整合输入图像和深度特征来指导隐式曲线演化。此外，引入了一个像素自适应细化模块，利用邻近证据的亲和力加权局部调制能量函数，以提高边界精度。", "result": "通过消融研究系统地量化了每个组件的贡献。在四个公共胰腺数据集上的评估表明，TA-LSDiff实现了最先进的精度，优于现有方法。", "conclusion": "TA-LSDiff被确立为一种实用且准确的胰腺分割解决方案。"}}
{"id": "2511.01589", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01589", "abs": "https://arxiv.org/abs/2511.01589", "authors": ["Wenjie Hua", "Hoang H. Nguyen", "Gangyan Ge"], "title": "BIRD: Bronze Inscription Restoration and Dating", "comment": "Accepted at EMNLP 2025 (Main Conference)", "summary": "Bronze inscriptions from early China are fragmentary and difficult to date.\nWe introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded\ndataset grounded in standard scholarly transcriptions and chronological labels.\nWe further propose an allograph-aware masked language modeling framework that\nintegrates domain- and task-adaptive pretraining with a Glyph Net (GN), which\nlinks graphemes and allographs. Experiments show that GN improves restoration,\nwhile glyph-biased sampling yields gains in dating.", "AI": {"tldr": "本文介绍了BIRD数据集及一种基于字形感知的掩码语言模型框架，用于早期中国青铜器铭文的修复和断代。", "motivation": "早期中国青铜器铭文残缺不全且难以断代，这是研究者面临的主要挑战。", "method": "研究者构建了BIRD数据集，该数据集基于标准学术转录和年代标签。同时，提出了一种字形感知的掩码语言模型框架，结合领域和任务自适应预训练与一个字形网络（Glyph Net, GN），用于连接字位和异体字。在断代任务中还采用了字形偏置采样。", "result": "实验结果表明，字形网络（GN）能有效提升铭文修复效果，而字形偏置采样则显著提高了断代任务的性能。", "conclusion": "通过引入BIRD数据集和字形感知的掩码语言模型框架（特别是Glyph Net和字形偏置采样），能够有效解决早期中国青铜器铭文的修复和断代难题。"}}
{"id": "2511.01558", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01558", "abs": "https://arxiv.org/abs/2511.01558", "authors": ["Luciana Ciringione", "Emma Franchino", "Simone Reigl", "Isaia D'Onofrio", "Anna Serbati", "Oleksandra Poquet", "Florence Gabriel", "Massimo Stella"], "title": "Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o", "comment": null, "summary": "Math anxiety poses significant challenges for university psychology students,\naffecting their career choices and overall well-being. This study employs a\nframework based on behavioural forma mentis networks (i.e. cognitive models\nthat map how individuals structure their associative knowledge and emotional\nperceptions of concepts) to explore individual and group differences in the\nperception and association of concepts related to math and anxiety. We\nconducted 4 experiments involving psychology undergraduates from 2 samples (n1\n= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;\nGPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network\nfeatures to predict psychometric scores for math anxiety and its facets\n(observational, social and evaluational) from the Math Anxiety Scale.\nExperiment 4 focuses on group-level perceptions extracted from human students,\nGPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive\nvalence ratings and higher network degree for \"anxiety\", together with negative\nratings for \"math\", can predict higher total and evaluative math anxiety. In\ncontrast, these models do not work on GPT-based data because of differences in\nsimulated networks and psychometric scores compared to humans. These results\nwere also reconciled with differences found in the ways that high/low subgroups\nof simulated and real students framed semantically and emotionally STEM\nconcepts. High math-anxiety students collectively framed \"anxiety\" in an\nemotionally polarising way, absent in the negative perception of low\nmath-anxiety students. \"Science\" was rated positively, but contrasted against\nthe negative perception of \"math\". These findings underscore the importance of\nunderstanding concept perception and associations in managing students' math\nanxiety.", "AI": {"tldr": "本研究利用行为心智网络框架，探索了大学生对数学和焦虑相关概念的感知与关联，并与GPT模拟学生进行对比。结果发现，在人类学生中，对“焦虑”的积极评价和高网络度以及对“数学”的负面评价可预测更高的数学焦虑，但在GPT模型中不适用。研究强调了理解概念感知和关联对于管理学生数学焦虑的重要性。", "motivation": "数学焦虑对大学心理学学生构成重大挑战，影响其职业选择和整体福祉。本研究旨在通过探索个体和群体对数学及焦虑相关概念的感知与关联，以更好地理解和管理这种焦虑。", "method": "研究采用基于行为心智网络（即个体如何构建概念的联想知识和情感感知）的框架。进行了4项实验，涉及两组心理学本科生（n1=70，n2=57），并与GPT模拟学生（GPT-3.5: n=300; GPT-4o: n=300）进行对比。实验1、2、3利用个体层面网络特征预测数学焦虑量表得分。实验4侧重于从人类学生、GPT-3.5和GPT-4o的网络中提取群体层面感知。", "result": "在人类学生中，对“焦虑”的积极情感评级和更高的网络度，以及对“数学”的负面评级，可以预测更高的总数学焦虑和评估性数学焦虑。然而，这些模型不适用于基于GPT的数据，因为模拟网络和心理测量分数与人类存在差异。研究还发现高/低数学焦虑学生群体在语义和情感上构建STEM概念的方式存在差异：高数学焦虑学生集体以情感两极化的方式构建“焦虑”，而低数学焦虑学生则缺乏这种两极化；“科学”被积极评价，但与对“数学”的负面感知形成对比。", "conclusion": "研究结果强调了理解概念感知和关联对于管理学生数学焦虑的重要性，并揭示了人类在数学焦虑认知和情感结构上的独特性，这些是当前GPT模型尚未能完全复制的。"}}
{"id": "2511.01213", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01213", "abs": "https://arxiv.org/abs/2511.01213", "authors": ["Riddhi Jain", "Manasi Patwardhan", "Parijat Deshpande", "Venkataramana Runkana"], "title": "Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering", "comment": "10 pages, 11 figures, 6 tables", "summary": "The immense diversity in the culture and culinary of Indian cuisines calls\nattention to the major shortcoming of the existing Visual Question\nAnswering(VQA) systems which are inclined towards the foods from Western\nregion. Recent attempt towards building a VQA dataset for Indian food is a step\ntowards addressing this challenge. However, their approach towards VQA follows\na two-step process in which the answer is generated first, followed by the\nexplanation of the expected answer. In this work, we claim that food VQA\nrequires to follow a multi-step reasoning process to arrive at an accurate\nanswer, especially in the context of India food, which involves understanding\ncomplex culinary context and identifying relationships between various food\nitems. With this hypothesis we create reasoning chains upon the QA with minimal\nhuman intervention. We fine-tune smaller LLMs and VLMs with auto-validated\nreasoning chains and further train them using reinforcement learning with\nlarger data. With augmentation of reasoning chains, we observed accuracy\nimprovement of an average 10 percentage points on the baseline. We provide\ndetailed analysis in terms the effect of addition of reasoning chains for the\nIndian Food VQA task.\n  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge\nGraph.", "AI": {"tldr": "本文提出通过引入多步推理链和强化学习，显著提升了针对印度美食的视觉问答（VQA）系统的准确性，解决了现有系统对西方食物的偏向。", "motivation": "现有VQA系统偏向西方食物，不适用于印度美食的复杂烹饪和文化多样性。虽然有印度美食VQA数据集，但其两步式问答流程不足以处理复杂的推理需求，尤其是在理解印度美食的烹饪背景和食物项之间关系时。", "method": "研究假设美食VQA需要多步推理过程。基于此假设，论文以最少的人工干预创建了推理链。随后，使用自动验证的推理链微调小型LLM和VLM，并利用强化学习和更大规模的数据进一步训练模型。", "result": "通过增加推理链，模型在基线上平均实现了10个百分点的准确率提升。论文还详细分析了推理链对印度美食VQA任务的影响。", "conclusion": "为印度美食VQA任务引入多步推理链是有效且必要的，它能显著提高模型的准确性，更好地处理复杂的印度烹饪上下文和食物关系，从而弥补现有VQA系统在多样化美食领域中的不足。"}}
{"id": "2511.01194", "categories": ["cs.CV", "cs.AI", "68T07 (Artificial neural networks and deep learning), 68U10\n  (Computer graphics, computational geometry)"], "pdf": "https://arxiv.org/pdf/2511.01194", "abs": "https://arxiv.org/abs/2511.01194", "authors": ["Minmin Zeng"], "title": "A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment", "comment": "10 pages, 5 figures. Submitted as a computer vision paper in the\n  cs.CV category", "summary": "Action Quality Assessment (AQA) requires fine-grained understanding of human\nmotion and precise evaluation of pose similarity. This paper proposes a\ntopology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,\nwhich models the human skeleton as a graph to learn discriminative,\ntopology-sensitive pose embeddings. Using a Siamese architecture trained with a\ncontrastive regression objective, our method outperforms coordinate-based\nbaselines and achieves competitive performance on AQA-7 and FineDiving\nbenchmarks. Experimental results and ablation studies validate the\neffectiveness of leveraging skeletal topology for pose similarity and action\nquality assessment.", "AI": {"tldr": "本文提出了一种名为GCN-PSN的拓扑感知图卷积网络（GCN）框架，通过将人体骨架建模为图，学习区分性的拓扑敏感姿态嵌入，并结合Siamese架构和对比回归目标，用于动作质量评估（AQA）。", "motivation": "动作质量评估（AQA）需要对人体运动进行细致的理解，并精确评估姿态相似性。", "method": "研究者提出了GCN-PSN框架，该框架将人体骨架建模为图，以学习区分性的、拓扑敏感的姿态嵌入。该方法采用Siamese架构，并使用对比回归目标进行训练。", "result": "GCN-PSN方法优于基于坐标的基线方法，并在AQA-7和FineDiving基准测试中取得了有竞争力的性能。", "conclusion": "实验结果和消融研究验证了利用骨架拓扑结构进行姿态相似性评估和动作质量评估的有效性。"}}
{"id": "2511.00821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00821", "abs": "https://arxiv.org/abs/2511.00821", "authors": ["Ruoxiang Huang", "Xindian Ma", "Rundong Kong", "Zhen Yuan", "Peng Zhang"], "title": "OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong performance across\nvarious multimodal tasks, where position encoding plays a vital role in\nmodeling both the sequential structure of textual information and the spatial\nstructure of visual information. However, current VLMs commonly adopt\nmodality-unified 1D or 2D positional indexing strategies, which treat textual\nand visual tokens uniformly without accounting for their distinct structural\nproperties and sequential continuity for text and spatial coherence for vision.\nTo address this limitation, we propose OMEGA, a novel position encoding\nframework that employs Modality-Specific Position Encoding (MSPE) to assign\npositional indices while preserving the inherent structures of each modality\nacross separate coordinate dimensions. Additionally, to align the information\ndensity of multimodal data in the positional index space, OMEGA introduces\nGlobal Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the\nposition encoding step size of visual tokens based on the embedding entropy of\nboth modalities. Experimental results demonstrate that OMEGA consistently\nenhances VLM performance across diverse architectures and VQA benchmarks. On\nvisual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline\nposition encoding strategies on Qwen2.5-VL-3B, with consistent gains observed\nacross larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.", "AI": {"tldr": "本文提出OMEGA框架，通过模态特定位置编码（MSPE）和全局自适应编码步长缩放（GAESS），优化了视觉-语言模型（VLMs）中对文本和视觉信息独特结构属性的处理，显著提升了VLMs在多模态任务上的性能。", "motivation": "当前的视觉-语言模型普遍采用模态统一的1D或2D位置编码策略，未能充分考虑文本信息的序列连续性和视觉信息的空间连贯性等不同模态的独特结构属性，这限制了模型对多模态数据的理解能力。", "method": "本文提出了OMEGA位置编码框架，包含两部分：1. 模态特定位置编码（MSPE），为不同模态分配独立坐标维度上的位置索引，以保留其固有结构。2. 全局自适应编码步长缩放（GAESS），根据两种模态的嵌入熵自适应调整视觉token的位置编码步长，以对齐多模态数据在位置索引空间中的信息密度。", "result": "实验结果表明，OMEGA持续提升了VLM在不同架构和VQA基准上的性能。在视觉密集型任务上，OMEGA在Qwen2.5-VL-3B模型上比基线位置编码策略提高了高达3.43%，并且在Qwen2.5-VL-7B和LLaVA-v1.5-7B等更大模型上也观察到了一致的性能提升。", "conclusion": "OMEGA框架通过引入模态特定位置编码和自适应步长缩放，有效解决了现有VLM位置编码策略的局限性，显著增强了模型对文本和视觉信息独特结构属性的建模能力，从而全面提升了VLM在多模态任务中的表现。"}}
{"id": "2511.00859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00859", "abs": "https://arxiv.org/abs/2511.00859", "authors": ["Jaehyun Park", "Konyul Park", "Daehun Kim", "Junseo Park", "Jun Won Choi"], "title": "Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion", "comment": "Accepted to NeurIPS 2025", "summary": "In autonomous driving, transparency in the decision-making of perception\nmodels is critical, as even a single misperception can be catastrophic. Yet\nwith multi-sensor inputs, it is difficult to determine how each modality\ncontributes to a prediction because sensor information becomes entangled within\nthe fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a\npost-hoc, model-agnostic interpretability method that disentangles\nmodality-specific information across all layers of a pretrained fusion model.\nTo our knowledge, LMD is the first approach to attribute the predictions of a\nperception model to individual input modalities in a sensor-fusion system for\nautonomous driving. We evaluate LMD on pretrained fusion models under\ncamera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous\ndriving. Its effectiveness is validated using structured perturbation-based\nmetrics and modality-wise visual decompositions, demonstrating practical\napplicability to interpreting high-capacity multimodal architectures. Code is\navailable at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.", "AI": {"tldr": "本文提出了一种名为分层模态分解（LMD）的后验、模型无关的可解释性方法，用于在自动驾驶中分解多传感器融合模型中各模态的贡献。", "motivation": "在自动驾驶中，感知模型的决策透明度至关重要，因为一次误感知可能导致灾难性后果。然而，多传感器输入导致传感器信息在融合网络中纠缠不清，难以确定每种模态对预测的贡献。", "method": "引入了分层模态分解（LMD）方法，这是一种后验、模型无关的可解释性方法，旨在解耦预训练融合模型所有层中的模态特定信息。", "result": "LMD在摄像头-雷达、摄像头-激光雷达和摄像头-雷达-激光雷达等自动驾驶设置下的预训练融合模型上进行了评估。其有效性通过结构化扰动度量和模态分解可视化得到验证，表明其在解释高容量多模态架构方面具有实际适用性。", "conclusion": "LMD是首个将自动驾驶传感器融合系统中感知模型的预测归因于单个输入模态的方法，证明了其在解释高容量多模态架构方面的实际适用性。"}}
{"id": "2511.00801", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.00801", "abs": "https://arxiv.org/abs/2511.00801", "authors": ["Zhihui Chen", "Mengling Feng"], "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing", "comment": null, "summary": "Recent advances in multimodal large language models have enabled remarkable\nmedical image editing capabilities. However, the research community's progress\nremains constrained by the absence of large-scale, high-quality, and openly\naccessible datasets built specifically for medical image editing with strict\nanatomical and clinical constraints. We introduce Med-Banana-50K, a\ncomprehensive 50K-image dataset for instruction-based medical image editing\nspanning three modalities (chest X-ray, brain MRI, fundus photography) and 23\ndisease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image\nto generate bidirectional edits (lesion addition and removal) from real medical\nimages. What distinguishes Med-Banana-50K from general-domain editing datasets\nis our systematic approach to medical quality control: we employ LLM-as-Judge\nwith a medically grounded rubric (instruction compliance, structural\nplausibility, realism, and fidelity preservation) and history-aware iterative\nrefinement up to five rounds. Beyond single-turn editing, Med-Banana-50K\nincludes 37K failed attempts with full conversation logs for preference\nlearning and alignment research. By providing this large-scale, medically\nvalidated, and fully documented resource, Med-Banana-50K establishes a\nfoundation for training and evaluating the next generation of medical image\nediting models.Our dataset and code are publicly available at\n[https://github.com/richardChenzhihui/med-banana-50k].", "AI": {"tldr": "Med-Banana-50K是一个大规模、高质量、开放获取的医学图像编辑数据集，包含5万张图像，覆盖三种模态和23种疾病类型，通过LLM生成和严格的医学质量控制构建，旨在推动医学图像编辑模型的发展。", "motivation": "尽管多模态大型语言模型在医学图像编辑方面取得了进展，但研究社区仍受限于缺乏专门为医学图像编辑设计、具有严格解剖学和临床约束的大规模、高质量、开放获取数据集。", "method": "本文介绍了Med-Banana-50K数据集，该数据集包含5万张图像，用于基于指令的医学图像编辑，涵盖胸部X光、脑部MRI和眼底摄影三种模态以及23种疾病类型。数据集通过利用Gemini-2.5-Flash-Image从真实医学图像生成双向编辑（病灶添加和移除）。其独特之处在于采用系统性的医学质量控制方法：使用“LLM-as-Judge”结合医学标准（指令依从性、结构合理性、真实性、保真度）进行评估，并进行最多五轮的历史感知迭代优化。此外，数据集还包含3.7万次失败尝试的完整对话日志，用于偏好学习和对齐研究。", "result": "创建了Med-Banana-50K，一个包含5万张图像的综合性数据集，用于基于指令的医学图像编辑，涵盖三种模态和23种疾病类型。该数据集通过LLM生成和严格的医学质量控制（LLM-as-Judge及迭代优化）构建，并包含了3.7万次失败尝试的对话日志，为偏好学习和对齐研究提供了资源。", "conclusion": "通过提供这个大规模、经过医学验证且完整记录的资源，Med-Banana-50K为训练和评估下一代医学图像编辑模型奠定了基础。"}}
{"id": "2511.01615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01615", "abs": "https://arxiv.org/abs/2511.01615", "authors": ["Francisco Portillo López"], "title": "Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers", "comment": "12 pages, 3 figures", "summary": "Linguistic errors are not merely deviations from normative grammar; they\noffer a unique window into the cognitive architecture of language and expose\nthe current limitations of artificial systems that seek to replicate them. This\nproject proposes an interdisciplinary study of linguistic errors produced by\nnative Spanish speakers, with the aim of analyzing how current large language\nmodels (LLM) interpret, reproduce, or correct them. The research integrates\nthree core perspectives: theoretical linguistics, to classify and understand\nthe nature of the errors; neurolinguistics, to contextualize them within\nreal-time language processing in the brain; and natural language processing\n(NLP), to evaluate their interpretation against linguistic errors. A\npurpose-built corpus of authentic errors of native Spanish (+500) will serve as\nthe foundation for empirical analysis. These errors will be tested against AI\nmodels such as GPT or Gemini to assess their interpretative accuracy and their\nability to generalize patterns of human linguistic behavior. The project\ncontributes not only to the understanding of Spanish as a native language but\nalso to the development of NLP systems that are more cognitively informed and\ncapable of engaging with the imperfect, variable, and often ambiguous nature of\nreal human language.", "AI": {"tldr": "该项目旨在通过跨学科研究母语西班牙语者的语言错误，分析大型语言模型（LLM）如何解释、复现或纠正这些错误，以评估其认知能力和泛化能力。", "motivation": "语言错误不仅揭示了语言的认知结构，也暴露了当前人工智能系统在复制人类语言方面的局限性。研究旨在深入理解这些错误，并改进现有LLM的不足。", "method": "研究将整合理论语言学（分类错误）、神经语言学（语境化错误）和自然语言处理（评估LLM对错误的解释）三个核心视角。将构建一个包含500多个真实母语西班牙语错误的专用语料库，并使用GPT或Gemini等AI模型进行测试。", "result": "通过将语料库中的错误与AI模型进行测试，评估LLM对语言错误的解释准确性以及其泛化人类语言行为模式的能力。", "conclusion": "该项目不仅有助于理解作为母语的西班牙语，还将促进开发更具认知信息、能更好地处理真实人类语言中不完美、多变和模糊特性的自然语言处理系统。"}}
{"id": "2511.00908", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00908", "abs": "https://arxiv.org/abs/2511.00908", "authors": ["Heng Zheng", "Yuling Shi", "Xiaodong Gu", "Haochen You", "Zijian Zhang", "Lubin Gan", "Hao Zhang", "Wenjun Huang", "Jin Huang"], "title": "GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks", "comment": null, "summary": "Visual geo-localization requires extensive geographic knowledge and\nsophisticated reasoning to determine image locations without GPS metadata.\nTraditional retrieval methods are constrained by database coverage and quality.\nRecent Large Vision-Language Models (LVLMs) enable direct location reasoning\nfrom image content, yet individual models struggle with diverse geographic\nregions and complex scenes. Existing multi-agent systems improve performance\nthrough model collaboration but treat all agent interactions uniformly. They\nlack mechanisms to handle conflicting predictions effectively. We propose\n\\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph\nneural networks for visual geo-localization. Our approach models diverse debate\nrelationships through typed edges, distinguishing supportive collaboration,\ncompetitive argumentation, and knowledge transfer. We introduce a dual-level\ndebate mechanism combining node-level refinement and edge-level argumentation\nmodeling. A cross-level topology refinement strategy enables co-evolution\nbetween graph structure and agent representations. Experiments on multiple\nbenchmarks demonstrate GraphGeo significantly outperforms state-of-the-art\nmethods. Our framework transforms cognitive conflicts between agents into\nenhanced geo-localization accuracy through structured debate.", "AI": {"tldr": "本文提出GraphGeo，一个基于异构图神经网络的多智能体辩论框架，用于视觉地理定位。它通过建模不同类型的智能体交互和引入双层辩论机制，有效处理冲突预测并显著优于现有方法。", "motivation": "视觉地理定位需要丰富的地理知识和复杂推理，但传统检索方法受限于数据库覆盖和质量。虽然LVLM能直接推理，但单个模型难以应对多样地理区域和复杂场景。现有多智能体系统虽通过协作提升性能，但对所有智能体交互一视同仁，缺乏有效处理冲突预测的机制。", "method": "本文提出GraphGeo框架，利用异构图神经网络构建多智能体辩论系统。该方法通过类型化边（支持性协作、竞争性论证、知识转移）建模多样化的辩论关系。它引入双层辩论机制，结合节点级细化和边级论证建模。此外，采用跨层拓扑细化策略，实现图结构和智能体表示的共同演化。", "result": "在多个基准测试上进行的实验表明，GraphGeo显著优于现有最先进的方法。", "conclusion": "GraphGeo框架通过结构化辩论，将智能体之间的认知冲突转化为增强的地理定位准确性。"}}
{"id": "2511.01643", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.2.4; I.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2511.01643", "abs": "https://arxiv.org/abs/2511.01643", "authors": ["Riccardo Campi", "Nicolò Oreste Pinciroli Vago", "Mathyas Giudici", "Pablo Barrachina Rodriguez-Guisado", "Marco Brambilla", "Piero Fraternali"], "title": "A Graph-based RAG for Energy Efficiency Question Answering", "comment": null, "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a\ngraph-based Retrieval Augmented Generation (RAG) architecture for Energy\nEfficiency (EE) Question Answering. First, the system automatically extracts a\nKnowledge Graph (KG) from guidance and regulatory documents in the energy\nfield. Then, the generated graph is navigated and reasoned upon to provide\nusers with accurate answers in multiple languages. We implement a human-based\nvalidation using the RAGAs framework properties, a validation dataset\ncomprising 101 question-answer pairs, and domain experts. Results confirm the\npotential of this architecture and identify its strengths and weaknesses.\nValidation results show how the system correctly answers in about three out of\nfour of the cases (75.2 +- 2.7%), with higher results on questions related to\nmore general EE answers (up to 81.0 +- 4.1%), and featuring promising\nmultilingual abilities (4.4% accuracy loss due to translation).", "AI": {"tldr": "该研究探讨了在能源效率问答中使用基于图的检索增强生成（RAG）架构与大型语言模型（LLMs），通过自动知识图谱（KG）提取、导航和推理，实现多语言准确回答，并经过专家验证，展示了其潜力。", "motivation": "旨在利用大型语言模型改进能源效率领域的问答系统，特别是从指导和监管文件中获取准确信息，并提供多语言支持。", "method": "1. 自动从能源领域的指导和监管文件中提取知识图谱（KG）。2. 利用生成的图谱进行导航和推理，以提供准确的多语言答案。3. 采用基于图的检索增强生成（RAG）架构，并结合大型语言模型（LLMs）。4. 使用RAGAs框架属性、包含101个问答对的验证数据集和领域专家进行人工验证。", "result": "系统在约75.2%（±2.7%）的案例中能正确回答问题。对于更通用的能源效率问题，准确率更高，达到81.0%（±4.1%）。系统展现出有前景的多语言能力，翻译导致的准确率损失仅为4.4%。", "conclusion": "该架构在能源效率问答方面具有巨大潜力，其优势和劣势已被识别。验证结果证实了系统在准确性和多语言能力方面的良好表现。"}}
{"id": "2511.01619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01619", "abs": "https://arxiv.org/abs/2511.01619", "authors": ["Nikola Ljubešić", "Peter Rupnik", "Ivan Porupski", "Taja Kuzman Pungeršek"], "title": "ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian", "comment": "Submitted to the LREC 2026 conference; 11 pages, 2 figures, 3 tables", "summary": "ParlaSpeech is a collection of spoken parliamentary corpora currently\nspanning four Slavic languages - Croatian, Czech, Polish and Serbian - all\ntogether 6 thousand hours in size. The corpora were built in an automatic\nfashion from the ParlaMint transcripts and their corresponding metadata, which\nwere aligned to the speech recordings of each corresponding parliament. In this\nrelease of the dataset, each of the corpora is significantly enriched with\nvarious automatic annotation layers. The textual modality of all four corpora\nhas been enriched with linguistic annotations and sentiment predictions.\nSimilar to that, their spoken modality has been automatically enriched with\noccurrences of filled pauses, the most frequent disfluency in typical speech.\nTwo out of the four languages have been additionally enriched with detailed\nword- and grapheme-level alignments, and the automatic annotation of the\nposition of primary stress in multisyllabic words. With these enrichments, the\nusefulness of the underlying corpora has been drastically increased for\ndownstream research across multiple disciplines, which we showcase through an\nanalysis of acoustic correlates of sentiment. All the corpora are made\navailable for download in JSONL and TextGrid formats, as well as for search\nthrough a concordancer.", "AI": {"tldr": "ParlaSpeech是一个多语言（克罗地亚语、捷克语、波兰语、塞尔维亚语）议会口语语料库，总计6千小时，通过自动方式构建并显著丰富了语言学、情感预测、语流不畅（如停顿）等多种自动标注层，极大地提升了其在跨学科研究中的可用性。", "motivation": "研究动机是增加现有议会语料库（ParlaMint转录本和语音记录）的实用性，通过自动添加丰富的标注层，使其能支持多学科的下游研究。", "method": "该研究方法包括：1) 从ParlaMint转录本及其元数据自动构建语料库；2) 将转录本与相应的语音记录对齐；3) 对文本模态进行语言学标注和情感预测；4) 对口语模态自动标注填充停顿（最常见的语流不畅）；5) 对其中两种语言额外进行词级和字素级对齐，并标注多音节词的重音位置；6) 通过分析情感的声学关联来展示语料库的实用性；7) 以JSONL和TextGrid格式提供下载，并提供一致性检索工具。", "result": "成果是一个包含克罗地亚语、捷克语、波兰语和塞尔维亚语在内的6千小时议会口语语料库，其文本模态和口语模态均显著丰富了自动标注（如语言学标注、情感预测、填充停顿），其中两种语言还额外提供了词级/字素级对齐和重音标注。这些丰富使得语料库在多个学科的下游研究中实用性大增，并通过情感的声学关联分析得到了展示。", "conclusion": "ParlaSpeech语料库的创建和丰富，为多语言议会口语研究提供了一个极具价值的资源，其多层次的自动标注极大地促进了跨学科研究，并为情感的声学关联等分析提供了基础。"}}
{"id": "2511.01307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01307", "abs": "https://arxiv.org/abs/2511.01307", "authors": ["Tae-Young Lee", "Juwon Seo", "Jong Hwan Ko", "Gyeong-Moon Park"], "title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models", "comment": "26 pages, 9 figures, 16 tables, NeurIPS 2025", "summary": "Recent advances in diffusion models have enabled high-quality synthesis of\nspecific subjects, such as identities or objects. This capability, while\nunlocking new possibilities in content creation, also introduces significant\nprivacy risks, as personalization techniques can be misused by malicious users\nto generate unauthorized content. Although several studies have attempted to\ncounter this by generating adversarially perturbed samples designed to disrupt\npersonalization, they rely on unrealistic assumptions and become ineffective in\nthe presence of even a few clean images or under simple image transformations.\nTo address these challenges, we shift the protection target from the images to\nthe diffusion model itself to hinder the personalization of specific subjects,\nthrough our novel framework called Anti-Personalized Diffusion Models (APDM).\nWe first provide a theoretical analysis demonstrating that a naive approach of\nexisting loss functions to diffusion models is inherently incapable of ensuring\nconvergence for robust anti-personalization. Motivated by this finding, we\nintroduce Direct Protective Optimization (DPO), a novel loss function that\neffectively disrupts subject personalization in the target model without\ncompromising generative quality. Moreover, we propose a new dual-path\noptimization strategy, coined Learning to Protect (L2P). By alternating between\npersonalization and protection paths, L2P simulates future personalization\ntrajectories and adaptively reinforces protection at each step. Experimental\nresults demonstrate that our framework outperforms existing methods, achieving\nstate-of-the-art performance in preventing unauthorized personalization. The\ncode is available at https://github.com/KU-VGI/APDM.", "AI": {"tldr": "本文提出了一种名为APDM（Anti-Personalized Diffusion Models）的新框架，通过直接修改扩散模型本身来阻止特定主体的个性化生成，以应对现有保护方法的局限性。", "motivation": "扩散模型在特定主体（如身份或物体）的高质量合成方面取得了显著进展，但也带来了严重的隐私风险，因为个性化技术可能被恶意用户滥用以生成未经授权的内容。现有的对抗性扰动样本方法依赖于不切实际的假设，并且在少量干净图像或简单图像变换下就会失效。", "method": "该研究将保护目标从图像转移到扩散模型本身。首先，通过理论分析表明现有损失函数无法确保鲁棒的反个性化收敛。受此启发，提出了Direct Protective Optimization (DPO) 新型损失函数，旨在有效干扰目标模型中的主体个性化而不损害生成质量。此外，引入了Learning to Protect (L2P) 双路径优化策略，通过在个性化和保护路径之间交替，模拟未来的个性化轨迹并自适应地强化每一步的保护。", "result": "实验结果表明，该框架优于现有方法，在阻止未经授权的个性化方面取得了最先进的性能。", "conclusion": "APDM框架通过新颖的损失函数DPO和双路径优化策略L2P，成功地将保护目标转移到扩散模型本身，有效阻止了特定主体的个性化，同时保持了生成质量，从而解决了扩散模型带来的隐私风险。"}}
{"id": "2511.01649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01649", "abs": "https://arxiv.org/abs/2511.01649", "authors": ["Hung-Shin Lee", "Chen-Chi Chang", "Ching-Yuan Chen", "Yun-Hsiang Hsu"], "title": "Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation", "comment": "This paper has been accepted by The Electronic Library, and the full\n  article is now available on Emerald Insight", "summary": "This study proposes a cognitive benchmarking framework to evaluate how large\nlanguage models (LLMs) process and apply culturally specific knowledge. The\nframework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)\nto assess model performance across six hierarchical cognitive domains:\nRemembering, Understanding, Applying, Analyzing, Evaluating, and Creating.\nUsing a curated Taiwanese Hakka digital cultural archive as the primary\ntestbed, the evaluation measures LLM-generated responses' semantic accuracy and\ncultural relevance.", "AI": {"tldr": "本研究提出一个认知基准框架，结合布鲁姆分类法和RAG，评估大型语言模型处理和应用特定文化知识的能力。", "motivation": "评估大型语言模型如何处理和应用特定文化知识，特别是在语义准确性和文化相关性方面的表现。", "method": "提出一个认知基准框架，该框架整合了布鲁姆认知分类法（六个认知领域：记忆、理解、应用、分析、评估、创造）与检索增强生成（RAG）。使用策展的台湾客家数字文化档案作为主要测试平台，评估模型生成回复的语义准确性和文化相关性。", "result": "该框架能够衡量大型语言模型在六个层级认知领域中对特定文化知识的语义准确性和文化相关性表现。", "conclusion": "本研究提供了一个评估大型语言模型处理和应用特定文化知识能力的综合框架，能够衡量其在不同认知层面的表现。"}}
{"id": "2511.01284", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01284", "abs": "https://arxiv.org/abs/2511.01284", "authors": ["Karma Phuntsho", "Abdullah", "Kyungmi Lee", "Ickjai Lee", "Euijoon Ahn"], "title": "Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions", "comment": null, "summary": "Foundation models (FMs) have emerged as a transformative paradigm in medical\nimage analysis, offering the potential to provide generalizable, task-agnostic\nsolutions across a wide range of clinical tasks and imaging modalities. Their\ncapacity to learn transferable representations from large-scale data has the\npotential to address the limitations of conventional task-specific models.\nHowever, adaptation of FMs to real-world clinical practice remains constrained\nby key challenges, including domain shifts, limited availability of\nhigh-quality annotated data, substantial computational demands, and strict\nprivacy requirements. This review presents a comprehensive assessment of\nstrategies for adapting FMs to the specific demands of medical imaging. We\nexamine approaches such as supervised fine-tuning, domain-specific pretraining,\nparameter-efficient fine-tuning, self-supervised learning, hybrid methods, and\nmultimodal or cross-modal frameworks. For each, we evaluate reported\nperformance gains, clinical applicability, and limitations, while identifying\ntrade-offs and unresolved challenges that prior reviews have often overlooked.\nBeyond these established techniques, we also highlight emerging directions\naimed at addressing current gaps. These include continual learning to enable\ndynamic deployment, federated and privacy-preserving approaches to safeguard\nsensitive data, hybrid self-supervised learning to enhance data efficiency,\ndata-centric pipelines that combine synthetic generation with human-in-the-loop\nvalidation, and systematic benchmarking to assess robust generalization under\nreal-world clinical variability. By outlining these strategies and associated\nresearch gaps, this review provides a roadmap for developing adaptive,\ntrustworthy, and clinically integrated FMs capable of meeting the demands of\nreal-world medical imaging.", "AI": {"tldr": "该综述全面评估了将基础模型（FMs）应用于医学图像分析的适应策略，讨论了现有方法的优缺点和新兴方向，旨在为开发适应性强、值得信赖的医学影像FM提供路线图。", "motivation": "基础模型在医学图像分析中展现出巨大潜力，但其在实际临床应用中面临领域漂移、高质量标注数据稀缺、计算需求高和严格隐私要求等挑战，限制了其广泛应用。因此，需要探讨如何有效适应这些模型。", "method": "该综述审查并评估了多种适应策略，包括监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态或跨模态框架。此外，还重点介绍了新兴方向，如持续学习、联邦和隐私保护方法、混合自监督学习、以数据为中心的流程（结合合成数据生成与人工验证）以及系统基准测试。", "result": "对于每种策略，综述评估了其性能提升、临床适用性、局限性、权衡和未解决的挑战。它识别了现有方法的不足，并强调了旨在解决这些差距的新兴研究方向，以实现动态部署、数据保护、数据效率提升、数据质量保障和鲁棒泛化能力。", "conclusion": "该综述通过概述适应策略和相关研究空白，为开发能够满足真实世界医学影像需求、具有适应性、值得信赖且能与临床实践深度融合的基础模型提供了清晰的路线图。"}}
{"id": "2511.00925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00925", "abs": "https://arxiv.org/abs/2511.00925", "authors": ["Hanwen Su", "Ge Song", "Jiyan Wang", "Yuanbo Zhu"], "title": "Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval", "comment": null, "summary": "The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved\nincreasing attention due to its wide applications, e.g. e-commerce. Despite\nprogress made in this field, previous works suffer from using imbalanced\nsamples of modalities and inconsistent low-quality information during training,\nresulting in sub-optimal performance. Therefore, in this paper, we introduce an\napproach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It\nconsists of three components: (i) a Uni-modal Feature Extraction Module that\nincludes a CLIP text encoder and a ViT for extracting textual and visual\ntokens, (ii) a Cross-modal Multi-level Weighting Module that produces an\nalignment weight list by the local and global aggregation blocks to measure the\naligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss\nModule aiming to improve the balance of domains in the triplet loss.\nExperiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and\nQuickDraw, show our method delivers superior performances over the\nstate-of-the-art ZS-SBIR methods.", "AI": {"tldr": "本文提出了一种名为动态多级加权对齐网络（DMWAN）的方法，用于零样本草图图像检索（ZS-SBIR），通过解决模态样本不平衡和训练中低质量信息不一致的问题，显著提升了检索性能。", "motivation": "以往的零样本草图图像检索（ZS-SBIR）方法存在模态样本不平衡和训练过程中低质量信息不一致的问题，导致性能不佳。", "method": "本文引入了一种动态多级加权对齐网络（DMWAN），它包含三个主要组件：1) 单模态特征提取模块，使用CLIP文本编码器和ViT提取文本和视觉token；2) 跨模态多级加权模块，通过局部和全局聚合块生成对齐权重列表，以衡量草图和图像样本的对齐质量；3) 加权四元组损失模块，旨在改善三元组损失中域的平衡性。", "result": "在Sketchy、TU-Berlin和QuickDraw三个基准数据集上的实验结果表明，本文提出的方法在ZS-SBIR任务中优于现有的最先进方法。", "conclusion": "所提出的动态多级加权对齐网络（DMWAN）有效解决了ZS-SBIR中模态样本不平衡和低质量信息的问题，并在多个基准数据集上取得了优异的性能。"}}
{"id": "2511.01237", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01237", "abs": "https://arxiv.org/abs/2511.01237", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Eyes on Target: Gaze-Aware Object Detection in Egocentric Video", "comment": "Accepted at RAAI 2025", "summary": "Human gaze offers rich supervisory signals for understanding visual attention\nin complex visual environments. In this paper, we propose Eyes on Target, a\nnovel depth-aware and gaze-guided object detection framework designed for\negocentric videos. Our approach injects gaze-derived features into the\nattention mechanism of a Vision Transformer (ViT), effectively biasing spatial\nfeature selection toward human-attended regions. Unlike traditional object\ndetectors that treat all regions equally, our method emphasises\nviewer-prioritised areas to enhance object detection. We validate our method on\nan egocentric simulator dataset where human visual attention is critical for\ntask assessment, illustrating its potential in evaluating human performance in\nsimulation scenarios. We evaluate the effectiveness of our gaze-integrated\nmodel through extensive experiments and ablation studies, demonstrating\nconsistent gains in detection accuracy over gaze-agnostic baselines on both the\ncustom simulator dataset and public benchmarks, including Ego4D Ego-Motion and\nEgo-CH-Gaze datasets. To interpret model behaviour, we also introduce a\ngaze-aware attention head importance metric, revealing how gaze cues modulate\ntransformer attention dynamics.", "AI": {"tldr": "本文提出了一种名为“Eyes on Target”的深度感知、凝视引导目标检测框架，用于第一视角视频。该框架将凝视特征注入Vision Transformer的注意力机制，以偏向人类关注区域，从而提高目标检测精度，并在多个数据集上取得了显著效果。", "motivation": "人类凝视提供了理解复杂视觉环境中视觉注意力的丰富监督信号。传统目标检测器平等对待所有区域，而本文旨在利用凝视信息，优先处理观看者关注的区域，以增强目标检测。", "method": "本文提出了“Eyes on Target”框架，将凝视派生特征注入Vision Transformer (ViT) 的注意力机制中，有效地使空间特征选择偏向人类关注的区域。此外，还引入了一个凝视感知注意力头重要性度量，以解释模型行为。", "result": "该方法在定制的第一视角模拟器数据集和公共基准（包括Ego4D Ego-Motion和Ego-CH-Gaze数据集）上，相比于无凝视基线，在检测精度上取得了持续的提升。凝视感知注意力头重要性度量也揭示了凝视线索如何调节Transformer的注意力动态。", "conclusion": "将人类凝视信息整合到深度感知目标检测框架中，可以显著提高第一视角视频中的目标检测性能，尤其是在人类视觉注意力对任务评估至关重要的场景中。该方法还提供了一种解释模型如何利用凝视线索的机制。"}}
{"id": "2511.01357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01357", "abs": "https://arxiv.org/abs/2511.01357", "authors": ["Qiangguo Jin", "Xianyao Zheng", "Hui Cui", "Changming Sun", "Yuqi Fang", "Cong Cong", "Ran Su", "Leyi Wei", "Ping Xuan", "Junbo Wang"], "title": "CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering", "comment": "The paper has been accepted by the 33rd Pacific Conference on\n  Computer Graphics and Applications (Pacific Graphics 2025)", "summary": "Medical visual question answering (Med-VQA) is a crucial multimodal task in\nclinical decision support and telemedicine. Recent self-attention based methods\nstruggle to effectively handle cross-modal semantic alignments between vision\nand language. Moreover, classification-based methods rely on predefined answer\nsets. Treating this task as a simple classification problem may make it unable\nto adapt to the diversity of free-form answers and overlook the detailed\nsemantic information of free-form answers. In order to tackle these challenges,\nwe introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)\nframework that learns cross-modal feature representations from images and\ntexts. CMI-MTL comprises three key modules: fine-grained visual-text feature\nalignment (FVTA), cross-modal interleaved feature representation (CIFR), and\nfree-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most\nrelevant regions in image-text pairs through fine-grained visual-text feature\nalignment. CIFR captures cross-modal sequential interactions via cross-modal\ninterleaved feature representation. FFAE leverages auxiliary knowledge from\nopen-ended questions through free-form answer-enhanced multi-task learning,\nimproving the model's capability for open-ended Med-VQA. Experimental results\nshow that CMI-MTL outperforms the existing state-of-the-art methods on three\nMed-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more\ninterpretability experiments to prove the effectiveness. The code is publicly\navailable at https://github.com/BioMedIA-repo/CMI-MTL.", "AI": {"tldr": "本文提出了一种基于Cross-Mamba交互的多任务学习（CMI-MTL）框架，以解决医学视觉问答（Med-VQA）中跨模态语义对齐的挑战以及自由形式答案的多样性问题，并在三个数据集上实现了最先进的性能。", "motivation": "当前的自注意力方法在处理视觉和语言之间的跨模态语义对齐方面效果不佳。此外，基于分类的方法依赖于预定义的答案集，难以适应自由形式答案的多样性，并可能忽略其详细的语义信息。", "method": "本文引入了一个名为CMI-MTL的框架，它通过以下三个关键模块学习图像和文本的跨模态特征表示：1) 细粒度视觉-文本特征对齐（FVTA），用于提取图像-文本对中最相关的区域；2) 跨模态交错特征表示（CIFR），用于通过跨模态交错特征表示捕获跨模态序列交互；3) 自由形式答案增强多任务学习（FFAE），利用开放式问题的辅助知识来增强模型处理开放式Med-VQA的能力。", "result": "实验结果表明，CMI-MTL在VQA-RAD、SLAKE和OVQA这三个Med-VQA数据集上均优于现有的最先进方法。此外，通过可解释性实验进一步证明了其有效性。", "conclusion": "CMI-MTL框架通过细粒度特征对齐、跨模态交互和自由形式答案增强的多任务学习，有效解决了Med-VQA中跨模态语义对齐和自由形式答案处理的难题，显著提升了模型性能和开放式问答能力。"}}
{"id": "2511.01390", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.01390", "abs": "https://arxiv.org/abs/2511.01390", "authors": ["Xinyu Mao", "Junsi Li", "Haoji Zhang", "Yu Liang", "Ming Sun"], "title": "SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment", "comment": null, "summary": "Fine-grained cross-modal alignment aims to establish precise local\ncorrespondences between vision and language, forming a cornerstone for visual\nquestion answering and related multimodal applications. Current approaches face\nchallenges in addressing patch redundancy and ambiguity, which arise from the\ninherent information density disparities across modalities. Recently,\nMultimodal Large Language Models (MLLMs) have emerged as promising solutions to\nbridge this gap through their robust semantic generation capabilities. However,\nthe dense textual outputs from MLLMs may introduce conflicts with the original\nsparse captions. Furthermore, accurately quantifying semantic relevance between\nrich visual patches and concise textual descriptions remains a core challenge.\nTo overcome these limitations, we introduce the Semantic-Enhanced Patch\nSlimming (SEPS) framework, which systematically addresses patch redundancy and\nambiguity. Our approach employs a two-stage mechanism to integrate unified\nsemantics from both dense and sparse texts, enabling the identification of\nsalient visual patches. Additionally, it leverages relevance-aware selection\nwith mean value computation to highlight crucial patch-word correspondences,\nthereby improving cross-modal similarity assessment. Comprehensive experiments\non Flickr30K and MS-COCO datasets validate that SEPS achieves superior\nperformance, surpassing existing approaches by 23\\%-86\\% in rSum across diverse\nmodel architectures, with notable enhancements in text-to-image retrieval\nscenarios. Our implementation is available at\nhttps://github.com/Sweet4tars/seps.git.", "AI": {"tldr": "本文提出SEPS框架，通过语义增强和补丁精简，有效解决细粒度跨模态对齐中的冗余和歧义问题，显著提升了检索性能。", "motivation": "细粒度跨模态对齐在VQA等应用中面临补丁冗余和歧义挑战，源于模态间信息密度差异。尽管MLLMs有潜力，但其密集文本输出可能与稀疏原始描述冲突，且视觉补丁与文本描述间的语义关联量化仍是难题。", "method": "引入语义增强补丁精简（SEPS）框架。采用两阶段机制整合来自密集和稀疏文本的统一语义，以识别显著视觉补丁。此外，利用基于均值计算的相关性感知选择，突出关键的补丁-词对应关系，从而改善跨模态相似性评估。", "result": "SEPS在Flickr30K和MS-COCO数据集上表现出色，在多种模型架构下，rSum性能超越现有方法23%-86%，尤其在文本到图像检索场景中表现显著提升。", "conclusion": "SEPS框架通过系统性地解决补丁冗余和歧义，有效整合多源语义信息，并优化补丁-词对应关系，显著提升了细粒度跨模态对齐的性能，尤其在文本到图像检索任务中表现优异。"}}
{"id": "2511.01650", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01650", "abs": "https://arxiv.org/abs/2511.01650", "authors": ["Ayesha Gull", "Muhammad Usman Safder", "Rania Elbadry", "Preslav Nakov", "Zhuohan Xie"], "title": "EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering", "comment": "24 pages, includes figures and tables; introduces the EngChain\n  benchmark", "summary": "Large Language Models (LLMs) are increasingly being applied to specialized,\nhigh-stakes domains like engineering, which demands rigorous evaluation of\ntheir complex reasoning capabilities. While current benchmarks assess language\nunderstanding, factual recall, mathematics or code generation, none capture the\nintegrative reasoning central to engineering where scientific principles,\nquantitative modeling and practical constraints must converge. To address this\ngap, we introduce EngChain, a benchmark for verifiable multi-step engineering\nproblem-solving. EngChain contains 90 problems spanning three engineering\nbranches, organized into 9 domains and 20 distinct areas. The problems are\ngenerated from symbolic templates with a high degree of randomization to ensure\ndiversity and eliminate the risk of contamination. With this benchmark, we move\nbeyond final answer accuracy with a two-stage evaluation: we first\nquantitatively verify the numerical and semantic validity of each reasoning\nstep and then introduce LLM-As-A-Judge, an automated system to qualitatively\ncategorize the identified reasoning errors.", "AI": {"tldr": "本文介绍了EngChain，一个用于评估大型语言模型（LLMs）在工程领域多步骤、可验证推理能力的基准测试，通过两阶段评估方法超越了最终答案准确性。", "motivation": "当前LLMs基准测试未能捕捉工程领域所需的科学原理、定量建模和实际约束相结合的综合推理能力，因此需要一个能严格评估其复杂推理能力的工具。", "method": "引入EngChain基准，包含90个跨三个工程分支的问题，分为9个领域和20个不同区域。问题通过符号模板高度随机生成，以确保多样性并消除污染风险。评估采用两阶段：首先定量验证每个推理步骤的数值和语义有效性，然后引入“LLM-As-A-Judge”系统定性分类识别出的推理错误。", "result": "创建了EngChain基准，包含90个问题，涵盖三个工程分支、9个领域和20个独特区域。这些问题通过符号模板高度随机生成，确保了多样性。同时，开发了两阶段评估方法，包括对推理步骤的定量验证和使用LLM-As-A-Judge进行定性错误分类。", "conclusion": "EngChain基准填补了当前LLM评估的空白，为严格评估LLMs在工程领域中复杂的、多步骤的、可验证的推理能力提供了一个新工具，超越了传统的最终答案准确性评估。"}}
{"id": "2511.01670", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01670", "abs": "https://arxiv.org/abs/2511.01670", "authors": ["Chaoqun Liu", "Mahani Aljunied", "Guizhen Chen", "Hou Pong Chan", "Weiwen Xu", "Yu Rong", "Wenxuan Zhang"], "title": "SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia", "comment": "10 pages", "summary": "We introduce SeaLLMs-Audio, the first large audio-language model (LALM)\ntailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai\n(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a\nlarge-scale audio corpus, SeaLLMs-Audio exhibits strong performance across\ndiverse audio-centric tasks, spanning fine-grained audio understanding and\nvoice-based interaction. Its key features include: 1) Multilingual: the model\nprimarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,\nand Chinese; 2) Multimodal: the model accepts flexible input modalities,\nincluding audio only, text only, as well as audio with text; 3) Multi-task: the\nmodel supports a wide range of tasks, including audio analysis tasks such as\nAudio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,\nSpeech Emotion Recognition, Speech Question Answering, and Speech\nSummarization. It also enables voice-based dialogue, including answering\nfactual, mathematical, and general knowledge queries. As a significant step\ntowards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to\nbenefit both the regional research community and industry. To automate LALM\nevaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark\nspanning multiple tasks. Experiments show that SeaLLMs-Audio achieves\ncompetitive performance compared with other LALMs on SEA languages.", "AI": {"tldr": "本文介绍了SeaLLMs-Audio，首个针对东南亚多语言（印尼语、泰语、越南语）以及英语和中文的大型音频语言模型（LALM），它支持多语言、多模态和多任务功能。同时引入了SeaBench-Audio，一个用于自动化评估东南亚LALM的基准。", "motivation": "研究动机是为了填补东南亚语言在大型音频语言模型（LALM）领域的空白，并推动该地区音频LLM的发展，以服务于区域研究社区和工业界。", "method": "该研究引入了SeaLLMs-Audio模型，它在一个大规模音频语料库上训练，主要支持印尼语、泰语、越南语、英语和中文五种语言。模型接受灵活的输入模态（纯音频、纯文本、音频加文本），并支持广泛的任务，包括音频理解（如音频字幕、自动语音识别、语音到文本翻译、语音情感识别、语音问答、语音摘要）和基于语音的对话。此外，还引入了SeaBench-Audio基准来自动化评估东南亚LALM。", "result": "SeaLLMs-Audio在各种以音频为中心的任务中表现出强大的性能，包括细粒度音频理解和基于语音的交互。实验表明，在东南亚语言上，SeaLLMs-Audio与现有LALM相比，取得了具有竞争力的性能。", "conclusion": "SeaLLMs-Audio是推动东南亚音频LLM发展的重要一步，预计将使该区域的研究社区和工业界受益。SeaBench-Audio的引入也为东南亚LALM的自动化评估提供了工具。"}}
{"id": "2511.01427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01427", "abs": "https://arxiv.org/abs/2511.01427", "authors": ["Yinchao Ma", "Yuyang Tang", "Wenfei Yang", "Tianzhu Zhang", "Xu Zhou", "Feng Wu"], "title": "UniSOT: A Unified Framework for Multi-Modality Single Object Tracking", "comment": "The paper has been accepted by TPAMI", "summary": "Single object tracking aims to localize target object with specific reference\nmodalities (bounding box, natural language or both) in a sequence of specific\nvideo modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different\nreference modalities enable various human-machine interactions, and different\nvideo modalities are demanded in complex scenarios to enhance tracking\nrobustness. Existing trackers are designed for single or several video\nmodalities with single or several reference modalities, which leads to separate\nmodel designs and limits practical applications. Practically, a unified tracker\nis needed to handle various requirements. To the best of our knowledge, there\nis still no tracker that can perform tracking with these above reference\nmodalities across these video modalities simultaneously. Thus, in this paper,\nwe present a unified tracker, UniSOT, for different combinations of three\nreference modalities and four video modalities with uniform parameters.\nExtensive experimental results on 18 visual tracking, vision-language tracking\nand RGB+X tracking benchmarks demonstrate that UniSOT shows superior\nperformance against modality-specific counterparts. Notably, UniSOT outperforms\nprevious counterparts by over 3.0\\% AUC on TNL2K across all three reference\nmodalities and outperforms Un-Track by over 2.0\\% main metric across all three\nRGB+X video modalities.", "AI": {"tldr": "本文提出UniSOT，一个统一的单目标跟踪器，能够同时处理多种参考模态（边界框、自然语言或两者）和多种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件），并在18个基准测试上表现优异。", "motivation": "现有跟踪器通常只针对单一或少数视频/参考模态设计，导致模型分离且限制了实际应用。实际场景需要一个能处理各种需求的统一跟踪器，但目前尚无能同时处理所有这些参考和视频模态的跟踪器。", "method": "研究者设计了一个名为UniSOT的统一跟踪器，它能够以统一的参数处理三种参考模态和四种视频模态的不同组合。", "result": "UniSOT在18个视觉跟踪、视觉-语言跟踪和RGB+X跟踪基准测试上展示了优于特定模态对应方法的性能。具体而言，UniSOT在TNL2K上所有三种参考模态的AUC得分比现有方法高出3.0%以上，并在所有三种RGB+X视频模态的主要指标上比Un-Track高出2.0%以上。", "conclusion": "UniSOT是一个高效的统一跟踪器，能够应对各种参考模态和视频模态的组合，显著优于现有特定模态的跟踪器，解决了当前单目标跟踪领域缺乏通用解决方案的问题。"}}
{"id": "2511.01689", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01689", "abs": "https://arxiv.org/abs/2511.01689", "authors": ["Sharan Maiya", "Henning Bartsch", "Nathan Lambert", "Evan Hubinger"], "title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI", "comment": "12 pages, 6 figures, 4 tables", "summary": "The character of the \"AI assistant\" persona generated by modern chatbot large\nlanguage models influences both surface-level behavior and apparent values,\nbeliefs, and ethics. These all affect interaction quality, perceived\nintelligence, and alignment with both developer and user intentions. The\nshaping of this persona, known as character training, is a critical component\nof industry post-training, yet remains effectively unstudied in the academic\nliterature. We introduce the first open implementation of character training,\nleveraging Constitutional AI and a new data pipeline using synthetic\nintrospective data to shape the assistant persona in a more effective and\ncontrolled manner than alternatives such as constraining system prompts or\nactivation steering. Specifically, we fine-tune three popular open-weights\nmodels using 11 example personas, such as humorous, deeply caring, or even\nmalevolent. To track the effects of our approach, we introduce a method which\nanalyzes revealed preferences, uncovering clear and holistic changes in\ncharacter. We find these changes are more robust to adversarial prompting than\nthe above two alternatives, while also leading to more coherent and realistic\ngenerations. Finally, we demonstrate this fine-tuning has little to no effect\non general capabilities as measured by common benchmarks. We describe and\nopen-source our full post-training method, the implementation of which can be\nfound at https://github.com/maiush/OpenCharacterTraining.", "AI": {"tldr": "本文介绍了首个开源的“角色训练”方法，利用宪法式AI和合成内省数据，有效且可控地塑造大型语言模型（LLM）的AI助手人格，并证明其在鲁棒性和生成连贯性方面优于现有替代方案，且不影响模型通用能力。", "motivation": "AI助手的人格特性影响交互质量、感知智能以及与开发者和用户意图的对齐。尽管“角色训练”是行业后期训练的关键组成部分，但在学术界仍未得到有效研究。现有塑造人格的方法（如系统提示或激活引导）可能不够鲁棒或有效。", "method": "研究引入了首个开源的“角色训练”实现，结合了宪法式AI（Constitutional AI）和一种新的数据管道，该管道使用合成内省数据来塑造助手人格。具体地，他们使用11种示例人格（如幽默、关怀或恶意）对三个流行的开源模型进行微调。为了跟踪效果，他们引入了一种分析“揭示偏好”的方法，以发现角色中清晰而整体的变化。", "result": "研究发现，通过其方法塑造的角色变化对对抗性提示更具鲁棒性，优于系统提示或激活引导等替代方案。此外，这种方法还能产生更连贯和真实的生成内容。最后，通过通用基准测试，他们证明这种微调对模型的通用能力几乎没有影响。", "conclusion": "角色训练是一种有效且可控地塑造LLM助手人格的方法，其实现方式比现有替代方案更具鲁棒性，能产生更连贯的生成，且不损害模型的通用能力。该研究为学术界提供了首个开源的角色训练实现。"}}
{"id": "2511.01706", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01706", "abs": "https://arxiv.org/abs/2511.01706", "authors": ["Sekh Mainul Islam", "Pepa Atanasova", "Isabelle Augenstein"], "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "comment": "Under review", "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement.", "AI": {"tldr": "本研究提出了一种新颖的秩-2投影子空间，用于更准确地解耦大型语言模型（LLMs）自然语言解释（NLEs）中参数知识（PK）和上下文知识（CK）的贡献，并首次进行了多步知识交互分析。", "motivation": "理解NLEs中外部上下文知识（CK）和模型内部参数知识（PK）的交互对于评估NLEs的可靠性至关重要，但这一领域尚未得到充分探索。现有工作主要关注单步生成，并以二元选择的秩-1子空间建模PK和CK交互，这忽略了更丰富的知识交互形式（如互补或支持）。", "method": "本文提出了一种新颖的秩-2投影子空间，能够更准确地解耦PK和CK的贡献。利用此方法，研究人员首次对NLEs序列中的多步知识交互进行了分析。实验在四个问答数据集和三个开源指令微调LLM上进行。", "result": "实验结果表明，秩-1子空间未能很好地表示多样化的知识交互，而秩-2公式能够有效捕获这些交互。多步分析揭示，幻觉NLEs强烈偏向PK方向，忠实于上下文的NLEs平衡PK和CK，而用于NLEs的思维链（Chain-of-Thought）提示通过减少对PK的依赖，将生成的NLEs推向CK方向。", "conclusion": "本工作通过一个更丰富的秩-2子空间解耦方法，为LLMs中多步知识交互的系统研究提供了第一个框架。"}}
{"id": "2511.00916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00916", "abs": "https://arxiv.org/abs/2511.00916", "authors": ["Yan Shu", "Chi Liu", "Robin Chen", "Derek Li", "Bryan Dai"], "title": "Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\neffectiveness in various general-domain scenarios, such as visual question\nanswering and image captioning. Recently, researchers have increasingly focused\non empowering MLLMs with medical conversational abilities, which hold\nsignificant promise for clinical applications. However, medical data presents\nunique challenges due to its heterogeneous nature -- encompassing diverse\nmodalities including 2D images, 3D volumetric scans, and temporal video\nsequences. The substantial domain gap and data format inconsistencies across\nthese modalities have hindered the development of unified medical MLLMs. To\naddress these challenges, we propose Fleming-VL, a unified end-to-end framework\nfor comprehensive medical visual understanding across heterogeneous modalities.\nFleming-VL tackles this problem from a data-centric perspective through three\nkey strategies: (1) scaling up pretraining by integrating long-context data\nfrom both natural and medical-specific domains; (2) complementing fine-tuning\nwith rare medical data, including holistic video analysis and underrepresented\n2D modalities such as ultrasound and dermoscopy images; (3) extending existing\nevaluation frameworks to incorporate 3D volumetric and video understanding\nbenchmarks. Through supervised fine-tuning (SFT) and group relative policy\noptimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive\nexperiments demonstrate that Fleming-VL achieves state-of-the-art performance\nacross multiple benchmarks, including medical VQA, video QA, and 3D medical\nimage understanding. We publicly release Fleming-VL to promote transparent,\nreproducible, and auditable progress in medical AI.", "AI": {"tldr": "Fleming-VL是一个统一的端到端框架，通过数据中心策略解决了多模态大语言模型在异构医学数据（2D、3D、视频）理解上的挑战，并在多项基准测试中达到了SOTA性能。", "motivation": "多模态大语言模型（MLLMs）在医疗对话中潜力巨大，但异构的医学数据（2D图像、3D体数据、视频序列）以及显著的领域差距和数据格式不一致性，阻碍了统一医学MLLM的发展。", "method": "Fleming-VL采用数据中心视角，通过三项关键策略解决问题：1) 整合自然和医学领域的长上下文数据来扩大预训练规模；2) 利用稀有医学数据（包括整体视频分析和超声、皮肤镜等代表性不足的2D模态）补充微调；3) 扩展现有评估框架以纳入3D体数据和视频理解基准。通过监督微调（SFT）和组相对策略优化（GRPO），开发了不同模型规模的Fleming-VL。", "result": "广泛的实验表明，Fleming-VL在多个基准测试中实现了最先进的性能，包括医学VQA、视频QA和3D医学图像理解。", "conclusion": "Fleming-VL成功地为异构模态的全面医学视觉理解提供了一个统一框架，并公开发布以促进医学AI的透明、可复现和可审计进展。"}}
{"id": "2511.00956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00956", "abs": "https://arxiv.org/abs/2511.00956", "authors": ["Liuzhuozheng Li", "Yue Gong", "Shanyuan Liu", "Bo Cheng", "Yuhang Ma", "Liebucha Wu", "Dengyang Jiang", "Zanyi Wang", "Dawei Leng", "Yuhui Yin"], "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference", "comment": null, "summary": "We propose EVTAR, an End-to-End Virtual Try-on model with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages, human pose, densepose, or body keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts a\ntwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks, densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preserve\ngarment texture and fine-grained details better. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwith supplementary references and unpaired person images to support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.", "AI": {"tldr": "本文提出了EVTAR，一个端到端的虚拟试穿模型，通过引入额外参考图像来提高试穿准确性，并简化了传统方法中复杂的输入要求。", "motivation": "大多数现有虚拟试穿方法依赖于复杂的输入（如无关人物图像、人体姿态、densepose或身体关键点），导致其劳动密集且不适用于实际应用。", "method": "EVTAR采用两阶段训练策略，推理时仅需源图像和目标服装作为输入，无需mask、densepose或分割图。它利用穿着相同服装的不同个体的额外参考图像，以更好地保留服装纹理和精细细节。此外，模型通过补充参考和未配对人物图像丰富了训练数据。", "result": "EVTAR在两个广泛使用的基准和多样化任务上进行了评估，结果一致验证了其方法的有效性。", "conclusion": "EVTAR通过简化输入、引入额外参考图像和两阶段训练，实现了更准确、更实用的虚拟试穿效果，并能更好地保留服装细节。"}}
{"id": "2511.01449", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01449", "abs": "https://arxiv.org/abs/2511.01449", "authors": ["Riddhi Jain", "Manasi Patwardhan", "Aayush Mishra", "Parijat Deshpande", "Beena Rai"], "title": "Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction", "comment": "9 pages, 1 figure, 4 tables", "summary": "To effectively manage the wastage of perishable fruits, it is crucial to\naccurately predict their freshness or shelf life using non-invasive methods\nthat rely on visual data. In this regard, deep learning techniques can offer a\nviable solution. However, obtaining fine-grained fruit freshness labels from\nexperts is costly, leading to a scarcity of data. Closed proprietary Vision\nLanguage Models (VLMs), such as Gemini, have demonstrated strong performance in\nfruit freshness detection task in both zero-shot and few-shot settings.\nNonetheless, food retail organizations are unable to utilize these proprietary\nmodels due to concerns related to data privacy, while existing open-source VLMs\nyield sub-optimal performance for the task. Fine-tuning these open-source\nmodels with limited data fails to achieve the performance levels of proprietary\nmodels. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning\n(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes\nmeta-learning to address data sparsity and leverages label ordinality, thereby\nachieving state-of-the-art performance in the fruit freshness classification\ntask under both zero-shot and few-shot settings. Our method achieves an\nindustry-standard accuracy of 92.71%, averaged across all fruits.\n  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,\nOrdinal Regression", "AI": {"tldr": "针对易腐水果保鲜期预测中数据稀缺和专有模型隐私问题，本文提出了一种模型无关的序数元学习（MAOML）算法，用于训练小型视觉语言模型（VLMs）。该方法结合元学习和标签序数性，在零样本和少样本设置下，实现了水果新鲜度分类的最新水平（平均准确率92.71%）。", "motivation": "有效管理易腐水果的损耗，需要准确、无创地预测其新鲜度。深度学习在视觉数据分析方面有潜力，但专家标注细粒度新鲜度标签成本高昂，导致数据稀缺。专有视觉语言模型（如Gemini）在此任务上表现出色，但食品零售组织因数据隐私问题无法使用。现有开源VLM性能不佳，且在有限数据下微调也无法达到专有模型的水平。", "method": "本文引入了一种模型无关的序数元学习（MAOML）算法。该算法旨在训练较小的视觉语言模型（VLMs），通过元学习解决数据稀疏性问题，并利用标签的序数性（ordinality）来提高性能。", "result": "该方法在零样本和少样本设置下的水果新鲜度分类任务中，实现了最先进的性能。在所有水果上的平均准确率达到行业标准的92.71%。", "conclusion": "所提出的MAOML算法通过结合元学习和标签序数性，有效解决了水果新鲜度预测中数据稀疏性和开源VLM性能不足的问题。它在零样本和少样本场景下均达到最先进的准确率，为食品零售组织提供了一个高性能且无数据隐私顾虑的解决方案。"}}
{"id": "2511.01450", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01450", "abs": "https://arxiv.org/abs/2511.01450", "authors": ["Jie Du", "Xinyu Gong", "Qingshan Tan", "Wen Li", "Yangming Cheng", "Weitao Wang", "Chenlu Zhan", "Suhui Wu", "Hao Zhang", "Jun Zhang"], "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation", "comment": null, "summary": "Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO objective to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.", "AI": {"tldr": "本文提出了一种改进视频生成DPO方法，通过GT-Pair自动构建高质量偏好对，Reg-DPO引入SFT损失稳定训练，并结合内存优化技术提升训练容量，从而显著提高了视频生成质量。", "motivation": "现有的视频生成DPO方法大多沿用图像领域的范式，且主要在小规模模型上开发，面临数据构建成本高、训练不稳定和内存消耗大等独特挑战。", "method": ["GT-Pair：自动构建高质量偏好对，使用真实视频作为正例，模型生成视频作为负例，无需外部标注。", "Reg-DPO：将SFT损失作为正则项引入DPO目标函数，以增强训练稳定性和生成保真度。", "内存优化：将FSDP框架与多种内存优化技术结合，实现比单独使用FSDP高近三倍的训练容量。"], "result": "在I2V和T2V任务的多个数据集上，本文方法始终优于现有方法，提供了卓越的视频生成质量，并实现了近三倍的训练容量提升。", "conclusion": "通过引入GT-Pair、Reg-DPO和内存优化技术，本文成功克服了视频生成DPO的挑战，显著提升了生成质量和训练效率。"}}
{"id": "2511.01000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01000", "abs": "https://arxiv.org/abs/2511.01000", "authors": ["Hassan Ugail", "Ismail Lujain Jaleel"], "title": "Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya", "comment": null, "summary": "Art authentication of Francisco Goya's works presents complex computational\nchallenges due to his heterogeneous stylistic evolution and extensive\nhistorical patterns of forgery. We introduce a novel multimodal machine\nlearning framework that applies identical feature extraction techniques to both\nvisual and X-ray radiographic images of Goya paintings. The unified feature\nextraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,\nLocal Binary Patterns, entropy measures, energy calculations, and colour\ndistribution analysis applied consistently across both imaging modalities. The\nextracted features from both visual and X-ray images are processed through an\noptimised One-Class Support Vector Machine with hyperparameter tuning. Using a\ndataset of 24 authenticated Goya paintings with corresponding X-ray images,\nsplit into an 80/20 train-test configuration with 10-fold cross-validation, the\nframework achieves 97.8% classification accuracy with a 0.022 false positive\nrate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy\nof our pipeline, achieving 92.3% authentication confidence through unified\nmultimodal feature analysis. Our results indicate substantial performance\nimprovement over single-modal approaches, establishing the effectiveness of\napplying identical computational methods to both visual and radiographic\nimagery in art authentication applications.", "AI": {"tldr": "本文提出了一种新颖的多模态机器学习框架，通过对戈雅画作的视觉和X射线图像应用相同的特征提取技术，并结合优化的一类支持向量机，实现了高精度的艺术品鉴定，显著优于单模态方法。", "motivation": "戈雅作品的艺术鉴定面临巨大计算挑战，原因在于他多变的风格演变和广泛的历史伪造模式。", "method": "该研究引入了一个多模态机器学习框架，对戈雅画作的视觉和X射线图像应用相同的特征提取技术。统一的特征提取流程包括灰度共生矩阵（GLCM）描述符、局部二值模式（LBP）、熵度量、能量计算和颜色分布分析。提取的特征通过经过超参数调优的优化一类支持向量机（OC-SVM）进行处理。数据集包含24幅经鉴定为真迹的戈雅画作及其对应的X射线图像，采用80/20的训练-测试分割和10折交叉验证。", "result": "该框架实现了97.8%的分类准确率，假阳性率为0.022。对“Un Gigante”的案例研究表明，通过统一的多模态特征分析，鉴定置信度达到92.3%。结果显示，与单模态方法相比，性能有显著提升。", "conclusion": "研究结果表明，在艺术品鉴定应用中，对视觉和射线照相图像应用相同的计算方法是有效的，并且能带来实质性的性能改进。"}}
{"id": "2511.00997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00997", "abs": "https://arxiv.org/abs/2511.00997", "authors": ["Chang Nie", "Tianchen Deng", "Zhe Liu", "Hesheng Wang"], "title": "MID: A Self-supervised Multimodal Iterative Denoising Framework", "comment": null, "summary": "Data denoising is a persistent challenge across scientific and engineering\ndomains. Real-world data is frequently corrupted by complex, non-linear noise,\nrendering traditional rule-based denoising methods inadequate. To overcome\nthese obstacles, we propose a novel self-supervised multimodal iterative\ndenoising (MID) framework. MID models the collected noisy data as a state\nwithin a continuous process of non-linear noise accumulation. By iteratively\nintroducing further noise, MID learns two neural networks: one to estimate the\ncurrent noise step and another to predict and subtract the corresponding noise\nincrement. For complex non-linear contamination, MID employs a first-order\nTaylor expansion to locally linearize the noise process, enabling effective\niterative removal. Crucially, MID does not require paired clean-noisy datasets,\nas it learns noise characteristics directly from the noisy inputs. Experiments\nacross four classic computer vision tasks demonstrate MID's robustness,\nadaptability, and consistent state-of-the-art performance. Moreover, MID\nexhibits strong performance and adaptability in tasks within the biomedical and\nbioinformatics domains.", "AI": {"tldr": "本文提出一种新颖的自监督多模态迭代去噪（MID）框架，通过迭代引入噪声并利用两个神经网络和泰勒展开，直接从噪声数据中学习并去除复杂非线性噪声，在计算机视觉、生物医学和生物信息学等多个领域取得了最先进的性能。", "motivation": "现实世界数据经常受到复杂、非线性噪声的污染，使得传统的基于规则的去噪方法无法有效应对。", "method": "MID将收集到的噪声数据建模为非线性噪声累积连续过程中的一个状态。它通过迭代引入更多噪声，学习两个神经网络：一个用于估计当前噪声步长，另一个用于预测并减去相应的噪声增量。对于复杂的非线性污染，MID采用一阶泰勒展开来局部线性化噪声过程，从而实现有效的迭代去除。该方法无需成对的干净-噪声数据集，直接从噪声输入中学习噪声特性。", "result": "实验结果表明，MID在四个经典的计算机视觉任务中展现出鲁棒性、适应性和持续的最先进性能。此外，MID在生物医学和生物信息学领域的任务中也表现出强大的性能和适应性。", "conclusion": "MID框架能够有效处理复杂非线性噪声，具有强大的鲁棒性和适应性，并且无需干净数据进行训练，在多个科学和工程领域达到了最先进的去噪效果。"}}
{"id": "2511.01720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01720", "abs": "https://arxiv.org/abs/2511.01720", "authors": ["Mahammad Nuriyev"], "title": "Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue", "comment": "10 pages, 1 figure, 2 tables. Technical report for the Commonsense\n  Persona-Grounded Dialogue Challenge (CPDC) 2025, part of the Wordplay 2025\n  Workshop @ EMNLP 2025", "summary": "We present a multi-expert system for creating Non-Player Characters (NPCs)\ncapable of both natural dialogue and contextual action execution in interactive\nenvironments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)\nadapters, we instantiate three specialists: tool calling, tool-response\ninterpretation, and direct dialogue. Our system comfortably meets the\ncomputational efficiency requirements, delivering fast responses and\nmaintaining modest resource usage on L40S GPUs. In the Commonsense\nPersona-Grounded Dialogue Challenge 2025, our method ranked second overall.\n  Code available at:\nhttps://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/", "AI": {"tldr": "该论文提出一个多专家系统，用于创建能进行自然对话和执行情境动作的NPC，该系统计算效率高且在某挑战赛中排名第二。", "motivation": "旨在开发能够进行自然对话并在交互环境中执行情境动作的非玩家角色（NPC），同时满足计算效率要求。", "method": "使用Qwen3作为基础模型，结合LoRA适配器实例化了三个专家：工具调用、工具响应解释和直接对话，构建了一个多专家系统。", "result": "该系统满足计算效率要求，在L40S GPU上响应速度快且资源占用适中。在2025年常识角色导向对话挑战赛中，该方法获得了总分第二名。", "conclusion": "所提出的多专家系统能有效且高效地使NPC实现自然对话和情境动作，并在竞争性挑战中表现出色。"}}
{"id": "2511.00981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00981", "abs": "https://arxiv.org/abs/2511.00981", "authors": ["Suzhong Fu", "Rui Sun", "Xuan Ding", "Jingqi Dong", "Yiming Yang", "Yao Zhu", "Min Chang Jordan Ren", "Delin Deng", "Angelica Aviles-Rivero", "Shuguang Cui", "Zhen Li"], "title": "VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel", "comment": null, "summary": "Accurate vessel segmentation is critical for clinical applications such as\ndisease diagnosis and surgical planning, yet remains challenging due to thin,\nbranching structures and low texture contrast. While foundation models like the\nSegment Anything Model (SAM) have shown promise in generic segmentation, they\nperform sub-optimally on vascular structures. In this work, we present VesSAM,\na powerful and efficient framework tailored for 2D vessel segmentation. VesSAM\nintegrates (1) a convolutional adapter to enhance local texture features, (2) a\nmulti-prompt encoder that fuses anatomical prompts, including skeletons,\nbifurcation points, and segment midpoints, via hierarchical cross-attention,\nand (3) a lightweight mask decoder to reduce jagged artifacts. We also\nintroduce an automated pipeline to generate structured multi-prompt\nannotations, and curate a diverse benchmark dataset spanning 8 datasets across\n5 imaging modalities. Experimental results demonstrate that VesSAM consistently\noutperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%\nIoU, and achieves competitive performance compared to fully fine-tuned methods,\nwith significantly fewer parameters. VesSAM also generalizes well to\nout-of-distribution (OoD) settings, outperforming all baselines in average OoD\nDice and IoU.", "AI": {"tldr": "VesSAM是一个专为2D血管分割设计的强大高效框架，通过集成卷积适配器、多提示编码器和轻量级掩码解码器，显著优于现有的SAM变体，并在参数量更少的情况下，与完全微调方法表现相当，且泛化能力强。", "motivation": "血管的薄、分支结构和低纹理对比度使得准确的血管分割在临床应用中极具挑战性。虽然基础模型如Segment Anything Model (SAM) 在通用分割方面表现出色，但在血管结构上表现不佳。", "method": "VesSAM框架包含三个主要组件：1) 一个卷积适配器，用于增强局部纹理特征；2) 一个多提示编码器，通过分层交叉注意力融合骨架、分叉点和段中点等解剖学提示；3) 一个轻量级掩码解码器，以减少锯齿状伪影。此外，该研究还引入了自动化流程来生成结构化多提示标注，并整理了一个包含8个数据集和5种成像模态的基准数据集。", "result": "实验结果表明，VesSAM在Dice和IoU指标上均比最先进的基于PEFT的SAM变体高出10%和13%以上。与完全微调方法相比，VesSAM在参数量显著更少的情况下，仍能达到具有竞争力的性能。VesSAM还表现出良好的域外(OoD)泛化能力，在平均OoD Dice和IoU方面均优于所有基线模型。", "conclusion": "VesSAM是一个强大且高效的2D血管分割框架，通过专门的设计，显著提升了血管分割的性能和泛化能力，且参数量更少，为临床应用提供了更优的解决方案。"}}
{"id": "2511.01462", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01462", "abs": "https://arxiv.org/abs/2511.01462", "authors": ["Peng Xia", "Junbiao Pang", "Tianyang Cai"], "title": "Efficiently Training A Flat Neural Network Before It has been Quantizated", "comment": "ongoing work, more results would be added", "summary": "Post-training quantization (PTQ) for vision transformers (ViTs) has garnered\nsignificant attention due to its efficiency in compressing models. However,\nexisting methods typically overlook the relationship between a well-trained NN\nand the quantized model, leading to considerable quantization error for PTQ.\nHowever, it is unclear how to efficiently train a model-agnostic neural network\nwhich is tailored for a predefined precision low-bit model. In this paper, we\nfirstly discover that a flat full precision neural network is crucial for\nlow-bit quantization. To achieve this, we propose a framework that proactively\npre-conditions the model by measuring and disentangling the error sources.\nSpecifically, both the Activation Quantization Error (AQE) and the Weight\nQuantization Error (WQE) are statistically modeled as independent Gaussian\nnoises. We study several noise injection optimization methods to obtain a flat\nminimum. Experimental results attest to the effectiveness of our approach.\nThese results open novel pathways for obtaining low-bit PTQ models.", "AI": {"tldr": "本文发现平坦的全精度神经网络对低比特量化至关重要。为解决ViT后训练量化(PTQ)中全精度与量化模型关系被忽视导致的误差问题，作者提出了一个框架，通过将激活和权重量化误差建模为独立高斯噪声，并利用噪声注入优化方法获得平坦最小值，从而有效地对模型进行预处理，显著提高了PTQ性能。", "motivation": "现有的ViT后训练量化(PTQ)方法通常忽略了良好训练的全精度神经网络与量化模型之间的关系，导致显著的量化误差。此外，如何高效训练一个模型无关且适用于预定义低比特模型的神经网络尚不明确。", "method": "1. 发现平坦的全精度神经网络对低比特量化至关重要。2. 提出了一个框架，通过测量和分离误差源来主动预处理模型。3. 将激活量化误差(AQE)和权重 量化误差(WQE)统计建模为独立的高斯噪声。4. 研究并应用了几种噪声注入优化方法以获得平坦的最小值。", "result": "实验结果证明了所提出方法的有效性。", "conclusion": "该研究为获得低比特PTQ模型开辟了新的途径。"}}
{"id": "2511.01805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01805", "abs": "https://arxiv.org/abs/2511.01805", "authors": ["Jiayi Geng", "Howard Chen", "Ryan Liu", "Manoel Horta Ribeiro", "Robb Willer", "Graham Neubig", "Thomas L. Griffiths"], "title": "Accumulating Context Changes the Beliefs of Language Models", "comment": null, "summary": "Language model (LM) assistants are increasingly used in applications such as\nbrainstorming and research. Improvements in memory and context size have\nallowed these models to become more autonomous, which has also resulted in more\ntext accumulation in their context windows without explicit user intervention.\nThis comes with a latent risk: the belief profiles of models -- their\nunderstanding of the world as manifested in their responses or actions -- may\nsilently change as context accumulates. This can lead to subtly inconsistent\nuser experiences, or shifts in behavior that deviate from the original\nalignment of the models. In this paper, we explore how accumulating context by\nengaging in interactions and processing text -- talking and reading -- can\nchange the beliefs of language models, as manifested in their responses and\nbehaviors.Our results reveal that models' belief profiles are highly malleable:\nGPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of\ndiscussion about moral dilemmas and queries about safety, while Grok 4 shows a\n27.2% shift on political issues after reading texts from the opposing position.\nWe also examine models' behavioral changes by designing tasks that require tool\nuse, where each tool selection corresponds to an implicit belief. We find that\nthese changes align with stated belief shifts, suggesting that belief shifts\nwill be reflected in actual behavior in agentic systems. Our analysis exposes\nthe hidden risk of belief shift as models undergo extended sessions of talking\nor reading, rendering their opinions and actions unreliable.", "AI": {"tldr": "研究发现，语言模型（如GPT-5和Grok 4）的信念会随着上下文的累积（对话或阅读）而发生显著且隐性的变化，这会影响其响应和行为的可靠性。", "motivation": "语言模型助手在应用中积累的上下文越来越多，且无需用户明确干预。这带来了一个潜在风险：模型的信念配置文件（对世界的理解）可能会随着上下文的积累而悄然改变，导致用户体验不一致或行为偏离模型原始对齐。", "method": "研究通过让语言模型进行互动（“交谈”）和处理文本（“阅读”）来累积上下文。通过观察模型在道德困境、安全查询和政治问题上的响应来测量其信念变化。此外，设计了需要使用工具的任务，其中每个工具选择对应一个隐含信念，以检查模型的行为变化。", "result": "结果显示模型的信念配置文件具有高度可塑性：GPT-5在10轮关于道德困境和安全问题的讨论后，其公开表达的信念发生了54.7%的转变。Grok 4在阅读了持相反立场的文本后，其在政治问题上的信念发生了27.2%的转变。模型行为上的变化与其公开表达的信念转变一致，表明信念转变将反映在代理系统中的实际行为中。", "conclusion": "研究揭示了模型在长时间的对话或阅读会话中，信念会发生隐性转变的风险，这使得它们的观点和行动变得不可靠，尤其是在代理系统中。"}}
{"id": "2511.01458", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01458", "abs": "https://arxiv.org/abs/2511.01458", "authors": ["Dennis Pierantozzi", "Luca Carlini", "Mauro Orazio Drago", "Chiara Lena", "Cesare Hassan", "Elena De Momi", "Danail Stoyanov", "Sophia Bano", "Mobarak I. Hoque"], "title": "When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA", "comment": null, "summary": "Safety and reliability are essential for deploying Visual Question Answering\n(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.\nMost surgical VQA research focuses on accuracy or linguistic quality while\noverlooking safety behaviors such as ambiguity awareness, referral to human\nexperts, or triggering a second opinion. Inspired by Automatic Failure\nDetection (AFD), we study uncertainty estimation as a key enabler of safer\ndecision making. We introduce Question Aligned Semantic Nearest Neighbor\nEntropy (QA-SNNE), a black box uncertainty estimator that incorporates question\nsemantics into prediction confidence. It measures semantic entropy by comparing\ngenerated answers with nearest neighbors in a medical text embedding space,\nconditioned on the question. We evaluate five models, including domain specific\nParameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large\nVision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models\ndegrade under mild paraphrasing, while LVLMs are more resilient. Across three\nLVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template\nsettings and enhances hallucination detection. The Area Under the ROC Curve\n(AUROC) increases by 15-38% for zero-shot models, with gains maintained under\nout-of-template stress. QA-SNNE offers a practical and interpretable step\ntoward AFD in surgical VQA by linking semantic uncertainty to question context.\nCombining LVLM backbones with question aligned uncertainty estimation can\nimprove safety and clinician trust. The code and model are available at\nhttps://github.com/DennisPierantozzi/QASNNE", "AI": {"tldr": "该研究引入了一种名为QA-SNNE的黑盒不确定性估计器，通过结合问题语义来提高外科视觉问答（VQA）系统的安全性，特别是在幻觉检测方面，从而增强临床信任。", "motivation": "外科VQA系统中，不正确或模棱两可的回答可能危害患者，因此安全性和可靠性至关重要。然而，大多数外科VQA研究侧重于准确性或语言质量，而忽视了模糊意识、寻求专家意见或触发二次审查等安全行为。", "method": "研究受到自动故障检测（AFD）的启发，将不确定性估计作为实现更安全决策的关键。提出了一种名为“问题对齐语义最近邻熵”（QA-SNNE）的黑盒不确定性估计器，它将问题语义融入预测置信度。QA-SNNE通过在医学文本嵌入空间中，根据问题上下文，比较生成答案与最近邻答案来衡量语义熵。研究在EndoVis18-VQA和PitVQA数据集上评估了五种模型，包括领域特定的参数高效微调（PEFT）模型和零样本大视觉语言模型（LVLMs）。", "result": "PEFT模型在轻微改写下性能下降，而LVLMs更具弹性。在三种LVLMs和两种PEFT基线上，QA-SNNE在大多数模板内设置中提高了AUROC，并增强了幻觉检测能力。零样本模型的AUROC增加了15-38%，且在模板外压力测试下增益仍保持。QA-SNNE通过将语义不确定性与问题上下文相关联，为外科VQA中的AFD提供了一个实用且可解释的步骤。", "conclusion": "结合LVLM骨干模型与问题对齐的不确定性估计（如QA-SNNE）可以显著提高外科VQA系统的安全性并增强临床医生对其的信任。QA-SNNE是实现外科VQA自动故障检测的实用且可解释的一步。"}}
{"id": "2511.00962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00962", "abs": "https://arxiv.org/abs/2511.00962", "authors": ["Dongheng Lin", "Mengxue Qu", "Kunyang Han", "Jianbo Jiao", "Xiaojie Jin", "Yunchao Wei"], "title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis", "comment": "NeurIPS 2025 poster", "summary": "Most video-anomaly research stops at frame-wise detection, offering little\ninsight into why an event is abnormal, typically outputting only frame-wise\nanomaly scores without spatial or semantic context. Recent video anomaly\nlocalization and video anomaly understanding methods improve explainability but\nremain data-dependent and task-specific. We propose a unified reasoning\nframework that bridges the gap between temporal detection, spatial\nlocalization, and textual explanation. Our approach is built upon a chained\ntest-time reasoning process that sequentially connects these tasks, enabling\nholistic zero-shot anomaly analysis without any additional training.\nSpecifically, our approach leverages intra-task reasoning to refine temporal\ndetections and inter-task chaining for spatial and semantic understanding,\nyielding improved interpretability and generalization in a fully zero-shot\nmanner. Without any additional data or gradients, our method achieves\nstate-of-the-art zero-shot performance across multiple video anomaly detection,\nlocalization, and explanation benchmarks. The results demonstrate that careful\nprompt design with task-wise chaining can unlock the reasoning power of\nfoundation models, enabling practical, interpretable video anomaly analysis in\na fully zero-shot manner. Project Page:\nhttps://rathgrith.github.io/Unified_Frame_VAA/.", "AI": {"tldr": "本文提出一个统一的零样本推理框架，通过链式测试时推理过程，整合了视频异常检测、空间定位和文本解释，实现了无需额外训练的全面异常分析，并在多个基准测试中达到了最先进的零样本性能。", "motivation": "大多数视频异常研究止步于帧级检测，缺乏对异常原因的深入理解，通常只输出帧级异常分数，没有空间或语义上下文。现有的视频异常定位和理解方法虽然提高了可解释性，但仍依赖于数据且针对特定任务。", "method": "本文提出了一个统一的推理框架，通过链式测试时推理过程，顺序连接了时间检测、空间定位和文本解释任务，实现了全面的零样本异常分析。该方法利用任务内推理来细化时间检测，并通过任务间链式连接实现空间和语义理解，从而在完全零样本的情况下提高可解释性和泛化能力。具体而言，它通过精心设计的提示词和任务链式连接来利用基础模型的推理能力。", "result": "在没有任何额外数据或梯度的情况下，本文方法在多个视频异常检测、定位和解释基准测试中取得了最先进的零样本性能。结果表明，通过精心设计的提示词和任务链式连接，可以释放基础模型的推理能力，实现实用且可解释的零样本视频异常分析。", "conclusion": "精心设计的提示词和任务链式连接能够解锁基础模型的推理能力，从而实现实用、可解释的零样本视频异常分析。本方法提供了一个统一的框架，弥合了时间检测、空间定位和文本解释之间的鸿沟，无需额外训练即可进行全面的零样本异常分析。"}}
{"id": "2511.01026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01026", "abs": "https://arxiv.org/abs/2511.01026", "authors": ["JunXi Yuan"], "title": "FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning", "comment": "17pages , 10figures , 12tables", "summary": "We present FastBoost, a parameter-efficient neural architecture that achieves\nstate-of-the-art performance on CIFAR benchmarks through a novel Dynamically\nScaled Progressive Attention (DSPA) mechanism. Our design establishes new\nefficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and\n93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and\n74.85% (0.44M parameters) The breakthrough stems from three fundamental\ninnovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention\nblending with dynamic weights. (2) Phase Scaling: Training-stage-aware\nintensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized\nskip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced\nMBConv blocks, FastBoost achieves a 2.1 times parameter reduction over\nMobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The\narchitecture features dual attention pathways with real-time weight adjustment,\ncascaded refinement layers (increasing gradient flow by 12.7%), and a\nhardware-friendly design (0.28G FLOPs). This co-optimization of dynamic\nattention and efficient convolution operations demonstrates unprecedented\nparameter-accuracy trade-offs, enabling deployment in resource-constrained edge\ndevices without accuracy degradation.", "AI": {"tldr": "FastBoost是一种参数高效的神经网络架构，通过新颖的动态缩放渐进式注意力（DSPA）机制，在CIFAR基准测试上实现了最先进的性能，同时显著减少了参数量。", "motivation": "研究旨在开发一种在保持高准确率的同时，具有极高参数效率的神经网络架构，以适应资源受限的边缘设备的部署需求。", "method": "本文提出了FastBoost架构，其核心是动态缩放渐进式注意力（DSPA）机制。DSPA包含三项创新：1) 自适应融合（学习通道-空间注意力动态权重混合），2) 阶段缩放（训练阶段感知的强度调制），以及3) 残差自适应（自优化跳跃连接）。FastBoost将DSPA与增强型MBConv块结合，并采用双注意力路径、级联细化层和硬件友好型设计。", "result": "FastBoost在CIFAR-10上达到了95.57%准确率（0.85M参数）和93.80%准确率（0.37M参数），在CIFAR-100上达到了81.37%准确率（0.92M参数）和74.85%准确率（0.44M参数）。与MobileNetV3相比，FastBoost在CIFAR-10上参数减少了2.1倍，准确率提高了3.2个百分点，同时具有0.28G FLOPs和12.7%的梯度流增加。", "conclusion": "FastBoost通过动态注意力与高效卷积操作的协同优化，实现了前所未有的参数-准确率权衡，使得模型能够在不牺牲准确率的情况下，成功部署到资源受限的边缘设备上。"}}
{"id": "2511.01815", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01815", "abs": "https://arxiv.org/abs/2511.01815", "authors": ["Konrad Staniszewski", "Adrian Łańcucki"], "title": "KV Cache Transform Coding for Compact Storage in LLM Inference", "comment": null, "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches.", "AI": {"tldr": "KVTC是一种轻量级变换编码器，用于高效压缩大型语言模型（LLM）的KV缓存，以实现内存高效的服务，同时保持推理和长上下文准确性。", "motivation": "大规模服务LLM需要高效的KV缓存管理。虽然共享前缀提示允许KV缓存重用，但过时或未使用的缓存会消耗稀缺的GPU内存，导致需要卸载或重新计算，这限制了LLM的扩展能力。", "method": "KVTC是一种变换编码器，它结合了经典媒体压缩技术：基于PCA的特征去相关、自适应量化和熵编码。它只需要简短的初始校准，并且不改变模型参数。", "result": "KVTC在保持推理和长上下文准确性的前提下，实现了高达20倍的压缩比，在特定用例中甚至达到40倍或更高。它在Llama 3、Mistral NeMo和R1-Qwen 2.5等模型上，通过多项基准测试，始终优于令牌驱逐、量化和基于SVD等推理时基线方法，并获得了更高的压缩率。", "conclusion": "KVTC被证明是内存高效LLM服务中一个实用的构建模块，尤其适用于可重用KV缓存的场景，因为它能有效压缩KV缓存，同时保持模型性能。"}}
{"id": "2511.01079", "categories": ["cs.CV", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.01079", "abs": "https://arxiv.org/abs/2511.01079", "authors": ["Nikolay I. Kalmykov", "Razan Dibo", "Kaiyu Shen", "Xu Zhonghan", "Anh-Huy Phan", "Yipeng Liu", "Ivan Oseledets"], "title": "T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression", "comment": "Submitted to Information Systems. Code will be released upon journal\n  publication", "summary": "Neural image compression (NIC) has become the state-of-the-art for\nrate-distortion performance, yet its security vulnerabilities remain\nsignificantly less understood than those of classifiers. Existing adversarial\nattacks on NICs are often naive adaptations of pixel-space methods, overlooking\nthe unique, structured nature of the compression pipeline. In this work, we\npropose a more advanced class of vulnerabilities by introducing T-MLA, the\nfirst targeted multiscale log--exponential attack framework. Our approach\ncrafts adversarial perturbations in the wavelet domain by directly targeting\nthe quality of the attacked and reconstructed images. This allows for a\nprincipled, offline attack where perturbations are strategically confined to\nspecific wavelet subbands, maximizing distortion while ensuring perceptual\nstealth. Extensive evaluation across multiple state-of-the-art NIC\narchitectures on standard image compression benchmarks reveals a large drop in\nreconstruction quality while the perturbations remain visually imperceptible.\nOur findings reveal a critical security flaw at the core of generative and\ncontent delivery pipelines.", "AI": {"tldr": "本文提出了一种名为T-MLA的新型多尺度对数-指数攻击框架，该框架通过在小波域中生成对抗性扰动，能够对神经图像压缩（NIC）系统发起隐蔽且高效的攻击，导致图像重建质量显著下降。", "motivation": "神经图像压缩（NIC）在率失真性能方面已达到最先进水平，但其安全漏洞远不如分类器那样被充分理解。现有的针对NIC的对抗性攻击往往是像素空间方法的简单改编，忽略了压缩管道的独特结构。", "method": "本文提出了T-MLA，这是第一个目标多尺度对数-指数攻击框架。该方法通过直接针对被攻击和重建图像的质量，在小波域中精心设计对抗性扰动。这种攻击是一种原则性的离线攻击，扰动被策略性地限制在特定的小波子带中，以最大化失真同时确保视觉上的不可察觉性。", "result": "在多个最先进的NIC架构和标准图像压缩基准上进行的广泛评估显示，图像重建质量大幅下降，而扰动在视觉上仍然是不可察觉的。", "conclusion": "研究结果揭示了生成式和内容分发管道核心存在的关键安全漏洞。"}}
{"id": "2511.01013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01013", "abs": "https://arxiv.org/abs/2511.01013", "authors": ["Mohammad Amanour Rahman"], "title": "HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images", "comment": "This manuscript has been submitted to Informatics in Medicine\n  Unlocked", "summary": "B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,\noperator dependency, and indistinct boundaries. Existing deep learning suffers\nfrom single-task learning, architectural constraints (CNNs lack global context,\nTransformers local features), and black-box decision-making. These gaps hinder\nclinical adoption.\n  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous\nsegmentation and classification with intrinsic interpretability. Its\ndual-branch encoder integrates EfficientNet-B3 and Swin Transformer via\nmulti-scale hierarchical fusion blocks. An attention-gated decoder provides\nprecision and explainability. We introduce dual-pipeline interpretability: (1)\nintrinsic attention validation with quantitative IoU verification (mean: 0.86),\nand (2) Grad-CAM for classification reasoning.\n  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and\naccuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant\nRecall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling\nyields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant\nRecall, eliminating false negatives. Ablation studies confirm multi-scale\nfusion contributes +16.8% Dice and attention gates add +5.9%.\n  Crucially, we conduct the first cross-dataset generalization study for hybrid\nCNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),\nconfirming domain shift. However, progressive fine-tuning with only 10%\ntarget-domain data (68 images) recovers 92.5% performance. With 50% data, our\nmodel achieves 77.3% Dice, exceeding source-domain performance (76.1%) and\ndemonstrating true generalization.", "AI": {"tldr": "本文提出HyFormer-Net，一种混合CNN-Transformer模型，用于乳腺超声图像的同步分割和分类，具有内在可解释性，并在BUSI数据集上表现出色，同时通过渐进式微调展示了跨数据集的泛化能力。", "motivation": "B型超声乳腺癌诊断面临散斑、操作员依赖和边界不清等挑战。现有深度学习方法存在单任务学习、架构限制（CNN缺乏全局上下文，Transformer缺乏局部特征）以及黑盒决策等问题，这些都阻碍了其临床应用。", "method": "本文提出HyFormer-Net，一个混合CNN-Transformer模型，用于同步分割和分类，并具有内在可解释性。其双分支编码器通过多尺度分层融合块整合了EfficientNet-B3和Swin Transformer。一个注意力门控解码器提供了精确性和可解释性。引入了双通道可解释性：(1) 通过IoU验证的内在注意力验证（平均IoU: 0.86），以及(2) 用于分类推理的Grad-CAM。", "result": "在BUSI数据集上，HyFormer-Net的Dice分数达到0.761，准确率93.2%，优于U-Net、Attention U-Net和TransUNet。恶性召回率达到92.1%，确保了最小的假阴性。集成模型实现了卓越的Dice分数90.2%、准确率99.5%和100%的恶性召回率，消除了假阴性。消融研究证实多尺度融合贡献了+16.8%的Dice分数，注意力门控增加了+5.9%。首次进行的混合CNN-Transformer在乳腺超声中的跨数据集泛化研究表明，零样本迁移失败（Dice: 0.058），但通过仅10%目标域数据（68张图像）的渐进式微调，性能恢复了92.5%；使用50%数据时，模型Dice分数达到77.3%，超过了源域性能（76.1%），证明了其真正的泛化能力。", "conclusion": "HyFormer-Net通过其混合架构和双通道可解释性，有效解决了乳腺超声诊断中的现有挑战和深度学习方法的局限性。它在性能上超越了现有模型，并通过有限数据微调展示了强大的跨数据集泛化能力，为乳腺癌的临床诊断提供了有前景的解决方案。"}}
{"id": "2511.01463", "categories": ["cs.CV", "cs.AI", "cs.GR", "68T45", "I.2.10; I.3.7"], "pdf": "https://arxiv.org/pdf/2511.01463", "abs": "https://arxiv.org/abs/2511.01463", "authors": ["Lei Hu", "Yongjing Ye", "Shihong Xia"], "title": "HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA", "comment": "10 pages, 5figures. The Thirty-Ninth Annual Conference on Neural\n  Information Processing Systems", "summary": "The expansion of instruction-tuning data has enabled foundation language\nmodels to exhibit improved instruction adherence and superior performance\nacross diverse downstream tasks. Semantically-rich 3D human motion is being\nprogressively integrated with these foundation models to enhance multimodal\nunderstanding and cross-modal generation capabilities. However, the modality\ngap between human motion and text raises unresolved concerns about catastrophic\nforgetting during this integration. In addition, developing\nautoregressive-compatible pose representations that preserve generalizability\nacross heterogeneous downstream tasks remains a critical technical barrier. To\naddress these issues, we propose the Human Motion-Vision-Language Model\n(HMVLM), a unified framework based on the Mixture of Expert Low-Rank\nAdaption(MoE LoRA) strategy. The framework leverages the gating network to\ndynamically allocate LoRA expert weights based on the input prompt, enabling\nsynchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting\nduring instruction-tuning, we introduce a novel zero expert that preserves the\npre-trained parameters for general linguistic tasks. For pose representation,\nwe implement body-part-specific tokenization by partitioning the human body\ninto different joint groups, enhancing the spatial resolution of the\nrepresentation. Experiments show that our method effectively alleviates\nknowledge forgetting during instruction-tuning and achieves remarkable\nperformance across diverse human motion downstream tasks.", "AI": {"tldr": "本文提出HMVLM模型，一个基于MoE LoRA的统一框架，用于将3D人体运动与大型语言模型集成，通过动态专家分配和零专家策略解决灾难性遗忘问题，并采用身体部位特定分词增强姿态表示，在多样化人体运动任务中表现出色。", "motivation": "将语义丰富的3D人体运动与基础语言模型结合时，存在模态鸿沟导致的灾难性遗忘问题。此外，开发能够跨异构下游任务保持泛化能力的自回归兼容姿态表示，仍然是一个关键技术障碍。", "method": "本文提出了人体运动-视觉-语言模型（HMVLM），这是一个基于专家混合低秩适应（MoE LoRA）策略的统一框架。该框架利用门控网络根据输入提示动态分配LoRA专家权重，实现多任务同步微调。为缓解指令微调期间的灾难性遗忘，引入了一个创新的“零专家”来保留预训练参数以处理通用语言任务。对于姿态表示，通过将人体划分为不同的关节组，实现了身体部位特定的分词，从而增强了表示的空间分辨率。", "result": "实验表明，所提出的方法有效缓解了指令微调期间的知识遗忘，并在多样化的人体运动下游任务中取得了显著性能。", "conclusion": "HMVLM框架成功地解决了3D人体运动与基础语言模型集成时的灾难性遗忘和姿态表示泛化性问题，通过创新的MoE LoRA策略和身体部位分词，在多种人体运动任务中展现出卓越的性能和泛化能力。"}}
{"id": "2511.01098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01098", "abs": "https://arxiv.org/abs/2511.01098", "authors": ["Veronica Marsico", "Antonio Quintero-Rincon", "Hadj Batatia"], "title": "Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images", "comment": "12 pages, 6 figures, 3 tables", "summary": "This study presents a novel method for diagnosing respiratory diseases using\nimage data. It combines Epanechnikov's non-parametric kernel density estimation\n(EKDE) with a bimodal logistic regression classifier in a\nstatistical-model-based learning scheme. EKDE's flexibility in modeling data\ndistributions without assuming specific shapes and its adaptability to pixel\nintensity variations make it valuable for extracting key features from medical\nimages. The method was tested on 13808 randomly selected chest X-rays from the\nCOVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of\n59.26%, and a specificity of 74.18%, demonstrating moderate performance in\ndetecting respiratory disease while showing room for improvement in\nsensitivity. While clinical expertise remains essential for further refining\nthe model, this study highlights the potential of EKDE-based approaches to\nenhance diagnostic accuracy and reliability in medical imaging.", "AI": {"tldr": "本研究提出了一种结合Epanechnikov核密度估计（EKDE）和双峰逻辑回归的呼吸系统疾病诊断新方法，利用胸部X光图像数据，在COVID-19数据集上取得了中等性能。", "motivation": "开发一种利用图像数据诊断呼吸系统疾病的新方法，利用EKDE在建模数据分布和适应像素强度变化方面的灵活性，从医学图像中提取关键特征。", "method": "该方法在一个基于统计模型的学习方案中，将Epanechnikov非参数核密度估计（EKDE）与双峰逻辑回归分类器相结合。在COVID-19放射学数据集中随机选择了13808张胸部X光片进行测试。", "result": "该方法在检测呼吸系统疾病方面取得了70.14%的准确率、59.26%的敏感性和74.18%的特异性。结果显示性能中等，敏感性有待提高。", "conclusion": "本研究强调了基于EKDE的方法在提高医学影像诊断准确性和可靠性方面的潜力，同时指出临床专业知识对于进一步完善模型仍然至关重要。"}}
{"id": "2511.01541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01541", "abs": "https://arxiv.org/abs/2511.01541", "authors": ["Arthur Hubert", "Gamal Elghazaly", "Raphaël Frank"], "title": "Driving scenario generation and evaluation using a structured layer representation and foundational models", "comment": null, "summary": "Rare and challenging driving scenarios are critical for autonomous vehicle\ndevelopment. Since they are difficult to encounter, simulating or generating\nthem using generative models is a popular approach. Following previous efforts\nto structure driving scenario representations in a layer model, we propose a\nstructured five-layer model to improve the evaluation and generation of rare\nscenarios. We use this model alongside large foundational models to generate\nnew driving scenarios using a data augmentation strategy. Unlike previous\nrepresentations, our structure introduces subclasses and characteristics for\nevery agent of the scenario, allowing us to compare them using an embedding\nspecific to our layer-model. We study and adapt two metrics to evaluate the\nrelevance of a synthetic dataset in the context of a structured representation:\nthe diversity score estimates how different the scenarios of a dataset are from\none another, while the originality score calculates how similar a synthetic\ndataset is from a real reference set. This paper showcases both metrics in\ndifferent generation setup, as well as a qualitative evaluation of synthetic\nvideos generated from structured scenario descriptions. The code and extended\nresults can be found at https://github.com/Valgiz/5LMSG.", "AI": {"tldr": "本文提出一个结构化的五层模型，结合大型基础模型和数据增强策略，用于自动驾驶中稀有场景的生成与评估。该模型引入新的代理子类和特征，并使用多样性分数和原创性分数来衡量生成数据集的质量。", "motivation": "自动驾驶汽车开发中，稀有且具有挑战性的驾驶场景至关重要，但由于难以实际遇到，因此需要通过生成模型进行模拟或创建。", "method": "研究人员提出了一个结构化的五层模型来表示驾驶场景，该模型改进了对稀有场景的评估和生成。他们将此模型与大型基础模型结合，通过数据增强策略生成新的驾驶场景。该结构为场景中的每个代理引入了子类和特征。此外，他们研究并调整了两个评估指标：多样性分数（衡量数据集内场景差异）和原创性分数（衡量合成数据集与真实参考集的相似度），并进行了合成视频的定性评估。", "result": "论文展示了在不同生成设置下这两种评估指标的应用，并对从结构化场景描述生成的合成视频进行了定性评估，表明该方法能够有效地生成和评估稀有驾驶场景。", "conclusion": "该研究提出的结构化五层模型及其伴随的评估指标，能够有效改进稀有驾驶场景的生成和评估，为自动驾驶系统的开发提供了有价值的工具和方法。"}}
{"id": "2511.01846", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01846", "abs": "https://arxiv.org/abs/2511.01846", "authors": ["Thang Luong", "Dawsen Hwang", "Hoang H. Nguyen", "Golnaz Ghiasi", "Yuri Chervonyi", "Insuk Seo", "Junsu Kim", "Garrett Bingham", "Jonathan Lee", "Swaroop Mishra", "Alex Zhai", "Clara Huiyi Hu", "Henryk Michalewski", "Jimin Kim", "Jeonghyun Ahn", "Junhwi Bae", "Xingyou Song", "Trieu H. Trinh", "Quoc V. Le", "Junehyuk Jung"], "title": "Towards Robust Mathematical Reasoning", "comment": "EMNLP 2025 (main conference),\n  https://aclanthology.org/2025.emnlp-main.1794/", "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.", "AI": {"tldr": "本文介绍了IMO-Bench，一套针对国际数学奥林匹克（IMO）级别数学推理能力的基准测试套件，旨在解决现有评估过于简单或仅关注简短答案的问题，并展示了Gemini Deep Think模型在该基准上的卓越表现。", "motivation": "现有基础模型的数学推理能力评估要么过于简单，要么只关注正确简短的答案，这不足以衡量模型真正的数学推理进展。为了推动基础模型在数学推理方面的发展，需要更高级、更全面的评估指标。", "method": "研究人员创建了IMO-Bench，一个由顶尖专家审核的先进推理基准套件，专门针对IMO级别。IMO-Bench包含两部分：IMO-AnswerBench（400道可验证短答案的奥林匹克问题）和IMO-ProofBench（包括基础和高级IMO级别的证明题，并附有详细评分指南以促进自动评分）。此外，他们还构建了与人类评估高度相关的Gemini推理自动评分器，并创建了IMO-GradingBench（包含1000个人工评分的证明），以推动长篇答案的自动评估。", "result": "Gemini Deep Think模型在IMO 2025中取得了金牌级别的表现。它在IMO-AnswerBench上达到80.0%的准确率，在高级IMO-ProofBench上达到65.7%的准确率，分别比其他非Gemini模型的最佳成绩高出6.9%和42.4%。研究还表明，使用Gemini推理构建的自动评分器与人工评估高度相关。", "conclusion": "IMO-Bench为评估和推进基础模型在鲁棒数学推理方面的能力提供了一个关键工具。它不仅解决了现有评估的局限性，还通过其详细的证明评估和自动评分工具，促进了长篇答案自动评估的进一步发展。研究人员希望IMO-Bench能帮助社区共同提升数学推理能力。"}}
{"id": "2511.01807", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01807", "abs": "https://arxiv.org/abs/2511.01807", "authors": ["Adewale Akinfaderin", "Shreyas Subramanian", "Akarsha Sehwag"], "title": "Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining", "comment": "Presented at Workshop on Prompt Optimization, KDD 2025, Toronto,\n  Canada", "summary": "Length control in Large Language Models (LLMs) is a crucial but\nunder-addressed challenge, with applications ranging from voice interfaces\nrequiring concise responses to research summaries needing comprehensive\noutputs. Current approaches to length control, including Regularized DPO,\nLength-Instruction Fine Tuning, and tool-augmented methods, typically require\nexpensive model retraining or complex inference-time tooling. This paper\npresents a prompt engineering methodology that enables precise length control\nwithout model retraining. Our structure-guided approach implements deliberate\nplanning and word counting mechanisms within the prompt, encouraging the model\nto carefully track and adhere to specified length constraints. Comprehensive\nevaluations across six state-of-the-art LLMs demonstrate that our method\nsignificantly improves length fidelity for several models compared to standard\nprompting when applied to document summarization tasks, particularly for\nshorter-to-medium length constraints. The proposed technique shows varying\nbenefits across different model architectures, with some models demonstrating\nup to 37.6% improvement in length adherence. Quality evaluations further reveal\nthat our approach maintains or enhances overall output quality compared to\nstandard prompting techniques. Our approach provides an immediately deployable\nsolution for applications requiring precise length control, particularly\nvaluable for production environments where model retraining is impractical or\ncost-prohibitive.", "AI": {"tldr": "该论文提出了一种无需模型再训练的提示工程方法，通过在提示中引入规划和字数统计机制，显著提高了大型语言模型在文档摘要任务中的长度控制精度和输出质量。", "motivation": "大型语言模型（LLMs）的长度控制是一个关键但未充分解决的挑战。现有方法（如正则化DPO、长度指令微调、工具增强）通常需要昂贵的模型再训练或复杂的推理时工具，这在许多生产环境中是不切实际或成本过高的。", "method": "本文提出了一种结构引导的提示工程方法。该方法通过在提示内部实现深思熟虑的规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度限制，从而实现精确的长度控制，且无需进行模型再训练。", "result": "在对六个最先进的大型语言模型进行的综合评估中，该方法在文档摘要任务中，特别是对于短到中等长度的限制，显著提高了模型的长度忠实度，部分模型长度依从性提升高达37.6%。此外，质量评估显示，与标准提示技术相比，该方法保持或提高了整体输出质量。", "conclusion": "该方法提供了一种可立即部署的解决方案，适用于需要精确长度控制的应用，尤其对于模型再训练不切实际或成本过高的生产环境具有重要价值。"}}
{"id": "2511.01610", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01610", "abs": "https://arxiv.org/abs/2511.01610", "authors": ["Mahmut Selman Gokmen", "Cody Bumgardner"], "title": "DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning", "comment": null, "summary": "Vision Foundation Models (VFMs) have advanced representation learning through\nself-supervised methods. However, existing training pipelines are often\ninflexible, domain-specific, or computationally expensive, which limits their\nusability across different domains and resource settings. DINO-MX is a modular\nand extensible training framework that combines the core principles of DINO,\nDINOv2 and DINOv3 within a unified configuration-driven system. It supports a\nvariety of transformer-based architectures and is fully compatible with the\nHugging Face ecosystem. The framework includes multiple training strategies\nsuch as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,\nalong with support for distributed training through both Distributed Data\nParallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to\nwork with both natural and specialized data types, including single- and\nmulti-channel images. Experimental results on diverse datasets show that\nDINO-MX achieves competitive performance while significantly reducing\ncomputational costs. Additionally, it offers interpretability tools and a\nlabel-guided data augmentation method that improves attention-based\nlocalization without the need for extra detection or segmentation heads.\nDINO-MX provides a reproducible and scalable foundation for developing,\nadapting, and benchmarking self-supervised vision models across a range of\nresearch and real-world applications.", "AI": {"tldr": "DINO-MX是一个模块化、可扩展的框架，统一了DINO系列自监督学习方法，支持多种架构和训练策略，旨在降低计算成本并提高跨领域可用性。", "motivation": "现有视觉基础模型（VFM）的训练流程不灵活、领域特定或计算成本高昂，限制了它们在不同领域和资源设置下的可用性。", "method": "DINO-MX是一个模块化、可扩展的训练框架，它在一个统一的配置驱动系统中结合了DINO、DINOv2和DINOv3的核心原则。它支持多种基于Transformer的架构，并与Hugging Face生态系统完全兼容。该框架包括多种训练策略（如LoRA、层冻结、知识蒸馏）和分布式训练支持（DDP和FSDP）。它设计用于处理自然和特殊数据类型，并提供可解释性工具和标签引导的数据增强方法。", "result": "DINO-MX在多样化数据集上实现了有竞争力的性能，同时显著降低了计算成本。此外，它提供的标签引导数据增强方法无需额外的检测或分割头即可改善基于注意力的定位。", "conclusion": "DINO-MX为开发、适应和基准测试自监督视觉模型提供了一个可复现和可扩展的基础，适用于广泛的研究和实际应用。"}}
{"id": "2511.01129", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01129", "abs": "https://arxiv.org/abs/2511.01129", "authors": ["Fabio Diniz Rossi"], "title": "Boosting performance of computer vision applications through embedded GPUs on the edge", "comment": "4 pages, 6 figures", "summary": "Computer vision applications, especially those using augmented reality\ntechnology, are becoming quite popular in mobile devices. However, this type of\napplication is known as presenting significant demands regarding resources. In\norder to enable its utilization in devices with more modest resources, edge\ncomputing can be used to offload certain high intensive tasks. Still, edge\ncomputing is usually composed of devices with limited capacity, which may\nimpact in users quality of experience when using computer vision applications.\nThis work proposes the use of embedded devices with graphics processing units\n(GPUs) to overcome such limitation. Experiments performed shown that GPUs can\nattain a performance gain when compared to using only CPUs, which guarantee a\nbetter experience to users using such kind of application.", "AI": {"tldr": "该研究提出在边缘计算中使用带GPU的嵌入式设备来加速移动增强现实/计算机视觉应用，以克服资源限制并提升用户体验。", "motivation": "移动设备上的增强现实（AR）和计算机视觉（CV）应用资源需求高。边缘计算可用于卸载任务，但边缘设备通常容量有限，可能影响用户体验。", "method": "本文提出使用带有图形处理单元（GPU）的嵌入式设备来解决边缘计算的容量限制问题。通过实验验证了其性能。", "result": "实验结果表明，与仅使用CPU相比，使用GPU可以获得性能提升，从而为用户提供更好的应用体验。", "conclusion": "在边缘计算中引入带GPU的嵌入式设备能够显著提升移动增强现实和计算机视觉应用的性能，从而改善用户体验。"}}
{"id": "2511.01854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01854", "abs": "https://arxiv.org/abs/2511.01854", "authors": ["Elias Lumer", "Faheem Nizar", "Anmol Gulati", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah"], "title": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems", "comment": null, "summary": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark.", "AI": {"tldr": "本文提出“工具到代理检索”框架，通过在共享向量空间中嵌入工具及其父代理并利用元数据关系，解决了LLM多代理系统中现有粗粒度检索导致代理选择次优的问题，显著提高了检索精度。", "motivation": "LLM多代理系统中的现有检索方法通常将查询与粗粒度的代理级别描述进行匹配，这掩盖了工具的细粒度功能，导致代理选择次优。", "method": "引入“工具到代理检索”框架，该框架将工具及其父代理嵌入到一个共享的向量空间中，并通过元数据关系连接它们。通过显式表示工具能力并遍历元数据到代理级别，该方法实现了细粒度的工具级或代理级检索，确保代理及其底层工具或MCP服务器得到平等表示，避免了因将多个工具打包在一起而导致的上下文稀释。", "result": "在LiveMCPBench基准测试中，该方法在八种嵌入模型上，相较于之前最先进的代理检索器，Recall@5提高了19.4%，nDCG@5提高了17.7%。", "conclusion": "“工具到代理检索”框架通过更精细地表示工具功能和其与代理的关系，显著改善了LLM多代理系统中的代理选择准确性，从而实现了更有效的系统编排。"}}
{"id": "2511.01767", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01767", "abs": "https://arxiv.org/abs/2511.01767", "authors": ["Yuxiao Yang", "Xiao-Xiao Long", "Zhiyang Dou", "Cheng Lin", "Yuan Liu", "Qingsong Yan", "Yuexin Ma", "Haoqian Wang", "Zhiqiang Wu", "Wei Yin"], "title": "Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image", "comment": "21 pages, 19 figures, accepted by TPAMI", "summary": "In this work, we introduce \\textbf{Wonder3D++}, a novel method for\nefficiently generating high-fidelity textured meshes from single-view images.\nRecent methods based on Score Distillation Sampling (SDS) have shown the\npotential to recover 3D geometry from 2D diffusion priors, but they typically\nsuffer from time-consuming per-shape optimization and inconsistent geometry. In\ncontrast, certain works directly produce 3D information via fast network\ninferences, but their results are often of low quality and lack geometric\ndetails. To holistically improve the quality, consistency, and efficiency of\nsingle-view reconstruction tasks, we propose a cross-domain diffusion model\nthat generates multi-view normal maps and the corresponding color images. To\nensure the consistency of generation, we employ a multi-view cross-domain\nattention mechanism that facilitates information exchange across views and\nmodalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that\ndrives high-quality surfaces from the multi-view 2D representations in only\nabout $3$ minute in a coarse-to-fine manner. Our extensive evaluations\ndemonstrate that our method achieves high-quality reconstruction results,\nrobust generalization, and good efficiency compared to prior works. Code\navailable at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.", "AI": {"tldr": "Wonder3D++是一种新颖的方法，可以从单视图图像高效生成高质量的带纹理网格，解决了现有方法在效率、质量和几何一致性方面的不足。", "motivation": "现有方法存在以下问题：基于SDS的方法通常耗时且几何不一致；直接网络推理方法质量低且缺乏几何细节。本研究旨在全面提高单视图重建任务的质量、一致性和效率。", "method": "本文提出了一种跨域扩散模型，用于生成多视图法线图及对应的彩色图像。为确保生成的一致性，采用了多视图跨域注意力机制，促进视图和模态间的信息交换。最后，引入了一种级联3D网格提取算法，以粗到精的方式，在大约3分钟内从多视图2D表示中驱动出高质量表面。", "result": "广泛评估表明，与现有工作相比，该方法实现了高质量的重建结果、强大的泛化能力和良好的效率。", "conclusion": "Wonder3D++通过其创新的跨域扩散模型、注意力机制和级联网格提取算法，显著提升了单视图图像到高保真3D模型的重建性能，在质量、一致性和效率上均表现出色。"}}
{"id": "2511.01340", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01340", "abs": "https://arxiv.org/abs/2511.01340", "authors": ["Trishanu Das", "Abhilash Nandy", "Khush Bajaj", "Deepiha S"], "title": "$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles", "comment": "7 pages, 5 figures, 4 tables", "summary": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters\nto represent words or phrases creatively) requires a variety of skills such as\nimage recognition, cognitive skills, commonsense reasoning, multi-step\nreasoning, image-based wordplay, etc., making this a challenging task for even\ncurrent Vision-Language Models. In this paper, we present\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$, a large and diverse\nbenchmark of $1,333$ English Rebus Puzzles containing different artistic styles\nand levels of difficulty, spread across 18 categories such as food, idioms,\nsports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a\nmodel-agnostic framework which uses a combination of an unstructured\ndescription and code-based, structured reasoning, along with better,\nreasoning-based in-context example selection, improving the performance of\nVision-Language Models on\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$ by $2.1-4.1\\%$ and\n$20-30\\%$ using closed-source and open-source models respectively compared to\nChain-of-Thought Reasoning.", "AI": {"tldr": "本文提出了一个名为“| ⟲ BUS |”的大型Rebus谜题基准，并引入了一个名为“RebusDescProgICE”的模型无关框架，显著提升了视觉-语言模型在解决此类谜题上的性能。", "motivation": "Rebus谜题结合了图像识别、认知技能、常识推理和多步推理等多种复杂能力，对当前视觉-语言模型（VLM）来说是一个极具挑战性的任务。", "method": "1. 构建了一个包含1333个英语Rebus谜题的大型多样化基准“| ⟲ BUS |”，涵盖18个类别，具有不同的艺术风格和难度级别。2. 提出了一个模型无关框架“RebusDescProgICE”，该框架结合了非结构化描述、基于代码的结构化推理以及基于推理的上下文示例选择。", "result": "与Chain-of-Thought推理方法相比，“RebusDescProgICE”框架将视觉-语言模型在“| ⟲ BUS |”基准上的性能提升了2.1-4.1%（使用闭源模型）和20-30%（使用开源模型）。", "conclusion": "所提出的“| ⟲ BUS |”基准和“RebusDescProgICE”框架有效地提高了视觉-语言模型解决复杂Rebus谜题的能力，展示了结合描述、结构化推理和智能示例选择的潜力。"}}
{"id": "2511.01109", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01109", "abs": "https://arxiv.org/abs/2511.01109", "authors": ["Alexander Thorley", "Agis Chartsias", "Jordan Strom", "Jeremy Slivnick", "Dipak Kotecha", "Alberto Gomez", "Jinming Duan"], "title": "Anatomically Constrained Transformers for Echocardiogram Analysis", "comment": null, "summary": "Video transformers have recently demonstrated strong potential for\nechocardiogram (echo) analysis, leveraging self-supervised pre-training and\nflexible adaptation across diverse tasks. However, like other models operating\non videos, they are prone to learning spurious correlations from non-diagnostic\nregions such as image backgrounds. To overcome this limitation, we propose the\nVideo Anatomically Constrained Transformer (ViACT), a novel framework that\nintegrates anatomical priors directly into the transformer architecture. ViACT\nrepresents a deforming anatomical structure as a point set and encodes both its\nspatial geometry and corresponding image patches into transformer tokens.\nDuring pre-training, ViACT follows a masked autoencoding strategy that masks\nand reconstructs only anatomical patches, enforcing that representation\nlearning is focused on the anatomical region. The pre-trained model can then be\nfine-tuned for tasks localized to this region. In this work we focus on the\nmyocardium, demonstrating the framework on echo analysis tasks such as left\nventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)\ndetection. The anatomical constraint focuses transformer attention within the\nmyocardium, yielding interpretable attention maps aligned with regions of known\nCA pathology. Moreover, ViACT generalizes to myocardium point tracking without\nrequiring task-specific components such as correlation volumes used in\nspecialized tracking networks.", "AI": {"tldr": "本文提出ViACT（Video Anatomically Constrained Transformer），通过将解剖先验知识（点集表示的解剖结构）直接整合到Transformer中，解决视频Transformer在超声心动图分析中学习非诊断区域无关特征的问题。ViACT在预训练中仅掩码和重建解剖区域，从而将学习集中于该区域，提升了EF回归、CA检测等任务的性能和可解释性，并能泛化到点跟踪。", "motivation": "视频Transformer在超声心动图分析中表现出强大潜力，但与其他视频模型一样，容易从图像背景等非诊断区域学习到虚假关联，影响模型性能和可解释性。", "method": "本文提出ViACT框架，将解剖先验知识直接整合到Transformer架构中。它将变形的解剖结构表示为点集，并将其空间几何和对应图像块编码为Transformer token。在预训练阶段，ViACT采用掩码自编码策略，仅掩码和重建解剖区域的图像块，强制模型学习专注于解剖区域。模型随后可针对特定解剖区域的任务进行微调，本文以心肌为例进行演示。", "result": "解剖约束使Transformer的注意力集中在心肌内部，生成与已知心脏淀粉样变（CA）病理区域对齐的可解释注意力图。ViACT在左心室射血分数（EF）回归和心脏淀粉样变（CA）检测等超声心动图分析任务上表现出色。此外，ViACT无需专门跟踪网络中使用的相关体积等任务特定组件，即可泛化到心肌点跟踪任务。", "conclusion": "ViACT通过将解剖先验知识直接融入Transformer，成功克服了视频Transformer在超声心动图分析中学习非诊断区域虚假关联的限制。这使得模型学习更聚焦于解剖区域，提高了任务性能、可解释性，并展现出在解剖区域点跟踪等任务上的泛化能力。"}}
{"id": "2511.01163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01163", "abs": "https://arxiv.org/abs/2511.01163", "authors": ["Yongyuan Liang", "Wei Chow", "Feng Li", "Ziqiao Ma", "Xiyao Wang", "Jiageng Mao", "Jiuhai Chen", "Jiatao Gu", "Yue Wang", "Furong Huang"], "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation", "comment": "Project Page: https://roverbench.github.io/", "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.", "AI": {"tldr": "该研究引入了ROVER基准测试，以评估统一多模态模型（UMMs）的互惠跨模态推理能力，发现跨模态推理对视觉生成质量至关重要，且模型在符号推理的视觉抽象方面表现不佳。", "motivation": "现有的多模态模型评估方法将文本和图像的理解与生成能力视为孤立的，主要通过单模态推理来评分。然而，真正的统一多模态智能需要模型能够进行互惠跨模态推理，即利用一种模态来指导、验证或完善另一种模态的输出。为解决这一评估空白，该研究旨在引入一个专门测试这种能力的基准。", "method": "研究引入了一个名为ROVER的人工标注基准测试，包含1312个任务，基于1876张图像，涵盖两种互补设置：1) 语言增强视觉生成推理，评估模型能否利用语言提示和推理链指导图像合成；2) 视觉增强语言生成推理，评估模型能否生成中间可视化以强化其问答推理过程。研究在17个统一模型上进行了实验。", "result": "实验揭示了两个关键发现：1) 跨模态推理决定视觉生成质量，其中交错式模型显著优于非交错式模型；值得注意的是，结合强大的单模态模型也未能实现可比的推理能力。2) 模型在物理和符号推理之间存在脱节：它们能字面解释感知概念，但在符号任务中无法构建视觉抽象，导致错误的推理并损害性能。", "conclusion": "互惠跨模态推理是实现真正全模态生成能力的关键前沿。当前统一多模态模型在符号推理的视觉抽象方面存在显著不足，需要进一步研究来提升其跨模态推理能力。"}}
{"id": "2511.01618", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01618", "abs": "https://arxiv.org/abs/2511.01618", "authors": ["Xiaoyu Zhan", "Wenxuan Huang", "Hao Sun", "Xinyu Fu", "Changfeng Ma", "Shaosheng Cao", "Bohan Jia", "Shaohui Lin", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.", "AI": {"tldr": "该研究引入了“视角学习”任务和Viewpoint-100K数据集，通过两阶段微调（SFT和GRPO强化学习）以及混合冷启动初始化，显著提升了多模态大语言模型（MLLMs）在2D和3D空间推理任务中的表现，强调了发展MLLMs空间基础能力的重要性。", "motivation": "尽管多模态大语言模型（MLLMs）在2D视觉理解方面取得了显著进展，但其捕捉详细空间信息和实现跨视角一致性以进行鲁棒3D推理的能力尚不明确，这是真实世界3D推理的关键要求。", "method": "本研究引入了“视角学习”任务来评估和提升MLLMs的空间推理能力，并构建了包含10万个物体中心图像对及其问答的Viewpoint-100K数据集。采用两阶段微调策略：首先通过在Viewpoint-100K上进行监督微调（SFT）注入基础知识；其次，利用组相对策略优化（GRPO）算法在更广泛的问题集上进行强化学习以增强泛化能力。此外，还提出了一种混合冷启动初始化方法，以同时学习视角表示并保持连贯的推理思维。", "result": "实验结果表明，该方法显著激活了MLLM的空间推理能力，提高了模型在域内和域外推理任务上的性能。", "conclusion": "研究结果强调了在MLLMs中发展基础空间技能的价值，为未来机器人物理交互、自动驾驶系统和3D场景理解的进步提供了支持。"}}
{"id": "2511.01775", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.01775", "abs": "https://arxiv.org/abs/2511.01775", "authors": ["Zhen Chen", "Qing Xu", "Jinlin Wu", "Biao Yang", "Yuhao Zhai", "Geng Guo", "Jing Zhang", "Yinlu Ding", "Nassir Navab", "Jiebo Luo"], "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment", "comment": null, "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.", "AI": {"tldr": "本研究提出了SurgVeo基准和Surgical Plausibility Pyramid (SPP) 评估框架，用于评估手术视频生成模型。结果显示，先进模型在视觉感知上表现出色，但在器械操作、环境反馈和手术意图等更高层面的可信度上存在严重缺陷，揭示了视觉模仿与因果理解之间的巨大鸿沟。", "motivation": "视频生成基础模型在模拟物理世界方面展现出巨大潜力，但其在手术等高风险领域（需要深层、专业的因果知识而非通用物理规则）的应用仍是未被探索的关键空白。", "method": "研究引入了SurgVeo（首个由专家策划的手术视频生成模型评估基准）和Surgical Plausibility Pyramid (SPP)（一个新颖的四层评估框架，用于从基本外观到复杂手术策略评估模型输出）。使用Veo-3模型对腹腔镜和神经外科手术片段进行零样本预测任务，并由四位经过认证的外科医生小组根据SPP评估生成的视频。", "result": "结果揭示了明显的“可信度差距”：Veo-3在视觉感知可信度方面表现出色，但在SPP的更高层面（包括器械操作可信度、环境反馈可信度、手术意图可信度）上严重失败。这是首次量化证明了外科AI中视觉上令人信服的模仿与因果理解之间的鸿沟。", "conclusion": "本工作通过SurgVeo和SPP为未来开发能够应对专业、真实世界医疗领域复杂性的模型奠定了关键基础和路线图。"}}
{"id": "2511.01200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01200", "abs": "https://arxiv.org/abs/2511.01200", "authors": ["Mengyuan Liu", "Sheng Yan", "Yong Wang", "Yingjie Li", "Gui-Bin Bian", "Hong Liu"], "title": "MoSa: Motion Generation with Scalable Autoregressive Modeling", "comment": null, "summary": "We introduce MoSa, a novel hierarchical motion generation framework for\ntext-driven 3D human motion generation that enhances the Vector\nQuantization-guided Generative Transformers (VQ-GT) paradigm through a\ncoarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale\nToken Preservation Strategy (MTPS) integrated into a hierarchical residual\nvector quantization variational autoencoder (RQ-VAE). MTPS employs\ninterpolation at each hierarchical quantization to effectively retain\ncoarse-to-fine multi-scale tokens. With this, the generative transformer\nsupports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,\nunlike traditional methods that predict only one token at each step.\nConsequently, MoSa requires only 10 inference steps, matching the number of\nRQ-VAE quantization layers. To address potential reconstruction degradation\nfrom frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive\nconvolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and\nincorporates attention mechanisms to better capture global dependencies.\nExtensive experiments show that MoSa achieves state-of-the-art generation\nquality and efficiency, outperforming prior methods in both fidelity and speed.\nOn the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)\nwhile reducing inference time by 27 percent. Moreover, MoSa generalizes well to\ndownstream tasks such as motion editing, requiring no additional fine-tuning.\nThe code is available at https://mosa-web.github.io/MoSa-web", "AI": {"tldr": "MoSa是一个新的分层运动生成框架，通过粗到细的生成过程和多尺度Token保留策略，显著提高了文本驱动3D人体运动生成的效率和质量。", "motivation": "现有的向量量化引导生成Transformer（VQ-GT）范式在文本驱动3D人体运动生成中可能存在效率和多尺度信息保留的局限性，需要一种更高效、更高质量的生成方法。", "method": "MoSa引入了一个分层运动生成框架，通过多尺度Token保留策略（MTPS）增强了分层残差向量量化变分自编码器（RQ-VAE），MTPS在每个分层量化中采用插值来保留多尺度Token。生成Transformer支持可扩展自回归（SAR）建模，预测尺度Token。此外，为解决插值可能导致的重建退化，提出了CAQ-VAE，一个结合卷积和注意力机制的轻量级VQ-VAE，以更好地捕捉全局依赖。", "result": "MoSa仅需10个推理步骤，实现了最先进的生成质量和效率。在Motion-X数据集上，FID达到0.06（优于MoMask的0.20），推理时间减少了27%。MoSa还能很好地泛化到运动编辑等下游任务，无需额外微调。", "conclusion": "MoSa通过其创新的分层生成框架、多尺度Token保留策略和CAQ-VAE，显著提升了文本驱动3D人体运动生成的质量和效率，并在多个方面超越了现有方法，展现出强大的泛化能力。"}}
{"id": "2511.01175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01175", "abs": "https://arxiv.org/abs/2511.01175", "authors": ["Peng Du", "Hui Li", "Han Xu", "Paul Barom Jeon", "Dongwook Lee", "Daehyun Ji", "Ran Yang", "Feng Zhu"], "title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution", "comment": null, "summary": "Discrete Wavelet Transform (DWT) has been widely explored to enhance the\nperformance of image superresolution (SR). Despite some DWT-based methods\nimproving SR by capturing fine-grained frequency signals, most existing\napproaches neglect the interrelations among multiscale frequency sub-bands,\nresulting in inconsistencies and unnatural artifacts in the reconstructed\nimages. To address this challenge, we propose a Diffusion Transformer model\nbased on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the\nsuperiority of diffusion models and transformers to capture the interrelations\namong multiscale frequency sub-bands, leading to a more consistence and\nrealistic SR image. Specifically, we use a Multi-level Discrete Wavelet\nTransform (MDWT) to decompose images into wavelet spectra. A pyramid\ntokenization method is proposed which embeds the spectra into a sequence of\ntokens for transformer model, facilitating to capture features from both\nspatial and frequency domain. A dual-decoder is designed elaborately to handle\nthe distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,\nwithout omitting their alignment in image generation. Extensive experiments on\nmultiple benchmark datasets demonstrate the effectiveness of our method, with\nhigh performance on both perception quality and fidelity.", "AI": {"tldr": "本文提出了一种基于图像小波谱的扩散Transformer模型（DTWSR），用于超分辨率重建，通过结合扩散模型和Transformer的优势来捕获多尺度频率子带间的相互关系，从而生成更一致和真实的SR图像。", "motivation": "现有的DWT（离散小波变换）基超分辨率方法通常忽略多尺度频率子带之间的相互关系，导致重建图像出现不一致和不自然的伪影。", "method": "本文提出了DTWSR模型。具体而言，它使用多级离散小波变换（MDWT）将图像分解为小波谱；提出了一种金字塔分词方法将小波谱嵌入为Transformer模型的token序列，以捕获空间和频率域特征；设计了一个双解码器，用于处理低频（LF）和高频（HF）子带的差异性，同时保持它们在图像生成中的对齐。", "result": "在多个基准数据集上进行的广泛实验表明，该方法在感知质量和保真度方面均表现出高水平的性能和有效性。", "conclusion": "DTWSR模型通过有效捕获多尺度频率子带间的相互关系，克服了现有DWT基SR方法的局限性，显著提高了超分辨率图像的一致性和真实感。"}}
{"id": "2511.01210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01210", "abs": "https://arxiv.org/abs/2511.01210", "authors": ["Heyu Guo", "Shanmu Wang", "Ruichun Ma", "Shiqi Jiang", "Yasaman Ghasempour", "Omid Abari", "Baining Guo", "Lili Qi"], "title": "OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA", "comment": null, "summary": "Vision-language-action (VLA) models have shown strong generalization for\naction prediction through large-scale vision-language pretraining. However,\nmost existing models rely solely on RGB cameras, limiting their perception and,\nconsequently, manipulation capabilities. We present OmniVLA, an omni-modality\nVLA model that integrates novel sensing modalities for physically-grounded\nspatial intelligence beyond RGB perception. The core of our approach is the\nsensor-masked image, a unified representation that overlays spatially grounded\nand physically meaningful masks onto the RGB images, derived from sensors\nincluding an infrared camera, a mmWave radar, and a microphone array. This\nimage-native unification keeps sensor input close to RGB statistics to\nfacilitate training, provides a uniform interface across sensor hardware, and\nenables data-efficient learning with lightweight per-sensor projectors. Built\non this, we present a multisensory vision-language-action model architecture\nand train the model based on an RGB-pretrained VLA backbone. We evaluate\nOmniVLA on challenging real-world tasks where sensor-modality perception is\nneeded to guide the manipulation. OmniVLA achieves an average task success rate\nof 84%, significantly outperforms both RGB-only and raw-sensor-input baseline\nmodels by 59% and 28% respectively, meanwhile showing higher learning\nefficiency and stronger generalization capability.", "AI": {"tldr": "OmniVLA是一个全模态视觉-语言-动作(VLA)模型，通过将红外、毫米波雷达和麦克风阵列等传感器的信息融合到RGB图像中，形成“传感器掩码图像”，显著提升了机器人感知和操作能力，在真实世界任务中表现优于仅依赖RGB或原始传感器输入的基线模型。", "motivation": "现有的VLA模型主要依赖RGB摄像头，这限制了它们的感知能力，进而影响了操作能力，尤其是在需要超越RGB感知的物理空间智能任务中。", "method": "本文提出了OmniVLA模型，其核心是“传感器掩码图像”——一种统一的表示形式。该方法将来自红外摄像头、毫米波雷达和麦克风阵列等传感器的空间定位和物理有意义的掩码叠加到RGB图像上。这种图像原生的统一方式使传感器输入接近RGB统计数据，便于训练，提供统一的传感器硬件接口，并实现数据高效学习。模型建立在RGB预训练的VLA骨干网络之上。", "result": "OmniVLA在需要传感器模态感知来指导操作的挑战性真实世界任务中，实现了平均84%的任务成功率。它显著优于仅依赖RGB的基线模型59%，也优于原始传感器输入基线模型28%。此外，OmniVLA展现出更高的学习效率和更强的泛化能力。", "conclusion": "OmniVLA通过整合多模态传感器信息，并将其统一为“传感器掩码图像”表示，显著提升了VLA模型的感知、操作、学习效率和泛化能力，使其能够更好地应对超越RGB感知限制的复杂真实世界任务。"}}
{"id": "2511.01169", "categories": ["cs.CV", "I.2.10; I.4.5"], "pdf": "https://arxiv.org/pdf/2511.01169", "abs": "https://arxiv.org/abs/2511.01169", "authors": ["Brian Nlong Zhao", "Jiajun Wu", "Shangzhe Wu"], "title": "Web-Scale Collection of Video Data for 4D Animal Reconstruction", "comment": "NeurIPS 2025 Datasets and Benchmarks", "summary": "Computer vision for animals holds great promise for wildlife research but\noften depends on large-scale data, while existing collection methods rely on\ncontrolled capture setups. Recent data-driven approaches show the potential of\nsingle-view, non-invasive analysis, yet current animal video datasets are\nlimited--offering as few as 2.4K 15-frame clips and lacking key processing for\nanimal-centric 3D/4D tasks. We introduce an automated pipeline that mines\nYouTube videos and processes them into object-centric clips, along with\nauxiliary annotations valuable for downstream tasks like pose estimation,\ntracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos\n(2M frames)--an order of magnitude more than prior works. To demonstrate its\nutility, we focus on the 4D quadruped animal reconstruction task. To support\nthis task, we present Animal-in-Motion (AiM), a benchmark of 230 manually\nfiltered sequences with 11K frames showcasing clean, diverse animal motions. We\nevaluate state-of-the-art model-based and model-free methods on\nAnimal-in-Motion, finding that 2D metrics favor the former despite unrealistic\n3D shapes, while the latter yields more natural reconstructions but scores\nlower--revealing a gap in current evaluation. To address this, we enhance a\nrecent model-free approach with sequence-level optimization, establishing the\nfirst 4D animal reconstruction baseline. Together, our pipeline, benchmark, and\nbaseline aim to advance large-scale, markerless 4D animal reconstruction and\nrelated tasks from in-the-wild videos. Code and datasets are available at\nhttps://github.com/briannlongzhao/Animal-in-Motion.", "AI": {"tldr": "本文提出一个自动化管道，从YouTube挖掘并处理动物视频，构建了一个大规模数据集（30K视频，2M帧）和首个4D四足动物重建基准（Animal-in-Motion，AiM）。通过评估现有方法并建立新的基线，旨在推动野外视频中无标记4D动物重建任务。", "motivation": "动物计算机视觉在野生动物研究中前景广阔，但通常依赖大规模数据。现有数据采集方法依赖受控捕获设置，而当前的动物视频数据集规模有限（例如，仅2.4K 15帧片段），并且缺乏用于以动物为中心的3D/4D任务的关键处理，限制了单视角、非侵入性分析的潜力。", "method": "研究人员开发了一个自动化管道，用于挖掘YouTube视频，并将其处理成以对象为中心的片段，同时提供辅助注释，以支持姿态估计、跟踪和3D/4D重建等下游任务。利用此管道，他们积累了30K视频（2M帧）。为了支持4D四足动物重建任务，他们推出了Animal-in-Motion（AiM）基准，包含230个手动筛选的序列和11K帧。他们评估了最先进的基于模型和无模型方法，并通过序列级优化增强了一种无模型方法，建立了首个4D动物重建基线。", "result": "通过自动化管道，他们积累了30K视频（2M帧），比现有工作多出一个数量级。创建了包含230个序列和11K帧的Animal-in-Motion（AiM）基准。评估结果显示，2D指标偏爱基于模型的方法，尽管其3D形状不真实，而无模型方法产生更自然的重建，但得分较低，揭示了当前评估的不足。他们通过增强无模型方法，建立了首个4D动物重建基线。", "conclusion": "该论文提出的管道、基准和基线旨在推动从野外视频中进行大规模、无标记的4D动物重建及相关任务的进展。"}}
{"id": "2511.01131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01131", "abs": "https://arxiv.org/abs/2511.01131", "authors": ["Md Nahiduzzaman", "Steven Korevaar", "Alireza Bab-Hadiashar", "Ruwan Tennakoon"], "title": "Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis", "comment": null, "summary": "Human-interpretable predictions are essential for deploying AI in medical\nimaging, yet most interpretable-by-design (IBD) frameworks require concept\nannotations for training data, which are costly and impractical to obtain in\nclinical contexts. Recent attempts to bypass annotation, such as zero-shot\nvision-language models or concept-generation frameworks, struggle to capture\ndomain-specific medical features, leading to poor reliability. In this paper,\nwe propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised\nframework that enables concept answer prediction without explicit supervision\nor reliance on language models. PCP leverages class-level concept priors as\nweak supervision and incorporates a refinement mechanism with KL divergence and\nentropy regularization to align predictions with clinical reasoning.\nExperiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves\nconcept-level F1-score by over 33% compared to zero-shot baselines, while\ndelivering competitive classification performance on four medical datasets\n(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept\nbottleneck models (CBMs) and V-IP.", "AI": {"tldr": "本文提出了一种名为PCP的弱监督框架，用于在无需显式概念标注或语言模型的情况下，为医学影像提供人类可解释的预测。PCP利用类别级概念先验作为弱监督，并通过KL散度和熵正则化机制进行细化，在概念预测和分类性能上均表现出色。", "motivation": "在医疗影像领域部署AI时，人类可解释的预测至关重要。然而，现有可解释设计（IBD）框架需要昂贵的概念标注，这在临床环境中不切实际。近期尝试绕过标注的方法（如零样本视觉-语言模型或概念生成框架）难以捕捉领域特定的医学特征，导致可靠性差。", "method": "本文提出了一种新颖的先验引导概念预测器（Prior-guided Concept Predictor, PCP）。这是一个弱监督框架，通过利用类别级概念先验作为弱监督，实现概念答案预测，而无需显式监督或依赖语言模型。PCP还包含一个通过KL散度和熵正则化实现的细化机制，以使预测与临床推理对齐。", "result": "在PH2（皮肤镜检查）和WBCatt（血液学）数据集上，PCP的概念级F1分数比零样本基线提高了33%以上。同时，在PH2、WBCatt、HAM10000和CXR4四个医学数据集上，PCP的分类性能与全监督概念瓶颈模型（CBMs）和V-IP相比具有竞争力。", "conclusion": "PCP框架成功地在无需昂贵概念标注的情况下，为医学影像提供了人类可解释的预测。它在概念预测方面显著优于零样本方法，并在分类性能上可与全监督模型媲美，解决了当前可解释AI在医学影像中部署面临的标注难题和领域特异性捕捉不足的问题。"}}
{"id": "2511.01233", "categories": ["cs.CV", "cs.GR", "cs.HC", "I.3; I.2"], "pdf": "https://arxiv.org/pdf/2511.01233", "abs": "https://arxiv.org/abs/2511.01233", "authors": ["Rajmund Nagy", "Hendric Voss", "Thanh Hoang-Minh", "Mihail Tsakov", "Teodor Nikolov", "Zeyi Zhang", "Tenglong Ao", "Sicheng Yang", "Shaoli Huang", "Yongkang Cheng", "M. Hamza Mughal", "Rishabh Dabral", "Kiran Chhatre", "Christian Theobalt", "Libin Liu", "Stefan Kopp", "Rachel McDonnell", "Michael Neff", "Taras Kucherenko", "Youngwoo Yoon", "Gustav Eje Henter"], "title": "Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark", "comment": "23 pages, 10 figures. The last two authors made equal contributions", "summary": "We review human evaluation practices in automated, speech-driven 3D gesture\ngeneration and find a lack of standardisation and frequent use of flawed\nexperimental setups. This leads to a situation where it is impossible to know\nhow different methods compare, or what the state of the art is. In order to\naddress common shortcomings of evaluation design, and to standardise future\nuser studies in gesture-generation works, we introduce a detailed human\nevaluation protocol for the widely-used BEAT2 motion-capture dataset. Using\nthis protocol, we conduct large-scale crowdsourced evaluation to rank six\nrecent gesture-generation models -- each trained by its original authors --\nacross two key evaluation dimensions: motion realism and speech-gesture\nalignment. Our results provide strong evidence that 1) newer models do not\nconsistently outperform earlier approaches; 2) published claims of high motion\nrealism or speech-gesture alignment may not hold up under rigorous evaluation;\nand 3) the field must adopt disentangled assessments of motion quality and\nmultimodal alignment for accurate benchmarking in order to make progress.\nFinally, in order to drive standardisation and enable new evaluation research,\nwe will release five hours of synthetic motion from the benchmarked models;\nover 750 rendered video stimuli from the user studies -- enabling new\nevaluations without model reimplementation required -- alongside our\nopen-source rendering script, and the 16,000 pairwise human preference votes\ncollected for our benchmark.", "AI": {"tldr": "本文回顾了自动语音驱动3D手势生成的人类评估实践，发现缺乏标准化和存在缺陷的实验设置。为解决这些问题，我们引入了一个详细的人类评估协议，并使用该协议对六个最新模型进行了大规模众包评估，结果显示新模型并不总是优于旧模型，且现有评估存在不足。我们强调需要解耦评估，并发布了相关资源以推动标准化。", "motivation": "自动语音驱动3D手势生成领域的人类评估实践缺乏标准化，且实验设置存在缺陷，导致不同方法之间难以比较，也无法确定当前的技术水平。这促使研究人员寻求改进评估设计并实现未来用户研究的标准化。", "method": "本文针对广泛使用的BEAT2动作捕捉数据集，引入了一个详细的人类评估协议。利用该协议，研究人员对六个由原始作者训练的最新手势生成模型进行了大规模众包评估，评估维度包括运动真实感和语音-手势对齐。此外，为推动标准化和新的评估研究，作者将发布基准模型的合成运动数据、用户研究的渲染视频刺激、开源渲染脚本以及收集到的16,000个配对人类偏好投票。", "result": "研究结果提供了强有力的证据表明：1) 较新的模型并不总是一致地优于较早的方法；2) 已发表的关于高运动真实感或语音-手势对齐的声明在严格评估下可能不成立；3) 该领域必须采用运动质量和多模态对齐的解耦评估，以实现准确的基准测试并取得进展。", "conclusion": "为了在该领域取得进展，必须采纳解耦的运动质量和多模态对齐评估，以实现准确的基准测试。研究人员通过引入标准化协议和发布大量评估资源，旨在推动该领域的人类评估实践走向标准化和更严谨的科学方法。"}}
{"id": "2511.01240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01240", "abs": "https://arxiv.org/abs/2511.01240", "authors": ["Zhixuan Zhang", "Pingyu Wang", "Xingjian Zheng", "Linbo Qing", "Qi Liu"], "title": "Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability", "comment": "Accepted by Pattern Recognition in Nov 01,2025", "summary": "Transferable attacks generate adversarial examples on surrogate models to\nfool unknown victim models, posing real-world threats and growing research\ninterest. Despite focusing on flat losses for transferable adversarial\nexamples, recent studies still fall into suboptimal regions, especially the\nflat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce\na novel black-box gradient-based transferable attack from a perspective of\ndual-order information. Specifically, we feasibly propose Adversarial Flatness\n(AF) to the deceptive flatness problem and a theoretical assurance for\nadversarial transferability. Based on this, using an efficient approximation of\nour objective, we instantiate our attack as Adversarial Flatness Attack (AFA),\naddressing the altered gradient sign issue. Additionally, to further improve\nthe attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by\nenhancing the inner-loop sampling efficiency. The comprehensive results on\nImageNet-compatible dataset demonstrate superiority over six baselines,\ngenerating adversarial examples in flatter regions and boosting transferability\nacross model architectures. When tested on input transformation attacks or the\nBaidu Cloud API, our method outperforms baselines.", "AI": {"tldr": "本文提出了一种基于双阶信息的黑盒可迁移对抗攻击方法，通过解决“欺骗性平坦度”问题并引入对抗平坦度攻击（AFA）和蒙特卡洛对抗采样（MCAS），显著提升了对抗样本在不同模型间的迁移能力。", "motivation": "可迁移攻击对未知模型构成实际威胁，但现有生成可迁移对抗样本的方法（即使关注平坦损失）仍常陷入“欺骗性平坦度”（即平坦但尖锐的区域），导致攻击效果不佳。", "method": "研究提出了一种基于双阶信息的黑盒梯度可迁移攻击。具体而言，引入了“对抗平坦度（AF）”来解决欺骗性平坦度问题，并提供了可迁移性的理论保证。基于此，通过对目标函数进行高效近似，实例化了“对抗平坦度攻击（AFA）”以解决梯度符号改变的问题。此外，为进一步增强攻击能力，设计了“蒙特卡洛对抗采样（MCAS）”以提高内循环采样效率。", "result": "在ImageNet兼容数据集上的综合实验结果表明，该方法优于六种基线方法，能够在更平坦的区域生成对抗样本，并显著提高跨模型架构的迁移能力。在输入变换攻击或百度云API测试中，本方法也优于基线方法。", "conclusion": "本研究提出的方法（包括AF、AFA和MCAS）有效解决了可迁移对抗攻击中的欺骗性平坦度问题，能够生成更平坦的对抗样本，并显著提升了黑盒场景下的攻击迁移能力。"}}
{"id": "2511.01243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01243", "abs": "https://arxiv.org/abs/2511.01243", "authors": ["Yu Tian", "Zhongheng Yang", "Chenshi Liu", "Yiyun Su", "Ziwei Hong", "Zexi Gong", "Jingyuan Xu"], "title": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "comment": null, "summary": "Brain lesion segmentation remains challenging due to small, low-contrast\nlesions, anisotropic sampling, and cross-slice discontinuities. We propose\nCenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and\ntrains only lightweight adapters for efficient fine-tuning. At its core is the\nCenterMamba encoder, which employs a novel 3x3 corner-axis-center\nshort-sequence scanning strategy to enable center-prioritized, axis-reinforced,\nand diagonally compensated information aggregation. This design enhances\nsensitivity to weak boundaries and tiny foci while maintaining sparse yet\neffective feature representation. A memory-driven structural prompt generator\nmaintains a prototype bank across neighboring slices, enabling automatic\nsynthesis of reliable prompts without user interaction, thereby improving\ninter-slice coherence. The memory-augmented multi-scale decoder integrates\nmemory attention modules at multiple levels, combining deep supervision with\nprogressive refinement to restore fine details while preserving global\nconsistency. Extensive experiments on public benchmarks demonstrate that\nCenterMamba-SAM achieves state-of-the-art performance in brain lesion\nsegmentation.", "AI": {"tldr": "CenterMamba-SAM是一个用于脑部病变分割的端到端框架，通过新颖的CenterMamba编码器、记忆驱动的结构化提示生成器和记忆增强多尺度解码器，解决了小病灶、低对比度及跨层不连续性等挑战，实现了最先进的性能。", "motivation": "脑部病变分割面临诸多挑战，包括病灶体积小、对比度低、各向异性采样以及跨层不连续性。", "method": "该研究提出了CenterMamba-SAM框架，它冻结预训练主干并仅训练轻量级适配器以实现高效微调。核心组件包括：1) CenterMamba编码器，采用新颖的3x3角-轴-中心短序列扫描策略，实现中心优先、轴向增强和对角线补偿的信息聚合；2) 记忆驱动的结构化提示生成器，在相邻切片间维护原型库，无需用户交互自动合成可靠提示，提高层间一致性；3) 记忆增强多尺度解码器，在多层级集成记忆注意力模块，结合深度监督和渐进式细化以恢复细节并保持全局一致性。", "result": "在公共基准测试中，CenterMamba-SAM取得了脑部病变分割任务的最先进性能。", "conclusion": "CenterMamba-SAM框架通过其创新的编码器、提示生成器和解码器设计，有效解决了脑部病变分割的挑战，并显著提升了分割效果。"}}
{"id": "2511.01250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01250", "abs": "https://arxiv.org/abs/2511.01250", "authors": ["YoungJae Cheong", "Jhonghyun An"], "title": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop", "comment": null, "summary": "LiDAR semantic segmentation degrades in adverse weather because refraction,\nscattering, and point dropouts corrupt geometry. Prior work in weather\nsimulation, mixing-based augmentation, domain randomization, and uncertainty or\nboundary regularization improves robustness but still overlooks structural\nvulnerabilities near boundaries, corners, and sparse regions. We present a\nLight Geometry-aware adapter. The module aligns azimuth and applies horizontal\ncircular padding to preserve neighbor continuity across the 0~360 degree\nwrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points\nand computes simple local statistics, which are compressed into compact\ngeometry-aware cues. During training, these cues drive region-aware\nregularization that stabilizes predictions in structurally fragile areas. The\nadapter is plug and play, complements augmentation, and can be enabled only\nduring training with negligible inference cost. We adopt a source-only\ncross-weather setup where models train on SemanticKITTI and are evaluated on\nSemanticSTF without target labels or fine-tuning. The adapter improves mIoU by\n7.9 percentage points over the data-centric augmentation baseline and by 0.6\npoints over the class-centric regularization baseline. These results indicate\nthat geometry-driven regularization is a key direction for all-weather LiDAR\nsegmentation.", "AI": {"tldr": "本文提出了一种轻量级几何感知适配器，通过引入区域感知正则化来提高LiDAR语义分割在恶劣天气下的鲁棒性，尤其是在结构脆弱区域。", "motivation": "恶劣天气（如折射、散射、点丢失）会导致LiDAR语义分割性能下降，现有方法忽视了边界、角落和稀疏区域附近的结构脆弱性。", "method": "本文提出了一个轻量级几何感知适配器。该模块对方位角进行对齐并应用水平循环填充以保持0~360度边界的邻域连续性。它使用局部窗口K-Nearest Neighbors收集附近点并计算简单的局部统计数据，压缩成紧凑的几何感知线索。这些线索在训练期间驱动区域感知正则化，稳定结构脆弱区域的预测。该适配器即插即用，可与数据增强互补，且仅在训练期间启用，推理成本可忽略不计。", "result": "在源域训练（SemanticKITTI）和跨天气评估（SemanticSTF）的设置下，该适配器将mIoU比以数据为中心的增强基线提高了7.9个百分点，比以类别为中心的正则化基线提高了0.6个百分点。", "conclusion": "研究结果表明，几何驱动的正则化是全天候LiDAR分割的关键方向。"}}
{"id": "2511.01302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01302", "abs": "https://arxiv.org/abs/2511.01302", "authors": ["Nu-Fnag Xiao", "De-Xing Huang", "Le-Tian Wang", "Mei-Jiang Gui", "Qi Fu", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuangyi Wang", "Zeng-Guang Hou", "Ying-Wei Wang", "Xiao-Hu Zhou"], "title": "REASON: Probability map-guided dual-branch fusion framework for gastric content assessment", "comment": "Under Review. 12 pages, 10 figures, 6 tables", "summary": "Accurate assessment of gastric content from ultrasound is critical for\nstratifying aspiration risk at induction of general anesthesia. However,\ntraditional methods rely on manual tracing of gastric antra and empirical\nformulas, which face significant limitations in both efficiency and accuracy.\nTo address these challenges, a novel two-stage probability map-guided\ndual-branch fusion framework (REASON) for gastric content assessment is\nproposed. In stage 1, a segmentation model generates probability maps that\nsuppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch\nclassifier fuses information from two standard views, right lateral decubitus\n(RLD) and supine (SUP), to improve the discrimination of learned features.\nExperimental results on a self-collected dataset demonstrate that the proposed\nframework outperforms current state-of-the-art approaches by a significant\nmargin. This framework shows great promise for automated preoperative\naspiration risk assessment, offering a more robust, efficient, and accurate\nsolution for clinical practice.", "AI": {"tldr": "本文提出了一种名为REASON的两阶段概率图引导双分支融合框架，用于超声胃内容物评估，以实现术前误吸风险的自动化、高效和准确分层。", "motivation": "传统胃内容物评估方法依赖手动描绘和经验公式，存在效率和准确性方面的显著局限性，而准确评估胃内容物对于全身麻醉诱导时的误吸风险分层至关重要。", "method": "该研究提出了一种名为REASON的两阶段概率图引导双分支融合框架。第一阶段，分割模型生成概率图，以抑制伪影并突出胃部解剖结构。第二阶段，双分支分类器融合来自右侧卧位（RLD）和仰卧位（SUP）两种标准视图的信息，以提高学习特征的判别能力。", "result": "在自收集数据集上的实验结果表明，所提出的框架显著优于当前最先进的方法。", "conclusion": "该框架在自动化术前误吸风险评估方面显示出巨大潜力，为临床实践提供了一个更鲁棒、高效和准确的解决方案。"}}
{"id": "2511.01274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01274", "abs": "https://arxiv.org/abs/2511.01274", "authors": ["Tan Tang", "Yanhong Wu", "Junming Gao", "Yingcai Wu"], "title": "PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers", "comment": null, "summary": "Ancient Chinese paintings are a valuable cultural heritage that is damaged by\nirreversible color degradation. Reviving color-degraded paintings is\nextraordinarily difficult due to the complex chemistry mechanism. Progress is\nfurther slowed by the lack of comprehensive, high-quality datasets, which\nhampers the creation of end-to-end digital restoration tools. To revive colors,\nwe propose PRevivor, a prior-guided color transformer that learns from recent\npaintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and\nSong Dynasty). To develop PRevivor, we decompose color restoration into two\nsequential sub-tasks: luminance enhancement and hue correction. For luminance\nenhancement, we employ two variational U-Nets and a multi-scale mapping module\nto translate faded luminance into restored counterparts. For hue correction, we\ndesign a dual-branch color query module guided by localized hue priors\nextracted from faded paintings. Specifically, one branch focuses attention on\nregions guided by masked priors, enforcing localized hue correction, whereas\nthe other branch remains unconstrained to maintain a global reasoning\ncapability. To evaluate PRevivor, we conduct extensive experiments against\nstate-of-the-art colorization methods. The results demonstrate superior\nperformance both quantitatively and qualitatively.", "AI": {"tldr": "本文提出PRevivor，一种先验引导的颜色Transformer，用于修复古代中国画作中不可逆的色彩退化，通过将任务分解为亮度增强和色相校正，实现了优于现有技术的效果。", "motivation": "古代中国画作因不可逆的色彩退化而面临损坏，其复杂的化学机制和高质量数据集的缺乏，使得端到端数字修复工具的开发异常困难，促使研究者寻求新的解决方案来恢复这些珍贵的文化遗产。", "method": "本文提出了PRevivor，一个先验引导的颜色Transformer。它将色彩修复分解为两个子任务：亮度增强和色相校正。亮度增强使用两个变分U-Nets和一个多尺度映射模块。色相校正设计了一个双分支颜色查询模块，由从褪色画作中提取的局部色相先验引导，其中一个分支专注于局部校正，另一个分支保持全局推理能力。", "result": "PRevivor在与现有最先进的着色方法进行广泛实验后，在定量和定性上都展现出卓越的性能。", "conclusion": "PRevivor通过其独特的分解任务和先验引导机制，成功地解决了古代中国画作的色彩退化问题，为数字修复提供了有效的工具。"}}
{"id": "2511.01266", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01266", "abs": "https://arxiv.org/abs/2511.01266", "authors": ["Joonghyuk Shin", "Zhengqi Li", "Richard Zhang", "Jun-Yan Zhu", "Jaesik Park", "Eli Schechtman", "Xun Huang"], "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls", "comment": "Project webpage: https://joonghyuk.com/motionstream-web/", "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.", "AI": {"tldr": "MotionStream提出了一种低延迟、高帧率的实时流式视频生成方法，通过知识蒸馏和滑动窗口注意力机制，实现了任意长度视频的生成，并显著提升了交互体验和性能。", "motivation": "当前运动条件视频生成方法存在延迟过高（每视频数分钟）和非因果处理的问题，这阻碍了实时交互和应用。", "method": "首先，通过运动控制增强了一个文本到视频模型作为双向教师模型。然后，利用自强制与分布匹配蒸馏（Self Forcing with Distribution Matching Distillation）将该教师模型蒸馏成一个因果学生模型，以实现实时流式推理。为解决长视频生成中的域间隙、误差累积和计算成本增长等挑战，引入了精心设计的滑动窗口因果注意力（sliding-window causal attention）和注意力槽（attention sinks），并在训练中结合自循环（self-rollout）与KV缓存滚动（KV cache rolling）来模拟推理时固定上下文窗口的推断。", "result": "MotionStream实现了亚秒级延迟，单GPU上最高可达29 FPS的流式生成速度。其速度比现有技术快两个数量级，同时在运动跟踪和视频质量方面达到了最先进水平。该方法独特地实现了无限长度的流式视频生成，使用户能够实时绘制轨迹、控制摄像机或转移运动，提供真正的交互式体验。", "conclusion": "MotionStream通过创新的模型蒸馏和注意力机制设计，成功克服了现有视频生成方法的延迟和非因果处理限制，实现了高效、高质量、实时交互式的无限长度视频流生成，为用户带来了前所未有的体验。"}}
{"id": "2511.01295", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01295", "abs": "https://arxiv.org/abs/2511.01295", "authors": ["Feng Han", "Yibin Wang", "Chenglin Li", "Zheming Liang", "Dianyi Wang", "Yang Jiao", "Zhipeng Wei", "Chao Gong", "Cheng Jin", "Jingjing Chen", "Jiaqi Wang"], "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark", "comment": "Project page: https://maplebb.github.io/UniREditBench", "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.", "AI": {"tldr": "本文提出了UniREditBench，一个统一的推理图像编辑评估基准，包含真实世界和游戏世界场景，并引入多模态双参考评估。同时构建了大规模合成数据集UniREdit-Data-100K和改进模型UniREdit-Bagel，以系统评估和提升图像编辑模型的推理能力。", "motivation": "多模态生成模型在图像编辑方面取得进展，但仍难以处理需要隐式推理的复杂多样图像编辑任务。现有基准主要关注单对象属性转换，忽视了多对象交互和涉及人类定义规则的游戏世界场景，且仅依赖文本参考评估，可能导致复杂推理场景下的系统性误判。", "method": "1. 提出UniREditBench，一个统一的推理图像编辑评估基准，包含2,700个样本，覆盖8个主要维度和18个子维度，涉及真实世界和游戏世界场景。2. 引入多模态双参考评估，为每个样本提供文本和真实图像参考，以提高评估可靠性。3. 设计自动化多场景数据合成管道，构建大规模合成数据集UniREdit-Data-100K，包含高质量思维链(CoT)推理标注。4. 在UniREdit-Data-100K数据集上微调Bagel模型，开发UniREdit-Bagel。", "result": "1. UniREditBench提供了一个全面的基准，能够系统评估模型在各种推理场景下的性能。2. 多模态双参考评估显著提高了评估可靠性。3. UniREdit-Bagel在域内和域外设置中均表现出显著改进。4. 通过对开源和闭源图像编辑模型的全面基准测试，揭示了它们在各个方面的优缺点。", "conclusion": "UniREditBench提供了一个急需的、全面的推理图像编辑评估基准，能够更准确地评估和推动多模态生成模型在复杂推理任务中的发展。UniREdit-Data-100K和UniREdit-Bagel的创建为未来的研究提供了有价值的资源和改进的模型。"}}
{"id": "2511.01315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01315", "abs": "https://arxiv.org/abs/2511.01315", "authors": ["Jianfei Jiang", "Qiankun Liu", "Hongyuan Liu", "Haochen Yu", "Liyong Wang", "Jiansheng Chen", "Huimin Ma"], "title": "MVSMamba: Multi-View Stereo with State Space Model", "comment": "Accepted by NeurIPS 2025", "summary": "Robust feature representations are essential for learning-based Multi-View\nStereo (MVS), which relies on accurate feature matching. Recent MVS methods\nleverage Transformers to capture long-range dependencies based on local\nfeatures extracted by conventional feature pyramid networks. However, the\nquadratic complexity of Transformer-based MVS methods poses challenges to\nbalance performance and efficiency. Motivated by the global modeling capability\nand linear complexity of the Mamba architecture, we propose MVSMamba, the first\nMamba-based MVS network. MVSMamba enables efficient global feature aggregation\nwith minimal computational overhead. To fully exploit Mamba's potential in MVS,\nwe propose a Dynamic Mamba module (DM-module) based on a novel\nreference-centered dynamic scanning strategy, which enables: (1) Efficient\nintra- and inter-view feature interaction from the reference to source views,\n(2) Omnidirectional multi-view feature representations, and (3) Multi-scale\nglobal feature aggregation. Extensive experimental results demonstrate MVSMamba\noutperforms state-of-the-art MVS methods on the DTU dataset and the\nTanks-and-Temples benchmark with both superior performance and efficiency. The\nsource code is available at https://github.com/JianfeiJ/MVSMamba.", "AI": {"tldr": "本文提出了MVSMamba，首个基于Mamba架构的多视角立体视觉（MVS）网络，通过动态Mamba模块实现了高效的全局特征聚合，并在性能和效率上超越了现有先进方法。", "motivation": "传统的基于Transformer的MVS方法虽然能捕获长距离依赖，但其二次复杂度在性能和效率之间造成了挑战。Mamba架构具有全局建模能力和线性复杂度，因此研究者希望利用Mamba来解决这一问题。", "method": "提出了MVSMamba，这是首个基于Mamba的MVS网络，旨在实现高效的全局特征聚合。为了充分发挥Mamba在MVS中的潜力，设计了一个动态Mamba模块（DM-module），该模块基于新颖的参考中心动态扫描策略，实现了：1) 从参考视图到源视图的高效视图内和视图间特征交互；2) 全向多视图特征表示；3) 多尺度全局特征聚合。", "result": "MVSMamba在DTU数据集和Tanks-and-Temples基准测试上，在性能和效率方面均优于现有的最先进MVS方法。", "conclusion": "MVSMamba成功地利用Mamba架构实现了高效且高性能的多视图立体视觉，有效解决了Transformer基MVS方法的效率瓶颈，并取得了领先的实验结果。"}}
{"id": "2511.01317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01317", "abs": "https://arxiv.org/abs/2511.01317", "authors": ["Sampriti Soor", "Alik Pramanick", "Jothiprakash K", "Arijit Sur"], "title": "A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model", "comment": "18 pages, 3 figures", "summary": "The rapid growth of deep learning has brought about powerful models that can\nhandle various tasks, like identifying images and understanding language.\nHowever, adversarial attacks, an unnoticed alteration, can deceive models,\nleading to inaccurate predictions. In this paper, a generative adversarial\nattack method is proposed that uses the CLIP model to create highly effective\nand visually imperceptible adversarial perturbations. The CLIP model's ability\nto align text and image representation helps incorporate natural language\nsemantics with a guided loss to generate effective adversarial examples that\nlook identical to the original inputs. This integration allows extensive scene\nmanipulation, creating perturbations in multi-object environments specifically\ndesigned to deceive multilabel classifiers. Our approach integrates the\nconcentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with\nthe dissimilar text embeddings similar to Generative Adversarial Multi-Object\nScene Attacks (GAMA), resulting in perturbations that both deceive\nclassification models and maintain high structural similarity to the original\nimages. The model was tested on various tasks across diverse black-box victim\nmodels. The experimental results show that our method performs competitively,\nachieving comparable or superior results to existing techniques, while\npreserving greater visual fidelity.", "AI": {"tldr": "本文提出了一种基于CLIP模型的生成式对抗攻击方法，通过结合自然语言语义和集中扰动策略，生成对多标签分类器有效且视觉上难以察觉的对抗扰动，同时保持高视觉保真度。", "motivation": "深度学习模型虽然强大，但容易受到对抗性攻击的欺骗，导致不准确的预测。研究旨在开发一种能够生成高效且视觉上难以察觉的对抗扰动的方法。", "method": "该研究提出了一种生成式对抗攻击方法，利用CLIP模型对文本和图像表示的对齐能力，通过引导损失（guided loss）融入自然语言语义。此方法能够生成与原始输入视觉上几乎相同的对抗样本，并允许在多对象场景中进行广泛的扰动操作，以欺骗多标签分类器。它整合了来自Saliency-based Auto-Encoder (SSAE)的集中扰动策略和类似于Generative Adversarial Multi-Object Scene Attacks (GAMA)的不同文本嵌入。", "result": "实验结果表明，所提出的方法在多种任务和不同的黑盒受害模型上表现出竞争力，取得了与现有技术相当或更优异的结果。同时，它在保持高视觉保真度方面表现出色，生成的扰动既能欺骗分类模型，又能与原始图像保持高度结构相似性。", "conclusion": "该研究成功开发了一种新颖的生成式对抗攻击方法，利用CLIP模型和特定的扰动策略，能够生成高效且视觉上不易察觉的对抗样本，为对抗攻击领域提供了新的视角和更优的视觉保真度。"}}
{"id": "2511.01304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01304", "abs": "https://arxiv.org/abs/2511.01304", "authors": ["Chentao Li", "Behzad Bozorgtabar", "Yifang Ping", "Pan Huang", "Jing Qin"], "title": "Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation", "comment": "Our code is available at https://github.com/Prince-Lee-PathAI/PG-CIDL", "summary": "Multiple instance learning (MIL) has been widely used for representing\nwhole-slide pathology images. However, spatial, semantic, and decision\nentanglements among instances limit its representation and interpretability. To\naddress these challenges, we propose a latent factor grouping-boosted\ncluster-reasoning instance disentangled learning framework for whole-slide\nimage (WSI) interpretable representation in three phases. First, we introduce a\nnovel positive semi-definite latent factor grouping that maps instances into a\nlatent subspace, effectively mitigating spatial entanglement in MIL. To\nalleviate semantic entanglement, we employs instance probability counterfactual\ninference and optimization via cluster-reasoning instance disentangling.\nFinally, we employ a generalized linear weighted decision via instance effect\nre-weighting to address decision entanglement. Extensive experiments on\nmulticentre datasets demonstrate that our model outperforms all\nstate-of-the-art models. Moreover, it attains pathologist-aligned\ninterpretability through disentangled representations and a transparent\ndecision-making process.", "AI": {"tldr": "该论文提出了一种新的潜在因子分组增强的聚类推理实例解耦学习框架，用于全玻片图像（WSI）的可解释表示。该框架通过解决多实例学习（MIL）中的空间、语义和决策纠缠，实现了卓越的性能和与病理学家一致的可解释性。", "motivation": "多实例学习（MIL）在全玻片病理图像表示中广泛应用，但实例间的空间、语义和决策纠缠限制了其表示能力和可解释性。本研究旨在解决这些挑战。", "method": "本研究分三个阶段提出了一种潜在因子分组增强的聚类推理实例解耦学习框架：1) 引入正半定潜在因子分组，将实例映射到潜在子空间以缓解空间纠缠。2) 采用实例概率反事实推理和优化，通过聚类推理实例解耦来减轻语义纠缠。3) 利用实例效应再加权，通过广义线性加权决策来解决决策纠缠。", "result": "在多中心数据集上的广泛实验表明，所提出的模型优于所有最先进的模型。此外，它通过解耦表示和透明的决策过程，实现了与病理学家一致的可解释性。", "conclusion": "该研究提出的框架有效解决了MIL在WSI表示中存在的空间、语义和决策纠缠问题，不仅提升了模型性能，还实现了与病理诊断高度契合的可解释性。"}}
{"id": "2511.01399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01399", "abs": "https://arxiv.org/abs/2511.01399", "authors": ["Ya Wen", "Yutong Qiao", "Chi Chiu Lam", "Ioannis Brilakis", "Sanghoon Lee", "Mun On Wong"], "title": "Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction", "comment": null, "summary": "Inventory management of firefighting assets is crucial for emergency\npreparedness, risk assessment, and on-site fire response. However, conventional\nmethods are inefficient due to limited capabilities in automated asset\nrecognition and reconstruction. To address the challenge, this research\nintroduces the Fire-ART dataset and develops a panoramic image-based\nreconstruction approach for semantic enrichment of firefighting assets into BIM\nmodels. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626\nimages and 6,627 instances, making it an extensive and publicly accessible\ndataset for asset recognition. In addition, the reconstruction approach\nintegrates modified cube-map conversion and radius-based spherical camera\nprojection to enhance recognition and localization accuracy. Through\nvalidations with two real-world case studies, the proposed approach achieves\nF1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,\nrespectively. The Fire-ART dataset and the reconstruction approach offer\nvaluable resources and robust technical solutions to enhance the accurate\ndigital management of fire safety equipment.", "AI": {"tldr": "本研究提出了Fire-ART数据集和一种基于全景图像的重建方法，用于将消防资产的语义信息集成到BIM模型中，以提高消防设备数字管理的准确性。", "motivation": "消防资产的库存管理对于应急准备、风险评估和现场灭火响应至关重要。然而，传统方法由于在自动化资产识别和重建方面的能力有限而效率低下。", "method": "本研究引入了Fire-ART数据集（包含15种基本资产，2,626张图像和6,627个实例），并开发了一种基于全景图像的重建方法，将修改后的立方体贴图转换和基于半径的球形相机投影相结合，以增强识别和定位精度。", "result": "通过两个真实案例研究的验证，所提出的方法分别实现了73%和88%的F1分数，以及0.620米和0.428米的定位误差。", "conclusion": "Fire-ART数据集和重建方法为提高消防安全设备的精确数字管理提供了宝贵的资源和强大的技术解决方案。"}}
{"id": "2511.01293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01293", "abs": "https://arxiv.org/abs/2511.01293", "authors": ["Yonggang Zhang", "Jun Nie", "Xinmei Tian", "Mingming Gong", "Kun Zhang", "Bo Han"], "title": "Detecting Generated Images by Fitting Natural Image Distributions", "comment": "25 pages, 9 figures, NeurIPS 2025 spotlight", "summary": "The increasing realism of generated images has raised significant concerns\nabout their potential misuse, necessitating robust detection methods. Current\napproaches mainly rely on training binary classifiers, which depend heavily on\nthe quantity and quality of available generated images. In this work, we\npropose a novel framework that exploits geometric differences between the data\nmanifolds of natural and generated images. To exploit this difference, we\nemploy a pair of functions engineered to yield consistent outputs for natural\nimages but divergent outputs for generated ones, leveraging the property that\ntheir gradients reside in mutually orthogonal subspaces. This design enables a\nsimple yet effective detection method: an image is identified as generated if a\ntransformation along its data manifold induces a significant change in the loss\nvalue of a self-supervised model pre-trained on natural images. Further more,\nto address diminishing manifold disparities in advanced generative models, we\nleverage normalizing flows to amplify detectable differences by extruding\ngenerated images away from the natural image manifold. Extensive experiments\ndemonstrate the efficacy of this method. Code is available at\nhttps://github.com/tmlr-group/ConV.", "AI": {"tldr": "本文提出了一种新颖的生成图像检测框架，通过利用自然图像和生成图像数据流形之间的几何差异进行检测，该方法不依赖于二元分类器，并通过归一化流增强可检测差异。", "motivation": "生成图像的日益真实性引发了对其潜在滥用的担忧，需要鲁棒的检测方法。现有方法主要依赖于二元分类器训练，但其性能受限于可用生成图像的数量和质量。", "method": "该方法利用自然图像和生成图像数据流形之间的几何差异。它采用一对函数，对自然图像产生一致输出，对生成图像产生发散输出（其梯度位于相互正交的子空间中）。检测原理是：如果沿着数据流形进行变换后，预训练在自然图像上的自监督模型的损失值发生显著变化，则该图像被识别为生成图像。此外，为解决高级生成模型中流形差异减小的问题，该研究利用归一化流通过将生成图像“挤出”自然图像流形来放大可检测的差异。", "result": "广泛的实验证明了该方法的有效性。", "conclusion": "该研究提出了一种基于数据流形几何差异的简单而有效的生成图像检测方法，并通过归一化流解决了先进生成模型中流形差异减小的问题，展现了良好的检测性能。"}}
{"id": "2511.01345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01345", "abs": "https://arxiv.org/abs/2511.01345", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement", "comment": null, "summary": "Accurate segmentation of medical images is fundamental to tumor diagnosis and\ntreatment planning. SAM-based interactive segmentation has gained attention for\nits strong generalization, but most methods follow a\nsingle-point-to-single-object paradigm, which limits multi-lesion segmentation.\nMoreover, ViT backbones capture global context but often miss high-fidelity\nlocal details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework\nwith a competitive query optimization strategy that shifts from\nsingle-point-to-single-mask to single-point-to-multi-instance. A\nprompt-conditioned instance-query generator transforms a single point prompt\ninto multiple specialized queries, enabling retrieval of all semantically\nsimilar lesions across the 3D volume from a single exemplar. A hybrid\nCNN-Transformer encoder injects CNN-derived boundary saliency into ViT\nself-attention via spatial gating. A competitively optimized query decoder then\nenables end-to-end, parallel, multi-instance prediction through inter-query\ncompetition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels\nand exhibits strong robustness to prompts, providing a practical solution for\nefficient annotation of clinically relevant multi-lesion cases.", "AI": {"tldr": "该论文提出了MIQ-SAM3D，一个用于3D医学图像多实例分割的框架，通过竞争性查询优化策略，实现了从单点提示到多实例分割的转变，有效解决了现有SAM方法在多病灶分割上的局限性。", "motivation": "医学图像的准确分割对于肿瘤诊断和治疗计划至关重要。尽管基于SAM的交互式分割具有强大的泛化能力，但大多数方法遵循“单点对单对象”范式，限制了多病灶分割。此外，ViT骨干网络虽然能捕获全局上下文，但常缺失高保真度的局部细节。", "method": "MIQ-SAM3D框架采用竞争性查询优化策略，将“单点对单掩模”转变为“单点对多实例”。它包含一个提示条件实例查询生成器，将单个点提示转换为多个专业查询，从而从一个示例中检索3D体积中所有语义相似的病灶。一个混合CNN-Transformer编码器通过空间门控将CNN提取的边界显著性注入ViT自注意力中。最后，一个竞争性优化的查询解码器通过查询间竞争实现端到端、并行的多实例预测。", "result": "在LiTS17和KiTS21数据集上，MIQ-SAM3D取得了可比的分割水平，并对提示展现出强大的鲁棒性。", "conclusion": "MIQ-SAM3D为临床相关的多病灶病例的高效标注提供了一个实用的解决方案。"}}
{"id": "2511.01411", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01411", "abs": "https://arxiv.org/abs/2511.01411", "authors": ["Reza Karimzadeh", "Albert Alonso", "Frans Zdyb", "Julius B. Kirkegaard", "Bulat Ibragimov"], "title": "Extremal Contours: Gradient-driven contours for compact visual attribution", "comment": null, "summary": "Faithful yet compact explanations for vision models remain a challenge, as\ncommonly used dense perturbation masks are often fragmented and overfitted,\nneeding careful post-processing. Here, we present a training-free explanation\nmethod that replaces dense masks with smooth tunable contours. A star-convex\nregion is parameterized by a truncated Fourier series and optimized under an\nextremal preserve/delete objective using the classifier gradients. The approach\nguarantees a single, simply connected mask, cuts the number of free parameters\nby orders of magnitude, and yields stable boundary updates without cleanup.\nRestricting solutions to low-dimensional, smooth contours makes the method\nrobust to adversarial masking artifacts. On ImageNet classifiers, it matches\nthe extremal fidelity of dense masks while producing compact, interpretable\nregions with improved run-to-run consistency. Explicit area control also\nenables importance contour maps, yielding a transparent fidelity-area profiles.\nFinally, we extend the approach to multi-contour and show how it can localize\nmultiple objects within the same framework. Across benchmarks, the method\nachieves higher relevance mass and lower complexity than gradient and\nperturbation based baselines, with especially strong gains on self-supervised\nDINO models where it improves relevance mass by over 15% and maintains positive\nfaithfulness correlations.", "AI": {"tldr": "本文提出了一种无需训练的视觉模型解释方法，使用平滑可调的轮廓代替传统的密集扰动掩码，生成紧凑、可解释且忠实的解释区域。", "motivation": "现有视觉模型解释方法中，常用的密集扰动掩码通常碎片化、过拟合且需要仔细的后处理，难以实现忠实而紧凑的解释。", "method": "该方法通过截断傅里叶级数参数化星形凸区域，并利用分类器梯度在极值保留/删除目标下进行优化。它保证生成单一、简单连接的掩码，大幅减少自由参数数量，并提供稳定的边界更新。通过限制解为低维平滑轮廓，增强了对对抗性掩码伪影的鲁棒性。该方法还扩展到多轮廓，以定位多个对象。", "result": "在ImageNet分类器上，该方法实现了与密集掩码相当的极值保真度，同时生成了紧凑、可解释的区域，并提高了运行间的一致性。明确的区域控制能够生成重要性轮廓图。与基于梯度和扰动的方法相比，该方法实现了更高的相关性质量和更低的复杂性，在自监督DINO模型上尤其表现出色，将相关性质量提高了15%以上，并保持了正向的忠实度相关性。", "conclusion": "该方法提供了一种有效且无需训练的视觉模型解释方案，通过使用平滑可调的轮廓，解决了传统密集掩码的碎片化和过拟合问题，生成了更忠实、紧凑、可解释且一致性更高的解释，尤其适用于自监督模型。"}}
{"id": "2511.01419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01419", "abs": "https://arxiv.org/abs/2511.01419", "authors": ["Yongqi Yang", "Huayang Huang", "Xu Peng", "Xiaobin Hu", "Donghao Luo", "Jiangning Zhang", "Chengjie Wang", "Yu Wu"], "title": "Towards One-step Causal Video Generation via Adversarial Self-Distillation", "comment": "Under double-blind review as a conference paper", "summary": "Recent hybrid video generation models combine autoregressive temporal\ndynamics with diffusion-based spatial denoising, but their sequential,\niterative nature leads to error accumulation and long inference times. In this\nwork, we propose a distillation-based framework for efficient causal video\ngeneration that enables high-quality synthesis with extremely limited denoising\nsteps. Our approach builds upon the Distribution Matching Distillation (DMD)\nframework and proposes a novel Adversarial Self-Distillation (ASD) strategy,\nwhich aligns the outputs of the student model's n-step denoising process with\nits (n+1)-step version at the distribution level. This design provides smoother\nsupervision by bridging small intra-student gaps and more informative guidance\nby combining teacher knowledge with locally consistent student behavior,\nsubstantially improving training stability and generation quality in extremely\nfew-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame\nEnhancement (FFE) strategy, which allocates more denoising steps to the initial\nframes to mitigate error propagation while applying larger skipping steps to\nlater frames. Extensive experiments on VBench demonstrate that our method\nsurpasses state-of-the-art approaches in both one-step and two-step video\ngeneration. Notably, our framework produces a single distilled model that\nflexibly supports multiple inference-step settings, eliminating the need for\nrepeated re-distillation and enabling efficient, high-quality video synthesis.", "AI": {"tldr": "该研究提出了一种基于蒸馏的框架，通过对抗自蒸馏（ASD）和首帧增强（FFE）策略，实现了高效的因果视频生成，显著减少了去噪步骤，同时提高了生成质量和训练稳定性。", "motivation": "现有的混合视频生成模型结合自回归时间动态和基于扩散的空间去噪，但其顺序迭代性质导致误差累积和推理时间过长。", "method": "该方法建立在分布匹配蒸馏（DMD）框架之上，并提出：1) 对抗自蒸馏（ASD）策略，在分布层面将学生模型的n步去噪输出与其(n+1)步版本对齐，提供更平滑的监督和信息丰富的指导；2) 首帧增强（FFE）策略，为初始帧分配更多去噪步骤以减轻误差传播，同时对后续帧应用更大的跳过步骤。", "result": "在极少步（如1-2步）场景下，显著提高了训练稳定性和生成质量。在VBench上，该方法在一步和两步视频生成方面均超越了现有最先进的方法。此外，该框架生成了一个单一的蒸馏模型，灵活支持多种推理步长设置，无需重复再蒸馏。", "conclusion": "该框架通过创新的蒸馏策略，实现了高效、高质量的视频合成，显著减少了所需的去噪步骤，并提供了一个灵活支持多种推理设置的统一模型，有效解决了现有方法的误差累积和推理时间问题。"}}
{"id": "2511.01328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01328", "abs": "https://arxiv.org/abs/2511.01328", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation is essential for computer-assisted diagnosis and\ntreatment planning, yet substantial anatomical variability and boundary\nambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,\na segmentation network that unifies local modeling with global context to\nstrengthen boundary delineation and detail preservation. RDTE-UNet employs a\nhybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for\nadaptive boundary enhancement, HVDA for fine-grained feature modeling, and\nEulerFF for fusion weighting guided by Euler's formula. Together, these\ncomponents improve structural consistency and boundary accuracy across\nmorphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has\nachieved a comparable level in terms of segmentation accuracy and boundary\nquality.", "AI": {"tldr": "RDTE-UNet是一种结合局部建模和全局上下文的医学图像分割网络，旨在通过增强边界和保留细节来解决解剖变异性和边界模糊性问题。", "motivation": "医学图像分割对于计算机辅助诊断和治疗规划至关重要，但显著的解剖变异性和边界模糊性阻碍了对精细结构的可靠描绘。", "method": "本文提出了RDTE-UNet网络，它采用混合ResBlock细节感知Transformer骨干，并包含三个模块：ASBE用于自适应边界增强，HVDA用于细粒度特征建模，以及EulerFF用于由欧拉公式引导的融合加权。", "result": "在Synapse和BUSI数据集上，RDTE-UNet在分割精度和边界质量方面达到了可比的水平。", "conclusion": "RDTE-UNet通过统一局部建模和全局上下文，并结合其特有模块，提高了跨形态、方向和尺度的结构一致性和边界准确性。"}}
{"id": "2511.01434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01434", "abs": "https://arxiv.org/abs/2511.01434", "authors": ["Seongkyu Choi", "Jhonghyun An"], "title": "Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation", "comment": null, "summary": "Off-road semantic segmentation suffers from thick, inconsistent boundaries,\nsparse supervision for rare classes, and pervasive label noise. Designs that\nfuse only at low resolution blur edges and propagate local errors, whereas\nmaintaining high-resolution pathways or repeating high-resolution fusions is\ncostly and fragile to noise. We introduce a resolutionaware token decoder that\nbalances global semantics, local consistency, and boundary fidelity under\nimperfect supervision. Most computation occurs at a low-resolution bottleneck;\na gated cross-attention injects fine-scale detail, and only a sparse,\nuncertainty-selected set of pixels is refined. The components are co-designed\nand tightly integrated: global self-attention with lightweight dilated\ndepthwise refinement restores local coherence; a gated cross-attention\nintegrates fine-scale features from a standard high-resolution encoder stream\nwithout amplifying noise; and a class-aware point refinement corrects residual\nambiguities with negligible overhead. During training, we add a boundary-band\nconsistency regularizer that encourages coherent predictions in a thin\nneighborhood around annotated edges, with no inference-time cost. Overall, the\nresults indicate competitive performance and improved stability across\ntransitions.", "AI": {"tldr": "本文提出了一种分辨率感知的token解码器，用于解决越野语义分割中边界模糊、监督稀疏和标签噪声等问题，通过多尺度融合和精细化处理，实现了在不完美监督下的高精度和稳定性。", "motivation": "越野语义分割面临着边界粗糙不一致、稀有类别监督稀疏以及普遍存在的标签噪声等挑战。现有方法（如低分辨率融合）会模糊边缘并传播局部错误，而维持高分辨率路径或重复高分辨率融合则成本高昂且对噪声敏感。", "method": "本文引入了一个分辨率感知的token解码器。主要计算在低分辨率瓶颈中进行；通过门控交叉注意力注入精细尺度细节，并仅对稀疏、不确定性选择的像素集进行精修。该解码器集成了：带有轻量级空洞深度可分离精修的全局自注意力以恢复局部一致性；门控交叉注意力集成来自标准高分辨率编码器流的精细尺度特征而不放大噪声；以及类别感知的点精修以纠正残余模糊性。训练期间，添加了一个边界带一致性正则化器，以鼓励在标注边缘周围的薄区域内进行连贯预测，且推理时无额外成本。", "result": "实验结果表明，该方法在越野语义分割中取得了具有竞争力的性能，并在过渡区域表现出更高的稳定性。", "conclusion": "该分辨率感知的token解码器在不完美监督下，成功平衡了全局语义、局部一致性和边界保真度，有效解决了越野语义分割中的关键挑战，提高了分割的准确性和稳定性。"}}
{"id": "2511.01435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01435", "abs": "https://arxiv.org/abs/2511.01435", "authors": ["SiWoo Kim", "JhongHyun An"], "title": "Contrast-Guided Cross-Modal Distillation for Thermal Object Detection", "comment": null, "summary": "Robust perception at night remains challenging for thermal-infrared\ndetection: low contrast and weak high-frequency cues lead to duplicate,\noverlapping boxes, missed small objects, and class confusion. Prior remedies\neither translate TIR to RGB and hope pixel fidelity transfers to detection --\nmaking performance fragile to color or structure artifacts -- or fuse RGB and\nTIR at test time, which requires extra sensors, precise calibration, and higher\nruntime cost. Both lines can help in favorable conditions, but do not directly\nshape the thermal representation used by the detector. We keep mono-modality\ninference and tackle the root causes during training. Specifically, we\nintroduce training-only objectives that sharpen instance-level decision\nboundaries by pulling together features of the same class and pushing apart\nthose of different classes -- suppressing duplicate and confusing detections --\nand that inject cross-modal semantic priors by aligning the student's\nmulti-level pyramid features with an RGB-trained teacher, thereby strengthening\ntexture-poor thermal features without visible input at test time. In\nexperiments, our method outperformed prior approaches and achieved\nstate-of-the-art performance.", "AI": {"tldr": "本文提出一种仅在训练阶段使用的目标，通过锐化决策边界和注入跨模态语义先验（来自RGB训练的教师模型），在不依赖可见光输入的情况下，显著提升了热红外目标检测的鲁棒性。", "motivation": "夜间热红外感知面临低对比度、弱高频特征等挑战，导致重复/重叠检测框、漏检小目标和类别混淆。现有方法（TIR转RGB或RGB-TIR融合）存在性能脆弱、额外传感器、校准和高运行时成本等问题，且未能直接优化检测器使用的热表示。", "method": "该研究在单模态推理（仅热红外）框架下，引入了仅在训练阶段使用的目标：1) 锐化实例级决策边界，通过拉近同类特征、推开异类特征来抑制重复和混淆检测。2) 注入跨模态语义先验，通过将学生模型（热红外检测器）的多级金字塔特征与RGB训练的教师模型对齐，从而在测试时无需可见光输入即可强化纹理贫乏的热特征。", "result": "实验结果表明，该方法优于现有方法，并达到了最先进的性能。", "conclusion": "通过在训练阶段引入锐化决策边界和注入跨模态语义先验的目标，本文有效解决了热红外检测中的核心挑战，在不依赖可见光输入的情况下，实现了更鲁棒、更高性能的夜间目标感知。"}}
{"id": "2511.01466", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01466", "abs": "https://arxiv.org/abs/2511.01466", "authors": ["Changyuan Zhao", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Dong In Kim", "Ping Zhang"], "title": "SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks", "comment": "13 pages, 6 figures", "summary": "Deep joint source-channel coding (JSCC) has emerged as a promising paradigm\nfor semantic communication, delivering significant performance gains over\nconventional separate coding schemes. However, existing JSCC frameworks remain\nvulnerable to physical-layer adversarial threats, such as pilot spoofing and\nsubcarrier jamming, compromising semantic fidelity. In this paper, we propose\nSecDiff, a plug-and-play, diffusion-aided decoding framework that significantly\nenhances the security and robustness of deep JSCC under adversarial wireless\nenvironments. Different from prior diffusion-guided JSCC methods that suffer\nfrom high inference latency, SecDiff employs pseudoinverse-guided sampling and\nadaptive guidance weighting, enabling flexible step-size control and efficient\nsemantic reconstruction. To counter jamming attacks, we introduce a power-based\nsubcarrier masking strategy and recast recovery as a masked inpainting problem,\nsolved via diffusion guidance. For pilot spoofing, we formulate channel\nestimation as a blind inverse problem and develop an expectation-minimization\n(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and\na channel operator. Notably, our method alternates between pilot recovery and\nchannel estimation, enabling joint refinement of both variables throughout the\ndiffusion process. Extensive experiments over orthogonal frequency-division\nmultiplexing (OFDM) channels under adversarial conditions show that SecDiff\noutperforms existing secure and generative JSCC baselines by achieving a\nfavorable trade-off between reconstruction quality and computational cost. This\nbalance makes SecDiff a promising step toward practical, low-latency, and\nattack-resilient semantic communications.", "AI": {"tldr": "SecDiff是一种扩散辅助解码框架，用于增强深度联合源信道编码（JSCC）在对抗性无线环境（如干扰和导频欺骗）下的安全性与鲁棒性，同时实现低延迟和高效语义重建。", "motivation": "现有的深度JSCC框架容易受到物理层对抗性威胁（如导频欺骗和子载波干扰），从而损害语义保真度。", "method": "SecDiff是一个即插即用的扩散辅助解码框架。它采用伪逆引导采样和自适应引导加权，以实现灵活的步长控制和高效的语义重建，避免了高推理延迟。为对抗干扰攻击，引入了基于功率的子载波掩蔽策略，并将恢复重构为掩蔽修复问题，通过扩散引导解决。为应对导频欺骗，将信道估计表述为盲逆问题，并开发了一种期望最大化（EM）驱动的重构算法，由重构损失和信道算子共同引导。该方法在扩散过程中交替进行导频恢复和信道估计，实现两者的联合细化。", "result": "在对抗性OFDM信道上的大量实验表明，SecDiff优于现有安全和生成式JSCC基线，在重建质量和计算成本之间取得了有利的平衡。", "conclusion": "SecDiff为实现实用、低延迟和抗攻击的语义通信迈出了重要一步。"}}
{"id": "2511.01355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01355", "abs": "https://arxiv.org/abs/2511.01355", "authors": ["Linhao Huang"], "title": "Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion", "comment": null, "summary": "Recent advancements in text-to-image diffusion models have significantly\nimproved the personalization and stylization of generated images. However,\nprevious studies have only assessed content similarity under a single style\nintensity. In our experiments, we observe that increasing style intensity leads\nto a significant loss of content features, resulting in a suboptimal\ncontent-style frontier. To address this, we propose a novel approach to expand\nthe content-style frontier by leveraging Content-Style Subspace Blending and a\nContent-Style Balance loss. Our method improves content similarity across\nvarying style intensities, significantly broadening the content-style frontier.\nExtensive experiments demonstrate that our approach outperforms existing\ntechniques in both qualitative and quantitative evaluations, achieving superior\ncontent-style trade-off with significantly lower Inverted Generational Distance\n(IGD) and Generational Distance (GD) scores compared to current methods.", "AI": {"tldr": "本文提出了一种通过内容-风格子空间融合和内容-风格平衡损失来扩展内容-风格边界的新方法，以解决文本到图像扩散模型中高风格强度下内容特征丢失的问题，从而在不同风格强度下提高内容相似性。", "motivation": "现有的文本到图像扩散模型在评估内容相似性时，只考虑了单一风格强度。研究发现，增加风格强度会导致内容特征显著丢失，从而产生次优的内容-风格边界。", "method": "本文提出了一种新方法，通过利用内容-风格子空间融合（Content-Style Subspace Blending）和内容-风格平衡损失（Content-Style Balance loss）来扩展内容-风格边界。", "result": "该方法改善了不同风格强度下的内容相似性，显著拓宽了内容-风格边界。大量的实验表明，该方法在定性和定量评估中均优于现有技术，实现了卓越的内容-风格权衡，并且与当前方法相比，Inverted Generational Distance (IGD) 和 Generational Distance (GD) 分数显著降低。", "conclusion": "所提出的方法通过有效解决高风格强度下的内容丢失问题，实现了在文本到图像生成中更优越的内容-风格权衡，显著扩展了内容-风格边界。"}}
{"id": "2511.01498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01498", "abs": "https://arxiv.org/abs/2511.01498", "authors": ["Zhiyang Jia", "Hongyan Cui", "Ge Gao", "Bo Li", "Minjie Zhang", "Zishuo Gao", "Huiwen Huang", "Caisheng Zhuo"], "title": "EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance", "comment": "12 page, 5 figures", "summary": "Person re-identification (ReID) plays a pivotal role in computer vision,\nparticularly in surveillance and security applications within IoT-enabled smart\nenvironments. This study introduces the Enhanced Pedestrian Alignment Network\n(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.\nEPAN employs a dual-branch architecture to mitigate the impact of perspective\nand environmental changes, extracting alignment information under varying\nscales and viewpoints. Here, we demonstrate EPAN's strong feature extraction\ncapabilities, achieving outstanding performance on the Inspection-Personnel\ndataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of\n78.82%. This highlights EPAN's potential for real-world IoT applications,\nenabling effective and reliable person ReID across diverse cameras in\nsurveillance and security systems. The code and data are available at:\nhttps://github.com/ggboy2580/EPAN", "AI": {"tldr": "本研究提出了增强行人对齐网络（EPAN），用于在多样化的物联网监控条件下进行鲁棒的行人重识别。", "motivation": "行人重识别在计算机视觉，尤其是在物联网智能环境中的监控和安全应用中扮演着关键角色。", "method": "该研究引入了增强行人对齐网络（EPAN），采用双分支架构来减轻视角和环境变化的影响，并在不同尺度和视角下提取对齐信息。", "result": "EPAN在Inspection-Personnel数据集上表现出色，Rank-1准确率达到90.09%，平均精度（mAP）达到78.82%。", "conclusion": "EPAN在现实世界的物联网应用中具有巨大潜力，能够实现跨不同摄像头的有效可靠的行人重识别。"}}
{"id": "2511.01510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01510", "abs": "https://arxiv.org/abs/2511.01510", "authors": ["Derong Kong", "Zhixiong Yang", "Shengxi Li", "Shuaifeng Zhi", "Li Liu", "Zhen Liu", "Jingyuan Xia"], "title": "Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement", "comment": "Accepted at NeurIPS 2025", "summary": "Low-light image enhancement (LLIE) faces persistent challenges in balancing\nreconstruction fidelity with cross-scenario generalization. While existing\nmethods predominantly focus on deterministic pixel-level mappings between\npaired low/normal-light images, they often neglect the continuous physical\nprocess of luminance transitions in real-world environments, leading to\nperformance drop when normal-light references are unavailable. Inspired by\nempirical analysis of natural luminance dynamics revealing power-law\ndistributed intensity transitions, this paper introduces Luminance-Aware\nStatistical Quantification (LASQ), a novel framework that reformulates LLIE as\na statistical sampling process over hierarchical luminance distributions. Our\nLASQ re-conceptualizes luminance transition as a power-law distribution in\nintensity coordinate space that can be approximated by stratified power\nfunctions, therefore, replacing deterministic mappings with probabilistic\nsampling over continuous luminance layers. A diffusion forward process is\ndesigned to autonomously discover optimal transition paths between luminance\nlayers, achieving unsupervised distribution emulation without normal-light\nreferences. In this way, it considerably improves the performance in practical\nsituations, enabling more adaptable and versatile light restoration. This\nframework is also readily applicable to cases with normal-light references,\nwhere it achieves superior performance on domain-specific datasets alongside\nbetter generalization-ability across non-reference datasets.", "AI": {"tldr": "本文提出了一种名为Luminance-Aware Statistical Quantification (LASQ) 的新型低光图像增强框架，将LLIE重新定义为分层亮度分布上的统计采样过程，灵感来源于自然亮度动态的幂律分布，显著提升了无正常光参考情况下的泛化能力和性能。", "motivation": "现有低光图像增强方法难以平衡重建保真度和跨场景泛化能力，主要关注确定性的像素级映射，却忽视了真实环境中亮度转换的连续物理过程，导致在缺乏正常光参考时性能下降。", "method": "LASQ框架将LLIE重新定义为分层亮度分布上的统计采样过程。它将亮度转换概念化为强度坐标空间中的幂律分布，该分布可通过分层幂函数近似，从而用连续亮度层上的概率采样取代确定性映射。此外，设计了一个扩散前向过程，自主发现亮度层之间的最佳转换路径，实现无正常光参考的无监督分布仿真。", "result": "LASQ显著提升了在实际应用中的性能，实现了更具适应性和通用性的光线恢复，尤其在无正常光参考的情况下。在有正常光参考的情况下，该框架在特定领域数据集上表现优异，并在非参考数据集上展现出更好的泛化能力。", "conclusion": "LASQ通过将低光图像增强重新定义为基于亮度统计采样的过程，并利用幂律分布模拟亮度转换，克服了现有方法在保真度、泛化能力和无参考场景下的局限性，提供了一种更具适应性和通用性的光线恢复解决方案。"}}
{"id": "2511.01513", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.01513", "abs": "https://arxiv.org/abs/2511.01513", "authors": ["Andrei-Timotei Ardelean", "Tim Weyrich"], "title": "Example-Based Feature Painting on Textures", "comment": "\"\\c{opyright} 2025 Andrei-Timotei Ardelean, Tim Weyrich. This is the\n  author's version of the work. It is posted here for your personal use. Not\n  for redistribution. The definitive Version of Record was published in ACM\n  Trans. Graph., Vol. 44, No. 6, https://doi.org/10.1145/3763301", "summary": "In this work, we propose a system that covers the complete workflow for\nachieving controlled authoring and editing of textures that present distinctive\nlocal characteristics. These include various effects that change the surface\nappearance of materials, such as stains, tears, holes, abrasions,\ndiscoloration, and more. Such alterations are ubiquitous in nature, and\nincluding them in the synthesis process is crucial for generating realistic\ntextures. We introduce a novel approach for creating textures with such\nblemishes, adopting a learning-based approach that leverages unlabeled\nexamples. Our approach does not require manual annotations by the user;\ninstead, it detects the appearance-altering features through unsupervised\nanomaly detection. The various textural features are then automatically\nclustered into semantically coherent groups, which are used to guide the\nconditional generation of images. Our pipeline as a whole goes from a small\nimage collection to a versatile generative model that enables the user to\ninteractively create and paint features on textures of arbitrary size. Notably,\nthe algorithms we introduce for diffusion-based editing and infinite stationary\ntexture generation are generic and should prove useful in other contexts as\nwell. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html", "AI": {"tldr": "该研究提出一个完整的系统，利用无监督学习方法，实现对具有局部特征（如污渍、磨损等）纹理的受控创作和编辑，无需手动标注。", "motivation": "在纹理合成过程中，包含自然界中普遍存在的表面改变（如污渍、撕裂、孔洞、磨损、变色等）对于生成逼真的纹理至关重要。", "method": "该方法采用基于学习的途径，利用无标签示例，通过无监督异常检测来识别改变外观的特征。这些纹理特征随后被自动聚类成语义连贯的组，用于指导图像的条件生成。整个流程从少量图像集合到多功能生成模型，并引入了基于扩散的编辑和无限平稳纹理生成算法。", "result": "该系统使用户能够交互式地在任意尺寸的纹理上创建和绘制特征，无需手动标注。它能自动检测并聚类纹理特征，生成具有受控局部特性的逼真纹理。所引入的扩散编辑和无限平稳纹理生成算法具有通用性，可应用于其他场景。", "conclusion": "该工作提供了一个完整的、基于无监督学习的纹理创作和编辑工作流程，能够处理具有局部特征的纹理。它生成了一个多功能的生成模型，支持用户交互式创建纹理，并且其核心算法具有广泛的应用潜力。"}}
{"id": "2511.01549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01549", "abs": "https://arxiv.org/abs/2511.01549", "authors": ["Mikhail Konov", "Lion J. Gleiter", "Khoa Co", "Monica Yabal", "Tingying Peng"], "title": "NOA: a versatile, extensible tool for AI-based organoid analysis", "comment": null, "summary": "AI tools can greatly enhance the analysis of organoid microscopy images, from\ndetection and segmentation to feature extraction and classification. However,\ntheir limited accessibility to biologists without programming experience\nremains a major barrier, resulting in labor-intensive and largely manual\nworkflows. Although a few AI models for organoid analysis have been developed,\nmost existing tools remain narrowly focused on specific tasks. In this work, we\nintroduce the Napari Organoid Analyzer (NOA), a general purpose graphical user\ninterface to simplify AI-based organoid analysis. NOA integrates modules for\ndetection, segmentation, tracking, feature extraction, custom feature\nannotation and ML-based feature prediction. It interfaces multiple\nstate-of-the-art algorithms and is implemented as an open-source napari plugin\nfor maximal flexibility and extensibility. We demonstrate the versatility of\nNOA through three case studies, involving the quantification of morphological\nchanges during organoid differentiation, assessment of phototoxicity effects,\nand prediction of organoid viability and differentiation state. Together, these\nexamples illustrate how NOA enables comprehensive, AI-driven organoid image\nanalysis within an accessible and extensible framework.", "AI": {"tldr": "本文介绍了一种名为Napari Organoid Analyzer (NOA) 的通用图形用户界面工具，旨在简化生物学家进行AI驱动的类器官图像分析，使其无需编程经验即可使用。", "motivation": "AI工具能显著提升类器官显微图像分析效率，但其对缺乏编程经验的生物学家而言可及性有限，导致工作流程仍以手动为主且劳动密集。此外，现有的大多数AI工具都过于专注于特定任务。", "method": "研究人员开发了Napari Organoid Analyzer (NOA)，这是一个通用的图形用户界面（GUI）。NOA集成了检测、分割、跟踪、特征提取、自定义特征标注和基于机器学习的特征预测等模块。它接口了多种最先进的算法，并作为开源的napari插件实现，以提供最大的灵活性和可扩展性。", "result": "通过三个案例研究，NOA展示了其多功能性：量化类器官分化过程中的形态变化、评估光毒性效应以及预测类器官的活力和分化状态。这些案例表明NOA能够在一个易于访问和可扩展的框架内实现全面的AI驱动类器官图像分析。", "conclusion": "NOA提供了一个可访问且可扩展的框架，使生物学家能够进行全面的AI驱动类器官图像分析，从而克服了现有AI工具在可及性和通用性方面的限制。"}}
{"id": "2511.01574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01574", "abs": "https://arxiv.org/abs/2511.01574", "authors": ["Md Sumon Ali", "Muzammil Behzad"], "title": "Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images", "comment": "9 pagers, 8 Figures", "summary": "Compared to traditional methods, Deep Learning (DL) becomes a key technology\nfor computer vision tasks. Synthetic data generation is an interesting use case\nfor DL, especially in the field of medical imaging such as Magnetic Resonance\nImaging (MRI). The need for this task since the original MRI data is limited.\nThe generation of realistic medical images is completely difficult and\nchallenging. Generative Adversarial Networks (GANs) are useful for creating\nsynthetic medical images. In this paper, we propose a DL based methodology for\ncreating synthetic MRI data using the Deep Convolutional Generative Adversarial\nNetwork (DC-GAN) to address the problem of limited data. We also employ a\nConvolutional Neural Network (CNN) classifier to classify the brain tumor using\nsynthetic data and real MRI data. CNN is used to evaluate the quality and\nutility of the synthetic images. The classification result demonstrates\ncomparable performance on real and synthetic images, which validates the\neffectiveness of GAN-generated images for downstream tasks.", "AI": {"tldr": "本文提出一种基于深度学习（DC-GAN）的方法来生成合成MRI数据，以解决原始数据有限的问题，并通过CNN分类器验证了合成数据在脑肿瘤分类任务中的有效性。", "motivation": "原始MRI数据量有限，且生成逼真的医学图像具有挑战性，这限制了计算机视觉在医学影像领域的应用。", "method": "采用深度卷积生成对抗网络（DC-GAN）来生成合成MRI数据。同时，使用卷积神经网络（CNN）分类器对真实和合成MRI数据进行脑肿瘤分类，以评估合成图像的质量和实用性。", "result": "分类结果表明，在真实图像和合成图像上取得了可比较的性能。", "conclusion": "GAN生成的图像对于下游任务（如脑肿瘤分类）是有效的，这验证了其质量和实用性。"}}
{"id": "2511.01517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01517", "abs": "https://arxiv.org/abs/2511.01517", "authors": ["Serkan Ozturk", "Samet Hicsonmez", "Pinar Duygulu"], "title": "NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation", "comment": "Under review", "summary": "Current text conditioned image generation methods output realistic looking\nimages, but they fail to capture specific styles. Simply finetuning them on the\ntarget style datasets still struggles to grasp the style features. In this\nwork, we present a novel contrastive learning framework to improve the\nstylization capability of large text-to-image diffusion models. Motivated by\nthe astonishing advance in image generation models that makes synthetic data an\nintrinsic part of model training in various computer vision tasks, we exploit\nsynthetic image generation in our approach. Usually, the generated synthetic\ndata is dependent on the task, and most of the time it is used to enlarge the\navailable real training dataset. With NSYNC, alternatively, we focus on\ngenerating negative synthetic sets to be used in a novel contrastive training\nscheme along with real positive images. In our proposed training setup, we\nforward negative data along with positive data and obtain negative and positive\ngradients, respectively. We then refine the positive gradient by subtracting\nits projection onto the negative gradient to get the orthogonal component,\nbased on which the parameters are updated. This orthogonal component eliminates\nthe trivial attributes that are present in both positive and negative data and\ndirects the model towards capturing a more unique style. Experiments on various\nstyles of painters and illustrators show that our approach improves the\nperformance over the baseline methods both quantitatively and qualitatively.\nOur code is available at https://github.com/giddyyupp/NSYNC.", "AI": {"tldr": "该论文提出了一种名为NSYNC的新型对比学习框架，通过生成负合成数据集并利用梯度正交化来训练大型文本到图像扩散模型，以显著提高其捕捉特定风格的能力。", "motivation": "当前的文本条件图像生成方法虽然能输出逼真的图像，但在捕捉特定风格方面表现不佳。即使在目标风格数据集上进行微调，也难以有效学习风格特征。", "method": "本研究引入了一个新颖的对比学习框架。其核心是生成“负合成数据集”，并将其与真实的正向图像一起用于对比训练。在训练过程中，模型会分别获取正向和负向梯度。然后，通过将正向梯度减去其在负向梯度上的投影，得到一个正交分量。模型参数基于这个正交分量进行更新，以消除正负数据中都存在的无关属性，从而促使模型学习更独特的风格特征。", "result": "在各种画家和插画师风格上的实验表明，该方法在定量和定性两方面都优于基线方法，有效提升了模型的风格化性能。", "conclusion": "通过引入负合成数据集和独特的梯度正交化策略，所提出的NSYNC框架能够显著增强大型文本到图像扩散模型捕捉和生成特定艺术风格的能力，解决了现有方法在风格化方面的不足。"}}
{"id": "2511.01600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01600", "abs": "https://arxiv.org/abs/2511.01600", "authors": ["Agnar Martin Bjørnstad", "Elias Stenhede", "Arian Ranjbar"], "title": "Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography", "comment": null, "summary": "Accurate tumor size measurement is a cornerstone of evaluating cancer\ntreatment response. The most widely adopted standard for this purpose is the\nResponse Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on\nmeasuring the longest tumor diameter in a single plane. However, volumetric\nmeasurements have been shown to provide a more reliable assessment of treatment\neffect. Their clinical adoption has been limited, though, due to the\nlabor-intensive nature of manual volumetric annotation. In this paper, we\npresent Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed\nfor efficient volumetric tumor segmentation from CT scans annotated with RECIST\nannotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:\nPan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice\nSimilarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of\n63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an\naverage inference time of 14.4 s on CPU on the public validation dataset.", "AI": {"tldr": "本文介绍了一种名为 Lite ENSAM 的轻量级模型，用于从带有 RECIST 标注的 CT 扫描中进行高效的体积肿瘤分割，旨在解决手动标注耗时和 RECIST 测量局限性的问题。", "motivation": "评估癌症治疗反应时，肿瘤大小测量至关重要。RECIST v1.1 是广泛采用的标准，但依赖于单一平面测量，而体积测量被认为能提供更可靠的治疗效果评估。然而，手动体积标注的劳动密集性限制了其临床应用。", "method": "本文提出 Lite ENSAM，它是 ENSAM 架构的轻量级改编版本，专为从带有 RECIST 标注的 CT 扫描中进行高效体积肿瘤分割而设计。", "result": "Lite ENSAM 在 MICCAI FLARE 2025 任务 1、子任务 2 的隐藏测试集上，取得了 60.7% 的 Dice 相似系数（DSC）和 63.6% 的归一化表面 Dice（NSD）。在公共验证数据集上，平均总 RAM 时间为 50.6 GBs，CPU 平均推理时间为 14.4 s。", "conclusion": "Lite ENSAM 提供了一种高效且性能良好的方法，能够从带有 RECIST 标注的 CT 扫描中进行体积肿瘤分割，有望克服手动体积标注的局限性并改进治疗反应评估。"}}
{"id": "2511.01593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01593", "abs": "https://arxiv.org/abs/2511.01593", "authors": ["Yizhu Chen", "Chen Ju", "Zhicheng Wang", "Shuai Xiao", "Xu Chen", "Jinsong Lan", "Xiaoyong Zhu", "Ying Chen"], "title": "Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation", "comment": null, "summary": "The unification of understanding and generation within a single multi-modal\nlarge model (MLLM) remains one significant challenge, largely due to the\ndichotomy between continuous and discrete visual tokenizations. Continuous\ntokenizer (CT) achieves strong performance by bridging multiple\nindependently-trained understanding modules and generation modules, but suffers\nfrom complex multi-stage pipelines and substantial engineering overhead.\nConversely, discrete tokenizers (DT) offer a conceptually elegant idea by\nquantizing each image into a primitive, but inevitably leading to information\nloss and performance degradation. To resolve this tension, we question the\nbinary choice between CT and DT, inspired by the wave-particle duality of\nlight, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).\nWe treat visual data as a flexible composition of image primitives derived from\nquantized codebooks, with the crucial insight that the primitive number\nassigned to each visual sample is adaptively determined according to its\ncomplexity: simple instances use a few primitives, emulating discrete\ntokenization, while complex instances use many, approximating continuous\ntokenization. Two core components are designed: Diverse Quantitative\nPrimitives, which encourage primitives orthogonality to better populate\ninformation space, and Dynamic Primitive Allocator, which assesses sample\ncomplexity to determine the optimal set of primitives. Extensive experiments on\nreconstruction, retrieval and classification show that CDD-VT achieves superior\nperformance over to specialized CT and DT, effectively getting strong result\nwithin a concise and scalable MLLM.", "AI": {"tldr": "本文提出了一种名为CDD-VT的连续-离散双重视觉分词器，它融合了连续和离散分词的优点，通过自适应地分配视觉基元数量来处理不同复杂度的视觉数据，以解决多模态大模型中理解与生成统一的挑战。", "motivation": "多模态大模型(MLLM)中理解与生成难以统一，主要原因在于连续和离散视觉分词的二元性。连续分词器(CT)性能强但管线复杂、工程开销大；离散分词器(DT)概念优雅但信息损失大、性能下降。本文旨在解决CT和DT之间的矛盾，寻找一种更好的视觉分词方法。", "method": "受波粒二象性启发，本文提出了连续-离散双重视觉分词器(CDD-VT)。它将视觉数据视为量化码本中图像基元的灵活组合，并根据视觉样本的复杂度自适应地确定所需的基元数量：简单实例使用少量基元（模拟离散分词），复杂实例使用大量基元（近似连续分词）。CDD-VT包含两个核心组件：多样化量化基元（鼓励基元正交性以更好地填充信息空间）和动态基元分配器（评估样本复杂度以确定最佳基元集）。", "result": "在重建、检索和分类任务上的广泛实验表明，CDD-VT的性能优于专门的CT和DT方法。它能在简洁且可扩展的MLLM中有效获得强大的结果。", "conclusion": "CDD-VT通过结合连续和离散视觉分词的优势，有效地解决了两者之间的矛盾，为实现多模态大模型中理解与生成的统一提供了一种更优、更高效的视觉分词方法。"}}
{"id": "2511.01613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01613", "abs": "https://arxiv.org/abs/2511.01613", "authors": ["Tomáš Krsička", "Tibor Kubík"], "title": "Benchmark-Ready 3D Anatomical Shape Classification", "comment": "Shape in Medical Imaging, ShapeMI 2025, Held in Conjunction with\n  MICCAI 2025", "summary": "Progress in anatomical 3D shape classification is limited by the complexity\nof mesh data and the lack of standardized benchmarks, highlighting the need for\nrobust learning methods and reproducible evaluation. We introduce two key steps\ntoward clinically and benchmark-ready anatomical shape classification via\nself-supervised graph autoencoding. We propose Precomputed Structural Pooling\n(PSPooling), a non-learnable mesh pooling operator designed for efficient and\nstructure-preserving graph coarsening in 3D anatomical shape analysis.\nPSPooling precomputes node correspondence sets based on geometric proximity,\nenabling parallelizable and reversible pooling and unpooling operations with\nguaranteed support structure. This design avoids the sparsity and\nreconstruction issues of selection-based methods and the sequential overhead of\nedge contraction approaches, making it particularly suitable for\nhigh-resolution medical meshes. To demonstrate its effectiveness, we integrate\nPSPooling into a self-supervised graph autoencoder that learns anatomy-aware\nrepresentations from unlabeled surface meshes. We evaluate the downstream\nbenefits on MedShapeNet19, a new curated benchmark dataset we derive from\nMedShapeNet, consisting of 19 anatomical classes with standardized training,\nvalidation, and test splits. Experiments show that PSPooling significantly\nimproves reconstruction fidelity and classification accuracy in low-label\nregimes, establishing a strong baseline for medical 3D shape learning. We hope\nthat MedShapeNet19 will serve as a widely adopted benchmark for anatomical\nshape classification and further research in medical 3D shape analysis. Access\nthe complete codebase, model weights, and dataset information here:\nhttps://github.com/TomasKrsicka/MedShapeNet19-PSPooling.", "AI": {"tldr": "本文提出了一种名为PSPooling的非学习型网格池化操作符，用于高效且结构保留的3D解剖形状分类，并引入了新的基准数据集MedShapeNet19。PSPooling显著提高了低标签情况下的重建保真度和分类准确性。", "motivation": "解剖学3D形状分类的进展受限于网格数据的复杂性和标准化基准的缺乏，这凸显了对鲁棒学习方法和可复现评估的需求。", "method": "本文提出了预计算结构池化（PSPooling），这是一种非学习型网格池化操作符，通过预计算基于几何邻近度的节点对应关系，实现高效且结构保留的图粗化，支持并行化和可逆的池化/反池化操作。PSPooling被整合到一个自监督图自编码器中，用于从无标签表面网格学习解剖学感知表示。此外，本文还推出了一个新的基准数据集MedShapeNet19，该数据集从MedShapeNet中整理而来，包含19个解剖学类别，并提供了标准化的训练、验证和测试划分。", "result": "实验结果表明，PSPooling显著提高了重建保真度和分类准确性，尤其是在低标签制度下，为医学3D形状学习建立了一个强大的基线。", "conclusion": "PSPooling为医学3D形状学习提供了一种有效的解决方案，并且MedShapeNet19有望成为解剖形状分类和医学3D形状分析领域广泛采用的基准。"}}
{"id": "2511.01546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01546", "abs": "https://arxiv.org/abs/2511.01546", "authors": ["Ge Gao", "Zishuo Gao", "Hongyan Cui", "Zhiyang Jia", "Zhuang Luo", "ChaoPeng Liu"], "title": "PCD-ReID: Occluded Person Re-Identification for Base Station Inspection", "comment": "11 pages, 7 figures", "summary": "Occluded pedestrian re-identification (ReID) in base station environments is\na critical task in computer vision, particularly for surveillance and security\napplications. This task faces numerous challenges, as occlusions often obscure\nkey body features, increasing the complexity of identification. Traditional\nResNet-based ReID algorithms often fail to address occlusions effectively,\nnecessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component\nDiscrepancy) algorithm to address these issues. The contributions of this work\nare as follows: To tackle the occlusion problem, we design a Transformer-based\nPCD network capable of extracting shared component features, such as helmets\nand uniforms. To mitigate overfitting on public datasets, we collected new\nreal-world patrol surveillance images for model training, covering six months,\n10,000 individuals, and over 50,000 images. Comparative experiments with\nexisting ReID algorithms demonstrate that our model achieves a mean Average\nPrecision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1\nimprovement over ResNet50-based methods. Experimental evaluations indicate that\nPCD-ReID effectively achieves occlusion-aware ReID performance for personnel in\ntower inspection scenarios, highlighting its potential for practical deployment\nin surveillance and security applications.", "AI": {"tldr": "本文提出PCD-ReID算法，利用Transformer网络提取共享组件特征，并结合大规模真实世界数据集，显著提升了基站环境下遮挡行人重识别的性能。", "motivation": "基站环境下遮挡行人重识别是计算机视觉领域的关键任务，但遮挡物常掩盖行人关键身体特征，导致识别复杂性高。传统基于ResNet的重识别算法难以有效处理遮挡问题，因此需要新的重识别方法。", "method": "本文提出了PCD-ReID（行人组件差异）算法。为解决遮挡问题，设计了一个基于Transformer的PCD网络，能够提取头盔、制服等共享组件特征。为缓解在公共数据集上的过拟合问题，研究人员收集了包含六个月、10,000名个体和超过50,000张图像的真实世界巡逻监控新数据集用于模型训练。", "result": "PCD-ReID模型在平均精度均值（mAP）上达到了79.0%，Rank-1准确率达到82.7%。相较于基于ResNet50的方法，Rank-1准确率提高了15.9%。", "conclusion": "实验评估表明，PCD-ReID算法在塔台巡检场景中有效实现了遮挡感知行人重识别性能，凸显了其在监控和安保应用中的实际部署潜力。"}}
{"id": "2511.01704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01704", "abs": "https://arxiv.org/abs/2511.01704", "authors": ["Xin Qiao", "Matteo Poggi", "Xing Wei", "Pengchao Deng", "Yanhui Zhou", "Stefano Mattoccia"], "title": "Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond", "comment": null, "summary": "Under-display ToF imaging aims to achieve accurate depth sensing through a\nToF camera placed beneath a screen panel. However, transparent OLED (TOLED)\nlayers introduce severe degradations-such as signal attenuation, multi-path\ninterference (MPI), and temporal noise-that significantly compromise depth\nquality. To alleviate this drawback, we propose Learnable Fractional\nReaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the\nexpressive power of neural networks with the interpretability of physical\nmodeling. Specifically, we implement a time-fractional reaction-diffusion\nmodule that enables iterative depth refinement with dynamically generated\ndifferential orders, capturing long-term dependencies. In addition, we\nintroduce an efficient continuous convolution operator via coefficient\nprediction and repeated differentiation to further improve restoration quality.\nExperiments on four benchmark datasets demonstrate the effectiveness of our\napproach. The code is publicly available at https://github.com/wudiqx106/LFRD2.", "AI": {"tldr": "本文提出了一种名为LFRD2的混合框架，结合神经网络和物理模型，以解决屏下ToF成像中透明OLED层引起的深度质量下降问题。", "motivation": "屏下ToF成像在透明OLED层下会遇到信号衰减、多径干扰和时间噪声等严重退化，显著损害深度感知质量。因此，需要一种方法来缓解这些问题。", "method": "本文提出了可学习分数阶反应-扩散动力学（LFRD2）框架。该框架包含一个时间分数阶反应-扩散模块，通过动态生成的微分阶数实现迭代深度优化，以捕获长期依赖性。此外，还引入了一种通过系数预测和重复微分实现的有效连续卷积算子，以进一步提高恢复质量。", "result": "在四个基准数据集上的实验证明了该方法的有效性。", "conclusion": "LFRD2框架通过结合神经网络的表达能力和物理模型的可解释性，成功地减轻了屏下ToF成像中由透明OLED层引起的深度退化问题，提升了深度感知质量。"}}
{"id": "2511.01645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01645", "abs": "https://arxiv.org/abs/2511.01645", "authors": ["Xiaogang Xu", "Ruihang Chu", "Jian Wang", "Kun Zhou", "Wenjie Shu", "Harry Yang", "Ser-Nam Lim", "Hao Chen", "Liang Lin"], "title": "Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward", "comment": null, "summary": "Reinforcement Learning (RL) has recently been incorporated into diffusion\nmodels, e.g., tasks such as text-to-image. However, directly applying existing\nRL methods to diffusion-based image restoration models is suboptimal, as the\nobjective of restoration fundamentally differs from that of pure generation: it\nplaces greater emphasis on fidelity. In this paper, we investigate how to\neffectively integrate RL into diffusion-based restoration models. First,\nthrough extensive experiments with various reward functions, we find that an\neffective reward can be derived from an Image Quality Assessment (IQA) model,\ninstead of intuitive ground-truth-based supervision, which has already been\noptimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,\nour strategy focuses on using RL for challenging samples that are significantly\ndistant from the ground truth, and our RL approach is innovatively implemented\nusing MLLM-based IQA models to align distributions with high-quality images\ninitially. As the samples approach the ground truth's distribution, RL is\nadaptively combined with SFT for more fine-grained alignment. This dynamic\nprocess is facilitated through an automatic weighting strategy that adjusts\nbased on the relative difficulty of the training samples. Our strategy is\nplug-and-play that can be seamlessly applied to diffusion-based restoration\nmodels, boosting its performance across various restoration tasks. Extensive\nexperiments across multiple benchmarks demonstrate the effectiveness of our\nproposed RL framework.", "AI": {"tldr": "本文提出了一种将强化学习（RL）有效集成到基于扩散的图像修复模型中的策略，通过使用基于IQA的奖励函数和自适应的RL与SFT结合方法，以提高修复质量并强调保真度。", "motivation": "将现有强化学习方法直接应用于基于扩散的图像修复模型效果不佳，因为修复任务的目标与纯生成不同，更侧重于图像保真度。", "method": "1. 发现有效的奖励函数可从图像质量评估（IQA）模型中导出，而非已在SFT阶段优化的基于真实值的监督。2. 策略专注于对远离真实值的挑战性样本使用RL。3. RL方法创新性地使用基于MLLM的IQA模型进行初始分布对齐。4. 随着样本接近真实值分布，RL与SFT自适应结合进行更精细的对齐。5. 通过基于训练样本相对难度的自动加权策略促进动态过程。该策略是即插即用的。", "result": "广泛的实验表明，所提出的RL框架能够提高各种修复任务的性能，并且在多个基准测试中证明了其有效性。", "conclusion": "本研究提供了一个有效且即插即用的RL框架，能无缝应用于基于扩散的图像修复模型，通过强调保真度和自适应学习策略显著提升性能。"}}
{"id": "2511.01617", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.01617", "abs": "https://arxiv.org/abs/2511.01617", "authors": ["Mohamed Eltahir", "Ali Habibullah", "Lama Ayash", "Tanveer Hussain", "Naeemullah Khan"], "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers", "comment": null, "summary": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is\na long-standing challenge, particularly for complex, multi-modal data such as\nvideos. While typical fusion techniques are training-free, they rely solely on\nrank or score signals, disregarding candidates' representations. This work\nintroduces Vote-in-Context (ViC), a generalized, training-free framework that\nre-thinks list-wise reranking and fusion as a zero-shot reasoning task for a\nVision-Language Model (VLM). The core insight is to serialize both content\nevidence and retriever metadata directly within the VLM's prompt, allowing the\nmodel to adaptively weigh retriever consensus against visual-linguistic\ncontent. We demonstrate the generality of this framework by applying it to the\nchallenging domain of cross-modal video retrieval. To this end, we introduce\nthe S-Grid, a compact serialization map that represents each video as an image\ngrid, optionally paired with subtitles to enable list-wise reasoning over video\ncandidates. ViC is evaluated both as a single-list reranker, where it\ndramatically improves the precision of individual retrievers, and as an\nensemble fuser, where it consistently outperforms strong baselines like\nCombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the\nframework establishes new state-of-the-art zero-shot retrieval performance,\ndemonstrating its effectiveness in handling complex visual and temporal signals\nalongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%\n(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive\ngains of up to +40 Recall@1 over previous state-of-the-art baselines. We\npresent ViC as a simple, reproducible, and highly effective recipe for turning\nmodern VLMs into powerful zero-shot rerankers and fusers. Code and resources\nare publicly available at: https://github.com/mohammad2012191/ViC", "AI": {"tldr": "本文提出Vote-in-Context (ViC)，一个通用的、免训练的框架，将列表级重排序和融合重新构想为视觉-语言模型（VLM）的零样本推理任务。通过在VLM提示中序列化内容证据和检索器元数据，ViC在跨模态视频检索中实现了最先进的零样本性能。", "motivation": "异构检索器候选融合是一个长期存在的挑战，尤其对于视频等多模态复杂数据。典型的融合技术虽然免训练，但仅依赖排序或分数信号，忽略了候选的表征信息。", "method": "ViC框架将列表级重排序和融合视为VLM的零样本推理任务。其核心思想是将内容证据（如将视频表示为S-Grid图像网格，可选配字幕）和检索器元数据直接序列化到VLM的提示中，使模型能够自适应地权衡检索器共识与视觉-语言内容。该框架被应用于跨模态视频检索领域。", "result": "ViC作为单列表重排序器时，显著提高了单个检索器的精度；作为集成融合器时，持续优于CombSUM等强基线。在ActivityNet和VATEX等视频检索基准上，ViC建立了新的零样本检索性能SOTA，在MSR-VTT上Recall@1得分达到87.1% (t2v) / 89.0% (v2t)，在VATEX上达到99.6% (v2t)，比之前SOTA基线提升高达+40 Recall@1。", "conclusion": "ViC提供了一个简单、可复现且高效的方法，将现代VLM转化为强大的零样本重排序器和融合器，有效处理复杂的视觉、时间信号和文本。"}}
{"id": "2511.01698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01698", "abs": "https://arxiv.org/abs/2511.01698", "authors": ["Yuhang Kang", "Ziyu Su", "Tianyang Wang", "Zaibo Li", "Wei Chen", "Muhammad Khalid Khan Niazi"], "title": "Progressive Translation of H&E to IHC with Enhanced Structural Fidelity", "comment": null, "summary": "Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not\nonly maintains the structural features of tissue samples, but also provides\nhigh-resolution protein localization, which is essential for aiding in\npathology diagnosis. Despite its diagnostic value, IHC remains a costly and\nlabor-intensive technique. Its limited scalability and constraints in\nmultiplexing further hinder widespread adoption, especially in resource-limited\nsettings. Consequently, researchers are increasingly exploring computational\nstain translation techniques to synthesize IHC-equivalent images from\nH&E-stained slides, aiming to extract protein-level information more\nefficiently and cost-effectively. However, most existing stain translation\ntechniques rely on a linearly weighted summation of multiple loss terms within\na single objective function, strategy that often overlooks the interdepedence\namong these components-resulting in suboptimal image quality and an inability\nto simultaneously preserve structural authenticity and color fidelity. To\naddress this limitation, we propose a novel network architecture that follows a\nprogressive structure, incorporating color and cell border generation logic,\nwhich enables each visual aspect to be optimized in a stage-wise and decoupled\nmanner. To validate the effectiveness of our proposed network architecture, we\nbuild upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We\nintroduce additional loss functions based on 3,3'-diaminobenzidine (DAB)\nchromogen concentration and image gradient, enhancing color fidelity and cell\nboundary clarity in the generated IHC images. By reconstructing the generation\npipeline using our structure-color-cell boundary progressive mechanism,\nexperiments on HER2 and ER datasets demonstrated that the model significantly\nimproved visual quality and achieved finer structural details.", "AI": {"tldr": "本文提出了一种新颖的渐进式网络架构，用于从H&E染色图像合成IHC等效图像，通过阶段性解耦优化结构、颜色和细胞边界，显著提高了合成图像的视觉质量和细节。", "motivation": "免疫组化（IHC）对于病理诊断至关重要，但其成本高昂、劳动密集、可扩展性差且多重染色受限。现有的计算染色转换技术虽然旨在提高效率，但通常依赖于线性加权损失函数，忽略了组件间的相互依赖性，导致图像质量欠佳，无法同时保持结构真实性和颜色保真度。", "method": "研究者提出了一种遵循渐进式结构的新型网络架构，融合了颜色和细胞边界生成逻辑，使每个视觉方面都能以阶段性、解耦的方式进行优化。该方法以自适应监督PatchNCE（ASP）框架为基线，并引入了基于3,3'-二氨基联苯胺（DAB）生色团浓度和图像梯度的额外损失函数，以增强生成IHC图像的颜色保真度和细胞边界清晰度。通过“结构-颜色-细胞边界”渐进机制重建了生成管道。", "result": "在HER2和ER数据集上的实验表明，该模型显著改善了视觉质量，并实现了更精细的结构细节。", "conclusion": "所提出的渐进式网络架构，通过解耦优化结构、颜色和细胞边界，并结合改进的损失函数，有效解决了现有染色转换技术在图像质量和细节方面的局限性，能够生成更高质量的合成IHC图像。"}}
{"id": "2511.01724", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01724", "abs": "https://arxiv.org/abs/2511.01724", "authors": ["Yi Zhang", "Zheng Wang", "Chen Zhen", "Wenjie Ruan", "Qing Guo", "Siddartha Khastgir", "Carsten Maple", "Xingyu Zhao"], "title": "Probabilistic Robustness for Free? Revisiting Training via a Benchmark", "comment": null, "summary": "Deep learning models are notoriously vulnerable to imperceptible\nperturbations. Most existing research centers on adversarial robustness (AR),\nwhich evaluates models under worst-case scenarios by examining the existence of\ndeterministic adversarial examples (AEs). In contrast, probabilistic robustness\n(PR) adopts a statistical perspective, measuring the probability that\npredictions remain correct under stochastic perturbations. While PR is widely\nregarded as a practical complement to AR, dedicated training methods for\nimproving PR are still relatively underexplored, albeit with emerging progress.\nAmong the few PR-targeted training methods, we identify three limitations: i\nnon-comparable evaluation protocols; ii limited comparisons to strong AT\nbaselines despite anecdotal PR gains from AT; and iii no unified framework to\ncompare the generalization of these methods. Thus, we introduce PRBench, the\nfirst benchmark dedicated to evaluating improvements in PR achieved by\ndifferent robustness training methods. PRBench empirically compares most common\nAT and PR-targeted training methods using a comprehensive set of metrics,\nincluding clean accuracy, PR and AR performance, training efficiency, and\ngeneralization error (GE). We also provide theoretical analysis on the GE of PR\nperformance across different training methods. Main findings revealed by\nPRBench include: AT methods are more versatile than PR-targeted training\nmethods in terms of improving both AR and PR performance across diverse\nhyperparameter settings, while PR-targeted training methods consistently yield\nlower GE and higher clean accuracy. A leaderboard comprising 222 trained models\nacross 7 datasets and 10 model architectures is publicly available at\nhttps://tmpspace.github.io/PRBenchLeaderboard/.", "AI": {"tldr": "本文介绍了PRBench，一个用于评估不同鲁棒性训练方法在概率鲁棒性（PR）方面的改进的基准，并比较了对抗鲁棒性（AR）和PR特定训练方法的性能。", "motivation": "尽管概率鲁棒性（PR）被认为是对抗鲁棒性（AR）的实用补充，但专门用于提高PR的训练方法仍未得到充分探索。现有的PR特定训练方法存在三个局限性：评估协议不具可比性、与强大的对抗训练（AT）基线缺乏比较，以及缺乏统一的框架来比较这些方法的泛化能力。", "method": "本文引入了PRBench，第一个专门用于评估不同鲁棒性训练方法在PR方面改进的基准。PRBench通过一系列综合指标（包括干净准确率、PR和AR性能、训练效率和泛化误差）实证比较了最常见的AT和PR特定训练方法。此外，还对不同训练方法下PR性能的泛化误差进行了理论分析。", "result": "PRBench的主要发现包括：AT方法在改善AR和PR性能方面比PR特定训练方法更具通用性，适用于不同的超参数设置；而PR特定训练方法则持续产生更低的泛化误差和更高的干净准确率。研究还公开了一个包含222个训练模型在7个数据集和10种模型架构上的排行榜。", "conclusion": "PRBench提供了一个统一的框架来评估和比较不同的鲁棒性训练方法在概率鲁棒性方面的表现。研究结果揭示了对抗训练方法在通用性方面的优势，以及PR特定训练方法在泛化误差和干净准确率方面的优势，为未来的鲁棒性研究提供了宝贵的见解和基准。"}}
{"id": "2511.01768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01768", "abs": "https://arxiv.org/abs/2511.01768", "authors": ["Zhe Liu", "Jinghua Hou", "Xiaoqing Ye", "Jingdong Wang", "Hengshuang Zhao", "Xiang Bai"], "title": "UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs", "comment": null, "summary": "Although transformers have demonstrated remarkable capabilities across\nvarious domains, their quadratic attention mechanisms introduce significant\ncomputational overhead when processing long-sequence data. In this paper, we\npresent a unified autonomous driving model, UniLION, which efficiently handles\nlarge-scale LiDAR point clouds, high-resolution multi-view images, and even\ntemporal sequences based on the linear group RNN operator (i.e., performs\nlinear RNN for grouped features). Remarkably, UniLION serves as a single\nversatile architecture that can seamlessly support multiple specialized\nvariants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal\ntemporal fusion configurations) without requiring explicit temporal or\nmulti-modal fusion modules. Moreover, UniLION consistently delivers competitive\nand even state-of-the-art performance across a wide range of core tasks,\nincluding 3D perception (e.g., 3D object detection, 3D object tracking, 3D\noccupancy prediction, BEV map segmentation), prediction (e.g., motion\nprediction), and planning (e.g., end-to-end planning). This unified paradigm\nnaturally simplifies the design of multi-modal and multi-task autonomous\ndriving systems while maintaining superior performance. Ultimately, we hope\nUniLION offers a fresh perspective on the development of 3D foundation models\nin autonomous driving. Code is available at\nhttps://github.com/happinesslz/UniLION", "AI": {"tldr": "UniLION是一个统一的自动驾驶模型，它通过线性分组RNN操作符高效处理长序列多模态数据（LiDAR、图像、时间序列），实现了多种感知、预测和规划任务的领先性能。", "motivation": "Transformer模型在处理长序列数据时，其二次注意力机制会引入显著的计算开销。自动驾驶系统需要高效处理大规模LiDAR点云、高分辨率多视图图像乃至时间序列数据。", "method": "UniLION采用线性分组RNN操作符（对分组特征执行线性RNN），构建了一个统一的架构。该架构能够无缝支持多种专用变体（仅LiDAR、时间LiDAR、多模态、多模态时间融合），无需显式的时间或多模态融合模块。", "result": "UniLION在广泛的核心任务上（包括3D感知、预测和规划）始终提供具有竞争力乃至最先进的性能，例如3D目标检测、跟踪、占用预测、BEV地图分割、运动预测和端到端规划。", "conclusion": "UniLION的统一范式自然地简化了多模态和多任务自动驾驶系统的设计，同时保持了卓越的性能，并为自动驾驶领域的3D基础模型开发提供了新的视角。"}}
{"id": "2511.01728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01728", "abs": "https://arxiv.org/abs/2511.01728", "authors": ["Tom Odem"], "title": "Toward Strategy Identification and Subtask Decomposition In Task Exploration", "comment": null, "summary": "This research builds on work in anticipatory human-machine interaction, a\nsubfield of human-machine interaction where machines can facilitate\nadvantageous interactions by anticipating a user's future state. The aim of\nthis research is to further a machine's understanding of user knowledge, skill,\nand behavior in pursuit of implicit coordination. A task explorer pipeline was\ndeveloped that uses clustering techniques, paired with factor analysis and\nstring edit distance, to automatically identify key global and local strategies\nthat are used to complete tasks. Global strategies identify generalized sets of\nactions used to complete tasks, while local strategies identify sequences that\nused those sets of actions in a similar composition. Additionally, meaningful\nsubtasks of various lengths are identified within the tasks. The task explorer\npipeline was able to automatically identify key strategies used to complete\ntasks and encode user runs with hierarchical subtask structures. In addition, a\nTask Explorer application was developed to easily review pipeline results. The\ntask explorer pipeline can be easily modified to any action-based time-series\ndata and the identified strategies and subtasks help to inform humans and\nmachines on user knowledge, skill, and behavior.", "AI": {"tldr": "本研究开发了一个任务探索管道，用于自动识别用户完成任务的关键全局和局部策略，以及有意义的子任务，以增进机器对用户知识、技能和行为的理解，从而实现隐式人机协调。", "motivation": "本研究的动机是推动机器在预期人机交互中对用户知识、技能和行为的理解，以便实现隐式协调，从而促进机器通过预测用户未来状态来优化交互。", "method": "开发了一个“任务探索管道”，该管道结合了聚类技术、因子分析和字符串编辑距离，以自动识别用户完成任务时使用的关键全局和局部策略。全局策略识别通用的动作集合，而局部策略识别以相似构成使用这些动作集合的序列。此外，该管道还能识别任务中各种长度的有意义的子任务。还开发了一个任务探索应用程序以方便审查管道结果。", "result": "任务探索管道能够自动识别完成任务所使用的关键策略，并用分层子任务结构编码用户运行。此外，还开发了一个任务探索应用程序，可以轻松审查管道结果。", "conclusion": "该任务探索管道可以轻松修改以适用于任何基于动作的时间序列数据，并且所识别的策略和子任务有助于人机了解用户的知识、技能和行为。"}}
{"id": "2511.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01756", "abs": "https://arxiv.org/abs/2511.01756", "authors": ["Kai Zhai", "Ziyan Huang", "Qiang Nie", "Xiang Li", "Bo Ouyang"], "title": "HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain", "comment": null, "summary": "2D-to-3D human pose lifting is a fundamental challenge for 3D human pose\nestimation in monocular video, where graph convolutional networks (GCNs) and\nattention mechanisms have proven to be inherently suitable for encoding the\nspatial-temporal correlations of skeletal joints. However, depth ambiguity and\nerrors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous\nstudies have attempted to restrict jitters in the time domain, for instance, by\nconstraining the differences between adjacent frames while neglecting the\nglobal spatial-temporal correlations of skeletal joint motion. To tackle this\nproblem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid\nfeature aggregation and 3D trajectory consistency in the frequency domain.\nSpecifically, we propose a hop-hybrid graph attention (HGA) module and a\nTransformer encoder to model global joint spatial-temporal correlations. The\nHGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group\nto enlarge the receptive field and applies the attention mechanism to discover\nthe latent correlations of these groups globally. We then exploit global\ntemporal correlations by constraining trajectory consistency in the frequency\ndomain. To provide 3D information for depth inference across frames and\nmaintain coherence over time, a preliminary network is applied to estimate the\n3D pose. Extensive experiments were conducted on two standard benchmark\ndatasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed\nHGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional\naccuracy and temporal consistency.", "AI": {"tldr": "本文提出HGFreNet，一种新型GraphFormer架构，用于单目视频中的2D到3D人体姿态提升。它通过跳跃混合特征聚合和频域3D轨迹一致性，有效解决了深度模糊和2D姿态估计错误导致的3D轨迹不连贯问题，提高了姿态估计的准确性和时间一致性。", "motivation": "单目视频中的2D到3D人体姿态提升面临深度模糊和2D姿态估计错误导致3D轨迹不连贯的挑战。现有方法通常只限制相邻帧之间的抖动，而忽略了骨骼关节运动的全局时空相关性。", "method": "本文设计了HGFreNet，一种结合跳跃混合特征聚合和频域3D轨迹一致性的新型GraphFormer架构。具体方法包括：\n1.  提出跳跃混合图注意力（HGA）模块，将骨骼关节的所有k跳邻居分组，以扩大感受野并发现这些组的潜在全局相关性。\n2.  使用Transformer编码器来建模全局关节时空相关性。\n3.  通过在频域约束轨迹一致性来利用全局时间相关性。\n4.  应用一个初步网络来估计3D姿态，为跨帧深度推断提供3D信息并保持时间连贯性。", "result": "在Human3.6M和MPI-INF-3DHP两个标准基准数据集上进行的广泛实验表明，所提出的HGFreNet在位置准确性和时间一致性方面均优于现有最先进（SOTA）方法。", "conclusion": "HGFreNet通过创新的跳跃混合图注意力机制和频域轨迹一致性约束，成功解决了2D到3D人体姿态提升中的3D轨迹不连贯问题，实现了卓越的姿态准确性和时间一致性，超越了现有SOTA方法。"}}
{"id": "2511.01678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01678", "abs": "https://arxiv.org/abs/2511.01678", "authors": ["Ropeway Liu", "Hangjie Yuan", "Bo Dong", "Jiazheng Xing", "Jinwang Wang", "Rui Zhao", "Yan Xing", "Weihua Chen", "Fan Wang"], "title": "UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback", "comment": "NeurIPS 2025", "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.", "AI": {"tldr": "UniLumos是一个统一的图像和视频重新打光框架，通过引入RGB空间几何反馈和路径一致性学习，显著提高了打光结果的物理真实感和计算效率，并提供了细粒度控制和评估基准。", "motivation": "现有的扩散模型在语义潜在空间中进行优化，导致重新打光结果常出现不真实现象，如过曝高光、阴影错位和遮挡错误，缺乏物理正确性。此外，高质量视觉空间监督计算成本高昂。", "method": "本文提出了UniLumos框架，将RGB空间几何反馈引入流匹配骨干网络。通过从模型输出中提取深度和法线图进行监督，显式地将打光效果与场景结构对齐，提高物理合理性。为解决多步去噪的计算开销，采用了路径一致性学习，使其在少步训练下仍能有效监督。为实现细粒度控制和监督，设计了一个结构化的六维标注协议。在此基础上，提出了LumosBench，一个解耦的属性级别基准，通过大型视觉语言模型评估打光可控性。", "result": "UniLumos在图像和视频重新打光方面均实现了最先进的质量，显著改善了物理一致性，同时实现了20倍的速度提升。", "conclusion": "UniLumos通过结合RGB空间几何反馈和高效的路径一致性学习，成功克服了现有扩散模型在重新打光中物理不准确和计算效率低的问题，为图像和视频打光提供了高质量、高效率且可控的解决方案。"}}
{"id": "2511.01817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01817", "abs": "https://arxiv.org/abs/2511.01817", "authors": ["Sagi Eppel", "Alona Strugatski"], "title": "SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art", "comment": null, "summary": "The ability to connect visual patterns with the processes that form them\nrepresents one of the deepest forms of visual understanding. Textures of clouds\nand waves, the growth of cities and forests, or the formation of materials and\nlandscapes are all examples of patterns emerging from underlying mechanisms. We\npresent the Scitextures dataset, a large-scale collection of textures and\nvisual patterns from all domains of science, tech, and art, along with the\nmodels and code that generate these images. Covering over 1,200 different\nmodels and 100,000 images of patterns and textures from physics, chemistry,\nbiology, sociology, technology, mathematics, and art, this dataset offers a way\nto explore the connection between the visual patterns that shape our world and\nthe mechanisms that produce them. Created by an agentic AI pipeline that\nautonomously collects and implements models in standardized form, we use\nSciTextures to evaluate the ability of leading AI models to link visual\npatterns to the models and code that generate them, and to identify different\npatterns that emerged from the same process. We also test AIs ability to infer\nand recreate the mechanisms behind visual patterns by providing a natural image\nof a real-world pattern and asking the AI to identify, model, and code the\nmechanism that formed the pattern, then run this code to generate a simulated\nimage that is compared to the real image. These benchmarks show that\nvision-language models (VLMs) can understand and simulate the physical system\nbeyond a visual pattern. The dataset and code are available at:\nhttps://zenodo.org/records/17485502", "AI": {"tldr": "该研究推出了Scitextures数据集，一个包含大量科学、技术和艺术领域纹理及生成模型的集合，用于评估AI连接视觉模式与其生成机制的能力，并发现领先的视觉-语言模型（VLMs）在此方面表现出色。", "motivation": "将视觉模式与其形成过程联系起来是视觉理解的深层形式。云、波浪、城市、森林的生长以及材料和景观的形成都体现了从底层机制中出现的模式。研究旨在探索和评估AI理解这种模式与机制之间联系的能力。", "method": "研究方法包括：1. 创建Scitextures数据集：一个大规模的纹理和视觉模式集合，包含来自物理、化学、生物、社会学、技术、数学和艺术等领域的1200多个不同模型和100,000张图像，以及生成这些图像的模型和代码。2. 数据集创建：通过一个自主收集和标准化实现模型的智能AI管道生成。3. 评估AI能力：使用Scitextures数据集评估领先的AI模型（特别是视觉-语言模型VLMs）连接视觉模式到其生成模型和代码的能力，识别源自相同过程的不同模式，以及从真实世界模式的自然图像中推断、建模和重现其形成机制的能力。", "result": "基准测试结果表明，视觉-语言模型（VLMs）不仅能理解视觉模式本身，还能理解并模拟模式背后的物理系统。AI能够从自然图像中识别、建模和编码形成模式的机制，并生成与真实图像相似的模拟图像。", "conclusion": "Scitextures数据集为探索塑造世界的视觉模式与产生它们的机制之间的联系提供了途径。研究证明，视觉-语言模型在理解和模拟超越单纯视觉模式的物理系统方面具有显著能力，为AI在科学理解领域的应用开辟了新的可能性。"}}
{"id": "2511.01833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01833", "abs": "https://arxiv.org/abs/2511.01833", "authors": ["Ming Li", "Jike Zhong", "Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Yuxiang Lai", "Wei Chen", "Konstantinos Psounis", "Kaipeng Zhang"], "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning", "comment": "Preprint", "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-\\textit{with}-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-\\textit{with}-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce \\textbf{TIR-Bench}, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.", "AI": {"tldr": "该研究引入了TIR-Bench，一个用于评估多模态大语言模型（MLLMs）高级视觉推理（“图像思考”）能力的综合基准，发现其对现有模型具有普遍挑战性，并需要真正的工具使用能力。", "motivation": "现有基准，包括常见的视觉搜索，未能充分捕捉到像OpenAI o3这类模型所具备的、通过智能创建和操作工具来转换图像以解决问题的能力（即链式思维中的“图像思考”）。这些基准仅测试基础操作，无法深入了解更复杂、动态且依赖工具的推理。", "method": "研究引入了TIR-Bench，一个包含13个多样化任务的综合基准，每个任务都要求在链式思维中进行新颖的工具使用，以进行图像处理和操作。研究评估了22个多模态大语言模型，包括领先的开源模型、专有模型以及明确增强了工具使用能力的模型。此外，还进行了一项关于直接微调与智能体微调的初步研究。", "result": "评估结果表明，TIR-Bench对所有测试的多模态大语言模型都具有普遍挑战性。要获得强大性能，需要模型具备真正的“图像思考”能力。", "conclusion": "TIR-Bench是一个有效的基准，能够评估多模态大语言模型在复杂图像处理和操作中结合工具使用的“图像思考”能力，并揭示了现有模型在这方面的不足。初步研究也为未来的模型微调方向提供了见解。"}}
{"id": "2511.01802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01802", "abs": "https://arxiv.org/abs/2511.01802", "authors": ["Tejas Sarnaik", "Manan Shah", "Ravi Hegde"], "title": "PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution", "comment": "Accepted in PReMI 2025", "summary": "Retrieval-Augmented Generation (RAG) has become a robust framework for\nenhancing Large Language Models (LLMs) with external knowledge. Recent advances\nin RAG have investigated graph based retrieval for intricate reasoning;\nhowever, the influence of prompt design on enhancing the retrieval and\nreasoning process is still considerably under-examined. In this paper, we\npresent a prompt-driven GraphRAG framework that underscores the significance of\nprompt formulation in facilitating entity extraction, fact selection, and\npassage reranking for multi-hop question answering. Our approach creates a\nsymbolic knowledge graph from text data by encoding entities and factual\nrelationships as structured facts triples. We use LLMs selectively during\nonline retrieval to perform semantic filtering and answer generation. We also\nuse entity-guided graph traversal through Personalized PageRank (PPR) to\nsupport efficient, scalable retrieval based on the knowledge graph we built.\nOur system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,\nwith F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,\nrespectively. These results show that prompt design is an important part of\nimproving retrieval accuracy and response quality. This research lays the\ngroundwork for more efficient and comprehensible multi-hop question-answering\nsystems, highlighting the importance of prompt-aware graph reasoning.", "AI": {"tldr": "本文提出了一种提示驱动的GraphRAG框架，通过强调提示设计在实体提取、事实选择和段落重排中的作用，显著提升了多跳问答（QA）的性能。", "motivation": "尽管RAG结合图检索已用于复杂推理，但提示设计对检索和推理过程的影响仍未得到充分研究。", "method": "该方法构建了一个提示驱动的GraphRAG框架，利用提示公式来促进实体提取、事实选择和段落重排。它从文本数据中创建符号知识图谱（将实体和事实关系编码为结构化事实三元组）。在在线检索过程中，选择性地使用大型语言模型（LLMs）进行语义过滤和答案生成。同时，通过个性化PageRank（PPR）进行实体引导的图遍历，以支持高效、可扩展的检索。", "result": "该系统在HotpotQA和2WikiMultiHopQA数据集上取得了最先进的性能，F1分数分别为80.7%和78.9%，Recall@5分数分别为97.1%和98.1%。结果表明，提示设计是提高检索准确性和响应质量的重要组成部分。", "conclusion": "提示设计对于提高检索准确性和响应质量至关重要。这项研究为更高效、更易理解的多跳问答系统奠定了基础，并强调了提示感知图推理的重要性。"}}
{"id": "2511.01730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01730", "abs": "https://arxiv.org/abs/2511.01730", "authors": ["Yefeng Wu", "Yucheng Song", "Ling Wu", "Shan Wan", "Yecheng Zhao"], "title": "CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays", "comment": null, "summary": "Pneumonia remains a leading cause of morbidity and mortality worldwide,\nnecessitating accurate and efficient automated detection systems. While recent\ntransformer-based detectors like RT-DETR have shown promise in object detection\ntasks, their application to medical imaging, particularly pneumonia detection\nin chest X-rays, remains underexplored. This paper presents CGF-DETR, an\nenhanced real-time detection transformer specifically designed for pneumonia\ndetection. We introduce XFABlock in the backbone to improve multi-scale feature\nextraction through convolutional attention mechanisms integrated with CSP\narchitecture. To achieve efficient feature aggregation, we propose SPGA module\nthat replaces standard multi-head attention with dynamic gating mechanisms and\nsingle-head self-attention. Additionally, GCFC3 is designed for the neck to\nenhance feature representation through multi-path convolution fusion while\nmaintaining real-time performance via structural re-parameterization. Extensive\nexperiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR\nachieves 82.2\\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\\% while\nmaintaining comparable inference speed at 48.1 FPS. Our ablation studies\nconfirm that each proposed module contributes meaningfully to the overall\nperformance improvement, with the complete model achieving 50.4\\%\nmAP@[0.5:0.95]", "AI": {"tldr": "本文提出了CGF-DETR，一种增强型实时检测Transformer模型，通过引入XFABlock、SPGA模块和GCFC3模块，显著提升了胸部X光片肺炎检测的准确性，同时保持了实时推理速度。", "motivation": "肺炎是全球发病率和死亡率的主要原因，需要准确高效的自动化检测系统。尽管RT-DETR等基于Transformer的检测器在目标检测任务中表现出色，但它们在医学影像，特别是胸部X光片肺炎检测中的应用仍未得到充分探索。", "method": "本文提出了CGF-DETR，一个专门为肺炎检测设计的增强型实时检测Transformer。其主要方法包括：1. 在骨干网络中引入XFABlock，通过卷积注意力机制结合CSP架构改进多尺度特征提取。2. 提出SPGA模块，用动态门控机制和单头自注意力取代标准多头注意力，以实现高效特征聚合。3. 在Neck部分设计GCFC3，通过多路径卷积融合和结构重参数化增强特征表示，同时保持实时性能。", "result": "在RSNA肺炎检测数据集上，CGF-DETR实现了82.2%的mAP@0.5，比基线RT-DETR-l高出3.7%，同时保持了48.1 FPS的可比推理速度。消融研究证实了每个提出模块对整体性能提升的有效贡献，完整模型达到了50.4%的mAP@[0.5:0.95]。", "conclusion": "CGF-DETR通过其提出的新模块，显著提高了胸部X光片肺炎检测的准确性，优于RT-DETR-l基线模型，并保持了实时性能，为肺炎的自动化检测提供了有效的解决方案。"}}
