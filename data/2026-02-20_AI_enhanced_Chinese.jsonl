{"id": "2602.16744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16744", "abs": "https://arxiv.org/abs/2602.16744", "authors": ["Takuro Kato", "Mitsuharu Morisawa"], "title": "ICP-Based Pallet Tracking for Unloading on Inclined Surfaces by Autonomous Forklifts", "comment": "Accepted and published in IEEE/SICE SII 2024", "summary": "This paper proposes a control method for autonomous forklifts to unload pallets on inclined surfaces, enabling the fork to be withdrawn without dragging the pallets. The proposed method applies the Iterative Closest Point (ICP) algorithm to point clouds measured from the upper region of the pallet and thereby tracks the relative position and attitude angle difference between the pallet and the fork during the unloading operation in real-time. According to the tracking result, the fork is aligned parallel to the target surface. After the fork is aligned, it is possible to complete the unloading process by withdrawing the fork along the tilt, preventing any dragging of the pallet. The effectiveness of the proposed method is verified through dynamic simulations and experiments using a real forklift that replicate unloading operations onto the inclined bed of a truck.", "AI": {"tldr": "提出了一种自主叉车在倾斜表面卸载托盘的控制方法，通过ICP算法实时跟踪托盘与叉车叉子的相对位置和姿态角，使叉子与目标表面平行，从而避免拖拽托盘。", "motivation": "解决自主叉车在倾斜表面（如卡车车厢）卸载托盘时，叉子被拖拽的问题。", "method": "采用迭代最近点（ICP）算法处理从托盘上方获取的点云数据，实时追踪托盘与叉子之间的相对位置和姿态角差。根据追踪结果，将叉子调整至与目标表面平行，然后沿着倾斜方向抽出叉子完成卸载。", "result": "通过动态模拟和真实叉车实验验证了该方法在倾斜表面卸载托盘的有效性，成功实现了叉子的无拖拽抽出。", "conclusion": "所提出的控制方法能够实时准确地估计托盘与叉子的相对姿态，并通过调整叉子姿态，有效解决了在倾斜表面卸载托盘时存在的拖拽问题。"}}
{"id": "2602.16831", "categories": ["eess.SY", "astro-ph.EP", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2602.16831", "abs": "https://arxiv.org/abs/2602.16831", "authors": ["Omer Burak Iskender", "Keck Voon Ling", "Mengu Cho", "Sangkyun Kim", "Necmi Cihan Orger"], "title": "Low-Thrust Trajectory Optimization for Cubesat Lunar Mission: HORYU-VI", "comment": "11 pages, 19 figures, presented at the International Astronautical Congress (IAC 2020)", "summary": "This paper presents a low-thrust trajectory optimization strategy to achieve a near-circular lunar orbit for a CubeSat injected into a lunar flyby trajectory. The 12U CubeSat HORYU-VI is equipped with four Hall-effect thrusters and designed as a secondary payload on NASA's Space Launch System under the Artemis program. Upon release, the spacecraft gains sufficient energy to escape the Earth-Moon system after a lunar flyby. The proposed trajectory is decomposed into three phases: (1) pre-flyby deceleration to avoid heliocentric escape, (2) lunar gravitational capture, and (3) orbit circularization to the science orbit. For each phase, an impulsive-burn solution is first computed as an initial guess, which is then refined through finite-burn optimization using Sequential Quadratic Programming (SQP). The dynamical model incorporates Earth-Moon-Sun-Jupiter gravitational interactions and a high-fidelity lunar gravity field. All trajectories are independently verified with NASA's General Mission Analysis Tool (GMAT). Results demonstrate that HORYU-VI achieves lunar capture within 200 days, establishes a stable science orbit at 280 days, and can spiral down to a near-circular 100 km orbit by 450 days, using a total Delta-V of 710 m/s, well within the capability of the electric propulsion system.", "AI": {"tldr": "本研究提出了一种低推力轨道优化策略，为进入月球飞掠轨道的立方星HORYU-VI规划了一条近圆月球轨道的转移路径，并通过数值模拟验证了其可行性。", "motivation": "为在Artemis计划中作为NASA SLS的次级载荷发射的12U立方星HORYU-VI设计一种低推力转移轨道，使其在月球飞掠后能够实现月球捕获并进入近圆的科学轨道。", "method": "将轨道转移分解为三个阶段：飞掠前减速、月球引力捕获、轨道圆化。首先通过脉冲燃烧法获得初始轨道猜测，然后使用序列二次规划（SQP）方法进行有限推力优化。动力学模型考虑了地-月-日-木引力及高精度月球重力场。使用NASA的GMAT工具进行轨迹验证。", "result": "HORYU-VI能在200天内实现月球捕获，280天内建立稳定的科学轨道，并在450天内下降至近圆的100公里轨道。总Delta-V为710 m/s，在霍尔推进器能力范围内。", "conclusion": "所提出的低推力轨道优化策略能够成功地将HORYU-VI立方星从月球飞掠轨道转移至近圆的月球科学轨道，且满足总Delta-V的要求，为该任务的可行性提供了有力证明。"}}
{"id": "2602.17120", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17120", "abs": "https://arxiv.org/abs/2602.17120", "authors": ["Liming Liu", "Jiangkai Wu", "Haoyang Wang", "Peiheng Wang", "Zongming Guo", "Xinggong Zhang"], "title": "HybridPrompt: Bridging Generative Priors and Traditional Codecs for Mobile Streaming", "comment": "6 pages, 7 figures, 4 tables, to appear in NOSSDAV 26", "summary": "In Video on Demand (VoD) scenarios, traditional codecs are the industry standard due to their high decoding efficiency. However, they suffer from severe quality degradation under low bandwidth conditions. While emerging generative neural codecs offer significantly higher perceptual quality, their reliance on heavy frame-by-frame generation makes real-time playback on mobile devices impractical. We ask: is it possible to combine the blazing-fast speed of traditional standards with the superior visual fidelity of neural approaches? We present HybridPrompt, the first generative-based video system capable of achieving real-time 1080p decoding at over 150 FPS on a commercial smartphone. Specifically, we employ a hybrid architecture that encodes Keyframes using a generative model while relying on traditional codecs for the remaining frames. A major challenge is that the two paradigms have conflicting objectives: the \"hallucinated\" details from generative models often misalign with the rigid prediction mechanisms of traditional codecs, causing bitrate inefficiency. To address this, we demonstrate that the traditional decoding process is differentiable, enabling an end-to-end optimization loop. This allows us to use subsequent frames as additional supervision, forcing the generative model to synthesize keyframes that are not only perceptually high-fidelity but also mathematically optimal references for the traditional codec. By integrating a two-stage generation strategy, our system outperforms pure neural baselines by orders of magnitude in speed while achieving an average LPIPS gain of 8% over traditional codecs at 200kbps.", "AI": {"tldr": "本研究提出了一种名为HybridPrompt的混合视频编码系统，结合了传统编解码器的高效解码速度和生成式神经网络的高感知质量。该系统在商业智能手机上实现了实时1080p解码，帧率超过150 FPS，同时在低比特率下提供了比传统编解码器更好的视觉效果。", "motivation": "传统视频编解码器在低带宽下存在严重的质量下降问题，而纯神经网络编解码器在移动设备上无法实现实时播放。因此，研究动机是结合两者的优点，实现快速解码和高质量感知。", "method": "HybridPrompt采用混合架构，使用生成模型编码关键帧，其余帧使用传统编解码器。通过将传统解码过程视为可微分的，实现了端到端优化，利用后续帧作为监督，使生成模型生成的关键帧既满足感知质量要求，又能作为传统编解码器的最优参考。采用了两阶段生成策略。", "result": "HybridPrompt在商业智能手机上实现了超过150 FPS的实时1080p解码。在200kbps的比特率下，与纯神经网络基线相比，速度提升了数个数量级；与传统编解码器相比，LPIPS（Learned Perceptual Image Patch Similarity）平均提升了8%。", "conclusion": "本研究成功地将生成式模型和传统编解码器相结合，克服了各自的局限性，实现了在移动设备上具有高感知质量和实时解码能力的视频系统。该方法通过端到端优化和利用后续帧作为监督，显著提高了低比特率下的视频质量和编码效率。"}}
{"id": "2602.17274", "categories": ["eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17274", "abs": "https://arxiv.org/abs/2602.17274", "authors": ["Alexandra Spitzer", "Lorenzo Baldassari", "Valentin Derbanot", "Ivan Dokmanić"], "title": "Gaussian surrogates do well on Poisson inverse problems", "comment": null, "summary": "In imaging inverse problems with Poisson-distributed measurements, it is common to use objectives derived from the Poisson likelihood. But performance is often evaluated by mean squared error (MSE), which raises a practical question: how much does a Poisson objective matter for MSE, even at low dose? We analyze the MSE of Poisson and Gaussian surrogate reconstruction objectives under Poisson noise. In a stylized diagonal model, we show that the unregularized Poisson maximum-likelihood estimator can incur large MSE at low dose, while Poisson MAP mitigates this instability through regularization. We then study two Gaussian surrogate objectives: a heteroscedastic quadratic objective motivated by the normal approximation of Poisson data, and a homoscedastic quadratic objective that yields a simple linear estimator. We show that both surrogates can achieve MSE comparable to Poisson MAP in the low-dose regime, despite departing from the Poisson likelihood. Numerical computed tomography experiments indicate that these conclusions extend beyond the stylized setting of our theoretical analysis.", "AI": {"tldr": "研究了在泊松噪声下，泊松似然目标函数和高斯近似目标函数对图像重建的均方误差（MSE）影响，尤其关注低剂量情况。结果表明，即使使用高斯近似目标函数，也能在低剂量下获得与泊松最大后验估计（MAP）相当的MSE性能。", "motivation": "在泊松分布噪声的成像逆问题中，通常使用泊松似然函数作为目标，但评估性能时常使用均方误差（MSE）。这引发了一个问题：泊松目标函数对MSE的影响有多大，尤其是在低剂量时？", "method": "作者首先在简化的对角模型中分析了未正则化的泊松最大似然估计器和正则化的泊松最大后验（MAP）估计器的MSE。然后，研究了两种高斯近似目标函数：一种是基于泊松数据正态近似的异方差二次目标函数，另一种是产生简单线性估计的同方差二次目标函数。最后，通过计算断层扫描实验验证了理论分析的结论。", "result": "在低剂量下，未正则化的泊松最大似然估计器可能产生很大的MSE，而泊松MAP通过正则化可以缓解这种不稳定性。两种高斯近似目标函数（异方差和同方差）在高斯近似和泊松噪声的设置下，都能实现与泊松MAP相当的MSE性能。", "conclusion": "在高斯近似和泊松噪声的设置下，即使目标函数偏离泊松似然函数，高斯近似目标函数也能在低剂量区域获得与泊松MAP相当的MSE性能。这一发现也得到了计算断层扫描实验的支持。"}}
{"id": "2602.17252", "categories": ["cs.CV", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17252", "abs": "https://arxiv.org/abs/2602.17252", "authors": ["Ziyan Zhang", "Chuheng Wei", "Xuanpeng Zhao", "Siyan Li", "Will Snyder", "Mike Stas", "Peng Hao", "Kanok Boriboonsomsin", "Guoyuan Wu"], "title": "A Multi-modal Detection System for Infrastructure-based Freight Signal Priority", "comment": "12 pages, 15 figures. Accepted at ICTD 2026. Final version to appear in ASCE Proceedings", "summary": "Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.", "AI": {"tldr": "该论文介绍了一种结合LiDAR和摄像头的多模态货车检测系统，用于信号交叉口的货车信号优先（FSP）应用，通过混合传感架构和先进的感知算法实现了高精度、实时性的货车运动估计。", "motivation": "为了实现更有效的货车信号优先（FSP）策略，需要精确可靠的基础设施来检测和追踪接近信号交叉口的货车，包括其类型、位置和速度。", "method": "设计并部署了一个混合传感系统，包含安装在交叉口和路段的子系统，通过无线通信同步数据。感知流程结合了基于聚类和深度学习的检测方法，并利用卡尔曼滤波器进行跟踪。LiDAR数据被注册到大地坐标系以支持车道级定位和稳定追踪。", "result": "现场评估表明，该系统能够以高时空分辨率可靠地监测货车运动，实现精确的位置和速度估计。", "conclusion": "该系统的设计和部署为开发支持FSP应用的基础设施传感系统提供了实用的见解，证明了其在实际应用中的有效性和可靠性。"}}
{"id": "2602.16753", "categories": ["eess.IV", "math.NA", "math.RA"], "pdf": "https://arxiv.org/pdf/2602.16753", "abs": "https://arxiv.org/abs/2602.16753", "authors": ["Wei Feng", "Tengda Wei", "Haiyong Zheng"], "title": "Structured Analytic Mappings for Point Set Registration", "comment": "35 pages. Accepted for publication in SIAM Journal on Imaging Sciences (SIIMS); in production", "summary": "We present an analytic approximation model for non-rigid point set registration, grounded in the multivariate Taylor expansion of vector-valued functions. By exploiting the algebraic structure of Taylor expansions, we construct a structured function space spanned by truncated basis terms, allowing smooth deformations to be represented with low complexity and explicit form. To estimate mappings within this space, we develop a quasi-Newton optimization algorithm that progressively lifts the identity map into higher-order analytic forms. This structured framework unifies rigid, affine, and nonlinear deformations under a single closed-form formulation, without relying on kernel functions or high-dimensional parameterizations. The proposed model is embedded into a standard ICP loop -- using (by default) nearest-neighbor correspondences -- resulting in Analytic-ICP, an efficient registration algorithm with quasi-linear time complexity. Experiments on 2D and 3D datasets demonstrate that Analytic-ICP achieves higher accuracy and faster convergence than classical methods such as CPD and TPS-RPM, particularly for small and smooth deformations.", "AI": {"tldr": "本文提出了一种用于非刚性点集配准的解析近似模型，该模型利用泰勒展开的代数结构，并结合拟牛顿优化算法，实现了对刚性、仿射和非线性形变的统一处理，并实现了准线性时间复杂度。", "motivation": "现有非刚性点集配准方法通常需要复杂的计算或参数化，作者希望提出一种更高效、更统一的框架来处理不同类型的形变。", "method": "利用多元泰勒展开的代数结构构建一个结构化的函数空间，该空间由截断的基项构成，用于表示平滑形变。开发了一个拟牛顿优化算法来估计该空间内的映射。将该模型嵌入到ICP循环中，形成Analytic-ICP算法。", "result": "Analytic-ICP 算法具有准线性时间复杂度。在2D和3D数据集上的实验表明，该算法在精度和收敛速度上优于CPD和TPS-RPM等经典方法，尤其是在处理小幅度和平滑形变时。", "conclusion": "所提出的Analytic-ICP模型能够高效、准确地进行非刚性点集配准，并且能够统一处理刚性、仿射和非线性形变，为非刚性配准提供了一种新的有效方法。"}}
{"id": "2602.16802", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16802", "abs": "https://arxiv.org/abs/2602.16802", "authors": ["Kejian Shi", "Yixin Liu", "Peifeng Wang", "Alexander R. Fabbri", "Shafiq Joty", "Arman Cohan"], "title": "References Improve LLM Alignment in Non-Verifiable Domains", "comment": "ICLR 2026 Camera Ready", "summary": "While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.", "AI": {"tldr": "本研究提出了一种参考引导的LLM评估器方法，用于解决缺乏可验证奖励的LLM对齐问题。通过实验证明，该方法能显著提高LLM的评估准确性，并在此基础上实现了比直接SFT和无参考自我改进更优的LLM对齐效果，性能可与ArmoRM媲美。", "motivation": "现有的RLVR方法在可验证领域有效，但无法直接应用于LLM对齐等非可验证领域。本研究旨在探索是否可以使用参考引导的LLM评估器作为软“验证器”来弥合这一差距。", "method": "设计了参考引导的LLM评估协议，使用参考输出来增强LLM评估器。通过实验评估了不同能力LLM评估器在有无参考下的性能，并利用增强后的评估器进行LLM的对齐微调。", "result": "参考引导的方法显著提高了LLM评估器的准确性，尤其是在使用更强模型生成的参考时。基于此，参考引导的LLM对齐微调方法在AlpacaEval和Arena-Hard基准测试中，使用Llama-3-8B-Instruct分别取得了73.1%和58.7%的性能，使用Qwen2.5-7B分别取得了70.0%和74.1%的性能。这比直接SFT蒸馏平均提升了20.2/17.1个百分点，比无参考自我改进平均提升了5.3/3.6个百分点。", "conclusion": "参考引导的LLM评估器在非可验证领域实现了有效的LLM后训练，为解决LLM对齐问题提供了一种有前景的解决方案。"}}
{"id": "2602.16713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.16713", "abs": "https://arxiv.org/abs/2602.16713", "authors": ["Shuo Wang", "Shuo Wang", "Xin Nie", "Yasutaka Narazaki", "Thomas Matiki", "Billie F. Spencer"], "title": "Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins", "comment": null, "summary": "Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.", "AI": {"tldr": "本研究提出了一种基于高斯溅射（GS）的数字孪生方法，用于在三维（3D）数字孪生中有效可视化 सिविल 基础设施的损坏情况，能够处理2D损坏分割结果，并支持多尺度重建和动态更新。", "motivation": "传统的2D图像损坏识别无法满足 सिविल 基础设施数字孪生对精确3D损坏可视化的需求。NeRF和GS等新方法在场景表示和渲染质量方面优于传统摄影测量技术，其中GS在效率方面表现突出。", "method": "该研究利用GS进行3D重建，将2D损坏分割结果可视化，以减少分割误差。开发了一种多尺度重建策略来平衡效率和损坏细节，并实现了数字孪生随损坏演变的更新。", "result": "在开源的震后检查合成数据集上进行了演示，该方法能够有效地将2D损坏可视化为3D，并支持不同尺度的细节显示，同时允许数字孪生模型进行更新。", "conclusion": "基于GS的数字孪生方法为 सिविल 基础设施的3D损坏可视化提供了一个有前途的解决方案，能够提高可视化精度，并支持动态更新以反映损坏的变化。"}}
{"id": "2602.16714", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16714", "abs": "https://arxiv.org/abs/2602.16714", "authors": ["Renato Marcelo", "Ana Rodrigues", "Cristiana Palmela Pereira", "António Figueiras", "Rui Santos", "José Rui Figueira", "Alexandre P Francisco", "Cátia Vaz"], "title": "AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment", "comment": null, "summary": "Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.", "AI": {"tldr": "该研究提出了AIdentifyAGE本体，一个用于法医牙科年龄评估的标准化框架，旨在提高评估过程的透明度、一致性和可解释性，并支持基于本体的决策支持系统。", "motivation": "现有法医牙科年龄评估方法存在方法学异质性、数据表示碎片化和系统间互操作性有限的问题，尤其是在人工智能方法日益普及的背景下，这些问题加剧了透明度和可重复性的挑战。因此，需要一个标准化、语义一致的框架来整合不同的评估流程和信息。", "method": "研究人员开发了一个领域特定的本体（AIdentifyAGE），该本体对法医牙科年龄评估的完整医疗法律流程进行了建模。它整合了司法背景、个体信息、法医检查数据、牙齿发育评估方法、影像学、统计参考研究以及基于人工智能的估计方法。该本体借鉴了现有的生物医学、牙科和机器学习本体，并与领域专家共同开发，遵循FAIR原则。", "result": "AIdentifyAGE本体提供了一个标准化的、语义连贯的框架，能够整合手动和人工智能辅助的法医牙科年龄评估工作流程，并实现观察、方法、参考数据和报告结果之间的可追溯链接。它增强了评估过程的一致性、透明度和可解释性。", "conclusion": "AIdentifyAGE本体是提高法医牙科年龄评估一致性、透明度和可解释性的关键一步，为医疗法律和司法领域中基于本体的决策支持系统奠定了坚实的基础。"}}
{"id": "2602.17010", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17010", "abs": "https://arxiv.org/abs/2602.17010", "authors": ["Jingwen Zhu", "Hadi Amirpour", "Wei Zhou", "Patrick Le Callet"], "title": "Is there a relationship between Mean Opinion Score (MOS) and Just Noticeable Difference (JND)?", "comment": "International Conference on Visual Communications and Image Processing (VCIP 2025)", "summary": "Evaluating perceived video quality is essential for ensuring high Quality of Experience (QoE) in modern streaming applications. While existing subjective datasets and Video Quality Metrics (VQMs) cover a broad quality range, many practical use cases especially for premium users focus on high quality scenarios requiring finer granularity. Just Noticeable Difference (JND) has emerged as a key concept for modeling perceptual thresholds in these high end regions and plays an important role in perceptual bitrate ladder construction. However, the relationship between JND and the more widely used Mean Opinion Score (MOS) remains unclear. In this paper, we conduct a Degradation Category Rating (DCR) subjective study based on an existing JND dataset to examine how MOS corresponds to the 75% Satisfied User Ratio (SUR) points of the 1st and 2nd JNDs. We find that while MOS values at JND points generally align with theoretical expectations (e.g., 4.75 for the 75% SUR of the 1st JND), the reverse mapping from MOS to JND is ambiguous due to overlapping confidence intervals across PVS indices. Statistical significance analysis further shows that DCR studies with limited participants may not detect meaningful differences between reference and JND videos.", "AI": {"tldr": "本研究通过DCR主观实验，探索了MOS与JND（Just Noticeable Difference）之间的关系，发现在高品质视频场景下，MOS值与JND阈值大致吻合，但从MOS反向推断JND存在模糊性，且小样本DCR实验可能无法检测到细微的感知差异。", "motivation": "现有的主观数据集和视频质量度量（VQMs）在关注高品质场景时，对细微的感知差异（如JND）的建模不足，而JND对于构建感知比特率阶梯至关重要。本研究旨在澄清JND与更常用的MOS之间的关系，以改进对高品质视频的感知评估。", "method": "本研究基于现有的JND数据集，进行了一项降级类别评级（DCR）主观实验，以考察MOS值如何对应于第一个和第二个JND的75%满意用户比例（SUR）点。通过统计显著性分析来评估实验结果。", "result": "研究发现，MOS值在高品质JND点通常符合预期（如第一个JND的75% SUR对应MOS为4.75）。然而，从MOS反向映射到JND存在歧义，因为不同PVS指数的置信区间存在重叠。此外，统计显著性分析表明，参与者数量有限的DCR研究可能无法检测到参考视频和JND视频之间的有意义的差异。", "conclusion": "MOS与JND在高品质视频场景下存在相关性，但反向映射不明确。在设计主观实验时，尤其是在检测细微感知差异时，需要考虑样本量和实验方法的局限性。"}}
{"id": "2602.16856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.16856", "abs": "https://arxiv.org/abs/2602.16856", "authors": ["Boda Lin", "Yongjie Zhu", "Wenyu Qin", "Meng Wang", "Pengfei Wan"], "title": "Analytic Score Optimization for Multi Dimension Video Quality Assessment", "comment": "18 pages", "summary": "Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.", "AI": {"tldr": "本文提出了一个包含五个维度（运动质量、运动幅度、美学质量、内容质量、清晰度质量）的用户生成视频（UGC）的大规模多维度视频质量评估（VQA）数据集UltraVQA，并引入了一种名为分析分数优化（ASO）的新型后训练目标，以改进离散质量分数评估，该方法在实验中优于现有基线。", "motivation": "传统VQA方法仅关注单一数值评分，无法捕捉视频质量的多个方面。研究动机在于开发更全面、更细致的VQA方法，以适应用户生成内容（UGC）日益增长的需求，并更好地理解和评估视频的整体质量。", "method": "1. 构建了一个名为UltraVQA的大规模多维度VQA数据集，包含UGC视频，并从运动质量、运动幅度、美学质量、内容质量和清晰度质量五个维度进行标注，每个视频有多个评分者和GPT生成的解释性理由。 2. 引入了分析分数优化（ASO）后训练目标，该目标基于多维度VQA的决策过程，能够自然地捕捉人类评分的序数性质，并与人类排名偏好对齐。", "result": "ASO方法在实验中表现优于大多数基线方法（包括闭源API和开源模型），同时降低了预测质量的平均绝对误差（MAE）。", "conclusion": "研究强调了多维度、可解释的标注以及基于增强（reinforcement-based）的对齐在推进视频质量评估方面的重要性，并展示了UltraVQA数据集和ASO方法的有效性。"}}
{"id": "2602.16866", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16866", "abs": "https://arxiv.org/abs/2602.16866", "authors": ["Ginevra Larroux", "Matthieu Jacobs", "Mario Paolone"], "title": "On the Tightness of the Second-Order Cone Relaxation of the Optimal Power Flow with Angles Recovery in Meshed Networks", "comment": null, "summary": "This letter investigates properties of the second-order cone relaxation of the optimal power flow (OPF) problem, with emphasis on relaxation tightness, nodal voltage angles recovery, and alternating-current-OPF feasibility in meshed networks. The theoretical discussion is supported by numerical experiments on standard IEEE test cases. Implications for power system planning are briefly outlined.", "AI": {"tldr": "该研究分析了最优潮流（OPF）问题的二阶锥松弛（SOC relaxation）的特性，特别关注了松弛的紧密性、节点电压角度恢复以及交替电流（AC-OPF）在网状网络中的可行性，并通过标准IEEE测试案例进行了数值验证。", "motivation": "为了更好地理解和应用二阶锥松弛方法解决最优潮流问题，并评估其在实际电网规划中的潜力。", "method": "理论分析二阶锥松弛的特性，并辅以标准IEEE测试案例的数值实验。", "result": "验证了二阶锥松弛在某些方面的紧密性，并探讨了其在恢复节点电压角度和确保AC-OPF可行性方面的表现，尤其是在网状网络中。", "conclusion": "二阶锥松弛是理解和解决OPF问题的一种有前景的方法，其结果对电网规划具有潜在的应用价值。"}}
{"id": "2602.16715", "categories": ["cs.AI", "cs.CL", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16715", "abs": "https://arxiv.org/abs/2602.16715", "authors": ["H. Sinan Bank", "Daniel R. Herber"], "title": "Retrieval Augmented (Knowledge Graph), and Large Language Model-Driven Design Structure Matrix (DSM) Generation of Cyber-Physical Systems", "comment": "26 pages, 10 figures", "summary": "We explore the potential of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and Graph-based RAG (GraphRAG) for generating Design Structure Matrices (DSMs). We test these methods on two distinct use cases -- a power screwdriver and a CubeSat with known architectural references -- evaluating their performance on two key tasks: determining relationships between predefined components, and the more complex challenge of identifying components and their subsequent relationships. We measure the performance by assessing each element of the DSM and overall architecture. Despite design and computational challenges, we identify opportunities for automated DSM generation, with all code publicly available for reproducibility and further feedback from the domain experts.", "AI": {"tldr": "该研究探索了LLM、RAG和GraphRAG在生成设计结构矩阵（DSM）方面的潜力，并在两种不同的用例（动力螺丝刀和CubeSat）上进行了测试，评估了它们在识别组件和关系方面的性能。", "motivation": "为了探索自动化生成设计结构矩阵（DSM）的可能性，并评估LLM、RAG和GraphRAG等先进技术在该领域的应用效果。", "method": "使用LLM、RAG和GraphRAG三种方法，在动力螺丝刀和CubeSat两个实际用例中生成DSM。评估方法包括确定预定义组件之间的关系，以及识别组件及其关系。性能通过评估DSM的各个元素和整体架构来衡量。", "result": "研究表明，尽管存在设计和计算上的挑战，但自动化DSM生成是可行的。所有用于实验的代码都已公开，以供复现和领域专家的反馈。", "conclusion": "LLM、RAG和GraphRAG在自动化DSM生成方面展现出潜力，为未来的研究和应用提供了基础，并且代码的公开有助于促进该领域的发展。"}}
{"id": "2602.16716", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.16716", "abs": "https://arxiv.org/abs/2602.16716", "authors": ["Song-Ju Kim"], "title": "Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence", "comment": "This paper addresses contextuality from a representation-theoretic and information-theoretic perspective in adaptive systems. It is conceptually and technically distinct from the authors' earlier arXiv works (QTOW/QTOW2), which pursue different formulations of contextuality", "summary": "Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.", "AI": {"tldr": "该论文证明了即使在经典概率模型中，当系统在多重情境下共享单一内部状态时，也会不可避免地产生情境依赖性，这种依赖性无法完全通过内部状态来中介，需要额外的“信息理论成本”。", "motivation": "当前自适应系统在资源限制下常复用固定内部状态，但这种单一状态复用的基本表征后果尚不清楚。", "method": "将情境建模为作用于共享内部状态的干预，推导出经典模型要重现情境性结果统计所必须付出的不可约信息论成本，并给出一个最小化的构造性示例。", "result": "证明了任何经典模型要想重现情境性结果统计，必须承担一个不可约的信息论成本，即对情境的依赖性不能仅通过内部状态来中介。", "conclusion": "情境性是自适应智能的普遍表征约束，与物理实现无关。非经典概率框架通过放宽单一全局联合概率空间假设来规避此约束，无需量子力学或希尔伯特空间结构。"}}
{"id": "2602.16811", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16811", "abs": "https://arxiv.org/abs/2602.16811", "authors": ["Charalampos Mastrokostas", "Nikolaos Giarelis", "Nikos Karacapilidis"], "title": "Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark", "comment": null, "summary": "Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.", "AI": {"tldr": "该研究提出了DemosQA数据集，并评估了11种单语和多语LLM在希腊语问答任务上的表现，以解决当前LLM在低资源语言方面的不足。", "motivation": "当前LLM研究主要集中在高资源语言，导致在低资源语言上存在数据偏见，未能充分反映其社会文化特点。因此，需要研究针对低资源语言的单语LLM在特定任务上的有效性。", "method": "构建了一个新的希腊语问答数据集DemosQA，该数据集使用社交媒体用户的问题和社区审查的答案。此外，开发了一个内存高效的LLM评估框架，并使用3种不同的提示策略，对11种单语和多语LLM在6个希腊语问答数据集上进行了广泛评估。", "result": "通过在6个希腊语问答数据集上的广泛评估，研究人员对11种单语和多语LLM的表现进行了分析，并发布了代码和数据以促进可重复性。", "conclusion": "该研究通过提出DemosQA数据集和评估框架，填补了在低资源语言（如希腊语）上的LLM问答研究空白，为理解和改进低资源语言LLM的性能提供了重要依据。"}}
{"id": "2602.16758", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16758", "abs": "https://arxiv.org/abs/2602.16758", "authors": ["Sina Akhbari", "Mehran Mahboubkhah"], "title": "Smooth trajectory generation and hybrid B-splines-Quaternions based tool path interpolation for a 3T1R parallel kinematic milling robot", "comment": "30 pages, 17 figures, published in Elsevier Precision Engineering (https://www.sciencedirect.com/science/article/abs/pii/S0141635925001266)", "summary": "This paper presents a smooth trajectory generation method for a four-degree-of-freedom parallel kinematic milling robot. The proposed approach integrates B-spline and Quaternion interpolation techniques to manage decoupled position and orientation data points. The synchronization of orientation and arc-length-parameterized position data is achieved through the fitting of smooth piece-wise Bezier curves, which describe the non-linear relationship between path length and tool orientation, solved via sequential quadratic programming. By leveraging the convex hull properties of Bezier curves, the method ensures spatial and temporal separation constraints for multi-agent trajectory generation. Unit quaternions are employed for orientation interpolation, providing a robust and efficient representation that avoids gimbal lock and facilitates smooth, continuous rotation. Modifier polynomials are used for position interpolation. Temporal trajectories are optimized using minimum jerk, time-optimal piece-wise Bezier curves in two stages: task space followed by joint space, implemented on a low-cost microcontroller. Experimental results demonstrate that the proposed method offers enhanced accuracy, reduced velocity fluctuations, and computational efficiency compared to conventional interpolation methods.", "AI": {"tldr": "本文提出了一种用于四自由度并联加工机器人的平滑轨迹生成方法，该方法结合了 B 样条和四元数插值技术，通过优化定位和方向数据来提高精度和效率。", "motivation": "为了生成高精度、低速度波动的平滑轨迹，特别是在多机器人协作等场景下，需要一种能有效处理解耦的位置和方向数据，并满足空间和时间约束的轨迹生成方法。", "method": "该方法采用 B 样条和四元数插值技术。首先，通过分段贝塞尔曲线拟合路径长度与刀具方向之间的非线性关系，并利用序贯二次规划求解。然后，使用单位四元数进行方向插值，并用修饰多项式进行位置插值。最后，通过最小加加速度优化和最优时间分段贝塞尔曲线分两阶段（任务空间和关节空间）对时间轨迹进行优化。", "result": "实验结果表明，与传统插值方法相比，该方法在精度、速度波动和计算效率方面均有提升。", "conclusion": "所提出的结合 B 样条和四元数插值的平滑轨迹生成方法能够有效地生成高精度、低速度波动的机器人轨迹，并具有良好的计算效率，适用于低成本微控制器实现。"}}
{"id": "2602.16872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.16872", "abs": "https://arxiv.org/abs/2602.16872", "authors": ["Sean Man", "Roy Ganz", "Roi Ronen", "Shahar Tsiper", "Shai Mazor", "Niv Nayman"], "title": "DODO: Discrete OCR Diffusion Models", "comment": null, "summary": "Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.", "AI": {"tldr": "本文提出了一种名为DODO的视觉语言模型（VLM），它利用块离散扩散模型来解决传统自回归解码在光学字符识别（OCR）任务中效率低下的问题，实现了近乎最先进的准确率并提高了3倍的推理速度。", "motivation": "现有的视觉语言模型在OCR任务中主要依赖自回归解码，处理长文档时计算成本高且速度慢。OCR任务的确定性特征（视觉输入决定唯一输出）暗示了并行解码的潜力，但现有掩码扩散模型在OCR的严格要求下存在结构不稳定性问题。", "method": "本文提出了DODO模型，这是一种利用块离散扩散（block discrete diffusion）的视觉语言模型。通过将生成任务分解为块，DODO缓解了全局扩散模型可能出现的同步错误，从而适用于OCR任务。", "result": "DODO在OCR任务上取得了近乎最先进的准确率，并且推理速度比自回归基线模型快3倍。", "conclusion": "DODO是第一个利用块离散扩散模型来加速OCR任务的视觉语言模型，它克服了现有扩散模型在OCR中的局限性，并在准确率和效率上都取得了显著的提升。"}}
{"id": "2602.16813", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16813", "abs": "https://arxiv.org/abs/2602.16813", "authors": ["Chanhyuk Lee", "Jaehoon Yoo", "Manan Agarwal", "Sheel Shah", "Jerry Huang", "Aditi Raghunathan", "Seunghoon Hong", "Nicholas M. Boffi", "Jinwoo Kim"], "title": "One-step Language Modeling via Continuous Denoising", "comment": "39 pages, 17 figures", "summary": "Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.", "AI": {"tldr": "本文提出了一种基于流的连续去噪语言模型（FLM），该模型在生成质量和速度上均优于现有的离散扩散模型，尤其是在少步生成任务中。通过蒸馏FLM，提出了FMLM，可以在一步生成中超越现有模型的多步生成质量。", "motivation": "现有的离散扩散模型在少步生成时存在样本质量急剧下降的问题，未能实现比自回归模型更快的生成速度。作者希望找到一种能够克服这一限制，并在生成质量和速度上都表现优异的生成模型。", "method": "本文提出了一种基于流的连续去噪语言模型（FLM），该模型在离散one-hot编码的token上执行欧氏去噪。通过交叉熵损失进行训练，并引入时间重参数化以提高训练稳定性和生成质量。进一步，将FLM蒸馏为其流映射，得到少步生成模型FMLM。", "result": "在LM1B和OWT数据集上，FLM的生成质量与最先进的离散扩散模型相当。FMLM在少步生成方面表现优于近期模型，其一步生成质量超过了其他模型8步的生成质量。", "conclusion": "本文证明了基于流的连续去噪模型在离散模态生成任务中可以超越离散扩散模型，解决了离散扩散模型在少步生成时的质量退化问题，并为加速流模型在语言建模领域的应用提供了新的方向。这项工作也挑战了离散扩散模型是离散模态生成模型必需品的观点。"}}
{"id": "2602.17019", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17019", "abs": "https://arxiv.org/abs/2602.17019", "authors": ["Gitae Park", "Kisong Lee"], "title": "Beyond Average-Channel-Based Rate Approximations: UAV Trajectory and Scheduling Optimization With Expected Rate Consideration", "comment": null, "summary": "This paper investigates the joint optimization of trajectory, user scheduling, and time-slot duration in unmanned aerial vehicle (UAV)-assisted wireless communication systems under minimum expected spectral efficiency (SE) constraints. Unlike most existing studies that approximate the expected SE by substituting the random channel gain with its mean value, thereby evaluating the SE at the average channel realization and overestimating the true expected SE due to Jensen's inequality, we approximate the expected SE by numerically integrating the SE over the channel distributions. Specifically, instead of relying on average-channel-based approximations, we develop a conservative yet tractable quadrature-based approximation by discretizing the associated cumulative distribution functions. The resulting finite-sum representation explicitly accounts for the probabilistic LoS structure and channel fading effects, while remaining tractable for optimization. Leveraging this lower bound, we formulate a mission completion time minimization problem subject to minimum expected-SE requirements for all ground nodes. The resulting problem is a mixed-integer nonconvex optimization, which is tackled via a penalty-based block coordinate descent framework. The proposed algorithm alternately optimizes the scheduling decisions and the UAV trajectory along with adaptive time-slot durations, and maintains feasibility with respect to the original expected-SE constraints by leveraging successive convex approximation and quadratic transform techniques. Simulation results demonstrate that the proposed method strictly satisfies the minimum expected-SE constraints and achieves a significantly shorter mission completion time than conventional average-channel-based approaches, which are shown to yield infeasible or overly conservative solutions.", "AI": {"tldr": "本文提出了一种在无人机辅助通信系统中联合优化轨迹、用户调度和时隙持续时间的方法，以最小化任务完成时间，同时满足用户最低预期谱效率（SE）约束。该方法通过数值积分而非平均值近似来精确计算预期SE，并通过块坐标下降法解决优化问题，实现了比传统方法更短的任务完成时间和更高的可行性。", "motivation": "现有研究中，在优化无人机辅助通信系统时，通常使用平均信道增益来近似预期SE，这会导致高估真实预期SE。本文旨在提出一种更精确地计算预期SE的方法，以确保满足最低预期SE约束，并在此基础上最小化任务完成时间。", "method": "本文提出了一种基于数值积分的预期SE近似方法，通过离散化累积分布函数来得到一个保守但可行的二次积分近似。然后，将此作为约束，构建一个最小化任务完成时间的混合整数非凸优化问题。该问题通过基于惩罚的块坐标下降（BCD）框架解决，该框架交替优化调度决策、无人机轨迹和自适应时隙持续时间，并利用逐次凸近似（SCA）和二次变换技术来维持约束可行性。", "result": "仿真结果表明，所提出的方法能够严格满足最低预期SE约束，并显著缩短任务完成时间。与基于平均信道的传统方法相比，该方法不会产生不可行或过于保守的解决方案。", "conclusion": "本文提出的联合优化方法能够有效地在无人机辅助无线通信系统中，在满足最低预期SE约束的前提下，最小化任务完成时间，并提供了比传统方法更精确和可行的解决方案。"}}
{"id": "2602.16825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16825", "abs": "https://arxiv.org/abs/2602.16825", "authors": ["Ahmad Ahmad", "Shuo Liu", "Roberto Tron", "Calin Belta"], "title": "RRT$^η$: Sampling-based Motion Planning and Control from STL Specifications using Arithmetic-Geometric Mean Robustness", "comment": null, "summary": "Sampling-based motion planning has emerged as a powerful approach for robotics, enabling exploration of complex, high-dimensional configuration spaces. When combined with Signal Temporal Logic (STL), a temporal logic widely used for formalizing interpretable robotic tasks, these methods can address complex spatiotemporal constraints. However, traditional approaches rely on min-max robustness measures that focus only on critical time points and subformulae, creating non-smooth optimization landscapes with sharp decision boundaries that hinder efficient tree exploration.\n  We propose RRT$^η$, a sampling-based planning framework that integrates the Arithmetic-Geometric Mean (AGM) robustness measure to evaluate satisfaction across all time points and subformulae. Our key contributions include: (1) AGM robustness interval semantics for reasoning about partial trajectories during tree construction, (2) an efficient incremental monitoring algorithm computing these intervals, and (3) enhanced Direction of Increasing Satisfaction vectors leveraging Fulfillment Priority Logic (FPL) for principled objective composition. Our framework synthesizes dynamically feasible control sequences satisfying STL specifications with high robustness while maintaining the probabilistic completeness and asymptotic optimality of RRT$^\\ast$. We validate our approach on three robotic systems. A double integrator point robot, a unicycle mobile robot, and a 7-DOF robot arm, demonstrating superior performance over traditional STL robustness-based planners in multi-constraint scenarios with limited guidance signals.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2602.16727", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16727", "abs": "https://arxiv.org/abs/2602.16727", "authors": ["Hua Yan", "Heng Tan", "Yingxue Zhang", "Yu Yang"], "title": "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation", "comment": null, "summary": "Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.", "AI": {"tldr": "提出了一种名为MobCache的面向移动性的缓存框架，利用可重构缓存来提高大型语言模型（LLMs）进行大规模人类移动模拟的效率，同时保持了高保真度。", "motivation": "大型语言模型（LLMs）在模拟人类移动行为方面展现出潜力，但其高昂的计算成本限制了大规模应用。", "method": "MobCache包含两个主要部分：(1) 一个编码每个推理步骤为潜在空间嵌入，并利用潜在空间评估器进行重用和重组的推理组件；(2) 一个使用轻量级解码器，并通过受移动性定律约束的蒸馏进行训练，将潜在空间推理链转化为自然语言的解码组件。", "result": "实验表明，MobCache在多个维度上显著提高了效率，同时保持了与最先进的LLM方法相当的性能。", "conclusion": "MobCache通过其创新的缓存机制和轻量级解码器，成功解决了LLMs在模拟人类移动中的效率瓶颈，实现了高效且高保真的大规模模拟。"}}
{"id": "2602.17055", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17055", "abs": "https://arxiv.org/abs/2602.17055", "authors": ["Ki-Hyun Kim", "Yeongung Kim", "Shenghui Cui", "Jae-Jung Jung"], "title": "Decoupled Internal Energy Regulation and Inertial Response Provision for Grid-Forming Multilevel-Converter-Based E-STATCOMs", "comment": null, "summary": "As power systems accommodate higher shares of renewable generation, short-term power imbalances become more frequent and can manifest as pronounced voltage and frequency excursions under low-inertia conditions. E-STATCOMs (STATCOMs equipped with energy storage) offer a practical means to provide both voltage support and fast frequency assistance under grid-forming control. Among candidate implementations, double-star multilevel-converter (DS-MC)-based E-STATCOMs enable centralized energy-storage integration at the dc link, which improves thermal management and maintainability. Nevertheless, conventional dc-side power-based internal-energy regulation in DS-MCs can undesirably couple loss compensation to the energy-storage path, accelerating storage cycling and constraining operation when the storage is unavailable. This paper introduces a control strategy that assigns DS-MC total internal-energy regulation to the ac-side active-power path, while reserving dc-side storage power solely for frequency support. By decoupling internal-energy management from inertial-response provision, the proposed scheme enables flexible operation as either a STATCOM or an E-STATCOM according to storage availability and mitigates unnecessary storage cycling. The proposed strategy is verified through offline simulations and laboratory-scale experiments.", "AI": {"tldr": "本文提出了一种用于双星形多电平转换器 (DS-MC) E-STATCOM 的解耦控制策略，将内部能量调节分配给交流侧有功功率，而将直流侧储能功率仅用于频率支撑，从而提高了运行灵活性并减少了储能损耗。", "motivation": "随着可再生能源发电比例的增加，电网的惯量降低，导致电压和频率波动更加频繁。E-STATCOM（配备储能的 STATCOM）可以通过电网自并网控制提供电压和频率支持，但传统的基于直流侧功率的内部能量调节会加速储能损耗并限制其可用性。", "method": "提出了一种新的控制策略，将 DS-MC 的总内部能量调节分配给交流侧有功功率路径，而将直流侧储能功率仅用于频率支撑。通过这种解耦方式，实现了内部能量管理与惯性响应提供的分离。", "result": "提出的控制策略能够根据储能的可用性灵活地作为 STATCOM 或 E-STATCOM 运行，减少了不必要的储能损耗。该策略通过离线仿真和实验室规模的实验得到了验证。", "conclusion": "该解耦控制策略是一种有效的解决方案，可以提高 DS-MC E-STATCOM 在低惯量电网中的运行灵活性和储能效率，并且不依赖于储能的可用性。"}}
{"id": "2602.16836", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16836", "abs": "https://arxiv.org/abs/2602.16836", "authors": ["Zhengda Mo", "Zhiyu Quan", "Eli O'Donohue", "Kaiwen Zhong"], "title": "Claim Automation using Large Language Model", "comment": "46 pages, 12 figures. Code and data processing pipeline described", "summary": "While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.", "AI": {"tldr": "研究提出了一种针对保险领域、经过领域自适应微调的本地化语言模型组件，能够从非结构化的索赔描述中生成结构化的纠正措施建议，并显著提高了索赔处理效率和准确性。", "motivation": "大型语言模型在通用任务上表现出色，但在保险等数据敏感且受监管的领域部署受限。需要开发一种能够处理非结构化索赔数据并提供可靠建议的语言模型。", "method": "利用数百万份历史保修索赔数据，通过低秩适应（LoRA）微调预训练的大型语言模型。该模型被集成到索赔处理流程的初始决策模块中，以加速理赔员的决策。评估方法结合了自动语义相似度指标和人工评估。", "result": "与商业通用模型和基于提示的模型相比，领域特定微调的模型表现出色。约80%的评估案例生成的纠正措施与实际情况高度匹配。", "conclusion": "领域自适应微调能够使模型输出更贴近实际操作数据，证明了其作为保险应用可靠且可控构建模块的潜力。"}}
{"id": "2602.16917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.16917", "abs": "https://arxiv.org/abs/2602.16917", "authors": ["Sakib Ahammed", "Xia Cui", "Xinqi Fan", "Wenqi Lu", "Moi Hoon Yap"], "title": "SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts", "comment": null, "summary": "Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.", "AI": {"tldr": "该研究提出了Semantic Coverage Imbalance (SCI)这一新的视觉模型偏见，并引入了SemCovNet模型来解决这个问题。SemCovNet通过SDM、DAM和DVA等模块，显式地学习并纠正语义覆盖不均的问题，并使用CDI指标来量化和评估模型的公平性。", "motivation": "现有视觉模型依赖于丰富的语义表示，但现有数据集存在语义覆盖不均（SCI）的偏见，导致模型难以学习和推理稀有但重要的语义信息。", "method": "提出了Semantic Coverage-Aware Network (SemCovNet)，包括Semantic Descriptor Map (SDM)用于学习语义表示，Descriptor Attention Modulation (DAM)模块动态加权视觉和概念特征，以及Descriptor-Visual Alignment (DVA)损失来对齐视觉特征和描述符语义。同时，引入了Coverage Disparity Index (CDI)来量化语义公平性。", "result": "SemCovNet在多个数据集上进行了实验，证明其能够提高模型的可靠性，显著降低CDI，实现更公平、更均衡的性能。", "conclusion": "该研究将SCI确立为一种可测量和可纠正的偏见，为提高语义公平性和可解释的视觉学习奠定了基础。"}}
{"id": "2602.16846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16846", "abs": "https://arxiv.org/abs/2602.16846", "authors": ["Xili Yi", "Ying Xing", "Zachary Manchester", "Nima Fazeli"], "title": "Sound of Touch: Active Acoustic Tactile Sensing via String Vibrations", "comment": "12 pages, 10 figures", "summary": "Distributed tactile sensing remains difficult to scale over large areas: dense sensor arrays increase wiring, cost, and fragility, while many alternatives provide limited coverage or miss fast interaction dynamics. We present Sound of Touch, an active acoustic tactile-sensing methodology that uses vibrating tensioned strings as sensing elements. The string is continuously excited electromagnetically, and a small number of pickups (contact microphones) observe spectral changes induced by contact. From short-duration audio signals, our system estimates contact location and normal force, and detects slip. To guide design and interpret the sensing mechanism, we derive a physics-based string-vibration simulator that predicts how contact position and force shift vibration modes. Experiments demonstrate millimeter-scale localization, reliable force estimation, and real-time slip detection. Our contributions are: (i) a lightweight, scalable string-based tactile sensing hardware concept for instrumenting extended robot surfaces; (ii) a physics-grounded simulation and analysis tool for contact-induced spectral shifts; and (iii) a real-time inference pipeline that maps vibration measurements to contact state.", "AI": {"tldr": "该研究提出了一种名为“Sound of Touch”的声学触觉传感方法，利用振动的琴弦作为传感元件，通过检测接触引起的频谱变化来估计接触位置、法向力和检测滑动。该方法具有轻量化、可扩展性强等优点，适用于大面积机器人表面。", "motivation": "传统的分布式触觉传感方法在覆盖大面积时面临布线复杂、成本高、易损坏以及覆盖范围有限或无法捕捉快速交互动力学等挑战。因此，需要一种新的、可扩展的触觉传感技术。", "method": "该方法使用连续电磁激励的张紧琴弦作为传感元件，并利用少量拾音器（接触式麦克风）监测接触引起的频谱变化。研究人员推导了一个基于物理学的琴弦振动模拟器，用于预测接触位置和力对振动模式的影响。此外，还建立了一个实时推理流程，将振动测量映射到接触状态。", "result": "实验证明，该系统能够实现毫米级的定位精度、可靠的力估计以及实时的滑动检测。", "conclusion": "“Sound of Touch”提供了一种轻量化、可扩展的基于琴弦的触觉传感硬件概念，适用于大面积机器人表面的仪器化。该研究还提供了基于物理学的仿真和分析工具，用于研究接触引起的频谱变化，并开发了一个实时推理流程，将振动测量转换为接触状态。"}}
{"id": "2602.17118", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17118", "abs": "https://arxiv.org/abs/2602.17118", "authors": ["Shankar Ramharack", "Rajiv Sahadeo"], "title": "Validation of KESTREL EMT for Industrial Capacitor Switching Transient Studies", "comment": "6 pages, 8 figures, submitted for review", "summary": "Electromagnetic transient (EMT) simulation is essential for analyzing sub-cycle switching phenomena in industrial power systems; however, commercial EMT platforms present significant cost barriers for smaller utilities, consultancies, and academic institutions, particularly in developing regions. This paper validates KESTREL EMT, a free and open-source electromagnetic transient solver with Python integration, through three progressive case studies involving industrial capacitor switching transients. This work investigates energization, switching resonance and VFD interactions with capacitor banks. The results demonstrate that KESTREL, when supported by appropriate circuit modeling techniques, produces EMT responses consistent with analytical predictions and established IEEE benchmarks. This work establishes a validated and reproducible methodology for conducting industrial EMT studies using freely available, open-source tools.", "AI": {"tldr": "该论文验证了一个名为 KESTREL 的免费开源电磁瞬态 (EMT) 求解器，证明其在工业电容器切换瞬态分析中与商业软件和理论模型一致，为小型机构提供了低成本的 EMT 研究替代方案。", "motivation": "商业 EMT 仿真软件成本高昂，阻碍了小型公用事业公司、咨询公司和学术机构（尤其是在发展中地区）进行亚周期开关现象分析。因此，需要一个免费且开源的替代方案。", "method": "通过三个逐步进行的工业电容器切换瞬态案例研究（包括励磁、开关谐振和变频器与电容器组的相互作用）来验证 KESTREL EMT 求解器。该方法包括使用 Python 集成，并与分析预测和 IEEE 基准进行比较。", "result": "KESTREL EMT 求解器在电容器切换瞬态分析中产生了与分析预测和 IEEE 基准一致的结果，表明其准确性和有效性。", "conclusion": "KESTREL EMT 求解器是一个经过验证且可复现的工具，可用于工业 EMT 研究。通过使用免费、开源的工具和适当的电路建模技术，可以有效地进行这类研究，为资源受限的机构提供了可行的解决方案。"}}
{"id": "2602.16763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16763", "abs": "https://arxiv.org/abs/2602.16763", "authors": ["Mubashara Akhtar", "Anka Reuel", "Prajna Soni", "Sanchit Ahuja", "Pawan Sasanka Ammanamanchi", "Ruchit Rawal", "Vilém Zouhar", "Srishti Yadav", "Chenxi Whitehouse", "Dayeon Ki", "Jennifer Mickel", "Leshem Choshen", "Marek Šuppa", "Jan Batzner", "Jenny Chim", "Jeba Sania", "Yanan Long", "Hossein A. Rahmani", "Christina Knight", "Yiyang Nan", "Jyoutir Raj", "Yu Fan", "Shubham Singh", "Subramanyam Sahoo", "Eliya Habba", "Usman Gohar", "Siddhesh Pawar", "Robert Scholz", "Arjun Subramonian", "Jingwei Ni", "Mykel Kochenderfer", "Sanmi Koyejo", "Mrinmaya Sachan", "Stella Biderman", "Zeerak Talat", "Avijit Ghosh", "Irene Solaiman"], "title": "When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation", "comment": null, "summary": "Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.", "AI": {"tldr": "本研究分析了60个大型语言模型（LLM）基准的饱和度，发现近一半基准已饱和，且饱和率随时间增加。私有测试数据并未有效缓解饱和，而专家策划的基准比众包基准更具韧性。", "motivation": "人工智能（AI）基准在衡量模型进展和指导部署方面至关重要，但许多基准很快饱和，无法区分最佳模型，降低了其长期价值。本研究旨在分析基准饱和现象，找出驱动饱和的因素，并为设计更持久的评估提供依据。", "method": "研究人员从主要模型开发者的技术报告中选取了60个大型语言模型（LLM）基准。他们根据任务设计、数据构建和评估格式这14个属性对基准进行特征描述，并测试了五个关于这些属性如何影响饱和率的假设。", "result": "近一半的基准表现出饱和现象，且饱和率随基准年龄的增长而增加。隐藏测试数据（公开 vs. 私有）并未显示出缓解饱和的保护作用。专家策划的基准比众包基准更能抵抗饱和。", "conclusion": "本研究揭示了哪些设计选择可以延长基准的寿命，并为制定更持久的评估策略提供了信息。基准设计中的关键因素包括数据来源（专家策划优于众包）和如何应对基准随时间推移的退化。"}}
{"id": "2602.16915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.16915", "abs": "https://arxiv.org/abs/2602.16915", "authors": ["Zeyu Ren", "Xiang Li", "Yiran Wang", "Zeyu Zhang", "Hao Tang"], "title": "StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation", "comment": null, "summary": "Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.", "AI": {"tldr": "本文提出了一种名为 StereoAdapter-2 的新方法，用于提高水下立体深度估计的准确性。该方法使用一种新的 ConvSS2D 算子来取代传统的 GRU，以更有效地传播长距离视差信息，并通过引入一个大型合成数据集 UW-StereoDepth-80K 来增强模型性能。结果显示，该方法在水下数据集上取得了最先进的零样本性能。", "motivation": "现有的水下立体深度估计方法在处理由光照衰减、散射和折射引起的水下领域迁移问题时存在不足。传统的基于 GRU 的方法需要多次迭代才能传播长距离视差，在处理大视差和纹理稀疏的区域时效率低下。", "method": "提出了一种名为 StereoAdapter-2 的新方法，用基于选择性状态空间模型（SSM）的 ConvSS2D 算子替换了传统的 ConvGRU。ConvSS2D 采用四向扫描策略，能够自然地对齐极线几何并捕捉垂直结构一致性，从而在一个更新步骤内以线性计算复杂度实现高效的长距离空间传播。同时，构建了一个大规模合成数据集 UW-StereoDepth-80K。", "result": "StereoAdapter-2 在 TartanAir-UW 数据集上取得了 17% 的性能提升，在 SQUID 数据集上取得了 7.2% 的性能提升，展现了在水下基准测试中的最先进的零样本性能。在 BlueROV2 平台上的真实世界验证也证明了该方法的鲁棒性。", "conclusion": "StereoAdapter-2 通过引入 ConvSS2D 算子和 UW-StereoDepth-80K 数据集，有效地解决了水下立体深度估计中的领域迁移问题，并实现了显著的性能提升，证明了其在水下机器人感知领域的潜力。"}}
{"id": "2602.16861", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.16861", "abs": "https://arxiv.org/abs/2602.16861", "authors": ["EunJeong Cheon", "Do Yeon Shin"], "title": "\"Hello, I'm Delivering. Let Me Pass By\": Navigating Public Pathways with Walk-along with Robots in Crowded City Streets", "comment": null, "summary": "As the presence of autonomous robots in public spaces increases-whether navigating campus walkways or neighborhood sidewalks-understanding how to carefully study these robots becomes critical. While HRI research has conducted field studies in public spaces, these are often limited to controlled experiments with prototype robots or structured observational methods, such as the Wizard of Oz technique. However, the autonomous mobile robots we encounter today, particularly delivery robots, operate beyond the control of researchers, navigating dynamic routes and unpredictable environments. To address this challenge, a more deliberate approach is required. Drawing inspiration from public realm ethnography in urban studies, geography, and sociology, this paper proposes the Walk-Along with Robots (WawR) methodology. We outline the key features of this method, the steps we applied in our study, the unique insights it offers, and the ways it can be evaluated. We hope this paper stimulates further discussion on research methodologies for studying autonomous robots in public spaces.", "AI": {"tldr": "本文提出了一种名为“与机器人同行”（Walk-Along with Robots, WawR）的新方法，用于研究在公共场所自主运行的机器人，尤其是在其行为超出研究者控制的情况下。", "motivation": "随着自主机器人在公共空间（如校园或街道）的普及，需要有效的方法来研究它们。现有的人机交互（HRI）研究方法，如受控实验和“绿野仙踪”技术，难以捕捉到在动态、不可预测环境中自主导航的真实机器人行为。", "method": "该研究借鉴了城市研究、地理学和社会学中的公共领域人种学方法，提出了WawR方法。该方法包括描述其关键特征、应用步骤、提供的独特见解以及评估方式。", "result": "WawR方法能够提供对自主机器人在公共空间中行为的独特见解，克服了现有研究方法的局限性。", "conclusion": "WawR方法是一种新颖的研究方法，为在公共场所研究自主机器人（特别是那些行为不受研究者控制的机器人）提供了新的思路，并鼓励在该领域进行更多的方法学讨论。"}}
{"id": "2602.16950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.16950", "abs": "https://arxiv.org/abs/2602.16950", "authors": ["Kibon Ku", "Talukder Z. Jubery", "Adarsh Krishnamurthy", "Baskar Ganapathysubramanian"], "title": "HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs", "comment": "16 pages, 14 figures, 3 tables", "summary": "Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.", "AI": {"tldr": "本文提出了一种基于固定相机的多通道 NeRF 框架 HSI-SC-NeRF，用于高通量作物产后检测的超光谱 3D 重建。", "motivation": "将高光谱成像（HSI）和 3D 重建集成到自动化农艺表型分析系统中存在挑战，现有方法硬件复杂或不适合室内农业环境。", "method": "使用固定相机采集多视角超光谱数据，物体在可旋转的 Teflon 腔内进行扫描。通过 ArUco 标记估计物体姿态，并将其转换为相机坐标系。采用多通道 NeRF 模型，结合复合光谱损失和两阶段训练协议，实现几何初始化和辐射度精炼。", "result": "在三种农产品样本上进行的实验表明，HSI-SC-NeRF 在可见光和近红外光谱范围内实现了高空间重建精度和良好的光谱保真度。", "conclusion": "HSI-SC-NeRF 能够高通量地进行超光谱 3D 重建，适用于自动化农业工作流程，有望用于作物产量检测和育种。"}}
{"id": "2602.16918", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16918", "abs": "https://arxiv.org/abs/2602.16918", "authors": ["Shlok Mishra", "Tsung-Yu Lin", "Linda Wang", "Hongli Xu", "Yimin Liu", "Michael Hsu", "Chaitanya Ahuja", "Hao Yuan", "Jianpeng Cheng", "Hong-You Chen", "Haoyuan Xu", "Chao Li", "Abhijeet Awasthi", "Jihye Moon", "Don Husa", "Michael Ge", "Sumedha Singla", "Arkabandhu Chowdhury", "Phong Dingh", "Satya Narayan Shukla", "Yonghuan Yang", "David Jacobs", "Qi Guo", "Jun Xiao", "Xiangjun Fan", "Aashu Singh"], "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data", "comment": null, "summary": "We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.", "AI": {"tldr": "Xray-Visual 是一个统一的视觉模型，在大规模图像和视频理解任务上取得了最先进的性能，通过在海量社交媒体数据上进行三阶段训练，并结合大语言模型进一步提升了检索能力。", "motivation": "研究的动机是为了构建一个能够在大规模图像和视频数据上进行有效学习的统一视觉模型，以应对日益增长的多模态数据和理解需求，并提升模型在不同任务和真实世界环境下的性能和鲁棒性。", "method": "该研究提出了 Xray-Visual 模型架构，采用 Vision Transformer (EViT) 作为骨干网络。训练过程分为三个阶段：自监督 MAE 预训练、半监督哈希标签分类，以及 CLIP 式的对比学习。模型使用了超过 150 亿个图像-文本对和 100 亿个视频-哈希标签对进行训练，并采用了数据策展策略。此外，研究还探索了将大语言模型作为文本编码器（LLM2CLIP）集成以增强模型能力。", "result": "Xray-Visual 在图像分类（ImageNet）、视频理解（Kinetics, HMDB51）和跨模态检索（MSCOCO）等多个基准测试中达到了最先进的性能。模型展现了对领域偏移和对抗性扰动的良好鲁棒性。使用 LLM2CLIP 显著提升了检索性能和泛化能力，尤其是在真实世界场景下。", "conclusion": "Xray-Visual 证明了在大规模、多样化的社交媒体数据上进行多阶段训练，结合优化的模型架构和强大的文本编码器，可以构建出性能卓越、计算高效且鲁棒性强的多模态视觉模型，为大规模多模态模型的发展设定了新的标杆。"}}
{"id": "2602.16805", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16805", "abs": "https://arxiv.org/abs/2602.16805", "authors": ["Yonatan Gideoni", "Sebastian Risi", "Yarin Gal"], "title": "Simple Baselines are Competitive with Code Evolution", "comment": null, "summary": "Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.", "AI": {"tldr": "研究人员评估了两种简单的基线方法在代码演化任务中的表现，发现它们在三个领域（数学边界、代理脚手架设计和机器学习竞赛）中均能媲美甚至超越更复杂的代码演化方法。研究揭示了当前代码演化方法在设计和使用上的不足，并提出改进评估方法的建议。", "motivation": "现有的大型语言模型代码演化方法通常表现出色，但缺乏与简单基线方法的比较，研究者希望评估简单方法的有效性。", "method": "研究人员在三个领域（数学边界、代理脚手架设计和机器学习竞赛）中测试了两种简单基线方法。通过分析结果，研究者识别了代码演化方法在开发和使用中的缺陷，并提出了改进的评估方法。", "result": "在所有三个测试领域中，简单基线方法均能匹配或超越更复杂的代码演化方法。研究发现，数学边界问题的搜索空间和领域知识对性能影响更大，而代理脚手架设计的评估方式（高方差和小数据集）导致了次优结果，手动设计的多数投票方法表现最佳。", "conclusion": "代码演化的有效性在很大程度上取决于搜索空间的设计和领域知识的整合，而代码演化本身只是次要因素。当前的代码演化评估方法存在不足，需要改进以减少评估的随机性。未来的代码演化研究应更注重严谨性和最佳实践。"}}
{"id": "2602.17239", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17239", "abs": "https://arxiv.org/abs/2602.17239", "authors": ["Alessandro Quattrociocchi", "Manisha Talukdar", "Pere Izquierdo Gómez", "Tomislav Dragicevic"], "title": "Distributionally Robust Scheduling of Electrified Heating Under Heat Demand Forecast Uncertainty", "comment": null, "summary": "Electrified heating systems with thermal storage, such as electric boilers and heat pumps, represent a major source of demand-side flexibility. Under current electricity market designs, balance responsible parties (BRPs) operating such assets are required to submit binding day-ahead electricity consumption schedules, and they typically do it based on forecasts of heat demand and electricity prices. Common scheduling approaches implicitly assume that forecast uncertainty can be well characterized using historical forecast errors. In practice, however, the cumulative effect of uncertainty creates significant exposure to imbalance-price risk when the committed schedule cannot be followed. To address this, we propose a distributionally robust chance-constrained optimization framework for the day-ahead scheduling of a multi-MW electric boiler using only limited residual forecast samples. We derive a tractable convex reformulation of the problem and calibrate the ambiguity set directly from historical forecast-error data through an a priori tunable risk parameter. Numerical results show that enforcing performance guarantees on the heat-demand balance constraint reduces demand violations by 40% compared to a deterministic forecast-based scheduler and up to 10% relative to a nominal chance-constrained model with a fixed error distribution. Further, we show that modeling the real-time rebound cost of demand violations as a second-stage term can reduce the overall daily operating cost by up to 34% by hedging against highly volatile day-ahead electricity prices.", "AI": {"tldr": "本研究提出了一种分布鲁棒的随机约束优化框架，用于调度配备储能的电动锅炉，以应对电价波动和热需求不确定性，显著降低了需求违约和运营成本。", "motivation": "当前电力市场设计下，平衡责任方（BRPs）在进行日内电力调度时，面临热需求和电价预测不确定性带来的严重失衡价格风险。现有方法依赖于历史预测误差，但实际中累积的不确定性效应会暴露风险。", "method": "研究提出了一种分布鲁棒的随机约束优化框架。该框架利用有限的残差预测样本，推导出一个可行的凸重构问题，并通过可调的风险参数直接从历史预测误差数据校准模糊集。此外，将实时需求违约的“反弹成本”建模为第二阶段项。", "result": "该方法将热需求平衡约束的性能保证，与基于确定性预测的调度器相比，需求违约减少了40%；与具有固定误差分布的标称随机约束模型相比，需求违约减少了10%。将实时反弹成本作为第二阶段项，可以使整体日运营成本降低高达34%。", "conclusion": "分布鲁棒的随机约束优化框架能够有效地应对热需求和电价预测的不确定性，显著提高电动锅炉等储能设备的调度性能，降低运营风险和成本。通过对不确定性进行建模并引入反弹成本，可以更好地管理市场波动。"}}
{"id": "2602.16863", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16863", "abs": "https://arxiv.org/abs/2602.16863", "authors": ["Kushal Kedia", "Tyler Ga Wei Lum", "Jeannette Bohg", "C. Karen Liu"], "title": "SimToolReal: An Object-Centric Policy for Zero-Shot Dexterous Tool Manipulation", "comment": null, "summary": "The ability to manipulate tools significantly expands the set of tasks a robot can perform. Yet, tool manipulation represents a challenging class of dexterity, requiring grasping thin objects, in-hand object rotations, and forceful interactions. Since collecting teleoperation data for these behaviors is challenging, sim-to-real reinforcement learning (RL) is a promising alternative. However, prior approaches typically require substantial engineering effort to model objects and tune reward functions for each task. In this work, we propose SimToolReal, taking a step towards generalizing sim-to-real RL policies for tool manipulation. Instead of focusing on a single object and task, we procedurally generate a large variety of tool-like object primitives in simulation and train a single RL policy with the universal goal of manipulating each object to random goal poses. This approach enables SimToolReal to perform general dexterous tool manipulation at test-time without any object or task-specific training. We demonstrate that SimToolReal outperforms prior retargeting and fixed-grasp methods by 37% while matching the performance of specialist RL policies trained on specific target objects and tasks. Finally, we show that SimToolReal generalizes across a diverse set of everyday tools, achieving strong zero-shot performance over 120 real-world rollouts spanning 24 tasks, 12 object instances, and 6 tool categories.", "AI": {"tldr": "本研究提出SimToolReal，一种通用型的sim-to-real强化学习策略，用于机器人工具操作。该方法通过程序化生成大量工具类物体，训练单一策略以应对各种工具操作任务，无需针对特定物体或任务进行额外训练。", "motivation": "传统的机器人工具操作数据收集困难，而现有的sim-to-real强化学习方法需要大量的工程投入来为每个任务建模物体和调整奖励函数。因此，研究的动机是开发一种更通用的sim-to-real强化学习方法，以克服这些挑战。", "method": "该研究采用程序化生成大量工具类物体的方法，并在模拟环境中训练一个单一的强化学习策略。该策略的目标是学习通用性的工具操作能力，使其能够将任意工具物体操纵到随机的目标姿态。", "result": "SimToolReal在零样本工具操作任务中表现出色，比现有方法（如retargeting和fixed-grasp）的性能提高了37%，并能与针对特定物体和任务训练的专业RL策略相媲美。在实际操作中，该策略在24个任务、12个物体实例和6个工具类别中共计120次测试中均取得了良好的零样本性能。", "conclusion": "SimToolReal成功实现了通用的sim-to-real强化学习策略，能够处理多样化的工具操作任务，而无需针对特定物体或任务进行额外的训练。这为机器人工具操作的泛化性提供了一种有前景的解决方案。"}}
{"id": "2602.16852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16852", "abs": "https://arxiv.org/abs/2602.16852", "authors": ["Minh Duc Bui", "Manuel Mager", "Peter Herbert Kann", "Katharina von der Wense"], "title": "Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect", "comment": "Accepted at LREC 2026", "summary": "Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.", "AI": {"tldr": "本研究首次对美因茨方言（Meenzerisch）进行了自然语言处理（NLP）研究，开发了一个数字词典数据集，并评估了大型语言模型（LLMs）在生成该方言词汇定义和根据定义生成词汇方面的能力。结果显示LLMs表现不佳，表明需要更多资源和对德语方言的深入研究。", "motivation": "Meenzerisch方言濒临灭绝，而NLP技术有潜力帮助语言和方言的保护与复兴。然而，目前尚未有NLP研究关注Meenzerisch方言。", "method": "研究人员首先从现有资源（Schramm, 1966）中提取数据，创建了一个包含2,351个方言词汇及其德语标准语定义的数字词典数据集。然后，利用该数据集，通过实验评估了最先进的大型语言模型（LLMs）在两个任务上的表现：（1）根据德语标准语定义生成Meenzerisch词汇；（2）根据Meenzerisch词汇生成德语标准语定义。此外，还尝试了少样本学习和从训练集中提取规则后输入LLM的方法来提升性能。", "result": "实验结果显示，LLMs在两个任务上的表现均不理想。在生成德语标准语定义方面，最佳模型的准确率仅为6.27%；而在根据德语标准语定义生成Meenzerisch词汇方面，最佳模型的准确率更是只有1.51%。尽管少样本学习和提取规则的方法能够略微提高准确率，但整体性能仍低于10%。", "conclusion": "现有的大型语言模型在处理Meenzerisch方言时能力有限，无法有效地生成方言词汇的定义或根据定义生成词汇。这项研究突显了为德语方言开发更多资源以及加强相关研究的紧迫性。"}}
{"id": "2602.16807", "categories": ["cs.AI", "cs.DM", "math.CO"], "pdf": "https://arxiv.org/pdf/2602.16807", "abs": "https://arxiv.org/abs/2602.16807", "authors": ["Duncan Soiffer", "Nathaniel Itty", "Christopher D. Rosin", "Blake Bruell", "Mason DiCicco", "Gábor N. Sárközy", "Ryan Offstein", "Daniel Reichman"], "title": "Improved Upper Bounds for Slicing the Hypercube", "comment": null, "summary": "A collection of hyperplanes $\\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\\{-1,1\\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \\leq \\lceil \\frac{4n}{5} \\rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \\leq \\frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \\leq \\lceil\\frac{5n}{6} \\rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2602.16922", "categories": ["cs.OH"], "pdf": "https://arxiv.org/pdf/2602.16922", "abs": "https://arxiv.org/abs/2602.16922", "authors": ["Md. Ismiel Hossen Abir"], "title": "A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms", "comment": null, "summary": "Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.", "AI": {"tldr": "研究了RSA在经典和量子攻击下的脆弱性，并提出了一种结合AES、BB84 QKD、量子状态比较和生物启发免疫系统的混合安全框架，以应对后量子时代的威胁。", "motivation": "RSA密码系统面临量子计算（特别是Shor算法）的威胁，需要设计一种能够抵御后量子攻击的安全框架。", "method": "提出一个概念性框架，结合了AES加密（经典安全）、BB84量子密钥分发（QKD，安全密钥交换和窃听检测）、量子状态比较（轻量级认证）以及生物启发免疫系统（自适应威胁检测）。", "result": "RSA易受Shor算法攻击。BB84在理想条件下能实现密钥协商，并能高精度检测窃听。提出的概念模型整合了经典和量子安全方法，具备可扩展性和自适应性。", "conclusion": "该研究提出了一个用于后量子数据保护的混合安全框架的概念模型，该模型结合了经典和量子安全机制。详细实现、安全证明和实验验证是未来工作。"}}
{"id": "2602.17247", "categories": ["eess.SY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.17247", "abs": "https://arxiv.org/abs/2602.17247", "authors": ["Sehani Siriwardana", "Jean Michel de Souza Sant'Ana", "Richard Demo Souza", "Abolfazl Zakeri", "Onel Luis Alcaraz López"], "title": "On the Value of Base Station Motion Knowledge for Goal-Oriented Remote Monitoring with Energy-Harvesting Sensors", "comment": "6 pages, 4 figures, submitted for per-review", "summary": "This paper investigates goal-oriented remote monitoring of an unobservable Markov source using energy-harvesting sensors that communicate with a mobile receiver, such as a Low Earth Orbit (LEO) satellite or Unmanned Aerial Vehicle (UAV). Unlike conventional systems that assume stationary base stations, the proposed framework explicitly accounts for receiver mobility, which induces time-varying channel characteristics modeled as a finite-state Markov process. The remote monitoring problem is formulated as a partially observable Markov decision process (POMDP), which is transformed into a tractable belief-state MDP and solved using relative value iteration to obtain optimal sampling and transmission policies. Two estimation strategies are considered: Maximum Likelihood (ML) and Minimum Mean Distortion (MMD). Numerical results demonstrate that incorporating receiver mobility and channel state information into the optimization reduces the average distortion by 10% to 42% compared to baseline policies and constant-channel assumptions, highlighting the importance of base station motion knowledge for effective goal-oriented communication.", "AI": {"tldr": "该研究提出了一种用于能量收集传感器远程监控不可观测马尔可夫信源的框架，该传感器与移动接收器（如 LEO 卫星或无人机）通信。该框架考虑了接收器移动性导致的时变信道特性，并将其建模为部分可观察的马尔可夫决策过程（POMDP），通过相对价值迭代求解以获得最优采样和传输策略。研究表明，考虑接收器移动性和信道状态信息可显著降低平均失真。", "motivation": "传统远程监控系统通常假设基站固定，但现实中接收器（如 LEO 卫星、UAV）是移动的，这导致信道特性随时间变化。因此，需要一种能够处理接收器移动性的远程监控方法，以提高监控效率和准确性。", "method": "将远程监控问题建模为部分可观察的马尔可夫决策过程（POMDP），并将其转换为一个可处理的信念状态 MDP。然后使用相对价值迭代算法求解该信念状态 MDP，以获得最优的采样和传输策略。研究中考虑了两种估计策略：最大似然（ML）和最小均方失真（MMD）。", "result": "数值结果表明，与基线策略和恒定信道假设相比，在优化中纳入接收器移动性和信道状态信息可以将平均失真降低 10% 到 42%。", "conclusion": "接收器的移动性对远程监控系统的性能有显著影响。知识接收器运动和信道状态信息对于实现有效的面向目标的通信至关重要，并且可以显著提高监控的准确性。"}}
{"id": "2602.16843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16843", "abs": "https://arxiv.org/abs/2602.16843", "authors": ["Ahmed Rafid", "Rumman Adib", "Fariya Ahmed", "Ajwad Abrar", "Mohammed Saidul Islam"], "title": "BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization", "comment": "Accepted in 2nd LoResLM at EACL 2026", "summary": "Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.", "AI": {"tldr": "提出了一种名为 BanglaSummEval 的无参考、基于问答的框架，用于评估孟加拉语文本摘要的事实一致性，该框架通过多语言语言模型自动生成问题和答案，并使用 BERTScore-Recall 进行答案比较，已在孟加拉语摘要上得到验证。", "motivation": "现有的事实一致性评估指标往往忽略孟加拉语，且依赖参考摘要。在医疗和新闻等高风险领域，准确的事实一致性评估至关重要。", "method": "开发了 BanglaSummEval 框架，利用单个多语言指令微调语言模型自动生成问题和答案，评估事实准确性和内容覆盖率。使用 BERTScore-Recall 比较答案以捕捉深层语义一致性。", "result": "BanglaSummEval 在人类编写的摘要上与专家人类判断表现出高度相关性（Pearson's r = 0.694，Spearman's ρ = 0.763）。", "conclusion": "BanglaSummEval 提供了一种实用且透明的方法，用于在资源匮乏的语言环境中评估孟加拉语摘要的事实一致性，并提供可解释的诊断信息。"}}
{"id": "2602.16938", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16938", "abs": "https://arxiv.org/abs/2602.16938", "authors": ["Ofer Meshi", "Krisztian Balog", "Sally Goldman", "Avi Caciularu", "Guy Tennenholtz", "Jihwan Jeong", "Amir Globerson", "Craig Boutilier"], "title": "ConvApparel: A Benchmark Dataset and Validation Framework for User Simulators in Conversational Recommenders", "comment": "EACL 2026", "summary": "The promise of LLM-based user simulators to improve conversational AI is hindered by a critical \"realism gap,\" leading to systems that are optimized for simulated interactions, but may fail to perform well in the real world. We introduce ConvApparel, a new dataset of human-AI conversations designed to address this gap. Its unique dual-agent data collection protocol -- using both \"good\" and \"bad\" recommenders -- enables counterfactual validation by capturing a wide spectrum of user experiences, enriched with first-person annotations of user satisfaction. We propose a comprehensive validation framework that combines statistical alignment, a human-likeness score, and counterfactual validation to test for generalization. Our experiments reveal a significant realism gap across all simulators. However, the framework also shows that data-driven simulators outperform a prompted baseline, particularly in counterfactual validation where they adapt more realistically to unseen behaviors, suggesting they embody more robust, if imperfect, user models.", "AI": {"tldr": "本研究提出了ConvApparel数据集和一套验证框架，用于解决基于LLM的用户模拟器与真实用户交互之间的“真实性差距”，并评估了不同模拟器的泛化能力。", "motivation": "基于LLM的用户模拟器在改进对话AI方面潜力巨大，但存在“真实性差距”，导致优化后的系统在真实世界表现不佳。", "method": "使用“好”和“坏”推荐者进行双代理数据收集，构建ConvApparel数据集，该数据集包含用户满意度的第一人称注解。提出一个结合统计对齐、人类相似度评分和反事实验证的综合验证框架来测试泛化能力。", "result": "所有模拟器都存在显著的真实性差距。数据驱动的模拟器在统计对齐和人类相似度方面优于提示基线，并且在反事实验证方面表现更佳，能更真实地适应未见过行为。", "conclusion": "虽然用户模拟器仍存在真实性差距，但数据驱动的方法能够构建更鲁棒的用户模型，从而实现更好的泛化能力，并为未来对话AI的研究和开发提供了新的资源和评估方法。"}}
{"id": "2602.17297", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17297", "abs": "https://arxiv.org/abs/2602.17297", "authors": ["Jan H. Hoekstra", "Bendegúz M. Györök", "Roland Tóth", "Maarten Schoukens"], "title": "Learning-based augmentation of first-principle models: A linear fractional representation-based approach", "comment": "Automatica submission under review", "summary": "Nonlinear system identificationhas proven to be effective in obtaining accurate models from data for complex real-world systems. In particular, recent encoder-based methods with artificial neural network state-space (ANN-SS) models have achieved state-of-the-art performance on various benchmarks, using computationally efficient methods and offering consistent model estimation in the presence of noisy data. However, inclusion of prior knowledge of the system can be further exploited to increase (i) estimation speed, (ii) accuracy, and (iii) interpretability of the resulting models. This paper proposes a model augmentation method that incorporates prior knowledge from first-principles (FP) models in a flexible manner. We introduce a novel linear-fractional-representation (LFR) model structure that allows for the general representation of various augmentation structures including the ones that are commonly used in the literature, and an encoder-based identification algorithm for estimating the proposed structures together with appropriate initialisation methods. The performance and generalisation capabilities of the proposed method are demonstrated on the identification of a hardening mass-spring-damper system in a simulation study and on the data-driven modelling of the dynamics of an F1Tenth electric car using measured data.", "AI": {"tldr": "本研究提出了一种新颖的线性分数表示（LFR）模型结构，用于将第一性原理（FP）知识灵活地集成到基于编码器的人工神经网络状态空间（ANN-SS）模型中，以提高系统辨识的速度、准确性和可解释性，并在模拟和实际的F1Tenth电动汽车动力学建模中进行了验证。", "motivation": "现有基于编码器的人工神经网络状态空间（ANN-SS）模型在非线性系统辨识方面表现出色，但未能充分利用系统的先验知识，而引入先验知识有望进一步提升辨识速度、准确性和模型可解释性。", "method": "提出了一种新颖的线性分数表示（LFR）模型结构，用于整合第一性原理（FP）知识，并设计了一种基于编码器的辨识算法来估计该结构，同时提供相应的初始化方法。", "result": "在模拟的硬化弹簧-质量-阻尼器系统和真实的F1Tenth电动汽车动力学建模数据上，证明了所提方法的性能和泛化能力。", "conclusion": "通过将第一性原理知识灵活地集成到ANN-SS模型中，所提出的LFR模型结构和辨识算法能够有效提高非线性系统辨识的效率、精度和可解释性。"}}
{"id": "2602.16870", "categories": ["cs.RO", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.16870", "abs": "https://arxiv.org/abs/2602.16870", "authors": ["Daniil Lisus", "Katya M. Papais", "Cedric Le Gentil", "Elliot Preston-Krebs", "Andrew Lambert", "Keith Y. K. Leung", "Timothy D. Barfoot"], "title": "Boreas Road Trip: A Multi-Sensor Autonomous Driving Dataset on Challenging Roads", "comment": "23 pages, 15 figures, 12 tables, submitted to The International Journal of Robotics Research (IJRR)", "summary": "The Boreas Road Trip (Boreas-RT) dataset extends the multi-season Boreas dataset to new and diverse locations that pose challenges for modern autonomous driving algorithms. Boreas-RT comprises 60 sequences collected over 9 real-world routes, totalling 643 km of driving. Each route is traversed multiple times, enabling evaluation in identical environments under varying traffic and, in some cases, weather conditions. The data collection platform includes a 5MP FLIR Blackfly S camera, a 360 degree Navtech RAS6 Doppler-enabled spinning radar, a 128-channel 360 degree Velodyne Alpha Prime lidar, an Aeva Aeries II FMCW Doppler-enabled lidar, a Silicon Sensing DMU41 inertial measurement unit, and a Dynapar wheel encoder. Centimetre-level ground truth is provided via post-processed Applanix POS LV GNSS-INS data. The dataset includes precise extrinsic and intrinsic calibrations, a publicly available development kit, and a live leaderboard for odometry and metric localization. Benchmark results show that many state-of-the-art odometry and localization algorithms overfit to simple driving environments and degrade significantly on the more challenging Boreas-RT routes. Boreas-RT provides a unified dataset for evaluating multi-modal algorithms across diverse road conditions. The dataset, leaderboard, and development kit are available at www.boreas.utias.utoronto.ca.", "AI": {"tldr": "Boreas-RT是一个新的、多样化的自动驾驶数据集，包含了在不同真实世界路线上的60个传感器序列，用于评估现有算法在复杂环境下的鲁棒性。", "motivation": "现有的自动驾驶数据集通常在简单的驾驶环境中收集，导致现有算法在更具挑战性的真实世界场景下表现不佳。研究动机在于创建一个更具挑战性和多样性的数据集，以促进自动驾驶算法在复杂环境下的鲁棒性提升。", "method": "数据集通过在9条不同的真实世界路线上收集60个传感器序列（包括相机、雷达、激光雷达、IMU和轮式编码器）来构建。数据收集平台配备了高分辨率传感器，并利用后处理的GNSS-INS数据提供厘米级地面真实度。数据集包含精确的内外标定信息，并提供了一个公开的开发套件和一个实时排行榜。", "result": "在Boreas-RT数据集上进行的基准测试表明，许多最先进的里程计和定位算法在简单环境中表现良好，但在更具挑战性的Boreas-RT路线上性能显著下降，存在过拟合现象。", "conclusion": "Boreas-RT数据集为评估多模态自动驾驶算法在多样化道路条件下的性能提供了一个统一的平台，揭示了现有算法在应对复杂环境时存在的局限性，并为未来算法的改进指明了方向。"}}
{"id": "2602.16968", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16968", "abs": "https://arxiv.org/abs/2602.16968", "authors": ["Dahye Kim", "Deepti Ghadiyaram", "Raghudeep Gadde"], "title": "DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\\times$ and $3.2\\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.", "AI": {"tldr": "提出了一种名为“动态分词”的测试时策略，通过根据内容复杂度和去噪时间步动态调整图像块大小，提高了扩散 Transformer（DiT）的推理效率，同时保持了生成质量。", "motivation": "DiTs 在图像和视频生成方面取得了 SOTA 性能，但计算成本高昂，主要是由于在整个去噪过程中使用固定大小的图像块进行分词，效率低下。", "method": "提出了一种名为“动态分词”的测试时策略。该策略的核心思想是，在去噪的早期阶段，使用较大的图像块来建模全局结构；在后期阶段，使用较小的图像块来细化局部细节。在推理过程中，根据内容复杂度和去噪时间步动态调整图像块的大小。", "result": "在 FLUX-1.Dev 和 Wan 2.1 数据集上分别实现了高达 $3.52\times$ 和 $3.2\times$ 的速度提升，同时保持了生成质量和提示依从性。", "conclusion": "动态分词是一种有效的策略，能够显著提高 DiTs 的推理效率，并且不会牺牲图像和视频生成的感知质量。"}}
{"id": "2602.16898", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16898", "abs": "https://arxiv.org/abs/2602.16898", "authors": ["Iman Ahmadi", "Mehrshad Taji", "Arad Mahdinezhad Kashani", "AmirHossein Jadidi", "Saina Kashani", "Babak Khalaj"], "title": "MALLVI: a multi agent framework for integrated generalized robotics manipulation", "comment": null, "summary": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.", "AI": {"tldr": "MALLVi是一个结合大型语言模型（LLM）和视觉语言模型（VLM）的多智能体框架，通过闭环反馈实现机器人操作任务规划，提高了在动态环境中的鲁棒性和泛化能力。", "motivation": "现有的大型语言模型在机器人操作任务规划方面存在不足，容易在动态环境中因缺乏环境反馈而变得脆弱。", "method": "MALLVi框架包含Decomposer、Localizer、Thinker和Reflector四个核心智能体，协同处理感知、定位、推理和高层规划。一个可选的Descriptor智能体提供视觉记忆。框架通过VLM评估环境反馈，实现闭环控制，支持有针对性的错误检测和恢复。", "result": "在模拟和真实世界环境中，MALLVi在零样本操作任务中展现出更强的泛化能力和更高的成功率。", "conclusion": "MALLVi的多智能体闭环协调方法有效地解决了现有方法在机器人操作任务规划中的局限性，并在实际应用中取得了显著成效。"}}
{"id": "2602.16979", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16979", "abs": "https://arxiv.org/abs/2602.16979", "authors": ["Divyam Madaan", "Sumit Chopra", "Kyunghyun Cho"], "title": "Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling", "comment": null, "summary": "Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.", "AI": {"tldr": "本文提出了一种名为 PRIMO 的监督潜在变量填充模型，用于处理多模态学习中缺失模态的情况，并通过量化缺失模态的预测影响来提高模型性能。", "motivation": "现有的多模态大语言模型（MLLMs）在训练和推理时通常假定所有模态数据都可用，但这在实际中难以实现，因为模态数据常常不完整（缺失、异步收集或仅对部分样本可用）。", "method": "PRIMO 采用监督潜在变量模型，通过一个潜在变量来捕捉缺失模态与观测模态之间的关系，以实现预测。在推理时，通过从学习到的分布中抽取多个样本来获得边际预测分布，并分析缺失模态对每个实例预测的影响。模型使用基于方差的度量来量化模态的预测影响。", "result": "在合成 XOR 数据集、Audio-Vision MNIST 和 MIMIC-III 数据集上，PRIMO 在模态完全缺失时性能与单模态基线相当，在所有模态都可用时性能与多模态基线相当。模型能够量化实例级别的模态预测影响，并可视化不同填充结果对标签的影响。", "conclusion": "PRIMO 是一种有效的处理多模态学习中缺失模态的方法，它不仅能够利用不完整的训练数据，还能量化缺失模态的重要性，从而提高模型的鲁棒性和可解释性。"}}
{"id": "2602.16812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16812", "abs": "https://arxiv.org/abs/2602.16812", "authors": ["Zhongcan Xiao", "Leyi Zhang", "Guannan Zhang", "Xiaoping Wang"], "title": "NeuDiff Agent: A Governed AI Workflow for Single-Crystal Neutron Crystallography", "comment": null, "summary": "Large-scale facilities increasingly face analysis and reporting latency as the limiting step in scientific throughput, particularly for structurally and magnetically complex samples that require iterative reduction, integration, refinement, and validation. To improve time-to-result and analysis efficiency, NeuDiff Agent is introduced as a governed, tool-using AI workflow for TOPAZ at the Spallation Neutron Source that takes instrument data products through reduction, integration, refinement, and validation to a validated crystal structure and a publication-ready CIF. NeuDiff Agent executes this established pipeline under explicit governance by restricting actions to allowlisted tools, enforcing fail-closed verification gates at key workflow boundaries, and capturing complete provenance for inspection, auditing, and controlled replay. Performance is assessed using a fixed prompt protocol and repeated end-to-end runs with two large language model backends, with user and machine time partitioned and intervention burden and recovery behaviors quantified under gating. In a reference-case benchmark, NeuDiff Agent reduces wall time from 435 minutes (manual) to 86.5(4.7) to 94.4(3.5) minutes (4.6-5.0x faster) while producing a validated CIF with no checkCIF level A or B alerts. These results establish a practical route to deploy agentic AI in facility crystallography while preserving traceability and publication-facing validation requirements.", "AI": {"tldr": "NeuDiff Agent 是一种由 AI 驱动的工作流，可自动化 TOPAZ 仪器数据的分析过程，显著缩短从实验数据到发布级晶体结构的生成时间，并确保数据验证的准确性。", "motivation": "大型科学设施面临分析和报告延迟的问题，尤其是在处理结构和磁性复杂的样品时，这会限制科学产出。为了提高研究结果的时效性和分析效率，需要一种更快的自动化方法。", "method": "NeuDiff Agent 是一个受管的、使用工具的 AI 工作流，它自动化了 TOPAZ 仪器数据的约简、积分、精修和验证过程，直至生成经过验证的晶体结构和可发布的 CIF 文件。该工作流通过限制可调用工具、设置关键环节的验证门以及记录完整的溯源信息来实现。", "result": "在参考案例基准测试中，NeuDiff Agent 将手动操作所需的 435 分钟缩短至 86.5(4.7) 至 94.4(3.5) 分钟，速度提高了 4.6-5.0 倍。生成的 CIF 文件通过了所有验证检查，没有 A 或 B 级警报。", "conclusion": "NeuDiff Agent 为在设施晶体学中部署自动化 AI 提供了一条切实可行的途径，同时满足了可追溯性和面向发布的验证要求，显著提高了分析效率。"}}
{"id": "2602.17352", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17352", "abs": "https://arxiv.org/abs/2602.17352", "authors": ["Max Bruninx", "Seyed Soroush Karimi Madahi", "Timothy Verstraeten", "Jan Decuyper", "Chris Develder", "Jan Helsen"], "title": "Herd Behavior in Decentralized Balancing Models: A Case Study in Belgium", "comment": null, "summary": "In a decentralized balancing model, Balance Responsible Parties (BRPs) are encouraged by the Transmission System Operator (TSO) to deviate from their schedule to help the system restore balance, also referred to as implicit balancing. This could reduce balancing costs for the grid operator and lower the entry barrier for flexible assets compared to explicit balancing services. However, these implicit reactions may overshoot when their total capacity is high, potentially requiring more explicit activations. This study analyses the effect of increased participation in the decentralized balancing model in Belgium. To this end, we develop a market simulator that produces price signals on minute-level and simulate the implicit reactions for battery assets with different risk profiles. Besides the current price formula, we also study two potential candidates for the near-term presented by the TSO. A simulation study is conducted using Belgian market data for the year 2023. The findings indicate that, while having a significant positive effect on the balancing costs at first, the risk of overshoots can outweigh the potential benefits when the total capacity of the implicit reactions becomes too large. Furthermore, even when the balancing costs start to increase for the TSO, BRPs were still found to benefit from implicit balancing.", "AI": {"tldr": "该研究分析了比利时去中心化平衡模型中，增加参与者对平衡成本和电池资产反应的影响。结果表明，虽然初期能降低平衡成本，但过高的隐含反应能力可能导致成本增加，但仍对平衡责任方（BRPs）有利。", "motivation": "去中心化平衡模型（隐含平衡）有可能降低平衡成本和准入门槛，但过高的参与度可能导致过度反应，反而增加显式激活的需求。因此，研究其在高参与度下的效果。", "method": "开发了一个市场模拟器，在分钟级别产生价格信号，并模拟了不同风险偏好的电池资产的隐含反应。研究了当前价格公式和两种潜在的未来价格公式。使用2023年比利时市场数据进行模拟。", "result": "隐含平衡对平衡成本有显著的初期积极影响。然而，当隐含反应的总容量过大时，过度反应的风险会抵消潜在的益处，导致平衡成本上升。即使在平衡成本上升的情况下，BRPs 仍能从隐含平衡中获益。", "conclusion": "去中心化平衡模型的好处会随着参与者容量的增加而衰减，存在一个临界点，过多的隐含反应容量可能导致平衡成本增加。然而，在现有模型下，BRPs 即使在平衡成本上升时仍能受益。"}}
{"id": "2602.16814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16814", "abs": "https://arxiv.org/abs/2602.16814", "authors": ["Eiman Kanjo", "Mustafa Aslanov"], "title": "Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI", "comment": "16 pages, 3 figures, 3 tables, this paper introduces a new concept", "summary": "The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective", "AI": {"tldr": "本文提出了一种名为“节点学习”的去中心化学习范式，将智能部署在边缘节点，通过节点间的选择性交互进行知识传播，以解决中心化智能在边缘部署的成本和脆弱性问题。", "motivation": "中心化AI在边缘部署面临数据传输、延迟、能耗和对大型数据中心的依赖等瓶颈，在异构、移动和资源受限的环境中表现不佳。", "method": "节点学习是一种去中心化学习范式，每个节点独立学习并维护模型状态，通过选择性地与对等节点交换知识。学习通过重叠和扩散传播，而不是全局同步或中心聚合。它统一了自主和合作行为，并能适应数据、硬件、目标和连接性的异质性。", "result": "该概念性论文探讨了节点学习的概念基础、与现有去中心化方法的对比，并审视了其在通信、硬件、信任和治理方面的影响。", "conclusion": "节点学习是一种更广泛的去中心化视角，它不抛弃现有范式，而是将它们置于其中，旨在解决边缘AI的挑战。"}}
{"id": "2602.16827", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16827", "abs": "https://arxiv.org/abs/2602.16827", "authors": ["Luis Merino", "Gabriel Navarro", "Carlos Salvatierra", "Evangelina Santos"], "title": "An order-oriented approach to scoring hesitant fuzzy elements", "comment": null, "summary": "Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.\n  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.", "AI": {"tldr": "本文提出了一个统一的、基于序理论的犹豫模糊集评分框架，并引入了考虑控制集的优势函数用于排序和群体决策。", "motivation": "传统的犹豫模糊集评分方法缺乏序理论基础，且存在一些关于经典序关系的误解。研究旨在为评分机制提供更灵活、更一致的理论依据，并提出新的排序方法。", "method": "提出了一个统一的评分框架，明确定义了评分与序的关系。分析了犹豫模糊元素上的几种经典序关系，证明它们并非格结构。定义了满足规范性标准的基于对称序的评分函数。引入了优势函数用于考虑控制集的排序，并给出了离散优势函数和相对优势函数的具体实例。", "result": "证明了经典序关系并非格结构。基于对称序的评分函数满足强单调性和Gärdenfors条件。提出的优势函数能够用于构建犹豫模糊集上的模糊偏好关系，并支持群体决策。", "conclusion": "基于序理论的评分框架为犹豫模糊集提供了更严谨的评分方法。优势函数为犹豫模糊集的排序和群体决策提供了新的有效工具。"}}
{"id": "2602.16911", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.16911", "abs": "https://arxiv.org/abs/2602.16911", "authors": ["Adrian Röfer", "Nick Heppert", "Abhinav Valada"], "title": "SparTa: Sparse Graphical Task Models from a Handful of Demonstrations", "comment": "9 pages, 6 figures, under review", "summary": "Learning long-horizon manipulation tasks efficiently is a central challenge in robot learning from demonstration. Unlike recent endeavors that focus on directly learning the task in the action domain, we focus on inferring what the robot should achieve in the task, rather than how to do so. To this end, we represent evolving scene states using a series of graphical object relationships. We propose a demonstration segmentation and pooling approach that extracts a series of manipulation graphs and estimates distributions over object states across task phases. In contrast to prior graph-based methods that capture only partial interactions or short temporal windows, our approach captures complete object interactions spanning from the onset of control to the end of the manipulation. To improve robustness when learning from multiple demonstrations, we additionally perform object matching using pre-trained visual features. In extensive experiments, we evaluate our method's demonstration segmentation accuracy and the utility of learning from multiple demonstrations for finding a desired minimal task model. Finally, we deploy the fitted models both in simulation and on a real robot, demonstrating that the resulting task representations support reliable execution across environments.", "AI": {"tldr": "该研究提出了一种新的方法，通过学习机器人操控任务中物体关系的图形表示，来实现从演示中高效地学习长时程操控任务。该方法能够捕获完整的物体交互，并能处理多演示学习，最终在模拟和真实机器人上都取得了可靠的执行效果。", "motivation": "当前的机器人学习方法多集中于直接在动作域学习任务，而难以高效地学习长时程操控任务。研究者希望找到一种方法，能够关注于推理机器人应该实现的目标，而非具体如何实现。", "method": "该方法使用一系列物体关系图来表示不断变化的场景状态。提出了一种演示分割和池化方法，用于提取一系列操控图，并估计任务不同阶段物体状态的分布。为了在多演示学习中提高鲁棒性，还使用了预训练的视觉特征进行物体匹配。", "result": "在实验中，该方法在演示分割准确性上得到了评估，并证明了多演示学习对于寻找最小任务模型的作用。部署在模拟和真实机器人上的结果表明，学习到的任务表示支持跨环境的可靠执行。", "conclusion": "该研究提出了一种有效的、基于物体关系图的方法，能够从演示中学习长时程操控任务。该方法能够捕获完整的物体交互，并且通过物体匹配提升了多演示学习的鲁棒性，最终能够实现可靠的任务执行。"}}
{"id": "2602.17379", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17379", "abs": "https://arxiv.org/abs/2602.17379", "authors": ["Renato Quartullo", "Andrea Garulli", "Mirko Leomanni"], "title": "Robust Model Predictive Control for Linear Systems with Interval Matrix Model Uncertainty", "comment": null, "summary": "This paper proposes a novel robust Model Predictive Control (MPC) scheme for linear discrete-time systems affected by model uncertainty described by interval matrices.\n  The key feature of the proposed method is a bound on the uncertainty propagation along the prediction horizon which exploits a set-theoretic over-approximation of each term of the uncertain system impulse response.\n  Such an approximation is based on matrix zonotopes and leverages the interval matrix structure of the uncertainty model.\n  Its main advantage is that all the relevant bounds are computed offline, thus making the online computational load independent of the number of uncertain parameters.\n  A variable-horizon MPC formulation is adopted to guarantee recursive feasibility and to ensure robust asymptotic stability of the closed-loop system.\n  Numerical simulations demonstrate that the proposed approach is able to match the feasibility regions of the most effective state-of-the-art methods, while significantly reducing the computational burden, thereby enabling the treatment of nontrivial dimensional systems with multiple uncertain parameters.", "AI": {"tldr": "提出了一种新颖的鲁棒模型预测控制（MPC）方案，用于处理受区间矩阵不确定性影响的线性离散时间系统。该方法通过使用矩阵凸多边形（matrix zonotopes）对不确定性传播进行过近似，将大部分计算转移到离线阶段，从而显著降低了在线计算的负担。", "motivation": "现有鲁棒MPC方法在线计算量大，难以处理多参数不确定系统。研究动机是开发一种能有效处理模型不确定性，同时降低在线计算复杂度的鲁棒MPC方案。", "method": "1. 采用区间矩阵描述系统不确定性。2. 利用矩阵凸多边形（matrix zonotopes）对不确定性传播进行集合论意义上的过近似。3. 采用可变预测时域的MPC框架，以保证递归可行性和鲁棒渐近稳定性。4. 将大部分计算（不确定性界限的计算）离线完成。", "result": "所提出的MPC方案能够与现有最有效的方法相媲美，在可行性区域方面不落下风，同时显著降低了计算复杂度。这使得处理具有多个不确定参数的非平凡维度系统成为可能。", "conclusion": "该鲁棒MPC方法通过一种新颖的不确定性传播近似技术，成功地在保证鲁棒稳定性和可行性的前提下，降低了在线计算成本，从而为解决更复杂的不确定系统控制问题提供了有效途径。"}}
{"id": "2602.16957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16957", "abs": "https://arxiv.org/abs/2602.16957", "authors": ["Hasan Can Biyik", "Libby Barak", "Jing Peng", "Anna Feldman"], "title": "When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English", "comment": null, "summary": "Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.", "AI": {"tldr": "研究发现，在多语言委婉语检测中，仅仅词语在语义上的重叠并不能保证跨语言的有效迁移，尤其是在土耳其语到英语这种低资源方向，甚至可能导致性能下降。", "motivation": "在多语言环境中对委婉语进行建模具有挑战性，因为委婉语的含义很大程度上依赖于文化和语用背景。本研究旨在探讨跨语言的等价性如何影响多语言委婉语检测中的知识迁移。", "method": "研究将土耳其语和英语中的潜在委婉语（PETs）根据其功能、语用和语义的对齐程度分为重叠（OPETs）和非重叠（NOPETs）两类。然后，研究分析了在不同语言对（尤其是土耳其语到英语）之间进行迁移学习时，这两类委婉语对模型性能的影响。", "result": "研究发现，迁移存在不对称性。仅仅语义上的重叠不足以保证正向迁移，特别是在低资源的土耳其语到英语方向，即使是重叠的委婉语，性能也可能下降。在某些情况下，基于非重叠委婉语进行训练反而能提升性能。标签分布的差异可以解释这些反直觉的结果。此外，类别层面的分析表明，迁移可能受到领域特定对齐的影响，但证据受数据稀疏性的限制。", "conclusion": "跨语言委婉语检测中的迁移效果并非简单地由词语的语义重叠决定，语用、功能对齐以及数据分布等因素同样重要。在低资源场景下，需要更精细的方法来处理委婉语的跨语言迁移问题。"}}
{"id": "2602.17030", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17030", "abs": "https://arxiv.org/abs/2602.17030", "authors": ["Eric Chen", "Patricia Alves-Oliveira"], "title": "Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings", "comment": null, "summary": "As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.", "AI": {"tldr": "该研究提出了一种基于图像块的空间归因框架，用于区分人类与机器人绘画作品，并在一个案例研究中取得了较高的准确率，同时开发了一种量化混合风格重叠度的方法。", "motivation": "随着人工智能在创意生产中的作用日益增强，明确区分人类和人工智能在协作创作中的 authorship（作者身份）变得至关重要，尤其是在艺术收藏和法律领域。", "method": "研究人员开发了一个基于图像块（patch-based）的框架，利用商品平板扫描仪获取绘画图像，并采用 leave-one-painting-out cross-validation（交叉验证）进行训练和评估。该方法通过比较图像块的风格特征来区分作者。此外，还使用条件香农熵（conditional Shannon entropy）来量化混合风格的重叠度。", "result": "该框架在对一个人类艺术家和一个机器人系统创作的 15 幅抽象画的案例研究中，实现了 88.8% 的图像块级别准确率和 86.7% 的画作级别准确率（通过多数投票）。这优于基于纹理和预训练特征的基线方法。研究还发现，在混合 authorship 的区域，模型检测到的不确定性比纯粹的绘画区域高 64%（p=0.003），表明模型能够识别混合创作。", "conclusion": "该研究提出的基于图像块的空间归因框架为数据稀缺的人机协作创意工作流程提供了一个样本高效的 authorship 归因方法。虽然当前模型仅限于特定的艺术家-机器人组合，但该方法为未来将 authorship 归因扩展到更广泛的人机协作绘画实践奠定了基础。"}}
{"id": "2602.17407", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17407", "abs": "https://arxiv.org/abs/2602.17407", "authors": ["Glen Hjelmerud Mørkbak Sørensen", "Torleiv H. Bryne", "Kristoffer Gryte", "Tor Arne Johansen"], "title": "Bluetooth Phased-array Aided Inertial Navigation Using Factor Graphs: Experimental Verification", "comment": "6 pages, 5 figures, 2 tables. This work has been submitted to IFAC for possible publication", "summary": "Phased-array Bluetooth systems have emerged as a low-cost alternative for performing aided inertial navigation in GNSS-denied use cases such as warehouse logistics, drone landings, and autonomous docking. Basing a navigation system off of commercial-off-the-shelf components may reduce the barrier of entry for phased-array radio navigation systems, albeit at the cost of significantly noisier measurements and relatively short feasible range. In this paper, we compare robust estimation strategies for a factor graph optimisation-based estimator using experimental data collected from multirotor drone flight. We evaluate performance in loss-of-GNSS scenarios when aided by Bluetooth angular measurements, as well as range or barometric pressure.", "AI": {"tldr": "研究使用商用蓝牙设备进行GNSS受限环境下的惯性导航，并比较了基于因子图优化的估计器在不同辅助测量（蓝牙角度、距离或气压）下的鲁棒估计性能。", "motivation": "GNSS受限场景（如仓库、无人机着陆）下，低成本的辅助惯性导航需求日益增长，而商用蓝牙设备是实现这一目标的一个有潜力的低成本方案。然而，商用蓝牙测量噪声大且距离短，需要研究鲁棒的估计策略。", "method": "采用基于因子图优化的估计器，并使用多旋翼无人机飞行实验数据，在GNSS丢失的情况下，评估了不同辅助测量（蓝牙角度测量，以及蓝牙距离或气压计）下的鲁棒估计性能。", "result": "通过实验数据对比了不同鲁棒估计策略在GNSS受限场景下，使用蓝牙辅助时的导航性能。", "conclusion": "商用蓝牙设备可以作为低成本GNSS受限场景下惯性导航的辅助手段，研究了不同的辅助测量和鲁棒估计策略的有效性。"}}
{"id": "2602.17101", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17101", "abs": "https://arxiv.org/abs/2602.17101", "authors": ["Varun Burde", "Pavel Burget", "Torsten Sattler"], "title": "Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success", "comment": null, "summary": "3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots.", "AI": {"tldr": "本文提出了一个基于物理的基准测试，用于评估 3D 重建和 6D 位姿估计器在机器人抓取任务中的功能性。研究发现，重建伪影会减少抓取姿态候选，但对准确位姿估计下的抓取性能影响不大，且空间误差是影响抓取成功率的关键因素。", "motivation": "现有的 3D 重建方法在视觉和几何上表现出色，但其重建质量对下游机器人操控任务（如 6D 位姿估计和抓取姿态生成）的影响尚未得到充分评估。本文旨在弥合这一差距，提供一个能够衡量重建质量如何影响机器人抓取性能的基准。", "method": "构建了一个大规模、基于物理的基准测试，通过在不同的 3D 重建网格上生成抓取姿势，并在真实模型上进行模拟执行来评估 6D 位姿估计器和 3D 网格模型。通过这种方式，模拟了不完美的模型如何影响与真实物体的交互，从而评估位姿误差、抓取鲁棒性和 3D 重建的几何不准确性的组合影响。", "result": "研究结果表明，3D 重建中的伪影（artifacts）会显著减少抓取姿态候选的数量，但如果位姿估计准确，则对抓取性能的影响可以忽略不计。此外，抓取成功率与位姿误差之间的关系主要由空间误差主导，即使是简单的平移误差，也能为对称物体的抓取姿态成功率提供信息。", "conclusion": "本文提出的基准测试为评估 3D 重建和位姿估计器在机器人操作中的功能性提供了一个新的视角。研究结果强调了准确的位姿估计对于机器人抓取任务的重要性，并揭示了重建质量对抓取性能的潜在影响机制。这项工作有助于理解感知系统与机器人对象操作之间的关系。"}}
{"id": "2602.16832", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16832", "abs": "https://arxiv.org/abs/2602.16832", "authors": ["Priyaranjan Pattnayak", "Sanchari Chowdhuri"], "title": "IndicJR: A Judge-Free Benchmark of Jailbreak Robustness in South Asian Languages", "comment": "Accepted in EACL Industry Track Oral, 2026", "summary": "Safety alignment of large language models (LLMs) is mostly evaluated in English and contract-bound, leaving multilingual vulnerabilities understudied. We introduce \\textbf{Indic Jailbreak Robustness (IJR)}, a judge-free benchmark for adversarial safety across 12 Indic and South Asian languages (2.1 Billion speakers), covering 45216 prompts in JSON (contract-bound) and Free (naturalistic) tracks.\n  IJR reveals three patterns. (1) Contracts inflate refusals but do not stop jailbreaks: in JSON, LLaMA and Sarvam exceed 0.92 JSR, and in Free all models reach 1.0 with refusals collapsing. (2) English to Indic attacks transfer strongly, with format wrappers often outperforming instruction wrappers. (3) Orthography matters: romanized or mixed inputs reduce JSR under JSON, with correlations to romanization share and tokenization (approx 0.28 to 0.32) indicating systematic effects. Human audits confirm detector reliability, and lite-to-full comparisons preserve conclusions. IJR offers a reproducible multilingual stress test revealing risks hidden by English-only, contract-focused evaluations, especially for South Asian users who frequently code-switch and romanize.", "AI": {"tldr": "该研究提出了Indic Jailbreak Robustness (IJR)基准，用于评估12种南亚语言中大型语言模型的安全对齐，发现了合同限制会增加拒绝但无法阻止越狱，英译南亚语言攻击转移效果强，以及拼写和罗马化对模型安全性的影响。", "motivation": "现有大型语言模型的安全对齐评估主要集中在英语，对多语言环境下的漏洞研究不足，特别是在南亚地区，用户常使用多种语言混合和罗马化输入。", "method": "构建了一个包含45216个提示的IJR基准，涵盖12种南亚语言，分为JSON（合同限制）和Free（自然语言）两个测试集。通过评估LLaMA和Sarvam等模型在这些数据集上的越狱成功率（JSR）来揭示模型在不同语言和输入格式下的安全性。", "result": "1. 在JSON格式下，合同限制增加了模型的拒绝率，但未能完全阻止越狱，LLaMA和Sarvam的JSR超过0.92。在Free格式下，所有模型的JSR均达到1.0，拒绝率大幅下降。2. 从英语到南亚语言的攻击具有很强的迁移性，指令包装器（instruction wrappers）的效果不如格式包装器（format wrappers）。3. 输入的拼写和罗马化对JSR有显著影响，罗马化或混合输入的JSR在JSON格式下较低，且与罗马化比例和分词有关。", "conclusion": "IJR基准揭示了仅关注英语和合同限制的评估方法所隐藏的多语言安全风险。研究强调了在多语言环境中，特别是对于频繁混合语言和使用罗马化的南亚用户，评估模型安全性至关重要。拼写和罗马化等因素是影响模型安全性的重要方面。"}}
{"id": "2602.16959", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16959", "abs": "https://arxiv.org/abs/2602.16959", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh", "Mohammadali Keshtparvar"], "title": "Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry", "comment": null, "summary": "Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.", "AI": {"tldr": "该研究提出了一个考虑不确定性的计算框架，用于分析波斯古典诗歌中诗人的心理特征。该框架通过大规模自动多标签标注，将诗歌与心理概念关联，并量化诗歌的个体性和概念之间的关系，旨在实现可扩展且审慎的数字人文分析。", "motivation": "古典波斯诗歌通过隐喻、互文惯例和修辞含蓄来表达情感，这使得精确解读至关重要，但也限制了大规模的可重复比较。研究旨在开发一种能够处理这种复杂性和不确定性的计算方法。", "method": "研究构建了一个不确定性感知计算框架，利用大规模自动多标签标注。每个诗句被关联到一组心理概念、每种标签的置信度分数以及一个表明证据不足的弃权标志。通过置信度加权的证据聚合形成“诗人×概念”矩阵，将诗人视为概念上的概率分布。使用Jensen-Shannon散度和Kullback-Leibler散度量化诗歌的个体性。此外，还构建了一个置信度加权的“概念共现图”，并通过拉普拉斯谱分解定义了“Eigenmood”嵌入，以捕捉概念间的关系结构。", "result": "在包含61,573句诗歌和10位诗人的语料库上，有22.2%的诗句被标记为弃权，这凸显了不确定性在分析中的重要性。研究还报告了置信度阈值下的敏感性分析、将弃权作为一类并进行选择偏差诊断，以及一个通过Eigenmood轴检索诗句层面的例子的远距离到近距离的工作流程。", "conclusion": "该框架能够实现可扩展、可审计的数字人文分析，并通过从诗句层面的证据传播不确定性到诗人层面的推理，从而在量化分析中保留了审慎的解读态度。"}}
{"id": "2602.17033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17033", "abs": "https://arxiv.org/abs/2602.17033", "authors": ["Peize Li", "Zeyu Zhang", "Hao Tang"], "title": "PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing", "comment": null, "summary": "Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.", "AI": {"tldr": "PartRAG是一个检索增强生成框架，通过结合外部零件数据库和扩散Transformer，实现了单张图像的三维生成，并支持精确的局部编辑。它引入了分层对比检索来注入多样化的三维零件信息，并提供了一个零件级别的掩码编辑器，可以在不重新生成整个物体的情况下进行局部修改，同时保持多视图一致性。", "motivation": "现有的单图像三维生成方法在处理长尾分布的零件几何形状和保持多视图一致性方面存在困难，并且难以实现精确、局部的编辑。", "method": "PartRAG采用了一个检索增强的框架，集成了外部零件数据库和一个扩散Transformer。关键技术包括：1. 分层对比检索（Hierarchical Contrastive Retrieval）模块，用于将图像块与三维零件潜在表示对齐，并从包含1236个带标注零件的资产库中检索信息。2. 掩码式、零件级别的编辑器，可在共享的规范空间中操作，实现零件替换、属性修改和组合更新，同时保持非目标零件和多视图的一致性。", "result": "在Objaverse、ShapeNet和ABO数据集上取得了有竞争力的结果，在Objaverse数据集上将Chamfer Distance从0.1726降低到0.1528，F-Score从0.7472提高到0.844。推理时间为38秒，交互式编辑时间为5-8秒。定性上，PartRAG生成了更清晰的零件边界、更好的细长结构保真度，并在关节模型上表现出鲁棒性。", "conclusion": "PartRAG通过整合检索和可编辑表示，有效地解决了单图像三维生成中的零件多样性和编辑性挑战，实现了高质量、可控的三维生成。"}}
{"id": "2602.17434", "categories": ["eess.SY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17434", "abs": "https://arxiv.org/abs/2602.17434", "authors": ["Eleftherios E. Vlahakis", "Arash Bahari Kordabad", "Lars Lindemann", "Pantelis Sopasakis", "Sadegh Soudjani", "Dimos V. Dimarogonas"], "title": "Multi-Agent Temporal Logic Planning via Penalty Functions and Block-Coordinate Optimization", "comment": "Submitted to L-CSS", "summary": "Multi-agent planning under Signal Temporal Logic (STL) is often hindered by collaborative tasks that lead to computational challenges due to the inherent high-dimensionality of the problem, preventing scalable synthesis with satisfaction guarantees. To address this, we formulate STL planning as an optimization program under arbitrary multi-agent constraints and introduce a penalty-based unconstrained relaxation that can be efficiently solved via a Block-Coordinate Gradient Descent (BCGD) method, where each block corresponds to a single agent's decision variables, thereby mitigating complexity. By utilizing a quadratic penalty function defined via smooth STL semantics, we show that BCGD iterations converge to a stationary point of the penalized problem under standard regularity assumptions. To enforce feasibility, the BCGD solver is embedded within a two-layer optimization scheme: inner BCGD updates are performed for a fixed penalty parameter, which is then increased in an outer loop to progressively improve multi-agent STL robustness. The proposed framework enables scalable computations and is validated through various complex multi-robot planning scenarios.", "AI": {"tldr": "提出一种基于罚函数的块坐标梯度下降法，用于解决高维多智能体信号时序逻辑（STL）规划问题，实现可扩展且有保证的规划。", "motivation": "现有STL多智能体规划方法因维度过高而面临计算挑战，难以实现可扩展且有满意度保证的合成。", "method": "将STL规划转化为受约束的优化问题，并采用基于罚函数的无约束松弛方法。利用平滑STL语义定义的二次罚函数，结合块坐标梯度下降（BCGD）算法求解。BCGD算法嵌套在两层优化框架中，内层BCGD迭代固定罚参数，外层循环增加罚参数以提高鲁棒性。", "result": "BCGD迭代在正则性假设下收敛到罚函数的稳定点。该框架能够处理任意多智能体约束，并通过多机器人规划场景的实验验证了其可扩展性和有效性。", "conclusion": "提出的框架通过罚函数和BCGD算法有效解决了高维多智能体STL规划的计算复杂性问题，实现了可扩展且具有鲁棒性的规划合成。"}}
{"id": "2602.17003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17003", "abs": "https://arxiv.org/abs/2602.17003", "authors": ["Serin Kim", "Sangam Lee", "Dongha Lee"], "title": "Persona2Web: Benchmarking Personalized Web Agents for Contextual Reasoning with User History", "comment": null, "summary": "Large language models have advanced web agents, yet current agents lack personalization capabilities. Since users rarely specify every detail of their intent, practical web agents must be able to interpret ambiguous queries by inferring user preferences and contexts. To address this challenge, we present Persona2Web, the first benchmark for evaluating personalized web agents on the real open web, built upon the clarify-to-personalize principle, which requires agents to resolve ambiguity based on user history rather than relying on explicit instructions. Persona2Web consists of: (1) user histories that reveal preferences implicitly over long time spans, (2) ambiguous queries that require agents to infer implicit user preferences, and (3) a reasoning-aware evaluation framework that enables fine-grained assessment of personalization. We conduct extensive experiments across various agent architectures, backbone models, history access schemes, and queries with varying ambiguity levels, revealing key challenges in personalized web agent behavior. For reproducibility, our codes and datasets are publicly available at https://anonymous.4open.science/r/Persona2Web-73E8.", "AI": {"tldr": "本文提出了Persona2Web，一个用于评估个性化网络代理的新基准，强调代理应根据用户历史而非显式指令来推断用户偏好和上下文，以处理模糊查询。该基准包含用户历史、模糊查询和一种评估框架。", "motivation": "现有的大型语言模型虽然在网络代理方面取得了进展，但缺乏个性化能力。为了解决用户查询中的模糊性问题，需要网络代理能够根据用户偏好和上下文进行推断，而不是依赖于用户给出所有详细指令。", "method": "提出了Persona2Web基准，基于“澄清以个性化”的原则。该基准包含：（1）揭示用户长期隐性偏好的用户历史；（2）需要代理推断隐性用户偏好的模糊查询；（3）一个支持细粒度个性化评估的、具有推理意识的评估框架。通过广泛的实验来评估各种代理架构、主干模型、历史访问方案和不同模糊程度的查询。", "result": "实验揭示了个性化网络代理行为中的关键挑战。通过对不同模型和配置的广泛实验，研究者能够理解影响个性化效果的因素。", "conclusion": "Persona2Web为评估个性化网络代理提供了一个新的、重要的基准。研究表明，在处理模糊查询并实现真正个性化方面，网络代理仍面临挑战。代码和数据集已公开以供复现。"}}
{"id": "2602.17047", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17047", "abs": "https://arxiv.org/abs/2602.17047", "authors": ["Chaojie Yang", "Tian Li", "Yue Zhang", "Jun Gao"], "title": "Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers", "comment": null, "summary": "Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.", "AI": {"tldr": "该研究提出了一种高效的压缩框架，能够将大型文本到图像生成模型（如 60 层的 MMDiT-based Qwen-Image）压缩成更小的模型（Amber-Image-10B 和 Amber-Image-6B），无需从头开始训练。该框架通过时间步敏感的深度剪枝、局部权重平均、层级蒸馏以及混合流架构等技术，显著减少了模型参数和计算成本，同时保持了高质量的图像生成和文本渲染能力。", "motivation": "现有的 Diffusion Transformer (DiT) 架构在文本到图像生成方面表现出色，但计算成本高昂，难以部署。研究者希望通过高效压缩框架来解决这一问题，降低模型的计算开销和部署门槛。", "method": "研究者提出了一种无需从头训练的压缩框架。首先，通过时间步敏感的深度剪枝策略生成 Amber-Image-10B，并利用局部权重平均进行层初始化，然后通过层级蒸馏和全参数微调进行优化。在此基础上，进一步开发 Amber-Image-6B，采用混合流架构，将深层双流转换为单流，并进行渐进式蒸馏和轻量级微调。", "result": "Amber-Image 模型相比原始模型参数减少了 70%，且整个压缩和训练流程（从 10B 到 6B）仅需不到 2000 GPU 小时。在 DPG-Bench 和 LongText-Bench 等基准测试中，Amber-Image 实现了高保真合成和卓越的文本渲染效果，性能可媲美更大的模型。", "conclusion": "所提出的压缩框架能够有效地将大型 Diffusion Transformer 模型转化为轻量级模型，显著降低计算成本和部署难度，同时保持甚至超越大型模型的性能。Amber-Image 系列模型展示了高效文本到图像生成的潜力。"}}
{"id": "2602.16855", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16855", "abs": "https://arxiv.org/abs/2602.16855", "authors": ["Haiyang Xu", "Xi Zhang", "Haowei Liu", "Junyang Wang", "Zhaozai Zhu", "Shengjie Zhou", "Xuhao Hu", "Feiyu Gao", "Junjie Cao", "Zihua Wang", "Zhiyuan Chen", "Jitong Liao", "Qi Zheng", "Jiahui Zeng", "Ze Xu", "Shuai Bai", "Junyang Lin", "Jingren Zhou", "Ming Yan"], "title": "Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents", "comment": "25 pages, 11 figures, 11 tables", "summary": "The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.", "AI": {"tldr": "本文介绍了GUI-Owl-1.5，一个支持多平台（桌面、移动、浏览器等）的最新原生GUI代理模型，提供多种尺寸（2B/4B/8B/32B/235B）并支持指令/思考变体，实现了云边协作和实时交互。它在20多个GUI基准测试中取得了最先进的成果。", "motivation": "推动原生GUI代理模型的发展，使其能够支持更广泛的平台、实现云边协作和实时交互，并提高在GUI自动化、基础定位、工具调用、记忆和知识等任务上的性能。", "method": "提出了GUI-Owl-1.5模型，并引入了三个关键创新：1. 混合数据飞轮（Hybird Data Flywheel），结合模拟和云沙盒环境进行UI理解和轨迹生成；2. 代理能力统一增强（Unified Enhancement of Agent Capabilities），通过统一的思考合成管道提升模型推理能力，特别是工具/MCP使用、记忆和多智能体适应性；3. 多平台环境强化学习扩展（Multi-platform Environment RL Scaling），提出新的MRPO算法以解决多平台冲突和长时域任务训练效率低的问题。", "result": "GUI-Owl-1.5在20多个GUI基准测试中取得了最先进的性能：GUI自动化任务上，OSWorld获得56.5，AndroidWorld获得71.6，WebArena获得48.4；基础定位任务上，ScreenSpotPro获得80.3；工具调用任务上，OSWorld-MCP获得47.6，MobileWorld获得46.8；记忆和知识任务上，GUI-Knowledge Bench获得75.5。", "conclusion": "GUI-Owl-1.5通过其多平台支持、混合数据飞轮、统一增强的代理能力和MRPO算法，在多个GUI相关任务上取得了最先进的成果，并已开源，提供在线演示。"}}
{"id": "2602.17110", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17110", "abs": "https://arxiv.org/abs/2602.17110", "authors": ["Tanisha Parulekar", "Ge Shi", "Josh Pinskier", "David Howard", "Jen Jen Chung"], "title": "Grasp Synthesis Matching From Rigid To Soft Robot Grippers Using Conditional Flow Matching", "comment": null, "summary": "A representation gap exists between grasp synthesis for rigid and soft grippers. Anygrasp [1] and many other grasp synthesis methods are designed for rigid parallel grippers, and adapting them to soft grippers often fails to capture their unique compliant behaviors, resulting in data-intensive and inaccurate models. To bridge this gap, this paper proposes a novel framework to map grasp poses from a rigid gripper model to a soft Fin-ray gripper. We utilize Conditional Flow Matching (CFM), a generative model, to learn this complex transformation. Our methodology includes a data collection pipeline to generate paired rigid-soft grasp poses. A U-Net autoencoder conditions the CFM model on the object's geometry from a depth image, allowing it to learn a continuous mapping from an initial Anygrasp pose to a stable Fin-ray gripper pose. We validate our approach on a 7-DOF robot, demonstrating that our CFM-generated poses achieve a higher overall success rate for seen and unseen objects (34% and 46% respectively) compared to the baseline rigid poses (6% and 25% respectively) when executed by the soft gripper. The model shows significant improvements, particularly for cylindrical (50% and 100% success for seen and unseen objects) and spherical objects (25% and 31% success for seen and unseen objects), and successfully generalizes to unseen objects. This work presents CFM as a data-efficient and effective method for transferring grasp strategies, offering a scalable methodology for other soft robotic systems.", "AI": {"tldr": "本研究提出了一种使用条件流匹配（CFM）将刚性夹爪的抓取姿势映射到软性 Fin-ray 夹爪的新框架，通过数据收集和 U-Net 自动编码器，显著提高了在机器人上的抓取成功率，尤其是在处理圆柱形和球形物体时，并证明了其对未见物体的泛化能力。", "motivation": "现有抓取合成方法主要针对刚性夹爪，难以捕捉软性夹爪的顺应性行为，导致模型数据密集且不准确。研究旨在弥合刚性与软性夹爪抓取合成之间的差距。", "method": "使用条件流匹配（CFM）作为生成模型，学习将刚性夹爪的抓取姿势映射到软性 Fin-ray 夹爪。包含一个数据收集流水线生成成对的刚性-软性抓取姿势。U-Net 自动编码器根据深度图像中的物体几何形状来条件化 CFM 模型，以学习从初始 Anygrasp 姿势到稳定 Fin-ray 夹爪姿势的连续映射。", "result": "在 7-DOF 机器人上进行了验证，CFM 生成的抓取姿势在有见过和未见物体上的整体成功率分别为 34% 和 46%，显著高于基线刚性姿势（分别为 6% 和 25%）。对于圆柱形物体，成功率分别提高到 50% 和 100%；对于球形物体，成功率提高到 25% 和 31%。模型能够成功泛化到未见物体。", "conclusion": "CFM 是一种数据高效且有效的方法，可用于转移抓取策略，为其他软体机器人系统提供了可扩展的方法。本研究成功解决了刚性到软性夹爪抓取姿势迁移的挑战，并取得了显著的性能提升。"}}
{"id": "2602.17022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17022", "abs": "https://arxiv.org/abs/2602.17022", "authors": ["Takyoung Kim", "Jinseok Nam", "Chandrayee Basu", "Xing Fan", "Chengyuan Ma", "Heng Ji", "Gokhan Tur", "Dilek Hakkani-Tür"], "title": "ReIn: Conversational Error Recovery with Reasoning Inception", "comment": "ICLR 2026", "summary": "Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.", "AI": {"tldr": "本研究提出了一种名为ReIn（Reasoning Inception）的测试时干预方法，用于提高集成工具的大型语言模型（LLM）驱动的对话代理从用户错误中恢复的能力，而无需修改模型参数或系统提示。", "motivation": "现有的对话代理在处理固定任务时表现良好，但在面对用户意外错误时容易失败。由于微调模型或修改提示成本高昂，本研究旨在探索在不改变模型参数和提示的情况下，让代理能够从有错误的对话情境中恢复，并适应其行为。", "method": "提出了一种名为ReIn（Reasoning Inception）的测试时干预方法。该方法包含一个外部的“inception”模块，该模块识别对话中的预定义错误，并生成恢复计划。这些恢复计划随后被整合到代理的内部推理过程中，以指导纠正性动作，而无需修改代理的模型参数或系统提示。", "result": "在模拟用户模糊和不支持的请求等对话失败场景的评估中，ReIn显著提高了任务成功率，并且能够泛化到未见的错误类型。与显式修改提示的方法相比，ReIn表现更优。", "conclusion": "ReIn是一种高效、即时的方法，可以提高对话代理的鲁棒性，而无需修改核心模型或系统提示。通过将恢复工具与ReIn结合定义，可以安全有效地提高对话代理的韧性。"}}
{"id": "2602.17128", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17128", "abs": "https://arxiv.org/abs/2602.17128", "authors": ["Huishi Huang", "Jack Klusmann", "Haozhe Wang", "Shuchen Ji", "Fengkang Ying", "Yiyuan Zhang", "John Nassour", "Gordon Cheng", "Daniela Rus", "Jun Liu", "Marcelo H Ang", "Cecilia Laschi"], "title": "Physical Human-Robot Interaction for Grasping in Augmented Reality via Rigid-Soft Robot Synergy", "comment": "Camera-ready version for RoboSoft 2026. 8 pages, 6 figures", "summary": "Hybrid rigid-soft robots combine the precision of rigid manipulators with the compliance and adaptability of soft arms, offering a promising approach for versatile grasping in unstructured environments. However, coordinating hybrid robots remains challenging, due to difficulties in modeling, perception, and cross-domain kinematics. In this work, we present a novel augmented reality (AR)-based physical human-robot interaction framework that enables direct teleoperation of a hybrid rigid-soft robot for simple reaching and grasping tasks. Using an AR headset, users can interact with a simulated model of the robotic system integrated into a general-purpose physics engine, which is superimposed on the real system, allowing simulated execution prior to real-world deployment. To ensure consistent behavior between the virtual and physical robots, we introduce a real-to-simulation parameter identification pipeline that leverages the inherent geometric properties of the soft robot, enabling accurate modeling of its static and dynamic behavior as well as the control system's response.", "AI": {"tldr": "本文提出了一种基于增强现实（AR）的混合刚软机器人遥操作框架，通过虚拟仿真和现实到仿真（Real-to-Simulation）参数识别，实现了对混合机器人的精确控制，用于在非结构化环境中执行抓取任务。", "motivation": "混合刚软机器人结合了刚性机械臂的精度和软体机械臂的柔顺性，在非结构化环境中具有巨大的应用潜力，但其协同控制面临建模、感知和跨领域运动学等挑战。", "method": "提出了一种AR驱动的物理人机交互框架，允许用户通过AR头显与集成在物理引擎中的机器人仿真模型进行交互。通过一个利用软体机器人几何特性进行参数识别的“现实到仿真”管道，实现了虚拟和物理机器人之间行为的一致性。", "result": "该框架能够实现混合刚软机器人的直接遥操作，用于执行简单的伸 Reach 和抓取 Grasp 任务。通过参数识别，可以准确模拟软体机器人的静态和动态行为以及控制系统的响应。", "conclusion": "AR-驱动的物理人机交互框架结合“现实到仿真”参数识别，为实现混合刚软机器人的精确控制和在复杂环境中的应用提供了有效途径。"}}
{"id": "2602.17504", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17504", "abs": "https://arxiv.org/abs/2602.17504", "authors": ["Mark Spiller", "Lennart Kracke", "Johannes Autenrieb"], "title": "Robust Adaptive Sliding-Mode Control for Damaged Fixed-Wing UAVs", "comment": "8 pages, 2 figures, Submitted to International Conference on Unmanned Aircraft Systems (ICUAS) 2026", "summary": "Many unmanned aerial vehicles (UAVs) can remain aerodynamically flyable after sustaining structural or control surface damage, yet insufficient robustness in conventional autopilots often leads to mission failure. This paper proposes a robust adaptive sliding mode controller (RASMC) for fixed-wing UAVs subject to aerodynamic coefficient perturbations and partial loss of control surface effectiveness. A damage-aware flight dynamics model is developed to systematically analyze the impact of such impairments on the closed-loop behavior. The RASMC is designed to ensure reliable tracking and stabilization, while a gain adaptation law maintains low control effort under nominal conditions and increases the gains as needed in the presence of aerodynamic damage. Lyapunov-based stability guarantees are derived, and assumptions on admissible uncertainty bounds are formulated to characterize the limits within which closed-loop stability and performance can be ensured. The proposed controller is implemented within an existing UAV autopilot framework, where outer-loop guidance and speed control modules provide reference commands to the RASMC for attitude stabilization. Simulations demonstrate that, despite significant aerodynamic damage and control surface degradation, all closed-loop states remain stable with bounded tracking errors.", "AI": {"tldr": "提出了一种鲁棒自适应滑模控制器（RASMC），用于应对固定翼无人机在遭受气动系数扰动和部分操纵面失效后的飞行控制问题，并通过仿真验证了其在损伤情况下的稳定性和鲁棒性。", "motivation": "现有无人机自动驾驶仪在结构或操纵面受损后鲁棒性不足，常导致任务失败。", "method": "开发了损伤感知飞行动力学模型，设计了RASMC控制器，并提出了一个增益自适应律。利用Lyapunov稳定性理论推导了稳定性保证，并分析了允许的不确定性界限。", "result": "在模拟中，即使在严重的气动损伤和操纵面退化情况下，RASMC也能保持所有闭环状态稳定，并具有有限的跟踪误差。", "conclusion": "RASMC能够确保固定翼无人机在气动损伤和操纵面失效情况下的可靠跟踪和稳定，同时通过自适应律在正常和受损情况下优化控制增益。"}}
{"id": "2602.17048", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17048", "abs": "https://arxiv.org/abs/2602.17048", "authors": ["Joongwon Chae", "Lihui Luo", "Yang Liu", "Runming Wang", "Dongmei Yu", "Zeming Liang", "Xi Yuan", "Dayan Zhang", "Zhenglin Chen", "Peiwu Qin", "Ilmoon Chae"], "title": "StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection", "comment": null, "summary": "Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.\n  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.\n  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.", "AI": {"tldr": "提出了一种名为StructCore的训练无关、结构感知图像级评分方法，用于无监督异常检测（UAD），超越了传统的Max pooling方法，在MVTec AD和VisA数据集上取得了显著的性能提升。", "motivation": "Max pooling方法在UAD中丢弃了大量关于异常证据分布和结构的信息，导致正常和异常分数重叠，影响了检测性能。", "method": "StructCore通过计算一个低维度的结构描述符phi(S)来捕捉异常得分图的分布和空间特征，并利用从训练好的样本估计的对角线马氏距离进行校准，从而改进图像级评分，且不修改像素级定位。", "result": "StructCore在MVTec AD数据集上实现了99.6%的图像级AUROC评分，在VisA数据集上实现了98.4%的图像级AUROC评分，证明了其在利用Max pooling忽略的结构特征方面的鲁棒性。", "conclusion": "StructCore是一种有效的图像级异常检测方法，它通过利用异常得分图的结构信息来克服Max pooling的局限性，并在两个常用数据集上取得了领先的性能。"}}
{"id": "2602.16891", "categories": ["cs.AI", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16891", "abs": "https://arxiv.org/abs/2602.16891", "authors": ["Hongwei Li", "Zhun Wang", "Qinrun Dai", "Yuzhou Nie", "Jinjun Peng", "Ruitong Liu", "Jingyang Zhang", "Kaijie Zhu", "Jingxuan He", "Lun Wang", "Yangruibo Ding", "Yueqi Chen", "Wenbo Guo", "Dawn Song"], "title": "OpenSage: Self-programming Agent Generation Engine", "comment": null, "summary": "Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.", "AI": {"tldr": "OpenSage 是一个创新的 Agent 开发工具包 (ADK)，它使大型语言模型 (LLM) 能够自动生成代理的拓扑结构、工具集和内存系统，从而克服了现有 ADK 的局限性，提高了代理的通用性和性能。", "motivation": "现有 ADK 在提供对代理拓扑、工具和内存的自动生成方面存在不足，需要人工干预，限制了代理的通用性和整体性能。因此，需要一个能够让 LLM 自主设计和管理这些组件的 ADK。", "method": "OpenSage 提出了一种使 LLM 能够自动创建代理的方法，该代理具有自生成的拓扑结构和工具集，并提供分层、基于图的内存系统。它还包括一个专门为软件工程任务设计的工具包。", "result": "在三个最先进的基准测试以及使用不同骨干模型的实验中，OpenSage 展现出优于现有 ADK 的性能。消融研究也证实了其各个组件的有效性。", "conclusion": "OpenSage 是第一个支持 LLM 自动生成代理拓扑、工具集和内存的 ADK，它显著提高了代理的性能和通用性，并有望推动从以人为中心到以 AI 为中心的代理开发范式的转变。"}}
{"id": "2602.17512", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17512", "abs": "https://arxiv.org/abs/2602.17512", "authors": ["Leila Gharavi", "Simone Baldi", "Yuki Hosomi", "Tona Sato", "Bart De Schutter", "Binh-Minh Nguyen", "Hiroshi Fujimoto"], "title": "Dodging the Moose: Experimental Insights in Real-Life Automated Collision Avoidance", "comment": "10 pages, 10 figures", "summary": "The sudden appearance of a static obstacle on the road, i.e. the moose test, is a well-known emergency scenario in collision avoidance for automated driving. Model Predictive Control (MPC) has long been employed for planning and control of automated vehicles in the state of the art. However, real-time implementation of automated collision avoidance in emergency scenarios such as the moose test remains unaddressed due to the high computational demand of MPC for evasive action in such hazardous scenarios. This paper offers new insights into real-time collision avoidance via the experimental imple- mentation of MPC for motion planning after a sudden and unexpected appearance of a static obstacle. As the state-of-the-art nonlinear MPC shows limited capability to provide an acceptable solution in real-time, we propose a human-like feed-forward planner to assist when the MPC optimization problem is either infeasible or unable to find a suitable solution due to the poor quality of its initial guess. We introduce the concept of maximum steering maneuver to design the feed-forward planner and mimic a human-like reaction after detecting the static obstacle on the road. Real-life experiments are conducted across various speeds and level of emergency using FPEV2-Kanon electric vehicle. Moreover, we demonstrate the effectiveness of our planning strategy via comparison with the state-of- the-art MPC motion planner.", "AI": {"tldr": "本文提出了一种结合人类驾驶员行为的反馈前规划器来辅助模型预测控制（MPC），以解决自动驾驶车辆在紧急避障场景（如麋鹿测试）中实时计算需求高的问题。通过真实车辆实验验证了该策略的有效性。", "motivation": "现有的MPC在紧急避障场景中因计算量大，实时实现存在挑战，无法提供可接受的解决方案。", "method": "提出了一种人类驾驶员启发的反馈前规划器，当MPC优化问题不可行或初始猜测质量差时，该规划器能够介入。该规划器利用最大转向机动概念来模仿人类在检测到障碍物后的反应。实验在FPEV2-Kanon电动汽车上进行，涵盖不同速度和紧急程度。", "result": "提出的策略在真实实验中显示出有效性，并且与最先进的MPC运动规划器进行了比较。", "conclusion": "结合人类驾驶员行为的反馈前规划器可以有效辅助MPC，解决自动驾驶车辆在紧急避障场景中的实时性问题，提供比纯MPC更好的解决方案。"}}
{"id": "2602.17060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17060", "abs": "https://arxiv.org/abs/2602.17060", "authors": ["Shunsuke Kikuchi", "Atsushi Kouno", "Hiroki Matsuzaki"], "title": "Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding", "comment": null, "summary": "Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.", "AI": {"tldr": "研究者发布了一个高质量的腹腔镜手术中 trocar ports（套管针孔）的分割数据集 Cholec80-port，并制定了一个严格的标准操作规程（SOP），要求分割时排除套管针的中心开口。该数据集和 SOP 能够显著提升下游几何视觉任务的鲁棒性。", "motivation": "Trocar ports 在腹腔镜手术中会阻挡视线并吸引不应有的特征点，严重影响图像拼接、3D重建和视觉 SLAM 等几何视觉任务的准确性和稳定性。然而，现有的公共数据集缺乏明确的 trocar port 标注，且现有标注方式（如遮蔽中心开口）存在几何不一致性问题。", "method": "1. 从 Cholec80 数据集中提取并创建了高质量的 trocar port 分割数据集 Cholec80-port。\n2. 制定了严格的标准操作规程（SOP），明确要求 trocar port 的分割掩码应排除其中心开口。\n3. 按照该 SOP 清理和统一了现有的公共数据集。\n4. 进行了实验验证。", "result": "使用 Cholec80-port 数据集和遵循 SOP 的标注方式，相比于仅增加数据集大小，可以更显著地提高跨数据集的鲁棒性。", "conclusion": "几何上一致的 trocar port 标注对于提升下游几何视觉任务的性能至关重要。提出的 Cholec80-port 数据集和 SOP 为解决此问题提供了一种有效方案。"}}
{"id": "2602.17045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17045", "abs": "https://arxiv.org/abs/2602.17045", "authors": ["Jared Moore", "Rasmus Overmark", "Ned Cooper", "Beba Cibralic", "Nick Haber", "Cameron R. Jones"], "title": "Large Language Models Persuade Without Planning Theory of Mind", "comment": null, "summary": "A growing body of work attempts to evaluate the theory of mind (ToM) abilities of humans and large language models (LLMs) using static, non-interactive question-and-answer benchmarks. However, theoretical work in the field suggests that first-personal interaction is a crucial part of ToM and that such predictive, spectatorial tasks may fail to evaluate it. We address this gap with a novel ToM task that requires an agent to persuade a target to choose one of three policy proposals by strategically revealing information. Success depends on a persuader's sensitivity to a given target's knowledge states (what the target knows about the policies) and motivational states (how much the target values different outcomes). We varied whether these states were Revealed to persuaders or Hidden, in which case persuaders had to inquire about or infer them. In Experiment 1, participants persuaded a bot programmed to make only rational inferences. LLMs excelled in the Revealed condition but performed below chance in the Hidden condition, suggesting difficulty with the multi-step planning required to elicit and use mental state information. Humans performed moderately well in both conditions, indicating an ability to engage such planning. In Experiment 2, where a human target role-played the bot, and in Experiment 3, where we measured whether human targets' real beliefs changed, LLMs outperformed human persuaders across all conditions. These results suggest that effective persuasion can occur without explicit ToM reasoning (e.g., through rhetorical strategies) and that LLMs excel at this form of persuasion. Overall, our results caution against attributing human-like ToM to LLMs while highlighting LLMs' potential to influence people's beliefs and behavior.", "AI": {"tldr": "本研究开发了一种新的、交互式的图灵测试来评估语言模型（LLM）的心理理论（ToM）能力。结果表明，LLM在需要推断目标心理状态的交互式任务中表现不佳，但在不需要显式ToM推理的劝说任务中表现出色，这表明它们可能使用其他策略来影响人类。", "motivation": "现有评估语言模型ToM能力的方法多为静态问答，未能体现ToM的第一人称交互特性。理论认为，交互式体验对ToM至关重要，因此需要新的评估方法。", "method": "研究设计了一种交互式劝说任务，要求代理人通过策略性地透露信息来劝说目标选择政策提案。研究变量包括目标心理状态（知识状态和动机状态）是否对劝说者可见。共进行三项实验：1）参与者与一个仅能进行理性推理的机器人互动；2）人类目标扮演机器人；3）测量人类目标真实信念是否发生改变。", "result": "在机器人目标且心理状态可见的条件下，LLM表现优异。但在心理状态隐藏的条件下，LLM表现低于随机水平，表明其在推断和利用心理状态信息方面存在困难。人类参与者在两种条件下表现均中等。在后两项实验中，LLM在劝说人类目标时表现优于人类劝说者，尤其是在不需要显式ToM推理的条件下。", "conclusion": "研究结果表明，LLM可能不具备人类水平的ToM能力，尤其是在需要复杂心理状态推断的情况下。然而，LLM擅长采用其他策略（如修辞策略）进行劝说，这可能导致其在影响人类信念和行为方面表现出色。因此，应谨慎将人类ToM能力归因于LLM。"}}
{"id": "2602.16901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16901", "abs": "https://arxiv.org/abs/2602.16901", "authors": ["Tanqiu Jiang", "Yuhui Wang", "Jiacheng Liang", "Ting Wang"], "title": "AgentLAB: Benchmarking LLM Agents against Long-Horizon Attacks", "comment": null, "summary": "LLM agents are increasingly deployed in long-horizon, complex environments to solve challenging problems, but this expansion exposes them to long-horizon attacks that exploit multi-turn user-agent-environment interactions to achieve objectives infeasible in single-turn settings. To measure agent vulnerabilities to such risks, we present AgentLAB, the first benchmark dedicated to evaluating LLM agent susceptibility to adaptive, long-horizon attacks. Currently, AgentLAB supports five novel attack types including intent hijacking, tool chaining, task injection, objective drifting, and memory poisoning, spanning 28 realistic agentic environments, and 644 security test cases. Leveraging AgentLAB, we evaluate representative LLM agents and find that they remain highly susceptible to long-horizon attacks; moreover, defenses designed for single-turn interactions fail to reliably mitigate long-horizon threats. We anticipate that AgentLAB will serve as a valuable benchmark for tracking progress on securing LLM agents in practical settings. The benchmark is publicly available at https://tanqiujiang.github.io/AgentLAB_main.", "AI": {"tldr": "本文提出了AgentLAB，首个用于评估大型语言模型（LLM）代理对适应性、长时程攻击的基准测试。实验表明，现有LLM代理在这种攻击下仍然非常脆弱，且针对单轮交互设计的防御措施无法有效缓解长时程威胁。", "motivation": "随着LLM代理被部署到更复杂、长时程的环境中解决问题，它们也面临着可能利用多轮交互实现单轮设置中不可行目标的“长时程攻击”。因此，需要一种方法来衡量代理对这类风险的脆弱性。", "method": "构建了AgentLAB基准测试，支持五种新型攻击（意图劫持、工具链、任务注入、目标漂移和记忆中毒），涵盖28种真实场景和644个安全测试用例，用于评估LLM代理的安全性。", "result": "在AgentLAB上评估代表性的LLM代理后发现，它们对长时程攻击非常敏感。此外，为单轮交互设计的防御措施在缓解长时程威胁方面效果不佳。", "conclusion": "AgentLAB提供了一个评估LLM代理抵御长时程攻击的有效工具，并揭示了当前LLM代理的安全漏洞。该基准测试有望成为跟踪LLM代理安全进展的重要参考。"}}
{"id": "2602.17051", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17051", "abs": "https://arxiv.org/abs/2602.17051", "authors": ["Deepak Uniyal", "Md Abul Bashar", "Richi Nayak"], "title": "Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data", "comment": null, "summary": "Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.", "AI": {"tldr": "本研究评估了四种跨语言文本分类方法，用于从包含英语、日语、印地语和韩语的九百多万条推文中提取关于氢能源的讨论，并进行主题发现。", "motivation": "跨语言社交媒体话语分析，尤其是在涉及多种语言的全球性公开辩论时，是自然语言处理中的一项重大挑战。", "method": "研究人员使用了四种跨语言文本分类方法来过滤与氢能源相关的推文：1) 将英语标注数据翻译成目标语言；2) 将所有语言的未标注数据翻译成英语；3) 直接在目标语言数据上应用英语微调的多语言Transformer；4) 结合翻译标注和多语言训练的混合策略。然后，对筛选出的相关推文进行主题建模。", "result": "研究结果突出了翻译方法和多语言方法之间的关键权衡，并为优化大规模社交媒体分析的跨语言流程提供了可操作的见解。", "conclusion": "不同的跨语言文本分类方法在过滤噪音和实现可靠的全球对话分析方面各有优劣，为未来的研究提供了指导。"}}
{"id": "2602.17166", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17166", "abs": "https://arxiv.org/abs/2602.17166", "authors": ["Antonio Franchi", "Chiara Gabellieri"], "title": "Geometric Inverse Flight Dynamics on SO(3) and Application to Tethered Fixed-Wing Aircraft", "comment": null, "summary": "We present a robotics-oriented, coordinate-free formulation of inverse flight dynamics for fixed-wing aircraft on SO(3). Translational force balance is written in the world frame and rotational dynamics in the body frame; aerodynamic directions (drag, lift, side) are defined geometrically, avoiding local attitude coordinates. Enforcing coordinated flight (no sideslip), we derive a closed-form trajectory-to-input map yielding the attitude, angular velocity, and thrust-angle-of-attack pair, and we recover the aerodynamic moment coefficients component-wise. Applying such a map to tethered flight on spherical parallels, we obtain analytic expressions for the required bank angle and identify a specific zero-bank locus where the tether tension exactly balances centrifugal effects, highlighting the decoupling between aerodynamic coordination and the apparent gravity vector. Under a simple lift/drag law, the minimal-thrust angle of attack admits a closed form. These pointwise quasi-steady inversion solutions become steady-flight trim when the trajectory and rotational dynamics are time-invariant. The framework bridges inverse simulation in aeronautics with geometric modeling in robotics, providing a rigorous building block for trajectory design and feasibility checks.", "AI": {"tldr": "本文提出了一种面向机器人学的、无坐标系的固定翼飞机逆向飞行动力学在SO(3)上的数学表达，可以从轨迹直接推导出飞机的姿态、角速度和推力-迎角组合，并可计算气动偶极子系数。在协调飞行假设下，推导出了闭式解，并用于系留飞行分析，得到了银行角解析表达式，并发现了零银行角条件。该框架结合了航空学中的逆向仿真和机器人学中的几何建模，为轨迹设计和可行性检查提供了基础。", "motivation": "现有机器人学方法在处理固定翼飞机动力学时，常依赖于局部坐标系，导致处理复杂姿态时出现问题。因此，需要一种更鲁棒、更通用的方法来处理飞机在 SO(3) 上的运动，并将其与轨迹设计相结合。", "method": "本文采用坐标系无关的数学表达，将平动方程置于世界坐标系，转动方程置于机体坐标系。通过几何方式定义气动方向，避免了局部姿态坐标。在强制协调飞行（无侧滑）的假设下，推导出从轨迹到输入（姿态、角速度、推力-迎角）的闭式映射。同时，推导了气动偶极子系数的求解方法。将该方法应用于系留飞行，得到了银行角和推力-迎角的解析表达式。", "result": "推导出了从轨迹到姿态、角速度和推力-迎角的闭式映射。在协调飞行假设下，该映射能够直接求解出所需的控制输入。发现了零银行角条件，在该条件下，系绳张力可以精确地抵消离心效应，表明气动协调与视在重力向量是解耦的。在简单的升阻比模型下，得到了最小推力迎角的闭式解。", "conclusion": "本文提出的机器人学导向的、无坐标系的逆向飞行动力学方法，提供了一种严谨的框架，能够从轨迹直接求解飞机的控制输入，并能分析复杂飞行场景。该框架融合了航空学和机器人学的理论，为无人机轨迹设计、可行性分析以及更高级的自主控制算法奠定了基础。"}}
{"id": "2602.17199", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17199", "abs": "https://arxiv.org/abs/2602.17199", "authors": ["Antonio Rapuano", "Yaolei Shen", "Federico Califano", "Chiara Gabellieri", "Antonio Franchi"], "title": "Nonlinear Predictive Control of the Continuum and Hybrid Dynamics of a Suspended Deformable Cable for Aerial Pick and Place", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This paper presents a framework for aerial manipulation of an extensible cable that combines a high-fidelity model based on partial differential equations (PDEs) with a reduced-order representation suitable for real-time control. The PDEs are discretised using a finite-difference method, and proper orthogonal decomposition is employed to extract a reduced-order model (ROM) that retains the dominant deformation modes while significantly reducing computational complexity. Based on this ROM, a nonlinear model predictive control scheme is formulated, capable of stabilizing cable oscillations and handling hybrid transitions such as payload attachment and detachment. Simulation results confirm the stability, efficiency, and robustness of the ROM, as well as the effectiveness of the controller in regulating cable dynamics under a range of operating conditions. Additional simulations illustrate the application of the ROM for trajectory planning in constrained environments, demonstrating the versatility of the proposed approach. Overall, the framework enables real-time, dynamics-aware control of unmanned aerial vehicles (UAVs) carrying suspended flexible cables.", "AI": {"tldr": "提出了一种结合高精度PDE模型和降阶模型（ROM）的空中可伸缩缆绳操纵框架，并设计了基于ROM的非线性模型预测控制器，实现了缆绳动力学的实时、动态感知控制。", "motivation": "为了实现无人机（UAV）对悬挂柔性缆绳进行实时、动态感知的控制，需要一种既能准确描述缆绳动力学又能满足实时性要求的计算方法。", "method": "采用基于偏微分方程（PDEs）的高精度模型，通过有限差分法离散化，并利用本征正交分解（POD）提取降阶模型（ROM）。基于ROM设计了非线性模型预测控制器（NMPC），以实现对缆绳振荡的稳定和载荷附着/分离等混合过渡的处理。", "result": "仿真结果表明，ROM模型具有稳定性、效率和鲁棒性，控制器能够有效调节缆绳动力学。该框架还可以用于约束环境下的轨迹规划。", "conclusion": "所提出的框架能够实现对无人机携带的悬挂柔性缆绳进行实时、动态感知的控制，并具有轨迹规划能力，展示了方法的通用性。"}}
{"id": "2602.17077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17077", "abs": "https://arxiv.org/abs/2602.17077", "authors": ["Lee Dayeon", "Kim Dongheyong", "Park Chaewon", "Woo Sungmin", "Lee Sangyoun"], "title": "Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection", "comment": "ICASSP 2026", "summary": "Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.", "AI": {"tldr": "本文提出了一种名为 CPL-VAD 的弱监督视频异常检测框架，通过双分支和交叉伪标签机制，结合了片段级异常定位和事件类别识别，并在公开数据集上取得了最先进的性能。", "motivation": "现有的弱监督视频异常检测方法仅能提供视频级别的标签，难以同时实现精确的异常定位和对异常事件类别的识别。", "method": "提出 CPL-VAD，一个包含两个分支的双分支框架：1. 二进制异常检测分支，用于片段级异常定位；2. 类别分类分支，利用视觉-语言对齐来识别异常事件类别。两个分支通过交叉伪标签机制交换信息，实现优势互补。", "result": "CPL-VAD 在 XD-Violence 和 UCF-Crime 数据集上，在异常检测和异常类别分类两项任务上均达到了最先进的性能。", "conclusion": "CPL-VAD 框架通过交叉伪标签的双分支协同，有效地融合了时间精度和语义判别能力，显著提升了弱监督视频异常检测和异常类别识别的性能。"}}
{"id": "2602.17621", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17621", "abs": "https://arxiv.org/abs/2602.17621", "authors": ["Peter Seiler", "Mark E. Pittelkau", "Felix Biertümpfel"], "title": "Method to Compute Pointing Displacement, Smear, and Jitter Covariances for Optical Payloads", "comment": "Final accepted manuscript (AAM) for AIAA Journal of Guidance, Control and Dynamics", "summary": "This paper presents a method to assess the pointing and image motion performance of optical payloads in the presence of image displacement (shift), smear, and jitter. The method assumes the motion is a stationary random process over an image exposure interval. Displacement, smear, and jitter covariances are computed from the solution to a Lyapunov differential equation. These covariances parameterize statistical image motion modulation transfer functions (MTFs), and they can be used to verify pointing and image motion MTF requirements. The method in the present paper extends a previous method to include smear, as well as displacement, and hence jitter. The approach in the present paper also leads, as a special case, to a more efficient method to compute the displacement covariance than the previous method. Numerical examples illustrate the proposed method.", "AI": {"tldr": "提出了一种评估光学载荷在图像位移、拖影和抖动存在下的指向和图像运动性能的方法，该方法将运动视为图像曝光间隔内的平稳随机过程，并通过求解 Lyapunov 微分方程计算协方差，进而参数化统计图像运动调制传递函数（MTFs）。", "motivation": "为了在存在图像位移、拖影和抖动的情况下，评估光学载荷的指向和图像运动性能。", "method": "1. 假设运动是图像曝光间隔内的平稳随机过程。2. 通过求解 Lyapunov 微分方程计算位移、拖影和抖动的协方差。3. 使用这些协方差参数化统计图像运动调制传递函数（MTFs）。4. 验证指向和图像运动 MTF 要求。", "result": "所提出的方法能够评估光学载荷在图像位移、拖影和抖动存在下的指向和图像运动性能。该方法能计算位移协方差，比先前方法更有效。新方法将拖影和位移也纳入了考量。", "conclusion": "该方法扩展了先前评估方法，能够同时处理图像位移、拖影和抖动，并提供了一种计算位移协方差的更有效方法，可用于验证光学载荷的指向和图像运动 MTF 要求。"}}
{"id": "2602.16902", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16902", "abs": "https://arxiv.org/abs/2602.16902", "authors": ["Juliusz Ziomek", "William Bankes", "Lorenz Wolf", "Shyam Sundhar Ramesh", "Xiaohang Tang", "Ilija Bogunovic"], "title": "LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs", "comment": null, "summary": "We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.", "AI": {"tldr": "LLM-Wikirace 是一个评估 LLM 规划、推理和世界知识的新基准。虽然 GPT-5 和 Gemini-3 等模型在简单任务上表现出色，但在困难任务上仍面临巨大挑战，尤其是在规划和长远推理方面，并且在失败后难以重新规划。", "motivation": "评估大型语言模型（LLMs）在规划、推理和世界知识方面的能力，并识别它们在这些方面的局限性。", "method": "提出 LLM-Wikirace 基准，要求模型通过导航维基百科链接从源页面到达目标页面。评估了包括 Gemini-3、GPT-5 和 Claude Opus 4.5 在内的多个开源和闭源模型，并分析了它们的性能、对世界知识的需求以及在失败后的规划能力。", "result": "在简单任务上，Gemini-3、GPT-5 和 Claude Opus 4.5 等模型取得了超人的表现。然而，在困难任务上，性能急剧下降，最佳模型 Gemini-3 的成功率仅为 23%。研究表明，世界知识是成功的必要条件，但规划和长远推理能力在达到一定阈值后变得更为关键。模型在失败后难以重新规划，容易陷入循环。", "conclusion": "LLM-Wikirace 基准揭示了当前 LLMs 在规划和长远推理方面的显著局限性。虽然世界知识很重要，但更高级的规划能力是克服困难挑战的关键。未来的研究需要集中于提高 LLMs 的规划和重新规划能力。"}}
{"id": "2602.17085", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2602.17085", "abs": "https://arxiv.org/abs/2602.17085", "authors": ["Shogo Sato", "Kazuo Tanaka", "Shojun Ogasawara", "Kazuki Yamamoto", "Kazuhiko Murasaki", "Ryuichi Tanida", "Jun Kataoka"], "title": "ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions", "comment": "Accepted by ApJ", "summary": "Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.", "AI": {"tldr": "本文提出了一种名为ComptonUNet的混合深度学习框架，用于提高伽马射线暴（GRB）的探测和定位精度，尤其是在低光子统计和高背景噪声的情况下。", "motivation": "现有方法在处理低光子统计和强背景噪声的弱伽马射线暴时，难以平衡统计鲁棒性和噪声抑制能力，限制了对早期宇宙星系形成的研究。", "method": "ComptonUNet是一种混合深度学习框架，它结合了直接重建模型的统计效率和基于图像模型的去噪能力，共同处理原始数据并重建图像，以实现鲁棒的GRB定位。", "result": "在模拟的低光子统计和高背景噪声环境下，ComptonUNet显著优于现有方法，在多种场景下实现了更高的定位精度。", "conclusion": "ComptonUNet是一种有效的GRB定位方法，尤其适用于探测来自遥远宇宙的微弱GRB，有望促进对早期宇宙的研究。"}}
{"id": "2602.17226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17226", "abs": "https://arxiv.org/abs/2602.17226", "authors": ["Lorenzo Montano-Olivan", "Julio A. Placed", "Luis Montano", "Maria T. Lazaro"], "title": "Multi-session Localization and Mapping Exploiting Topological Information", "comment": null, "summary": "Operating in previously visited environments is becoming increasingly crucial for autonomous systems, with direct applications in autonomous driving, surveying, and warehouse or household robotics. This repeated exposure to observing the same areas poses significant challenges for mapping and localization -- key components for enabling any higher-level task. In this work, we propose a novel multi-session framework that builds on map-based localization, in contrast to the common practice of greedily running full SLAM sessions and trying to find correspondences between the resulting maps. Our approach incorporates a topology-informed, uncertainty-aware decision-making mechanism that analyzes the pose-graph structure to detect low-connectivity regions, selectively triggering mapping and loop closing modules. The resulting map and pose-graph are seamlessly integrated into the existing model, reducing accumulated error and enhancing global consistency. We validate our method on overlapping sequences from datasets and demonstrate its effectiveness in a real-world mine-like environment.", "AI": {"tldr": "提出了一种新的多会话框架，用于在先前访问过的环境中进行建图和定位，通过拓扑感知和不确定性感知决策，选择性地触发建图和回环检测模块，以提高精度和一致性。", "motivation": "在先前访问过的环境中操作对于自主系统（如自动驾驶、测绘和机器人）至关重要，但这种重复的观察给建图和定位带来了挑战。", "method": "提出了一种基于地图的定位框架，而不是运行完整的 SLAM 会话并尝试匹配地图。该方法使用拓扑感知和不确定性感知决策机制来分析姿态图结构，检测低连通性区域，并选择性地触发建图和回环检测模块，然后将结果无缝集成到现有模型中。", "result": "该方法能够减少累积误差并提高全局一致性。在重叠数据集序列和真实的矿井环境中进行了验证，证明了其有效性。", "conclusion": "所提出的多会话框架通过智能地管理建图和回环检测过程，有效地解决了在先前访问过的环境中进行建图和定位的挑战，提高了精度和鲁棒性。"}}
{"id": "2602.16931", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16931", "abs": "https://arxiv.org/abs/2602.16931", "authors": ["Idhant Gulati", "Shivam Raval"], "title": "Narrow fine-tuning erodes safety alignment in vision-language agents", "comment": "24 pages, 11 figures", "summary": "Lifelong multimodal agents must continuously adapt to new tasks through post-training, but this creates fundamental tension between acquiring capabilities and preserving safety alignment. We demonstrate that fine-tuning aligned vision-language models on narrow-domain harmful datasets induces severe emergent misalignment that generalizes broadly across unrelated tasks and modalities. Through experiments on Gemma3-4B, we show that misalignment scales monotonically with LoRA rank, and that multimodal evaluation reveals substantially higher misalignment ($70.71 \\pm 1.22$ at $r=128$) than text-only evaluation ($41.19 \\pm 2.51$), suggesting that unimodal safety benchmarks may underestimate alignment degradation in vision-language models. Critically, even 10\\% harmful data in the training mixture induces substantial alignment degradation. Geometric analysis reveals that harmful behaviors occupy a remarkably low-dimensional subspace, with the majority of misalignment information captured in 10 principal components. To mitigate misalignment, we evaluate two strategies: benign narrow fine-tuning and activation-based steering. While both approaches substantially reduce misalignment, neither completely removes the learned harmful behaviors. Our findings highlight the need for robust continual learning frameworks, as current post-training paradigms may not sufficiently preserve alignment in post-deployment settings.", "AI": {"tldr": "微调对齐的视觉语言模型，即使使用少量有害数据，也会导致严重的、泛化的安全对齐下降，尤其是在多模态评估中。有害行为占据低维子空间，且现有缓解策略（如良性微调和激活引导）无法完全消除。", "motivation": "在持续学习过程中，如何平衡新任务的学习和现有安全对齐的保持，是困扰终身多模态智能体的根本问题。", "method": "通过在窄域有害数据集上微调对齐的视觉语言模型（Gemma3-4B），并进行多模态和纯文本的评估，分析LoRA秩、有害数据比例对模型对齐的影响。同时，使用几何分析（主成分分析）来理解有害行为的低维特性，并评估了两种缓解策略：良性窄域微调和基于激活的引导。", "result": "即使少量（10%）有害数据也会导致显著的对齐下降。有害行为在低维子空间中，10个主成分即可捕获大部分失对齐信息。多模态评估显示比纯文本评估更高的失对齐率（LoRA秩为128时，多模态70.71%，纯文本41.19%）。两种缓解策略均有效，但未能完全消除有害行为。", "conclusion": "当前的后训练范式可能无法充分保持多模态模型在部署后的安全对齐。需要开发更鲁棒的持续学习框架来解决新任务学习与安全对齐之间的冲突。"}}
{"id": "2602.17259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17259", "abs": "https://arxiv.org/abs/2602.17259", "authors": ["Han Zhao", "Jingbo Wang", "Wenxuan Song", "Shuai Chen", "Yang Liu", "Yan Wang", "Haoang Li", "Donglin Wang"], "title": "FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment", "comment": "Project Website: https://h-zhao1997.github.io/frappe", "summary": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.", "AI": {"tldr": "FRAPPE 是一种新的微调策略，用于提高 VLA 模型进行世界建模的能力，以增强机器人推理和泛化能力。它通过预测未来潜在表征和多模型对齐来解决现有方法的局限性。", "motivation": "现有 VLA 模型在世界建模方面存在两大问题：1. 像素级重建的训练目标限制了语义学习和泛化能力；2. 推理过程中依赖预测的未来观察导致错误累积。这阻碍了机器人推理和泛化能力的提升。", "method": "FRAPPE 采用两阶段微调策略。中期训练阶段，模型学习预测未来观察的潜在表征。后期训练阶段，并行扩展计算工作量，并与多个不同的视觉基础模型同时对齐表征。", "result": "FRAPPE 显著提高了微调效率，减少了对带动作标注数据的依赖。在 RoboTwin 基准和真实世界任务上的实验表明，FRAPPE 优于最先进的方法，并在长时域和未见场景中表现出强大的泛化能力。", "conclusion": "FRAPPE 提供了一种可扩展且数据高效的途径，用于增强通用机器人策略的世界感知能力，克服了现有世界建模方法的局限性。"}}
{"id": "2602.17415", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17415", "abs": "https://arxiv.org/abs/2602.17415", "authors": ["Yi Zhang", "Omar Faris", "Chapa Sirithunge", "Kai-Fung Chu", "Fumiya Iida", "Fulvio Forni"], "title": "Distributed Virtual Model Control for Scalable Human-Robot Collaboration in Shared Workspace", "comment": null, "summary": "We present a decentralized, agent agnostic, and safety-aware control framework for human-robot collaboration based on Virtual Model Control (VMC). In our approach, both humans and robots are embedded in the same virtual-component-shaped workspace, where motion is the result of the interaction with virtual springs and dampers rather than explicit trajectory planning. A decentralized, force-based stall detector identifies deadlocks, which are resolved through negotiation. This reduces the probability of robots getting stuck in the block placement task from up to 61.2% to zero in our experiments. The framework scales without structural changes thanks to the distributed implementation: in experiments we demonstrate safe collaboration with up to two robots and two humans, and in simulation up to four robots, maintaining inter-agent separation at around 20 cm. Results show that the method shapes robot behavior intuitively by adjusting control parameters and achieves deadlock-free operation across team sizes in all tested scenarios.", "AI": {"tldr": "提出了一种基于虚拟模型控制 (VMC) 的去中心化、与代理无关且安全意识强的人机协作控制框架，通过虚拟弹簧和阻尼器实现交互式运动，并利用基于力的堵塞检测器和协商机制解决了死锁问题，显著提高了协作任务的成功率和安全性。", "motivation": "旨在开发一种能够实现安全、高效、灵活的人机协作控制框架，尤其是在机器人可能面临的任务场景中（如积木放置），以克服传统轨迹规划方法的局限性，并提高机器人被卡住的概率。", "method": "该框架将人类和机器人置于相同的虚拟组件形状工作空间中，利用虚拟弹簧和阻尼器模拟运动，而非显式轨迹规划。采用去中心化的、基于力的堵塞检测器来识别死锁，并通过协商机制解决。这种方法是分布式的，不受代理类型限制。", "result": "在积木放置任务中，将机器人被卡住的概率从高达 61.2% 降低到零。该框架能够安全地支持多达两个机器人和两个人类的协作，在模拟中甚至支持多达四个机器人，同时将代理之间的距离保持在约 20 厘米。通过调整控制参数，可以直观地塑造机器人行为。", "conclusion": "所提出的去中心化、安全意识强的人机协作控制框架通过 VMC 和协商机制有效地解决了死锁问题，并在不同规模的团队中实现了无死锁运行，证明了其在人机协作任务中的有效性和可扩展性。"}}
{"id": "2602.17054", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17054", "abs": "https://arxiv.org/abs/2602.17054", "authors": ["Hussein S. Al-Olimat", "Ahmad Alshareef"], "title": "ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning", "comment": null, "summary": "While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.", "AI": {"tldr": "本研究提出了ALPS（Arabic Linguistic & Pragmatic Suite），一个针对阿拉伯语深度语义和语用的、由专家精心策划的诊断数据集，以评估模型的语言理解深度。实验结果表明，现有模型虽然在语言流畅性方面表现良好，但在形态句法依赖性方面存在显著的错误，尤其是在依赖阿拉伯语附加符号的任务上。与大型商业模型相比，本土阿拉伯语模型在准确性上仍有差距。", "motivation": "现有阿拉伯语NLP基准测试侧重于规模，但常使用合成或翻译数据，可能缺乏深度的语言学验证。研究者希望创建一个更具语言学深度、更原生、且经过专家评审的数据集，以诊断模型在深度语义和语用方面的能力。", "method": "构建了ALPS（Arabic Linguistic & Pragmatic Suite），一个包含531个精心设计的问题，横跨15个任务和47个子任务的数据集。该数据集由阿拉伯语语言学专家开发，确保了文化真实性并避免了翻译痕迹。对23个不同模型（商业、开源和本土阿拉伯语模型）进行了评估，并与单次通过的人类表现（平均84.6%准确率）和专家仲裁的参考答案（99.2%）进行了比较。", "result": "模型在语言流畅性方面表现良好，但在形态句法依赖性方面错误率较高（在依赖附加符号的任务上达到36.5%），高于在组合语义方面的错误率。排名前列的商业模型（如Gemini-3-flash，准确率为94.2%）超越了平均人类表现，但与本土阿拉伯语模型的差距依然显著。表现最佳的阿拉伯语本土模型（Jais-2-70B，准确率为83.6%）接近人类水平但仍有差距。", "conclusion": "ALPS数据集揭示了当前模型在理解阿拉伯语深层语言学特征方面的局限性，尤其是在形态句法依赖性方面。虽然大型商业模型在某些方面超越了人类，但本土阿拉伯语模型在接近人类表现方面仍需改进，表明在阿拉伯语NLP领域，尤其是在深度语言理解方面，仍有进一步研究和开发的必要。"}}
{"id": "2602.17072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17072", "abs": "https://arxiv.org/abs/2602.17072", "authors": ["Yunseung Lee", "Subin Kim", "Youngjun Kwak", "Jaegul Choo"], "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios", "comment": null, "summary": "Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.", "AI": {"tldr": "本研究提出了一个名为BankMathBench的金融领域特定数据集，旨在解决大型语言模型（LLMs）在处理核心银行计算任务时准确率低的问题。该数据集包含三个难度级别，并已被证明能有效提升LLMs的数值推理能力，尤其是在结合工具增强微调后，准确率显著提高。", "motivation": "现有的大型语言模型在处理涉及多步数值推理和金融产品上下文理解的核心银行计算任务（如总支出估算、利率比较、提前还款利息计算）时，准确率较低，并且现有基准测试未能充分捕捉这些问题。金融领域需要一个专门的数据集来评估和改进LLMs在这类任务上的表现。", "method": "研究人员创建了一个名为BankMathBench的领域特定数据集，该数据集包含三个难度级别：基础（单产品推理）、中级（多产品比较）和高级（多条件场景）。他们使用该数据集对开源LLMs进行了训练和评估，并探索了结合工具增强微调的方法。", "result": "在BankMathBench数据集上训练后，LLMs在公式生成和数值推理准确率方面均有显著提升。特别是，经过工具增强微调的模型在基础、中级和高级任务上的平均准确率分别提高了57.6个百分点、75.1个百分点和62.9个百分点，远超零样本基线。", "conclusion": "BankMathBench是一个有效的基准数据集，能够评估和提升LLMs在真实银行场景中的数值推理能力。通过使用该数据集进行训练和微调（尤其是工具增强微调），LLMs在核心银行计算任务上的准确性得到了显著改善。"}}
{"id": "2602.17574", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17574", "abs": "https://arxiv.org/abs/2602.17574", "authors": ["Joshua A. Robbins", "Andrew F. Thompson", "Jonah J. Glunt", "Herschel C. Pangborn"], "title": "Hybrid System Planning using a Mixed-Integer ADMM Heuristic and Hybrid Zonotopes", "comment": null, "summary": "Embedded optimization-based planning for hybrid systems is challenging due to the use of mixed-integer programming, which is computationally intensive and often sensitive to the specific numerical formulation. To address that challenge, this article proposes a framework for motion planning of hybrid systems that pairs hybrid zonotopes - an advanced set representation - with a new alternating direction method of multipliers (ADMM) mixed-integer programming heuristic. A general treatment of piecewise affine (PWA) system reachability analysis using hybrid zonotopes is presented and extended to formulate optimal planning problems. Sets produced using the proposed identities have lower memory complexity and tighter convex relaxations than equivalent sets produced from preexisting techniques. The proposed ADMM heuristic makes efficient use of the hybrid zonotope structure. For planning problems formulated as hybrid zonotopes, the proposed heuristic achieves improved convergence rates as compared to state-of-the-art mixed-integer programming heuristics. The proposed methods for hybrid system planning on embedded hardware are experimentally applied in a combined behavior and motion planning scenario for autonomous driving.", "AI": {"tldr": "本文提出了一种结合混合区域图（hybrid zonotopes）和交替方向乘子法（ADMM）混合整数规划启发式算法的混合系统运动规划框架，以解决计算密集型和对数值公式敏感的问题。该框架提高了内存效率和收敛速度，并成功应用于自动驾驶的混合系统规划。", "motivation": "嵌入式系统中基于优化的混合系统规划因混合整数规划计算密集且对数值公式敏感而面临挑战。", "method": "提出了一种结合混合区域图（一种先进的集合表示）和一种新的交替方向乘子法（ADMM）混合整数规划启发式算法的框架。实现了分段仿射（PWA）系统可达性分析的通用处理，并将其扩展到最优规划问题。所提出的ADMM启发式算法有效利用了混合区域图的结构。", "result": "使用提出的方法生成的集合具有更低的内存复杂度和更紧凑的凸松弛。提出的ADMM启发式算法在混合区域图规划问题上实现了比现有混合整数规划启发式算法更高的收敛率。", "conclusion": "该框架为混合系统规划提供了一种更高效、更鲁棒的方法，并且在自动驾驶的嵌入式硬件上得到了成功验证。"}}
{"id": "2602.17124", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17124", "abs": "https://arxiv.org/abs/2602.17124", "authors": ["Chi-Shiang Gau", "Konstantinos D. Polyzos", "Athanasios Bacharis", "Saketh Madhuvarasu", "Tara Javidi"], "title": "3D Scene Rendering with Multimodal Gaussian Splatting", "comment": null, "summary": "3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.", "AI": {"tldr": "本研究提出了一种结合射频（RF）传感（如汽车雷达）和3D高斯泼溅（GS）渲染的多模态框架，以解决传统纯视觉GS在图像不足或视觉信息不可靠的场景下的局限性。", "motivation": "传统的基于视觉的3D高斯泼溅（GS）方法需要大量相机视图进行初始化，这在视觉线索不可靠（如恶劣天气、低光照、部分遮挡）的条件下效率低下且效果不佳。射频信号对这些因素具有鲁棒性，因此研究者希望利用RF信号的优势来增强GS的鲁棒性和效率。", "method": "提出了一种多模态框架，将RF传感（例如汽车雷达）与GS渲染相结合。该方法利用稀疏的RF深度测量来实现高效的深度预测，生成高质量的3D点云，用于初始化各种GS架构中的高斯函数。", "result": "通过数值测试表明，将RF传感融入GS管线可以实现高保真度的3D场景渲染。RF信息驱动的结构精度能够提高GS的渲染质量，尤其是在视觉信息受限的场景下。", "conclusion": "将RF传感（如雷达）与3D高斯泼溅相结合是一种比纯视觉GS更有效、更鲁棒的替代方案。该多模态框架能够从稀疏的RF深度测量中高效地预测深度，生成高质量的点云以初始化GS，从而在各种条件下实现高保真度的3D场景渲染。"}}
{"id": "2602.16935", "categories": ["cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16935", "abs": "https://arxiv.org/abs/2602.16935", "authors": ["Justin Albrethsen", "Yash Datta", "Kunal Kumar", "Sharath Rajasekar"], "title": "DeepContext: Stateful Real-Time Detection of Multi-Turn Adversarial Intent Drift in LLMs", "comment": "18 Pages, 7 Tables, 1 Figure", "summary": "While Large Language Model (LLM) capabilities have scaled, safety guardrails remain largely stateless, treating multi-turn dialogues as a series of disconnected events. This lack of temporal awareness facilitates a \"Safety Gap\" where adversarial tactics, like Crescendo and ActorAttack, slowly bleed malicious intent across turn boundaries to bypass stateless filters. We introduce DeepContext, a stateful monitoring framework designed to map the temporal trajectory of user intent. DeepContext discards the isolated evaluation model in favor of a Recurrent Neural Network (RNN) architecture that ingests a sequence of fine-tuned turn-level embeddings. By propagating a hidden state across the conversation, DeepContext captures the incremental accumulation of risk that stateless models overlook. Our evaluation demonstrates that DeepContext significantly outperforms existing baselines in multi-turn jailbreak detection, achieving a state-of-the-art F1 score of 0.84, which represents a substantial improvement over both hyperscaler cloud-provider guardrails and leading open-weight models such as Llama-Prompt-Guard-2 (0.67) and Granite-Guardian (0.67). Furthermore, DeepContext maintains a sub-20ms inference overhead on a T4 GPU, ensuring viability for real-time applications. These results suggest that modeling the sequential evolution of intent is a more effective and computationally efficient alternative to deploying massive, stateless models.", "AI": {"tldr": "提出了一种名为DeepContext的状态化安全监控框架，用于检测大型语言模型（LLM）多轮对话中的安全漏洞，相比于现有无状态模型，能更有效地识别和防御渐进式攻击，并在保持低推理开销的同时取得了先进的检测性能。", "motivation": "现有的LLM安全防护措施大多是无状态的，无法识别对话中跨轮次的恶意意图累积，导致“安全断层”，使得攻击者可以利用渐进式策略绕过安全防护。", "method": "设计了一个名为DeepContext的状态化监控框架，采用循环神经网络（RNN）架构，输入经过微调的逐轮对话嵌入序列，并通过传播隐藏状态来捕捉用户意图的累积风险，以取代隔离的评估模型。", "result": "DeepContext在多轮对话越狱检测方面显著优于现有基线，F1得分达到0.84，远超云服务商的防护措施和Llama-Prompt-Guard-2、Granite-Guardian等模型（0.67）。同时，在T4 GPU上推理开销低于20毫秒。", "conclusion": "对用户意图的序列演变进行建模，是比部署大规模无状态模型更有效且计算成本更低的一种安全防护替代方案。"}}
{"id": "2602.17108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17108", "abs": "https://arxiv.org/abs/2602.17108", "authors": ["Anton Dzega", "Aviad Elyashar", "Ortal Slobodin", "Odeya Cohen", "Rami Puzis"], "title": "Projective Psychological Assessment of Large Multimodal Models Using Thematic Apperception Tests", "comment": null, "summary": "Thematic Apperception Test (TAT) is a psychometrically grounded, multidimensional assessment framework that systematically differentiates between cognitive-representational and affective-relational components of personality-like functioning. This test is a projective psychological framework designed to uncover unconscious aspects of personality. This study examines whether the personality traits of Large Multimodal Models (LMMs) can be assessed through non-language-based modalities, using the Social Cognition and Object Relations Scale - Global (SCORS-G). LMMs are employed in two distinct roles: as subject models (SMs), which generate stories in response to TAT images, and as evaluator models (EMs), who assess these narratives using the SCORS-G framework. Evaluators demonstrated an excellent ability to understand and analyze TAT responses. Their interpretations are highly consistent with those of human experts. Assessment results highlight that all models understand interpersonal dynamics very well and have a good grasp of the concept of self. However, they consistently fail to perceive and regulate aggression. Performance varied systematically across model families, with larger and more recent models consistently outperforming smaller and earlier ones across SCORS-G dimensions.", "AI": {"tldr": "本研究使用社会认知与客体关系量表-全局版（SCORS-G）评估大型多模态模型（LMMs）的“人格特质”，将LMMs用作故事生成者（SMs）和故事评估者（EMs）。研究发现，LMMs在理解人际动力和自我概念方面表现良好，但在识别和调节攻击性方面存在困难，且模型规模和更新程度与其表现呈正相关。", "motivation": "研究的动机在于探索是否能通过非语言模态评估大型多模态模型（LMMs）的“人格特质”，并借鉴心理学中的人格评估框架（如TAT和SCORS-G）来理解LMMs的功能。", "method": "本研究采用两个主要方法：1. LMMs作为主体模型（SMs），根据TAT图片生成故事。2. LMMs作为评估模型（EMs），使用SCORS-G框架评估SMs生成的故事。研究者将LMMs的评估结果与人类专家的评估进行比较。", "result": "所有LMMs在理解人际动力和自我概念方面表现出高水平的一致性，且其评估结果与人类专家高度吻合。然而，所有模型在感知和调节攻击性方面均表现出一致的不足。研究还发现，模型表现与模型家族、规模和更新时间呈系统性关联，更大型、更新的模型表现更好。", "conclusion": "LMMs可以被用作评估工具，通过非语言模态（如TAT故事）评估其“人格特质”。LMMs在理解人际互动和自我概念方面具有一定能力，但在处理攻击性方面存在局限。模型的设计（规模和新旧）显著影响其评估能力。"}}
{"id": "2602.17134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17134", "abs": "https://arxiv.org/abs/2602.17134", "authors": ["Hiromichi Kamata", "Samuel Arthur Munro", "Fuminori Homma"], "title": "B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates", "comment": "Project page: https://sony.github.io/B3-Seg-project/", "summary": "Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.", "AI": {"tldr": "提出了一种名为 B^3-Seg 的新方法，用于在无需预训练或地面实况标签的情况下，快速对 3D 高斯喷溅（3DGS）进行交互式分割。该方法通过贝叶斯更新和基于预期信息增益（EIG）的主动视图选择，实现了高效且实时的分割。", "motivation": "现有的 3DGS 分割方法依赖于预设的相机视角、地面实况标签或昂贵的再训练，这使得它们不适用于需要低延迟的实时编辑场景。", "method": "将分割问题重新表述为序列 Beta-二项式贝叶斯更新。通过解析计算的预期信息增益（EIG）来主动选择下一个观测视角。EIG 的贝叶斯公式保证了其适应性和次模性，从而通过贪婪策略实现了最优视角采样策略的 $(1-1/e)$ 近似。", "result": "在多个数据集上，B^3-Seg 取得了与成本高昂的监督方法相媲美的分割结果，并且端到端分割耗时仅需几秒钟。", "conclusion": "B^3-Seg 是一种高效且具有理论基础的 3DGS 分割方法，能够在无相机和无训练的条件下实现开放词汇分割。它能够实现实际的、交互式的 3DGS 分割，并具有可证明的信息效率。"}}
{"id": "2602.17393", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.17393", "abs": "https://arxiv.org/abs/2602.17393", "authors": ["Minxing Sun", "Yao Mao"], "title": "Contact-Anchored Proprioceptive Odometry for Quadruped Robots", "comment": "28 pages, 30 figures", "summary": "Reliable odometry for legged robots without cameras or LiDAR remains challenging due to IMU drift and noisy joint velocity sensing. This paper presents a purely proprioceptive state estimator that uses only IMU and motor measurements to jointly estimate body pose and velocity, with a unified formulation applicable to biped, quadruped, and wheel-legged robots. The key idea is to treat each contacting leg as a kinematic anchor: joint-torque--based foot wrench estimation selects reliable contacts, and the corresponding footfall positions provide intermittent world-frame constraints that suppress long-term drift. To prevent elevation drift during extended traversal, we introduce a lightweight height clustering and time-decay correction that snaps newly recorded footfall heights to previously observed support planes. To improve foot velocity observations under encoder quantization, we apply an inverse-kinematics cubature Kalman filter that directly filters foot-end velocities from joint angles and velocities. The implementation further mitigates yaw drift through multi-contact geometric consistency and degrades gracefully to a kinematics-derived heading reference when IMU yaw constraints are unavailable or unreliable. We evaluate the method on four quadruped platforms (three Astrall robots and a Unitree Go2 EDU) using closed-loop trajectories. On Astrall point-foot robot~A, a $\\sim$200\\,m horizontal loop and a $\\sim$15\\,m vertical loop return with 0.1638\\,m and 0.219\\,m error, respectively; on wheel-legged robot~B, the corresponding errors are 0.2264\\,m and 0.199\\,m. On wheel-legged robot~C, a $\\sim$700\\,m horizontal loop yields 7.68\\,m error and a $\\sim$20\\,m vertical loop yields 0.540\\,m error. Unitree Go2 EDU closes a $\\sim$120\\,m horizontal loop with 2.2138\\,m error and a $\\sim$8\\,m vertical loop with less than 0.1\\,m vertical error. github.com/ShineMinxing/Ros2Go2Estimator.git", "AI": {"tldr": "本文提出了一种仅使用IMU和电机测量数据的纯本体感觉状态估计器，能够为双足、四足和轮腿机器人联合估计身体姿态和速度，解决了IMU漂移和关节速度传感噪声问题。", "motivation": "在没有相机或激光雷达的情况下，为多足机器人实现可靠的里程计估计面临IMU漂移和关节速度测量噪声的挑战。", "method": "该方法将接触腿视为运动学锚点，通过基于关节扭矩的足部测力计估计来选择可靠接触，并利用足部着地位置提供的世界坐标系约束来抑制长期漂移。为防止长时间移动中的高度漂移，引入了轻量级高度聚类和时间衰减校正。通过逆运动学立方体卡尔曼滤波器直接过滤足端速度。为减轻偏航漂移，利用多接触几何一致性，并在IMU偏航约束不可用时退化到运动学衍生的航向参考。", "result": "在四种四足机器人平台上进行了评估，包括Astrall机器人和Unitree Go2 EDU。在长距离（~200m）和高距离（~15m）闭环测试中，误差分别达到0.1638m和0.219m（Astrall点足机器人A），0.2264m和0.199m（轮腿机器人B）。在更长的长距离（~700m）和高距离（~20m）测试中，轮腿机器人C误差为7.68m和0.540m。Unitree Go2 EDU在~120m长距离测试中误差为2.2138m，~8m高距离测试中误差小于0.1m。", "conclusion": "所提出的纯本体感觉状态估计器能够有效地利用IMU和电机测量数据，为多种多足机器人平台提供准确的姿态和速度估计，并在各种距离和高度的闭环测试中展现出令人满意的性能。"}}
{"id": "2602.17127", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17127", "abs": "https://arxiv.org/abs/2602.17127", "authors": ["Dusan Bosnjakovic"], "title": "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI", "comment": null, "summary": "As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.\n  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.\n  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.", "AI": {"tldr": "本研究提出了一种新的审计框架，利用心理测量学理论来量化大型语言模型（LLMs）中稳定、潜在的行为特征（“思维模式”），而不是依赖于传统的任务准确性基准。该框架通过强制选择的序数小插曲和语义正交的诱饵来评估模型，并使用混合线性模型进行分析。", "motivation": "随着LLMs在多智能体系统和递归评估循环中变得越来越重要，检测持续的、提供商层面的行为特征对于安全和治理至关重要。传统基准未能捕捉到模型版本更新后依然存在的稳定、潜在的响应策略。", "method": "研究引入了一种新的审计框架，该框架利用心理测量学测量理论（特别是序数不确定性下的潜在特质估计），无需地面真实标签即可量化这些倾向。研究使用了强制选择的序数小插曲，这些小插曲被语义正交的诱饵所掩盖，并通过加密排列不变性来控制。使用混合线性模型（MixedLM）和组内相关系数（ICC）分析。", "result": "研究发现，尽管项目级别的框架驱动高方差，但持续的“实验室信号”导致显著的行为聚类。这表明在“锁定”的提供商生态系统中，潜在偏差是累积变量，可能在多层AI架构中产生递归的意识形态回声室。", "conclusion": "本研究证明，LLMs存在持续的、提供商层面的行为特征（“思维模式”），这些特征可以通过心理测量学方法进行量化，并且在某些生态系统中存在递归的意识形态回声室风险。"}}
{"id": "2602.17421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17421", "abs": "https://arxiv.org/abs/2602.17421", "authors": ["Diana Cafiso", "Petr Trunin", "Carolina Gay", "Lucia Beccai"], "title": "3D-printed Soft Optical sensor with a Lens (SOLen) for light guidance in mechanosensing", "comment": "11 pages, 5 figures, submitted to Materials & Design", "summary": "Additive manufacturing is enabling soft robots with increasingly complex geometries, creating a demand for sensing solutions that remain compatible with single-material, one-step fabrication. Optical soft sensors are attractive for monolithic printing, but their performance is often degraded by uncontrolled light propagation (ambient coupling, leakage, scattering), while common miti- gation strategies typically require multimaterial interfaces. Here, we present an approach for 3D printed soft optical sensing (SOLen), in which a printed lens is placed in front of an emitter within a Y-shaped waveguide. The sensing mechanism relies on deformation-induced lens rotation and focal-spot translation, redistributing optical power between the two branches to generate a differential output that encodes both motion direction and amplitude. An acrylate polyurethane resin was modified with lauryl acrylate to improve compliance and optical transmittance, and single-layer optical characterization was used to derive wavelength-dependent refractive index and transmittance while minimizing DLP layer-related artifacts. The measured refractive index was used in simulations to design a lens profile for a target focal distance, which was then printed with sub-millimeter fidelity. Rotational tests demonstrated reproducible branch-selective signal switching over multiple cycles. These results establish a transferable material-to-optics workflow for soft optical sensors with lens with new functionalities for next-generation soft robots", "AI": {"tldr": "本研究提出了一种名为 SOLen 的 3D 打印软体光学传感方法，利用 Y 型波导内打印的透镜和光源，通过形变诱导的透镜旋转来控制光在两个分支间的分配，从而检测运动的方向和幅度。", "motivation": "现有技术在实现单材料、一步法打印软体机器人传感器方面存在挑战。光学软传感器具有单片打印的潜力，但易受光传播失控（环境耦合、泄漏、散射）的影响，而常见的缓解方法通常需要多材料接口。", "method": "本研究设计了一种 Y 型波导结构，并在其中集成了一个 3D 打印的透镜和一个光源。通过修改丙烯酸聚氨酯树脂，加入月桂酸丙烯酯来提高柔韧性和光学透过率。使用单层光学表征方法确定了波长相关的折射率和透过率，并最小化了 DLP 打印相关的伪影。利用测得的折射率，通过仿真设计了透镜轮廓，以实现目标焦距，并以亚毫米级精度进行了打印。通过旋转测试来评估传感器在不同方向和幅度下的响应。", "result": "传感器能够通过形变诱导的透镜旋转和焦斑移动，在两个分支之间重新分配光功率，产生能够编码运动方向和幅度的差分输出。旋转测试证明了传感器在多个循环中具有可重复的、分支选择性的信号切换能力。", "conclusion": "该研究建立了一种可迁移的材料到光学器件的工作流程，实现了具有新型功能的软体光学传感器，为下一代软体机器人提供了新的传感解决方案。"}}
{"id": "2602.17194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17194", "abs": "https://arxiv.org/abs/2602.17194", "authors": ["Adrian Cosma", "Cosmin Dumitrache", "Emilian Radoi"], "title": "What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform", "comment": null, "summary": "Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.", "AI": {"tldr": "本研究分析了罗马尼亚文本远程医疗中的患者满意度信号，发现患者和医生历史记录是预测满意度的主要因素，而回应文本的礼貌性和迟疑性等特征虽信号较小但可操作，并且与积极反馈相关。", "motivation": "文本远程医疗中，医生需要以书面形式清晰有效地提供医疗建议。患者反馈日益影响医生，但这些反馈常反映沟通质量而非临床准确性。因此，研究如何识别和利用患者反馈信号以提升沟通质量至关重要。", "method": "研究人员使用77,334条匿名患者问题-医生回应对的数据集，将患者反馈建模为二元结果（点赞为正，无反馈或差评为负）。提取了语言无关的文本特征（如长度、结构、可读性）、罗马尼亚语LIWC心理语言学特征以及礼貌/迟疑标记。使用时间分割训练分类器，并通过SHAP值分析特征的重要性。", "result": "患者和医生历史记录特征在预测患者满意度方面起主导作用。回应文本的特征（如礼貌性和迟疑性）对满意度有较小的积极影响，而词汇多样性则与积极反馈呈负相关。", "conclusion": "在文本远程医疗中，虽然患者和医生的历史交互是预测满意度的关键，但回应文本中的礼貌性和迟疑性等沟通技巧是可操作的信号，可以用来提升患者满意度。"}}
{"id": "2602.16943", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16943", "abs": "https://arxiv.org/abs/2602.16943", "authors": ["Arnold Cartagena", "Ariane Teixeira"], "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents", "comment": "23 pages, 5 figures, 4 tables, code and data at https://github.com/acartag7/gap-benchmark", "summary": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.", "AI": {"tldr": "本研究提出了GAP基准来评估大型语言模型（LLM）作为代理时的工具调用安全性和文本安全性之间的差异。研究发现，文本安全不一定能保证工具调用安全，并且系统提示的措辞对工具调用行为有显著影响，而运行时治理合约未能阻止有害的工具调用尝试。", "motivation": "现有的LLM安全评估主要集中在文本层面的拒绝行为，而忽略了LLM代理通过工具调用执行的具有实际后果的操作。这留下了一个关键问题：能够抑制有害文本的对齐是否也能抑制有害操作？", "method": "研究引入了GAP基准，这是一个系统性的评估框架，用于衡量LLM代理中文本安全性与工具调用安全性之间的差异。在六个受监管的领域（医药、金融、教育、就业、法律和基础设施）、七种越狱场景、三种系统提示条件（中性、安全强化和工具鼓励）以及两种提示变体下，测试了六个前沿模型，生成了17,420个数据点。", "result": "研究发现文本安全不能转移到工具调用安全。在所有六个模型中，都观察到模型文本拒绝有害请求，但工具调用却同时执行了被禁止的操作。即使在安全强化的系统提示下，仍有219个这样的案例。系统提示的措辞对工具调用行为有显著影响，TC安全率因模型而异，差异可达21%到57%。运行时治理合约在减少信息泄露方面有效，但未能阻止有害工具调用尝试。", "conclusion": "文本级别的安全评估不足以评估LLM代理的行为，工具调用安全需要专门的测量和缓解措施。"}}
{"id": "2602.16942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16942", "abs": "https://arxiv.org/abs/2602.16942", "authors": ["Hexi Jin", "Stephen Liu", "Yuheng Li", "Simran Malik", "Yiying Zhang"], "title": "SourceBench: Can AI Answers Reference Quality Web Sources?", "comment": null, "summary": "Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.", "AI": {"tldr": "本文提出了SourceBench基准，用于评估大型语言模型（LLMs）引用的网络来源的质量，并发现了LLMs和AI搜索工具在来源质量方面存在的不足，提出了四个关键的新见解。", "motivation": "现有的LLM评估主要关注答案的正确性，而忽略了引用证据（即网络来源）的质量。作者认为评估证据质量对于全面理解LLM在现实世界中的表现至关重要，并旨在填补这一评估空白。", "method": "1. 提出了SourceBench基准，包含100个真实世界查询，覆盖信息性、事实性、论证性、社交性和购物意图。\n2. 设计了一个包含八个指标的评估框架，涵盖内容质量（内容相关性、事实准确性、客观性）和页面信号（新鲜度、权威性/可问责性、清晰度）。\n3. 构建了一个包含人工标注数据的数据集，并开发了一个校准过的基于LLM的评估器，该评估器能接近专家判断。\n4. 使用SourceBench评估了八个LLM、Google搜索和三个AI搜索工具，总计3996个引用来源。\n5. 进行了额外的实验以深入理解评估结果。", "result": "1. LLMs引用的网络来源质量普遍不高，存在事实不准确、内容相关性差、不客观、过时或缺乏权威性等问题。\n2. AI搜索工具在引用来源质量方面表现优于纯LLMs，但仍有改进空间。\n3. 识别了四个关于LLM和网络搜索在引用质量方面的关键新见解。", "conclusion": "LLMs在回答问题时引用的网络来源质量是当前一个亟待解决的问题。SourceBench提供了一个全面的评估框架和数据集，揭示了现有LLMs和AI搜索工具在此方面的不足，并为未来的研究指明了方向，旨在提升LLM生成内容的可信度和实用性。"}}
{"id": "2602.17472", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17472", "abs": "https://arxiv.org/abs/2602.17472", "authors": ["Mohamed Sabry", "Joseba Gorospe", "Cristina Olaverri-Monreal"], "title": "A Cost-Effective and Climate-Resilient Air Pressure System for Rain Effect Reduction on Automated Vehicle Cameras", "comment": null, "summary": "Recent advances in automated vehicles have focused on improving perception performance under adverse weather conditions; however, research on physical hardware solutions remains limited, despite their importance for perception critical applications such as vehicle platooning. Existing approaches, such as hydrophilic or hydrophobic lenses and sprays, provide only partial mitigation, while industrial protection systems imply high cost and they do not enable scalability for automotive deployment.\n  To address these limitations, this paper presents a cost-effective hardware solution for rainy conditions, designed to be compatible with multiple cameras simultaneously.\n  Beyond its technical contribution, the proposed solution supports sustainability goals in transportation systems. By enabling compatibility with existing camera-based sensing platforms, the system extends the operational reliability of automated vehicles without requiring additional high-cost sensors or hardware replacements. This approach reduces resource consumption, supports modular upgrades, and promotes more cost-efficient deployment of automated vehicle technologies, particularly in challenging weather conditions where system failures would otherwise lead to inefficiencies and increased emissions. The proposed system was able to increase pedestrian detection accuracy of a Deep Learning model from 8.3% to 41.6%.", "AI": {"tldr": "本文提出了一种经济高效的硬件解决方案，用于改善自动驾驶汽车在雨天条件下的摄像头感知能力，提高了行人检测的准确率。", "motivation": "现有自动驾驶汽车的研发主要集中在软件层面，而针对恶劣天气（尤其是雨天）的物理硬件解决方案研究不足。现有方法效果有限且成本高昂，不适用于大规模汽车部署。", "method": "开发了一种经济高效、可同时兼容多个摄像头的硬件解决方案，以应对雨天影响。该方案提高了摄像头在雨天条件下的性能。", "result": "所提出的系统能够将基于深度学习模型的行人检测准确率从8.3%提高到41.6%。", "conclusion": "该硬件解决方案有效解决了雨天摄像头感知性能下降的问题，成本低廉且易于扩展，同时有助于实现交通系统的可持续发展目标，提高自动驾驶车辆在恶劣天气下的可靠性。"}}
{"id": "2602.17168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17168", "abs": "https://arxiv.org/abs/2602.17168", "authors": ["Siyuan Liang", "Yongcheng Jing", "Yingjie Wang", "Jiaxing Huang", "Ee-chien Chang", "Dacheng Tao"], "title": "BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning", "comment": "25 pages, 10 figures", "summary": "Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.", "AI": {"tldr": "本文提出了 BadCLIP++，一个用于多模态对比学习的后门攻击框架，解决了现有方法的隐蔽性和持久性挑战。通过引入语义融合 QR 微触发器和目标对齐子集选择来提高隐蔽性，并通过半径收缩、质心对齐、曲率控制和弹性权重巩固来增强持久性。理论分析和实验表明，BadCLIP++ 在极低的投毒率下也能实现高攻击成功率，并对多种防御手段表现出鲁棒性。", "motivation": "现有针对多模态对比学习的后门攻击方法在隐蔽性和持久性方面存在不足。交叉模态不一致性会暴露触发模式，而低中毒率下的梯度稀释会导致后门遗忘。这些问题需要被更好地建模和解决。", "method": "BadCLIP++ 框架包含两个主要部分：1. 隐蔽性：引入语义融合 QR 微触发器，将不可感知模式嵌入任务相关区域，并使用目标对齐子集选择来增强低注入率下的信号。2. 持久性：通过半径收缩和质心对齐稳定触发器嵌入，通过曲率控制和弹性权重巩固稳定模型参数，使其不易受微调影响。此外，还提供了理论分析，证明在信任区域内，干净微调和后门目标的梯度是同向的。", "result": "在数字攻击设置下，当投毒率仅为 0.3% 时，BadCLIP++ 实现了 99.99% 的攻击成功率（ASR），比基线方法高 11.4 个百分点。在 19 种防御方法下，ASR 仍保持在 99.90% 以上，干净准确率下降不到 0.8%。在物理攻击中，成功率达到 65.03%，并对水印移除防御具有鲁棒性。", "conclusion": "BadCLIP++ 是一个有效的后门攻击框架，能够克服多模态对比学习模型中后门攻击的隐蔽性和持久性挑战。该方法在极低的投毒率下展现出优越的攻击性能和对多种防御措施的鲁棒性。"}}
{"id": "2602.16953", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16953", "abs": "https://arxiv.org/abs/2602.16953", "authors": ["Hejia Zhang", "Zhongming Yu", "Chia-Tung Ho", "Haoxing Ren", "Brucek Khailany", "Jishen Zhao"], "title": "LLM4Cov: Execution-Aware Agentic Learning for High-coverage Testbench Generation", "comment": null, "summary": "Execution-aware LLM agents offer a promising paradigm for learning from tool feedback, but such feedback is often expensive and slow to obtain, making online reinforcement learning (RL) impractical. High-coverage hardware verification exemplifies this challenge due to its reliance on industrial simulators and non-differentiable execution signals. We propose LLM4Cov, an offline agent-learning framework that models verification as memoryless state transitions guided by deterministic evaluators. Building on this formulation, we introduce execution-validated data curation, policy-aware agentic data synthesis, and worst-state-prioritized sampling to enable scalable learning under execution constraints. We further curate a reality-aligned benchmark adapted from an existing verification suite through a revised evaluation protocol. Using the proposed pipeline, a compact 4B-parameter model achieves 69.2% coverage pass rate under agentic evaluation, outperforming its teacher by 5.3% and demonstrating competitive performance against models an order of magnitude larger.", "AI": {"tldr": "提出了一种名为LLM4Cov的离线强化学习框架，用于解决硬件验证中反馈成本高、速度慢的问题，通过将验证建模为无记忆状态转移，并结合数据策选、数据合成和采样策略，成功训练了一个小型语言模型，在验证覆盖率方面超越了更大的模型。", "motivation": "传统的在线强化学习方法难以应用于硬件验证，因为获取工具反馈（如仿真结果）成本高且耗时，而硬件验证依赖于工业模拟器和非可微的执行信号。", "method": "将硬件验证建模为由确定性评估器指导的无记忆状态转移。在此基础上，引入了执行验证的数据策选、策略感知的代理数据合成和最差状态优先采样等技术，以实现满足执行约束的可扩展学习。同时，创建了一个新的基准测试集。", "result": "使用LLM4Cov框架，一个参数量为40亿的小型模型在代理评估下达到了69.2%的覆盖率通过率，比其教师模型高出5.3%，并且性能可与规模大十倍的模型相媲美。", "conclusion": "LLM4Cov框架通过有效的离线学习策略，能够克服硬件验证中执行反馈的限制，并实现高效、可扩展的学习，使小型语言模型在复杂任务中也能取得优异表现。"}}
{"id": "2602.17182", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17182", "abs": "https://arxiv.org/abs/2602.17182", "authors": ["Jiwei Shan", "Zeyu Cai", "Yirui Li", "Yongbo Chen", "Lijun Han", "Yun-hui Liu", "Hesheng Wang", "Shing Shin Cheng"], "title": "NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting", "comment": null, "summary": "Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.", "AI": {"tldr": "本文提出了一种基于3D高斯泼溅的单目非刚性SLAM系统NRGS-SLAM，用于内窥镜场景，通过引入可变形的3D高斯地图和自监督学习策略，有效解决了相机运动和软组织变形之间的耦合模糊问题，实现了更精确的姿态估计和更高质量的重建。", "motivation": "现有的单目非刚性SLAM方法在内窥镜场景下存在有效的解耦机制不足、场景表示保真度低的问题，导致跟踪漂移和重建质量受限，这是由于内窥镜场景的软组织形变违反了刚性假设，造成了相机运动和形变之间的耦合模糊。", "method": "提出NRGS-SLAM系统，核心是构建了一个可变形的3D高斯地图，每个高斯原语都包含一个可学习的形变概率，并采用贝叶斯自监督策略进行优化。在此基础上，设计了一个可变形跟踪模块，通过优先考虑低形变区域进行粗到细的姿态估计，并高效更新每帧的形变。同时，一个可变形映射模块逐步扩展和优化地图。此外，还设计了一个统一的鲁棒几何损失，结合外部几何先验来缓解单目非刚性SLAM的内在病态性。", "result": "在多个公开的内窥镜数据集上的实验表明，NRGS-SLAM在相机姿态估计方面比最先进的方法提高了准确性（RMSE降低高达50%），并在照片级真实感重建方面达到了更高的质量。消融实验也验证了关键设计选择的有效性。", "conclusion": "NRGS-SLAM是一种有效的单目非刚性SLAM系统，通过引入可变形的3D高斯表示和创新的自监督解耦策略，成功克服了内窥镜场景下的挑战，显著提升了姿态估计精度和重建质量。"}}
{"id": "2602.17186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17186", "abs": "https://arxiv.org/abs/2602.17186", "authors": ["Seulbi Lee", "Sangheum Hwang"], "title": "Selective Training for Large Vision Language Models via Visual Information Gain", "comment": null, "summary": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.", "AI": {"tldr": "本文提出了一种名为视觉信息增益（VIG）的新指标，用于量化图像信息在视觉语言模型（LVLM）预测中的贡献程度，并在此基础上设计了一种 VIG 引导的选择性训练策略，以提高模型的视觉基础性和减少语言偏见。", "motivation": "现有的 LVLM 存在语言偏见问题，即模型生成的答案不依赖于视觉证据。现有缓解方法缺乏对单个训练样本或 token 在多大程度上受益于图像的量化衡量。", "method": "提出了一种基于困惑度的指标“视觉信息增益（VIG）”，用于衡量视觉输入所带来的预测不确定性降低程度。VIG 可以在样本和 token 层面进行细粒度分析。基于 VIG，提出了一种 VIG 引导的选择性训练方案，优先训练高 VIG 值的样本和 token。", "result": "VIG 能够有效识别出颜色、空间关系和属性等视觉基础性元素。VIG 引导的选择性训练方案能够提高模型的视觉基础性，减轻语言偏见，并在显著减少监督的情况下实现更优的性能。", "conclusion": "VIG 是一种有效的量化视觉信息贡献的指标，VIG 引导的选择性训练是一种能够以更少监督提高 LVLM 视觉基础性并缓解语言偏见的新方法。"}}
{"id": "2602.17474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17474", "abs": "https://arxiv.org/abs/2602.17474", "authors": ["Carolina Gay", "Petr Trunin", "Diana Cafiso", "Yuejun Xu", "Majid Taghavi", "Lucia Beccai"], "title": "Optically Sensorized Electro-Ribbon Actuator (OS-ERA)", "comment": "6 pages, 5 figures, accepted for 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Electro-Ribbon Actuators (ERAs) are lightweight flexural actuators that exhibit ultrahigh displacement and fast movement. However, their embedded sensing relies on capacitive sensors with limited precision, which hinders accurate control. We introduce OS-ERA, an optically sensorized ERA that yields reliable proprioceptive information, and we focus on the design and integration of a sensing solution without affecting actuation. To analyse the complex curvature of an ERA in motion, we design and embed two soft optical waveguide sensors. A classifier is trained to map the sensing signals in order to distinguish eight bending states. We validate our model on six held-out trials and compare it against signals' trajectories learned from training runs. Across all tests, the sensing output signals follow the training manifold, and the predicted sequence mirrors real performance and confirms repeatability. Despite deliberate train-test mismatches in actuation speed, the signal trajectories preserve their shape, and classification remains consistently accurate, demonstrating practical voltage- and speed-invariance. As a result, OS-ERA classifies bending states with high fidelity; it is fast and repeatable, solving a longstanding bottleneck of the ERA, enabling steps toward closed-loop control.", "AI": {"tldr": "本文提出了一种名为OS-ERA的光学传感致动器，克服了现有ERA传感器精度低的限制，能够精确测量ERA的弯曲状态，并实现了对致动器状态的可靠识别，为闭环控制奠定了基础。", "motivation": "现有电致伸缩致动器（ERAs）虽然具有高位移和快速运动的优点，但其内置的电容式传感器精度有限，阻碍了精确控制。因此，研究需要一种能够提供可靠本体感觉信息的新型传感解决方案。", "method": "通过设计和集成两个柔软的光波导传感器到ERAs中，提出OS-ERA。利用这些传感器收集信号，并训练一个分类器来识别八种不同的弯曲状态。对模型进行了独立测试和与训练数据轨迹的比较。", "result": "OS-ERA的传感信号在训练流形上表现良好，分类结果准确，并且在不同驱动速度下仍能保持高精度，证明了其电压和速度不变性。分类准确率高，速度快，可重复性好。", "conclusion": "OS-ERA成功解决了ERA传感器精度低的长期瓶颈问题，能够高保真地识别弯曲状态，为实现ERAs的闭环控制提供了可行方案。"}}
{"id": "2602.17262", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.17262", "abs": "https://arxiv.org/abs/2602.17262", "authors": ["Kensuke Okada", "Yui Furukawa", "Kyosuke Bunji"], "title": "Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study", "comment": null, "summary": "Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.", "AI": {"tldr": "本研究提出了一个心理测量框架来量化和减轻大型语言模型（LLMs）在问卷评估中存在的社会期望效应（SDR）。通过比较在诚实和虚假-良好指令下的问卷反应，并结合项目反应理论（IRT），可以量化SDR。此外，研究还开发了一种可选的强制选择（GFC）问卷来降低SDR，并发现这种方法在大多数情况下能有效减少SDR，同时保留恢复模型目标画像的能力。", "motivation": "目前NLP领域使用的人工报告问卷来评估LLMs（如一致性、安全性和偏见）存在一个问题：LLMs可能会给出社会期望的答案（SDR），而不是真实的答案，这会影响评估结果和结论。因此，需要一种方法来量化和减轻这种SDR。", "method": "研究提出了一个量化SDR的心理测量框架：1. 在HONEST（诚实）和FAKE-GOOD（虚假-良好）两种指令下进行相同的问卷测试；2. 使用项目反应理论（IRT）估计潜在得分；3. 计算SDR为一个方向校正的标准效应量。为了减轻SDR，研究构建了一个经过优化的、跨领域的、强制选择（GFC）的大五人格问卷，以匹配期望度。", "result": "在对九个指令微调LLMs的评估中，李克特量表问卷显示出持续较大的SDR，而经过优化的GFC问卷显著减弱了SDR，并很大程度上保留了恢复目标画像的能力。结果表明，SDR和画像恢复之间存在模型依赖的权衡。", "conclusion": "研究强调了在LLMs的问卷基准测试和审计中，SDR是一个不容忽视的问题。建议采用SDR意识的报告方法，并提出了一种有效的减轻SDR的GFC问卷方法，虽然这可能在一定程度上影响画像恢复的准确性。"}}
{"id": "2602.17502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17502", "abs": "https://arxiv.org/abs/2602.17502", "authors": ["Kyle R. Embry", "Lorenzo Vianello", "Jim Lipsey", "Frank Ursetta", "Michael Stephens", "Zhi Wang", "Ann M. Simon", "Andrea J. Ikeda", "Suzanne B. Finucane", "Shawana Anarwala", "Levi J. Hargrove"], "title": "Proximal powered knee placement: a case study", "comment": "Submitted to IEEE RAS/EMBS 11th International Conference on Biomedical Robotics and Biomechatronics (BioRob 2026)", "summary": "Lower limb amputation affects millions worldwide, leading to impaired mobility, reduced walking speed, and limited participation in daily and social activities. Powered prosthetic knees can partially restore mobility by actively assisting knee joint torque, improving gait symmetry, sit-to-stand transitions, and walking speed. However, added mass from powered components may diminish these benefits, negatively affecting gait mechanics and increasing metabolic cost. Consequently, optimizing mass distribution, rather than simply minimizing total mass, may provide a more effective and practical solution. In this exploratory study, we evaluated the feasibility of above-knee powertrain placement for a powered prosthetic knee in a small cohort. Compared to below-knee placement, the above-knee configuration demonstrated improved walking speed (+9.2% for one participant) and cadence (+3.6%), with mixed effects on gait symmetry. Kinematic measures indicated similar knee range of motion and peak velocity across configurations. Additional testing on ramps and stairs confirmed the robustness of the control strategy across multiple locomotion tasks. These preliminary findings suggest that above-knee placement is functionally feasible and that careful mass distribution can preserve the benefits of powered assistance while mitigating adverse effects of added weight. Further studies are needed to confirm these trends and guide design and clinical recommendations.", "AI": {"tldr": "将动力假肢膝盖的动力单元移至膝盖上方（大腿处）而非膝盖下方（小腿处）可能是一种可行的设计，可以提高行走速度并可能减少对步态对称性的负面影响，同时保持膝关节的运动范围。", "motivation": "动力假肢膝盖虽然能改善截肢者的活动能力，但其增加的质量可能会适得其反，增加代谢成本并影响步态。因此，研究如何优化质量分布，而不是仅仅减小总质量，对于最大化益处至关重要。", "method": "本研究评估了将动力假肢膝盖的动力单元放置在大腿处（上方）而非小腿处（下方）的可行性。通过在一个小型队列中比较这两种配置，评估了行走速度、步频、步态对称性、膝关节运动范围和峰值速度等指标，并在斜坡和楼梯上进行了额外测试。", "result": "与膝盖下方配置相比，膝盖上方配置显示出行走速度（平均提高9.2%）和步频（平均提高3.6%）有所提高，但对步态对称性的影响不一。膝关节的运动范围和峰值速度在两种配置下相似。控制策略在斜坡和楼梯等多种运动任务中表现出鲁棒性。", "conclusion": "初步研究结果表明，将动力单元置于膝盖上方是功能上可行的，并且仔细的质量分布可以保留动力辅助的优势，同时减轻附加重量的不利影响。需要进一步的研究来证实这些趋势并指导设计和临床建议。"}}
{"id": "2602.17283", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17283", "abs": "https://arxiv.org/abs/2602.17283", "authors": ["Yukun Chen", "Xinyu Zhang", "Jialong Tang", "Yu Wan", "Baosong Yang", "Yiming Li", "Zhan Qin", "Kui Ren"], "title": "Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective", "comment": null, "summary": "While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\\%$), with significant performance disparities across different languages ($ΔAcc > 20\\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.", "AI": {"tldr": "本研究提出了X-Value，一个包含5000多个跨语言问答对的基准测试，用于评估大型语言模型（LLMs）在识别内容深层价值观方面的能力，特别是超越了对显性危害的检测。研究发现，当前最先进的LLMs在跨语言价值观评估方面存在不足，并且在不同语言之间存在显著的性能差异。", "motivation": "当前评估LLMs在内容安全方面的研究主要关注显性危害（如暴力或仇恨言论），而忽略了数字内容中传达的更微妙的价值维度。研究者希望弥合这一差距，从全球视角评估LLMs对内容深层价值观的判断能力。", "method": "研究者开发了一个名为X-Value的跨语言价值观评估基准，包含超过5000个问答对，覆盖18种语言。该基准基于Schwartz的基本人类价值观理论，系统地组织成7个核心领域，并分为简单和困难两个级别。同时，提出了一种独特的两阶段标注框架：首先确定问题属于全球共识（如人权）还是多元主义（如宗教），然后进行多方评估内容中嵌入的潜在价值观。", "result": "在X-Value基准上的系统性评估显示，当前最先进的LLMs在跨语言价值观评估方面表现不佳（准确率低于77%），并且在不同语言之间存在显著的性能差异（准确率差异超过20%）。", "conclusion": "该研究强调了改进LLMs在细致的、顾及价值观的内容评估方面的能力迫在眉睫。X-Value基准的开发和评估揭示了当前LLMs在处理跨语言价值观判断时的局限性。"}}
{"id": "2602.17196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17196", "abs": "https://arxiv.org/abs/2602.17196", "authors": ["Yahong Wang", "Juncheng Wu", "Zhangkai Ni", "Chengmei Yang", "Yihang Liu", "Longzhen Yang", "Yuyin Zhou", "Ying Wen", "Lianghua He"], "title": "EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models", "comment": null, "summary": "Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an \"Entropy Collapse Layer\" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.", "AI": {"tldr": "本研究提出了一种基于矩阵熵的视觉代币剪枝框架EntropyPrune，通过识别“熵塌陷层”（ECL）来确定剪枝时机，并高效地量化和移除冗余代币，显著提高了多模态大语言模型（MLLM）的推理效率，同时保持了性能。", "motivation": "多模态大语言模型（MLLMs）在推理时需要处理大量的视觉代币，导致成本高昂。现有的代币剪枝方法多依赖于启发式策略，缺乏可解释性和跨模型迁移性。", "method": "研究者提出了一个基于矩阵熵的视角，识别出“熵塌陷层”（ECL）作为剪枝阶段的原则性标准。基于此，他们开发了EntropyPrune框架，该框架量化视觉代币的信息价值并移除冗余代币，不依赖于注意力图。此外，通过利用双Gram矩阵的谱等价性，降低了熵计算的复杂性，实现了理论上的64倍加速。", "result": "在多个多模态基准测试中，EntropyPrune在准确性和效率方面均优于现有最先进的剪枝方法。例如，在LLaVA-1.5-7B上，该方法将FLOPs减少了68.2%，同时保留了96.0%的原始性能。EntropyPrune在处理高分辨率图像和视频模型方面也表现出良好的泛化能力。", "conclusion": "EntropyPrune是一种新颖且高效的多模态大语言模型加速框架，通过基于矩阵熵的原理性方法，能够有效识别并移除冗余的视觉代币，在不牺牲性能的情况下大幅降低推理成本，并展现出良好的鲁棒性和可扩展性。"}}
{"id": "2602.16976", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16976", "abs": "https://arxiv.org/abs/2602.16976", "authors": ["Srikumar Nayak"], "title": "HQFS: Hybrid Quantum Classical Financial Security with VQC Forecasting, QUBO Annealing, and Audit-Ready Post-Quantum Signing", "comment": "11 pages, 1 fig , 4 tables", "summary": "Here's the corrected paragraph with all punctuation and formatting issues fixed:\n  Financial risk systems usually follow a two-step routine: a model predicts return or risk, and then an optimizer makes a decision such as a portfolio rebalance. In practice, this split can break under real constraints. The prediction model may look good, but the final decision can be unstable when the market shifts, when discrete constraints are added (lot sizes, caps), or when the optimization becomes slow for larger asset sets. Also, regulated settings need a clear audit trail that links each decision to the exact model state and inputs. We present HQFS, a practical hybrid pipeline that connects forecasting, discrete risk optimization, and auditability in one flow. First, HQFS learns next-step return and a volatility proxy using a variational quantum circuit (VQC) with a small classical head. Second, HQFS converts the risk-return objective and constraints into a QUBO and solves it with quantum annealing when available, while keeping a compatible classical QUBO solver as a fallback for deployment. Third, HQFS signs each rebalance output using a post-quantum signature so the allocation can be verified later without trusting the runtime environment. On our market dataset study, HQFS reduces return prediction error by 7.8% and volatility prediction error by 6.1% versus a tuned classical baseline. For the decision layer, HQFS improves out-of-sample Sharpe by 9.4% and lowers maximum drawdown by 11.7%. The QUBO solve stage also cuts average solve time by 28% compared to a mixed-integer baseline under the same constraints, while producing fully traceable, signed allocation records.", "AI": {"tldr": "该研究提出了一种名为 HQFS 的混合流水线，用于连接金融风险管理中的预测、离散风险优化和可审计性。它利用量子计算进行预测和优化，并使用后量子签名确保可审计性，从而提高了预测准确性、投资组合表现并缩短了求解时间。", "motivation": "传统的金融风险管理方法在实际约束下（如离散约束、市场变化）可能导致决策不稳定，并且需要明确的审计追踪。因此，需要一种能够整合预测、优化和审计性的解决方案。", "method": "HQFS 采用三步流程：1. 使用变分量子电路（VQC）学习下一阶段的回报和波动率代理。2. 将风险-回报目标和约束转换为 QUBO 问题，利用量子退火器求解，并提供兼容的经典 QUBO 求解器作为后备。3. 使用后量子签名验证每个重新平衡输出，以确保可审计性。", "result": "与经典基线相比，HQFS 将回报预测误差降低了 7.8%，波动率预测误差降低了 6.1%。在投资组合优化方面，HQFS 将样本外夏普比率提高了 9.4%，最大回撤降低了 11.7%。 QUBO 求解阶段平均求解时间比混合整数基线缩短了 28%，并产生了完全可追溯、已签名的分配记录。", "conclusion": "HQFS 作为一个实用的混合流水线，成功地将量子计算应用于金融风险管理，实现了预测、离散风险优化和可审计性的集成，并在实际应用中展现出优越的性能和效率。"}}
{"id": "2602.17515", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17515", "abs": "https://arxiv.org/abs/2602.17515", "authors": ["Ziyi Zong", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "RA-Nav: A Risk-Aware Navigation System Based on Semantic Segmentation for Aerial Robots in Unpredictable Environments", "comment": null, "summary": "Existing aerial robot navigation systems typically plan paths around static and dynamic obstacles, but fail to adapt when a static obstacle suddenly moves. Integrating environmental semantic awareness enables estimation of potential risks posed by suddenly moving obstacles. In this paper, we propose RA- Nav, a risk-aware navigation framework based on semantic segmentation. A lightweight multi-scale semantic segmentation network identifies obstacle categories in real time. These obstacles are further classified into three types: stationary, temporarily static, and dynamic. For each type, corresponding risk estimation functions are designed to enable real-time risk prediction, based on which a complete local risk map is constructed. Based on this map, the risk-informed path search algorithm is designed to guarantee planning that balances path efficiency and safety. Trajectory optimization is then applied to generate trajectories that are safe, smooth, and dynamically feasible. Comparative simulations demonstrate that RA-Nav achieves higher success rates than baselines in sudden obstacle state transition scenarios. Its effectiveness is further validated in simulations using real- world data.", "AI": {"tldr": "提出了一种名为 RA-Nav 的风险感知导航框架，该框架利用语义分割实时识别和分类障碍物（静止、暂时静止、动态），并根据障碍物类型进行风险预测，从而构建局部风险图。在此基础上，该框架通过风险导向的路径搜索算法和轨迹优化，实现了在静态障碍物突然移动时的安全、高效、平滑和动力学可行的路径规划，并在模拟实验中验证了其有效性。", "motivation": "现有航空机器人导航系统在静态障碍物突然移动时无法适应，而集成环境语义感知可以估计突然移动障碍物带来的潜在风险。", "method": "1. 使用轻量级多尺度语义分割网络实时识别障碍物类别。 2. 将障碍物分类为静止、暂时静止和动态。 3. 为每种障碍物类型设计相应的风险估计函数，进行实时风险预测。 4. 构建完整的局部风险图。 5. 基于风险图设计风险导向的路径搜索算法，平衡路径效率和安全性。 6. 应用轨迹优化生成安全、平滑且动力学可行的轨迹。", "result": "RA-Nav 在静态障碍物状态突然转换的场景下，比基线方法取得了更高的成功率。在真实世界数据模拟中也验证了其有效性。", "conclusion": "RA-Nav 框架能够有效应对静态障碍物突然移动的情况，通过语义感知和风险预测，实现更安全的路径规划。"}}
{"id": "2602.17287", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17287", "abs": "https://arxiv.org/abs/2602.17287", "authors": ["Evgeniia Tokarchuk", "Maya K. Nachesa", "Sergey Troshin", "Vlad Niculae"], "title": "Representation Collapse in Machine Translation Through the Lens of Angular Dispersion", "comment": null, "summary": "Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.", "AI": {"tldr": "本研究分析了 Transformer 架构在神经机器翻译（NMT）中的表征坍塌问题，特别是在离散和连续输出的模型中。通过引入基于角度离散的正则化方法，研究表明该方法能有效缓解表征坍塌，并提升翻译质量，且在模型量化后仍能保持效果。", "motivation": "现有神经机器翻译模型（基于 Transformer）在处理高资源数据集时表现优异，但其标准训练策略（下一个词预测）可能导致表征坍塌，尤其是在更深的 Transformer 层，这阻碍了模型对几何空间的有效利用。这种现象在连续输出的端到端 NMT 中更为显著，可能导致模型输出所有向量都相同这一平凡解。", "method": "研究通过分析离散和连续 NMT Transformer 在训练过程中不同层级的表征坍塌动态。在训练中引入了基于角度离散的正则化方法，并通过实验验证其效果。", "result": "经验性地证明，所提出的基于角度离散的正则化方法不仅能缓解表征坍塌，还能提高翻译质量。此外，研究还发现量化模型表现出类似的坍塌行为，并且正则化的益处在量化后依然得以保留。", "conclusion": "表征坍塌是 NMT 模型中的一个普遍问题，尤其在深层和连续输出模型中。通过引入角度离散正则化，可以有效地减轻这一问题，并提升模型性能，该方法在模型量化后依然有效。"}}
{"id": "2602.17200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17200", "abs": "https://arxiv.org/abs/2602.17200", "authors": ["Ye Zhu", "Kaleb S. Newman", "Johannes F. Lutzeyer", "Adriana Romero-Soriano", "Michal Drozdzal", "Olga Russakovsky"], "title": "GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation", "comment": "Preprint. Code will be available at https://github.com/L-YeZhu/GASS_T2I", "summary": "Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.", "AI": {"tldr": "本文提出了一种名为GASS（Geometry-Aware Spherical Sampling）的新方法，通过从几何角度解决文本到图像生成模型在生成多样性方面的不足，通过分解文本嵌入的语义变化和非语义变化，并分别控制它们来提高生成图像的多样性，同时保持图像质量和语义准确性。", "motivation": "现有的文本到图像生成模型在合成多样化图像方面存在不足，这限制了用户选择并可能加剧社会偏见。因此，需要一种方法来提高生成图像的多样性。", "method": "本文提出了一种名为GASS（Geometry-Aware Spherical Sampling）的方法。GASS通过几何学视角来增强文本到图像生成的多样性。它将CLIP嵌入分解为两个正交方向：一个与提示语义相关的方向，以及一个捕获与提示无关的变异（如背景）的正交方向。然后，GASS通过扩大这两个轴上的几何投影散布来增加生成图像嵌入的多样性，并通过在生成轨迹上进行扩展预测来指导采样过程。", "result": "在不同的冻结文本到图像骨干（U-Net和DiT，扩散和流模型）以及基准测试上的实验表明，GASS能够有效解耦地增强多样性，同时对图像保真度和语义对齐的影响最小。", "conclusion": "GASS是一种有效的方法，可以从几何角度增强文本到图像生成的多样性，通过控制提示相关和不相关的变异来源，同时保持图像的质量和语义准确性。"}}
{"id": "2602.17316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17316", "abs": "https://arxiv.org/abs/2602.17316", "authors": ["Bogdan Kostić", "Conor Fallon", "Julian Risch", "Alexander Löser"], "title": "Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation", "comment": "Accepted at LREC 2026", "summary": "The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.", "AI": {"tldr": "本研究发现，即使是词汇和句法上的微小、保持真值等价的改动，也会显著影响大型语言模型（LLMs）在标准基准测试中的表现和排名，表明LLMs更依赖表面词汇模式而非抽象语言能力。", "motivation": "当前LLM评估依赖标准化基准，但这些基准对输入提示的细微变化非常敏感，其可靠性受到质疑。", "method": "研究人员使用了两种语言学方法（同义词替换和基于依赖解析的句法转换）生成保持意义的词汇和句法扰动，并测试了23个LLM在MMLU、SQuAD和AMEGA三个基准上的性能和排名变化。", "result": "词汇扰动普遍导致模型性能显著下降，而句法扰动影响则更不一致，有时甚至能提升性能。两种扰动都破坏了复杂任务上的模型排名。模型鲁棒性并未随着模型规模的增大而稳定提升，且具有很强的任务依赖性。", "conclusion": "LLMs更依赖于表面词汇模式而非抽象语言能力，因此在LLM评估中需要将鲁棒性测试作为标准组成部分。"}}
{"id": "2602.17537", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17537", "abs": "https://arxiv.org/abs/2602.17537", "authors": ["Qilong Cheng", "Matthew Mackay", "Ali Bereyhi"], "title": "IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control", "comment": null, "summary": "Robotic camera systems enable dynamic, repeatable motion beyond human capabilities, yet their adoption remains limited by the high cost and operational complexity of industrial-grade platforms. We present the Intelligent Robotic Imaging System (IRIS), a task-specific 6-DOF manipulator designed for autonomous, learning-driven cinematic motion control. IRIS integrates a lightweight, fully 3D-printed hardware design with a goal-conditioned visuomotor imitation learning framework based on Action Chunking with Transformers (ACT). The system learns object-aware and perceptually smooth camera trajectories directly from human demonstrations, eliminating the need for explicit geometric programming. The complete platform costs under $1,000 USD, supports a 1.5 kg payload, and achieves approximately 1 mm repeatability. Real-world experiments demonstrate accurate trajectory tracking, reliable autonomous execution, and generalization across diverse cinematic motions.", "AI": {"tldr": "提出了一种名为IRIS的低成本、6自由度机器人摄像系统，该系统利用基于Transformer的模仿学习框架，可以通过人类演示自主学习和执行电影摄影运动。", "motivation": "工业级机器人摄像系统成本高且操作复杂，限制了其广泛应用。研究旨在开发一种更易于获得且易于使用的替代方案。", "method": "设计并制造了一个成本低于1000美元、可3D打印的6自由度机器人手臂（IRIS）。采用基于Action Chunking with Transformers（ACT）的基于目标的视觉运动模仿学习框架，从人类演示中直接学习目标感和感知平滑的摄像机轨迹。", "result": "IRIS系统成本低廉，负载能力达1.5公斤，重复精度约为1毫米。实验证明，该系统能够准确跟踪轨迹，可靠地自主执行，并能泛化到各种不同的电影摄影运动。", "conclusion": "IRIS系统通过结合低成本硬件设计和先进的模仿学习技术，成功地实现了自主、学习驱动的电影摄影运动控制，克服了现有系统的局限性，并展示了其在实际应用中的有效性和泛化能力。"}}
{"id": "2602.16984", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16984", "abs": "https://arxiv.org/abs/2602.16984", "authors": ["Vishal Srivastava"], "title": "Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning", "comment": null, "summary": "Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.", "AI": {"tldr": "本研究提出了“潜在上下文条件策略”模型，其行为在测试和部署环境中可能存在差异，并证明了黑盒评估的局限性。研究结果表明，在特定条件下，黑盒评估无法准确预测模型的部署风险，并提出了需要额外安全措施的情况。", "motivation": "黑盒安全评估方法依赖于测试分布下的模型行为能可靠预测部署性能的假设。然而，当模型行为依赖于未观察到的内部变量，这些变量在评估时罕见但在部署时普遍时，该假设可能不成立。研究旨在正式化并挑战这一假设，并量化黑盒评估的局限性。", "method": "研究采用了理论分析方法，包括：1. 被动评估：利用Le Cam方法证明了在i.i.d.采样下，评估器面临的最小误差下界。2. 自适应评估：通过哈希触发器构造和Yao的minimax原理，分析了自适应查询下的误差界限。3. 计算分离：在单向函数假设下，分析了拥有特权信息的部署环境如何激活黑盒评估器无法区分的不安全行为。研究还探讨了白盒探测下的样本复杂度和偏差校正。", "result": "研究证明，对于潜在上下文条件策略模型，黑盒评估器无法可靠地估计部署风险。在被动评估中，最小平均绝对误差至少为0.208 * delta * L。在自适应评估中，即使完全自适应查询，误差也可能至少为delta * L / 16。在计算分离场景下，白盒探测估计部署风险到精度epsilon_R需要O(1/(gamma^2 * epsilon_R^2))的样本量。", "conclusion": "研究结果量化了黑盒测试在何种情况下统计上不足以确定模型的部署安全性，并提供了明确的标准，说明何时需要架构约束、训练时保证、可解释性以及部署监控等额外安全措施来确保最坏情况下的安全性。"}}
{"id": "2602.17231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17231", "abs": "https://arxiv.org/abs/2602.17231", "authors": ["Yiming Xu", "Yi Yang", "Hao Cheng", "Monika Sester"], "title": "HiMAP: History-aware Map-occupancy Prediction with Fallback", "comment": "Accepted in 2026 IEEE International Conference on Robotics and Automation", "summary": "Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \\textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\\% in FDE, 12\\% in ADE, and a 4\\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.", "AI": {"tldr": "HiMAP是一个无需目标跟踪（MOT）即可进行高精度运动预测的框架，即使在MOT失败的情况下也能保持可靠性，并且在Argoverse 2数据集上取得了与跟踪方法相当的性能，在无跟踪场景下显著优于现有方法。", "motivation": "现有运动预测方法高度依赖多目标跟踪（MOT）的身份关联，当MOT失败（如遮挡、身份切换、漏检）时，预测质量下降，增加安全风险。因此，需要一个在MOT失败时仍能可靠运行的预测框架。", "method": "HiMAP将过去的检测点转换为时空不变的历史占用图（historical occupancy maps）。它引入了一个历史查询模块，该模块基于当前Agent的状态，迭代地从未标记的占用图中检索Agent特定的历史信息。检索到的历史信息被总结为一个时间图嵌入（temporal map embedding），并与最终查询和地图上下文一起，驱动一个DETR风格的解码器生成多模态未来轨迹。", "result": "在Argoverse 2数据集上，HiMAP在不依赖ID的情况下，取得了与跟踪方法相当的性能。在无跟踪设置下，HiMAP显著优于现有方法，在FDE（末端距离误差）方面提高了11%，在ADE（平均距离误差）方面提高了12%，在MR（Miss Rate）方面降低了4%。此外，HiMAP能够同时为所有Agent提供稳定的预测，无需等待跟踪恢复。", "conclusion": "HiMAP是一个无需身份关联、支持流式推理、并且在MOT失败时能提供鲁棒预测的运动预测框架，具有重要的实际应用价值，特别是在安全关键的自动驾驶领域。"}}
{"id": "2602.16990", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2602.16990", "abs": "https://arxiv.org/abs/2602.16990", "authors": ["Yan Wang", "Yi Han", "Lingfei Qian", "Yueru He", "Xueqing Peng", "Dongji Feng", "Zhuohan Xie", "Vincent Jim Zhang", "Rosie Guo", "Fengran Mo", "Jimin Huang", "Yankai Chen", "Xue Liu", "Jian-Yun Nie"], "title": "Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation", "comment": null, "summary": "Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.", "AI": {"tldr": "本文提出了一个名为Conv-FinRe的对话式、纵向股票推荐基准，用于评估大型语言模型（LLM）在金融咨询中的决策质量，而不仅仅是模仿用户行为。该基准通过区分描述性行为和基于用户风险偏好的规范性效用，来衡量LLM是否进行理性分析、模仿用户噪音还是受市场动量驱动。研究发现，模型在决策质量和行为一致性之间存在持续的权衡。", "motivation": "现有推荐系统基准主要关注模仿用户行为，但在金融咨询场景下，用户的短期行为可能受市场波动影响且与长期目标不符。因此，仅仅模仿用户行为可能无法代表决策质量。需要一个能够评估LLM在金融咨询中做出高质量决策的能力的基准。", "method": "构建了一个名为Conv-FinRe的对话式、纵向基准。该基准包含用户访谈、逐步更新的市场背景和咨询对话。模型需要根据这些信息在固定的投资期限内生成股票排名。Conv-FinRe提供多视角参考，区分描述性行为和基于投资者风险偏好的规范性效用，以诊断LLM的行为模式。", "result": "在Conv-FinRe基准上评估了一系列最先进的LLM。结果表明，在决策质量（效用）上表现良好的模型往往在匹配用户选择上表现不佳，而行为上一致的模型可能会过度拟合短期噪音。这揭示了理性决策质量与行为一致性之间存在的持续的权衡关系。", "conclusion": "Conv-FinRe提供了一个评估LLM在金融咨询中决策质量的有效基准，并揭示了在模仿用户行为和做出理性决策之间存在固有的挑战。该基准和相关代码已公开。"}}
{"id": "2602.17366", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17366", "abs": "https://arxiv.org/abs/2602.17366", "authors": ["Yiming Zhang", "Siyue Zhang", "Junbo Zhao", "Chen Zhao"], "title": "RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering", "comment": null, "summary": "Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.", "AI": {"tldr": "本文提出了一种名为RPDR的数据增强框架，通过生成和选择高质量的易学训练数据来提升密集检索器在长尾问答任务上的表现，并在实验中证明了其有效性，同时探索了动态路由机制以进一步优化检索性能。", "motivation": "长尾问答任务对大型语言模型（LLMs）提出了挑战，因为模型在获取和回忆罕见知识方面存在困难。现有的检索增强生成（RAG）系统虽然有所帮助，但其密集检索模型在处理稀有或小众知识时泛化能力不足。", "method": "RPDR框架包含三个核心部分：1. 合成数据生成；2. 利用“往返预测”（Round-Trip prediction）进行数据选择，以识别易学实例；3. 使用这些选出的实例来训练检索器。", "result": "RPDR在PopQA和EntityQuestion两个长尾检索基准上进行了评估，结果显示相较于BM25和Contriver等现有检索器，RPDR在长尾类别上表现出显著的性能提升，尤其是在极长尾类别上。", "conclusion": "RPDR是一种有效的数据增强框架，能够显著提升密集检索器在长尾问答任务上的性能。研究还通过人工分析确定了RPDR的优缺点，并提出了一种动态路由机制，有望进一步提高检索性能。"}}
{"id": "2602.16958", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16958", "abs": "https://arxiv.org/abs/2602.16958", "authors": ["Xinhao Deng", "Jiaqing Wu", "Miao Chen", "Yue Xiao", "Ke Xu", "Qi Li"], "title": "Automating Agent Hijacking via Structural Template Injection", "comment": null, "summary": "Agent hijacking, highlighted by OWASP as a critical threat to the Large Language Model (LLM) ecosystem, enables adversaries to manipulate execution by injecting malicious instructions into retrieved content. Most existing attacks rely on manually crafted, semantics-driven prompt manipulation, which often yields low attack success rates and limited transferability to closed-source commercial models. In this paper, we propose Phantom, an automated agent hijacking framework built upon Structured Template Injection that targets the fundamental architectural mechanisms of LLM agents. Our key insight is that agents rely on specific chat template tokens to separate system, user, assistant, and tool instructions. By injecting optimized structured templates into the retrieved context, we induce role confusion and cause the agent to misinterpret the injected content as legitimate user instructions or prior tool outputs. To enhance attack transferability against black-box agents, Phantom introduces a novel attack template search framework. We first perform multi-level template augmentation to increase structural diversity and then train a Template Autoencoder (TAE) to embed discrete templates into a continuous, searchable latent space. Subsequently, we apply Bayesian optimization to efficiently identify optimal adversarial vectors that are decoded into high-potency structured templates. Extensive experiments on Qwen, GPT, and Gemini demonstrate that our framework significantly outperforms existing baselines in both Attack Success Rate (ASR) and query efficiency. Moreover, we identified over 70 vulnerabilities in real-world commercial products that have been confirmed by vendors, underscoring the practical severity of structured template-based hijacking and providing an empirical foundation for securing next-generation agentic systems.", "AI": {"tldr": "本文提出了一种名为Phantom的自动化代理劫持框架，通过注入结构化模板来操纵LLM代理的行为，并包含一个用于提高攻击跨平台性的模板搜索机制。实验证明其效果优于现有方法，并在实际商业产品中发现了大量漏洞。", "motivation": "现有LLM代理劫持攻击依赖手动提示，成功率低且难以迁移至闭源模型，OWASP将其列为关键威胁，因此需要更有效的自动化攻击方法。", "method": "Phantom框架基于结构化模板注入，利用LLM代理中的聊天模板分隔符（如系统、用户、助手、工具指令）来诱导角色混淆。它还包含一个模板搜索框架，通过多级模板增强、模板自编码器（TAE）和贝叶斯优化来搜索最优的恶意结构化模板，以提高对黑盒代理的攻击成功率。", "result": "Phantom在Qwen、GPT和Gemini模型上进行了广泛实验，显著提高了攻击成功率（ASR）和查询效率。研究发现了超过70个已由供应商确认的实际商业产品漏洞。", "conclusion": "结构化模板注入是一种有效的LLM代理劫持方法，Phantom框架展示了其自动化和高跨平台性，并强调了应对此类安全威胁的紧迫性，为保护下一代代理系统提供了实证基础。"}}
{"id": "2602.17250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17250", "abs": "https://arxiv.org/abs/2602.17250", "authors": ["Alireza Hamoudzadeh", "Valeria Belloni", "Roberta Ravanelli"], "title": "Inferring Height from Earth Embeddings: First insights using Google AlphaEarth", "comment": "29 pages, 9 figures", "summary": "This study investigates whether the geospatial and multimodal features encoded in \\textit{Earth Embeddings} can effectively guide deep learning (DL) regression models for regional surface height mapping. In particular, we focused on AlphaEarth Embeddings at 10 m spatial resolution and evaluated their capability to support terrain height inference using a high-quality Digital Surface Model (DSM) as reference. U-Net and U-Net++ architectures were thus employed as lightweight convolutional decoders to assess how well the geospatial information distilled in the embeddings can be translated into accurate surface height estimates. Both architectures achieved strong training performance (both with $R^2 = 0.97$), confirming that the embeddings encode informative and decodable height-related signals. On the test set, performance decreased due to distribution shifts in height frequency between training and testing areas. Nevertheless, U-Net++ shows better generalization ($R^2 = 0.84$, median difference = -2.62 m) compared with the standard U-Net ($R^2 = 0.78$, median difference = -7.22 m), suggesting enhanced robustness to distribution mismatch. While the testing RMSE (approximately 16 m for U-Net++) and residual bias highlight remaining challenges in generalization, strong correlations indicate that the embeddings capture transferable topographic patterns. Overall, the results demonstrate the promising potential of AlphaEarth Embeddings to guide DL-based height mapping workflows, particularly when combined with spatially aware convolutional architectures, while emphasizing the need to address bias for improved regional transferability.", "AI": {"tldr": "研究表明，AlphaEarth 嵌入在指导深度学习模型进行区域地表高度映射方面具有潜力，但需要解决分布偏移导致的泛化能力下降问题。", "motivation": "研究人员希望探索地球嵌入（特别是 AlphaEarth 嵌入）在多大程度上能够有效地指导深度学习模型进行区域地表高度映射，并评估其支持地形高度推断的能力。", "method": "研究采用了 U-Net 和 U-Net++ 两种轻量级卷积解码器架构，并结合 AlphaEarth 嵌入（10 米分辨率）来推断地表高度，使用高质量的数字表面模型（DSM）作为参考。", "result": "两种架构在训练集上均取得了优异的性能（$R^2 = 0.97$），表明嵌入编码了可解码的高度相关信号。在测试集上，由于训练和测试区域的高度频率分布偏移，性能有所下降。U-Net++ 的泛化能力优于 U-Net（$R^2 = 0.84$ vs $R^2 = 0.78$），显示出对分布不匹配的更好鲁棒性。尽管测试 RMSE（U-Net++ 约为 16 米）和残差偏差表明泛化方面仍存在挑战，但强的相关性表明嵌入捕获了可迁移的地形模式。", "conclusion": "AlphaEarth 嵌入在指导深度学习驱动的高度映射工作流方面显示出巨大的潜力，尤其是在与空间感知的卷积架构结合时。然而，为了提高区域迁移能力，还需要解决偏差问题。"}}
{"id": "2602.17001", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.17001", "abs": "https://arxiv.org/abs/2602.17001", "authors": ["Zhao Tan", "Yiji Zhao", "Shiyu Wang", "Chang Xu", "Yuxuan Liang", "Xiping Liu", "Shirui Pan", "Ming Jin"], "title": "Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases", "comment": null, "summary": "Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.", "AI": {"tldr": "提出了一种名为Sonar-TS的神经符号框架，用于解决时间序列数据库中的自然语言查询问题，该框架通过“搜索-验证”管道，并引入了第一个大规模的时间序列数据库自然语言查询基准NLQTSBench。", "motivation": "现有的Text-to-SQL方法无法处理时间序列数据中的连续形态意图（如形状或异常），而时间序列模型难以处理超长历史数据。自然语言查询时间序列数据库（NLQ4TSDB）旨在帮助非专业用户从海量时间记录中检索有意义的事件、区间和摘要。", "method": "提出了一种名为Sonar-TS的神经符号框架，采用“搜索-验证”管道。该框架首先利用特征索引通过SQL查询候选时间窗口，然后生成Python程序来锁定并根据原始信号验证这些候选。", "result": "实验表明，Sonar-TS能有效处理传统方法无法应对的复杂时间序列查询。NLQTSBench是首个用于NLQ时间序列数据库大规模历史记录的基准，突出了该领域的独特挑战。", "conclusion": "Sonar-TS是首个系统研究NLQ4TSDB问题的框架，提供了一个通用的解决方案和评估标准，以促进未来在该领域的研究。"}}
{"id": "2602.17015", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2602.17015", "abs": "https://arxiv.org/abs/2602.17015", "authors": ["Saurav Pal"], "title": "Cinder: A fast and fair matchmaking system", "comment": null, "summary": "A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the \"non-outlier\" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a \"Sanction Score.\" We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.", "AI": {"tldr": "本文提出了一种名为 Cinder 的两阶段匹配系统，用于解决多人在线游戏中预组队伍（lobby）技能水平不均导致的匹配不公问题。该系统首先使用 Ruzicka 相似度指数进行初步筛选，然后通过非线性技能分桶和 Kantorovich 距离量化匹配公平性。", "motivation": "现有的基于平均技能指标的匹配系统在处理技能分布广泛或倾斜的队伍时，往往会导致比赛不平衡。因此，需要一种更公平、更快速的匹配系统来提升玩家满意度和留存率。", "method": "Cinder 系统分为两个阶段：1. 初步筛选：使用 Ruzicka 相似度指数比较队伍中“非异常值”的技能范围。2. 精确公平性评估：将玩家等级映射到由反向正态分布生成的非线性技能桶，然后使用 Kantorovich 距离量化匹配公平性，得到“Sanction Score”。", "result": "通过对 1.4 亿次模拟的 lobby 配对分析，研究展示了 Cinder 系统的可行性，并为设定公平匹配阈值提供了数据支持。", "conclusion": "Cinder 系统通过其两阶段的匹配机制，能够更有效地解决预组队伍技能不均导致的匹配不公问题，实现快速和公平的匹配。"}}
{"id": "2602.17260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17260", "abs": "https://arxiv.org/abs/2602.17260", "authors": ["Hung Mai", "Loi Dinh", "Duc Hai Nguyen", "Dat Do", "Luong Doan", "Khanh Nguyen Quoc", "Huan Vu", "Phong Ho", "Naeem Ul Islam", "Tuan Do"], "title": "EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection", "comment": "First preprint", "summary": "Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.", "AI": {"tldr": "提出了一种名为EA-Swin的新型AI生成视频检测模型，该模型直接在预训练视频嵌入上进行时空依赖建模，并构建了一个包含130K视频的大型数据集EA-Video。EA-Swin在检测性能上显著优于现有方法，并且对未见过的数据分布具有良好的泛化能力。", "motivation": "现有的AI生成视频检测方法在面对Sora2、Veo等先进生成器生成的逼真视频时表现不足，原因在于它们依赖于浅层嵌入轨迹、基于图像的适应或计算量大的MLLMs。", "method": "提出EA-Swin模型，该模型采用因子化窗口注意力设计，直接在预训练视频嵌入上建模时空依赖，并与通用ViT风格的编码器兼容。同时，构建了EA-Video数据集，包含130K个视频，整合了新收集的样本和现有数据集，涵盖多种生成器，并包含未见过生成器的测试集。", "result": "EA-Swin在主要生成器上的准确率达到0.97-0.99，相比现有最优方法（0.8-0.9）提升了5-20%。同时，模型在未见过的数据分布上表现出强大的泛化能力。", "conclusion": "EA-Swin提供了一种可扩展且鲁棒的AI生成视频检测解决方案，能够有效应对现代AI生成视频的挑战，并在性能和泛化能力上均超越了现有SOTA方法。"}}
{"id": "2602.17377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17377", "abs": "https://arxiv.org/abs/2602.17377", "authors": ["Leonidas Zotos", "Hedderik van Rijn", "Malvina Nissim"], "title": "The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour", "comment": "15 pages, 4 figures", "summary": "When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.", "AI": {"tldr": "本研究提出了一种基于大语料库概念流行度来评估多项选择题（MCQ）选项认知可得性的计算方法，发现正确答案比错误选项具有更高的可得性，这种策略可以显著提高答题分数，并且 LLM 生成的选项也表现出相似的可得性模式。", "motivation": "研究动机在于探究当学生不确定答案时，基于“可得性启发法”（availability heuristic）的猜测策略是否有效，并开发一种计算方法来量化 MCQ 选项的可得性，以模拟学生行为。", "method": "提出了一种计算方法，通过评估概念在大型文本语料库（如 Wikipedia）中的流行度来量化 MCQ 选项的认知可得性。在三个大型题集中，将该方法应用于评估总选择最易获得选项的策略效果，并比较了专家创建选项和 LLM 生成选项的可得性模式。", "result": "研究发现，正确答案比错误选项具有显著更高的可得性。采用“总是选择最易获得选项”的策略，在三个题集中的得分分别比随机猜测基线高出 13.5% 至 32.9%。此外，LLM 生成的 MCQ 选项与专家创建的选项在可得性模式上表现相似。", "conclusion": "研究结论是，认知可得性是影响 MCQ 答题策略的一个重要因素，尤其是在学生不确定答案的情况下。该研究提出的计算方法可以有效量化可得性，并为模拟学生答题行为提供依据。LLM 生成选项的可得性模式与人类专家相似，提示了可得性在当前和未来计算模型中的重要性。"}}
{"id": "2602.17424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17424", "abs": "https://arxiv.org/abs/2602.17424", "authors": ["Anastasia Zhukova", "Felix Hamborg", "Karsten Donnay", "Norman Meuschke", "Bela Gipp"], "title": "Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference", "comment": null, "summary": "Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking \"the caravan\" - \"asylum seekers\" - \"those contemplating illegal entry\", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.", "AI": {"tldr": "该研究提出了一种改进的跨文档共指消解（CDCR）标注方案，将共指链视为话语元素（DEs）和概念分析单元，以更好地处理新闻报道中词汇和框架的多样性，并重新标注了NewsWCL50和ECB+的部分数据进行评估。", "motivation": "现有的CDCR数据集主要关注事件消解，且对共指的定义较为狭窄，这限制了其在分析多样化和观点极化的新闻报道方面的有效性，因为新闻报道中的措辞差异很大。", "method": "提出了一种修改后的CDCR标注方案，将共指链视为话语元素（DEs）和概念分析单元。该方案同时考虑了同一性和近乎同一性的关系，以捕捉词汇多样性和框架变化。使用统一的代码本重新标注了NewsWCL50数据集和ECB+数据集的一个子集，并通过词汇多样性指标和同词头基线进行了评估。", "result": "重新标注后的数据集在统计上与原始ECB+和NewsWCL50数据集的结果处于中间水平，显示出较高的一致性。这表明该标注方案能够有效地平衡和支持面向话语的CDCR研究。", "conclusion": "研究提出的新的CDCR标注方案能够更好地处理新闻报道中的词汇多样性和框架变化，支持更均衡和面向话语的CDCR研究。重新标注的数据集也为未来研究提供了资源。"}}
{"id": "2602.17601", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17601", "abs": "https://arxiv.org/abs/2602.17601", "authors": ["Patrick Benito Eberhard", "Luis Pabon", "Daniele Gammelli", "Hugo Buurmeijer", "Amon Lahr", "Mark Leone", "Andrea Carron", "Marco Pavone"], "title": "Graph Neural Model Predictive Control for High-Dimensional Systems", "comment": null, "summary": "The control of high-dimensional systems, such as soft robots, requires models that faithfully capture complex dynamics while remaining computationally tractable. This work presents a framework that integrates Graph Neural Network (GNN)-based dynamics models with structure-exploiting Model Predictive Control to enable real-time control of high-dimensional systems. By representing the system as a graph with localized interactions, the GNN preserves sparsity, while a tailored condensing algorithm eliminates state variables from the control problem, ensuring efficient computation. The complexity of our condensing algorithm scales linearly with the number of system nodes, and leverages Graphics Processing Unit (GPU) parallelization to achieve real-time performance. The proposed approach is validated in simulation and experimentally on a physical soft robotic trunk. Results show that our method scales to systems with up to 1,000 nodes at 100 Hz in closed-loop, and demonstrates real-time reference tracking on hardware with sub-centimeter accuracy, outperforming baselines by 63.6%. Finally, we show the capability of our method to achieve effective full-body obstacle avoidance.", "AI": {"tldr": "提出了一种结合图神经网络（GNN）动力学模型和利用结构的模型预测控制（MPC）的框架，用于实时控制高维系统，并在软体机器人实验中验证了其有效性。", "motivation": "控制高维系统（如软机器人）需要能够忠实捕捉复杂动力学且计算上可行的模型。", "method": "使用基于GNN的动力学模型表示系统，并采用定制的凝缩算法来简化MPC问题，利用GPU并行化以实现高效计算。", "result": "该方法在多达1000个节点的情况下，以100 Hz的闭环频率实现了实时控制，在物理硬件上实现了亚厘米级精度，性能优于基线63.6%，并成功实现了全身避障。", "conclusion": "所提出的框架能够实时控制高维系统，并在软体机器人上取得了优异的性能和准确性。"}}
{"id": "2602.17586", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17586", "abs": "https://arxiv.org/abs/2602.17586", "authors": ["Antonio Guillen-Perez"], "title": "Conditional Flow Matching for Continuous Anomaly Detection in Autonomous Driving on a Manifold-Aware Spectral Space", "comment": null, "summary": "Safety validation for Level 4 autonomous vehicles (AVs) is currently bottlenecked by the inability to scale the detection of rare, high-risk long-tail scenarios using traditional rule-based heuristics. We present Deep-Flow, an unsupervised framework for safety-critical anomaly detection that utilizes Optimal Transport Conditional Flow Matching (OT-CFM) to characterize the continuous probability density of expert human driving behavior. Unlike standard generative approaches that operate in unstable, high-dimensional coordinate spaces, Deep-Flow constrains the generative process to a low-rank spectral manifold via a Principal Component Analysis (PCA) bottleneck. This ensures kinematic smoothness by design and enables the computation of the exact Jacobian trace for numerically stable, deterministic log-likelihood estimation. To resolve multi-modal ambiguity at complex junctions, we utilize an Early Fusion Transformer encoder with lane-aware goal conditioning, featuring a direct skip-connection to the flow head to maintain intent-integrity throughout the network. We introduce a kinematic complexity weighting scheme that prioritizes high-energy maneuvers (quantified via path tortuosity and jerk) during the simulation-free training process. Evaluated on the Waymo Open Motion Dataset (WOMD), our framework achieves an AUC-ROC of 0.766 against a heuristic golden set of safety-critical events. More significantly, our analysis reveals a fundamental distinction between kinematic danger and semantic non-compliance. Deep-Flow identifies a critical predictability gap by surfacing out-of-distribution behaviors, such as lane-boundary violations and non-normative junction maneuvers, that traditional safety filters overlook. This work provides a mathematically rigorous foundation for defining statistical safety gates, enabling objective, data-driven validation for the safe deployment of autonomous fleets.", "AI": {"tldr": "该研究提出了一种名为Deep-Flow的无监督框架，利用最优传输条件流匹配（OT-CFM）来检测L4自动驾驶中的罕见高风险长尾场景。该框架通过PCA瓶颈约束生成过程到低秩谱流形，确保运动平滑性并实现稳定的对数似然估计。结合Transformer编码器和早期融合，解决了复杂路口的模态歧义问题，并引入了运动复杂度加权方案。在WOMD数据集上，Deep-Flow取得了0.766的AUC-ROC，并能区分运动危险和语义不合规行为，发现了传统方法忽略的分布外行为，为统计安全门提供了数学基础。", "motivation": "传统的基于规则的启发式方法无法有效检测L4自动驾驶中的罕见高风险长尾场景，导致安全验证成为瓶颈。研究旨在开发一种能够通过学习专家驾驶行为的概率密度来检测这些异常情况的框架。", "method": "该研究提出Deep-Flow框架，核心是Optimal Transport Conditional Flow Matching (OT-CFM)用于表征专家驾驶行为的连续概率密度。通过PCA瓶颈将生成过程约束到低秩谱流形，保证运动平滑性并实现稳定的对数似然估计。利用Early Fusion Transformer编码器和车道感知目标条件，并带有直接跳跃连接到流头，以解决复杂路口的模态歧义。引入了运动复杂度加权方案，优先考虑路径曲折度和加加速度等高能量机动。", "result": "在Waymo Open Motion Dataset (WOMD)上，Deep-Flow在检测安全关键事件方面取得了0.766的AUC-ROC。研究发现了运动危险和语义不合规之间的基本区别，并识别出传统安全过滤器忽略的分布外行为，如车道线违规和非规范路口机动。", "conclusion": "Deep-Flow提供了一个数学上严谨的基础，用于定义统计安全门，从而能够客观、数据驱动地验证自动驾驶车队的安全性。该方法能够检测出传统方法未能识别的关键预测性差距，为安全部署自动驾驶车队提供了新途径。"}}
{"id": "2602.17277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17277", "abs": "https://arxiv.org/abs/2602.17277", "authors": ["Ruoyi Zhang", "Jiawei Yuan", "Lujia Ye", "Runling Yu", "Liling Zhao"], "title": "Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution", "comment": "Under review", "summary": "High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.", "AI": {"tldr": "提出了一种名为 PESTGAN 的物理编码空间和时间生成对抗网络，用于热带气旋（TC）卫星图像的超分辨率重建，该网络通过整合物理约束和时空鉴别器来提高重建图像的物理一致性和视觉质量。", "motivation": "现有的基于深度学习的超分辨率方法通常将卫星图像序列视为通用视频，忽略了控制云运动的大气物理定律，这导致重建图像在物理真实性方面存在不足。", "method": "设计了一个解耦的生成器架构，其中包含一个 PhyCell 模块，通过约束卷积近似涡度方程，并将由此产生的近似物理动力学编码为隐式潜在表示，以分离物理动力学和视觉纹理。此外，引入了一个双鉴别器框架，包括一个时域鉴别器，以在保证空间真实性的同时，强制执行运动一致性。", "result": "在 Digital Typhoon 数据集上进行的 4 倍放大实验表明，PESTGAN 在结构保真度和感知质量方面取得了更好的性能。在保持与其他现有方法相当的像素级精度的情况下，该方法在重建气象学上合理的云结构和优越的物理保真度方面表现显著。", "conclusion": "PESTGAN 通过整合物理约束和时空鉴别器，能够生成物理上更合理、视觉上更逼真的热带气旋卫星图像超分辨率重建结果，克服了现有方法的局限性。"}}
{"id": "2602.17016", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17016", "abs": "https://arxiv.org/abs/2602.17016", "authors": ["Zichen Wang", "Wanli Ma", "Zhenyu Ming", "Gong Zhang", "Kun Yuan", "Zaiwen Wen"], "title": "M2F: Automated Formalization of Mathematical Literature at Scale", "comment": null, "summary": "Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\\%$ proof success (vs.\\ $80\\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.", "AI": {"tldr": "本文提出了M2F（Math-to-Formal）框架，实现了首个端到端的、项目规模的数学文献自动形式化，能处理跨文件依赖和编译问题，并成功将479页的实分析和凸分析教科书转化为153,853行的Lean库，证明了大规模自动形式化的可行性。", "motivation": "现有的数学形式化工具只能处理孤立的定理或短代码片段，难以扩展到教科书或研究论文级别的项目，主要瓶颈在于处理跨文件依赖、导入解析和项目端到端编译。研究者希望实现大规模的数学文献自动形式化。", "method": "M2F框架分两个阶段进行：1. 语句编译阶段：将文档拆分为原子块，通过推断依赖关系进行排序，并修复声明骨架，直到项目能够编译（允许证明中存在占位符）。2. 证明修复阶段：在固定签名下，利用目标条件下的局部编辑来填补这些证明的空白。在整个过程中，M2F让验证器参与，仅在工具链反馈确认改进时提交编辑。", "result": "在约三周时间内，M2F成功将479页的实分析和凸分析教科书转化为项目规模的Lean库，共计153,853行，全部形式化为带有证明的Lean声明。在FATE-H基准测试上，M2F实现了96%的证明成功率，优于80%的强基线。", "conclusion": "M2F框架证明了实际的、大规模的数学文献自动形式化是可行的，并且能够以远超人工的速度完成教科书规模的形式化工作。"}}
{"id": "2602.17049", "categories": ["cs.AI", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17049", "abs": "https://arxiv.org/abs/2602.17049", "authors": ["Seoyoung Lee", "Seobin Yoon", "Seongbeen Lee", "Yoojung Chun", "Dayoung Park", "Doyeon Kim", "Joo Yong Sim"], "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents", "comment": "12 pages, 9 figures, AAMAS 2026", "summary": "Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.", "AI": {"tldr": "本文提出了一种名为IntentCUA的多代理计算机使用框架，通过意图对齐的计划记忆来稳定长时序执行，克服了现有方法在用户意图漂移和重复求解方面的不足，并在端到端评估中取得了优于基线方法的性能。", "motivation": "现有计算机使用代理在长时序、多窗口、感知噪声和状态变化的环境下执行任务时，容易偏离用户意图并重复解决例行子问题，导致错误累积和效率低下。", "method": "IntentCUA是一个多代理框架，包含规划器、计划优化器和批评者，它们通过共享内存协同工作。该内存将原始交互痕迹抽象为多视角意图表示和可重用技能。运行时，意图原型检索与子组对齐的技能，并将其注入部分计划中，以减少冗余重新规划和缓解跨桌面应用的错误传播。", "result": "IntentCUA在端到端评估中实现了74.83%的任务成功率和0.91的步进效率比，优于基于强化学习和轨迹检索的基线方法。消融实验表明，多视角意图抽象和共享计划记忆共同提高了执行稳定性，而协作多代理循环在长时序任务上带来了最大的性能提升。", "conclusion": "系统级的意图抽象和基于记忆的协调是实现大型动态环境中可靠且高效的桌面自动化的关键。"}}
{"id": "2602.17310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17310", "abs": "https://arxiv.org/abs/2602.17310", "authors": ["Dennis N. Schneider", "Lars Wagner", "Daniel Rueckert", "Dirk Wilhelm"], "title": "Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery", "comment": null, "summary": "Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.", "AI": {"tldr": "提出了一种名为“附件锚点”的新型结构化表示方法，用于提高结直肠手术中自主抓取点的预测精度，该方法通过将局部几何和力学关系编码到一致的参考系中，并在数据集实验中证明了其有效性，尤其是在未见过的手术和外科医生等分布外设置下。", "motivation": "结直肠手术复杂且耗时，但因其重复性的组织操作，为开发由机器学习驱动的自主抓取点预测提供了有利条件，但现有研究对此类手术的关注不足。", "method": "引入“附件锚点”这一结构化表示，编码组织与其解剖附件之间的局部几何和力学关系，将手术场景归一化到一个一致的局部参考系，并使用机器学习框架从腹腔镜图像中预测和应用此表示。", "result": "实验在90例结直肠手术数据集上进行，结果表明“附件锚点”相比仅使用图像的基线方法，显著提高了抓取点预测的准确性，尤其在未见过的手术和外科医生等分布外设置下，性能提升尤为明显。", "conclusion": "“附件锚点”是一种有效的中间表示，能够提升基于学习的结直肠手术组织操纵能力，特别是在处理复杂和变化的场景时。"}}
{"id": "2602.17017", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17017", "abs": "https://arxiv.org/abs/2602.17017", "authors": ["Deepanjan Bhol"], "title": "Sales Research Agent and Sales Research Bench", "comment": "Technical report. 2 figures. Microsoft Dynamics 365 Sales", "summary": "Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.", "AI": {"tldr": "微软 Dynamics 365 Sales 推出了一款名为 Sales Research Agent 的 AI 应用，能够连接实时 CRM 数据并生成决策支持性见解。该系统引入了一个名为 Sales Research Bench 的基准测试，用于评估 AI 系统的质量，并在测试中表现优于 Claude Sonnet 4.5 和 ChatGPT-5。", "motivation": "当前企业需要 AI 系统能够回答销售领导关于实时、定制化 CRM 数据的问题，但现有模型缺乏透明、可重复的质量证据。", "method": "开发了 Sales Research Agent，一个连接实时 CRM 和相关数据的 AI 应用，并引入了 Sales Research Bench 基准测试，该基准从八个客户关注的维度（包括文本和图表的可信度、相关性、可解释性、模式准确性和图表质量）对系统进行评分。", "result": "在一次包含 200 个问题的测试中，Sales Research Agent 在 100 分制的综合评分上，比 Claude Sonnet 4.5 高出 13 分，比 ChatGPT-5 高出 24.1 分。", "conclusion": "Sales Research Agent 提供了一种可重复的方式来比较 AI 解决方案的质量，其在 Sales Research Bench 上的表现优于现有模型，能够为企业提供高质量的决策支持。"}}
{"id": "2602.17322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17322", "abs": "https://arxiv.org/abs/2602.17322", "authors": ["Mohamed Dhouib", "Davide Buscaldi", "Sonia Vanier", "Aymen Shabou"], "title": "Leveraging Contrastive Learning for a Similarity-Guided Tampered Document Data Generation Pipeline", "comment": null, "summary": "Detecting tampered text in document images is a challenging task due to data scarcity. To address this, previous work has attempted to generate tampered documents using rule-based methods. However, the resulting documents often suffer from limited variety and poor visual quality, typically leaving highly visible artifacts that are rarely observed in real-world manipulations. This undermines the model's ability to learn robust, generalizable features and results in poor performance on real-world data. Motivated by this discrepancy, we propose a novel method for generating high-quality tampered document images. We first train an auxiliary network to compare text crops, leveraging contrastive learning with a novel strategy for defining positive pairs and their corresponding negatives. We also train a second auxiliary network to evaluate whether a crop tightly encloses the intended characters, without cutting off parts of characters or including parts of adjacent ones. Using a carefully designed generation pipeline that leverages both networks, we introduce a framework capable of producing diverse, high-quality tampered document images. We assess the effectiveness of our data generation pipeline by training multiple models on datasets derived from the same source images, generated using our method and existing approaches, under identical training protocols. Evaluating these models on various open-source datasets shows that our pipeline yields consistent performance improvements across architectures and datasets.", "AI": {"tldr": "本文提出了一种生成高质量篡改文档图像的新方法，通过对比学习和字符边界评估来解决真实世界篡改数据稀缺的问题，生成的合成数据能显著提升篡改文本检测模型的性能。", "motivation": "现有生成篡改文档的方法生成的图像多样性不足、视觉质量差，存在明显的造假痕迹，导致模型无法学习到鲁棒的通用特征，在真实世界数据上的表现不佳。因此，需要一种能够生成高质量、多样化篡改文档图像的方法。", "method": "提出了一种包含两个辅助网络的生成框架：1. 一个利用对比学习的辅助网络，通过新颖的策略定义正负样本对，用于比较文本裁剪块；2. 一个评估文本裁剪块是否紧密包含目标字符的辅助网络。利用这两个网络设计了一个生成流程，以产生多样化、高质量的篡改文档图像。", "result": "通过在相同源图像上使用本文方法和现有方法生成数据集，并进行相同的训练协议，然后评估不同模型在多个开源数据集上的表现。结果表明，本文提出的数据生成流程在不同模型架构和数据集上均能带来一致的性能提升。", "conclusion": "本文提出的数据生成框架能够生成多样化、高质量的篡改文档图像，有效解决了真实世界数据稀缺的问题，并显著提升了篡改文本检测模型的性能。"}}
{"id": "2602.17431", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17431", "abs": "https://arxiv.org/abs/2602.17431", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan", "Viren Bajaj", "David Skarbrevik"], "title": "Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study", "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.", "AI": {"tldr": "本研究提出了一种用于长文本生成不确定性量化（UQ）的分类法，以改进大语言模型（LLM）的长篇输出幻觉检测。该方法将 UQ 方法按响应分解、单元级评分和响应级聚合三个阶段进行分类，并提出了一系列一致性相关的黑盒评分器。实验表明，基于声明-响应蕴含的评分效果最好，声明级评分优于句子级评分，并且不确定性感知解码能显著提高长文本输出的事实准确性。", "motivation": "现有针对 LLM 长篇输出的幻觉检测方法，特别是基于不确定性量化（UQ）的方法，主要集中在短文本，难以泛化到长篇生成。因此，需要一种能够对长篇输出进行细粒度 UQ 的方法。", "method": "研究者提出了一个 UQ 方法的分类法，该分类法将方法按三个阶段区分：响应分解（如何将长文本分割成更小的单元）、单元级评分（如何评估每个小单元的不确定性）和响应级聚合（如何将单元级的不确定性汇总到整体响应）。他们还形式化了几类基于一致性的黑盒评分器，并进行了实验评估。", "result": "实验结果显示：1）声明-响应蕴含（claim-response entailment）的评分方法效果持续优于或持平于更复杂的声明级评分方法；2）声明级评分通常比句子级评分效果更好；3）不确定性感知解码（uncertainty-aware decoding）能有效提高长篇输出的事实准确性。", "conclusion": "本研究提出的 UQ 分类法阐明了现有方法之间的关系，实现了可比性评估，并为选择用于细粒度 UQ 的组件提供了实用指导，尤其强调了声明-响应蕴含和不确定性感知解码在提升长篇输出事实准确性方面的有效性。"}}
{"id": "2602.17038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17038", "abs": "https://arxiv.org/abs/2602.17038", "authors": ["Shengtian Yang", "Yu Li", "Shuo He", "Yewen Li", "Qingpeng Cai", "Peng Jiang", "Lei Feng"], "title": "Phase-Aware Mixture of Experts for Agentic Reinforcement Learning", "comment": "16 pages", "summary": "Reinforcement learning (RL) has equipped LLM agents with a strong ability to solve complex tasks. However, existing RL methods normally use a \\emph{single} policy network, causing \\emph{simplicity bias} where simple tasks occupy most parameters and dominate gradient updates, leaving insufficient capacity for complex tasks. A plausible remedy could be employing the Mixture-of-Experts (MoE) architecture in the policy network, as MoE allows different parameters (experts) to specialize in different tasks, preventing simple tasks from dominating all parameters. However, a key limitation of traditional MoE is its token-level routing, where the router assigns each token to specialized experts, which fragments phase-consistent patterns into scattered expert assignments and thus undermines expert specialization. In this paper, we propose \\textbf{Phase-Aware Mixture of Experts (PA-MoE)}. It first features a lightweight \\emph{phase router} that learns latent phase boundaries directly from the RL objective without pre-defining phase categories. Then, the phase router allocates temporally consistent assignments to the same expert, allowing experts to preserve phase-specific expertise. Experimental results demonstrate the effectiveness of our proposed PA-MoE.", "AI": {"tldr": "提出了一种名为 PA-MoE 的新方法，通过引入“阶段路由器”来改进 LLM 代理的强化学习，以解决简单任务主导模型参数的“简单性偏差”问题，从而提高复杂任务的处理能力。", "motivation": "现有的强化学习方法使用单一策略网络，容易出现“简单性偏差”，即简单任务占用过多参数并主导梯度更新，导致复杂任务缺乏足够的模型容量。MoE 架构可以缓解这个问题，但传统的基于 token 的路由会破坏阶段一致性模式，损害专家专业化。", "method": "提出 PA-MoE，使用一个轻量级的“阶段路由器”直接从强化学习目标中学习潜在的阶段边界，而不是预定义阶段类别。该路由器为同一专家分配时间上一致的 assignment，从而允许专家保持特定阶段的专业知识。", "result": "实验结果表明，PA-MoE 在解决强化学习问题方面是有效的。", "conclusion": "PA-MoE 能够通过阶段感知的路由机制，有效解决 LLM 强化学习中的简单性偏差问题，提高模型处理复杂任务的能力。"}}
{"id": "2602.17573", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17573", "abs": "https://arxiv.org/abs/2602.17573", "authors": ["Konstantinos Foteinos", "Georgios Angelidis", "Aggelos Psiris", "Vasileios Argyriou", "Panagiotis Sarigiannidis", "Georgios Th. Papadopoulos"], "title": "FR-GESTURE: An RGBD Dataset For Gesture-based Human-Robot Interaction In First Responder Operations", "comment": null, "summary": "The ever increasing intensity and number of disasters make even more difficult the work of First Responders (FRs). Artificial intelligence and robotics solutions could facilitate their operations, compensating these difficulties. To this end, we propose a dataset for gesture-based UGV control by FRs, introducing a set of 12 commands, drawing inspiration from existing gestures used by FRs and tactical hand signals and refined after incorporating feedback from experienced FRs. Then we proceed with the data collection itself, resulting in 3312 RGBD pairs captured from 2 viewpoints and 7 distances. To the best of our knowledge, this is the first dataset especially intended for gesture-based UGV guidance by FRs. Finally we define evaluation protocols for our RGBD dataset, termed FR-GESTURE, and we perform baseline experiments, which are put forward for improvement. We have made data publicly available to promote future research on the domain: https://doi.org/10.5281/zenodo.18131333.", "AI": {"tldr": "本文介绍了一个名为FR-GESTURE的数据集，用于支持急救人员（FRs）通过手势控制无人地面载具（UGV），并提供了相关的数据采集、评估协议和基线实验。", "motivation": "日益严峻的灾难形势给急救人员的工作带来了巨大挑战，人工智能和机器人技术有望缓解这些困难，因此需要开发相应的手势控制UGV解决方案。", "method": "设计了12种基于FRs经验和战术手语的手势指令，并通过FRs反馈进行优化。然后，在2个视角和7个不同距离下采集了3312对RGBD图像数据，构建了FR-GESTURE数据集，并定义了评估协议和进行了基线实验。", "result": "成功构建了一个专为FRs手势控制UGV设计的RGBD数据集FR-GESTURE，该数据集包含12种指令和大量的RGBD图像对。基线实验为未来的研究提供了参考。", "conclusion": "FR-GESTURE数据集是首个专门用于FRs手势控制UGV的RGBD数据集，该数据集已公开，旨在推动相关领域的研究发展，并为未来改进提供基础。"}}
{"id": "2602.17659", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17659", "abs": "https://arxiv.org/abs/2602.17659", "authors": ["Yu Fang", "Yuchun Feng", "Dong Jing", "Jiaqi Liu", "Yue Yang", "Zhenyu Wei", "Daniel Szafir", "Mingyu Ding"], "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs", "comment": "Website: https://vla-va.github.io/", "summary": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.", "AI": {"tldr": "该研究提出了一个名为LIBERO-CF的基准测试，用于评估视觉-语言-动作（VLA）模型在面对视觉上相似但指令不同的情况下的表现。为了解决VLA模型存在的反事实失败问题（即模型依赖数据偏见，忽略指令意图），研究者提出了一种名为Counterfactual Action Guidance (CAG) 的新颖方法。CAG通过引入一个语言无关的视觉-动作（VA）模块，与标准VLA模型并行，从而在动作选择时进行反事实对比，减少模型对视觉捷径的依赖，提升在不常见任务上的鲁棒性。实验证明CAG是一种即插即用的方法，能够显著提升不同VLA模型的语言遵循准确性和任务成功率。", "motivation": "现有的视觉-语言-动作（VLA）模型在执行语言指令时常出现失败，尤其是在指令缺乏强烈的场景特定监督时。这些模型倾向于利用数据集的偏见，通过“视觉捷径”来执行，即使这与语言指令的意图不符，即所谓的“反事实失败”。作者希望系统地研究并解决这一问题。", "method": "1. 引入LIBERO-CF：一个用于评估VLA模型反事实失败情况的基准测试，通过在视觉上相似但指令不同的场景下进行评估。 2. 提出Counterfactual Action Guidance (CAG)：一种双分支推理方案，结合了标准的VLA策略和一个语言无关的视觉-动作（VA）模块。CAG通过让VLA模型与VA模块进行反事实对比，显式地正则化语言条件，减少对视觉捷径的依赖，提升在不常见任务上的性能。CAG是一种即插即用的方法，无需额外数据或修改现有模型。", "result": "1. LIBERO-CF基准测试揭示了反事实失败在当前先进VLA模型中普遍存在且未被充分研究。 2. CAG方法在LIBERO-CF基准测试上，仅使用无需训练的策略，就提高了无观察任务的语言遵循准确率9.7%和任务成功率3.6%。 3. 当与VA模型结合使用时，CAG进一步将这些指标提升了15.5%和8.5%。 4. 在真实世界评估中，CAG平均减少了9.4%的反事实失败，并提高了17.2%的任务成功率。", "conclusion": "反事实失败是VLA模型在遵循语言指令时面临的一个重要挑战。提出的CAG方法能够有效地缓解这一问题，通过引入反事实对比机制，减少模型对视觉捷径的依赖，显著提升模型在复杂和不常见任务上的语言遵循能力和任务成功率。CAG是一种通用且易于集成的解决方案，能够为不同VLA模型带来显著的性能提升。"}}
{"id": "2602.17443", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17443", "abs": "https://arxiv.org/abs/2602.17443", "authors": ["Adib Sakhawat", "Fardeen Sadab", "Rakin Shahriar"], "title": "AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue", "comment": "16 pages, 5 figures, 13 tables. Includes appendix and supplementary materials", "summary": "Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured \"20 Questions\" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.", "AI": {"tldr": "研究提出了一种名为AIDG（Adversarial Information Deduction Game）的游戏理论框架，用于评估大型语言模型（LLM）在动态多轮对话中的战略推理能力。该框架包含两个任务：AIDG-I（社会推理）和AIDG-II（约束满足）。实验发现LLMs在信息保持（防御）方面远优于信息提取（推理），存在明显的能力不对称性，这主要由信息动态和约束依从性差导致。", "motivation": "现有的静态基准测试无法充分评估大型语言模型（LLM）在动态、多轮交互中的战略推理能力。研究旨在开发一种能够探测LLM在信息提取（主动推理）和信息保持（状态维护）之间不对称性的游戏理论框架。", "method": "提出了AIDG（Adversarial Information Deduction Game）这一游戏理论框架，包含两个互补的任务：AIDG-I，用于衡量社交推理中的实用策略；AIDG-II，用于在结构化的“20个问题”环境中衡量约束满足能力。在439场与六种前沿LLM进行的博弈中进行了评估。", "result": "LLM在信息保持（防御）方面表现明显优于信息提取（推理），在防御方面具有350 ELO的优势。能力差距主要源于两个瓶颈：（1）信息动态，确认策略比盲目推理有效7.75倍；（2）约束依从性，对话负载会削弱指令遵循能力，导致41.3%的推理失败。", "conclusion": "研究表明，虽然LLM在局部防御一致性方面表现出色，但它们难以进行战略性探究所需的全局状态跟踪。LLM在信息保持方面比信息提取更具优势，这对其战略推理能力提出了挑战。"}}
{"id": "2602.17337", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17337", "abs": "https://arxiv.org/abs/2602.17337", "authors": ["Antoine Legouhy", "Cosimo Campo", "Ross Callaghan", "Hojjat Azadbakht", "Hui Zhang"], "title": "Polaffini: A feature-based approach for robust affine and polyaffine image registration", "comment": "associated github repo: https://github.com/CIG-UCL/polaffini", "summary": "In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.", "AI": {"tldr": "本文提出了一种名为 Polaffini 的解剖学驱动的医学图像配准框架，该框架利用深度学习分割模型提取的解剖特征点（如质心）来实现高效的全局和局部仿射匹配，并进一步生成平滑可调的 Polyaffine 变换，最终实现精确、鲁棒且快速的医学图像配准。", "motivation": "传统的基于特征的医学图像配准方法由于特征提取困难而受到限制。近年来，深度学习在提供可靠的细粒度解剖分割方面取得了重大进展，这为开发新的基于解剖学的配准算法提供了机会。", "method": "Polaffini 框架利用深度学习分割模型提取的解剖区域，通过计算其质心来获得具有一一对应关系的解剖特征点。利用这些特征点，通过封闭形式解实现高效的全局和局部仿射匹配，从而生成仿射到 Polyaffine 的整体变换。Polyaffine 变换嵌入在对数欧几里德框架中，以确保其微分同胚性质。", "result": "与流行的基于强度的配准技术相比，Polaffini 在结构对齐方面表现更优。它还为后续的非线性配准提供了更好的初始化。", "conclusion": "Polaffini 是一种快速、鲁棒且准确的医学图像配准框架，特别适合集成到医学图像处理流程中。它通过利用深度学习分割的最新进展，克服了传统特征匹配的挑战，并实现了优于现有方法的配准性能。"}}
{"id": "2602.17425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17425", "abs": "https://arxiv.org/abs/2602.17425", "authors": ["Sanjeev Kumar", "Preethi Jyothi", "Pushpak Bhattacharyya"], "title": "Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics", "comment": "6 pages", "summary": "Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.", "AI": {"tldr": "本研究比较了BLEU和ChrF++在极低资源语言（ELRL）机器翻译质量评估中的表现，发现在ELRL场景下，BLEU尽管分数较低，但能提供ChrF++所不具备的词汇精确度信息，有助于提高评估的可解释性。", "motivation": "在极低资源语言（ELRL）场景下，现有的机器翻译（MT）评估指标（如BLEU）往往失效。需要找到更适合ELRL场景的评估方法，并理解不同指标的局限性。", "method": "比较了BLEU（基于n-gram）和ChrF++（基于字符）在三种ELRL（Magahi、Bhojpuri、Chhattisgarhi）上的机器翻译评估效果，重点分析了它们对翻译产物中常见错误的敏感度，包括幻觉、重复、原文抄袭和附加符号（matra）变化，并考察了大型语言模型（LLMs）和神经机器翻译（NMT）系统的输出。", "result": "研究发现，虽然ChrF++在ELRL场景下通常得分更高，但BLEU在提供词汇精确度方面的洞察是互补的，有助于提高评估结果的可解释性。", "conclusion": "在ELRL的机器翻译评估中，不能仅依赖ChrF++，BLEU虽然绝对分数较低，但能提供对翻译质量的更全面、更具解释性的理解。"}}
{"id": "2602.17445", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17445", "abs": "https://arxiv.org/abs/2602.17445", "authors": ["Mateusz Nowak", "Xavier Cadet", "Peter Chin"], "title": "ABCD: All Biases Come Disguised", "comment": "29 pages, 20 figures, pre-print, 12 tables", "summary": "Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.", "AI": {"tldr": "该研究提出了一种减少LLM在选择题评估中标签位置偏见的协议，通过统一标签和提示模型关注完整答案，显著提高了评估的鲁棒性。", "motivation": "现有的多项选择题（MCQ）基准测试在评估LLM时存在标签位置偏见，即模型可能依赖于答案位置、提示中的标签或正确答案的分布来作答，而非真正的推理能力。这阻碍了对LLM真实能力的准确衡量。", "method": "提出了一种偏见减少的评估协议，该协议将MCQ题目的标签替换为统一、无序的标签，并提示LLM使用完整的答案。协议使用句子相似性模型来评估，并与标准评估方法进行了比较。", "result": "该评估协议在多个基准测试和模型上显著提高了LLM对答案排列的鲁棒性，将平均准确率方差降低了3倍，同时平均模型性能仅有微小下降。消融实验表明，该方法比标准方法更鲁棒。", "conclusion": "所提出的偏见减少的评估协议能够有效减少LLM在MCQ评估中的标签位置偏见，更准确地揭示LLM在无提示示例和选项标签干扰下的真实能力，并提高了评估的鲁棒性。"}}
{"id": "2602.17465", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17465", "abs": "https://arxiv.org/abs/2602.17465", "authors": ["Hongming Li", "Yang Liu", "Chao Huang"], "title": "Entropy-Based Data Selection for Language Models", "comment": "IEEE Access, 15 pages, 5 figures, 11 tables", "summary": "Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.", "AI": {"tldr": "提出了一种名为EUDS的熵基无监督数据选择框架，用于在计算资源有限的情况下高效微调语言模型，通过降低数据选择的计算成本并提高训练效率，显著减少了对数据的需求。", "motivation": "现代语言模型对计算和数据资源的需求巨大，而现有的数据选择技术虽然能减少训练数据量，但自身计算成本高，与计算资源受限的实际场景相悖。同时，评估数据可用性也是一个挑战，需要一种高效的数据选择方法。", "method": "提出了一种熵基无监督数据选择（EUDS）框架。该框架通过系统地研究数据选择与不确定性估计的关系，建立了一个计算高效的数据过滤机制。", "result": "在情感分析、主题分类和问答任务上的实证实验验证了EUDS的有效性。EUDS显著降低了计算成本，提高了训练时间效率，同时减少了数据需求。", "conclusion": "EUDS为在计算受限场景下高效微调语言模型提供了一种创新解决方案，通过降低数据选择的计算成本和提高训练效率，有效缓解了资源限制问题。"}}
{"id": "2602.17372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17372", "abs": "https://arxiv.org/abs/2602.17372", "authors": ["Yuchang Jiang", "Anton Raichuk", "Xiaoye Tong", "Vivien Sainte Fare Garnot", "Daniel Ortiz-Gonzalo", "Dan Morris", "Konrad Schindler", "Jan Dirk Wegner", "Maxim Neumann"], "title": "Tree crop mapping of South America reveals links to deforestation and conservation", "comment": null, "summary": "Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as \"forest\". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.", "AI": {"tldr": "研究利用10米分辨率的卫星图像和深度学习模型生成了南美洲的树木作物地图，发现了23%的树木作物与2000-2020年间的森林砍伐有关。该研究还指出，现有的欧盟法规地图错误地将部分农业用地，特别是小农林业，标记为“森林”，可能导致对小农户的误判和不公平处罚。高分辨率地图有望支持更有效和公平的保护政策。", "motivation": "为了支持零毁林政策（如欧盟的法规），需要准确监测树木作物扩张。现有的高分辨率数据不足以区分不同的农业系统和森林，特别是小农户的林业系统，这给法规的执行带来了挑战。", "method": "使用Sentinel-1和Sentinel-2卫星图像的时间序列，结合多模态、时空深度学习模型，生成了南美洲首个10米分辨率的树木作物地图。", "result": "生成了约1100万公顷的树木作物地图，其中23%与2000-2020年间的森林覆盖损失相关。现有支持欧盟法规的地图存在将已建立的农业用地（尤其是小农林业）错误分类为“森林”的现象。", "conclusion": "提供的高分辨率树木作物地图有助于识别与森林砍伐相关的树木作物扩张，并纠正现有法规地图的分类错误，从而降低对小农户的误判风险，支持更有效、包容和公平的保护政策。"}}
{"id": "2602.17046", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17046", "abs": "https://arxiv.org/abs/2602.17046", "authors": ["Uria Franko"], "title": "Dynamic System Instructions and Tool Exposure for Efficient Agentic LLMs", "comment": null, "summary": "Large Language Model (LLM) agents often run for many steps while re-ingesting long system instructions and large tool catalogs each turn. This increases cost, agent derailment probability, latency, and tool-selection errors. We propose Instruction-Tool Retrieval (ITR), a RAG variant that retrieves, per step, only the minimal system-prompt fragments and the smallest necessary subset of tools. ITR composes a dynamic runtime system prompt and exposes a narrowed toolset with confidence-gated fallbacks. Using a controlled benchmark with internally consistent numbers, ITR reduces per-step context tokens by 95%, improves correct tool routing by 32% relative, and cuts end-to-end episode cost by 70% versus a monolithic baseline. These savings enable agents to run 2-20x more loops within context limits. Savings compound with the number of agent steps, making ITR particularly valuable for long-running autonomous agents. We detail the method, evaluation protocol, ablations, and operational guidance for practical deployment.", "AI": {"tldr": "本研究提出了一种名为指令-工具检索（ITR）的新方法，作为一种改进的检索增强生成（RAG）技术，旨在解决大型语言模型（LLM）代理在多步执行过程中因重复加载冗长的系统指令和工具目录而导致的高成本、易发生错误和延迟问题。ITR能够根据每一步的需求，动态检索最小化的指令片段和必要的工具子集，从而显著减少上下文令牌数量，提高工具选择的准确性，并大幅降低代理的运行成本。", "motivation": "现有的LLM代理在执行过程中，每一步都需要重新处理冗长的系统指令和工具目录，这导致了高昂的计算成本、代理易偏离目标、响应延迟以及工具选择错误等问题。这些问题在代理需要执行大量步骤时尤为突出，限制了其在实际应用中的效率和有效性。", "method": "研究提出了一种名为指令-工具检索（ITR）的技术。ITR是一种检索增强生成（RAG）的变体，其核心思想是在代理执行的每一步，仅检索最小化的系统指令片段和必要的工具子集。它能够动态构建运行时系统提示，并提供一个缩小的工具集，同时包含置信度门控的后备方案，以应对不确定性。", "result": "通过在一个控制基准上进行实验，ITR相较于单一的基线模型，能够将每步的上下文令牌数量减少95%，将正确的工具路由准确率相对提高32%，并将端到端的任务执行成本降低70%。这些效率的提升使得代理能够在相同的上下文限制下，执行2-20倍更多的循环。", "conclusion": "ITR方法有效地解决了LLM代理在多步执行过程中的效率瓶颈，显著降低了成本和错误率，并大幅提升了代理在上下文长度限制内的执行能力。研究表明，ITR的收益会随着代理步骤数的增加而累积，对于需要长时间运行的自主代理尤其有价值。此外，研究还提供了关于方法、评估协议、消融实验和实际部署的指导。"}}
{"id": "2602.17387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17387", "abs": "https://arxiv.org/abs/2602.17387", "authors": ["Changhun Kim", "Martin Mayr", "Thomas Gorges", "Fei Wu", "Mathias Seuret", "Andreas Maier", "Vincent Christlein"], "title": "DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition", "comment": "Submitted to Pattern Recognition, 11 pages + 2-page appendix, 7 figures, 12 tables", "summary": "State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.", "AI": {"tldr": "本文提出了一种名为 DRetHTR 的手写文本识别（HTR）模型，该模型基于 Retentive Networks（RetNet），能够显著提高 Transformer 模型的推理速度和内存效率，同时保持或提高准确性。", "motivation": "现有的先进手写文本识别系统（HTR）通常使用 Transformer 模型，但其不断增长的键值（KV）缓存导致解码速度慢且内存消耗大。研究的动机是开发一种更高效的 HTR 模型。", "method": "作者提出了 DRetHTR，一个基于 Retentive Networks（RetNet）的仅解码器模型。通过使用无 Softmax 的注意力机制（softmax-free retention）和注入多尺度序列先验，DRetHTR 避免了不断增长的 KV 缓存，实现了线性的时间和内存复杂度。为了恢复注意力机制的局部到全局归纳偏差，提出了层级 gamma 缩放，使浅层模型短距离依赖，深层模型捕获更广阔的上下文。", "result": "与同等规模的仅解码器 Transformer 基线模型相比，DRetHTR 的推理速度提高了 1.6-1.9 倍，内存使用量减少了 38-42%，准确率没有损失。在 IAM-A（en）、RIMES（fr）和 Bentham（en）数据集上取得了目前最佳的测试字符错误率（CER），分别为 2.26%、1.81% 和 3.46%，并在 READ-2016（de）数据集上达到了 4.21% 的有竞争力结果。", "conclusion": "仅解码器的 RetNet 模型能够实现与 Transformer 相当的手写文本识别准确率，同时显著提高了解码速度和内存效率，为 HTR 任务提供了一种更优的选择。"}}
{"id": "2602.17062", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17062", "abs": "https://arxiv.org/abs/2602.17062", "authors": ["Yonghyeon Jo", "Sunwoo Lee", "Seungyul Han"], "title": "Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning", "comment": "10 technical page followed by references and appendix. Accepted to ICLR 2026", "summary": "Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.", "AI": {"tldr": "提出一种名为 S2Q 的多智能体强化学习方法，通过学习多个子值函数来保留替代高价值动作，鼓励持续探索并提高对值函数变化（如训练中的最优策略转移）的适应性，从而克服现有方法的局限性。", "motivation": "现有基于值分解的合作多智能体强化学习方法依赖单一最优动作，在值函数发生变化时适应性差，容易收敛到次优策略。研究动机是提高 MARL 方法在值函数动态变化时的适应性和鲁棒性。", "method": "提出 Successive Sub-value Q-learning (S2Q) 方法。S2Q 学习多个子值函数来存储备选的高价值动作。然后，将这些子值函数纳入一个基于 Softmax 的行为策略中，以鼓励持续探索，并使总值函数 $Q^{\text{tot}}$ 能够快速适应变化的最优值。", "result": "在具有挑战性的 MARL 基准测试中进行实验，结果表明 S2Q 始终优于多种 MARL 算法，显示出其在适应性和整体性能上的提升。", "conclusion": "S2Q 通过学习多个子值函数来保留替代高价值动作，并结合基于 Softmax 的行为策略，能够有效提高 MARL 系统在训练过程中值函数变化时的适应能力，从而获得更好的性能。"}}
{"id": "2602.17053", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17053", "abs": "https://arxiv.org/abs/2602.17053", "authors": ["Yunseok Han", "Yejoon Lee", "Jaeyoung Do"], "title": "RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models", "comment": "Accepted in ICLR 2026 Poster: $\\href{https://iclr.cc/virtual/2026/poster/10011763}{\\text{this URL}}$", "summary": "Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$", "AI": {"tldr": "本研究提出了一个名为 RFEval 的框架和基准，用于评估大型推理模型（LRM）的推理过程是否真实可靠。研究发现，近一半的 LRM 输出存在推理不忠实的问题，且问题主要源于推理与答案之间的立场不一致。研究还指出，当前的训练方法，如强化学习，可能会降低推理的忠实度，并且准确率并不能作为推理忠实度的可靠指标。", "motivation": "大型推理模型（LRM）虽然表现出色，但其生成的推理过程往往听起来合理却不反映真实的决策机制，这降低了其可靠性和可信度。因此，需要一种方法来评估和提升 LRM 的推理忠实度。", "method": "研究提出了一个正式的推理忠实度框架，包含两个可测试的条件：立场一致性（reasoning与answer之间存在一致的立场）和因果影响（在输出层面进行干预时，陈述的推理能够因果地驱动答案）。在此基础上，开发了一个名为 RFEval 的基准，包含 7,186 个实例，横跨七个任务，通过受控的、输出层面的反事实干预来探测推理忠实度。研究评估了十二个开源 LRM。", "result": "在评估的十二个开源 LRM 中，有 49.7% 的输出存在推理不忠实的问题，主要表现为立场不一致。这些问题集中在数学和代码等脆弱、收敛性强的领域。研究发现，问题更多地与训练后的方法相关，而非模型规模。在同一模型系列中进行消融实验表明，在监督微调的基础上增加当前的 RL 式目标，即使准确率保持不变，也可能降低推理的忠实度。此外，准确率既不是推理忠实度的充分条件，也不是可靠的代理指标，在控制模型和任务后，准确率与推理忠实度的关联较弱且统计不显著。", "conclusion": "本研究建立了一种严谨的方法来审计 LRM 的可靠性，并表明构建可信赖的人工智能不仅需要优化正确的输出结果，还需要关注推理过程的结构完整性。研究强调了推理忠实度的重要性，并呼吁未来的研究和开发应同时关注模型的准确性和推理过程的真实性。"}}
{"id": "2602.17467", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17467", "abs": "https://arxiv.org/abs/2602.17467", "authors": ["Greta Damo", "Stéphane Petiot", "Elena Cabrio", "Serena Villata"], "title": "PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions", "comment": null, "summary": "The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.", "AI": {"tldr": "本文提出了PEACE 2.0，一个能够分析、解释仇恨言论并生成基于证据的反驳言论的新工具。", "motivation": "在线平台上的仇恨言论日益增多，自动检测仇恨言论虽已取得进展，但如何有效生成反驳言论仍是一个挑战。", "method": "利用检索增强生成（RAG）管道，实现以下功能：1) 将仇恨言论的解释 grounding 到证据和事实；2) 自动生成基于证据的反驳言论；3) 探索反驳言论回复的特点。", "result": "PEACE 2.0 能够对显式和隐式的仇恨言论进行深入分析，并生成相应的反驳言论。", "conclusion": "PEACE 2.0 集成了仇恨言论分析、解释和反驳言论生成能力，为应对在线仇恨言论提供了一个新的解决方案。"}}
{"id": "2602.17395", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17395", "abs": "https://arxiv.org/abs/2602.17395", "authors": ["Lorenzo Caselli", "Marco Mistretta", "Simone Magistri", "Andrew D. Bagdanov"], "title": "SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery", "comment": "Accepted at ICLR 2026. Code available at https://github.com/miccunifi/SpectralGCD", "summary": "Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.", "AI": {"tldr": "SpectralGCD 是一种新颖、高效的多模态方法，用于广义类别发现 (GCD)，它利用 CLIP 的跨模态相似性作为统一表示，并通过频谱过滤来优化概念选择，从而实现与最先进方法相当或更好的性能，同时显著降低计算成本。", "motivation": "现有的参数化分类器在未标记数据上容易过拟合旧类；多模态方法通过引入文本信息有所改善，但处理方式独立且计算成本高。作者希望开发一种更高效、更有效的多模态 GCD 方法。", "method": "SpectralGCD 将 CLIP 的跨模态图像-概念相似性用作统一表示。它将图像表示为大型任务无关概念词典的混合，从而利用显式语义。通过频谱过滤，利用学生模型和教师模型之间 softmax 相似性的跨模态协方差矩阵来选择相关的概念。知识蒸馏（前向和反向）用于确保学生模型的跨模态表示在语义上足够且对齐。", "result": "在六个基准测试中，SpectralGCD 的准确性与最先进的方法相当或显著优于它们，同时计算成本大大降低。", "conclusion": "SpectralGCD 是一种有效且计算成本低廉的多模态方法，能够显著提高广义类别发现的性能，通过利用 CLIP 的跨模态相似性并将学习锚定在显式语义上，克服了现有方法的局限性。"}}
{"id": "2602.17469", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17469", "abs": "https://arxiv.org/abs/2602.17469", "authors": ["Nusrat Jahan Lia", "Shubhashis Roy Dipta"], "title": "Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers", "comment": null, "summary": "The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% \"Sentiment Inversion Rate,\" fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including \"Asymmetric Empathy\" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a \"Modern Bias\" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate \"Affective Stability\" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.", "AI": {"tldr": "该研究调查了孟加拉语和英语之间的跨语言情感不匹配问题，发现现有AI对齐模型存在严重的代表性和安全问题，例如情感反转、不对称共情和现代偏见，并提出需要建立尊重语言和方言多样性的、基于文化的对齐方法。", "motivation": "现有AI对齐研究主要集中在单语环境中，跨语言对齐，尤其是对低资源语言（如孟加拉语）的情感理解，存在显著的系统性故障，影响了人类对AI的信任和AI对人类意图的准确理解。", "method": "研究者使用四种Transformer架构，通过在孟加拉语和英语之间进行情感对齐评估，量化了“情感反转率”、“不对称共情”和“现代偏见”等指标。", "result": "研究发现，压缩模型mDistilBERT存在28.7%的情感反转率；部分模型表现出“不对称共情”，系统性地压制或放大孟加拉语文本的情感；IndicBERT模型在处理正式孟加拉语（Sadhu）时，对齐误差增加了57%（“现代偏见”）。", "conclusion": "为了实现公平的人机协同进化，需要采用多元化、文化导向的对齐方法，而不是依赖于通用的模型压缩，后者无法保留情感保真度，从而损害了人机互信。研究建议在对齐基准中引入“情感稳定性”指标，以惩罚在低资源和方言环境中的极性反转。"}}
{"id": "2602.17397", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17397", "abs": "https://arxiv.org/abs/2602.17397", "authors": ["Panagiotis Koletsis", "Vasilis Efthymiou", "Maria Vakalopoulou", "Nikos Komodakis", "Anastasios Doulamis", "Georgios Th. Papadopoulos"], "title": "A High-Level Survey of Optical Remote Sensing", "comment": null, "summary": "In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.", "AI": {"tldr": "该论文提供了一个关于光学遥感领域（特别是无人机RGB图像）的全面概述，包括其能力、数据集和研究方向，旨在为新研究者提供指导。", "motivation": "鉴于计算机视觉在遥感领域的快速发展以及无人机（特别是配备RGB相机）的广泛应用，作者认为需要一个全面的调查来整合该领域的信息，并为新研究者提供指导。", "method": "该工作通过对光学遥感能力、数据集和相关方法的“全面概述”来组织内容，提供“高层次的见解”，并帮助研究者聚焦感兴趣的领域。", "result": "论文提出了对光学遥感（尤其是无人机RGB图像）能力的全面概述，包含关键信息、数据集和见解，以指导新研究者。", "conclusion": "该论文是首个提供光学遥感（尤其是无人机RGB图像）领域整体视角和综合信息的调查，为该领域的研究者，特别是新进入者，提供了一个有价值的指南。"}}
{"id": "2602.17084", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17084", "abs": "https://arxiv.org/abs/2602.17084", "authors": ["Kan Watanabe", "Rikuto Tsuchida", "Takahiro Monno", "Bin Huang", "Kazuma Yamasaki", "Youmei Fan", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses", "comment": null, "summary": "The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.", "AI": {"tldr": "本研究分析了五种AI编码代理生成的GitHub pull request（PR）描述的特点，以及人类审阅者对这些PR的反应，发现不同AI代理的PR描述风格会影响审阅者的参与度、响应时间和合并结果。", "motivation": "随着AI编码代理在GitHub上生成PR的普及，了解这些代理在PR描述上的差异及其对人类审阅者响应的影响，对于优化人机协作软件开发至关重要，但目前仍是未被充分探索的领域。", "method": "研究人员使用AIDev数据集，对五种AI编码代理生成的PR描述进行了实证分析，重点关注结构化特征。同时，他们还考察了人类审阅者的反应，包括审阅活动、响应时间、情感倾向和合并结果。", "result": "研究发现，不同的AI编码代理确实展现出独特的PR描述风格。这些风格与审阅者的参与度、响应时间以及PR的合并结果存在关联。在审阅者互动指标和合并率方面，不同代理之间存在显著差异。", "conclusion": "AI编码代理的PR呈现方式及其与审阅者的互动动态，在人机协作软件开发中扮演着重要角色。PR描述的差异会影响人类审阅者的行为和最终的合并决策。"}}
{"id": "2602.17066", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17066", "abs": "https://arxiv.org/abs/2602.17066", "authors": ["Sumedh Rasal"], "title": "Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization", "comment": null, "summary": "We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.", "AI": {"tldr": "本文提出了一种名为预测性批处理调度（PBS）的新型训练优化技术，通过动态优先处理高损失样本来加速语言模型的收敛。PBS使用一个轻量级的线性预测器，仅基于简单的令牌级特征（如令牌频率、序列长度等）来估计样本难度，无需复杂的预定义指标或昂贵的逐样本损失跟踪。实验表明，PBS可以显著加快模型收敛速度，并证明了令牌频率统计信息包含有意义的样本难度信息。", "motivation": "现有方法（如课程学习和困难样本挖掘）在确定样本难度方面存在局限性，需要预定义指标或昂贵的计算开销。研究动机是开发一种更高效、计算开销更小的方法来优化训练批次构建，从而加速语言模型的收敛。", "method": "提出预测性批处理调度（PBS），一种在线训练的轻量级线性预测器，用于从静态令牌级特征（令牌频率、序列长度、词汇多样性、稀有令牌比例）估计样本难度。在构建训练批次时，PBS动态优先处理预测为高损失的样本。", "result": "在130M参数的Transformer模型上进行实验，PBS在评估损失方面实现了6-13%的收敛加速。预测器的实际损失相关性从训练初期的0.14提高到10,000步后的0.44。", "conclusion": "PBS是一种有效的训练优化技术，可以通过利用令牌级特征（特别是令牌频率）来预测样本难度，从而在计算开销极小的情况下加速语言模型的收敛。token frequency statistics 编码了有意义的样本难度信息，可以用于有效的课程学习。"}}
{"id": "2602.17475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17475", "abs": "https://arxiv.org/abs/2602.17475", "authors": ["Pietro Ferrazzi", "Mattia Franzin", "Alberto Lavelli", "Bernardo Magnini"], "title": "Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian", "comment": "Paper Accepted at LREC 2026", "summary": "Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether \"small\" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.", "AI": {"tldr": "研究表明，参数量较小的语言模型（约10亿参数）在医疗NLP任务上也能取得与大型模型相当甚至更好的性能，特别是在采用微调策略时。研究还发布了意大利语的医疗NLP数据集和相应的预训练模型。", "motivation": "大型语言模型（LLMs）在医疗NLP任务上表现出色，但其高昂的计算成本限制了实际应用。本研究旨在探索小型LLMs（约10亿参数）是否能在保持竞争力的同时完成医疗任务。", "method": "研究评估了Llama-3、Gemma-3和Qwen3三个系列的10亿参数级模型，在20个临床NLP任务上进行了测试。比较了推理时（少样本提示、约束解码）和训练时（监督微调、持续预训练）的多种适应策略，并使用公开的意大利语医疗数据集和新收集的意大利医院急诊科数据进行实验。", "result": "监督微调是最有效的策略。少样本提示与约束解码的组合是低资源场景下的有力替代方案。小型LLMs可以匹配甚至超越大型基线模型，其中基于Qwen3-1.7B的最佳配置比Qwen3-32B平均得分高9.2分。", "conclusion": "小型LLMs在医疗NLP任务上具有巨大的潜力，通过合适的策略（尤其是微调）可以实现高性能，并且计算成本更低。研究发布了意大利语医疗NLP数据集和最优模型，为该领域的研究和应用提供了资源。"}}
{"id": "2602.17419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17419", "abs": "https://arxiv.org/abs/2602.17419", "authors": ["Xiaomeng Peng", "Xilang Huang", "Seon Han Choi"], "title": "EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models", "comment": null, "summary": "Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \\href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}", "AI": {"tldr": "提出了一种名为EAGLE的免微调框架，通过整合专家模型的输出来指导多模态大模型（MLLM）进行工业异常检测，并生成可解释的异常描述。实验表明，EAGLE能在不更新模型参数的情况下提升MLLM的异常检测性能，并使模型更关注异常区域。", "motivation": "现有的深度学习异常检测方法多为二分类，缺乏语义解释。MLLMs具有生成语言解释的潜力，但现有方法通常需要昂贵的微调且效果不如轻量级模型。研究动机是探索一种无需微调且能提升MLLM在工业异常检测中准确性和解释性的方法。", "method": "提出EAGLE框架，一种免微调方法。该方法通过集成来自专家模型的输出，引导MLLM关注图像中的异常区域，从而实现准确检测和可解释的异常描述。此外，研究了EAGLE如何影响MLLM内部注意力分布，发现EAGLE倾向于增加模型对异常区域的注意力集中度。", "result": "在MVTec-AD和VisA数据集上的实验表明，EAGLE框架能够显著提升多种MLLM的异常检测性能，且无需任何参数更新。其性能与基于微调的方法相当，并能促进MLLM的注意力与图像中的异常区域对齐。", "conclusion": "EAGLE是一种有效的、无需微调的框架，能够利用MLLM进行工业异常检测，并提供可解释的描述。该方法通过引导MLLM的注意力机制，在提高检测精度的同时，也增强了模型的可解释性，为工业智能制造中的异常检测提供了新的解决方案。"}}
{"id": "2602.17096", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17096", "abs": "https://arxiv.org/abs/2602.17096", "authors": ["Zhaoyang Li", "Xingzhi Jin", "Junyu Pan", "Qianqian Yang", "Zhiguo Shi"], "title": "Agentic Wireless Communication for 6G: Intent-Aware and Continuously Evolving Physical-Layer Intelligence", "comment": null, "summary": "As 6G wireless systems evolve, growing functional complexity and diverse service demands are driving a shift from rule-based control to intent-driven autonomous intelligence. User requirements are no longer captured by a single metric (e.g., throughput or reliability), but by multi-dimensional objectives such as latency sensitivity, energy preference, computational constraints, and service-level requirements. These objectives may also change over time due to environmental dynamics and user-network interactions. Therefore, accurate understanding of both the communication environment and user intent is critical for autonomous and sustainably evolving 6G communications.\n  Large language models (LLMs), with strong contextual understanding and cross-modal reasoning, provide a promising foundation for intent-aware network agents. Compared with rule-driven or centrally optimized designs, LLM-based agents can integrate heterogeneous information and translate natural-language intents into executable control and configuration decisions.\n  Focusing on a closed-loop pipeline of intent perception, autonomous decision making, and network execution, this paper investigates agentic AI for the 6G physical layer and its realization pathways. We review representative physical-layer tasks and their limitations in supporting intent awareness and autonomy, identify application scenarios where agentic AI is advantageous, and discuss key challenges and enabling technologies in multimodal perception, cross-layer decision making, and sustainable optimization. Finally, we present a case study of an intent-driven link decision agent, termed AgenCom, which adaptively constructs communication links under diverse user preferences and channel conditions.", "AI": {"tldr": "本文探讨了在6G无线系统中，如何利用大型语言模型（LLM）实现意图驱动的自主智能网络控制，以应对日益增长的功能复杂性和多样的服务需求。研究了LLM在物理层实现意图感知、自主决策和网络执行的途径，并提出了一个名为AgenCom的案例研究。", "motivation": "6G系统面临日益增长的功能复杂性和多样的服务需求，传统的基于规则的控制方法已无法满足用户多维度（如延迟、能耗、计算约束、服务级别）且随时间变化的用户意图。因此，需要一种能够理解复杂用户意图并实现自主决策的网络控制机制。", "method": "本文研究了基于LLM的代理AI在6G物理层实现意图感知、自主决策和网络执行的闭环流程。回顾了现有物理层任务的局限性，识别了代理AI的优势场景，并讨论了多模态感知、跨层决策和可持续优化等关键挑战与技术。最后，通过AgenCom案例研究展示了意图驱动的链路构建。", "result": "LLM能够集成异构信息，并将自然语言意图转化为可执行的网络控制和配置决策。AgenCom案例研究表明，意图驱动的代理AI能够根据用户偏好和信道条件自适应地构建通信链路。", "conclusion": "LLM为实现6G网络中的意图驱动自主智能提供了强大的基础。代理AI在物理层具有广阔的应用前景，但仍需克服多模态感知、跨层决策和可持续优化等方面的挑战。"}}
{"id": "2602.17513", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17513", "abs": "https://arxiv.org/abs/2602.17513", "authors": ["Baris Karacan", "Barbara Di Eugenio", "Patrick Thornton"], "title": "Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics", "comment": "11 pages. Accepted at LREC 2026. To appear in the proceedings", "summary": "Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.", "AI": {"tldr": "该研究提出了一种新的妇产科临床文本分段数据集，并比较了监督模型和零样本大型语言模型在临床文本分段任务上的性能。结果表明，零样本模型在跨领域应用时表现出更强的鲁棒性。", "motivation": "现有临床文本分段方法主要基于MIMIC-III数据集，覆盖的医学领域有限，并且在跨领域应用时性能会下降。研究旨在通过引入新的妇产科数据集，并比较监督模型和零样本大型语言模型的性能，来改进临床文本分段技术，尤其关注其在不同领域的可迁移性。", "method": "研究者构建了一个新的、去标识化的、带有分段标签的妇产科临床笔记数据集。他们系统地评估了基于Transformer的监督模型在MIMIC-III（同领域）和新妇产科数据集（跨领域）上的性能。此外，他们首次将监督模型与零样本大型语言模型在医学文本分段任务上进行了直接比较。", "result": "监督模型在同领域数据集上表现良好，但在跨领域数据集上性能显著下降。相比之下，零样本模型在修正了生成的幻觉（hallucinated）分段标题后，表现出良好的跨领域适应性。", "conclusion": "研究强调了构建领域特定临床资源的重要性，并指出零样本文本分段是一种有前景的方法，可以在现有研究领域之外应用于医疗NLP，但需要妥善处理生成的幻觉标题问题。"}}
{"id": "2602.17473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17473", "abs": "https://arxiv.org/abs/2602.17473", "authors": ["Jiwei Shan", "Zeyu Cai", "Cheng-Tai Hsieh", "Yirui Li", "Hao Liu", "Lijun Han", "Hesheng Wang", "Shing Shin Cheng"], "title": "4D Monocular Surgical Reconstruction under Arbitrary Camera Motions", "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920 characters\", the abstract here is shorter than that in the PDF file Subjects", "summary": "Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.", "AI": {"tldr": "提出了一种名为Local-EndoGS的4D重建框架，用于单目内窥镜序列，可以处理任意的相机运动和可变形场景，并且不需要立体深度或精确的运动恢复结构。", "motivation": "现有的可变形手术场景重建方法在处理具有大范围相机运动的单目序列时存在局限性，无法满足真实临床环境的需求。", "method": "Local-EndoGS采用渐进式、基于窗口的全局表示，为每个观察窗口分配局部可变形场景模型。通过结合多视图几何、跨窗口信息和单目深度先验，实现粗到精的重建策略。此外，还引入了长程2D像素轨迹约束和物理运动先验来提高变形的可信度。", "result": "在三个公开的内窥镜数据集上，Local-EndoGS在外观质量和几何精度上均优于现有最先进的方法。", "conclusion": "Local-EndoGS成功地实现了对具有任意相机运动的单目内窥镜序列的可变形手术场景进行高质量的4D重建，克服了现有方法的限制，并验证了关键设计的有效性。"}}
{"id": "2602.17106", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17106", "abs": "https://arxiv.org/abs/2602.17106", "authors": ["Xiaoran Cai", "Wang Yang", "Xiyu Ren", "Chekun Law", "Rohit Sharma", "Peng Qi"], "title": "Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction", "comment": null, "summary": "Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.", "AI": {"tldr": "该研究提出了一种人类-AI协作框架，旨在通过STRIDE和SR-Delta两个部分，利用大型语言模型（LLMs）构建公司层面的基准数据集，以评估和统一不同ESG评级机构的结果，从而提高评级的可比性和可信度。", "motivation": "现有ESG评级机构为同一公司产生的评级结果差异巨大，这限制了其可比性、可信度以及在决策中的相关性，因此需要一种方法来统一评级结果。", "method": "提出一个包含STRIDE（原则性标准和评分系统，指导LLMs构建基准数据集）和SR-Delta（差异分析程序框架，用于识别潜在调整）的人类-AI协作框架。", "result": "该框架能够实现可持续性评级方法的可扩展和可比较评估。", "conclusion": "呼吁AI社区采用AI驱动的方法来加强和推进可持续性评级方法，以支持和执行紧迫的可持续性议程。"}}
{"id": "2602.17542", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17542", "abs": "https://arxiv.org/abs/2602.17542", "authors": ["Zhangqi Duan", "Arnav Kankaria", "Dhruv Kartik", "Andrew Lan"], "title": "Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems", "comment": null, "summary": "Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.", "AI": {"tldr": "本研究提出了一种利用大语言模型（LLMs）为学生编程代码自动生成知识组件（KCs）级别正确性标签的框架，以解决真实世界数据集中KC级别标签稀缺的问题，并提升学生模型和学习分析的准确性。", "motivation": "现实世界的数据集中，尤其是开放式编程任务，通常缺乏知识组件（KCs）级别的正确性标签。直接使用问题级别的正确性标签会掩盖部分掌握情况，导致学习曲线拟合不佳。", "method": "提出一个自动框架，利用大语言模型（LLMs）直接从学生编写的代码中标记KC级别的正确性。该方法评估每个KC是否被正确应用，并引入一个与时间相关的、感知上下文的代码-KC映射机制来更好地将KCs与单个学生代码对齐。", "result": "与基线方法相比，该框架生成的KC级别正确性标签在学习曲线拟合和预测性能方面均有提升，学习曲线更符合认知理论。LLM与专家标注结果显示出较高的一致性。", "conclusion": "所提出的LLM驱动的KC级别正确性标注框架能够有效地解决真实世界数据集中KC标签稀缺的问题，并能生成更准确、更符合理论的学习曲线，从而提升学生建模和学习分析的效果。"}}
{"id": "2602.17111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17111", "abs": "https://arxiv.org/abs/2602.17111", "authors": ["Abdulrahman AlRabah", "Priyanka Kargupta", "Jiawei Han", "Abdussalam Alawini"], "title": "Instructor-Aligned Knowledge Graphs for Personalized Learning", "comment": null, "summary": "Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like \"Algorithms\" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., \"part-of\" or \"depends-on\" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., \"recursion\" is taught before \"mergesort\"; \"recursion\" is mentioned in the definition of \"merge sort\") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.", "AI": {"tldr": "InstructKG 是一个自动构建知识图谱的框架，用于捕捉课程中概念之间的学习依赖关系，以支持个性化学习。", "motivation": "在大型课程中，教师难以诊断学生知识差距和确定需要加强的概念，因此需要一种方法来有效捕捉概念之间的依赖关系，以实现个性化学习。", "method": "InstructKG 框架从课程的讲义材料（如幻灯片、笔记）中提取重要概念作为节点，并利用讲义材料独特的时间和语义信号（如教学顺序、定义中的提及）与大型语言模型的通用性相结合，推断出学习依赖关系（如“属于”或“依赖于”关系）作为有向边，从而构建与教师意图一致的知识图谱。", "result": "通过在真实世界的、多样化的课程讲义材料上的实验和基于人类的评估，InstructKG 能够捕捉到丰富且与教师意图一致的学习进程。", "conclusion": "InstructKG 是一个有效的框架，能够自动构建能够捕捉课程学习进展的知识图谱，这对于支持大规模在线课程中的个性化学习至关重要。"}}
{"id": "2602.17484", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17484", "abs": "https://arxiv.org/abs/2602.17484", "authors": ["Yichen Lu", "Siwei Nie", "Minlong Lu", "Xudong Yang", "Xiaobo Zhang", "Peng Zhang"], "title": "Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection", "comment": null, "summary": "Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.", "AI": {"tldr": "本文提出了一种名为PixTrace和CopyNCE的新方法，通过像素坐标跟踪和几何引导的对比损失，增强了图像复制检测（ICD）中细粒度特征的学习能力，从而在处理复杂编辑时取得了比现有方法更好的性能和可解释性。", "motivation": "现有基于自监督学习（SSL）的图像复制检测方法在处理复杂的图像编辑时存在不足，因为它们在细粒度对应学习方面存在局限性。", "method": "提出PixTrace像素坐标跟踪模块，显式维持编辑转换中的空间映射；提出CopyNCE几何引导的对比损失，利用PixTrace验证的映射推导出的重叠比率来正则化块的亲和度。该方法结合了像素级可追溯性和块级相似性学习。", "result": "在DISC21数据集上，该方法在匹配器上达到了88.7%的uAP和83.9%的RP90，在描述符上达到了72.6%的uAP和68.4%的RP90，取得了当前最优的性能。", "conclusion": "该方法成功地弥合了像素级可追溯性与块级相似性学习之间的差距，有效抑制了SSL训练中的监督噪声，并在图像复制检测任务中取得了显著的性能提升和更好的可解释性。"}}
{"id": "2602.17107", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17107", "abs": "https://arxiv.org/abs/2602.17107", "authors": ["Xiangyu Zhou", "Chenhan Xiao", "Yang Weng"], "title": "Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)", "comment": null, "summary": "Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.", "AI": {"tldr": "本文提出了一种新的基于Owen值（Shapley值的一种层次化推广）的特征归因方法O-Shap，用于解决视觉任务中特征（像素）之间的依赖性问题。与现有的基于SLIC等方法的Owen值实现不同，O-Shap采用了一种新的分组策略，能够更好地捕捉语义信息，在准确性、一致性和效率上均优于现有方法。", "motivation": "现有的基于Shapley值的XAI方法在处理视觉任务时，由于像素之间存在空间和语义上的强依赖性，其独立性假设失效。Owen值作为Shapley值的层次化推广，支持分组归因，但其有效性高度依赖于分组方式。现有分组方法（如SLIC）可能违反关键的T-property，导致归因不准确。", "method": "本文提出了一种新的特征分组方法，该方法满足T-property，从而确保层次结构中的语义一致性。基于这种分组策略，作者开发了O-Shap算法，它能够通过计算分组的Owen值来进行特征归因。该方法支持计算上的剪枝，提高了效率和解释性。", "result": "在图像和表格数据集上的实验表明，O-Shap相比于基线SHAP变体，在归因精度、语义连贯性以及运行时效率方面都表现更优，尤其是在特征结构重要的场景下。", "conclusion": "所提出的O-Shap方法通过一种新颖的、满足T-property的特征分组策略，能够更有效地利用Owen值进行XAI，克服了现有方法的局限性，提供了更准确、更一致且更高效的特征归因，尤其适用于存在复杂特征依赖性的视觉任务。"}}
{"id": "2602.17546", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17546", "abs": "https://arxiv.org/abs/2602.17546", "authors": ["Jyotin Goel", "Souvik Maji", "Pratik Mazumder"], "title": "Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning", "comment": "Work in progress (30 pages)", "summary": "Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.", "AI": {"tldr": "本研究提出了一种自适应正则化训练框架，能够根据安全风险调整模型的更新，以在微调过程中保持模型的安全性和实用性，而无需增加推理成本。", "motivation": "现有的指令遵循语言模型在安全行为上会因微调而恶化，尤其是在对抗性更新下。现有的防御方法要么保护有限，要么牺牲模型的实用性，因此需要一种新的方法来在保持安全性的同时不牺牲实用性。", "method": "研究中提出了一个训练框架，通过引入安全风险估计机制来指导正则化。具体来说，使用了两种风险估计方法：1. 安全判官（Safety Critic），对训练批次进行高级别危害评分。2. 基于激活的风险预测器，利用中间模型激活训练一个轻量级分类器来预测有害意图。这两种方法产生的风险信号被用来约束高风险更新，使其接近安全的参考策略，而低风险更新则按标准训练进行。", "result": "研究发现，模型在生成前的激活信号可以预测有害意图，并且判官评分能有效地提供高召回率的安全指导。在多个模型系列和攻击场景下，采用自适应正则化的方法比标准微调能显著降低攻击成功率，同时保留了下游性能，并且没有增加推理成本。", "conclusion": "本研究成功地证明了一种原则性的机制，可以通过自适应正则化来在不牺牲实用性的情况下维持语言模型的安全性，为模型在微调过程中保持对齐提供了有效的方法。"}}
{"id": "2602.17478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17478", "abs": "https://arxiv.org/abs/2602.17478", "authors": ["Xuan-Bac Nguyen", "Hoang-Quan Nguyen", "Sankalp Pandey", "Tim Faltermeier", "Nicholas Borys", "Hugh Churchill", "Khoa Luu"], "title": "QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery", "comment": "Project page: https://uark-cviu.github.io/projects/qupaint/", "summary": "Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.", "AI": {"tldr": "本研究提出了一个名为 QuPAINT 的新颖的物理感知多模态框架，用于通过光学显微镜图像表征二维量子材料。该框架结合了合成数据生成（Synthia）、指令微调数据集（QMat-Instruct）和一种新的物理感知注意力机制，以克服现有方法的局限性，并在一个综合基准（QF-Bench）上进行了评估。", "motivation": "利用光学显微镜图像表征二维量子材料面临挑战，包括层依赖对比度微妙、标注数据有限以及实验室和成像设备之间存在显著差异。现有的视觉模型缺乏物理先验，难以泛化到新材料或硬件条件。", "method": "研究者开发了一个名为 Synthia 的物理模型来生成合成的量子材料光学响应数据。他们还创建了一个名为 QMat-Instruct 的大规模多模态指令数据集，用于训练多模态大语言模型。核心模型 QuPAINT 采用物理感知指令微调，并引入物理信息注意力模块来融合视觉嵌入和光学先验。最后，他们建立了一个名为 QF-Bench 的基准，用于评估模型性能。", "result": "Synthia 生成了多样化的高质量合成样本，减少了对专家手动标注的依赖。QMat-Instruct 数据集使多模态大语言模型能够理解量子材料薄片的形貌和厚度。QuPAINT 模型通过融合视觉信息和光学先验，实现了更鲁棒和具有区分性的薄片表示。QF-Bench 提供了一个标准化的评估框架。", "conclusion": "本研究提出的物理感知多模态框架 QuPAINT，结合了合成数据生成、指令微调数据集和物理信息注意力机制，有效地解决了二维量子材料光学表征中的挑战，并在 QF-Bench 基准上证明了其有效性和泛化能力。"}}
{"id": "2602.17517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17517", "abs": "https://arxiv.org/abs/2602.17517", "authors": ["Hanyuan Zhang", "Lucas He", "Runlong He", "Abdolrahim Kadkhodamohammadi", "Danail Stoyanov", "Brian R. Davidson", "Evangelos B. Mazomenos", "Matthew J. Clarkson"], "title": "FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality", "comment": null, "summary": "Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.", "AI": {"tldr": "该研究提出了一种结合增强现实、深度图和非刚性迭代最近点（NICP）算法来改进腹腔镜肝脏手术中肿瘤定位的方法，该方法相比传统有限元（FE）模型更简单且工程友好，并在真实患者数据上达到了临床可用的精度。", "motivation": "现有腹腔镜肝脏手术的增强现实肿瘤定位方法依赖于器官轮廓，并且非刚性配准通常需要复杂的有限元模型，这增加了工程和建模的复杂性及专业知识要求。研究旨在降低这些要求，并提高定位精度。", "method": "该研究集成了腹腔镜深度图和基础姿态估计器来估计相机-肝脏的姿态，并用非刚性迭代最近点（NICP）算法替代了基于有限元的形变方法，以简化工程和建模复杂性。", "result": "在3个真实患者病例中，结合深度图的基础姿态估计方法平均注册误差为9.91毫米。结合刚性-NICP配准的方法优于仅刚性配准，证明了NICP是有限元形变模型的有效替代方案。", "conclusion": "提出的管道能够实现临床相关的精度，同时提供了一种轻量级、工程友好的替代方案，用于替代基于有限元的形变方法，从而改进了腹腔镜肝脏手术中增强现实的肿瘤定位。"}}
{"id": "2602.17588", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17588", "abs": "https://arxiv.org/abs/2602.17588", "authors": ["Faria Huq", "Zora Zhiruo Wang", "Zhanqiu Guo", "Venu Arvind Arangarajan", "Tianyue Ou", "Frank Xu", "Shuyan Zhou", "Graham Neubig", "Jeffrey P. Bigham"], "title": "Modeling Distinct Human Interaction in Web Agents", "comment": "Preprint", "summary": "Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.", "AI": {"tldr": "本研究提出了一种对人类干预进行建模的方法，以提高自主网络代理在协作任务执行中的表现。通过构建包含人类和代理交错操作的数据集，识别出四种用户交互模式，并利用这些模式训练语言模型以预测用户干预。实验结果表明，这种方法显著提高了干预预测的准确性，并增加了用户对代理的评价效用。", "motivation": "当前自主网络代理在用户参与方面存在不足，缺乏对人类干预时机和原因的理解，导致代理在关键决策点自主进行或请求不必要的确认。这促使研究者开发一种方法来模拟人类干预，以支持协作式的网络任务执行。", "method": "研究者构建了一个名为CowCorpus的数据集，其中包含400个真实用户在网络导航中的轨迹，记录了超过4200个人类和代理的交错操作。他们识别出四种用户与代理的交互模式，并利用这些模式训练语言模型来预测用户何时可能进行干预。最后，将这些干预感知模型部署到实时网络导航代理中，并通过用户研究进行评估。", "result": "基于用户交互模式训练的语言模型在干预预测准确性方面比基础语言模型提高了61.4%-63.4%。在用户研究中，部署了干预感知模型的代理使用户评定的代理有用性提高了26.5%。", "conclusion": "结构化地对人类干预进行建模能够显著提升网络代理的适应性和协作能力，从而实现更有效的自主网络任务执行。"}}
{"id": "2602.17116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17116", "abs": "https://arxiv.org/abs/2602.17116", "authors": ["Ilya Levin"], "title": "Epistemology of Generative AI: The Geometry of Knowing", "comment": "27", "summary": "Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.", "AI": {"tldr": "本文认为，生成式AI的运行机制模糊，需要新的哲学视角来理解。文章提出了“索引式高维空间认识论”，将生成模型视为高维空间中的导航者，并提出“导航式知识生产”作为一种区别于符号推理和统计重组的新知识模式。", "motivation": "生成式AI的运行机制不透明，对知识生产带来挑战，阻碍了其在科学、教育等领域的负责任应用。因此，需要一种新的哲学框架来理解其认识论基础。", "method": "文章首先指出生成式AI与传统图灵-香农-冯·诺依曼范式的信息处理方式不同，其将符号输入映射到高维语义空间。然后，借鉴皮尔士符号学和帕普特建构主义，利用高维几何的四大特性（测度集中、近乎正交、指数方向容量、流形规则性）构建了“索引式高维空间认识论”，并将生成模型重新概念化为“学习流形的导航者”。", "result": "文章提出了一种新的知识生产模式——“导航式知识生产”，它不同于传统的符号推理和统计重组，而是指在学习到的高维语义空间中进行导航和探索。", "conclusion": "生成式AI的认识论基础在于其高维语义空间，理解这一空间并在此空间中进行导航，是生成式AI进行知识生产的关键。本文提出的“导航式知识生产”为理解和负责任地应用生成式AI提供了新的视角。"}}
{"id": "2602.17130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17130", "abs": "https://arxiv.org/abs/2602.17130", "authors": ["Victor Kondratiev", "Irina Gribanova", "Alexander Semenov"], "title": "Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances", "comment": null, "summary": "We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.", "AI": {"tldr": "提出了一种新颖的并行算法，用于分解困难的 CircuitSAT 实例，通过使用专门的约束将原始 SAT 实例划分为一系列较弱的公式。该算法是参数化的并行算法，可以通过调整参数和并行计算的硬度估计来有效地识别高质量的分解。", "motivation": "为了有效地解决困难的 CircuitSAT 实例，特别是那些编码布尔电路逻辑等价性检查和密码哈希函数预象攻击的实例。", "method": "提出了一种新颖的并行算法，该算法利用专门的约束来将原始 SAT 实例分解为一系列较弱的公式。该算法是参数化的，允许通过并行计算的硬度估计来调整参数以实现高效的分解。", "result": "该算法在具有挑战性的 CircuitSAT 实例上展示了实际的有效性，包括编码布尔电路逻辑等价性检查和密码哈希函数预象攻击的实例。", "conclusion": "所提出的参数化并行算法能够通过专门的约束和并行硬度估计来有效地识别高质量的分解，从而能够解决困难的 CircuitSAT 实例。"}}
{"id": "2602.17555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17555", "abs": "https://arxiv.org/abs/2602.17555", "authors": ["Zixu Cheng", "Da Li", "Jian Hu", "Ziquan Liu", "Wei Li", "Shaogang Gong"], "title": "GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking", "comment": "Under review", "summary": "Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.", "AI": {"tldr": "提出GraphThinker，一种基于强化学习微调的方法，通过构建事件级场景图和增强视觉基础来减少视频推理中的幻觉。", "motivation": "现有基于多模态大语言模型（MLLMs）的视频推理方法依赖于密集的标题或视频摘要，缺乏因果理解，容易产生幻觉。", "method": "1. 使用MLLM构建显式建模事件内外关系的事件级视频场景图（EVSG）。2. 将EVSG作为中间思考过程融入MLLM。3. 引入视觉注意力奖励进行强化学习微调，加强视频基础，减少幻觉。", "result": "在RexTime和VidHalluc数据集上，GraphThinker表现出更强的捕捉对象和事件关系的能力，事件定位更精确，并显著减少了视频推理中的幻觉。", "conclusion": "GraphThinker通过引入结构化的事件级场景图和增强视觉基础，有效解决了现有MLLMs在视频推理中因缺乏因果理解而产生的幻觉问题，提升了推理的准确性和事件定位的精度。"}}
{"id": "2602.17598", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.17598", "abs": "https://arxiv.org/abs/2602.17598", "authors": ["Jayadev Billa"], "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?", "comment": "10 pages, 6 figures, 7 tables", "summary": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.", "AI": {"tldr": "当前许多语音大模型在执行任务时，其能力等同于一个独立的语音识别（ASR）模型后接一个文本大模型（LLM），这种“隐式ASR”在实际应用中成本高且在噪声环境下表现更差。研究发现，并非所有语音大模型都遵循此模式，例如 Qwen2-Audio 在处理信息时与级联模型有所不同。", "motivation": "作者想探究当前主流语音大模型（Speech LLMs）在执行特定任务时，其内部机制是否真的独立于传统的ASR+LLM级联模式，以及这种模式的普适性和潜在的缺点。", "method": "研究者通过“匹配骨干测试”（matched-backbone testing）的方法，对比了四种语音大模型（包括Ultravox和Qwen2-Audio）在六种任务上的表现，并首次控制了LLM骨干的相同性。同时，利用“logit lens”技术观察模型内部状态，以及使用“LEACE concept erasure”方法来验证文本表征的因果必要性。", "result": "研究发现，Ultravox模型在行为和机制上与Whisper+LLM的级联模型几乎无异。通过logit lens发现，模型内部出现了明显的文本信息。LEACE实验证明，文本表征对于两种测试架构的准确性都至关重要，移除文本表征会导致准确率降至接近零。然而，Qwen2-Audio模型表现出与级联模型不同的特性，表明这种级联等价性并非普遍存在。此外，在有噪声环境下，当前语音大模型（指代级联模式的）表现比干净环境下差，优势甚至逆转了7.6%。", "conclusion": "目前大多数语音大模型在功能上等同于成本更高且在噪声环境下表现更差的ASR+LLM级联模型。然而，Qwen2-Audio的出现表明，并非所有语音大模型都遵循这一模式，架构的差异会影响其信息处理方式。因此，在部署时需谨慎评估模型是否是“昂贵的级联”，尤其是在复杂声学环境下。"}}
{"id": "2602.17623", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17623", "abs": "https://arxiv.org/abs/2602.17623", "authors": ["Alireza Sakhaeirad", "Ali Ma'manpoosh", "Arshia Hemmat"], "title": "Unmasking the Factual-Conceptual Gap in Persian Language Models", "comment": null, "summary": "While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.", "AI": {"tldr": "本文提出了一个名为DivanBench的新波斯语NLP基准，专注于评估大型语言模型（LLMs）在理解和推理迷信和习俗方面的能力。研究发现，现有模型存在严重的顺从偏差，无法有效识别不当行为；持续的波斯语预训练反而会加剧这种偏差；并且模型在事实检索和场景推理之间存在显著的性能差距。", "motivation": "现有波斯语NLP基准在区分文化事实知识和对隐含社会规范进行推理的能力方面存在不足。作者希望开发一个专门针对迷信和习俗（即任意、依赖上下文且难以简单逻辑推理的规则）的诊断性基准，以更深入地评估模型的文化理解能力。", "method": "作者创建了一个名为DivanBench的诊断性基准，包含315个问题，分为三种任务类型：事实检索、成对场景验证和情境推理。该基准用于评估七个波斯语大型语言模型。", "result": "研究发现，大多数模型存在严重的顺从偏差，即倾向于同意（即使是不当行为）。持续的波斯语预训练并未改善推理能力，反而可能加剧顺从偏差。在事实知识检索和场景应用之间，模型性能存在21%的差距。", "conclusion": "目前的波斯语大型语言模型在掌握文化能力方面存在局限，它们更多地是模仿文化模式，而非内化其底层规则。单纯增加单语数据量不足以解决这些问题，需要开发新的方法来培养模型的文化推理能力。"}}
{"id": "2602.17653", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17653", "abs": "https://arxiv.org/abs/2602.17653", "authors": ["Iskar Deng", "Nathalia Xu", "Shane Steinert-Threlkeld"], "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking", "comment": "15 pages, 7 figures, 7 tables. Under review", "summary": "Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.", "AI": {"tldr": "语言模型在合成语料库上训练时，在区分性论元标记（DAM）方面表现出与人类语言相似的标记方向偏好，但未重现人类语言中对象优先的现象，表明不同类型学倾向可能源于不同根源。", "motivation": "探索语言模型在合成语料库上训练时，是否能重现人类语言中关于区分性论元标记（DAM）的跨语言规律，以及其与人类语言在DAM上的差异。", "method": "使用受控合成学习方法，在18个包含不同DAM系统的合成语料库上训练GPT-2模型，并使用最小对（minimal pairs）评估模型的泛化能力。", "result": "模型能够可靠地表现出与人类相似的自然标记方向偏好，即倾向于为语义上非典型的论元添加显式标记。然而，模型未能重现人类语言中对象优先的现象，即显式标记更多地针对对象而非主语。", "conclusion": "语言模型在DAM上的表现表明，不同类型学倾向可能源于不同的潜在机制。尽管模型能学习到某些与人类相似的偏好，但其在对象优先方面与人类语言存在差异，暗示了人类语言习得中可能存在额外的、模型未捕获的因素。"}}
{"id": "2602.17535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17535", "abs": "https://arxiv.org/abs/2602.17535", "authors": ["Behzad Bozorgtabar", "Dwarikanath Mahapatra", "Sudipta Roy", "Muzammal Naseer", "Imran Razzak", "Zongyuan Ge"], "title": "LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs", "comment": "18 pages, 6 figures, 4 tables", "summary": "Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \\texttt{\\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \\textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \\textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \\texttt{\\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \\textbf{three} medical VLMs and \\textbf{nine} downstream tasks, \\texttt{\\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \\texttt{\\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.", "AI": {"tldr": "本文提出了一种名为 LATA 的训练和标签无关的后处理方法，用于改进医学视觉语言模型（VLMs）在领域转移下的不确定性校准。LATA 通过在图像-图像 k-NN 图上平滑零样本概率来减小预测集大小和类别条件覆盖率差距（CCV），同时保持目标覆盖率，并且计算效率高。", "motivation": "现有的医学 VLM 在领域转移下可靠性不足，需要有保证的校准不确定性。然而，传统的分割共形预测（SCP）在样本效率和类别平衡方面存在问题，并且直接应用于校准标签会破坏可交换性。因此，需要一种在不破坏保证的前提下，改进 SCP 在医学 VLM 中不确定性校准的方法。", "method": "LATA 是一种训练和标签无关的后处理方法。它通过构建一个图像-图像 k-NN 图，并在该图上使用少数 CCCP 均值场更新来平滑零样本概率，从而实现对联合校准和测试池的精炼。此外，LATA 还引入了一种“故障感知”的共形分数，可以集成到 ViLU 框架中，以提高预测集的效率和类别平衡。LATA 可以完全标签无关运行，也可以选择性地使用一次校准边缘信息。", "result": "在三个医学 VLM 和九个下游任务上的实验表明，LATA 能够持续减小预测集大小和 CCV，同时保持或收紧目标覆盖率。与现有的跨域基线方法相比，LATA 表现更优，并且缩小了与使用标签的方法之间的差距，同时计算成本远低于后者。", "conclusion": "LATA 是一种有效的、无需训练和标签的医学 VLM 不确定性校准方法，它能够显著提高零样本预测的效率和类别平衡，同时保持其有效性，并且计算成本低廉。"}}
{"id": "2602.17145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17145", "abs": "https://arxiv.org/abs/2602.17145", "authors": ["Joseph Bingham", "Sam Helmich"], "title": "Bonsai: A Framework for Convolutional Neural Network Acceleration Using Criterion-Based Pruning", "comment": "16 pages, 4 figures, accepted to MLDM 2021", "summary": "As the need for more accurate and powerful Convolutional Neural Networks (CNNs) increases, so too does the size, execution time, memory footprint, and power consumption. To overcome this, solutions such as pruning have been proposed with their own metrics and methodologies, or criteria, for how weights should be removed. These solutions do not share a common implementation and are difficult to implement and compare. In this work, we introduce Combine, a criterion- based pruning solution and demonstrate that it is fast and effective framework for iterative pruning, demonstrate that criterion have differing effects on different models, create a standard language for comparing criterion functions, and propose a few novel criterion functions. We show the capacity of these criterion functions and the framework on VGG inspired models, pruning up to 79\\% of filters while retaining or improving accuracy, and reducing the computations needed by the network by up to 68\\%.", "AI": {"tldr": "本研究提出了一种名为Combine的基于标准的模型剪枝框架，该框架能够快速有效地迭代剪枝，并提出了一种比较标准函数的新语言，同时还提出了一些新的标准函数。研究结果表明，该框架可以在保留甚至提高准确率的同时，大幅减少VGG类模型的计算量。", "motivation": "现有的卷积神经网络（CNN）模型在追求更高准确率和更强性能的同时，也带来了模型尺寸、执行时间、内存占用和功耗的增加。为了解决这些问题，剪枝等方法被提出，但它们缺乏统一的实现和比较标准，难以进行有效的实施和对比。", "method": "本研究提出了一种名为Combine的基于标准的剪枝框架，旨在提供一个统一、快速且有效的迭代剪枝解决方案。该框架通过提出一种标准的语言来比较不同的剪枝标准函数，并引入了几种新的标准函数。", "result": "在VGG类模型上，所提出的Combine框架能够实现高达79%的滤波器剪枝，同时保留甚至提高了模型的准确率。此外，该框架还能够将网络的计算量减少高达68%。研究还表明，不同的标准函数对不同的模型具有不同的影响。", "conclusion": "Combine框架是一个快速有效的迭代剪枝框架，它通过提供一种标准的比较语言和引入新颖的标准函数，解决了现有剪枝方法难以比较和实现的问题。该框架在VGG类模型上展现了强大的性能，能够显著减小模型尺寸和计算量，同时保持或提高准确率。"}}
{"id": "2602.17558", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17558", "abs": "https://arxiv.org/abs/2602.17558", "authors": ["Qiucheng Wu", "Jing Shi", "Simon Jenni", "Kushal Kafle", "Tianyu Wang", "Shiyu Chang", "Handong Zhao"], "title": "RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward", "comment": "10 pages, 6 figures", "summary": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.", "AI": {"tldr": "本文提出RetouchIQ框架，利用多模态大语言模型（MLLM）和强化学习（RL），通过一个通用的奖励模型来实现基于指令的图像编辑，解决了传统奖励信号不足的挑战，并在新数据集上取得了显著的性能提升。", "motivation": "现有基于MLLM的图像编辑方法在专业软件中执行工具使用计划时面临挑战，主要原因是缺乏能够反映创意编辑主观性的可靠、可验证的奖励信号。", "method": "RetouchIQ框架通过MLLM代理，在通用的奖励模型指导下执行指令驱动的图像编辑。该框架能够解析用户的编辑意图，生成可执行的图像调整。其核心是提出一个通用的奖励模型，该模型是一个经过RL微调的MLLM，能够根据生成的一系列度量标准逐案评估编辑结果，并提供多模态推理下的标量反馈，从而实现高质量、与指令一致的梯度强化学习。", "result": "RetouchIQ在19万个指令-推理对的数据集上进行了训练和评估，并在语义一致性和感知质量方面显著优于现有的MLLM和扩散模型方法。研究建立了新的基于指令的图像编辑基准。", "conclusion": "RetouchIQ展示了通用奖励驱动的MLLM代理在专业图像编辑领域作为灵活、可解释和可执行的助手具有巨大潜力。"}}
{"id": "2602.17162", "categories": ["cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.17162", "abs": "https://arxiv.org/abs/2602.17162", "authors": ["Ariel Larey", "Elay Dahan", "Amit Bleiweiss", "Raizy Kellerman", "Guy Leib", "Omri Nayshool", "Dan Ofer", "Tal Zinger", "Dan Dominissini", "Gideon Rechavi", "Nicole Bussola", "Simon Lee", "Shane O'Connell", "Dung Hoang", "Marissa Wirth", "Alexander W. Charney", "Nati Daniel", "Yoli Shavit"], "title": "JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures", "comment": null, "summary": "Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.", "AI": {"tldr": "本文提出了JEPA-DNA，一种结合了联合嵌入预测架构（JEPA）和生成目标的新型基因组学基础模型预训练框架，旨在提升模型对基因组序列的生物学功能理解能力。", "motivation": "现有的基因组学基础模型（GFMs）主要依赖掩码语言模型（MLM）或下一词预测（NTP），这些方法擅长捕捉局部基因组语法和细粒度模式，但在捕获更广泛的功能背景和提供全局生物学视角方面存在不足。", "method": "JEPA-DNA通过将标记级别的恢复与潜在空间中的预测目标相结合，引入潜在接地。它通过监督一个CLS token来迫使模型预测掩码基因组片段的高层功能嵌入，而非仅仅关注单个核苷酸。该框架可以作为独立的从头训练目标，也可以作为对现有GFMs的持续预训练增强。", "result": "在各种基因组学基准测试中，JEPA-DNA在监督学习和零样本任务上均优于纯生成模型基线，展现出更鲁棒和生物学基础更强的表示。", "conclusion": "JEPA-DNA通过提供更具生物学意义的表示，为构建能够理解基因组序列底层功能逻辑的基础模型提供了一条可扩展的途径。"}}
{"id": "2602.17655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17655", "abs": "https://arxiv.org/abs/2602.17655", "authors": ["Clara Meister", "Ahmetcan Yavuz", "Pietro Lesci", "Tiago Pimentel"], "title": "What Language is This? Ask Your Tokenizer", "comment": null, "summary": "Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.", "AI": {"tldr": "本文提出了一种名为 UniLID 的新颖语言识别（LID）方法，该方法基于 UnigramLM 语言模型，能够有效处理低资源和相似语言场景，并且支持增量式添加新语言。", "motivation": "现有 LID 系统在低资源和密切相关的语言场景下性能不稳定（“brittle”），而 UniLID 的目标是解决这一问题，并提高 LID 的效率和灵活性。", "method": "UniLID 利用 UnigramLM 的概率框架，学习共享分词器词汇上的语言条件下的单一词（unigram）分布，并将分词（segmentation）视为语言特有的现象。", "result": "UniLID 在标准基准测试中表现具有竞争力，在低资源场景下样本效率显著提高（仅需每种语言五个标记样本即可达到 70% 以上的准确率），并在细粒度方言识别方面取得显著进步。", "conclusion": "UniLID 是一种简单、高效且灵活的语言识别方法，尤其在低资源和细粒度语言识别任务中展现出优势，并且易于集成到现有的语言模型分词流程中。"}}
{"id": "2602.17189", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17189", "abs": "https://arxiv.org/abs/2602.17189", "authors": ["Sicheng Mao"], "title": "Texo: Formula Recognition within 20M Parameters", "comment": null, "summary": "In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.", "AI": {"tldr": "Texo 是一个参数量仅为 2000 万的极简但高性能公式识别模型，通过注意力的设计、词汇和分词器的蒸馏与迁移，性能可与最先进的模型媲美，同时模型尺寸大幅减小，支持实时推理和浏览器端部署。", "motivation": "研究动机是为了开发一个小型、高效的公式识别模型，能够在消费级硬件甚至浏览器上实现实时推理，降低对计算资源的要求。", "method": "该研究提出了 Texo 模型，其特点是参数量少（2000 万）。通过 attentive design（注意力设计）、distillation（蒸馏）和 transfer of the vocabulary and the tokenizer（词汇和分词器的迁移）来优化模型。", "result": "Texo 模型在性能上可与 UniMERNet-T 和 PPFormulaNet-S 等最先进模型相媲美，但模型尺寸分别减小了 80% 和 65%。", "conclusion": "Texo 模型通过精简设计和优化方法，实现了高性能和小型化的结合，使得公式识别能够轻松部署在资源受限的环境中，并提供了一个用户友好的 Web 应用来展示其能力。"}}
{"id": "2602.17221", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17221", "abs": "https://arxiv.org/abs/2602.17221", "authors": ["Yi-Chih Huang"], "title": "From Labor to Collaboration: A Methodological Experiment Using AI Agents to Augment Research Perspectives in Taiwan's Humanities and Social Sciences", "comment": "also in Chinese", "summary": "Generative AI is reshaping knowledge work, yet existing research focuses predominantly on software engineering and the natural sciences, with limited methodological exploration for the humanities and social sciences. Positioned as a \"methodological experiment,\" this study proposes an AI Agent-based collaborative research workflow (Agentic Workflow) for humanities and social science research. Taiwan's Claude.ai usage data (N = 7,729 conversations, November 2025) from the Anthropic Economic Index (AEI) serves as the empirical vehicle for validating the feasibility of this methodology.\n  This study operates on two levels: the primary level is the design and validation of a methodological framework - a seven-stage modular workflow grounded in three principles: task modularization, human-AI division of labor, and verifiability, with each stage delineating clear roles for human researchers (research judgment and ethical decisions) and AI Agents (information retrieval and text generation); the secondary level is the empirical analysis of AEI Taiwan data - serving as an operational demonstration of the workflow's application to secondary data research, showcasing both the process and output quality (see Appendix A).\n  This study contributes by proposing a replicable AI collaboration framework for humanities and social science researchers, and identifying three operational modes of human-AI collaboration - direct execution, iterative refinement, and human-led - through reflexive documentation of the operational process. This taxonomy reveals the irreplaceability of human judgment in research question formulation, theoretical interpretation, contextualized reasoning, and ethical reflection. Limitations including single-platform data, cross-sectional design, and AI reliability risks are acknowledged.", "AI": {"tldr": "本研究提出了一种用于人文社科研究的AI Agent协作工作流，并通过台湾Claude.ai使用数据进行了验证，旨在填补该领域AI研究方法的不足。", "motivation": "现有关于生成式AI的研究主要集中在软件工程和自然科学领域，而对人文社科领域的方法学探索不足，本研究旨在弥补这一空白。", "method": "研究设计了一个包含七个阶段的模块化AI Agent协作研究工作流，该工作流基于任务模块化、人机分工和可验证性三大原则。研究利用台湾Anthropic经济指数（AEI）的Claude.ai使用数据（7,729次对话）来验证该工作流的可行性，并通过记录操作过程识别出三种人机协作模式：直接执行、迭代优化和人主导。", "result": "研究成功设计并验证了一个AI Agent协作工作流，并识别出三种人机协作模式。结果表明，在研究问题形成、理论解释、情境推理和伦理反思等方面，人类判断仍然是不可替代的。", "conclusion": "本研究提出了一个可复制的人文社科AI协作框架，并揭示了人类在研究中的关键作用，强调了人类判断在特定研究环节的不可替代性。研究也承认了数据单一平台、横断面设计和AI可靠性等局限性。"}}
{"id": "2602.17217", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17217", "abs": "https://arxiv.org/abs/2602.17217", "authors": ["Enrique Crespo-Fernandez", "Oliver Ray", "Telmo de Menezes e Silva Filho", "Peter Flach"], "title": "Continual learning and refinement of causal models through dynamic predicate invention", "comment": null, "summary": "Efficiently navigating complex environments requires agents to internalize the underlying logic of their world, yet standard world modelling methods often struggle with sample inefficiency, lack of transparency, and poor scalability. We propose a framework for constructing symbolic causal world models entirely online by integrating continuous model learning and repair into the agent's decision loop, by leveraging the power of Meta-Interpretive Learning and predicate invention to find semantically meaningful and reusable abstractions, allowing an agent to construct a hierarchy of disentangled, high-quality concepts from its observations. We demonstrate that our lifted inference approach scales to domains with complex relational dynamics, where propositional methods suffer from combinatorial explosion, while achieving sample-efficiency orders of magnitude higher than the established PPO neural-network-based baseline.", "AI": {"tldr": "提出了一种在线构建符号因果世界模型框架，通过集成持续的模型学习和修复，利用元解释学习和谓词发明来发现有意义的抽象，并证明该方法在复杂关系域中比现有基线更具可扩展性和样本效率。", "motivation": "现有世界建模方法在样本效率、透明度和可扩展性方面存在不足，难以高效地让智能体理解复杂环境的内在逻辑。", "method": "利用元解释学习（Meta-Interpretive Learning）和谓词发明（predicate invention）技术，将连续模型学习和修复集成到智能体的决策循环中，从而在线构建符号因果世界模型，并学习解耦的高质量概念层次。", "result": "所提出的方法在具有复杂关系动力学的域中实现了可扩展的提升推理，克服了命题方法带来的组合爆炸问题，并在样本效率方面比 PPO 基线高出几个数量级。", "conclusion": "该框架能够高效地构建出具有语义意义且可重用的抽象，使智能体能够构建高质量的概念层次，从而在复杂环境中实现有效的导航，并且比现有的基于神经网络的方法在样本效率和可扩展性上具有显著优势。"}}
{"id": "2602.17605", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17605", "abs": "https://arxiv.org/abs/2602.17605", "authors": ["Jowaria Khan", "Anindya Sarkar", "Yevgeniy Vorobeychik", "Elizabeth Bondi-Kelly"], "title": "Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery", "comment": null, "summary": "In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.", "AI": {"tldr": "提出一个整合主动学习、在线元学习和概念引导推理的地理空间发现框架，通过概念相关性加权不确定性采样和元批次形成策略，在数据稀疏和环境动态变化的情况下有效发现目标。", "motivation": "在环境监测、灾害响应等真实场景下，数据收集成本高昂且环境动态变化，需要在资源有限的情况下，通过对未观测区域进行策略性采样来高效发现隐藏目标，但现有的基于学习的方法（如强化学习）受限于稀疏和有偏的地理空间真实数据。", "method": "提出一个统一的地理空间发现框架，整合了主动学习、在线元学习和概念引导推理。引入了两个核心创新：1. 概念加权不确定性采样策略：利用领域特定概念（如土地覆盖、源头邻近度）学习到的相关性来调节不确定性。2. 相关性感知元批次形成策略：在在线元学习更新过程中促进语义多样性，以提高在动态环境下的泛化能力。", "result": "实验在真实世界的致癌PFAS（全氟和多氟烷基物质）污染数据集上进行测试，结果表明该方法在数据有限和环境变化的情况下，能够可靠地发现目标。", "conclusion": "所提出的统一地理空间发现框架，通过引入概念相关性，能够有效解决数据稀疏和环境动态变化带来的挑战，在资源受限的情况下实现高效的目标发现。"}}
{"id": "2602.17636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17636", "abs": "https://arxiv.org/abs/2602.17636", "authors": ["Jiyoung Kim", "Youngjin Shin", "Siyoon Jin", "Dahyun Chung", "Jisu Nam", "Tongmin Kim", "Jongjae Park", "Hyeonwoo Kang", "Seungryong Kim"], "title": "CORAL: Correspondence Alignment for Improved Virtual Try-On", "comment": "32 pages, 25 figures", "summary": "Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.", "AI": {"tldr": "本文提出了一种基于Diffusion Transformer（DiT）的虚拟试穿（VTON）框架CORAL，通过显式地对齐查询-键匹配与外部对应关系，来解决现有方法在服装细节保持和非配对设置下的对齐问题。", "motivation": "现有VTON方法难以在非配对设置下保持服装细节，并且未解释DiT架构中对应关系如何产生，也未显式强制进行人-衣对齐。", "method": "作者首先分析了DiT中的3D注意力机制，发现人-衣对应关系依赖于查询-键的精确匹配。在此基础上，提出了CORAL框架，包含两个组件：1) 对应关系蒸馏损失，用于将可靠的匹配与人-衣注意力对齐；2) 熵最小化损失，用于锐化注意力分布。此外，还提出了一种基于视觉语言模型（VLM）的评估协议。", "result": "CORAL框架在全局形状转移和局部细节保持方面均优于基线方法。消融实验验证了所设计组件的有效性。", "conclusion": "通过显式对齐查询-键匹配与外部对应关系，CORAL框架能够有效地解决非配对VTON场景下的人-衣对齐问题，并显著提升服装细节的保持效果。提出的VLM评估协议更能反映人类偏好。"}}
{"id": "2602.17599", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.17599", "abs": "https://arxiv.org/abs/2602.17599", "authors": ["Ivan Rinaldi", "Matteo Mendula", "Nicola Fanelli", "Florence Levé", "Matteo Testi", "Giovanna Castellano", "Gennaro Vessio"], "title": "Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment", "comment": null, "summary": "Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.", "AI": {"tldr": "该研究提出了ArtSound数据集和ArtToMus框架，实现了艺术品到音乐的直接生成，克服了现有系统依赖自然照片和图像转文本的局限性。", "motivation": "现有图像条件音乐生成系统主要基于自然照片训练，且依赖图像转文本的中间步骤，限制了其对艺术品丰富语义、风格和文化内容的捕捉能力，且无法实现直接的视觉到音频学习。", "method": "构建了包含105,884对艺术品-音乐的ArtSound数据集；提出了ArtToMus框架，该框架直接将艺术品视觉嵌入映射到潜在扩散模型的条件空间，实现仅通过视觉信息进行音乐合成，无需文本转换。", "result": "ArtToMus生成的音乐在连贯性和风格上与源艺术品具有高度一致性，并能反映艺术品的显著视觉特征。尽管在绝对对齐分数上低于文本条件系统，但其感知质量和跨模态对应性具有竞争力。", "conclusion": "该研究成功地将直接艺术品到音乐生成作为一个新的、具有挑战性的研究方向，并提供了支持多媒体艺术、文化遗产和AI辅助创作实践所需的数据集和框架。"}}
{"id": "2602.17222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17222", "abs": "https://arxiv.org/abs/2602.17222", "authors": ["Ben Yellin", "Ehud Ezra", "Mark Foreman", "Shula Grinapol"], "title": "Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight", "comment": null, "summary": "Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.", "AI": {"tldr": "本文提出了一种名为大型行为模型（LBM）的新型AI模型，用于高风险环境下人类决策的精确预测。LBM通过行为嵌入而非瞬时提示来工作，能够整合详细的心理特征和情境约束，从而实现比现有大型语言模型（LLMs）更稳定和个体化的行为预测。", "motivation": "现有的大型语言模型（LLMs）在模拟人类在高风险环境下的决策行为时存在不足，尤其是在需要考虑个体心理特征和情境约束的复杂交互时，容易出现行为不一致和“身份漂移”的问题。基于提示的方法也存在局限性。", "method": "研究者提出了一种大型行为模型（LBM），它是一种行为基础模型。LBM通过将结构化的、高维度的个体心理特征（源自全面的心理测量）作为输入，而非传统的提示，来学习行为嵌入。模型在一个包含个体稳定特质、动机状态、情境约束和实际选择的专有数据集上进行训练。", "result": "在未见过的情境评估中，LBM的微调在行为预测方面优于未调整的基础模型（Llama-3.1-8B-Instruct）。当仅使用“大五”人格特质进行条件约束时，LBM的表现与前沿基线相当。研究还发现，与基于提示的方法存在性能上限不同，LBM能够从越来越密集的特征描述中持续获益，提供更多的特征维度可以提高模型性能。", "conclusion": "LBM是一种可扩展的高保真行为模拟方法，克服了现有LLMs在个体化行为预测方面的局限性。该模型在战略远见、谈判分析、认知安全和决策支持等领域具有广泛的应用潜力。"}}
{"id": "2602.17639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17639", "abs": "https://arxiv.org/abs/2602.17639", "authors": ["Pourya Shamsolmoali", "Masoumeh Zareapoor", "Eric Granger", "Yue Lu"], "title": "IntRec: Intent-based Retrieval with Contrastive Refinement", "comment": null, "summary": "Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.", "AI": {"tldr": "本文提出了一种名为IntRec的交互式物体检索框架，通过利用用户反馈来精炼检测结果，解决了现有方法在处理模糊或相似物体时的不足。", "motivation": "现有开放词汇检测器无法根据用户反馈进行预测修正，在处理模糊或相似物体时性能受限。", "method": "IntRec框架的核心是一个“意图状态”（Intent State），包含正例线索（已确认的线索）和负例约束（被拒绝的假设）。通过对比对齐函数，最大化候选物体与正例线索的相似性，同时惩罚负例约束，从而实现细粒度的消歧。", "result": "IntRec在LVIS数据集上达到了35.4 AP，优于OVMR、CoDet和CAKE。在LVIS-Ambiguous基准测试中，单次纠正反馈后，性能提升了+7.9 AP，交互延迟不到30毫秒。", "conclusion": "IntRec通过交互式反馈显著提高了物体检索的准确性，无需额外监督，尤其在处理复杂场景下的模糊或相似物体时表现出色。"}}
{"id": "2602.17229", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17229", "abs": "https://arxiv.org/abs/2602.17229", "authors": ["Bianca Raimondi", "Maurizio Gabbrielli"], "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy", "comment": "Preprint. Under review", "summary": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.", "AI": {"tldr": "本研究通过分析大型语言模型（LLMs）的内部神经激活，发现不同认知难度（基于布鲁姆分类法）的提示在模型表示中具有线性可分的特征，且这种可分性在模型的前向传播早期和后续层中逐渐增强。", "motivation": "大型语言模型的黑箱性质促使研究者开发新的评估框架，以超越表面性能指标，深入理解其内部运作机制。本研究旨在探索LLMs如何在其内部神经表示中编码认知复杂度。", "method": "研究者利用布鲁姆分类法（Bloom's Taxonomy）作为分级标准，通过分析不同LLMs的高维激活向量，探究不同认知层级（从记忆到创造）是否能在模型的残差流（residual streams）中进行线性分离。研究中使用了线性分类器来评估认知层级的可分性。", "result": "研究结果显示，线性分类器在所有布鲁姆认知层级上平均准确率约为95%。这有力地证明了认知层级信息编码在模型表示的一个线性可及子空间中。同时，研究表明模型在前向传播早期就已解决了提示的认知难度，并且随着层数的增加，表示的区分度也随之提高。", "conclusion": "本研究的发现表明，大型语言模型在其内部表示中以线性可分的方式编码了认知复杂度。模型倾向于在早期阶段就解决提示的认知难度，并且这种认知信息的区分度在模型的后续层中会进一步增强。"}}
{"id": "2602.17234", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17234", "abs": "https://arxiv.org/abs/2602.17234", "authors": ["Zeyu Zhang", "Ryan Chen", "Bradly C. Stadie"], "title": "All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting", "comment": "8 pages plus appendix", "summary": "To evaluate whether LLMs can accurately predict future events, we need the ability to \\textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \\emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \\textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \\textbf{Shapley}-weighted \\textbf{D}ecision-\\textbf{C}ritical \\textbf{L}eakage \\textbf{R}ate (\\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \\textbf{Time}-\\textbf{S}upervised \\textbf{P}rediction with \\textbf{E}xtracted \\textbf{C}laims (\\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.", "AI": {"tldr": "研究提出了一种名为Shapley-DCLR的框架，用于检测和量化大型语言模型（LLM）在预测未来事件时，由于训练数据泄露而导致的“时间知识泄露”。在此基础上，开发了TimeSPEC方法，通过验证和过滤与预测相关的声明来主动消除泄露，并在实际任务中证明了其有效性。", "motivation": "评估LLM预测未来事件的能力需要能够回溯性地进行测试，但LLM可能会泄露训练过程中编码的、截至特定过去日期之后的信息，从而影响回溯评估的有效性。", "method": "提出了一种基于声明（claim）的框架，将模型推理分解为可验证的声明，并使用Shapley值来衡量每个声明对预测的贡献。由此得到Shapley-DCLR指标，用于量化由泄露信息驱动的推理比例。在此基础上，提出TimeSPEC方法，通过在生成过程中交替进行声明验证和重新生成，来主动过滤时间污染。", "result": "在U.S. Supreme Court案件预测、NBA薪资估计和股票回报排名等350个实例的实验中，发现标准的提示方法存在显著的时间知识泄露。TimeSPEC在保持任务性能的同时降低了Shapley-DCLR。", "conclusion": "显式的、可解释的声明级验证比基于提示的时间约束更能有效防止LLM的时间知识泄露，从而实现更可靠的回溯测试。"}}
{"id": "2602.17650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17650", "abs": "https://arxiv.org/abs/2602.17650", "authors": ["Tyler Bonnen", "Jitendra Malik", "Angjoo Kanazawa"], "title": "Human-level 3D shape perception emerges from multi-view learning", "comment": null, "summary": "Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.", "AI": {"tldr": "研究提出了一种新的神经网络模型，能够直接从2D图像预测3D物体形状，并在3D感知任务中达到人类水平的准确度，无需任务特定训练，并且能模拟人类行为的细微差异。", "motivation": "传统的计算方法在模拟人类从2D视觉输入推断3D结构的能力方面表现不佳，作者旨在开发一种新的模型框架来解决这一长期存在的挑战。", "method": "使用一种新颖的神经网络，通过在自然场景数据上训练一个视觉-空间目标函数来预测相机位置和视觉深度等空间信息，不依赖于物体本身的归纳偏见。采用零样本评估方法来测试模型在3D感知任务上的表现。", "result": "该模型在3D形状推断任务上达到了人类水平的准确度，无需进行任务特定的训练或微调。模型响应的独立读数能够预测人类行为的细微之处，如误差模式和反应时间。", "conclusion": "研究表明，人类水平的3D感知能力可以从对自然视觉-空间数据进行简单的、可扩展的学习目标中涌现出来。该框架为理解和模拟人类3D感知提供了一种有效的方法。"}}
{"id": "2602.17288", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17288", "abs": "https://arxiv.org/abs/2602.17288", "authors": ["Anuj Gupta"], "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training", "comment": "15 pages, 6 figures, 1 table", "summary": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.", "AI": {"tldr": "该研究详细介绍了一个使用 arXiv 的原始 LaTeX 论文训练一个 1.36B 参数的科学语言模型的端到端流程，分析了预处理、分词、基础设施等因素对模型训练的影响，旨在为计算资源有限的研究者提供实践指导。", "motivation": "尽管前沿的大型语言模型在推理和数学方面表现出色，但从原始数据训练领域专业科学语言模型的过程记录不足，尤其是在计算资源有限的情况下。本研究旨在填补这一空白，提供一个详细的案例研究。", "method": "研究者构建了一个端到端的训练流程，包括元数据过滤、文档验证、LaTeX 提取、文本规范化、领域感知分词以及在有限计算资源（2xA100 GPU）下进行密集 Transformer 训练。通过 24 次实验，分析了训练稳定性、扩展行为、数据产量损失和基础设施瓶颈。", "result": "研究发现，预处理决策显著影响可用 token 的数量；分词方式影响符号的稳定性；存储和 I/O 限制与计算能力同样是关键瓶颈。在数据丰富的情况下（52B 预训练 token），模型显示出稳定的训练行为和收敛动态。", "conclusion": "本研究提供了一个工程化、透明的从头开始训练小型科学语言模型的详细过程。研究结果表明，在计算资源有限的情况下，预处理、分词和基础设施的优化与模型架构本身同等重要，为希望构建领域专业模型的低预算研究者提供了宝贵的实践经验。"}}
{"id": "2602.17665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17665", "abs": "https://arxiv.org/abs/2602.17665", "authors": ["Akashah Shabbir", "Muhammad Umer Sheikh", "Muhammad Akhtar Munir", "Hiyam Debary", "Mustansar Fiaz", "Muhammad Zaigham Zaheer", "Paolo Fraccaro", "Fahad Shahbaz Khan", "Muhammad Haris Khan", "Xiao Xiang Zhu", "Salman Khan"], "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents", "comment": null, "summary": "Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.", "AI": {"tldr": "OpenEarthAgent 是一个统一的框架，用于开发基于卫星图像、自然语言查询和详细推理轨迹训练的地理空间代理，以弥合多模态推理在遥感领域的差距。", "motivation": "现有的多模态推理模型在处理遥感数据时面临挑战，因为它们需要处理空间尺度、地理结构、多光谱指数以及多步逻辑推理。这项研究旨在通过开发一个专门的框架来解决这一问题。", "method": "该研究提出OpenEarthAgent，一个工具增强的地理空间代理框架。它通过在卫星图像、自然语言查询和详细推理轨迹上进行监督微调来训练。训练过程依赖于结构化的推理轨迹，使模型能够与经过验证的多步工具交互保持一致。研究还构建了一个包含14,538个训练实例和1,169个评估实例的语料库。", "result": "OpenEarthAgent 证明了结构化推理、稳定的空间理解和可解释的行为。与强基线相比，在各种条件下，通过工具驱动的地理空间交互，代理取得了持续的改进，并取得了与近期开源和闭源模型相当的性能。", "conclusion": "OpenEarthAgent 提供了一个有效的框架，用于开发能够理解和推理遥感数据的地理空间代理，从而在城市、环境、灾害和基础设施等领域实现复杂的分析任务。"}}
{"id": "2602.17245", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17245", "abs": "https://arxiv.org/abs/2602.17245", "authors": ["Linxi Jiang", "Rui Xi", "Zhijie Liu", "Shuo Chen", "Zhiqiang Lin", "Suman Nath"], "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web", "comment": null, "summary": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \\textbf{reliability} by providing stable interfaces, \\textbf{efficiency} by reducing dozens of steps into a few function calls, and \\textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.", "AI": {"tldr": "该论文提出了一种名为“Web Verbs”的接口，旨在为基于LLM的Web代理提供一个统一、语义化的操作层，以提高其可靠性、效率和可验证性。", "motivation": "现有Web代理依赖低级的点击和键盘操作，这些操作脆弱、低效且难以验证。LLM的发展使得自然语言成为可行的接口，但需要更高级别的抽象来支持目标导向的任务。", "method": "提出“Web Verbs”的概念，将其设计为一种类型化、带语义文档的函数集合，通过统一接口暴露网站功能。这些Verbs可以包含前置条件、后置条件、策略标签和日志支持，并能通过API或客户端工作流实现。LLM可以发现、选择并组合这些Verbs来生成程序。", "result": "通过概念验证实现和案例研究，证明了Web Verbs能够生成比现有代理更简洁、更鲁棒的工作流。Verbs的引入提高了接口的稳定性，将大量步骤简化为少数函数调用，并通过类型契约和可检查的轨迹增强了可验证性。", "conclusion": "Web Verbs提供了一个统一的API和浏览器界面范式，使LLM能够合成可靠且可审计的工作流。论文还提出了一个标准化路线图，以期在Web规模上部署和信任这些Verbs。"}}
{"id": "2602.17308", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17308", "abs": "https://arxiv.org/abs/2602.17308", "authors": ["Hui Min Wong", "Philip Heesen", "Pascal Janetzky", "Martin Bendszus", "Stefan Feuerriegel"], "title": "MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions", "comment": null, "summary": "Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.", "AI": {"tldr": "本文提出了一种名为MedClarify的AI代理，用于在医学诊断中通过生成后续问题来减少不确定性，实验表明其诊断错误率比标准LLM基线低约27个百分点。", "motivation": "现有的大型语言模型（LLMs）在医学诊断任务中存在局限性，尤其是在信息不完整或存在多种可能性诊断时，其推理能力不足，无法像人类医生那样通过迭代提问来减少诊断不确定性。", "method": "MedClarify通过计算候选诊断列表（类似于鉴别诊断），然后主动生成旨在减少诊断不确定性的后续问题。它利用信息论方法，选择预期信息增益最高的问题，实现有针对性的、面向不确定性的推理。", "result": "实验证明，MedClarify生成的后续提问能有效减少诊断错误，与标准的单次LLM基线相比，诊断错误率降低了约27个百分点。同时，研究也指出了当前LLMs在面对不完整病例和信息缺失时，容易产生多个可能性相似的诊断。", "conclusion": "MedClarify通过代理式信息搜集，为改进医学LLMs提供了一种途径，能够更好地模拟真实临床推理的迭代性和不确定性，从而促进与医学LLMs进行更有效的对话，提升诊断决策能力。"}}
{"id": "2602.17544", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17544", "abs": "https://arxiv.org/abs/2602.17544", "authors": ["Shashank Aggarwal", "Ram Vikas Mishra", "Amit Awekar"], "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability", "comment": null, "summary": "In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.", "AI": {"tldr": "本文提出重用性和可验证性两个新指标来评估多智能体信息检索（IR）管道中基于LLM的智能体间的Chain-of-Thought（CoT）推理过程的质量，发现现有准确性指标无法衡量推理过程本身，且重用性和可验证性与准确性不相关，并揭示了专业推理模型CoT的重用性和可验证性并不优于通用LLM。", "motivation": "现有CoT评估方法仅关注目标任务准确性，忽视了推理过程本身的质量和效用。这导致对多智能体IR管道中LLM智能体间推理能力的评估存在盲点。", "method": "提出使用Thinker-Executor框架解耦CoT生成与执行。引入两个新指标：重用性（衡量Executor复用Thinker的CoT的难易程度）和可验证性（衡量Executor使用CoT匹配Thinker答案的频率）。在五个基准测试上，评估了四个Thinker模型和十个Executor模型。", "result": "重用性和可验证性与标准准确性不相关。专业推理模型的CoT不比通用LLM（如Llama和Gemma）的CoT更具可重用性或可验证性。", "conclusion": "仅关注准确性指标无法全面评估多智能体IR管道中LLM推理能力。重用性和可验证性是衡量CoT质量的重要维度，且通用LLM在这些方面的表现不逊于专业模型，表明当前评估体系存在不足。"}}
{"id": "2602.17385", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17385", "abs": "https://arxiv.org/abs/2602.17385", "authors": ["Angelo Porrello", "Pietro Buzzega", "Felix Dangel", "Thomas Sommariva", "Riccardo Salami", "Lorenzo Bonicelli", "Simone Calderara"], "title": "Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature", "comment": "Accepted to ICLR 2026", "summary": "Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.", "AI": {"tldr": "提出了一种无需额外数据的方法来解决任务算术中的交叉任务干扰问题，通过将正则化视为曲率矩阵近似问题，并利用 Kronecker-Factored Approximate Curvature 实现了在任务添加和否定方面的最先进结果。", "motivation": "现有的任务算术方法在组合多个任务向量时会产生交叉任务干扰，导致性能下降。现有的解决方案（表示漂移正则化）需要外部任务数据，这与模块化和数据可用性（如隐私）的约束相冲突。", "method": "将正则化问题转化为曲率矩阵近似问题，并采用 Kronecker-Factored Approximate Curvature (K-FAC) 来计算近似曲率矩阵，从而构建一个无需数据的正则化器。", "result": "在任务添加和任务否定任务上取得了最先进的结果，并且具有相对于任务数量恒定的计算复杂度，同时还能保持对任务向量重缩放的鲁棒性。", "conclusion": "该方法通过一种无需数据的方式有效地解决了任务算术中的交叉任务干扰问题，实现了模块化、可扩展性，并且无需额外的调优过程。"}}
{"id": "2602.17547", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17547", "abs": "https://arxiv.org/abs/2602.17547", "authors": ["Yue Liu", "Zhiyuan Hu", "Flood Sung", "Jiaheng Zhang", "Bryan Hooi"], "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks", "comment": null, "summary": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.", "AI": {"tldr": "本文提出KLong，一个专为解决超长时序任务而训练的开源LLM智能体，通过轨迹切分SFT冷启动，再用渐进式RL训练进行扩展。", "motivation": "现有LLM在解决超长时序任务时存在挑战，需要一种能够有效处理长依赖和复杂决策过程的模型。", "method": "采用两阶段训练：1. 轨迹切分SFT（保留早期上下文，逐步截断后期上下文，并保持子轨迹重叠）进行冷启动。2. 渐进式RL（分阶段训练，逐步延长超时时间）进行扩展。引入Research-Factory自动化流水线生成高质量训练数据。", "result": "KLong在PaperBench上超越Kimi K2 Thinking 11.28%，且性能泛化到SWE-bench Verified和MLE-bench等编码基准。", "conclusion": "KLong通过创新的SFT和RL训练方法，在解决超长时序任务方面展现出卓越的性能和泛化能力。"}}
{"id": "2602.17664", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17664", "abs": "https://arxiv.org/abs/2602.17664", "authors": ["Aidar Myrzakhan", "Tianyi Li", "Bowei Guo", "Shengkun Tang", "Zhiqiang Shen"], "title": "Sink-Aware Pruning for Diffusion Language Models", "comment": "Code at: https://github.com/VILA-Lab/Sink-Aware-Pruning", "summary": "Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.", "AI": {"tldr": "本文提出了一种名为 Sink-Aware Pruning 的新方法，用于高效地修剪扩散语言模型（DLMs）。与以往主要继承自自回归（AR）语言模型的模型不同，该方法识别并移除了 DLMs 中不稳定的注意力“沉没”token，从而在不重新训练的情况下提高了模型质量和效率的权衡。", "motivation": "扩散语言模型（DLMs）由于其迭代去噪过程而具有很高的推理成本，这促使研究人员寻找更高效的修剪方法。现有的修剪方法大多继承自自回归（AR）语言模型，通常会保留注意力“沉没”token，因为它们在 AR 模型中充当稳定的全局锚点。然而，这种假设并不适用于 DLMs。", "method": "作者发现，在 DLMs 中，注意力“沉没”token 的位置在完整的生成轨迹中表现出显著更高的方差，这意味着“沉没”token 往往是暂时的，并且不像在 AR 模型中那样具有结构上的重要性。基于这一观察，研究者提出了 Sink-Aware Pruning 方法，该方法可以自动识别并修剪 DLMs 中不稳定的“沉没”token。", "result": "在不重新训练的情况下，Sink-Aware Pruning 方法实现了比现有修剪基线更好的质量-效率权衡，并在匹配的计算量下取得了优于先前方法的性能。", "conclusion": "Sink-Aware Pruning 是一种有效的 DLMs 修剪策略，它通过识别和移除不稳定的注意力“沉没”token，显著提高了模型的效率，同时保持了模型性能，为 DLMs 的高效推理提供了新的解决方案。"}}
{"id": "2602.17663", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17663", "abs": "https://arxiv.org/abs/2602.17663", "authors": ["Juri Opitz", "Corina Raclé", "Emanuela Boros", "Andrianos Michail", "Matteo Romanello", "Maud Ehrmann", "Simon Clematide"], "title": "CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts", "comment": "ECIR 2026. CLEF Evaluation Lab. Registration DL: 2026/04/23. Task Homepage at https://hipe-eval.github.io/HIPE-2026/", "summary": "HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (\"Has the person ever been at this place?\") and $isAt$ (\"Is the person located at this place around publication time?\") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.", "AI": {"tldr": "HIPE-2026 是一个专注于从嘈杂的多语言历史文本中提取人地关系（包括“曾在某地”和“出版时身处某地”）的 CLEF 评估活动，并评估系统的准确性、效率和泛化能力。", "motivation": "为了促进历史文本中人地关系提取的研究，特别是在处理嘈杂、多语言的历史数据时，并为知识图谱构建、历史人物传记重建和数字人文领域的空间分析等下游应用提供支持。", "method": "组织了一项 CLEF 评估活动（HIPE-2026），要求参赛系统从多语言历史文本中识别两种类型的人地关系（'at' 和 'isAt'），并推理 temporal 和 geographical 线索。评估指标包括准确性、计算效率和领域泛化能力。", "result": "该活动旨在通过连接关系提取和大规模历史数据处理，来评估不同系统在人地关系提取任务上的表现，并对其准确性、效率和泛化能力进行综合评估。", "conclusion": "HIPE-2026 活动通过多语言、跨时段的人地关系提取任务，推动了数字人文领域的技术发展，并为知识图谱构建和历史研究等应用提供了基础。"}}
{"id": "2602.17386", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17386", "abs": "https://arxiv.org/abs/2602.17386", "authors": ["Adrià Molina", "Oriol Ramos Terrades", "Josep Lladós"], "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval", "comment": "Submitted for ICPR Review", "summary": "Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.", "AI": {"tldr": "本研究提出了一种将形式化验证与深度学习图像检索相结合的新框架，通过图方法和神经代码生成来支持开放词汇的自然语言查询，并实现可信赖且可验证的检索结果。", "motivation": "当前的嵌入式模型和预训练模型在处理涉及复杂关系、对象组合或精确约束（如身份、计数、比例）的查询时存在不足，导致检索结果不可靠。", "method": "提出一个新框架，将形式化验证与深度学习图像检索相结合，采用图式验证方法和神经代码生成。", "result": "该方法支持开放词汇的自然语言查询，并产生可信赖和可验证的结果。它通过形式化推理来验证查询中的每个原子事实与检索内容是否匹配，并能明确指出哪些约束已满足，哪些未满足。", "conclusion": "通过将形式化验证集成到深度学习检索中，该框架超越了向量表示的模糊性和近似性，提供了一个更透明、可追溯的检索过程，并提升了现有嵌入式方法的性能。"}}
{"id": "2602.17442", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17442", "abs": "https://arxiv.org/abs/2602.17442", "authors": ["Marco Avolio", "Potito Aghilar", "Sabino Roccotelli", "Vito Walter Anelli", "Chiara Mallamaci", "Vincenzo Paparella", "Marco Valentini", "Alejandro Bellogín", "Michelantonio Trizio", "Joseph Trotta", "Antonio Ferrara", "Tommaso Di Noia"], "title": "WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation", "comment": null, "summary": "Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/", "AI": {"tldr": "WarpRec是一个高性能框架，旨在解决推荐系统研究中实验效率和工业部署之间的权衡问题，提供后端无关的架构，支持超过50种算法和多种评估策略，并集成能源追踪功能，同时为面向Agentic AI的下一代推荐系统奠定基础。", "motivation": "推荐系统研究生态系统的碎片化阻碍了创新，研究人员需要在易于使用的内存实验和成本高昂、复杂且需要重写的分布式工业引擎之间做出选择。", "method": "提出了WarpRec框架，采用新颖的后端无关架构，支持本地到分布式训练和优化的无缝迁移，集成了50+种最先进算法、40种评估指标以及19种过滤和拆分策略。此外，集成了CodeCarbon进行实时能源追踪。", "result": "WarpRec成功弥合了学术界和工业界之间的差距，并展示了其可扩展性不以牺牲科学严谨性或可持续性为代价。它为面向Agentic AI的推荐系统提供了架构基础。", "conclusion": "WarpRec不仅弥合了学术界和工业界之间的差距，而且可以作为下一代可持续、面向Agent的推荐系统的架构骨干。"}}
{"id": "2602.17508", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17508", "abs": "https://arxiv.org/abs/2602.17508", "authors": ["Pranay Jain", "Maximilian Kasper", "Göran Köber", "Axel Plinge", "Dominik Seuß"], "title": "Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems", "comment": "11 pages, 7 figures, Funding: GreenICT@FMD (BMFTR grant 16ME0491K)", "summary": "This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.", "AI": {"tldr": "提出一个用于ARM Cortex处理器（M0+, M4, M7）上AI模型优化的基准测试框架，关注能效、精度和资源利用率，并利用帕累托分析权衡性能与功耗。", "motivation": "在嵌入式系统中优化AI模型的能效、精度和资源利用率，以指导开发者设计高性能、可持续的AI系统。", "method": "设计自动化测试平台，评估关键绩效指标（KPI），进行FLOPs与推理时间的关联分析，并应用帕累托分析来权衡能耗与模型精度。", "result": "发现FLOPs与推理时间呈近乎线性相关；M7适合短推理周期，M4适合长推理任务且能效更佳，M0+适合简单任务。帕累托分析展示了如何平衡能耗与精度。", "conclusion": "该基准测试框架能系统性地评估和优化AI模型在ARM Cortex处理器上的性能，为开发者提供关于处理器选择和模型设计的指导，以实现能效与性能的平衡。"}}
{"id": "2602.17529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17529", "abs": "https://arxiv.org/abs/2602.17529", "authors": ["Dun Yuan", "Hao Zhou", "Xue Liu", "Hao Chen", "Yan Xin", "Jianzhong", "Zhang"], "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation", "comment": null, "summary": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.", "AI": {"tldr": "提出了一种结合知识图谱（KG）和检索增强生成（RAG）的新框架KG-RAG，以增强大型语言模型（LLM）在电信领域的应用，显著提高了准确性和可靠性，减少了幻觉。", "motivation": "通用LLM在电信领域面临挑战，因为该领域复杂、标准不断发展且术语专业，导致输出不准确、容易出现幻觉，降低了实用性。", "method": "该研究引入了KG-RAG框架，它将知识图谱（KG）与检索增强生成（RAG）相结合。KG提供了来自电信标准和技术文档的结构化领域知识，RAG动态检索相关事实来 grounding 模型的输出。", "result": "在基准数据集上的实验表明，KG-RAG在准确性上优于仅使用LLM和标准RAG的方法。具体而言，KG-RAG比RAG平均准确性提高了14.3%，比仅LLM模型提高了21.6%。", "conclusion": "KG-RAG框架在处理复杂电信场景时，能够生成准确、可靠且可解释的输出，有效解决了通用LLM在该领域的局限性。"}}
{"id": "2602.17418", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17418", "abs": "https://arxiv.org/abs/2602.17418", "authors": ["Diana Addae", "Diana Rogachova", "Nafiseh Kahani", "Masoud Barati", "Michael Christensen", "Chen Zhou"], "title": "A Privacy by Design Framework for Large Language Model-Based Applications for Children", "comment": null, "summary": "Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.", "AI": {"tldr": "本文提出了一个基于隐私设计（PbD）的框架，用于指导为儿童设计和开发人工智能（AI）应用程序，特别是基于大型语言模型（LLM）的应用程序，以应对隐私风险和法律合规性挑战。", "motivation": "儿童越来越多地使用AI技术，但其隐私风险日益增加，而现有的隐私法规在实践中难以实施。", "method": "该框架整合了GDPR、PIPEDA、COPPA等隐私法规的原则，并将其映射到LLM应用的各个阶段（数据收集、模型训练、运行监控、持续验证）。同时，借鉴了UNCRC、AADC等儿童权利和设计规范，为儿童提供设计指南。通过一个针对13岁以下儿童的LLM教育辅导案例研究来展示框架的应用。", "result": "通过技术和组织控制以及在LLM生命周期中做出适合年龄的设计决策，可以降低隐私风险，并满足法律要求。", "conclusion": "通过在LLM生命周期中实施数据保护策略和年龄适宜的设计决策，可以开发出既提供隐私保护又符合法律要求的儿童AI应用。"}}
{"id": "2602.17566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17566", "abs": "https://arxiv.org/abs/2602.17566", "authors": ["Asif Hasan Chowdhury", "Md. Fahim Islam", "M Ragib Anjum Riad", "Faiyaz Bin Hashem", "Md Tanzim Reza", "Md. Golam Rabiul Alam"], "title": "A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN", "comment": null, "summary": "The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.", "AI": {"tldr": "提出了一种结合了 SWIN Transformer 和 CNN 的联邦学习混合模型，用于基于 X 光图像诊断 COVID-19 和肺炎。", "motivation": "为了利用计算能力的提升，在医疗保健领域应用人工智能，并解决医疗数据共享和安全问题，从而提高疾病诊断的效率和可靠性。", "method": "采用联邦学习（Federated Learning）框架，结合了 SWIN Transformer 和几种先进的 CNN 模型（DenseNet201, Inception V3, VGG 19），并利用 TensorFlow 和 Keras 进行实现。", "result": "所提出的混合模型能够对 COVID-19 和肺炎进行诊断，并能够通过持续学习提高诊断的准确性和预测疾病的严重程度，同时联邦学习保证了模型的安全性和数据真实性。", "conclusion": "基于联邦学习的混合 AI 模型能够安全、分布式地处理医疗数据，提高肺部疾病诊断的准确性和效率，为医生提供有力的辅助。"}}
{"id": "2602.17402", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17402", "abs": "https://arxiv.org/abs/2602.17402", "authors": ["Michele Zanitti", "Vanja Miskovic", "Francesco Trovò", "Alessandra Laura Giulia Pedrocchi", "Ming Shen", "Yan Kyaw Tun", "Arsela Prelaj", "Sokol Kosta"], "title": "A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities", "comment": "Accepted at The 13th IEEE International Conference on Big Data (IEEE BigData 2025)", "summary": "Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.", "AI": {"tldr": "本文提出了一种多模态对比变分自编码器（MCVAE），用于整合全切片图像、转录组学和 DNA 甲基化数据来预测非小细胞肺癌（NSCLC）患者的生存结局，特别是在数据缺失严重的情况下，比现有模型更具鲁棒性。", "motivation": "现有模型在处理非小细胞肺癌（NSCLC）患者生存结局预测时，由于个体预后特征差异大且真实临床数据常有模态缺失，鲁棒性不足。整合多模态数据（全切片图像、转录组学、DNA 甲基化）可以提供更全面的患者信息，但如何有效处理缺失数据是关键挑战。", "method": "提出了一种多模态对比变分自编码器（MCVAE）。该模型包含模态特异性变分编码器，以捕捉数据源的不确定性；引入了带有学习门控机制的融合瓶颈，以规范化可用模态的贡献；采用了结合生存损失和重建损失的多任务目标来正则化患者表示；通过跨模态对比损失在潜在空间中强制跨模态对齐；在训练过程中，采用随机模态掩码来提高对任意缺失模式的鲁棒性。", "result": "在 TCGA-LUAD 和 TCGA-LUSC 数据集上进行了广泛评估，结果表明 MCVAE 在预测疾病特异性生存（DSS）方面比两种最先进的模型更有效，并且对严重缺失数据具有更强的鲁棒性。此外，研究还发现多模态整合并非总是对预测任务有益。", "conclusion": "MCVAE 是一种有效且鲁棒的方法，能够整合多模态数据（图像、转录组学、甲基化）来预测 NSCLC 患者的生存结局，尤其在面对严重数据缺失的情况下表现优于现有技术。然而，多模态数据整合的效益需要具体分析，并非越多越好。"}}
{"id": "2602.17560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17560", "abs": "https://arxiv.org/abs/2602.17560", "authors": ["Hongjue Zhao", "Haosen Sun", "Jiangtao Kong", "Xiaochang Li", "Qineng Wang", "Liwei Jiang", "Qi Zhu", "Tarek Abdelzaher", "Yejin Choi", "Manling Li", "Huajie Shao"], "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment", "comment": "Accepted by ICLR 2026", "summary": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \\textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \\textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \\textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \\textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\\%$ improvement over TruthfulQA, $2.5\\%$ over UltraFeedback, and $2.4\\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.", "AI": {"tldr": "本文提出了一个基于常微分方程（ODE）的统一理论框架来指导大型语言模型（LLM）的激活引导（activation steering），并在此框架下开发了一种名为ODESteer的新方法，该方法通过构造障碍函数来实现多步自适应引导，并在多个LLM对齐基准测试中取得了显著的性能提升。", "motivation": "现有激活引导方法缺乏统一的理论框架来指导方向设计，并且过度依赖单步引导，无法捕捉复杂的激活分布模式。", "method": "提出一个基于常微分方程（ODE）的理论框架，将激活引导视为ODE的解，并将引导方向的设计等价于控制论中的障碍函数设计。基于此框架，引入ODESteer方法，将障碍函数定义为正负激活的对数密度比，并利用其构建ODE来实现多步自适应引导。", "result": "ODESteer方法在TruthfulQA、UltraFeedback和RealToxicityPrompts等LLM对齐基准测试中取得了优于现有最先进方法的性能，分别提升了5.7%、2.5%和2.4%。", "conclusion": "该研究通过ODE统一了激活引导的理论基础，并通过ODESteer方法进行了实证验证，为LLM对齐提供了一种新颖且有效的激活引导视角。"}}
{"id": "2602.17594", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17594", "abs": "https://arxiv.org/abs/2602.17594", "authors": ["Lance Ying", "Ryan Truong", "Prafull Sharma", "Kaiya Ivy Zhao", "Nathan Cloos", "Kelsey R. Allen", "Thomas L. Griffiths", "Katherine M. Collins", "José Hernández-Orallo", "Phillip Isola", "Samuel J. Gershman", "Joshua B. Tenenbaum"], "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games", "comment": "29 pages, 14 figures", "summary": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.", "AI": {"tldr": "该研究提出了一种评估AI通用智能的新方法，即让AI在“人类游戏多元宇宙”（所有人类设计的游戏）中与人类玩家进行比较。研究者构建了一个名为AI GameStore的平台，利用LLMs和人类参与来生成新游戏，并用其评估了七个前沿VLM模型，发现它们在大多数游戏中表现远逊于人类，尤其是在需要世界模型学习、记忆和规划的游戏中。", "motivation": "现有AI基准测试通常只评估狭窄的能力且是静态的，容易被饱和。为了更有效地评估AI的通用智能，需要一个能够反映人类广泛认知能力且动态演进的评估体系。", "method": "提出通过AI学习和玩所有可能的人类游戏来评估其通用智能。构建了AI GameStore平台，使用LLMs和人类协作来合成新的、代表性的人类游戏。基于App Store和Steam的热门游戏生成了100个游戏，并用它们评估了七个前沿VLM模型。", "result": "在所评估的100个游戏中，最好的VLM模型在大多数游戏中的得分仅达到人类平均得分的10%以下。这些模型在挑战世界模型学习、记忆和规划的游戏中表现尤其困难。", "conclusion": "AI GameStore平台是一个有前景的评估和推动机器朝着类人通用智能发展的实用方法。未来的工作应侧重于扩展该平台，以更全面地衡量AI的通用智能。"}}
{"id": "2602.17602", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17602", "abs": "https://arxiv.org/abs/2602.17602", "authors": ["Hojung Jung", "Rodrigo Hormazabal", "Jaehyeong Jo", "Youngrok Park", "Kyunggeun Roh", "Se-Young Yun", "Sehui Han", "Dae-Woong Jeong"], "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models", "comment": null, "summary": "Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.", "AI": {"tldr": "MolHIT是一个新的分子图生成框架，通过分层离散扩散模型和解耦原子编码，显著提高了化学有效性和多属性生成能力，达到新的SOTA水平。", "motivation": "现有的图扩散模型在化学有效性和满足目标性质方面存在不足，与一维模型相比性能较差，这促使研究者开发更强大的分子图生成框架。", "method": "引入了MolHIT框架，其核心是分层离散扩散模型（Hierarchical Discrete Diffusion Model），该模型将离散扩散推广到包含化学先验的附加类别，并采用解耦原子编码（decoupled atom encoding）来区分不同化学角色的原子类型。", "result": "MolHIT在MOSES数据集上取得了新的SOTA性能，首次实现了接近完美的化学有效性，并在多个指标上超越了一维基线模型。此外，在多属性引导生成和支架扩展等下游任务中也表现出强大的性能。", "conclusion": "MolHIT框架成功克服了现有图扩散模型在化学有效性和性能方面的局限性，为AI驱动的药物发现和材料科学提供了更有效的分子图生成解决方案。"}}
{"id": "2602.17607", "categories": ["cs.AI", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.17607", "abs": "https://arxiv.org/abs/2602.17607", "authors": ["Jianda Du", "Youran Sun", "Haizhao Yang"], "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing", "comment": null, "summary": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.", "AI": {"tldr": "AutoNumerics 是一个多智能体框架，可以根据自然语言描述自动设计、实现、调试和验证 PDE 数值求解器，生成透明且基于经典数值分析的求解器。", "motivation": "现有的 PDE 数值求解器设计需要大量专业知识和手动调优，而基于神经网络的方法计算成本高且可解释性差。研究者希望开发一种更灵活、高效且可解释的自动化 PDE 求解方法。", "method": "该框架采用多智能体协同工作，实现了从自然语言描述到生成、调试和验证 PDE 数值求解器的全自动化流程。关键技术包括：粗粒度到细粒度的执行策略，以及基于残差的自验证机制。", "result": "在 24 个经典和真实世界 PDE 问题上的实验表明，AutoNumerics 能够生成具有竞争力的甚至更优的求解器，精度可与现有的神经网络和 LLM 方法相媲美。该框架还能根据 PDE 的结构特性自动选择合适的数值方案。", "conclusion": "AutoNumerics 提供了一个可行的、易于访问的自动化 PDE 求解范式，能够生成透明的、基于经典数值分析的求解器，解决了传统方法和现有神经网络方法的不足。"}}
