{"id": "2510.16247", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16247", "abs": "https://arxiv.org/abs/2510.16247", "authors": ["H. Mozaffari", "A. Nahvi"], "title": "A Motivational Driver Steering Model: Task Difficulty Homeostasis From Control Theory Perspective", "comment": "Cognitive systems Research", "summary": "A general and psychologically plausible collision avoidance driver model can\nimprove transportation safety significantly. Most computational driver models\nfound in the literature have used control theory methods only, and they are not\nestablished based on psychological theories. In this paper, a unified approach\nis presented based on concepts taken from psychology and control theory. The\n\"task difficulty homeostasis theory\", a prominent motivational theory, is\ncombined with the \"Lyapunov stability method\" in control theory to present a\ngeneral and psychologically plausible model. This approach is used to model\ndriver steering behavior for collision avoidance. The performance of this model\nis measured by simulation of two collision avoidance scenarios at a wide range\nof speeds from 20 km/h to 170 km/h. The model is validated by experiments on a\ndriving simulator. The results demonstrate that the model follows human\nbehavior accurately with a mean error of 7 percent.", "AI": {"tldr": "本文提出了一种结合心理学（任务难度稳态理论）和控制理论（Lyapunov稳定性方法）的通用且心理学上合理的避碰驾驶员模型，并通过仿真和实验验证了其准确性。", "motivation": "现有计算驾驶员模型多仅使用控制理论方法，缺乏心理学基础，限制了其在提高交通安全方面的潜力。", "method": "将心理学中的“任务难度稳态理论”与控制理论中的“Lyapunov稳定性方法”相结合，构建了一个通用的避碰驾驶员转向行为模型。通过在20-170 km/h速度范围内模拟两种避碰场景，并在驾驶模拟器上进行实验来验证模型性能。", "result": "模型能准确模拟人类行为，平均误差为7%。", "conclusion": "该结合心理学和控制理论的统一方法提供了一个通用且心理学上合理的避碰驾驶员模型，并被证明具有高准确性。"}}
{"id": "2510.16205", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16205", "abs": "https://arxiv.org/abs/2510.16205", "authors": ["João Carlos Virgolino Soares", "Gabriel Fischer Abati", "Claudio Semini"], "title": "VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments", "comment": "Code available at https://github.com/iit-DLSLab/VAR-SLAM", "summary": "Visual SLAM in dynamic environments remains challenging, as several existing\nmethods rely on semantic filtering that only handles known object classes, or\nuse fixed robust kernels that cannot adapt to unknown moving objects, leading\nto degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual\nAdaptive and Robust SLAM), an ORB-SLAM3-based system that combines a\nlightweight semantic keypoint filter to deal with known moving objects, with\nBarron's adaptive robust loss to handle unknown ones. The shape parameter of\nthe robust kernel is estimated online from residuals, allowing the system to\nautomatically adjust between Gaussian and heavy-tailed behavior. We evaluate\nVAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which\ninclude both known and unknown moving objects. Results show improved trajectory\naccuracy and robustness over state-of-the-art baselines, achieving up to 25%\nlower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining\nperformance at 27 FPS on average.", "AI": {"tldr": "VAR-SLAM是一种基于ORB-SLAM3的视觉SLAM系统，它结合了轻量级语义关键点滤波器处理已知移动物体和Barron自适应鲁棒损失处理未知移动物体，显著提高了动态环境下轨迹精度和鲁棒性。", "motivation": "现有视觉SLAM方法在动态环境中面临挑战，因为它们要么依赖于仅处理已知对象类别的语义过滤，要么使用无法适应未知移动对象的固定鲁棒核，导致在场景中出现未知移动对象时精度下降。", "method": "VAR-SLAM是基于ORB-SLAM3的系统。它结合了轻量级语义关键点滤波器来处理已知移动对象，并使用Barron的自适应鲁棒损失来处理未知移动对象。鲁棒核的形状参数通过残差在线估计，使系统能够自动调整高斯和重尾行为。", "result": "在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上进行了评估，这些数据集包含已知和未知移动对象。结果表明，VAR-SLAM在轨迹精度和鲁棒性方面优于最先进的基线方法，在挑战性序列上比NGD-SLAM的ATE RMSE降低了高达25%，同时平均保持27 FPS的性能。", "conclusion": "VAR-SLAM通过结合语义过滤和自适应鲁棒损失，有效地解决了动态环境中视觉SLAM的挑战，能够同时处理已知和未知移动物体，从而提高了系统的精度和鲁棒性，并保持了高效的运行速度。"}}
{"id": "2510.16352", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16352", "abs": "https://arxiv.org/abs/2510.16352", "authors": ["Sayak Mukherjee", "Himanshu Sharma", "Wenceslao Shaw Cortez", "Genevieve Starke", "Michael Sinner", "Brooke J. Stanislawski", "Zachary Tully", "Paul Fleming", "Sonja Glavaski"], "title": "Supervisory Control of Hybrid Power Plants Using Online Feedback Optimization: Designs and Validations with a Hybrid Co-Simulation Engine", "comment": "20 pages, 9 figures", "summary": "This research investigates designing a supervisory feedback controller for a\nhybrid power plant that coordinates the wind, solar, and battery energy storage\nplants to meet the desired power demands. We have explored an online feedback\ncontrol design that does not require detailed knowledge about the models, known\nas feedback optimization. The control inputs are updated using the gradient\ninformation of the cost and the outputs with respect to the input control\ncommands. This enables us to adjust the active power references of wind, solar,\nand storage plants to meet the power generation requirements set by grid\noperators. The methodology also ensures robust control performance in the\npresence of uncertainties in the weather. In this paper, we focus on describing\nthe supervisory feedback optimization formulation and control-oriented modeling\nfor individual renewable and storage components of the hybrid power plant. The\nproposed supervisory control has been integrated with the hybrid plant\nco-simulation engine, Hercules, demonstrating its effectiveness in more\nrealistic simulation scenarios.", "AI": {"tldr": "本研究提出了一种基于反馈优化的监控反馈控制器，用于协调混合电厂（风能、太阳能、电池储能）以满足电力需求，该方法无需详细模型，并能应对天气不确定性。", "motivation": "研究动机在于设计一个能够协调风能、太阳能和电池储能的混合电厂的监控反馈控制器，以满足所需的电力需求。特别地，需要探索一种无需详细模型知识的在线反馈控制设计，即反馈优化。", "method": "本研究采用反馈优化方法，通过成本和输出相对于输入控制指令的梯度信息来更新控制输入。这使得系统能够调整风能、太阳能和储能电厂的有效功率参考，以满足电网运营商设定的发电要求。该监控控制已集成到混合电厂协同仿真引擎 Hercules 中进行验证。", "result": "该方法能够调整风能、太阳能和储能电厂的有效功率参考以满足发电要求，并确保在存在天气不确定性时仍具有鲁棒的控制性能。通过与 Hercules 协同仿真引擎的集成，证明了其在更真实的模拟场景中的有效性。", "conclusion": "所提出的监控反馈优化公式和控制导向建模对于混合电厂的各个可再生能源和储能组件是有效的。该方法实现了无需详细模型知识的在线控制，并在存在不确定性时提供鲁棒性能，在实际模拟场景中表现出其有效性。"}}
{"id": "2510.15972", "categories": ["cs.CL", "cs.AI", "81P68 (Primary), 68T50, 68T07 (Secondary)", "I.2.7; F.1.2"], "pdf": "https://arxiv.org/pdf/2510.15972", "abs": "https://arxiv.org/abs/2510.15972", "authors": ["Ling Sun", "Peter Sullivan", "Michael Martin", "Yun Zhou"], "title": "Quantum NLP models on Natural Language Inference", "comment": "Accepted, presented, and to appear in the Proceedings of the Quantum\n  AI and NLP 2025 Conference", "summary": "Quantum natural language processing (QNLP) offers a novel approach to\nsemantic modeling by embedding compositional structure directly into quantum\ncircuits. This paper investigates the application of QNLP models to the task of\nNatural Language Inference (NLI), comparing quantum, hybrid, and classical\ntransformer-based models under a constrained few-shot setting. Using the lambeq\nlibrary and the DisCoCat framework, we construct parameterized quantum circuits\nfor sentence pairs and train them for both semantic relatedness and inference\nclassification. To assess efficiency, we introduce a novel\ninformation-theoretic metric, Information Gain per Parameter (IGPP), which\nquantifies learning dynamics independent of model size. Our results demonstrate\nthat quantum models achieve performance comparable to classical baselines while\noperating with dramatically fewer parameters. The Quantum-based models\noutperform randomly initialized transformers in inference and achieve lower\ntest error on relatedness tasks. Moreover, quantum models exhibit significantly\nhigher per-parameter learning efficiency (up to five orders of magnitude more\nthan classical counterparts), highlighting the promise of QNLP in low-resource,\nstructure-sensitive settings. To address circuit-level isolation and promote\nparameter sharing, we also propose a novel cluster-based architecture that\nimproves generalization by tying gate parameters to learned word clusters\nrather than individual tokens.", "AI": {"tldr": "本研究将量子自然语言处理（QNLP）应用于自然语言推理（NLI）任务，发现在参数量极少的情况下，量子模型能达到与经典模型相当的性能，并展示出显著更高的参数学习效率，同时提出了一种新的基于聚类的架构以提高泛化能力。", "motivation": "QNLP通过将组合结构直接嵌入量子电路中，为语义建模提供了一种新颖的方法。本研究旨在探索QNLP模型在自然语言推理（NLI）任务中的应用潜力，特别是在受限的少样本设置下。", "method": "研究使用lambeq库和DisCoCat框架构建句子对的参数化量子电路，并将其训练用于语义相关性和推理分类。在少样本设置下，比较了量子、混合和经典基于Transformer的模型。为评估效率，引入了新的信息论度量“每参数信息增益”（IGPP）。此外，还提出了一种新的基于聚类的架构，通过将门参数与学习到的词聚类而非单个token绑定，以促进参数共享并改善泛化。", "result": "量子模型在参数量极少的情况下，达到了与经典基线模型相当的性能。量子模型在推理任务中优于随机初始化的Transformer模型，并在相关性任务中实现了更低的测试误差。量子模型展现出显著更高的每参数学习效率（比经典模型高出多达五个数量级）。提出的基于聚类的架构通过参数共享改善了泛化能力。", "conclusion": "QNLP在低资源、结构敏感的环境中具有巨大潜力，能够以更少的参数实现可比的性能和极高的每参数学习效率。新提出的基于聚类的架构进一步提升了模型的泛化能力。"}}
{"id": "2510.15963", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15963", "abs": "https://arxiv.org/abs/2510.15963", "authors": ["Jiani Huang", "Amish Sethi", "Matthew Kuo", "Mayank Keoliya", "Neelay Velingker", "JungHo Jung", "Ser-Nam Lim", "Ziyang Li", "Mayur Naik"], "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "comment": "Accepted as a Spotlight Paper at NeurIPS 2025", "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, current training pipelines primarily\nrely on high-level vision-sound-text pairs and lack fine-grained, structured\nalignment between pixel-level visual content and textual semantics. To overcome\nthis challenge, we propose ESCA, a new framework for contextualizing embodied\nagents through structured spatial-temporal understanding. At its core is\nSGClip, a novel CLIP-based, open-domain, and promptable model for generating\nscene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic\nlearning pipeline, which harnesses model-driven self-supervision from\nvideo-caption pairs and structured reasoning, thereby eliminating the need for\nhuman-labeled scene graph annotations. We demonstrate that SGClip supports both\nprompt-based inference and task-specific fine-tuning, excelling in scene graph\ngeneration and action localization benchmarks. ESCA with SGClip consistently\nimproves both open-source and commercial MLLMs, achieving state-of-the-art\nperformance across two embodied environments. Notably, it significantly reduces\nagent perception errors and enables open-source models to surpass proprietary\nbaselines.", "AI": {"tldr": "本文提出ESCA框架，通过引入SGClip（一个基于CLIP的场景图生成模型）来解决多模态大语言模型（MLLMs）在具身智能体中像素级视觉内容与文本语义之间缺乏细粒度对齐的问题，从而显著提升了具身智能体的性能并减少了感知错误。", "motivation": "当前用于具身智能体的多模态大语言模型主要依赖高层次的视觉-声音-文本对进行训练，缺乏像素级视觉内容与文本语义之间的细粒度、结构化对齐。", "method": "本文提出了ESCA框架，旨在通过结构化的时空理解来情境化具身智能体。其核心是SGClip，一个新颖的、基于CLIP、开放域且可提示的场景图生成模型。SGClip通过神经符号学习管道在超过8.7万个开放域视频上进行训练，该管道利用视频-字幕对的模型驱动自监督和结构化推理，从而无需人工标注的场景图。SGClip支持基于提示的推理和任务特定的微调。", "result": "SGClip在场景图生成和动作定位基准测试中表现出色。结合SGClip的ESCA框架持续改进了开源和商业多模态大语言模型，在两个具身环境中实现了最先进的性能。值得注意的是，它显著减少了智能体的感知错误，并使开源模型超越了专有基线。", "conclusion": "ESCA框架通过SGClip提供的结构化时空理解，成功解决了当前多模态大语言模型在具身智能体中像素级与文本语义对齐不足的问题，显著提升了具身智能体的感知能力和整体性能，甚至使开源模型超越了专有模型。"}}
{"id": "2510.15991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15991", "abs": "https://arxiv.org/abs/2510.15991", "authors": ["Huiming Yang"], "title": "CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection", "comment": "13 pages", "summary": "The sparse cross-modality detector offers more advantages than its\ncounterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of\nadaptability for downstream tasks and computational cost savings. However,\nexisting sparse detectors overlook the quality of token representation, leaving\nit with a sub-optimal foreground quality and limited performance. In this\npaper, we identify that the geometric structure preserved and the class\ndistribution are the key to improving the performance of the sparse detector,\nand propose a Sparse Selector (SS). The core module of SS is Ray-Aware\nSupervision (RAS), which preserves rich geometric information during the\ntraining stage, and Class-Balanced Supervision, which adaptively reweights the\nsalience of class semantics, ensuring that tokens associated with small objects\nare retained during token sampling. Thereby, outperforming other sparse\nmulti-modal detectors in the representation of tokens. Additionally, we design\nRay Positional Encoding (Ray PE) to address the distribution differences\nbetween the LiDAR modality and the image. Finally, we integrate the\naforementioned module into an end-to-end sparse multi-modality detector, dubbed\nCrossRay3D. Experiments show that, on the challenging nuScenes benchmark,\nCrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,\nwhile running 1.84 faster than other leading methods. Moreover, CrossRay3D\ndemonstrates strong robustness even in scenarios where LiDAR or camera data are\npartially or entirely missing.", "AI": {"tldr": "本文提出CrossRay3D，一个稀疏跨模态检测器，通过引入稀疏选择器（SS），包含射线感知监督（RAS）和类别平衡监督，以及射线位置编码（Ray PE），显著提升了token表示质量，在nuScenes基准上实现了最先进的性能、更快的速度和强大的鲁棒性。", "motivation": "现有稀疏跨模态检测器忽略了token表示的质量，导致前景质量欠佳和性能受限，未能充分发挥其在下游任务适应性和计算成本方面的优势。", "method": "本文识别出几何结构和类别分布是提升稀疏检测器性能的关键。提出了稀疏选择器（SS），其核心模块包括：1) 射线感知监督（RAS），用于在训练阶段保留丰富的几何信息；2) 类别平衡监督，自适应地重新加权类别语义显著性，以保留小物体token。此外，设计了射线位置编码（Ray PE）来解决LiDAR和图像模态之间的分布差异。最终，将这些模块集成到一个端到端的稀疏多模态检测器中，命名为CrossRay3D。", "result": "在具有挑战性的nuScenes基准测试中，CrossRay3D实现了最先进的性能，mAP达到72.4，NDS达到74.7。同时，其运行速度比其他领先方法快1.84倍。此外，CrossRay3D即使在LiDAR或相机数据部分或完全缺失的情况下，也表现出强大的鲁棒性。", "conclusion": "通过提出稀疏选择器（SS）及其射线感知监督（RAS）和类别平衡监督模块，以及射线位置编码（Ray PE），CrossRay3D有效解决了稀疏检测器中token表示质量的问题，显著提升了3D目标检测的性能、效率和鲁棒性，使其成为稀疏跨模态检测领域的领先方法。"}}
{"id": "2510.16310", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.0; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16310", "abs": "https://arxiv.org/abs/2510.16310", "authors": ["Olajumoke O. Adekunle", "Joseph D. Akinyemi", "Khadijat T. Ladoja", "Olufade F. W. Onifade"], "title": "Lung Cancer Classification from CT Images Using ResNet", "comment": "9 pages,4 figures, 3 tables", "summary": "Lung cancer, a malignancy originating in lung tissues, is commonly diagnosed\nand classified using medical imaging techniques, particularly computed\ntomography (CT). Despite the integration of machine learning and deep learning\nmethods, the predictive efficacy of automated systems for lung cancer\nclassification from CT images remains below the desired threshold for clinical\nadoption. Existing research predominantly focuses on binary classification,\ndistinguishing between malignant and benign lung nodules. In this study, a\nnovel deep learning-based approach is introduced, aimed at an improved\nmulti-class classification, discerning various subtypes of lung cancer from CT\nimages. Leveraging a pre-trained ResNet model, lung tissue images were\nclassified into three distinct classes, two of which denote malignancy and one\nbenign. Employing a dataset comprising 15,000 lung CT images sourced from the\nLC25000 histopathological images, the ResNet50 model was trained on 10,200\nimages, validated on 2,550 images, and tested on the remaining 2,250 images.\nThrough the incorporation of custom layers atop the ResNet architecture and\nmeticulous hyperparameter fine-tuning, a remarkable test accuracy of 98.8% was\nrecorded. This represents a notable enhancement over the performance of prior\nmodels on the same dataset.", "AI": {"tldr": "本研究提出了一种基于深度学习的新方法，利用预训练的ResNet50模型对CT图像进行肺癌多类别分类（两种恶性，一种良性），在测试集上实现了98.8%的准确率，显著优于现有模型。", "motivation": "尽管机器学习和深度学习已被用于肺癌CT图像分类，但现有自动化系统的预测效果仍未达到临床应用所需的阈值。此外，现有研究主要集中于良恶性二分类，而临床上需要识别不同肺癌亚型进行多类别分类。", "method": "研究采用了一种新颖的深度学习方法，利用预训练的ResNet50模型对肺部CT图像进行三类别分类（两种恶性、一种良性）。该方法在ResNet架构之上添加了自定义层，并进行了精细的超参数调优。数据集包含15,000张肺部CT图像（源自LC25000组织病理学图像），其中10,200张用于训练，2,550张用于验证，2,250张用于测试。", "result": "通过该方法，模型在测试集上取得了98.8%的显著准确率。与之前在相同数据集上表现的模型相比，这是一个显著的提升。", "conclusion": "本研究引入的深度学习方法显著提高了肺癌CT图像的多类别分类准确性，为肺癌的自动化诊断和分类提供了有前景的解决方案，并有望推动其在临床中的应用。"}}
{"id": "2510.16297", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16297", "abs": "https://arxiv.org/abs/2510.16297", "authors": ["Muhammad Hamza Ali", "Amritanshu Pandey"], "title": "AC Dynamics-aware Trajectory Optimization with Binary Enforcement for Adaptive UFLS Design", "comment": null, "summary": "The high penetration of distributed energy resources, resulting in backfeed\nof power at the transmission and distribution interface, is causing\nconventional underfrequency load shedding (UFLS) schemes to become\nnonconforming. Adaptive schemes that update UFLS relay settings recursively in\ntime offer a solution, but existing adaptive techniques that obtain UFLS relay\nsettings with linearized or reduced-order model formulations fail to capture AC\nnonlinear network behavior. In practice, this will result in relays unable to\nrestore system frequency during adverse disturbances. We formulate an adaptive\nUFLS problem as a trajectory optimization and include the full AC nonlinear\nnetwork dynamics to ensure AC feasibility and time-coordinated control actions.\nWe include binary decisions to model relay switching action and time-delayed\nmulti-stage load-shedding. However, this formulation results in an intractable\nMINLP problem. To enforce model tractability, we relax these binary variables\ninto continuous surrogates and reformulate the MINLP as a sequence of NLPs. We\nsolve the NLPs with a homotopy-driven method that enforces\nnear-integer-feasible solutions. We evaluate the framework on multiple\nsynthetic transmission systems and demonstrate that it scales efficiently to\nnetworks exceeding 1500+ nodes with over 170k+ continuous and 73k+ binary\ndecision variables, while successfully recovering binary-feasible solutions\nthat arrest the frequency decline during worst-case disturbance.", "AI": {"tldr": "针对高渗透分布式能源导致传统UFLS失效的问题，本文提出了一种基于完整交流非线性网络动力学的自适应UFLS方案。该方案将问题建模为轨迹优化，通过松弛二值变量并采用同伦驱动方法求解，实现了在大型电网中有效恢复系统频率。", "motivation": "高渗透分布式能源导致电力回馈，使得传统欠频减载（UFLS）方案不再适用。现有自适应方案使用线性化或降阶模型，未能捕捉交流非线性网络行为，导致在恶劣扰动下无法有效恢复系统频率。", "method": "将自适应UFLS问题公式化为轨迹优化，并包含完整的交流非线性网络动力学，以确保交流可行性和时间协调控制。引入二值决策来建模继电器切换和延时多级减载，形成一个难以处理的混合整数非线性规划（MINLP）问题。为提高可处理性，将二值变量松弛为连续替代变量，将MINLP重构为一系列非线性规划（NLP），并使用同伦驱动方法求解这些NLP，以获得接近整数可行的解。", "result": "该框架在多个合成输电系统上进行了评估，并证明其能有效扩展到超过1500个节点、包含17万+连续变量和7.3万+二值决策变量的大型网络。同时，成功恢复了二值可行解，在最坏情况扰动下有效阻止了频率下降。", "conclusion": "所提出的自适应UFLS框架，通过纳入完整的交流非线性网络动力学和创新的MINLP求解方法，为解决高分布式能源渗透下电网频率稳定问题提供了一个高效且可扩展的解决方案。"}}
{"id": "2510.16262", "categories": ["eess.SY", "cs.IT", "cs.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16262", "abs": "https://arxiv.org/abs/2510.16262", "authors": ["Jose Guajardo", "Ali Niknejad"], "title": "Spatial-to-Spectral Harmonic-Modulated Arrays for 6G Multi-Beam MIMO", "comment": null, "summary": "This article presents an overview and analysis of spatial-to-spectral\nharmonic-modulated arrays (SHAs). Compared to traditional analog or digital\nbeamforming arrays, SHAs enable concurrent multi-beamforming without requiring\nsubstantial hardware replication. SHAs replace the need for hardware\nreplication with frequency-domain multiplexing. Furthermore, SHAs have the\npotential to become key contributors to future 6G networks by enabling scalable\nmulti-user communications, joint communication and sensing, and spatial\ninterference mitigation. In addition, an analysis of the SHA's\nharmonic-modulation waveform and its effects on gain, noise and bandwidth is\npresented. A comb-like modulation waveform for SHAs that minimizes spectral\ninefficiency is proposed. Further, an analysis of the SHA's capability to\nindependently steer multiple beams is presented. This capability is quantified\nin terms of the SHA's spatial-to-spectral degrees of freedom. Lastly, this work\nintroduces a novel SHA architecture that provides three spatial-to-spectral\ndegrees of freedom with minimal hardware replication.", "AI": {"tldr": "本文概述并分析了空间到频谱谐波调制阵列（SHAs），其通过频域复用实现并发多波束成形，显著减少硬件需求，并有望成为未来6G网络中的关键技术，支持多用户通信、通信感知一体化和空间干扰抑制。", "motivation": "传统模拟或数字波束成形阵列实现并发多波束成形需要大量的硬件复制，导致成本和复杂性增加。SHAs旨在通过频域复用解决这一问题，为未来6G网络提供可扩展的多用户通信、联合通信与感知以及空间干扰抑制能力。", "method": "本文对SHAs进行了综述和分析，包括：1) 探讨了SHAs如何通过频域复用取代硬件复制；2) 分析了SHA的谐波调制波形及其对增益、噪声和带宽的影响；3) 提出了一种最小化频谱效率低下的梳状调制波形；4) 分析了SHA独立控制多个波束的能力，并以空间到频谱自由度进行量化；5) 介绍了一种具有最小硬件复制、提供三个空间到频谱自由度的新型SHA架构。", "result": "研究结果表明，SHAs无需大量硬件复制即可实现并发多波束成形，通过频域复用取代了硬件需求。SHAs在可扩展多用户通信、通信感知一体化和空间干扰抑制方面对未来6G网络具有巨大潜力。提出了一种最小化频谱低效率的梳状调制波形。此外，对SHA独立控制多波束的能力进行了量化，并引入了一种提供三个空间到频谱自由度且硬件复制最少的新型SHA架构。", "conclusion": "SHAs是一种通过频域复用实现并发多波束成形的创新技术，能够显著减少硬件复制需求。它在未来6G网络中具有巨大应用潜力，特别是在多用户通信、通信感知一体化和空间干扰抑制方面。通过优化调制波形和引入新型架构，SHAs能够提高频谱效率并提供更多的空间到频谱自由度。"}}
{"id": "2510.16280", "categories": ["eess.SY", "cs.SY", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16280", "abs": "https://arxiv.org/abs/2510.16280", "authors": ["Hui Yang", "Faisal Aqlan", "Richard Zhao"], "title": "Towards Smart Manufacturing Metaverse via Digital Twinning in Extended Reality", "comment": null, "summary": "The rapid evolution of modern manufacturing systems is driven by the\nintegration of emerging metaverse technologies such as artificial intelligence\n(AI), digital twin (DT) with different forms of extended reality (XR) like\nvirtual reality (VR), augmented reality (AR), and mixed reality (MR). These\nadvances confront manufacturing workers with complex and evolving environments\nthat demand digital literacy for problem solving in the future workplace.\nHowever, manufacturing industry faces a critical shortage of skilled workforce\nwith digital literacy in the world. Further, global pandemic has significantly\nchanged how people work and collaborate digitally and remotely. There is an\nurgent need to rethink digital platformization and leverage emerging\ntechnologies to propel industrial evolution toward human-centered manufacturing\nmetaverse (MfgVerse). This paper presents a forward-looking perspective on the\ndevelopment of smart MfgVerse, highlighting current efforts in learning\nfactory, cognitive digital twinning, and the new sharing economy of\nmanufacturing-as-a-service (MaaS). MfgVerse is converging into multiplex\nnetworks, including a social network of human stakeholders, an interconnected\nnetwork of manufacturing things or agents (e.g., machines, robots, facilities,\nmaterial handling systems), a network of digital twins of physical things, as\nwell as auxiliary networks of sales, supply chain, logistics, and\nremanufacturing systems. We also showcase the design and development of a\nlearning factory for workforce training in extended reality. Finally, future\ndirections, challenges, and opportunities are discussed for human-centered\nmanufacturing metaverse. We hope this work helps stimulate more comprehensive\nstudies and in-depth research efforts to advance MfgVerse technologies.", "AI": {"tldr": "本文提出以人为中心的智能制造元宇宙（MfgVerse）的前瞻性视角，探讨了新兴元宇宙技术（AI、数字孪生、XR）如何推动制造业发展，并应对数字素养人才短缺的挑战。文章讨论了学习工厂、认知数字孪生和制造即服务等现有努力，并展示了用于劳动力培训的XR学习工厂设计，最后展望了MfgVerse的未来方向和挑战。", "motivation": "现代制造业系统正迅速融入AI、数字孪生和XR等元宇宙技术，导致工作环境日益复杂，要求工人具备数字素养。然而，全球制造业面临严重的数字素养技能人才短缺。此外，全球疫情显著改变了人们的工作和协作方式，因此迫切需要重新思考数字化平台化，并利用新兴技术推动工业向以人为中心的制造元宇宙（MfgVerse）发展。", "method": "本文采用前瞻性视角，阐述了智能MfgVerse的发展。它着重介绍了学习工厂、认知数字孪生和制造即服务（MaaS）等当前努力。文章将MfgVerse描述为多重网络的融合，包括人类利益相关者社交网络、制造事物/代理互联网络、物理事物数字孪生网络以及销售、供应链、物流和再制造系统辅助网络。此外，文中展示了用于扩展现实（XR）劳动力培训的学习工厂的设计和开发。", "result": "本文提出了一个以人为中心的智能制造元宇宙（MfgVerse）概念，并详细阐述了MfgVerse如何融合为多重网络（包括社会网络、互联事物网络、数字孪生网络和辅助网络）。此外，文章展示并设计了一个用于XR劳动力培训的学习工厂，以应对制造业数字素养人才短缺的问题。", "conclusion": "本文讨论了以人为中心的制造元宇宙的未来方向、挑战和机遇，旨在激发更全面和深入的研究，以推进MfgVerse技术的发展。"}}
{"id": "2510.16231", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16231", "abs": "https://arxiv.org/abs/2510.16231", "authors": ["Bihao Zhang", "Davood Soleymanzadeh", "Xiao Liang", "Minghui Zheng"], "title": "DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly", "comment": null, "summary": "Intelligent robotic disassembly of end-of-life (EOL) products has been a\nlong-standing challenge in robotics. While machine learning techniques have\nshown promise, the lack of specialized hardware limits their application in\nreal-world scenarios. We introduce DeGrip, a customized gripper designed for\nthe disassembly of EOL computer desktops. DeGrip provides three degrees of\nfreedom (DOF), enabling arbitrary configurations within the disassembly\nenvironment when mounted on a robotic manipulator. It employs a cable-driven\ntransmission mechanism that reduces its overall size and enables operation in\nconfined spaces. The wrist is designed to decouple the actuation of wrist and\njaw joints. We also developed an EOL desktop disassembly environment in Isaac\nSim to evaluate the effectiveness of DeGrip. The tasks were designed to\ndemonstrate its ability to operate in confined spaces and disassemble\ncomponents in arbitrary configurations. The evaluation results confirm the\ncapability of DeGrip for EOL desktop disassembly.", "AI": {"tldr": "本文介绍了一种名为DeGrip的定制夹持器，专为报废电脑拆卸设计，具有三自由度、缆绳驱动机制和解耦腕部设计，并在仿真环境中验证了其在狭小空间和任意配置下进行拆卸的能力。", "motivation": "报废产品（EOL）的智能机器人拆卸一直是机器人领域的一大挑战。尽管机器学习技术前景广阔，但缺乏专用硬件限制了其在实际场景中的应用。", "method": "研究人员引入了DeGrip，一种定制的三自由度（DOF）夹持器，专为报废电脑拆卸而设计。它采用缆绳驱动传输机制以减小尺寸并在狭小空间操作，并设计了可解耦腕部和夹爪关节的腕部。此外，还在Isaac Sim中开发了一个报废电脑拆卸环境来评估DeGrip的有效性。", "result": "评估结果证实了DeGrip在报废电脑拆卸方面的能力。实验任务展示了其在狭小空间操作以及拆卸任意配置组件的能力。", "conclusion": "DeGrip夹持器能够有效执行报废电脑的拆卸任务，尤其适用于在狭小空间和不同配置下进行操作，克服了现有硬件限制的挑战。"}}
{"id": "2510.16057", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16057", "abs": "https://arxiv.org/abs/2510.16057", "authors": ["Md Kamrul Siam", "Md Jobair Hossain Faruk", "Jerry Q. Cheng", "Huanying Gu"], "title": "Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus", "comment": "7 pages (Accepted to IEEE BHI 2025)", "summary": "This study presents a novel multi-model fusion framework leveraging two\nstate-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance\nthe reliability of chest X-ray interpretation on the CheXpert dataset. From the\nfull CheXpert corpus of 224,316 chest radiographs, we randomly selected 234\nradiologist-annotated studies to evaluate unimodal performance using image-only\nprompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of\n62.8% and 76.9%, respectively. A similarity-based consensus approach, using a\n95% output similarity threshold, improved accuracy to 77.6%. To assess the\nimpact of multimodal inputs, we then generated synthetic clinical notes\nfollowing the MIMIC-CXR template and evaluated a separate subset of 50 randomly\nselected cases paired with both images and synthetic text. On this multimodal\ncohort, performance improved to 84% for ChatGPT and 76% for Claude, while\nconsensus accuracy reached 91.3%. Across both experimental conditions,\nagreement-based fusion consistently outperformed individual models. These\nfindings highlight the utility of integrating complementary modalities and\nusing output-level consensus to improve the trustworthiness and clinical\nutility of AI-assisted radiological diagnosis, offering a practical path to\nreduce diagnostic errors with minimal computational overhead.", "AI": {"tldr": "本研究提出了一种多模型融合框架，结合ChatGPT和Claude两个大型语言模型，通过多模态输入和基于共识的融合方法，显著提高了胸部X光片判读的可靠性。", "motivation": "本研究旨在提高胸部X光片判读的可靠性，并减少AI辅助放射诊断中的诊断错误，从而增强其临床实用性和可信度。", "method": "研究首先在CheXpert数据集的234张X光片上评估了ChatGPT和Claude在仅图像输入下的单模态性能。随后，通过95%输出相似度阈值应用了基于相似度的共识方法。为了评估多模态输入的影响，研究生成了遵循MIMIC-CXR模板的合成临床笔记，并在50个随机选择的图像和合成文本配对案例上再次评估了模型性能和共识准确性。", "result": "在单模态（仅图像）设置下，ChatGPT和Claude的诊断准确率分别为62.8%和76.9%，而共识方法将其提高到77.6%。在多模态（图像和文本）设置下，ChatGPT的性能提高到84%，Claude为76%，共识准确率达到91.3%。在两种实验条件下，基于共识的融合方法均持续优于单个模型。", "conclusion": "研究结果表明，整合互补模态（图像和临床笔记）并采用输出层共识机制，能够有效提高AI辅助放射诊断的可信度和临床实用性。这提供了一种以最小计算开销减少诊断错误的实用途径。"}}
{"id": "2510.16062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16062", "abs": "https://arxiv.org/abs/2510.16062", "authors": ["Guiyao Tie", "Zenghui Yuan", "Zeli Zhao", "Chaoran Hu", "Tianhe Gu", "Ruihang Zhang", "Sizhe Zhang", "Junran Wu", "Xiaoyue Tu", "Ming Jin", "Qingsong Wen", "Lixing Chen", "Pan Zhou", "Lichao Sun"], "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "comment": "38 pages, 25 figures, 8 tables", "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/", "AI": {"tldr": "本研究引入CorrectBench基准，全面评估了大型语言模型（LLM）的自我纠正策略，发现自我纠正能提高准确性，但效率仍是挑战，并指出需要平衡推理能力与操作效率。", "motivation": "尽管存在多种自我纠正方法，但缺乏对其的全面评估，且LLM是否能真正自我纠正仍是未解之谜，这促使了本研究的开展。", "method": "研究开发了CorrectBench基准，用于评估包括内在、外部和微调方法在内的自我纠正策略。评估范围涵盖常识推理、数学推理和代码生成三项任务。", "result": "研究发现：1) 自我纠正方法能提高准确性，尤其对于复杂推理任务；2) 混合不同自我纠正策略可进一步提升效果，但会降低效率；3) 推理型LLM（如DeepSeek-R1）在额外自我纠正方法下优化有限且时间成本高。此外，简单的思维链（CoT）基线展现出具有竞争力的准确性和效率。", "conclusion": "自我纠正有潜力提升LLM的推理性能，但其效率仍是持续的挑战。因此，需要进一步研究以优化推理能力和操作效率之间的平衡。"}}
{"id": "2510.16408", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16408", "abs": "https://arxiv.org/abs/2510.16408", "authors": ["Sen Zhan", "Lingkang Jin", "Haoyang Zhang", "Nikolaos G. Paterakis"], "title": "Real-time Measurement-based Optimization for Distribution System Operation Considering Battery Voltage and Thermal Constraints", "comment": "7 pages, submitted to PSCC 2026", "summary": "The secure operation of power distribution systems is challenged by the\ngrowing integration of distributed energy resources. Leveraging the flexibility\nof battery storage offers a cost-effective alternative to measures like\ngeneration curtailment, which results in energy losses. However, developing an\neffective operational model for battery storage is hindered by inaccurate grid\nmodels, unavailability of load data, nonlinear relationship between power\ninjections and network states, intertemporal constraints, and complex\nelectrochemical and thermal dynamics. To address these challenges, this paper\nproposes a data-driven operational control scheme for battery storage in\ndistribution systems. Linear and convex quadratic operational constraints are\nconstructed based on real-time distribution system and battery storage\nmeasurements. Lyapunov optimization decouples multi-period battery operation,\nenabling a real-time, forecast-free control strategy with low computational\ncomplexity. Numerical studies using nonlinear distribution system and battery\nstorage simulators validate the effectiveness of the approach in ensuring\nsecure distribution system operation and satisfaction of voltage and thermal\nconstraints of battery storage.", "AI": {"tldr": "本文提出了一种数据驱动的电池储能运行控制方案，用于配电系统，以应对分布式能源集成带来的挑战，确保系统安全运行并满足电池约束。", "motivation": "配电系统中分布式能源（DERs）的日益集成对系统安全运行构成挑战。电池储能提供了一种成本效益高的替代方案，以避免发电削减造成的能源损失。然而，开发有效的电池储能运行模型面临电网模型不准确、负荷数据不可用、功率注入与网络状态之间的非线性关系、跨期约束以及复杂的电化学和热力学动态等问题。", "method": "本文提出了一种数据驱动的配电系统电池储能运行控制方案。该方案基于实时配电系统和电池储能测量数据构建线性和凸二次运行约束。通过Lyapunov优化解耦多周期电池运行，实现了一种计算复杂度低的实时、无需预测的控制策略。", "result": "使用非线性配电系统和电池储能模拟器进行的数值研究验证了该方法的有效性，能够确保配电系统的安全运行，并满足电池储能的电压和热力学约束。", "conclusion": "所提出的数据驱动、实时、无需预测的电池储能运行控制方案，能够有效应对配电系统中分布式能源集成带来的挑战，确保系统安全运行并满足电池的运行约束。"}}
{"id": "2510.16321", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16321", "abs": "https://arxiv.org/abs/2510.16321", "authors": ["Junno Yun", "Yaşar Utku Alçalar", "Mehmet Akçakaya"], "title": "Time-Embedded Algorithm Unrolling for Computational MRI", "comment": "Neural Information Processing Systems (NeurIPS), 2025", "summary": "Algorithm unrolling methods have proven powerful for solving the regularized\nleast squares problem in computational magnetic resonance imaging (MRI). These\napproaches unfold an iterative algorithm with a fixed number of iterations,\ntypically alternating between a neural network-based proximal operator for\nregularization, a data fidelity operation and auxiliary updates with learnable\nparameters. While the connection to optimization methods dictate that the\nproximal operator network should be shared across unrolls, this can introduce\nartifacts or blurring. Heuristically, practitioners have shown that using\ndistinct networks may be beneficial, but this significantly increases the\nnumber of learnable parameters, making it challenging to prevent overfitting.\nTo address these shortcomings, by taking inspirations from proximal operators\nwith varying thresholds in approximate message passing (AMP) and the success of\ntime-embedding in diffusion models, we propose a time-embedded algorithm\nunrolling scheme for inverse problems. Specifically, we introduce a novel\nperspective on the iteration-dependent proximal operation in vector AMP (VAMP)\nand the subsequent Onsager correction in the context of algorithm unrolling,\nframing them as a time-embedded neural network. Similarly, the scalar weights\nin the data fidelity operation and its associated Onsager correction are cast\nas time-dependent learnable parameters. Our extensive experiments on the\nfastMRI dataset, spanning various acceleration rates and datasets, demonstrate\nthat our method effectively reduces aliasing artifacts and mitigates noise\namplification, achieving state-of-the-art performance. Furthermore, we show\nthat our time-embedding strategy extends to existing algorithm unrolling\napproaches, enhancing reconstruction quality without increasing the\ncomputational complexity significantly.", "AI": {"tldr": "本文提出了一种时间嵌入的算法展开方案，用于解决计算磁共振成像（MRI）中的逆问题。该方案通过将迭代相关的近端算子和数据保真度操作参数化为时间嵌入神经网络和时间依赖参数，有效解决了传统算法展开中共享近端算子导致的伪影和独立网络导致的过拟合问题，实现了最先进的重建性能。", "motivation": "在计算MRI中，算法展开方法常用于求解正则化最小二乘问题。然而，将近端算子网络在不同展开步骤中共享会引入伪影或模糊；而使用独立的网络虽然可能带来益处，但会显著增加可学习参数的数量，导致难以防止过拟合。", "method": "受近似消息传递（AMP）中变阈值近端算子和扩散模型中时间嵌入成功的启发，本文提出了一种时间嵌入的算法展开方案。具体来说，将向量AMP（VAMP）中迭代相关的近端操作及其Onsager校正视为时间嵌入神经网络。类似地，数据保真度操作中的标量权重及其Onsager校正被视为时间依赖的可学习参数。", "result": "在fastMRI数据集上，针对不同的加速率和数据集进行的大量实验表明，所提出的方法有效减少了混叠伪影并减轻了噪声放大，达到了最先进的性能。此外，研究还表明，这种时间嵌入策略可以推广到现有的算法展开方法，在不显著增加计算复杂度的前提下提高重建质量。", "conclusion": "本文提出的时间嵌入算法展开方案通过引入迭代相关的近端操作和数据保真度参数的时间依赖性，成功解决了计算MRI中算法展开方法的挑战。该方法不仅提高了重建质量，减少了伪影和噪声，而且在参数效率和泛化性方面也表现出色，为MRI重建领域提供了新的SOTA解决方案。"}}
{"id": "2510.16347", "categories": ["eess.IV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.16347", "abs": "https://arxiv.org/abs/2510.16347", "authors": ["Songyuan Lu", "Jingwen Hui", "Jake Weeks", "David B. Berry", "Fanny Chapelin", "Frank Talke"], "title": "Computer Navigated Spinal Surgery Using Magnetic Resonance Imaging and Augmented Reality", "comment": null, "summary": "Current spinal pain management procedures, such as radiofrequency ablation\n(RFA) and epidural steroid injection (ESI), rely on fluoroscopy for needle\nplacement which exposes patients and physicians to ionizing radiation. In this\npaper, we investigate a radiation-free surgical navigation system for spinal\npain management procedures that combines magnetic resonance imaging (MRI) with\nfiducial ArUco marker-based augmented reality (AR). High-resolution MRI scans\nof a lumbar spinal phantom were obtained and assembled as a surface mesh.\nLaplacian smoothing algorithms were then applied to smoothen the surface and\nimprove the model fidelity. A commercially available stereo camera (ZED2) was\nused to track single or dual fiducial ArUco markers on the patient to determine\nthe patient's real-time pose. Custom AR software was applied to overlay the MRI\nimage onto the patient, allowing the physician to see not only the outer\nsurface of the patient but also the complete anatomy of the patient below the\nsurface. Needle-insertion trials on a 3D-printed 3-vertebra phantom showed that\ndual-ArUco marker tracking increased the accuracy of needle insertions and\nreduced the average needle misplacement distance compared to single-ArUco\nmarker procedures. The average needle misplacement is comparable to the average\ndeviation of 2 mm for conventional epidural techniques using fluoroscopy. Our\nradiation-free system demonstrates promise to serve as an alternative to\nfluoroscopy by improving image-guided spinal navigation.", "AI": {"tldr": "本文提出了一种结合MRI和ArUco标记增强现实的无辐射脊柱疼痛管理手术导航系统，旨在替代传统的荧光透视。", "motivation": "当前的脊柱疼痛管理程序（如射频消融和硬膜外类固醇注射）依赖荧光透视进行针头放置，这会使患者和医生暴露于电离辐射。", "method": "该系统将MRI扫描（获取腰椎模型表面网格并进行拉普拉斯平滑处理）与基于ArUco标记的增强现实技术相结合。使用商用立体相机（ZED2）跟踪患者身上的ArUco标记以确定实时姿态，并通过定制AR软件将MRI图像叠加到患者身上，实现皮下解剖结构的可视化。", "result": "在3D打印的3椎骨模型上进行的针头插入试验表明，双ArUco标记跟踪比单ArUco标记程序更能提高针头插入的准确性，并减少平均针头错位距离。平均针头错位距离与传统荧光透视硬膜外技术2毫米的平均偏差相当。", "conclusion": "该无辐射系统在改善影像引导脊柱导航方面显示出潜力，有望成为荧光透视的替代方案。"}}
{"id": "2510.16036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16036", "abs": "https://arxiv.org/abs/2510.16036", "authors": ["Zewen Li", "Zitong Yu", "Qilang Ye", "Weicheng Xie", "Wei Zhuo", "Linlin Shen"], "title": "IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection", "comment": "Accepted by IEEE Transactions on Instrumentation and Measurement\n  (TIM)", "summary": "The robust causal capability of Multimodal Large Language Models (MLLMs) hold\nthe potential of detecting defective objects in Industrial Anomaly Detection\n(IAD). However, most traditional IAD methods lack the ability to provide\nmulti-turn human-machine dialogues and detailed descriptions, such as the color\nof objects, the shape of an anomaly, or specific types of anomalies. At the\nsame time, methods based on large pre-trained models have not fully stimulated\nthe ability of large models in anomaly detection tasks. In this paper, we\nexplore the combination of rich text semantics with both image-level and\npixel-level information from images and propose IAD-GPT, a novel paradigm based\non MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate\ndetailed anomaly prompts for specific objects. These specific prompts from the\nlarge language model (LLM) are used to activate the detection and segmentation\nfunctions of the pre-trained visual-language model (i.e., CLIP). To enhance the\nvisual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein\nimage features interact with normal and abnormal text prompts to dynamically\nselect enhancement pathways, which enables language models to focus on specific\naspects of visual data, enhancing their ability to accurately interpret and\nrespond to anomalies within images. Moreover, we design a Multi-Mask Fusion\nmodule to incorporate mask as expert knowledge, which enhances the LLM's\nperception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA\ndatasets demonstrate our state-of-the-art performance on self-supervised and\nfew-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA\ndatasets. The codes are available at\n\\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.", "AI": {"tldr": "本文提出IAD-GPT，一个基于多模态大语言模型（MLLM）的新范式，结合图像和文本信息，通过异常提示生成器、文本引导增强器和多掩码融合模块，实现了工业异常检测（IAD）中自监督和少样本任务的SOTA性能。", "motivation": "传统的工业异常检测（IAD）方法缺乏多轮人机对话和详细描述异常的能力（如颜色、形状、类型）。同时，基于大型预训练模型的方法尚未充分激发大模型在异常检测任务中的潜力，而多模态大语言模型（MLLM）具有强大的因果能力来检测缺陷物体。", "method": "本文提出IAD-GPT，一种基于MLLM的IAD新范式。它利用异常提示生成器（APG）为特定对象生成详细异常提示，这些提示激活预训练视觉-语言模型（CLIP）的检测和分割功能。为增强MLLM的视觉基础能力，提出文本引导增强器，使图像特征与正常和异常文本提示交互，动态选择增强路径。此外，设计多掩码融合模块，将掩码作为专家知识融入，增强LLM对像素级异常的感知。", "result": "在MVTec-AD和VisA数据集上的广泛实验表明，IAD-GPT在自监督和少样本异常检测与分割任务中取得了最先进的性能。", "conclusion": "IAD-GPT通过结合丰富的文本语义与图像级和像素级信息，有效解决了传统IAD方法的局限性，并充分利用了多模态大语言模型的能力，在工业异常检测任务中表现出卓越的性能。"}}
{"id": "2510.16240", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16240", "abs": "https://arxiv.org/abs/2510.16240", "authors": ["Lukas Zbinden", "Nigel Nelson", "Juo-Tung Chen", "Xinhao Chen", "Ji Woong", "Kim", "Mahdi Azizian", "Axel Krieger", "Sean Huver"], "title": "Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning", "comment": null, "summary": "The rise of surgical robots and vision-language-action models has accelerated\nthe development of autonomous surgical policies and efficient assessment\nstrategies. However, evaluating these policies directly on physical robotic\nplatforms such as the da Vinci Research Kit (dVRK) remains hindered by high\ncosts, time demands, reproducibility challenges, and variability in execution.\nWorld foundation models (WFM) for physical AI offer a transformative approach\nto simulate complex real-world surgical tasks, such as soft tissue deformation,\nwith high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune\nof the Cosmos WFM, which, together with a trained video classifier, enables\nfully automated online evaluation and benchmarking of surgical policies. We\nevaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop\nsuture pad tasks, the automated pipeline achieves strong correlation between\nonline rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si\nplatform, as well as good agreement between human labelers and the V-JEPA\n2-derived video classifier. Additionally, preliminary experiments with ex-vivo\nporcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising\nalignment with real-world evaluations, highlighting the platform's potential\nfor more complex surgical procedures.", "AI": {"tldr": "本文介绍了Cosmos-Surg-dVRK，一个经过外科手术微调的世界基础模型，结合视频分类器，实现了手术策略的自动化在线评估和基准测试，并在模拟和真实机器人之间显示出高度相关性。", "motivation": "在物理机器人平台（如dVRK）上直接评估手术策略存在成本高、耗时长、重现性差和执行变异性大等问题，阻碍了自主手术策略和高效评估策略的发展。", "method": "本文引入了Cosmos-Surg-dVRK，它是Cosmos世界基础模型（WFM）的一个外科手术微调版本。该模型与一个训练过的视频分类器（源自V-JEPA 2）结合使用，构建了一个全自动的在线评估和基准测试手术策略的管道。研究在桌面缝合垫任务和离体猪胆囊切除术任务两种不同的手术数据集上对Cosmos-Surg-dVRK进行了评估。", "result": "在桌面缝合垫任务上，自动化管道在Cosmos-Surg-dVRK中的在线运行结果与真实dVRK Si平台上的策略结果之间实现了强相关性，并且人工标注者与V-JEPA 2派生的视频分类器之间也表现出良好的一致性。此外，Cosmos-Surg-dVRK在离体猪胆囊切除术任务的初步实验中，也展示了与真实世界评估的良好对齐，突显了该平台在更复杂手术过程中的潜力。", "conclusion": "Cosmos-Surg-dVRK与视频分类器相结合，为手术策略的自动化在线评估和基准测试提供了一个有前景的平台。它能有效解决在物理机器人上评估的挑战，并有望应用于更复杂的手术场景。"}}
{"id": "2510.16414", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16414", "abs": "https://arxiv.org/abs/2510.16414", "authors": ["Yuang Chen", "Fengqian Guo", "Chang Wu", "Shuyi Liu", "Hancheng Lu", "Chang Wen Chen"], "title": "AoI-Aware Task Offloading and Transmission Optimization for Industrial IoT Networks: A Branching Deep Reinforcement Learning Approach", "comment": "15 pages, 13 figures, submitted to IEEE journal for potential\n  publication", "summary": "In the Industrial Internet of Things (IIoT), the frequent transmission of\nlarge amounts of data over wireless networks should meet the stringent\ntimeliness requirements. Particularly, the freshness of packet status updates\nhas a significant impact on the system performance. In this paper, we propose\nan age-of-information (AoI)-aware multi-base station (BS) real-time monitoring\nframework to support extensive IIoT deployments. To meet the freshness\nrequirements of IIoT, we formulate a joint task offloading and resource\nallocation optimization problem with the goal of minimizing long-term average\nAoI. Tackling the core challenges of combinatorial explosion in multi-BS\ndecision spaces and the stochastic dynamics of IIoT systems is crucial, as\nthese factors render traditional optimization methods intractable. Firstly, an\ninnovative branching-based Dueling Double Deep Q-Network (Branching-D3QN)\nalgorithm is proposed to effectively implement task offloading, which optimizes\nthe convergence performance by reducing the action space complexity from\nexponential to linear levels. Then, an efficient optimization solution to\nresource allocation is proposed by proving the semi-definite property of the\nHessian matrix of bandwidth and computation resources. Finally, we propose an\niterative optimization algorithm for efficient joint task offloading and\nresource allocation to achieve optimal average AoI performance. Extensive\nsimulations demonstrate that our proposed Branching-D3QN algorithm outperforms\nboth state-of-the-art DRL methods and classical heuristics, achieving up to a\n75% enhanced convergence speed and at least a 22% reduction in the long-term\naverage AoI.", "AI": {"tldr": "本文提出了一种针对工业物联网（IIoT）的多基站实时监测框架，通过创新的Branching-D3QN算法和迭代优化方法，解决了任务卸载和资源分配问题，以最小化长期平均信息年龄（AoI），显著提升了收敛速度并降低了AoI。", "motivation": "工业物联网（IIoT）中，无线网络需频繁传输大量数据，并满足严格的实时性要求，尤其是数据新鲜度（AoI）对系统性能有重大影响。现有方法难以应对多基站决策空间的组合爆炸和IIoT系统的随机动态性带来的挑战。", "method": "1. 提出了一个感知AoI的多基站实时监测框架。2. 构建了一个联合任务卸载和资源分配优化问题，目标是最小化长期平均AoI。3. 提出了一种创新的分支式Dueling Double Deep Q-Network (Branching-D3QN) 算法，将动作空间复杂度从指数级降至线性级，以有效实现任务卸载。4. 通过证明带宽和计算资源Hessian矩阵的半定性，提出了一种高效的资源分配优化方案。5. 提出了一种迭代优化算法，用于联合任务卸载和资源分配，以实现最优的平均AoI性能。", "result": "仿真结果表明，所提出的Branching-D3QN算法优于最先进的深度强化学习（DRL）方法和传统启发式算法。具体表现为：收敛速度提高了高达75%，长期平均AoI至少降低了22%。", "conclusion": "本文提出的感知AoI的多基站实时监测框架及其Branching-D3QN和迭代优化算法，能够有效应对IIoT的实时性需求和系统复杂性挑战，显著优化了任务卸载和资源分配，从而大幅降低了信息年龄并提升了系统性能。"}}
{"id": "2510.16394", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16394", "abs": "https://arxiv.org/abs/2510.16394", "authors": ["Jinqi Zhang", "Lamei Zhang", "Bin Zou"], "title": "FSAR-Cap: A Fine-Grained Two-Stage Annotated Dataset for SAR Image Captioning", "comment": "5pages,4figures", "summary": "Synthetic Aperture Radar (SAR) image captioning enables scene-level semantic\nunderstanding and plays a crucial role in applications such as military\nintelligence and urban planning, but its development is limited by the scarcity\nof high-quality datasets. To address this, we present FSAR-Cap, a large-scale\nSAR captioning dataset with 14,480 images and 72,400 image-text pairs. FSAR-Cap\nis built on the FAIR-CSAR detection dataset and constructed through a two-stage\nannotation strategy that combines hierarchical template-based representation,\nmanual verification and supplementation, prompt standardization. Compared with\nexisting resources, FSAR-Cap provides richer fine-grained annotations, broader\ncategory coverage, and higher annotation quality. Benchmarking with multiple\nencoder-decoder architectures verifies its effectiveness, establishing a\nfoundation for future research in SAR captioning and intelligent image\ninterpretation.", "AI": {"tldr": "本文提出了FSAR-Cap，一个大规模的SAR图像描述数据集，旨在解决高质量数据集稀缺的问题，并为SAR图像描述和智能图像解释研究奠定基础。", "motivation": "合成孔径雷达（SAR）图像描述对于场景级语义理解至关重要，在军事情报和城市规划等领域具有重要应用，但其发展受限于高质量数据集的稀缺性。", "method": "FSAR-Cap基于FAIR-CSAR检测数据集构建，采用两阶段标注策略，包括分层模板表示、人工验证和补充以及提示标准化，共包含14,480张图像和72,400个图像-文本对。", "result": "FSAR-Cap数据集提供了更丰富的细粒度标注、更广泛的类别覆盖和更高的标注质量。通过多编码器-解码器架构的基准测试验证了其有效性。", "conclusion": "FSAR-Cap数据集为未来的SAR图像描述和智能图像解释研究奠定了基础，解决了现有资源在标注质量和覆盖范围上的不足。"}}
{"id": "2510.16017", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16017", "abs": "https://arxiv.org/abs/2510.16017", "authors": ["Ibrahim Sheikh Mohamed", "Abdullah Yahya Abdullah Omaisan"], "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects", "comment": null, "summary": "Infrastructure in smart cities is increasingly monitored by networks of\nclosed circuit television (CCTV) cameras. Roads, bridges and tunnels develop\ncracks, potholes, and fluid leaks that threaten public safety and require\ntimely repair. Manual inspection is costly and hazardous, and existing\nautomatic systems typically address individual defect types or provide\nunstructured outputs that cannot directly guide maintenance crews. This paper\nproposes a comprehensive pipeline that leverages street CCTV streams for multi\ndefect detection and segmentation using the YOLO family of object detectors and\npasses the detections to a vision language model (VLM) for scene aware\nsummarization. The VLM generates a structured action plan in JSON format that\nincludes incident descriptions, recommended tools, dimensions, repair plans,\nand urgent alerts. We review literature on pothole, crack and leak detection,\nhighlight recent advances in large vision language models such as QwenVL and\nLLaVA, and describe the design of our early prototype. Experimental evaluation\non public datasets and captured CCTV clips demonstrates that the system\naccurately identifies diverse defects and produces coherent summaries. We\nconclude by discussing challenges and directions for scaling the system to city\nwide deployments.", "AI": {"tldr": "本文提出一个利用城市CCTV视频流的综合管道，结合YOLO目标检测器和视觉语言模型（VLM），实现多缺陷检测、分割和场景感知总结，并生成结构化的JSON行动计划，以指导基础设施维护。", "motivation": "智慧城市基础设施（如道路、桥梁、隧道）的缺陷（裂缝、坑洼、渗漏）威胁公共安全并需及时修复。手动检查成本高且危险，现有自动化系统通常只针对单一缺陷或提供非结构化输出，无法直接指导维护人员。", "method": "该研究提出一个综合管道：首先利用YOLO系列目标检测器对街头CCTV视频流进行多缺陷检测和分割；然后将检测结果传递给视觉语言模型（VLM），进行场景感知总结；最后，VLM生成一个结构化的JSON格式行动计划，包含事件描述、推荐工具、尺寸、修复计划和紧急警报。文中还回顾了坑洼、裂缝和渗漏检测的文献，并讨论了QwenVL和LLaVA等大型视觉语言模型的最新进展。", "result": "在公共数据集和捕获的CCTV视频片段上的实验评估表明，该系统能够准确识别各种缺陷并生成连贯的摘要。", "conclusion": "该系统在缺陷识别和总结方面表现良好，但仍面临将系统扩展到全市范围部署的挑战，这些挑战将是未来的研究方向。"}}
{"id": "2510.16079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16079", "abs": "https://arxiv.org/abs/2510.16079", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "comment": null, "summary": "Current Large Language Model (LLM) agents show strong performance in tool\nuse, but lack the crucial capability to systematically learn from their own\nexperiences. While existing frameworks mainly focus on mitigating external\nknowledge gaps, they fail to address a more fundamental limitation: the\ninability to iteratively refine problem-solving strategies. In this work, we\nintroduce EvolveR, a framework designed to enable agent to self-improve through\na complete, closed-loop experience lifecycle. This lifecycle comprises two key\nstages: (1) Offline Self-Distillation, where the agent's interaction\ntrajectories are synthesized into a structured repository of abstract, reusable\nstrategic principles; (2) Online Interaction, where the agent interacts with\ntasks and actively retrieves distilled principles to guide its decision-making,\naccumulating a diverse set of behavioral trajectories. This loop employs a\npolicy reinforcement mechanism to iteratively update the agent based on its\nperformance. We demonstrate the effectiveness of EvolveR on complex multi-hop\nquestion-answering benchmarks, where it achieves superior performance over\nstrong agentic baselines. Our work presents a comprehensive blueprint for\nagents that learn not only from external data but also from the consequences of\ntheir own actions, paving the way for more autonomous and continuously\nimproving systems. Code is available at https://github.com/Edaizi/EvolveR.", "AI": {"tldr": "EvolveR是一个使大型语言模型(LLM)智能体能够通过闭环经验生命周期进行自我改进的框架，它通过离线自蒸馏和在线交互迭代地完善问题解决策略，在复杂多跳问答任务上表现出色。", "motivation": "当前LLM智能体在工具使用方面表现良好，但缺乏从自身经验中系统学习的关键能力。现有框架主要关注弥补外部知识差距，未能解决更根本的局限性：无法迭代地完善问题解决策略。", "method": "EvolveR框架包含一个完整的闭环经验生命周期：1) 离线自蒸馏：将智能体的交互轨迹合成为抽象、可重用的战略原则存储库；2) 在线交互：智能体与任务互动，主动检索提炼出的原则指导决策，并积累多样化的行为轨迹。该循环采用策略强化机制，根据性能迭代更新智能体。", "result": "EvolveR在复杂的、多跳问答基准测试中，表现优于强大的基线智能体，取得了卓越的性能。", "conclusion": "这项工作为智能体提供了一个全面的蓝图，使其不仅能从外部数据中学习，还能从自身行动的后果中学习，为更自主和持续改进的系统铺平了道路。"}}
{"id": "2510.16263", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16263", "abs": "https://arxiv.org/abs/2510.16263", "authors": ["Jierui Peng", "Yanyan Zhang", "Yicheng Duan", "Tuo Liang", "Vipin Chaudhary", "Yu Yin"], "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?", "comment": "Homepage: https://vulab-ai.github.io/NEBULA-Alpha/", "summary": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the\ncoarse, end-task success metric that fails to provide precise skill diagnosis\nor measure robustness to real-world perturbations. This challenge is\nexacerbated by a fragmented data landscape that impedes reproducible research\nand the development of generalist models. To address these limitations, we\nintroduce \\textbf{NEBULA}, a unified ecosystem for single-arm manipulation that\nenables diagnostic and reproducible evaluation. NEBULA features a novel\ndual-axis evaluation protocol that combines fine-grained \\textit{capability\ntests} for precise skill diagnosis with systematic \\textit{stress tests} that\nmeasure robustness. A standardized API and a large-scale, aggregated dataset\nare provided to reduce fragmentation and support cross-dataset training and\nfair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle\nwith key capabilities such as spatial reasoning and dynamic adaptation, which\nare consistently obscured by conventional end-task success metrics. By\nmeasuring both what an agent can do and when it does so reliably, NEBULA\nprovides a practical foundation for robust, general-purpose embodied agents.", "AI": {"tldr": "NEBULA是一个统一的VLA代理评估生态系统，通过诊断性能力测试和系统性压力测试，解决了现有评估粗糙、数据分散的问题，揭示了当前VLA在空间推理和动态适应方面的不足。", "motivation": "现有VLA代理的评估受限于粗糙的最终任务成功指标，无法提供精确的技能诊断或衡量对真实世界扰动的鲁棒性。此外，数据分散也阻碍了可复现研究和通用模型的开发。", "method": "引入NEBULA，一个用于单臂操作的统一生态系统。它采用新颖的双轴评估协议，结合细粒度的“能力测试”进行精确技能诊断和系统的“压力测试”衡量鲁棒性。同时提供标准化API和大规模聚合数据集，以减少碎片化并支持跨数据集训练和公平比较。", "result": "通过NEBULA评估，发现表现最佳的VLA在空间推理和动态适应等关键能力上仍有不足，而这些问题常被传统的最终任务成功指标所掩盖。", "conclusion": "NEBULA通过同时衡量代理的能力和其可靠性，为开发鲁棒、通用型具身代理提供了实用基础。"}}
{"id": "2510.16451", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16451", "abs": "https://arxiv.org/abs/2510.16451", "authors": ["Lidong Li", "Rui Huang", "Lin Zhao"], "title": "Stabilization of Nonlinear Systems with State-Dependent Representation: From Model-Based to Direct Data-Driven Control", "comment": null, "summary": "This paper presents a novel framework for stabilizing nonlinear systems\nrepresented in state-dependent form. We first reformulate the nonlinear\ndynamics as a state-dependent parameter-varying model and synthesize a\nstabilizing controller offline via tractable linear matrix inequalities (LMIs).\nThe resulting controller guarantees local exponential stability, maintains\nrobustness against disturbances, and provides an estimate of the region of\nattraction under input saturation. We then extend the formulation to the direct\ndata-driven setting, where a known library of basis functions represents the\ndynamics with unknown coefficients consistent with noisy experimental data. By\nleveraging Petersen's lemma, we derive data-dependent LMIs that ensure\nstability and robustness for all systems compatible with the data. Numerical\nand physical experimental results validate that our approach achieves rigorous\nend-to-end guarantees on stability, robustness, and safety directly from finite\ndata without explicit model identification.", "AI": {"tldr": "本文提出了一种稳定非线性系统的新框架，通过将非线性动力学重构为状态依赖的参数时变模型，并利用线性矩阵不等式（LMIs）离线合成控制器。该方法进一步扩展到数据驱动设置，通过数据依赖的LMIs，直接从有限数据中实现稳定性、鲁棒性和安全性保证，无需明确的模型识别。", "motivation": "稳定非线性系统是一个挑战，尤其是在存在扰动、输入饱和或仅有噪声实验数据的情况下，需要一种能够提供严格稳定性、鲁棒性和安全保证的方法。", "method": "1. 将非线性动力学重构为状态依赖的参数时变模型。\n2. 通过可处理的线性矩阵不等式（LMIs）离线合成稳定控制器，保证局部指数稳定性、抗扰动鲁棒性并估计吸引域。\n3. 将该方法扩展到直接数据驱动设置，利用基函数库表示动力学，并结合Petersen引理推导出数据依赖的LMIs，确保与数据兼容的所有系统的稳定性和鲁棒性。", "result": "所设计的控制器保证了局部指数稳定性，对扰动具有鲁棒性，并能估计输入饱和下的吸引域。在数据驱动设置中，该方法直接从有限数据中实现了关于稳定性、鲁棒性和安全性的严格端到端保证，无需显式模型识别。数值和物理实验结果验证了该方法的有效性。", "conclusion": "本文提出了一种新颖的框架，能够稳定状态依赖形式的非线性系统，无论是基于模型的离线合成还是直接数据驱动的方式。通过LMIs和数据依赖的LMIs，该方法在不进行明确模型识别的情况下，为系统的稳定性、鲁棒性和安全性提供了严格的保证。"}}
{"id": "2510.16070", "categories": ["cs.CV", "cs.AI", "cs.HC", "eess.IV", "H.5.5; H.1.2; I.4.0"], "pdf": "https://arxiv.org/pdf/2510.16070", "abs": "https://arxiv.org/abs/2510.16070", "authors": ["Mahta Khoobi", "Marc Sebastian von der Stueck", "Felix Barajas Ordonez", "Anca-Maria Iancu", "Eric Corban", "Julia Nowak", "Aleksandar Kargaliev", "Valeria Perelygina", "Anna-Sophie Schott", "Daniel Pinto dos Santos", "Christiane Kuhl", "Daniel Truhn", "Sven Nebelung", "Robert Siepmann"], "title": "Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography", "comment": "Preprint version - Under second revision at Radiology (manuscript\n  RAD-25-1348)", "summary": "Structured reporting (SR) and artificial intelligence (AI) may transform how\nradiologists interact with imaging studies. This prospective study (July to\nDecember 2024) evaluated the impact of three reporting modes: free-text (FT),\nstructured reporting (SR), and AI-assisted structured reporting (AI-SR), on\nimage analysis behavior, diagnostic accuracy, efficiency, and user experience.\nFour novice and four non-novice readers (radiologists and medical students)\neach analyzed 35 bedside chest radiographs per session using a customized\nviewer and an eye-tracking system. Outcomes included diagnostic accuracy\n(compared with expert consensus using Cohen's $\\kappa$), reporting time per\nradiograph, eye-tracking metrics, and questionnaire-based user experience.\nStatistical analysis used generalized linear mixed models with Bonferroni\npost-hoc tests with a significance level of ($P \\le .01$). Diagnostic accuracy\nwas similar in FT ($\\kappa = 0.58$) and SR ($\\kappa = 0.60$) but higher in\nAI-SR ($\\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \\pm 38$\ns (FT) to $37 \\pm 18$ s (SR) and $25 \\pm 9$ s (AI-SR) ($P < .001$). Saccade\ncounts for the radiograph field ($205 \\pm 135$ (FT), $123 \\pm 88$ (SR), $97 \\pm\n58$ (AI-SR)) and total fixation duration for the report field ($11 \\pm 5$ s\n(FT), $5 \\pm 3$ s (SR), $4 \\pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <\n.001$ each). Novice readers shifted gaze towards the radiograph in SR, while\nnon-novice readers maintained their focus on the radiograph. AI-SR was the\npreferred mode. In conclusion, SR improves efficiency by guiding visual\nattention toward the image, and AI-prefilled SR further enhances diagnostic\naccuracy and user satisfaction.", "AI": {"tldr": "本研究比较了自由文本(FT)、结构化报告(SR)和AI辅助结构化报告(AI-SR)三种模式对放射科医生图像分析行为、诊断准确性、效率和用户体验的影响。结果显示，AI-SR显著提高了诊断准确性和效率，并受到用户青睐，SR也提升了效率。", "motivation": "结构化报告(SR)和人工智能(AI)有望改变放射科医生与影像学检查的互动方式。本研究旨在前瞻性评估这两种技术及其结合对放射科医生工作流程和结果的具体影响。", "method": "本前瞻性研究（2024年7月至12月）评估了三种报告模式（自由文本、结构化报告、AI辅助结构化报告）的影响。四名新手和四名非新手读者（放射科医生和医学生）每人使用定制查看器和眼动追踪系统分析了35张床旁胸部X光片。评估指标包括诊断准确性（与专家共识比较）、每张X光片的报告时间、眼动追踪指标和基于问卷的用户体验。统计分析采用广义线性混合模型和Bonferroni事后检验，显著性水平为P ≤ .01。", "result": "诊断准确性在FT（κ=0.58）和SR（κ=0.60）中相似，但在AI-SR中更高（κ=0.71，P < .001）。报告时间从FT的88 ± 38秒减少到SR的37 ± 18秒和AI-SR的25 ± 9秒（P < .001）。X光片区域的眼跳计数和报告区域的总注视时间在SR和AI-SR模式下均较低（P < .001）。新手读者在SR中将目光转向X光片，而非新手读者则保持对X光片的关注。AI-SR是首选模式。", "conclusion": "结构化报告（SR）通过引导视觉注意力到图像上提高了效率，而AI预填充的SR进一步提高了诊断准确性和用户满意度。"}}
{"id": "2510.16428", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16428", "abs": "https://arxiv.org/abs/2510.16428", "authors": ["Alok Panigrahi", "Jayaprakash Katual", "Satish Mulleti"], "title": "Dictionary-Based Deblurring for Unpaired Data", "comment": "10 pages", "summary": "Effective image deblurring typically relies on large and fully paired\ndatasets of blurred and corresponding sharp images. However, obtaining such\naccurately aligned data in the real world poses a number of difficulties,\nlimiting the effectiveness and generalizability of existing deblurring methods.\nTo address this scarcity of data dependency, we present a novel dictionary\nlearning based deblurring approach for jointly estimating a structured blur\nmatrix and a high resolution image dictionary. This framework enables robust\nimage deblurring across different degrees of data supervision. Our method is\nthoroughly evaluated across three distinct experimental settings: (i) full\nsupervision involving paired data with explicit correspondence, (ii) partial\nsupervision employing unpaired data with implicit relationships, and (iii)\nunsupervised learning using non-correspondence data where direct pairings are\nabsent. Extensive experimental validation, performed on synthetically blurred\nsubsets of the CMU-Cornell iCoseg dataset and the real-world FocusPath dataset,\nconsistently shows that the proposed framework has superior performance\ncompared to conventional coupled dictionary learning approaches. The results\nvalidate that our approach provides an efficient and robust solution for image\ndeblurring in data-constrained scenarios by enabling accurate blur modeling and\nadaptive dictionary representation with a notably smaller number of training\nsamples.", "AI": {"tldr": "本文提出了一种新颖的基于字典学习的图像去模糊方法，该方法能联合估计结构化模糊矩阵和高分辨率图像字典，在全监督、部分监督和无监督等不同数据监督程度下均表现出色，尤其适用于数据受限场景。", "motivation": "现有的图像去模糊方法通常依赖于大量且完全配对的模糊-清晰图像数据集，但在实际世界中获取此类精确对齐的数据非常困难，这限制了现有方法的有效性和泛化能力。", "method": "本文提出了一种基于字典学习的去模糊方法，该方法能联合估计结构化模糊矩阵和高分辨率图像字典。此框架支持在不同程度的数据监督下进行鲁棒的图像去模糊，包括全监督（配对数据）、部分监督（未配对数据）和无监督（无对应关系数据）学习。", "result": "在CMU-Cornell iCoseg数据集的合成模糊子集和真实世界的FocusPath数据集上进行的广泛实验验证表明，与传统的耦合字典学习方法相比，所提出的框架性能更优。结果一致证明，该方法在数据受限场景下，通过准确的模糊建模和自适应字典表示，仅用少量训练样本即可提供高效且鲁棒的图像去模糊解决方案。", "conclusion": "本文的方法通过实现精确的模糊建模和自适应字典表示，显著减少了所需的训练样本数量，为数据受限场景下的图像去模糊提供了一种高效且鲁棒的解决方案。"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "本研究量化了提示策略与大型语言模型（LLMs）在自动化系统文献综述（SLR）筛选阶段的交互作用，并提供了针对任务的LLM部署基准和实用指南。", "motivation": "本研究旨在量化提示策略与大型语言模型（LLMs）如何相互作用，以自动化系统文献综述（SLR）的筛选阶段，从而提高SLR的效率。", "method": "研究评估了六种LLM（GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3, Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick），采用五种提示类型（零样本、少样本、思维链CoT、CoT-少样本、自我反思），在相关性分类和六个二级任务上，使用准确率、精确率、召回率和F1分数进行评估，并进行了成本-性能分析。", "result": "结果显示显著的模型-提示交互效应：CoT-少样本提供最可靠的精确率-召回率平衡；零样本最大化高灵敏度通过的召回率；自我反思因过度包容和模型间不稳定性而表现不佳。GPT-4o和DeepSeek提供稳健的整体性能，而GPT-4o-mini以显著更低的成本表现出竞争力。成本-性能分析表明，GPT-4o-mini在各种提示下成本较低，且其上的结构化提示（CoT/CoT-少样本）能以较小的增量成本提供有吸引力的F1分数。", "conclusion": "LLMs在自动化文献筛选方面潜力巨大，但表现不均。研究建议采用分阶段工作流程：首先使用低成本模型和结构化提示进行初筛，然后仅将边缘案例升级到更高容量的模型。通过系统分析提示-模型交互，本研究提供了比较基准和任务自适应LLM部署的实用指导。"}}
{"id": "2510.16534", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16534", "abs": "https://arxiv.org/abs/2510.16534", "authors": ["Christoph Kaufmann", "Georg Pangalos", "Gerwald Lichtenberg", "Oriol Gomis-Bellmunt"], "title": "Small-Signal Stability Analysis of Power Systems by Implicit Multilinear Models", "comment": null, "summary": "This paper proposes a new approach to perform small-signal stability analysis\nbased on linearization of implicit multilinear models. Multilinear models\ndescribe the system dynamics by multilinear functions of state, input, and\nalgebraic variables. Using suitable transformations of variables, they can also\nrepresent trigonometric functions, which often occur in power systems modeling.\nThis allows tensor representations of grid-following and grid-forming power\nconverters. This paper introduces small-signal stability analysis of\nequilibrium points based on implicit multilinear models using generalized\neigenvalues. The generalized eigenvalues are computed from linear descriptor\nmodels of the linearized implicit multilinear model. The proposed approach is\ntested using a 3-bus network example, first by comparing time-domain\nsimulations of the implicit multilinear model with those of the nonlinear\nmodel, and second by comparing the generalized eigenvalues with those of the\nlinearized nonlinear model. The results show that the decomposed tensor\nrepresentation of the implicit multilinear model allows for a faster\nlinearization compared to conventional methods in MATLAB Simulink.", "AI": {"tldr": "本文提出了一种基于隐式多线性模型线性化的新方法，用于小信号稳定性分析，通过广义特征值计算，并在电网示例中展示了比传统方法更快的线性化速度。", "motivation": "需要一种新的方法来执行小信号稳定性分析，能够有效描述包含三角函数（常见于电力系统建模）的复杂系统动态，并处理电网跟随型和电网形成型电力转换器。", "method": "本文提出使用隐式多线性模型来描述系统动态，这些模型是状态、输入和代数变量的多线性函数。通过变量变换，它们可以表示三角函数。该方法引入了基于广义特征值的平衡点小信号稳定性分析，这些特征值从线性化隐式多线性模型的线性描述符模型中计算。通过一个3总线网络示例进行测试，将隐式多线性模型的时间域仿真与非线性模型进行比较，并将广义特征值与线性化非线性模型进行比较。", "result": "研究结果表明，隐式多线性模型的时间域仿真与非线性模型吻合良好，广义特征值与线性化非线性模型的特征值一致。此外，隐式多线性模型的分解张量表示法比MATLAB Simulink中的传统方法实现了更快的线性化速度。", "conclusion": "所提出的基于隐式多线性模型的小信号稳定性分析方法是有效的，并且其分解张量表示能够实现比传统方法更快的线性化，为电力系统分析提供了一种有前景的工具。"}}
{"id": "2510.17037", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.17037", "abs": "https://arxiv.org/abs/2510.17037", "authors": ["Chongyuan Bi", "Jie Liang"], "title": "A Low-Complexity View Synthesis Distortion Estimation Method for 3D Video with Large Baseline Considerations", "comment": null, "summary": "Depth-image-based rendering is a key view synthesis algorithm in 3D video\nsystems, which enables the synthesis of virtual views from texture images and\ndepth maps. An efficient view synthesis distortion estimation model is critical\nfor optimizing resource allocation in real-time applications such as\ninteractive free-viewpoint video and 3D video streaming services. However,\nexisting estimation methods are often computationally intensive, require\nparameter training, or performance poorly in challenging large baseline\nconfigurations. This paper presents a novel, low-complexity, and training-free\nmethod to accurately estimate the distortion of synthesized views without\nperforming the actual rendering process. Key contributions include: (1) A joint\ntexture-depth classification method that accurately separates texture image\ninto locally stationary and non-stationary regions, which mitigates\nmisclassifications by using texture-only methods. (2) A novel baseline distance\nindicator is designed for the compensation scheme for distortions caused by\nlarge baseline configurations. (3) A region-based blending estimation strategy\nthat geometrically identifies overlapping, single-view, and mutual disocclusion\nregions, predicting distortion in synthesized views from two reference views\nwith differing synthesis conditions. Experiments on standard MPEG 3D video\nsequences validate the proposed method's high accuracy and efficiency,\nespecially in large baseline configurations. This method enables more flexible\ncamera arrangements in 3D content acquisition by accurately predicting\nsynthesis quality under challenging geometric configurations.", "AI": {"tldr": "本文提出了一种新颖、低复杂度、免训练的方法，用于准确估计基于深度图像渲染（DIBR）的合成视图失真，尤其在大的基线配置下表现出色。", "motivation": "深度图像渲染是3D视频系统中的关键视图合成算法。高效的视图合成失真估计模型对于实时应用（如交互式自由视点视频和3D视频流服务）中的资源优化至关重要。然而，现有方法通常计算量大、需要参数训练，或在大的基线配置下性能不佳。", "method": "该方法包括：1) 联合纹理-深度分类，将纹理图像准确分为局部静止和非静止区域，以减少纯纹理方法的误分类。2) 设计了一种新颖的基线距离指标，用于补偿大基线配置引起的失真。3) 采用基于区域的融合估计策略，几何识别重叠、单视图和相互遮挡区域，从而预测不同合成条件下两个参考视图的合成视图失真。", "result": "在标准MPEG 3D视频序列上的实验验证了所提方法的高准确性和效率，特别是在大基线配置下表现优异。", "conclusion": "该方法能够准确预测挑战性几何配置下的合成质量，从而实现3D内容采集中更灵活的摄像机布置。"}}
{"id": "2510.16281", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16281", "abs": "https://arxiv.org/abs/2510.16281", "authors": ["Yilin Wu", "Anqi Li", "Tucker Hermans", "Fabio Ramos", "Andrea Bajcsy", "Claudia P'erez-D'Arpino"], "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification", "comment": null, "summary": "Reasoning Vision Language Action (VLA) models improve robotic\ninstruction-following by generating step-by-step textual plans before low-level\nactions, an approach inspired by Chain-of-Thought (CoT) reasoning in language\nmodels. Yet even with a correct textual plan, the generated actions can still\nmiss the intended outcomes in the plan, especially in out-of-distribution (OOD)\nscenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,\nand introduce a training-free, runtime policy steering method for\nreasoning-action alignment. Given a reasoning VLA's intermediate textual plan,\nour framework samples multiple candidate action sequences from the same model,\npredicts their outcomes via simulation, and uses a pre-trained Vision-Language\nModel (VLM) to select the sequence whose outcome best aligns with the VLA's own\ntextual plan. Only executing action sequences that align with the textual\nreasoning turns our base VLA's natural action diversity from a source of error\ninto a strength, boosting robustness to semantic and visual OOD perturbations\nand enabling novel behavior composition without costly re-training. We also\ncontribute a reasoning-annotated extension of LIBERO-100, environment\nvariations tailored for OOD evaluation, and demonstrate up to 15% performance\ngain over prior work on behavior composition tasks and scales with compute and\ndata diversity. Project Website at:\nhttps://yilin-wu98.github.io/steering-reasoning-vla/", "AI": {"tldr": "本文提出了一种无需训练的运行时策略引导方法，通过模拟预测候选动作序列的结果并使用预训练的视觉-语言模型（VLM）选择与文本规划最匹配的动作，从而提高推理视觉-语言-动作（VLA）模型在分布外（OOD）场景下动作与规划的一致性。", "motivation": "推理VLA模型虽然能生成分步文本规划（CoT），但其生成的低级动作可能无法实现规划意图，尤其是在OOD场景中。这种现象被称为“具身CoT忠实度不足”，即动作与规划之间缺乏对齐。", "method": "该方法是一种无需训练的运行时策略引导框架，用于实现推理与动作的对齐。它首先从给定的VLA模型中采样多个候选动作序列，然后通过模拟预测这些序列的潜在结果。接着，利用一个预训练的VLM来选择其结果与VLA自身文本规划最匹配的动作序列。最终，只执行与文本推理对齐的动作序列。此外，本文还贡献了一个带有推理标注的LIBERO-100扩展数据集和用于OOD评估的环境变体。", "result": "该方法将基础VLA模型固有的动作多样性从错误源转化为优势，显著增强了模型对语义和视觉OOD扰动的鲁棒性。它还无需昂贵的重新训练即可实现新颖的行为组合，并在行为组合任务上比现有技术实现了高达15%的性能提升，且性能随计算资源和数据多样性而扩展。", "conclusion": "通过引入一种训练无关的策略引导方法，本文成功解决了推理VLA模型在OOD场景下动作与文本规划不一致的问题。该方法通过利用模型自身的动作多样性并结合VLM进行结果对齐选择，显著提升了模型的鲁棒性和行为组合能力，为机器人指令遵循提供了更可靠的解决方案。"}}
{"id": "2510.16072", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16072", "abs": "https://arxiv.org/abs/2510.16072", "authors": ["Farjana Yesmin"], "title": "Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation", "comment": "18 pages", "summary": "Machine learning models trained on imbalanced datasets often exhibit\nintersectional biases-systematic errors arising from the interaction of\nmultiple attributes such as object class and environmental conditions. This\npaper presents a data-driven framework for analyzing and mitigating such biases\nin image classification. We introduce the Intersectional Fairness Evaluation\nFramework (IFEF), which combines quantitative fairness metrics with\ninterpretability tools to systematically identify bias patterns in model\npredictions. Building on this analysis, we propose Bias-Weighted Augmentation\n(BWA), a novel data augmentation strategy that adapts transformation\nintensities based on subgroup distribution statistics. Experiments on the Open\nImages V7 dataset with five object classes demonstrate that BWA improves\naccuracy for underrepresented class-environment intersections by up to 24\npercentage points while reducing fairness metric disparities by 35%.\nStatistical analysis across multiple independent runs confirms the significance\nof improvements (p < 0.05). Our methodology provides a replicable approach for\nanalyzing and addressing intersectional biases in image classification systems.", "AI": {"tldr": "本文提出一个数据驱动框架，用于分析和缓解图像分类中由不平衡数据集引起的交叉偏见，并引入了交叉公平性评估框架（IFEF）和偏见加权增强（BWA）数据增强策略。", "motivation": "机器学习模型在不平衡数据集上训练时，常因多个属性（如对象类别和环境条件）的交互作用而表现出交叉偏见，即系统性错误。", "method": "引入了交叉公平性评估框架（IFEF），结合定量公平性指标和可解释性工具来系统识别模型预测中的偏见模式。在此基础上，提出偏见加权增强（BWA），这是一种新颖的数据增强策略，根据子组分布统计数据调整变换强度。", "result": "在Open Images V7数据集上的实验表明，BWA使代表性不足的类别-环境交叉点的准确率提高了24个百分点，同时将公平性指标差异降低了35%。多轮独立运行的统计分析证实了改进的显著性（p < 0.05）。", "conclusion": "所提出的方法为分析和解决图像分类系统中的交叉偏见提供了一种可复现的途径。"}}
{"id": "2510.16308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16308", "abs": "https://arxiv.org/abs/2510.16308", "authors": ["Chi Zhang", "Xian Huang", "Wei Dong"], "title": "SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling", "comment": null, "summary": "UAVs equipped with a single depth camera encounter significant challenges in\ndynamic obstacle avoidance due to limited field of view and inevitable blind\nspots. While active vision strategies that steer onboard cameras have been\nproposed to expand sensing coverage, most existing methods separate motion\nplanning from sensing considerations, resulting in less effective and delayed\nobstacle response. To address this limitation, we introduce SPOT\n(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning\nframework for observation-aware trajectory planning that explicitly\nincorporates sensing objectives into motion optimization. At the core of our\nmethod is a Gaussian Process-based obstacle belief map, which establishes a\nunified probabilistic representation of both recognized (previously observed)\nand potential obstacles. This belief is further processed through a\ncollision-aware inference mechanism that transforms spatial uncertainty and\ntrajectory proximity into a time-varying observation urgency map. By\nintegrating urgency values within the current field of view, we define\ndifferentiable objectives that enable real-time, observation-aware trajectory\nplanning with computation times under 10 ms. Simulation and real-world\nexperiments in dynamic, cluttered, and occluded environments show that our\nmethod detects potential dynamic obstacles 2.8 seconds earlier than baseline\napproaches, increasing dynamic obstacle visibility by over 500\\%, and enabling\nsafe navigation through cluttered, occluded environments.", "AI": {"tldr": "本文提出SPOT框架，通过障碍物威胁建模将感知目标融入运动优化，为配备单深度摄像头的无人机实现观察感知的轨迹规划，显著提高动态障碍物检测能力和导航安全性。", "motivation": "配备单深度摄像头的无人机在动态避障中面临视野有限和盲点等挑战。现有方法通常将运动规划与感知分离，导致障碍物响应效果不佳且延迟。", "method": "本文引入SPOT（Sensing-augmented Planning via Obstacle Threat modeling）框架，将感知目标明确融入运动优化中。核心是一个基于高斯过程的障碍物信念图，统一表示已知和潜在障碍物。通过碰撞感知推理机制，将空间不确定性和轨迹接近度转化为时变的观察紧迫性图。通过整合视野内的紧迫性值，定义可微分目标，实现实时、观察感知的轨迹规划。", "result": "该方法计算时间低于10毫秒。在动态、杂乱和遮挡环境中，比基线方法能提前2.8秒检测到潜在动态障碍物，将动态障碍物可见性提高500%以上，并能安全地通过杂乱、遮挡的环境。", "conclusion": "SPOT框架通过将感知目标与运动规划统一，显著解决了单深度摄像头无人机在动态避障中的局限性，实现了更早的障碍物检测和更安全的导航。"}}
{"id": "2510.17427", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17427", "abs": "https://arxiv.org/abs/2510.17427", "authors": ["Julien Zouein", "Vibhoothi Vibhoothi", "Anil Kokaram"], "title": "AV1 Motion Vector Fidelity and Application for Efficient Optical Flow", "comment": "Accepted PCS 2025, camera-ready version", "summary": "This paper presents a comprehensive analysis of motion vectors extracted from\nAV1-encoded video streams and their application in accelerating optical flow\nestimation. We demonstrate that motion vectors from AV1 video codec can serve\nas a high-quality and computationally efficient substitute for traditional\noptical flow, a critical but often resource-intensive component in many\ncomputer vision pipelines. Our primary contributions are twofold. First, we\nprovide a detailed comparison of motion vectors from both AV1 and HEVC against\nground-truth optical flow, establishing their fidelity. In particular we show\nthe impact of encoder settings on motion estimation fidelity and make\nrecommendations about the optimal settings. Second, we show that using these\nextracted AV1 motion vectors as a \"warm-start\" for a state-of-the-art deep\nlearning-based optical flow method, RAFT, significantly reduces the time to\nconvergence while achieving comparable accuracy. Specifically, we observe a\nfour-fold speedup in computation time with only a minor trade- off in end-point\nerror. These findings underscore the potential of reusing motion vectors from\ncompressed video as a practical and efficient method for a wide range of\nmotion-aware computer vision applications.", "AI": {"tldr": "本文分析了AV1视频编码流中的运动向量，并证明它们可以作为光流估计的高效替代品或加速器，显著提高计算速度。", "motivation": "传统光流估计是计算机视觉中关键但资源密集型组件，需要更高效的替代方案或加速方法。", "method": "首先，将AV1和HEVC的运动向量与真实光流进行详细比较，评估其保真度，并分析编码器设置的影响。其次，将提取的AV1运动向量作为最先进的深度学习光流方法RAFT的“热启动”，以加速其收敛。", "result": "研究表明，AV1运动向量具有高保真度，并提供了关于最佳编码器设置的建议。此外，使用AV1运动向量作为RAFT的热启动，计算时间缩短了四倍，而终点误差（accuracy）仅有微小权衡。", "conclusion": "这些发现强调了重用压缩视频中的运动向量在各种运动感知计算机视觉应用中作为实用且高效方法的巨大潜力。"}}
{"id": "2510.16550", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16550", "abs": "https://arxiv.org/abs/2510.16550", "authors": ["Siyuan Yin", "Yuncheng Xu", "Lin Liu", "Fan Yang", "Xuan Zeng", "Chengtao An", "Yangfeng Su"], "title": "SMP-RCR: A Sparse Multipoint Moment Matching Method for RC Reduction", "comment": null, "summary": "In post--layout circuit simulation, efficient model order reduction (MOR) for\nmany--port resistor--capacitor (RC) circuits remains a crucial issue. The\ncurrent mainstream MOR methods for such circuits include high--order moment\nmatching methods and elimination methods. High-order moment matching\nmethods--characterized by high accuracy, such as PRIMA and TurboMOR--tend to\ngenerate large dense reduced-order systems when the number of ports is large,\nwhich impairs the efficiency of MOR. Another common type of MOR method for\nmany--port circuits is based on Gaussian elimination, with the SIP method as a\nrepresentative. The main limitation of this method lies in the inadequate\nmatching of high--order moments. In this paper, we propose a sparse multipoint\nmoment matching method and present comprehensive theoretical analysis results\nregarding the multi--frequency high--order moment matching property. Meanwhile,\nto enhance the algorithm's efficiency, sparse control and deflation techniques\nare introduced to further optimize the algorithm. Numerical experiments\ndemonstrated that, compared to SIP, the accuracy is improved by more than two\norders of magnitude at high frequency points without adding many extra linear\ncomponents. Compared to TurboMOR methods, our method achieves a speed\nimprovement of more than twice while maintaining the same level of precision.", "AI": {"tldr": "本文提出了一种稀疏多点矩匹配模型降阶（MOR）方法，用于多端口RC电路，旨在解决现有方法在准确性和效率方面的局限性。", "motivation": "在后布局电路仿真中，多端口电阻-电容（RC）电路的高效模型降阶（MOR）是一个关键问题。现有方法存在不足：高阶矩匹配方法（如PRIMA、TurboMOR）在端口数量多时会生成大型密集降阶系统，影响效率；而基于高斯消元的消除方法（如SIP）在高阶矩匹配方面表现不足。", "method": "本文提出了一种稀疏多点矩匹配方法，并进行了多频高阶矩匹配特性的综合理论分析。同时，引入了稀疏控制和缩减技术以提高算法效率。", "result": "数值实验表明，与SIP方法相比，本文方法在不增加太多额外线性组件的情况下，在高频点将精度提高了两个数量级以上。与TurboMOR方法相比，在保持相同精度水平的同时，速度提升了两倍以上。", "conclusion": "所提出的稀疏多点矩匹配方法在多端口RC电路的MOR中，能够显著提高高频精度，并在保持精度的同时提高计算效率，克服了现有主流方法的局限性。"}}
{"id": "2510.16096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16096", "abs": "https://arxiv.org/abs/2510.16096", "authors": ["Tina Behnia", "Puneesh Deora", "Christos Thrampoulidis"], "title": "Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization", "comment": "28 pages, 15 figures", "summary": "Language models are pretrained on sequences that blend statistical\nregularities (making text fluent) with factual associations between specific\ntokens (knowledge of facts). While recent work suggests that the variability of\ntheir interaction, such as paraphrases of factual associations, critically\ndetermines generalization ability, we lack a systematic analysis of these\nimpacts. This paper introduces a flexible synthetic testbed that combines a\nstatistical stream of generic tokens with an abstract factual stream of\nsource-target token pairs, enabling fine-grained control over their\ninteraction. The design enables the independent control of diversity nature by\nmanipulating stream composition (contextual structure) and the diversity level\nby varying which statistical streams each fact appears in. Through controlled\nexperiments, we find that while higher contextual diversity delays\nin-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)\nfactual generalization depends critically on contextual structure. In some\ncases, OOD performance follows the same trend as ID, but in others, diversity\nbecomes essential for non-trivial factual recall. Even when low diversity\nprohibits factual recall, optimal diversity levels depend on training duration.\nBeyond factual recall failures, we identify structures where statistical\ngeneralization fails independently, and others where both capabilities degrade.\nThis shows how the interplay between contextual design and diversity level\nimpacts different generalization aspects. Further, through a series of\ncontrolled interventions on the model components, we trace the OOD failures to\ndistinct optimization bottlenecks, highlighting the importance of the embedding\nand unembedding layers. Our synthetic framework allows us to isolate effects\nthat would be confounded in large-scale studies, offering a controlled testbed\nfor future investigations.", "AI": {"tldr": "本文通过一个灵活的合成测试平台，系统性地分析了语言模型中统计规律和事实关联的交互多样性如何影响其分布内（ID）和分布外（OOD）的泛化能力，并揭示了上下文结构和多样性水平的关键作用。", "motivation": "现有研究表明，语言模型中统计规律与事实关联（如事实的释义）的交互变异性是决定泛化能力的关键，但目前缺乏对其影响的系统性分析。", "method": "引入了一个灵活的合成测试平台，该平台结合了通用标记的统计流和源-目标标记对的抽象事实流。这使得研究人员能够独立控制多样性的性质（通过流组合/上下文结构）和多样性的水平（通过事实出现的统计流）。通过受控实验和对模型组件的干预，分析了这些因素对泛化能力的影响。", "result": "研究发现，较高的上下文多样性会延迟分布内（ID）的事实准确性，但其对分布外（OOD）事实泛化的影响关键取决于上下文结构。在某些情况下，OOD表现与ID趋势一致，但在其他情况下，多样性对于非平凡的事实召回至关重要。即使低多样性阻碍事实召回，最佳多样性水平也取决于训练时长。此外，还发现了统计泛化独立失败的结构，以及两种能力都下降的结构。通过模型组件干预，将OOD失败追溯到不同的优化瓶颈，突出了嵌入和反嵌入层的重要性。", "conclusion": "上下文设计和多样性水平之间的相互作用会影响不同的泛化方面。本研究的合成框架能够分离在大型研究中可能混淆的效应，为未来的研究提供了一个受控的测试平台，并揭示了模型特定组件（如嵌入层）的重要性。"}}
{"id": "2510.16088", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16088", "abs": "https://arxiv.org/abs/2510.16088", "authors": ["Zia Badar"], "title": "Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch", "comment": null, "summary": "Quantization of neural networks provides benefits of inference in less\ncompute and memory requirements. Previous work in quantization lack two\nimportant aspects which this work provides. First almost all previous work in\nquantization used a non-differentiable approach and for learning; the\nderivative is usually set manually in backpropogation which make the learning\nability of algorithm questionable, our approach is not just differentiable, we\nalso provide proof of convergence of our approach to the optimal neural\nnetwork. Second previous work in shift/logrithmic quantization either have\navoided activation quantization along with weight quantization or achieved less\naccuracy. Learning logrithmic quantize values of form $2^n$ requires the\nquantization function can scale to more than 1 bit quantization which is\nanother benifit of our quantization that it provides $n$ bits quantization as\nwell. Our approach when tested with image classification task using imagenet\ndataset, resnet18 and weight quantization only achieves less than 1 percent\naccuracy compared to full precision accuracy while taking only 15 epochs to\ntrain using shift bit quantization and achieves comparable to SOTA approaches\naccuracy in both weight and activation quantization using shift bit\nquantization in 15 training epochs with slightly higher(only higher cpu\ninstructions) inference cost compared to 1 bit quantization(without logrithmic\nquantization) and not requiring any higher precision multiplication.", "AI": {"tldr": "该研究提出了一种可微分的神经网络量化方法，解决了以往非可微分量化和激活量化难题，实现了高精度、快速训练的移位/对数量化。", "motivation": "以往的神经网络量化方法存在两个主要问题：1) 大多数采用非可微分方法，反向传播时导数需手动设置，导致学习能力存疑；2) 移位/对数量化中，要么避免激活量化，要么准确率较低，且通常缺乏对多位量化的支持。", "method": "本文提出了一种可微分的量化方法，并提供了其收敛到最优神经网络的证明。该方法支持n位量化，特别适用于形式为2^n的对数量化值，并可同时对权重和激活进行量化。", "result": "1) 在ImageNet数据集上使用ResNet18进行图像分类任务，仅进行权重移位比特量化时，准确率与全精度模型相比下降不到1%，且仅需15个epoch训练。2) 在权重和激活都进行移位比特量化时，其准确率与SOTA方法相当，同样在15个训练epoch内完成，推理成本略高于1比特量化（无对数量化），但不需要更高精度的乘法。", "conclusion": "本研究提供了一种可微分的、支持多位量化的移位/对数量化方法，有效解决了现有量化方案中非可微分性、激活量化困难以及多位量化支持不足的问题，在保持高准确率的同时显著减少了训练时间。"}}
{"id": "2510.16173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16173", "abs": "https://arxiv.org/abs/2510.16173", "authors": ["Aria Pessianzadeh", "Naima Sultana", "Hildegarde Van den Bulck", "David Gefen", "Shahin Jabari", "Rezvaneh Rezapour"], "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions", "comment": null, "summary": "The rise of generative AI (GenAI) has impacted many aspects of human life. As\nthese systems become embedded in everyday practices, understanding public trust\nin them also becomes essential for responsible adoption and governance. Prior\nwork on trust in AI has largely drawn from psychology and human-computer\ninteraction, but there is a lack of computational, large-scale, and\nlongitudinal approaches to measuring trust and distrust in GenAI and large\nlanguage models (LLMs). This paper presents the first computational study of\nTrust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)\nspanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a\nrepresentative sample were combined with classification models to scale\nanalysis. We find that Trust and Distrust are nearly balanced over time, with\nshifts around major model releases. Technical performance and usability\ndominate as dimensions, while personal experience is the most frequent reason\nshaping attitudes. Distinct patterns also emerge across trustors (e.g.,\nexperts, ethicists, general users). Our results provide a methodological\nframework for large-scale Trust analysis and insights into evolving public\nperceptions of GenAI.", "AI": {"tldr": "本研究首次通过计算方法，利用Reddit多年度数据分析了公众对生成式AI（GenAI）的信任与不信任。发现信任与不信任大致平衡，并随模型发布而波动，技术性能和可用性是主要维度，个人经验是影响态度的最常见原因。", "motivation": "随着生成式AI日益融入日常生活，理解公众对其的信任对于负责任的采纳和治理至关重要。然而，以往对AI信任的研究多基于心理学和人机交互，缺乏针对GenAI和大型语言模型的计算性、大规模和长期性方法。", "method": "本研究是首个针对GenAI信任与不信任的计算性研究。使用了跨越2022-2025年的Reddit多年度数据集，涵盖39个子版块和197,618篇帖子。通过众包标注代表性样本，并结合分类模型进行大规模分析。", "result": "研究发现，公众对GenAI的信任和不信任程度在时间上几乎平衡，并随主要模型发布而发生转变。技术性能和可用性是影响信任态度的主要维度，而个人经验是塑造态度的最常见原因。此外，在不同信任主体（如专家、伦理学家、普通用户）之间也出现了不同的模式。", "conclusion": "本研究为大规模信任分析提供了一个方法论框架，并深入洞察了公众对生成式AI不断演变的看法。"}}
{"id": "2510.16344", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16344", "abs": "https://arxiv.org/abs/2510.16344", "authors": ["Chenrui Tie", "Shengxiang Sun", "Yudi Lin", "Yanbo Wang", "Zhongrui Li", "Zhouhan Zhong", "Jinxuan Zhu", "Yiman Pang", "Haonan Chen", "Junting Chen", "Ruihai Wu", "Lin Shao"], "title": "Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models", "comment": null, "summary": "Assembly hinges on reliably forming connections between parts; yet most\nrobotic approaches plan assembly sequences and part poses while treating\nconnectors as an afterthought. Connections represent the critical \"last mile\"\nof assembly execution, while task planning may sequence operations and motion\nplan may position parts, the precise establishment of physical connections\nultimately determines assembly success or failure. In this paper, we consider\nconnections as first-class primitives in assembly representation, including\nconnector types, specifications, quantities, and placement locations. Drawing\ninspiration from how humans learn assembly tasks through step-by-step\ninstruction manuals, we present Manual2Skill++, a vision-language framework\nthat automatically extracts structured connection information from assembly\nmanuals. We encode assembly tasks as hierarchical graphs where nodes represent\nparts and sub-assemblies, and edges explicitly model connection relationships\nbetween components. A large-scale vision-language model parses symbolic\ndiagrams and annotations in manuals to instantiate these graphs, leveraging the\nrich connection knowledge embedded in human-designed instructions. We curate a\ndataset containing over 20 assembly tasks with diverse connector types to\nvalidate our representation extraction approach, and evaluate the complete task\nunderstanding-to-execution pipeline across four complex assembly scenarios in\nsimulation, spanning furniture, toys, and manufacturing components with\nreal-world correspondence.", "AI": {"tldr": "本文提出Manual2Skill++，一个视觉-语言框架，能从装配手册中自动提取结构化的连接信息，并将装配任务编码为分层图，以提升机器人装配的成功率。", "motivation": "目前的机器人装配方法通常将连接器视为次要因素，但在装配执行中，连接的精确建立是决定成败的关键“最后一公里”。研究旨在将连接作为装配表示中的“一等公民”来处理。", "method": "Manual2Skill++框架通过大型视觉-语言模型解析装配手册中的符号图和注释，自动提取连接类型、规格、数量和放置位置等结构化信息。装配任务被编码为分层图，其中节点代表零件和子组件，边明确建模组件之间的连接关系。研究还策划了一个包含20多种装配任务和多样化连接器类型的数据集来验证方法。", "result": "研究验证了其表示提取方法在所策划数据集上的有效性，并在模拟环境中，针对家具、玩具和制造组件等四种复杂装配场景，评估了从任务理解到执行的完整流程。", "conclusion": "该研究成功展示了一个能够从人类设计的装配手册中提取并利用明确连接知识的框架，显著提升了机器人对装配任务的理解和执行能力，将连接提升为装配规划中的核心元素。"}}
{"id": "2510.16693", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16693", "abs": "https://arxiv.org/abs/2510.16693", "authors": ["Ayan Das", "Anushka Sharma", "Anamitra Pal"], "title": "Linear State Estimation in Presence of Bounded Uncertainties: A Comparative Analysis", "comment": null, "summary": "A variety of algorithms have been proposed to address the power system state\nestimation problem in the presence of uncertainties in the data. However, less\nemphasis has been given to handling perturbations in the model. In the context\nof linear state estimation (LSE), which is the focus of this paper,\nperturbations in the model come from variations in the line parameters. Since\nthe actual values of the line parameters can be different from the values\nstored in a power utility's database, we investigate three approaches in this\npaper to estimate the states in the presence of bounded uncertainties in the\ndata and the model. The first approach is based on interval arithmetic, the\nsecond is based on convex optimization, and the third is based on generalized\nlinear fractional programming. The three algorithms are applied to multiple\nIEEE test systems and compared in terms of their speed and accuracy. The\nresults indicate that the first two algorithms are extremely fast and give\nexpected results, while the third suffers from scalability issues and is\nunsuitable for LSE.", "AI": {"tldr": "本文研究了在数据和模型均存在有界不确定性时，电力系统线性状态估计问题。提出了三种方法（区间算术、凸优化、广义线性分数规划）并进行比较，结果表明前两种方法快速且有效，而第三种方法存在可扩展性问题。", "motivation": "现有电力系统状态估计算法主要关注数据不确定性，但对模型扰动（如线路参数变化）的关注较少。由于实际线路参数可能与数据库值不同，因此需要开发能同时处理数据和模型不确定性的方法。", "method": "本文提出了三种方法来估计数据和模型均存在有界不确定性时的状态：1) 基于区间算术；2) 基于凸优化；3) 基于广义线性分数规划。这些算法在多个IEEE测试系统上应用，并根据速度和准确性进行比较。", "result": "研究结果表明，前两种算法（区间算术和凸优化）速度极快且结果符合预期。然而，第三种算法（广义线性分数规划）存在可扩展性问题，不适用于线性状态估计。", "conclusion": "对于存在数据和模型不确定性的线性状态估计，区间算术和凸优化是高效且有效的解决方案。广义线性分数规划由于其可扩展性问题，不适合用于线性状态估计。"}}
{"id": "2510.16424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16424", "abs": "https://arxiv.org/abs/2510.16424", "authors": ["Dan Guo", "Xibin Jin", "Shuai Wang", "Zhigang Wen", "Miaowen Wen", "Chengzhong Xu"], "title": "Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach", "comment": null, "summary": "Edge robotics involves frequent exchanges of large-volume multi-modal data.\nExisting methods ignore the interdependency between robotic functionalities and\ncommunication conditions, leading to excessive communication overhead. This\npaper revolutionizes edge robotics systems through integrated perception,\nmotion, and communication (IPMC). As such, robots can dynamically adapt their\ncommunication strategies (i.e., compression ratio, transmission frequency,\ntransmit power) by leveraging the knowledge of robotic perception and motion\ndynamics, thus reducing the need for excessive sensor data uploads.\nFurthermore, by leveraging the learning to optimize (LTO) paradigm, an\nimitation learning neural network is designed and implemented, which reduces\nthe computational complexity by over 10x compared to state-of-the art\noptimization solvers. Experiments demonstrate the superiority of the proposed\nIPMC and the real-time execution capability of LTO.", "AI": {"tldr": "本文通过集成感知、运动和通信（IPMC）以及学习优化（LTO）范式，革命性地改进了边缘机器人系统，以减少通信开销和计算复杂性。", "motivation": "现有的边缘机器人通信方法忽略了机器人功能与通信条件之间的相互依赖性，导致过度的通信开销，尤其是在交换大量多模态数据时。", "method": "提出了一种集成感知、运动和通信（IPMC）框架，使机器人能够利用感知和运动动力学知识动态调整通信策略（如压缩比、传输频率、发射功率）。此外，利用学习优化（LTO）范式，设计并实现了一个模仿学习神经网络，以降低计算复杂性。", "result": "IPMC框架减少了过度的传感器数据上传需求。LTO神经网络将计算复杂性比现有最先进的优化求解器降低了10倍以上。实验证明了所提出的IPMC的优越性和LTO的实时执行能力。", "conclusion": "通过集成感知、运动和通信以及引入高效的学习优化范式，本文提出的方法有效解决了边缘机器人通信和计算效率问题，实现了系统性能的显著提升。"}}
{"id": "2510.16735", "categories": ["eess.SY", "cs.LG", "cs.SY", "93C40 (Primary) 68T05, 91B82 (Secondary)", "I.2.6; I.2.8; C.2.4; K.4.4"], "pdf": "https://arxiv.org/pdf/2510.16735", "abs": "https://arxiv.org/abs/2510.16735", "authors": ["Aniket Agrawal", "Harsharanga Patil"], "title": "A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization", "comment": "7 Pages, 8 Figures", "summary": "This paper introduces a control-theoretic framework for dynamic payment\nrouting, implemented within JUSPAY's Payment Orchestrator to maximize\ntransaction success rate. The routing system is modeled as a closed-loop\nfeedback controller continuously sensing gateway performance, computing\ncorrective actions, and dynamically routes transactions across gateway to\nensure operational resilience. The system leverages concepts from control\ntheory, reinforcement learning, and multi-armed bandit optimization to achieve\nboth short-term responsiveness and long-term stability. Rather than relying on\nexplicit PID regulation, the framework applies generalized feedback-based\nadaptation, ensuring that corrective actions remain proportional to observed\nperformance deviations and the computed gateway score gradually converges\ntoward the success rate. This hybrid approach unifies control theory and\nadaptive decision systems, enabling self-regulating transaction routing that\ndampens instability, and improves reliability. Live production results show an\nimprovement of up to 1.15% in success rate over traditional rule-based routing,\ndemonstrating the effectiveness of feedback-based control in payment systems.", "AI": {"tldr": "本文提出了一种基于控制理论的动态支付路由框架，通过反馈机制、强化学习和多臂老虎机优化，以最大化交易成功率，并在实际生产中实现了显著提升。", "motivation": "传统的基于规则的路由系统在最大化交易成功率和确保运营弹性方面存在局限性，需要一种能够持续适应网关性能并动态调整路由的解决方案。", "method": "该研究将路由系统建模为闭环反馈控制器，持续感知网关性能，计算纠正措施，并动态路由交易。它结合了控制理论、强化学习和多臂老虎机优化概念，采用广义反馈式自适应，确保纠正措施与观察到的性能偏差成比例，并使计算出的网关得分逐渐收敛于成功率。", "result": "在JUSPAY的支付编排器中实施后，实际生产结果显示，与传统的基于规则的路由相比，交易成功率提高了高达1.15%。", "conclusion": "基于反馈的控制理论在支付系统中是有效的，能够实现自调节交易路由，抑制不稳定性，提高可靠性，并显著提升交易成功率。"}}
{"id": "2510.16118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16118", "abs": "https://arxiv.org/abs/2510.16118", "authors": ["Nishad Sahu", "Shounak Sural", "Aditya Satish Patil", "Ragunathan", "Rajkumar"], "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025\n  Workshops", "summary": "Reliable perception is fundamental for safety critical decision making in\nautonomous driving. Yet, vision based object detector neural networks remain\nvulnerable to uncertainty arising from issues such as data bias and\ndistributional shifts. In this paper, we introduce ObjectTransforms, a\ntechnique for quantifying and reducing uncertainty in vision based object\ndetection through object specific transformations at both training and\ninference times. At training time, ObjectTransforms perform color space\nperturbations on individual objects, improving robustness to lighting and color\nvariations. ObjectTransforms also uses diffusion models to generate realistic,\ndiverse pedestrian instances. At inference time, object perturbations are\napplied to detected objects and the variance of detection scores are used to\nquantify predictive uncertainty in real time. This uncertainty signal is then\nused to filter out false positives and also recover false negatives, improving\nthe overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K\ndataset demonstrate that our method yields notable accuracy improvements and\nuncertainty reduction across all object classes during training, while\npredicting desirably higher uncertainty values for false positives as compared\nto true positives during inference. Our results highlight the potential of\nObjectTransforms as a lightweight yet effective mechanism for reducing and\nquantifying uncertainty in vision-based perception during training and\ninference respectively.", "AI": {"tldr": "本文提出ObjectTransforms技术，通过在训练和推理时进行对象特定变换（如颜色扰动和扩散模型生成），量化并降低视觉目标检测中的不确定性，从而提高自动驾驶感知的可靠性。", "motivation": "自动驾驶中安全关键的决策依赖于可靠的感知，但基于视觉的目标检测神经网络容易受到数据偏差和分布偏移等问题引起的不确定性影响。", "method": "ObjectTransforms在训练时对单个对象执行颜色空间扰动以提高对光照和颜色变化的鲁棒性，并使用扩散模型生成逼真多样的行人实例。在推理时，对检测到的对象应用扰动，并利用检测分数的方差实时量化预测不确定性。此不确定性信号用于过滤假阳性并恢复假阴性，从而改善整体查准率-查全率曲线。", "result": "在NuImages 10K数据集上使用YOLOv8进行的实验表明，该方法在训练期间显著提高了所有对象类别的准确性并降低了不确定性。在推理期间，对于假阳性预测的不确定性值明显高于真阳性，并改善了整体精度召回曲线。", "conclusion": "ObjectTransforms是一种轻量级而有效的机制，分别在训练和推理过程中减少和量化基于视觉的感知中的不确定性，具有显著的应用潜力。"}}
{"id": "2510.16115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16115", "abs": "https://arxiv.org/abs/2510.16115", "authors": ["Jianhan Lin", "Yuchu Qin", "Shuai Gao", "Yikang Rui", "Jie Liu", "Yanjie Lv"], "title": "StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection", "comment": null, "summary": "Well-maintained road networks are crucial for achieving Sustainable\nDevelopment Goal (SDG) 11. Road surface damage not only threatens traffic\nsafety but also hinders sustainable urban development. Accurate detection,\nhowever, remains challenging due to the diverse shapes of damages, the\ndifficulty of capturing slender cracks with high aspect ratios, and the high\nerror rates in small-scale damage recognition. To address these issues, we\npropose StripRFNet, a novel deep neural network comprising three modules: (1) a\nShape Perception Module (SPM) that enhances shape discrimination via large\nseparable kernel attention (LSKA) in multi-scale feature aggregation; (2) a\nStrip Receptive Field Module (SRFM) that employs large strip convolutions and\npooling to capture features of slender cracks; and (3) a Small-Scale\nEnhancement Module (SSEM) that leverages a high-resolution P2 feature map, a\ndedicated detection head, and dynamic upsampling to improve small-object\ndetection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses\nexisting methods. On the Chinese subset, it improves F1-score, mAP50, and\nmAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,\nrespectively. On the full dataset, it achieves the highest F1-score of 80.33%\ncompared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while\nmaintaining competitive inference speed. These results demonstrate that\nStripRFNet achieves state-of-the-art accuracy and real-time efficiency,\noffering a promising tool for intelligent road maintenance and sustainable\ninfrastructure management.", "AI": {"tldr": "本文提出StripRFNet，一个用于准确检测路面损伤的深度神经网络，通过解决损伤形状多样性、细长裂缝捕获和小尺度损伤识别的挑战，实现了最先进的精度和实时效率。", "motivation": "良好的道路网络对实现可持续发展目标11至关重要。路面损伤不仅威胁交通安全，还阻碍城市可持续发展。然而，由于损伤形状多样、难以捕获高长宽比的细长裂缝以及小尺度损伤识别错误率高，准确检测仍然充满挑战。", "method": "本文提出了StripRFNet，一个包含三个模块的深度神经网络：1) 形状感知模块（SPM），通过多尺度特征聚合中的大可分离核注意力（LSKA）增强形状辨别能力；2) 条形感受野模块（SRFM），利用大条形卷积和池化来捕获细长裂缝的特征；3) 小尺度增强模块（SSEM），利用高分辨率P2特征图、专用检测头和动态上采样来改进小目标检测。", "result": "在RDD2022基准测试中，StripRFNet超越了现有方法。在中国子集上，其F1-score、mAP50和mAP50:95分别比基线提高了4.4、2.9和3.4个百分点。在完整数据集上，其F1-score达到80.33%，与CRDDC'2022参与者和ORDDC'2024第二阶段结果相比最高，同时保持了有竞争力的推理速度。", "conclusion": "StripRFNet实现了最先进的精度和实时效率，为智能道路维护和可持续基础设施管理提供了一个有前景的工具。"}}
{"id": "2510.17436", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.17436", "abs": "https://arxiv.org/abs/2510.17436", "authors": ["Vladyslav Zalevskyi", "Dondu-Busra Bulut", "Thomas Sanchez", "Meritxell Bach Cuadra"], "title": "Segmenting infant brains across magnetic fields: Domain randomization and annotation curation in ultra-low field MRI", "comment": "1st place (hippocampus) and 3rd place (basal ganglia) in the Low\n  field pediatric brain magnetic resonance Image Segmentation and quality\n  Assurance Challenge (LISA) 2025", "summary": "Early identification of neurodevelopmental disorders relies on accurate\nsegmentation of brain structures in infancy, a task complicated by rapid brain\ngrowth, poor tissue contrast, and motion artifacts in pediatric MRI. These\nchallenges are further exacerbated in ultra-low-field (ULF, 0.064~T) MRI,\nwhich, despite its lower image quality, offers an affordable, portable, and\nsedation-free alternative for use in low-resource settings. In this work, we\npropose a domain randomization (DR) framework to bridge the domain gap between\nhigh-field (HF) and ULF MRI in the context of the hippocampi and basal ganglia\nsegmentation in the LISA challenge. We show that pre-training on whole-brain HF\nsegmentations using DR significantly improves generalization to ULF data, and\nthat careful curation of training labels, by removing misregistered HF-to-ULF\nannotations from training, further boosts performance. By fusing the\npredictions of several models through majority voting, we are able to achieve\ncompetitive performance. Our results demonstrate that combining robust\naugmentation with annotation quality control can enable accurate segmentation\nin ULF data. Our code is available at\nhttps://github.com/Medical-Image-Analysis-Laboratory/lisasegm", "AI": {"tldr": "本研究提出了一种域随机化 (DR) 框架，结合标签质量控制，以弥合高场 (HF) 和超低场 (ULF) MRI 之间的域差距，从而实现在婴儿 ULF MRI 中对海马体和基底神经节的准确分割。", "motivation": "婴儿神经发育障碍的早期识别依赖于准确的脑结构分割，但这在儿科MRI中因快速脑生长、组织对比度差和运动伪影而复杂。超低场 (ULF) MRI 尽管图像质量较低，但其经济、便携和无需镇静的优势使其适用于资源匮乏地区。然而，ULF MRI 的挑战进一步加剧，因此需要解决 HF 和 ULF MRI 之间的域差距问题。", "method": "本研究提出了一种域随机化 (DR) 框架，用于弥合 HF 和 ULF MRI 之间的域差距。具体方法包括：使用 DR 对全脑 HF 分割进行预训练，以提高对 ULF 数据的泛化能力；通过移除训练集中 HF 到 ULF 错位标注来精心策划训练标签，进一步提升性能；通过多数投票融合多个模型的预测。", "result": "研究结果表明，使用 DR 对 HF 分割进行预训练显著改善了模型在 ULF 数据上的泛化能力。仔细筛选和移除错位标注的训练标签进一步提升了性能。通过多数投票融合多个模型预测，实现了有竞争力的性能。这证明了结合鲁棒的数据增强和标注质量控制能够实现 ULF 数据中的准确分割。", "conclusion": "结合鲁棒的数据增强（如域随机化）和严格的标注质量控制，能够有效利用高场 MRI 数据，在超低场 MRI 数据中实现准确的脑结构分割，这对于资源匮乏地区神经发育障碍的早期识别具有重要意义。"}}
{"id": "2510.16198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16198", "abs": "https://arxiv.org/abs/2510.16198", "authors": ["Mohamed Gamil", "Abdelrahman Elsayed", "Abdelrahman Lila", "Ahmed Gad", "Hesham Abdelgawad", "Mohamed Aref", "Ahmed Fares"], "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture", "comment": null, "summary": "Despite recent advances in AI, multimodal culturally diverse datasets are\nstill limited, particularly for regions in the Middle East and Africa. In this\npaper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian\nculture. By designing and running a new data collection pipeline, we collected\nover 3,000 images, covering 313 concepts across landmarks, food, and folklore.\nEach entry in the dataset is manually validated for cultural authenticity and\nmultimodal coherence. EgMM-Corpus aims to provide a reliable resource for\nevaluating and training vision-language models in an Egyptian cultural context.\nWe further evaluate the zero-shot performance of Contrastive Language-Image\nPre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and\n36.4% Top-5 accuracy in classification. These results underscore the existing\ncultural bias in large-scale vision-language models and demonstrate the\nimportance of EgMM-Corpus as a benchmark for developing culturally aware\nmodels.", "AI": {"tldr": "本文介绍了EgMM-Corpus，一个专注于埃及文化的多模态数据集，旨在解决中东和非洲地区文化多样性数据集的缺乏问题，并评估了现有视觉-语言模型的文化偏见。", "motivation": "尽管人工智能取得了进步，但多模态文化多样性数据集仍然有限，尤其是在中东和非洲地区。这导致现有大型视觉-语言模型存在文化偏见，无法在特定文化背景下有效运作。", "method": "研究人员设计并运行了一个新的数据收集流程，收集了超过3,000张图像，涵盖了地标、食物和民俗等313个埃及文化概念。数据集中的每个条目都经过人工验证，以确保文化真实性和多模态一致性。此外，他们还在EgMM-Corpus上评估了CLIP模型的零样本性能。", "result": "CLIP模型在EgMM-Corpus上的零样本分类表现为Top-1准确率21.2%，Top-5准确率36.4%。这些结果突显了现有大型视觉-语言模型中存在的文化偏见。", "conclusion": "EgMM-Corpus为在埃及文化背景下评估和训练视觉-语言模型提供了可靠的资源。该数据集作为开发具有文化意识模型的基准，对于解决现有模型中的文化偏见至关重要。"}}
{"id": "2510.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16227", "abs": "https://arxiv.org/abs/2510.16227", "authors": ["Jennifer Hu", "Ethan Gotlieb Wilcox", "Siyuan Song", "Kyle Mahowald", "Roger P. Levy"], "title": "What Can String Probability Tell Us About Grammaticality?", "comment": null, "summary": "What have language models (LMs) learned about grammar? This question remains\nhotly debated, with major ramifications for linguistic theory. However, since\nprobability and grammaticality are distinct notions in linguistics, it is not\nobvious what string probabilities can reveal about an LM's underlying\ngrammatical knowledge. We present a theoretical analysis of the relationship\nbetween grammar, meaning, and string probability, based on simple assumptions\nabout the generative process of corpus data. Our framework makes three\npredictions, which we validate empirically using 280K sentence pairs in English\nand Chinese: (1) correlation between the probability of strings within minimal\npairs, i.e., string pairs with minimal semantic differences; (2) correlation\nbetween models' and humans' deltas within minimal pairs; and (3) poor\nseparation in probability space between unpaired grammatical and ungrammatical\nstrings. Our analyses give theoretical grounding for using probability to learn\nabout LMs' structural knowledge, and suggest directions for future work in LM\ngrammatical evaluation.", "AI": {"tldr": "本文理论分析了语法、语义与字符串概率之间的关系，并基于语料生成过程的假设，提出了三个可经验验证的预测。这些预测通过英汉双语28万句对得到验证，为使用概率评估语言模型（LM）的结构知识提供了理论基础。", "motivation": "语言模型（LM）对语法的学习程度是一个热议且对语言理论有重大影响的问题。然而，在语言学中，概率和语法性是不同的概念，因此不清楚字符串概率能如何揭示LM潜在的语法知识。", "method": "本文首先基于语料数据生成过程的简单假设，对语法、语义和字符串概率之间的关系进行了理论分析。随后，通过使用28万个英汉语句对，经验性地验证了三个预测。", "result": "研究结果验证了三个预测：(1) 最小对（语义差异最小的字符串对）中字符串概率之间存在相关性；(2) 模型和人类在最小对内的差异（deltas）之间存在相关性；(3) 未配对的语法和非语法字符串在概率空间中分离性较差。", "conclusion": "本研究的分析为利用概率来理解语言模型的结构知识提供了理论基础，并为未来语言模型语法评估工作指明了方向。"}}
{"id": "2510.16444", "categories": ["cs.CV", "cs.MM", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.16444", "abs": "https://arxiv.org/abs/2510.16444", "authors": ["Kunyu Peng", "Di Wen", "Jia Fu", "Jiamin Wu", "Kailun Yang", "Junwei Zheng", "Ruiping Liu", "Yufan Chen", "Yuqian Fu", "Danda Pani Paudel", "Luc Van Gool", "Rainer Stiefelhagen"], "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba", "comment": "Extended version of ECCV 2024 paper arXiv:2407.01872. The dataset and\n  code are released at https://github.com/KPeng9510/refAVA2", "summary": "Referring Atomic Video Action Recognition (RAVAR) aims to recognize\nfine-grained, atomic-level actions of a specific person of interest conditioned\non natural language descriptions. Distinct from conventional action recognition\nand detection tasks, RAVAR emphasizes precise language-guided action\nunderstanding, which is particularly critical for interactive human action\nanalysis in complex multi-person scenarios. In this work, we extend our\npreviously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million\nframes and >75.1k annotated persons in total. We benchmark this dataset using\nbaselines from multiple related domains, including atomic action localization,\nvideo question answering, and text-video retrieval, as well as our earlier\nmodel, RefAtomNet. Although RefAtomNet surpasses other baselines by\nincorporating agent attention to highlight salient features, its ability to\nalign and retrieve cross-modal information remains limited, leading to\nsuboptimal performance in localizing the target person and predicting\nfine-grained actions. To overcome the aforementioned limitations, we introduce\nRefAtomNet++, a novel framework that advances cross-modal token aggregation\nthrough a multi-hierarchical semantic-aligned cross-attention mechanism\ncombined with multi-trajectory Mamba modeling at the partial-keyword,\nscene-attribute, and holistic-sentence levels. In particular, scanning\ntrajectories are constructed by dynamically selecting the nearest visual\nspatial tokens at each timestep for both partial-keyword and scene-attribute\nlevels. Moreover, we design a multi-hierarchical semantic-aligned\ncross-attention strategy, enabling more effective aggregation of spatial and\ntemporal tokens across different semantic hierarchies. Experiments show that\nRefAtomNet++ establishes new state-of-the-art results. The dataset and code are\nreleased at https://github.com/KPeng9510/refAVA2.", "AI": {"tldr": "本文扩展了RefAVA数据集至RefAVA++，并提出了RefAtomNet++模型，通过多层次语义对齐交叉注意力机制和多轨迹Mamba建模，解决了指代原子视频动作识别（RAVAR）中跨模态信息对齐和精细动作预测的挑战，取得了最先进的成果。", "motivation": "指代原子视频动作识别（RAVAR）需要精确的语言引导动作理解，尤其在复杂的多人场景中对交互式人类动作分析至关重要。现有模型（如RefAtomNet）在对齐和检索跨模态信息方面存在局限性，导致在定位目标人物和预测精细动作时表现不佳。", "method": "研究方法包括：1) 将RefAVA数据集扩展为RefAVA++，包含超过290万帧和7.51万个标注人物。2) 使用原子动作定位、视频问答、文本-视频检索等领域基线模型以及RefAtomNet进行基准测试。3) 提出RefAtomNet++框架，通过多层次语义对齐交叉注意力机制和多轨迹Mamba建模（在部分关键词、场景属性和整体句子层面）来推进跨模态token聚合。具体地，通过动态选择最近的视觉空间token构建扫描轨迹，并设计多层次语义对齐交叉注意力策略以更有效地聚合空间和时间token。", "result": "实验结果表明，RefAtomNet++在指代原子视频动作识别任务上取得了新的最先进（state-of-the-art）成果。", "conclusion": "RefAtomNet++通过其新颖的跨模态token聚合方法，有效解决了RAVAR任务中目标人物定位和精细动作预测的挑战，显著提升了性能。"}}
{"id": "2510.16953", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.16953", "abs": "https://arxiv.org/abs/2510.16953", "authors": ["Ersin Das", "William A. Welch", "Patrick Spieler", "Keenan Albee", "Aurelio Noca", "Jeffrey Edlund", "Jonathan Becktor", "Thomas Touma", "Jessica Todd", "Sriramya Bhamidipati", "Stella Kombo", "Maira Saboia", "Anna Sabel", "Grace Lim", "Rohan Thakker", "Amir Rahmani", "Joel W. Burdick"], "title": "Safe Payload Transfer with Ship-Mounted Cranes: A Robust Model Predictive Control Approach", "comment": null, "summary": "Ensuring safe real-time control of ship-mounted cranes in unstructured\ntransportation environments requires handling multiple safety constraints while\nmaintaining effective payload transfer performance. Unlike traditional crane\nsystems, ship-mounted cranes are consistently subjected to significant external\ndisturbances affecting underactuated crane dynamics due to the ship's dynamic\nmotion response to harsh sea conditions, which can lead to robustness issues.\nTo tackle these challenges, we propose a robust and safe model predictive\ncontrol (MPC) framework and demonstrate it on a 5-DOF crane system, where a\nStewart platform simulates the external disturbances that ocean surface motions\nwould have on the supporting ship. The crane payload transfer operation must\navoid obstacles and accurately place the payload within a designated target\narea. We use a robust zero-order control barrier function (R-ZOCBF)-based\nsafety constraint in the nonlinear MPC to ensure safe payload positioning,\nwhile time-varying bounding boxes are utilized for collision avoidance. We\nintroduce a new optimization-based online robustness parameter adaptation\nscheme to reduce the conservativeness of R-ZOCBFs. Experimental trials on a\ncrane prototype demonstrate the overall performance of our safe control\napproach under significant perturbing motions of the crane base. While our\nfocus is on crane-facilitated transfer, the methods more generally apply to\nsafe robotically-assisted parts mating and parts insertion.", "AI": {"tldr": "本文提出了一种鲁棒安全的模型预测控制（MPC）框架，用于在非结构化环境中控制船载起重机，以应对外部扰动，同时确保有效载荷转移性能和多重安全约束。", "motivation": "船载起重机在恶劣海况下会受到显著的外部扰动，影响欠驱动起重机动力学，导致鲁棒性问题。传统的起重机系统无法有效处理这些挑战，因此需要一种能同时处理多重安全约束并保持有效载荷转移性能的控制方法。", "method": "研究人员提出了一种鲁棒安全的模型预测控制（MPC）框架，并在一个5自由度起重机系统上进行了验证。该系统使用Stewart平台模拟海面运动对船舶造成的外部扰动。方法包括：使用基于鲁棒零阶控制障碍函数（R-ZOCBF）的安全约束来确保有效载荷定位安全；利用时变包围盒进行避障；引入一种新的基于优化的在线鲁棒性参数自适应方案来减少R-ZOCBF的保守性。", "result": "在起重机原型上进行的实验验证表明，在起重机基座受到显著扰动的情况下，所提出的安全控制方法表现出良好的整体性能，能够实现安全的有效载荷定位和避障。", "conclusion": "所提出的鲁棒安全控制方法能够有效解决船载起重机在外部扰动下的安全控制问题，并具有更广泛的应用前景，例如机器人辅助部件装配和插入任务。"}}
{"id": "2510.16435", "categories": ["cs.RO", "cs.CL", "cs.HC", "I.2.9; H.5.2; H.5.0; I.2.8; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2510.16435", "abs": "https://arxiv.org/abs/2510.16435", "authors": ["Lennart Wachowiak", "Andrew Coles", "Gerard Canal", "Oya Celiktutan"], "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics", "comment": null, "summary": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations.", "AI": {"tldr": "本研究引入了一个包含1,893个用户问题的家庭机器人问答数据集，旨在提升机器人的问答能力，并揭示用户对机器人解释和交互的真实需求。", "motivation": "随着大型语言模型和对话界面在人机交互中的日益普及，机器人回答用户问题的能力变得前所未有的重要。现有可解释机器人研究主要关注“为什么”类问题，但用户实际提出的问题种类更为广泛，因此需要一个数据集来深入了解机器人需要回答哪些类型的问题。", "method": "研究从100名参与者那里收集了1,893个用户问题，并将其组织成12个主要类别和70个子类别。为了收集数据，研究者创建了15个视频和7个文本刺激，描绘了机器人在执行各种家庭任务的情景，然后要求参与者（通过Prolific平台）提出他们在这些情景中想问机器人的问题。", "result": "数据集中最常见的类别是关于任务执行细节的问题（22.5%）、机器人能力的问题（12.7%）和性能评估（11.3%）。尽管关于机器人如何处理潜在困难情景和确保正确行为的问题频率较低，但用户将其评为最重要的问题。此外，研究发现机器人学新手用户与经验丰富的用户提出的问题类型不同，新手更倾向于询问简单的事实，如机器人做了什么或环境的当前状态。", "conclusion": "该数据集为以下方面提供了宝贵的基础：1) 识别机器人需要记录并暴露给对话界面的信息；2) 对问答模块进行基准测试；3) 设计符合用户期望的解释策略。这对于机器人进入人类共享环境并以语言为核心进行指令和交互至关重要。"}}
{"id": "2510.17071", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17071", "abs": "https://arxiv.org/abs/2510.17071", "authors": ["Samuel Talkington", "Daniel Turizo", "Sergio A. Dorado-Rojas", "Rahul K. Gupta", "Daniel K. Molzahn"], "title": "Differentiating Through Power Flow Solutions for Admittance and Topology Control", "comment": "10 pages, 6 figures", "summary": "The power flow equations relate bus voltage phasors to power injections via\nthe network admittance matrix. These equations are central to the key\noperational and protection functions of power systems (e.g., optimal power flow\nscheduling and control, state estimation, protection, and fault location, among\nothers). As control, optimization, and estimation of network admittance\nparameters are central to multiple avenues of research in electric power\nsystems, we propose a linearization of power flow solutions obtained by\nimplicitly differentiating them with respect to the network admittance\nparameters. This is achieved by utilizing the implicit function theorem, in\nwhich we show that such a differentiation is guaranteed to exist under mild\nconditions and is applicable to generic power systems (radial or meshed). The\nproposed theory is applied to derive sensitivities of complex voltages, line\ncurrents, and power flows. The developed theory of linearizing the power flow\nequations around changes in the complex network admittance parameters has\nnumerous applications. We demonstrate several of these applications, such as\npredicting the nodal voltages when the network topology changes without solving\nthe power flow equations. We showcase the application for continuous admittance\ncontrol, which is used to increase the hosting capacity of a given distribution\nnetwork.", "AI": {"tldr": "本文提出了一种通过对网络导纳参数进行隐式微分，从而线性化电力潮流解的方法，并展示了其在预测电压和导纳控制等方面的应用。", "motivation": "电力潮流方程是电力系统运行和保护的核心，而网络导纳参数的控制、优化和估计是电力系统研究的关键方向。因此，需要一种有效的方法来理解和利用这些参数的变化。", "method": "利用隐函数定理，对电力潮流方程的解相对于网络导纳参数进行隐式微分，从而实现电力潮流解的线性化。该方法在温和条件下保证存在，适用于径向或网状的通用电力系统。", "result": "该理论成功推导了复电压、线路电流和潮流对网络导纳参数变化的敏感度。它能够预测网络拓扑变化时的节点电压，而无需重新求解潮流方程，并可应用于连续导纳控制以提高配电网络的承载能力。", "conclusion": "所开发的关于复网络导纳参数变化的电力潮流方程线性化理论具有广泛的应用前景，为电力系统的分析、控制和优化提供了新的工具。"}}
{"id": "2510.16500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16500", "abs": "https://arxiv.org/abs/2510.16500", "authors": ["Chen Min", "Jilin Mei", "Heng Zhai", "Shuai Wang", "Tong Sun", "Fanjie Kong", "Haoyang Li", "Fangyuan Mao", "Fuyang Liu", "Shuo Wang", "Yiming Nie", "Qi Zhu", "Liang Xiao", "Dawei Zhao", "Yu Hu"], "title": "Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks", "comment": "Off-road robotics", "summary": "A major bottleneck in off-road autonomous driving research lies in the\nscarcity of large-scale, high-quality datasets and benchmarks. To bridge this\ngap, we present ORAD-3D, which, to the best of our knowledge, is the largest\ndataset specifically curated for off-road autonomous driving. ORAD-3D covers a\nwide spectrum of terrains, including woodlands, farmlands, grasslands,\nriversides, gravel roads, cement roads, and rural areas, while capturing\ndiverse environmental variations across weather conditions (sunny, rainy,\nfoggy, and snowy) and illumination levels (bright daylight, daytime, twilight,\nand nighttime). Building upon this dataset, we establish a comprehensive suite\nof benchmark evaluations spanning five fundamental tasks: 2D free-space\ndetection, 3D occupancy prediction, rough GPS-guided path planning,\nvision-language model-driven autonomous driving, and world model for off-road\nenvironments. Together, the dataset and benchmarks provide a unified and robust\nresource for advancing perception and planning in challenging off-road\nscenarios. The dataset and code will be made publicly available at\nhttps://github.com/chaytonmin/ORAD-3D.", "AI": {"tldr": "本文介绍了ORAD-3D，一个目前规模最大的越野自动驾驶数据集和基准，涵盖了多种地形、天气和光照条件，并为五项核心任务提供了评估基准。", "motivation": "越野自动驾驶研究面临的主要瓶颈是缺乏大规模、高质量的数据集和基准。", "method": "作者收集并整理了ORAD-3D数据集，该数据集包含多种越野地形、天气（晴朗、多雨、多雾、多雪）和光照（明亮白天、白天、黄昏、夜晚）条件。在此基础上，建立了涵盖五项基本任务的综合基准评估套件：2D自由空间检测、3D占用预测、粗略GPS引导路径规划、视觉语言模型驱动的自动驾驶以及越野环境世界模型。", "result": "ORAD-3D是迄今为止最大的越野自动驾驶数据集，覆盖了广泛的环境变化。该数据集和基准为五项核心任务提供了统一且强大的评估资源，以推动越野感知和规划技术的发展。", "conclusion": "ORAD-3D数据集和基准为在挑战性越野场景中推进感知和规划提供了统一且强大的资源，有助于解决越野自动驾驶领域的数据瓶颈问题。"}}
{"id": "2510.17043", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.17043", "abs": "https://arxiv.org/abs/2510.17043", "authors": ["Md Ahmed Al Muzaddid", "William J. Beksi"], "title": "Person Re-Identification via Generalized Class Prototypes", "comment": "18 pages, 11 figures, and 4 tables", "summary": "Advanced feature extraction methods have significantly contributed to\nenhancing the task of person re-identification. In addition, modifications to\nobjective functions have been developed to further improve performance.\nNonetheless, selecting better class representatives is an underexplored area of\nresearch that can also lead to advancements in re-identification performance.\nAlthough past works have experimented with using the centroid of a gallery\nimage class during training, only a few have investigated alternative\nrepresentations during the retrieval stage. In this paper, we demonstrate that\nthese prior techniques yield suboptimal results in terms of re-identification\nmetrics. To address the re-identification problem, we propose a generalized\nselection method that involves choosing representations that are not limited to\nclass centroids. Our approach strikes a balance between accuracy and mean\naverage precision, leading to improvements beyond the state of the art. For\nexample, the actual number of representations per class can be adjusted to meet\nspecific application requirements. We apply our methodology on top of multiple\nre-identification embeddings, and in all cases it substantially improves upon\ncontemporary results", "AI": {"tldr": "该论文提出了一种广义的类代表选择方法，用于行人重识别任务，该方法不限于使用类质心，能在准确性和平均精度之间取得平衡，并显著提升了现有技术的性能。", "motivation": "尽管特征提取和目标函数优化已显著提升行人重识别性能，但选择更好的类代表这一领域仍未被充分探索。过去的工作主要关注训练时使用质心，而检索阶段的替代表示研究较少，且现有技术在重识别指标上表现不佳。", "method": "本文提出了一种广义的类代表选择方法，该方法不局限于类质心。它允许根据特定应用需求调整每类的实际代表数量。该方法被应用于多种重识别嵌入之上。", "result": "该方法在准确性和平均精度之间取得了平衡，超越了现有技术的水平。在所有应用案例中，它都显著改善了当前的重识别结果。", "conclusion": "所提出的广义类代表选择方法能够有效提升行人重识别性能，通过提供超越传统质心的灵活表示选择，在准确性和平均精度方面取得了优于现有技术的结果。"}}
{"id": "2510.16134", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16134", "abs": "https://arxiv.org/abs/2510.16134", "authors": ["Chen Kong", "James Fort", "Aria Kang", "Jonathan Wittmer", "Simon Green", "Tianwei Shen", "Yipu Zhao", "Cheng Peng", "Gustavo Solaira", "Andrew Berkovich", "Nikhil Raina", "Vijay Baiyya", "Evgeniy Oleinik", "Eric Huang", "Fan Zhang", "Julian Straub", "Mark Schwesinger", "Luis Pesqueira", "Xiaqing Pan", "Jakob Julian Engel", "Carl Ren", "Mingfei Yan", "Richard Newcombe"], "title": "Aria Gen 2 Pilot Dataset", "comment": null, "summary": "The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset\ncaptured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely\naccess, A2PD is released incrementally with ongoing dataset enhancements. The\ninitial release features Dia'ane, our primary subject, who records her daily\nactivities alongside friends, each equipped with Aria Gen 2 glasses. It\nencompasses five primary scenarios: cleaning, cooking, eating, playing, and\noutdoor walking. In each of the scenarios, we provide comprehensive raw sensor\ndata and output data from various machine perception algorithms. These data\nillustrate the device's ability to perceive the wearer, the surrounding\nenvironment, and interactions between the wearer and the environment, while\nmaintaining robust performance across diverse users and conditions. The A2PD is\npublicly available at projectaria.com, with open-source tools and usage\nexamples provided in Project Aria Tools.", "AI": {"tldr": "Aria Gen 2 Pilot Dataset (A2PD)是一个使用Aria Gen 2眼镜捕获的以自我为中心的多模态开放数据集，分阶段发布，包含日常活动场景的原始传感器数据和机器感知算法输出，旨在展示设备感知能力。", "motivation": "研究动机是提供一个高质量、以自我为中心的多模态开放数据集，利用先进的Aria Gen 2眼镜捕获，以促进对穿戴者、环境及其交互感知的研究，并确保及时访问和持续增强。", "method": "该研究方法是使用Aria Gen 2眼镜，由主要受试者Dia'ane及其朋友记录日常活动。数据涵盖清洁、烹饪、饮食、玩耍和户外散步五种主要场景，并提供全面的原始传感器数据以及各种机器感知算法的输出数据。", "result": "主要结果是A2PD数据集成功展示了设备感知穿戴者、周围环境以及穿戴者与环境之间交互的能力，并且在不同用户和条件下均保持了稳健的性能。该数据集已公开可用，并提供了开源工具和使用示例。", "conclusion": "结论是A2PD提供了一个宝贵的、以自我为中心的多模态开放数据集，为研究人员提供了丰富的数据和工具，以探索和开发先进的感知技术，特别是在穿戴式设备的人机交互和环境理解方面。"}}
{"id": "2510.16257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16257", "abs": "https://arxiv.org/abs/2510.16257", "authors": ["Chu Fei Luo", "Samuel Dahan", "Xiaodan Zhu"], "title": "Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback", "comment": "Findings of EMNLP 2025, 5 pages", "summary": "As language models have a greater impact on society, it is important to\nensure they are aligned to a diverse range of perspectives and are able to\nreflect nuance in human values. However, the most popular training paradigms\nfor modern language models often assume there is one optimal answer for every\nquery, leading to generic responses and poor alignment. In this work, we aim to\nenhance pluralistic alignment of language models in a low-resource setting with\ntwo methods: pluralistic decoding and model steering. We empirically\ndemonstrate that model steering offers consistent improvement over zero-shot\nand few-shot baselines with only 50 annotated samples. Our proposed methods\ndecrease false positives in several high-stakes tasks such as hate speech\ndetection and misinformation detection, and improves the distributional\nalignment to human values in GlobalOpinionQA. We hope our work highlights the\nimportance of diversity and how language models can be adapted to consider\nnuanced perspectives.", "AI": {"tldr": "本研究旨在低资源环境下，通过多元解码和模型引导两种方法，增强语言模型的多元对齐能力，以反映人类价值观的细微差别，并在高风险任务和价值观对齐方面取得显著改进。", "motivation": "语言模型对社会影响日益增大，但主流训练范式常假设每个查询只有一个最优答案，导致响应通用且与多元人类价值观对齐不佳。因此，需要提升语言模型的多元对齐能力，尤其是在低资源环境中。", "method": "提出了两种方法：多元解码（pluralistic decoding）和模型引导（model steering）。这些方法在低资源设置下（仅使用50个标注样本）进行实证研究。", "result": "模型引导在仅有50个标注样本的情况下，比零样本和少样本基线模型表现出持续的改进。所提出的方法减少了仇恨言论检测和错误信息检测等高风险任务中的误报，并改善了GlobalOpinionQA中与人类价值观的分布对齐。", "conclusion": "本工作强调了多样性的重要性，并展示了语言模型如何适应以考虑细微的观点，从而提升其多元对齐能力。"}}
{"id": "2510.16517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16517", "abs": "https://arxiv.org/abs/2510.16517", "authors": ["Haokai Ding", "Wenzeng Zhang"], "title": "A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "Accepted author manuscript (AAM) for IEEE/RSJ IROS 2025. 6 pages, 10\n  figures", "summary": "This paper introduces a novel robotic gripper, named as the SPD gripper. It\nfeatures a palm and two mechanically identical and symmetrically arranged\nfingers, which can be driven independently or by a single motor. The fingertips\nof the fingers follow a linear motion trajectory, facilitating the grasping of\nobjects of various sizes on a tabletop without the need to adjust the overall\nheight of the gripper. Traditional industrial grippers with parallel gripping\ncapabilities often exhibit an arcuate motion at the fingertips, requiring the\nentire robotic arm to adjust its height to avoid collisions with the tabletop.\nThe SPD gripper, with its linear parallel gripping mechanism, effectively\naddresses this issue. Furthermore, the SPD gripper possesses adaptive\ncapabilities, accommodating objects of different shapes and sizes. This paper\npresents the design philosophy, fundamental composition principles, and\noptimization analysis theory of the SPD gripper. Based on the design theory, a\nrobotic gripper prototype was developed and tested. The experimental results\ndemonstrate that the robotic gripper successfully achieves linear parallel\ngripping functionality and exhibits good adaptability. In the context of the\nongoing development of embodied intelligence technologies, this robotic gripper\ncan assist various robots in achieving effective grasping, laying a solid\nfoundation for collecting data to enhance deep learning training.", "AI": {"tldr": "本文提出了一种新型SPD机器人夹持器，其特点是手指尖端沿直线运动，实现线性平行抓取，并具有良好的适应性，解决了传统夹持器弧线运动导致的高度调整问题。", "motivation": "传统的工业平行夹持器在抓取时指尖呈弧线运动，需要机器人手臂调整整体高度以避免与桌面碰撞，且对不同形状和大小物体的适应性有限。", "method": "本文介绍了SPD夹持器的设计理念、基本组成原理和优化分析理论，并基于此开发了机器人夹持器原型进行测试。", "result": "实验结果表明，SPD机器人夹持器成功实现了线性平行抓取功能，并展现出良好的适应性，能够抓取不同形状和大小的物体。", "conclusion": "SPD机器人夹持器通过其线性平行抓取机制，有效解决了传统夹持器的碰撞问题，并具有出色的适应性，为具身智能技术的发展和深度学习训练数据收集奠定了基础。"}}
{"id": "2510.16282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16282", "abs": "https://arxiv.org/abs/2510.16282", "authors": ["Zhaoxuan Tan", "Zixuan Zhang", "Haoyang Wen", "Zheng Li", "Rongzhi Zhang", "Pei Chen", "Fengran Mo", "Zheyuan Liu", "Qingkai Zeng", "Qingyu Yin", "Meng Jiang"], "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork", "comment": null, "summary": "Personalized large language models (LLMs) tailor content to individual\npreferences using user profiles or histories. However, existing\nparameter-efficient fine-tuning (PEFT) methods, such as the\n``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for\neach user, making them computationally expensive and impractical for real-time\nupdates. We introduce Profile-to-PEFT, a scalable framework that employs a\nhypernetwork, trained end-to-end, to map a user's encoded profile directly to a\nfull set of adapter parameters (e.g., LoRA), eliminating per-user training at\ndeployment. This design enables instant adaptation, generalization to unseen\nusers, and privacy-preserving local deployment. Experimental results\ndemonstrate that our method outperforms both prompt-based personalization and\nOPPU while using substantially fewer computational resources at deployment. The\nframework exhibits strong generalization to out-of-distribution users and\nmaintains robustness across varying user activity levels and different\nembedding backbones. The proposed Profile-to-PEFT framework enables efficient,\nscalable, and adaptive LLM personalization suitable for large-scale\napplications.", "AI": {"tldr": "现有个性化LLM的PEFT方法（如OPPU）因需为每位用户训练适配器而计算昂贵。本文提出Profile-to-PEFT框架，利用超网络将用户画像直接映射到适配器参数，实现即时、可扩展、高效的LLM个性化，且性能优于现有方法。", "motivation": "现有个性化大型语言模型（LLMs）的参数高效微调（PEFT）方法（如“每用户一个PEFT”OPPU范式）需要为每位用户单独训练适配器，导致计算成本高昂，且不适用于实时更新。", "method": "本文引入Profile-to-PEFT框架，该框架采用一个端到端训练的超网络，将用户编码后的画像直接映射到一套完整的适配器参数（例如LoRA）。这种设计在部署时无需为每位用户进行训练，从而实现即时适应、对未见用户的泛化以及保护隐私的本地部署。", "result": "实验结果表明，该方法在部署时使用的计算资源显著少于基于提示的个性化和OPPU，同时性能更优。该框架对分布外用户表现出强大的泛化能力，并在不同的用户活跃度水平和嵌入骨干网络下保持鲁棒性。", "conclusion": "所提出的Profile-to-PEFT框架实现了高效、可扩展和自适应的LLM个性化，适用于大规模应用。"}}
{"id": "2510.16136", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16136", "abs": "https://arxiv.org/abs/2510.16136", "authors": ["Sayan Deb Sarkar", "Sinisa Stekovic", "Vincent Lepetit", "Iro Armeni"], "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer", "comment": "NeurIPS 2025. Project Page: https://sayands.github.io/guideflow3d/", "summary": "Transferring appearance to 3D assets using different representations of the\nappearance object - such as images or text - has garnered interest due to its\nwide range of applications in industries like gaming, augmented reality, and\ndigital content creation. However, state-of-the-art methods still fail when the\ngeometry between the input and appearance objects is significantly different. A\nstraightforward approach is to directly apply a 3D generative model, but we\nshow that this ultimately fails to produce appealing results. Instead, we\npropose a principled approach inspired by universal guidance. Given a\npretrained rectified flow model conditioned on image or text, our training-free\nmethod interacts with the sampling process by periodically adding guidance.\nThis guidance can be modeled as a differentiable loss function, and we\nexperiment with two different types of guidance including part-aware losses for\nappearance and self-similarity. Our experiments show that our approach\nsuccessfully transfers texture and geometric details to the input 3D asset,\noutperforming baselines both qualitatively and quantitatively. We also show\nthat traditional metrics are not suitable for evaluating the task due to their\ninability of focusing on local details and comparing dissimilar inputs, in\nabsence of ground truth data. We thus evaluate appearance transfer quality with\na GPT-based system objectively ranking outputs, ensuring robust and human-like\nassessment, as further confirmed by our user study. Beyond showcased scenarios,\nour method is general and could be extended to different types of diffusion\nmodels and guidance functions.", "AI": {"tldr": "本文提出了一种无需训练的通用指导方法，通过周期性地向预训练的整流流模型（基于图像或文本条件）中添加指导，成功将外观（纹理和几何细节）转移到几何差异显著的3D资产上，并在定性和定量上超越了现有基线。", "motivation": "将外观（如图像或文本）转移到3D资产在游戏、增强现实和数字内容创作等行业有广泛应用。然而，现有最先进的方法在输入和外观对象几何差异显著时会失效。直接应用3D生成模型也无法产生吸引人的结果，因此需要一种更有效的方法来解决这一挑战。", "method": "本研究提出了一种受通用指导启发的原则性方法。给定一个基于图像或文本条件预训练的整流流模型，该方法通过周期性地添加指导来与采样过程交互，且无需额外训练。指导被建模为可微分的损失函数，实验中使用了两种指导类型：外观的部分感知损失和自相似性。此外，由于传统指标不适用于评估该任务，研究引入了一个基于GPT的系统进行客观排名，并通过用户研究进一步验证其评估的稳健性和类人性。", "result": "该方法成功地将纹理和几何细节转移到输入的3D资产上，在定性和定量上均优于基线。研究还表明，传统指标不适合评估此任务，而所提出的基于GPT的系统和用户研究确保了稳健且类人的外观转移质量评估。", "conclusion": "所提出的方法能够成功地将外观转移到具有显著几何差异的3D资产上，优于现有方法。该方法具有通用性，可扩展到不同类型的扩散模型和指导函数，并引入了一种新的、更适合该任务的评估范式。"}}
{"id": "2510.16145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16145", "abs": "https://arxiv.org/abs/2510.16145", "authors": ["Ahmad Arrabi", "Jay hwasung Jung", "J Le", "A Nguyen", "J Reed", "E Stahl", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy", "comment": null, "summary": "Thrombectomy is one of the most effective treatments for ischemic stroke, but\nit is resource and personnel-intensive. We propose employing deep learning to\nautomate critical aspects of thrombectomy, thereby enhancing efficiency and\nsafety. In this work, we introduce a self-supervised framework that classifies\nvarious skeletal landmarks using a regression-based pretext task. Our\nexperiments demonstrate that our model outperforms existing methods in both\nregression and classification tasks. Notably, our results indicate that the\npositional pretext task significantly enhances downstream classification\nperformance. Future work will focus on extending this framework toward fully\nautonomous C-arm control, aiming to optimize trajectories from the pelvis to\nthe head during stroke thrombectomy procedures. All code used is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance", "AI": {"tldr": "该研究提出一个自监督深度学习框架，通过回归预训练任务对骨骼标志物进行分类，旨在自动化血栓切除术中的C型臂引导，以提高效率和安全性。", "motivation": "血栓切除术是治疗缺血性卒中的有效方法，但资源和人力密集。通过深度学习自动化其关键环节，可以提高效率和安全性。", "method": "引入一个自监督框架，使用基于回归的预训练任务来分类各种骨骼标志物。", "result": "该模型在回归和分类任务中均优于现有方法。研究结果表明，位置预训练任务显著提升了下游分类性能。", "conclusion": "所提出的框架在骨骼标志物分类方面表现出色，并有望应用于未来全自主C型臂控制，以优化卒中血栓切除术中的轨迹规划。"}}
{"id": "2510.17129", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17129", "abs": "https://arxiv.org/abs/2510.17129", "authors": ["Wenbing Tang", "Meilin Zhu", "Fenghua Wu", "Yang Liu"], "title": "Semantic Intelligence: A Bio-Inspired Cognitive Framework for Embodied Agents", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have greatly enhanced\nnatural language understanding and content generation. However, these models\nprimarily operate in disembodied digital environments and lack interaction with\nthe physical world. To address this limitation, Embodied Artificial\nIntelligence (EAI) has emerged, focusing on agents that can perceive and\ninteract with their surroundings. Despite progress, current embodied agents\nface challenges in unstructured real-world environments due to insufficient\nsemantic intelligence, which is critical for understanding and reasoning about\ncomplex tasks. This paper introduces the Semantic Intelligence-Driven Embodied\n(SIDE) agent framework, which integrates a hierarchical semantic cognition\narchitecture with a semantic-driven decision-making process. This enables\nagents to reason about and interact with the physical world in a contextually\nadaptive manner. The framework is inspired by biological cognitive mechanisms\nand utilizes bio-inspired principles to design a semantic cognitive\narchitecture that mimics how humans and animals integrate and process sensory\ninformation. We present this framework as a step toward developing more\nintelligent and versatile embodied agents.", "AI": {"tldr": "本文提出了一种名为SIDE的语义智能驱动具身智能体框架，旨在通过整合分层语义认知架构和语义驱动决策过程，解决当前具身智能体在非结构化真实世界环境中语义智能不足的问题，使其能更自适应地与物理世界互动。", "motivation": "大型语言模型（LLMs）缺乏与物理世界的互动，主要在数字环境中运行。而现有的具身智能体在非结构化真实世界环境中面临挑战，原因在于缺乏足够的语义智能来理解和推理复杂任务。", "method": "引入了语义智能驱动具身（SIDE）智能体框架。该框架整合了分层语义认知架构和语义驱动的决策过程，并借鉴生物认知机制，设计了一个模仿人类和动物如何整合和处理感官信息的语义认知架构。", "result": "该框架使智能体能够以情境自适应的方式对物理世界进行推理和交互。", "conclusion": "所提出的SIDE框架是朝着开发更智能、更通用的具身智能体迈出的重要一步。"}}
{"id": "2510.17155", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17155", "abs": "https://arxiv.org/abs/2510.17155", "authors": ["Mohammadamin Lari"], "title": "A Data-Driven Framework for Online Mitigation of False Data Injection Signals in Networked Control Systems", "comment": "17 pages, 9 figures", "summary": "This paper introduces a novel two-stage framework for online mitigation of\nFalse Data Injection (FDI) signals to improve the resiliency of Networked\nControl Systems (NCSs) and ensure their safe operation in the presence of\nmalicious activities. The first stage involves meta learning to select a base\ntime series forecasting model within a stacked ensemble learning architecture.\nThis is achieved by converting time series data into scalograms using\ncontinuous wavelet transform, which are then split into image frames to\ngenerate a scalo-temporal representation of the data and to distinguish between\ndifferent complexity levels of time series data based on an entropy metric\nusing a convolutional neural network. In the second stage, the selected model\nmitigates false data injection signals in real-time. The proposed framework's\neffectiveness is demonstrated through rigorous simulations involving the\nformation control of differential drive mobile robots. By addressing the\nsecurity challenges in NCSs, this framework offers a promising approach to\nmaintaining system integrity and ensuring operational safety.", "AI": {"tldr": "本文提出了一种新颖的两阶段框架，用于在线缓解虚假数据注入（FDI）信号，以提高网络控制系统（NCS）的弹性并确保其安全运行。", "motivation": "提高网络控制系统（NCS）在恶意活动下的弹性，并确保其安全运行。", "method": "该框架包括两个阶段：第一阶段使用元学习选择堆叠集成学习架构中的基础时间序列预测模型。此过程通过连续小波变换将时间序列数据转换为尺度图，然后分割成图像帧，生成数据的尺度-时间表示，并使用卷积神经网络和熵度量区分不同复杂程度的时间序列数据。第二阶段，选定的模型实时缓解虚假数据注入信号。", "result": "通过对差分驱动移动机器人编队控制的严格仿真，证明了所提框架的有效性。", "conclusion": "该框架解决了NCS中的安全挑战，为维护系统完整性和确保操作安全提供了一种有前景的方法。"}}
{"id": "2510.16340", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16340", "abs": "https://arxiv.org/abs/2510.16340", "authors": ["Pratham Singla", "Shivank Garg", "Ayush Singh", "Ishan Garg", "Ketan Suhaas Saichandran"], "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models", "comment": null, "summary": "Recent advances in post-training techniques have endowed Large Language\nModels (LLMs) with enhanced capabilities for tackling complex, logic-intensive\ntasks through the generation of supplementary planning tokens. This development\nraises a fundamental question: Are these models aware of what they \"learn\" and\n\"think\"? To address this, we define three core competencies: (1) awareness of\nlearned latent policies, (2) generalization of these policies across domains,\nand (3) alignment between internal reasoning traces and final outputs. We\nempirically evaluate these abilities on several tasks, each designed to require\nlearning a distinct policy. Furthermore, we contrast the profiles of models\npost-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization\n(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate\nthat RL-trained models not only demonstrate greater awareness of their learned\nbehaviors and stronger generalizability to novel, structurally similar tasks\nthan SFT models but also often exhibit weak alignment between their reasoning\ntraces and final outputs, an effect most pronounced in GRPO-trained models.", "AI": {"tldr": "本文探讨了大型语言模型（LLMs）在后训练技术（如RL）增强复杂任务处理能力后，是否具备对其“学习”和“思考”的“意识”。研究发现，RL训练的模型在意识和泛化能力上优于SFT模型，但在推理过程与最终输出之间存在对齐问题，尤其在GRPO模型中更为明显。", "motivation": "随着后训练技术赋予LLMs处理复杂逻辑任务的能力，一个基本问题随之产生：这些模型是否“意识”到它们所“学习”和“思考”的内容？", "method": "研究定义了三个核心能力：1) 对学习到的潜在策略的意识，2) 这些策略在不同领域间的泛化能力，3) 内部推理痕迹与最终输出之间的一致性（对齐）。通过在多个需要学习不同策略的任务上进行实证评估，并对比了经过监督微调（SFT）、直接策略优化（DPO）和群组相对策略优化（GRPO）后训练的模型表现。", "result": "研究结果表明，经过强化学习（RL）训练的模型（DPO和GRPO）比SFT模型展现出更强的对其学习行为的意识，以及对结构相似的新任务更强的泛化能力。然而，这些模型（尤其是GRPO训练的模型）在推理痕迹与最终输出之间常常表现出较弱的一致性（对齐）。", "conclusion": "RL训练的LLMs在策略意识和泛化能力上有所提升，但其内部推理过程与最终输出的对齐问题仍是一个挑战，这提示了未来研究需要关注如何提高模型的透明度和可解释性。"}}
{"id": "2510.16359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16359", "abs": "https://arxiv.org/abs/2510.16359", "authors": ["Utsav Dhanuka", "Soham Poddar", "Saptarshi Ghosh"], "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets", "comment": "14 pages, 1 figure, work done as a part of B.Tech project at IIT\n  Kharagpur", "summary": "In an era where public health is increasingly influenced by information\nshared on social media, combatting vaccine skepticism and misinformation has\nbecome a critical societal goal. Misleading narratives around vaccination have\nspread widely, creating barriers to achieving high immunisation rates and\nundermining trust in health recommendations. While efforts to detect\nmisinformation have made significant progress, the generation of real time\ncounter-arguments tailored to debunk such claims remains an insufficiently\nexplored area. In this work, we explore the capabilities of LLMs to generate\nsound counter-argument rebuttals to vaccine misinformation. Building on prior\nresearch in misinformation debunking, we experiment with various prompting\nstrategies and fine-tuning approaches to optimise counter-argument generation.\nAdditionally, we train classifiers to categorise anti-vaccine tweets into\nmulti-labeled categories such as concerns about vaccine efficacy, side effects,\nand political influences allowing for more context aware rebuttals. Our\nevaluation, conducted through human judgment, LLM based assessments, and\nautomatic metrics, reveals strong alignment across these methods. Our findings\ndemonstrate that integrating label descriptions and structured fine-tuning\nenhances counter-argument effectiveness, offering a promising approach for\nmitigating vaccine misinformation at scale.", "AI": {"tldr": "本研究探索并优化了大型语言模型（LLMs）生成疫苗错误信息反驳论证的能力，通过分类反疫苗推文和采用精细化提示及微调策略，旨在大规模对抗疫苗怀疑论。", "motivation": "公共卫生日益受到社交媒体信息的影响，疫苗怀疑论和错误信息广泛传播，阻碍了高免疫率的实现并损害了对健康建议的信任。尽管错误信息检测取得了进展，但生成针对性强的实时反驳论证仍是一个未充分探索的领域。", "method": "本研究探索了LLMs生成有效疫苗错误信息反驳论证的能力，实验了多种提示策略和微调方法来优化反驳论证的生成。此外，训练了分类器将反疫苗推文归类为多标签类别（如疫苗效力、副作用、政治影响），以实现更具上下文感知的反驳。评估通过人工判断、基于LLM的评估和自动指标进行。", "result": "评估结果显示，人工判断、LLM评估和自动指标之间存在高度一致性。研究发现，整合标签描述和结构化微调显著增强了反驳论证的有效性。", "conclusion": "整合标签描述和结构化微调的LLMs为大规模缓解疫苗错误信息提供了一种有前景的方法。"}}
{"id": "2510.16146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16146", "abs": "https://arxiv.org/abs/2510.16146", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Vi Vu", "Ba-Thinh Lam", "Phat Huynh", "Tianyang Wang", "Xingjian Li", "Ulas Bagci", "Min Xu"], "title": "DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization", "comment": "The paper is under review at CMIG", "summary": "The limited availability of annotated data in medical imaging makes\nsemi-supervised learning increasingly appealing for its ability to learn from\nimperfect supervision. Recently, teacher-student frameworks have gained\npopularity for their training benefits and robust performance. However, jointly\noptimizing the entire network can hinder convergence and stability, especially\nin challenging scenarios. To address this for medical image segmentation, we\npropose DuetMatch, a novel dual-branch semi-supervised framework with\nasynchronous optimization, where each branch optimizes either the encoder or\ndecoder while keeping the other frozen. To improve consistency under noisy\nconditions, we introduce Decoupled Dropout Perturbation, enforcing\nregularization across branches. We also design Pair-wise CutMix Cross-Guidance\nto enhance model diversity by exchanging pseudo-labels through augmented input\npairs. To mitigate confirmation bias from noisy pseudo-labels, we propose\nConsistency Matching, refining labels using stable predictions from frozen\nteacher models. Extensive experiments on benchmark brain MRI segmentation\ndatasets, including ISLES2022 and BraTS, show that DuetMatch consistently\noutperforms state-of-the-art methods, demonstrating its effectiveness and\nrobustness across diverse semi-supervised segmentation scenarios.", "AI": {"tldr": "DuetMatch是一种双分支半监督框架，通过异步优化、解耦Dropout扰动、成对CutMix交叉指导和一致性匹配，提高了医疗图像分割在有限标注数据下的性能和鲁棒性。", "motivation": "医疗图像标注数据稀缺，半监督学习具有吸引力；现有师生框架在联合优化整个网络时，在复杂场景下存在收敛和稳定性问题。", "method": "提出DuetMatch，一个双分支半监督框架，采用异步优化（每个分支冻结一部分，只优化编码器或解码器）。引入解耦Dropout扰动以在噪声条件下增强一致性。设计成对CutMix交叉指导，通过增强输入对交换伪标签来增加模型多样性。提出一致性匹配，利用冻结教师模型的稳定预测来细化伪标签，以减轻确认偏差。", "result": "在ISLES2022和BraTS等脑部MRI分割基准数据集上进行了广泛实验，DuetMatch始终优于最先进的方法，证明了其在各种半监督分割场景中的有效性和鲁棒性。", "conclusion": "DuetMatch是一种有效且鲁棒的半监督医疗图像分割方法，能够很好地应对有限标注数据和噪声条件下的挑战。"}}
{"id": "2510.16518", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16518", "abs": "https://arxiv.org/abs/2510.16518", "authors": ["Jesús Ortega-Peimbert", "Finn Lukas Busch", "Timon Homberger", "Quantao Yang", "Olov Andersson"], "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation", "comment": null, "summary": "Advances in open-vocabulary semantic mapping and object navigation have\nenabled robots to perform an informed search of their environment for an\narbitrary object. However, such zero-shot object navigation is typically\ndesigned for simple queries with an object name like \"television\" or \"blue\nrug\". Here, we consider more complex free-text queries with spatial\nrelationships, such as \"find the remote on the table\" while still leveraging\nrobustness of a semantic map. We present DIV-Nav, a real-time navigation system\nthat efficiently addresses this problem through a series of relaxations: i)\nDecomposing natural language instructions with complex spatial constraints into\nsimpler object-level queries on a semantic map, ii) computing the Intersection\nof individual semantic belief maps to identify regions where all objects\nco-exist, and iii) Validating the discovered objects against the original,\ncomplex spatial constrains via a LVLM. We further investigate how to adapt the\nfrontier exploration objectives of online semantic mapping to such spatial\nsearch queries to more effectively guide the search process. We validate our\nsystem through extensive experiments on the MultiON benchmark and real-world\ndeployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More\ndetails and videos are available at https://anonsub42.github.io/reponame/", "AI": {"tldr": "DIV-Nav是一个实时导航系统，使机器人能够根据包含复杂空间关系的自然语言指令（如“找到桌子上的遥控器”）搜索并定位物体，通过分解查询、交叉语义地图和使用LVLM进行验证来实现。", "motivation": "现有零样本物体导航系统通常只能处理简单的物体名称查询（如“电视”），无法应对包含复杂空间关系的自由文本查询，而这在实际应用中非常重要。", "method": "DIV-Nav系统通过一系列放松策略解决此问题：i) 将复杂自然语言指令分解为语义地图上的简单物体级查询；ii) 计算单个语义置信地图的交集，以识别所有物体共存的区域；iii) 使用大型视觉语言模型（LVLM）根据原始复杂空间约束验证发现的物体。此外，该方法还调整了在线语义建图的探索目标，以更有效地指导空间搜索过程。", "result": "该系统在MultiON基准测试和波士顿动力Spot机器人的真实世界部署（使用Jetson Orin AGX）中进行了广泛验证，证明了其有效性。", "conclusion": "DIV-Nav通过分解、交集和验证的策略，并结合适应性探索目标，成功地使机器人能够根据复杂的自由文本查询（包含空间关系）进行物体导航，从而扩展了零样本物体导航的能力。"}}
{"id": "2510.16160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16160", "abs": "https://arxiv.org/abs/2510.16160", "authors": ["Ahmad Arrabi", "Jay Hwasung Jung", "Jax Luo", "Nathan Franssen", "Scott Raymond", "Safwan Wshah"], "title": "Automated C-Arm Positioning via Conformal Landmark Localization", "comment": null, "summary": "Accurate and reliable C-arm positioning is essential for fluoroscopy-guided\ninterventions. However, clinical workflows rely on manual alignment that\nincreases radiation exposure and procedural delays. In this work, we present a\npipeline that autonomously navigates the C-arm to predefined anatomical\nlandmarks utilizing X-ray images. Given an input X-ray image from an arbitrary\nstarting location on the operating table, the model predicts a 3D displacement\nvector toward each target landmark along the body. To ensure reliable\ndeployment, we capture both aleatoric and epistemic uncertainties in the\nmodel's predictions and further calibrate them using conformal prediction. The\nderived prediction regions are interpreted as 3D confidence regions around the\npredicted landmark locations. The training framework combines a probabilistic\nloss with skeletal pose regularization to encourage anatomically plausible\noutputs. We validate our approach on a synthetic X-ray dataset generated from\nDeepDRR. Results show not only strong localization accuracy across multiple\narchitectures but also well-calibrated prediction bounds. These findings\nhighlight the pipeline's potential as a component in safe and reliable\nautonomous C-arm systems. Code is available at\nhttps://github.com/AhmadArrabi/C_arm_guidance_APAH", "AI": {"tldr": "该研究提出了一种利用X射线图像自主导航C臂到预定义解剖标志的流水线，通过预测3D位移向量并量化不确定性来提高定位精度和安全性。", "motivation": "在荧光透视引导手术中，C臂的准确可靠定位至关重要。然而，目前的临床工作流程依赖手动对齐，这增加了辐射暴露和手术延迟。", "method": "该方法通过X射线图像预测从任意起始位置到目标解剖标志的3D位移向量。为确保可靠部署，模型量化了预测中的偶然不确定性（aleatoric）和认知不确定性（epistemic），并使用共形预测（conformal prediction）进行校准，将预测区域解释为3D置信区域。训练框架结合了概率损失和骨骼姿态正则化，以生成符合解剖学原理的输出。", "result": "在DeepDRR生成的合成X射线数据集上进行验证，结果显示该方法在多种架构下均具有强大的定位精度和良好校准的预测边界。", "conclusion": "研究结果突出了该流水线作为安全可靠自主C臂系统组件的潜力。"}}
{"id": "2510.16363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16363", "abs": "https://arxiv.org/abs/2510.16363", "authors": ["Nilmadhab Das", "Vishal Vaibhav", "Yash Sunil Choudhary", "V. Vijaya Saradhi", "Ashish Anand"], "title": "End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction", "comment": "Accepted version. To appear in IJCNN 2025", "summary": "Argument Mining (AM) helps in automating the extraction of complex\nargumentative structures such as Argument Components (ACs) like Premise, Claim\netc. and Argumentative Relations (ARs) like Support, Attack etc. in an\nargumentative text. Due to the inherent complexity of reasoning involved with\nthis task, modelling dependencies between ACs and ARs is challenging. Most of\nthe recent approaches formulate this task through a generative paradigm by\nflattening the argumentative structures. In contrast to that, this study\njointly formulates the key tasks of AM in an end-to-end fashion using\nAutoregressive Argumentative Structure Prediction (AASP) framework. The\nproposed AASP framework is based on the autoregressive structure prediction\nframework that has given good performance for several NLP tasks. AASP framework\nmodels the argumentative structures as constrained pre-defined sets of actions\nwith the help of a conditional pre-trained language model. These actions build\nthe argumentative structures step-by-step in an autoregressive manner to\ncapture the flow of argumentative reasoning in an efficient way. Extensive\nexperiments conducted on three standard AM benchmarks demonstrate that AASP\nachieves state-of-theart (SoTA) results across all AM tasks in two benchmarks\nand delivers strong results in one benchmark.", "AI": {"tldr": "本研究提出了一种自回归论证结构预测 (AASP) 框架，用于端到端地联合建模论证挖掘任务，并在多个基准测试中取得了最先进的性能。", "motivation": "论证挖掘任务涉及提取论证成分（如前提、主张）和论证关系（如支持、攻击），推理复杂，且论证成分和关系之间的依赖建模具有挑战性。现有方法通常通过扁平化论证结构来解决，但这可能无法有效捕捉内在的推理复杂性。", "method": "本研究采用自回归结构预测框架，提出了自回归论证结构预测 (AASP)。该框架利用条件预训练语言模型，将论证结构建模为受限的预定义动作集。这些动作以自回归方式逐步构建论证结构，以高效捕捉论证推理流程。", "result": "在三个标准论证挖掘基准测试中进行了广泛实验。结果表明，AASP 在其中两个基准测试的所有论证挖掘任务中都取得了最先进 (SoTA) 的结果，并在另一个基准测试中取得了强劲的结果。", "conclusion": "AASP 框架能够有效地以端到端方式联合建模论证挖掘任务，通过自回归方式捕捉论证推理流程，并在多个标准数据集上显著优于现有方法，达到了最先进的性能。"}}
{"id": "2510.17290", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17290", "abs": "https://arxiv.org/abs/2510.17290", "authors": ["Qihao Peng", "Tierui Gong", "Zihang Song", "Qu Luo", "Zihuai Lin", "Pei Xiao", "Chau Yuen"], "title": "Enhanced Ground-Satellite Direct Access via Onboard Rydberg Atomic Quantum Receivers", "comment": "Submitted to IEEE Journal", "summary": "Ground-satellite links for 6G networks face critical challenges, including\nsevere path loss, tight size-weight-power limits, and congested spectrum, all\nof which significantly hinder the performance of traditional radio frequency\n(RF) front ends. This article introduces the Rydberg Atomic Quantum Receiver\n(RAQR) for onboard satellite systems, a millimeter-scale front end that\nconverts radio fields to optical signals through atomic electromagnetically\ninduced transparency. RAQR's high sensitivity and high frequency selectivity\naddress link budget, payload, and interference challenges while fitting within\nspace constraints. A hybrid atomic-electronic design and supporting signal\nmodel demonstrate enhanced data rate, coverage, and sensing accuracy relative\nto conventional RF receivers. The article concludes with integration\nstrategies, distributed-satellite concepts, and open research problems for\nbringing RAQR-enabled satellite payloads into service.", "AI": {"tldr": "本文提出了一种基于里德堡原子量子接收机（RAQR）的毫米级星载前端，用于6G地空链路，通过原子电磁感应透明将射频信号转换为光信号，以解决传统射频前端面临的挑战。", "motivation": "6G地空链路面临严重的路径损耗、严格的尺寸-重量-功率（SWaP）限制和频谱拥堵等关键挑战，这些都严重阻碍了传统射频前端的性能。", "method": "引入里德堡原子量子接收机（RAQR），这是一种毫米级前端，利用原子电磁感应透明（EIT）将射频场转换为光信号。采用混合原子-电子设计，并建立了支持的信号模型。", "result": "RAQR具有高灵敏度和高频率选择性，解决了链路预算、有效载荷和干扰等挑战，同时符合空间限制。相对于传统射频接收机，RAQR展示了更高的数据速率、更广的覆盖范围和更高的传感精度。", "conclusion": "文章总结了RAQR在卫星有效载荷中的集成策略、分布式卫星概念以及将RAQR投入实际服务所需的开放研究问题。"}}
{"id": "2510.16524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16524", "abs": "https://arxiv.org/abs/2510.16524", "authors": ["Haokai Ding", "Zhaohan Chen", "Tao Yang", "Wenzeng Zhang"], "title": "Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "6 pages, 9 figures, Accepted author manuscript for IEEE CASE 2025", "summary": "This paper presents the SP-Diff parallel gripper system, addressing the\nlimited adaptability of conventional end-effectors in intelligent industrial\nautomation. The proposed design employs an innovative differential linkage\nmechanism with a modular symmetric dual-finger configuration to achieve\nlinear-parallel grasping. By integrating a planetary gear transmission, the\nsystem enables synchronized linear motion and independent finger pose\nadjustment while maintaining structural rigidity, reducing Z-axis recalibration\nrequirements by 30% compared to arc-trajectory grippers. The compact palm\narchitecture incorporates a kinematically optimized parallelogram linkage and\nDifferential mechanism, demonstrating adaptive grasping capabilities for\ndiverse industrial workpieces and deformable objects such as citrus fruits.\nFuture-ready interfaces are embedded for potential force/vision sensor\nintegration to facilitate multimodal data acquisition (e.g., trajectory\nplanning and object deformation) in digital twin frameworks. Designed as a\nflexible manufacturing solution, SP-Diff advances robotic end-effector\nintelligence through its adaptive architecture, showing promising applications\nin collaborative robotics, logistics automation, and specialized operational\nscenarios.", "AI": {"tldr": "本文提出SP-Diff平行夹持器系统，采用差动连杆机构和行星齿轮传动，实现线性平行抓取、同步运动和独立指姿调整，具有出色的适应性，适用于多样化工业工件和可变形物体。", "motivation": "传统末端执行器在智能工业自动化中适应性有限，促使研究人员开发更具柔性和智能的抓取解决方案。", "method": "该系统采用创新的差动连杆机构和模块化对称双指配置实现线性平行抓取。通过集成行星齿轮传动，实现同步线性运动和独立的指姿调整。紧凑的掌部结构结合了运动学优化的平行四边形连杆和差动机构，以实现自适应抓取。", "result": "SP-Diff系统实现了线性平行抓取，能够同步运动并独立调整指姿，同时保持结构刚性。与弧线轨迹夹持器相比，Z轴校准要求减少30%。它展示了对多样化工业工件和柑橘类水果等可变形物体的自适应抓取能力。此外，还嵌入了未来接口，用于集成力/视觉传感器，以支持数字孪生框架中的多模态数据采集。", "conclusion": "SP-Diff系统通过其自适应架构提升了机器人末端执行器的智能化水平，提供了灵活的制造解决方案，在协作机器人、物流自动化和专业操作场景中具有广阔的应用前景。"}}
{"id": "2510.17176", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17176", "abs": "https://arxiv.org/abs/2510.17176", "authors": ["Lakshmikanta Sau", "Priyadarshi Mukherjee", "Sasthi C. Ghosh"], "title": "Generalized Group Selection Strategies for Self-sustainable RIS-aided Communication", "comment": "This work has been submitted to an IEEE journal for possible\n  publication", "summary": "Reconfigurable intelligent surface (RIS) is a cutting-edge communication\ntechnology that has been proposed as aviable option for beyond fifth-generation\nwireless communication networks. This paper investigates various group\nselection strategies in the context of grouping-based self-sustainable\nRIS-aided device-to-device (D2D) communication with spatially correlated\nwireless channels. Specifically, we consider both power splitting (PS) and time\nswitching (TS) configurations, of the self-sustainable RIS to analyze the\nsystem performance and propose appropriate bounds on the choice of system\nparameters. The analysis takes into account a simplified linear energy\nharvesting (EH) model as well as a practical non-linear EH model. Based on the\napplication requirements, we propose various group selection strategies at the\nRIS. Notably, each strategy schedules the k-th best available group at the RIS\nbased on the end-to-end signal-to-noise ratio (SNR) and also the energy\nharvested at a particular group of the RIS. Accordingly, by using tools from\nhigh order statistics, we derive analytical expressions for the outage\nprobability of each selection strategy. Moreover, by applying the tools from\nextreme value theory, we also investigate an asymptotic scenario, where the\nnumber of groups available for selection at an RIS approaches infinity. The\nnontrivial insights obtained from this approach is especially beneficial in\napplications like large intelligent surface-aided wireless communication.\nFinally, the numerical results demonstrate the importance and benefits of the\nproposed approaches in terms of metrics such as the data throughput and the\noutage (both data and energy) performance.", "AI": {"tldr": "本文研究了基于分组的自可持续可重构智能表面（RIS）辅助设备到设备（D2D）通信中的分组选择策略，考虑了功率分裂（PS）和时间切换（TS）配置、线性和非线性能量收集（EH）模型，并通过高阶统计和极值理论推导了中断概率的解析表达式，数值结果验证了所提方法的性能优势。", "motivation": "可重构智能表面（RIS）是超越第五代无线通信网络的前沿技术。本文旨在研究在具有空间相关无线信道的自可持续RIS辅助D2D通信中，如何通过分组选择策略来优化系统性能。", "method": "研究了RIS处各种分组选择策略，考虑了自可持续RIS的功率分裂（PS）和时间切换（TS）配置，并分析了简化的线性与实际的非线性能量收集（EH）模型。基于应用需求，提出了基于端到端信噪比（SNR）和特定RIS分组收集能量的最佳分组选择策略。利用高阶统计工具推导了各选择策略的中断概率解析表达式。此外，应用极值理论研究了RIS可用分组数量趋于无限的渐近场景。", "result": "推导了每种选择策略中断概率的解析表达式。通过渐近分析获得了对大型智能表面辅助无线通信应用特别有益的非平凡见解。数值结果表明，所提出的方法在数据吞吐量和中断（数据和能量）性能方面具有重要性和优势。", "conclusion": "所提出的分组选择策略和分析方法在自可持续RIS辅助D2D通信中具有显著优势，尤其对于大型智能表面辅助的无线通信，能够有效提升数据吞吐量和降低中断概率。"}}
{"id": "2510.16373", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16373", "abs": "https://arxiv.org/abs/2510.16373", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira"], "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment", "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is transforming AI,\nopening new opportunities in sensitive and high-impact areas such as Mental\nHealth (MH). Yet, despite these advancements, recent evidence reveals that\nsmaller-scale models still struggle to deliver optimal performance in\ndomain-specific applications. In this study, we present a cost-efficient yet\npowerful approach to improve MH assessment capabilities of an LLM, without\nrelying on any computationally intensive techniques. Our lightweight method\nconsists of a linear transformation applied to a specific layer's activations,\nleveraging steering vectors to guide the model's output. Remarkably, this\nintervention enables the model to achieve improved results across two distinct\ntasks: (1) identifying whether a Reddit post is useful for detecting the\npresence or absence of depressive symptoms (relevance prediction task), and (2)\ncompleting a standardized psychological screening questionnaire for depression\nbased on users' Reddit post history (questionnaire completion task). Results\nhighlight the untapped potential of steering mechanisms as computationally\nefficient tools for LLMs' MH domain adaptation.", "AI": {"tldr": "本研究提出了一种经济高效且强大的轻量级方法，通过对特定层激活应用线性变换和转向向量，显著提升了大型语言模型（LLM）在心理健康（MH）评估方面的能力，尤其是在抑郁症检测相关任务上。", "motivation": "尽管大型语言模型（LLM）发展迅速，并在心理健康等高影响领域带来新机遇，但现有证据表明，小型模型在特定领域应用中仍难以达到最佳性能。", "method": "本研究采用了一种轻量级方法，无需依赖计算密集型技术。该方法包括对特定层激活进行线性变换，并利用转向向量来引导模型的输出。", "result": "该干预措施显著改善了模型在两项不同任务上的表现：(1) 识别Reddit帖子是否有助于检测抑郁症状（相关性预测任务）；(2) 根据用户的Reddit帖子历史完成标准化抑郁症心理筛查问卷（问卷完成任务）。", "conclusion": "研究结果强调了转向机制作为计算高效工具在LLM心理健康领域适应中的巨大潜力。"}}
{"id": "2510.16617", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16617", "abs": "https://arxiv.org/abs/2510.16617", "authors": ["Ruihan Zhao", "Tyler Ingebrand", "Sandeep Chinchali", "Ufuk Topcu"], "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation", "comment": null, "summary": "Vision-Language-Action (VLA) models trained on large robot datasets promise\ngeneral-purpose, robust control across diverse domains and embodiments.\nHowever, existing approaches often fail out-of-the-box when deployed in novel\nenvironments, embodiments, or tasks. We introduce Mixture of Skills VLA\n(MoS-VLA), a framework that represents robot manipulation policies as linear\ncombinations of a finite set of learned basis functions. During pretraining,\nMoS-VLA jointly learns these basis functions across datasets from the Open\nX-Embodiment project, producing a structured skill space. At test time,\nadapting to a new task requires only a single expert demonstration. The\ncorresponding skill representation is then inferred via a lightweight convex\noptimization problem that minimizes the L1 action error, without requiring\ngradient updates. This gradient-free adaptation incurs minimal overhead while\nenabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower\naction-prediction error on five out of five unseen datasets and succeeds in\nboth simulation and real-robot tasks where a pretrained VLA model fails\noutright. Project page: mos-vla.github.io/", "AI": {"tldr": "MoS-VLA是一种新的视觉-语言-动作（VLA）模型框架，它将机器人操作策略表示为有限学习基础技能的线性组合。该模型通过单次专家演示和轻量级凸优化实现快速、无梯度适应新任务，在未见过的数据集和实际机器人任务中表现优于现有VLA模型。", "motivation": "现有的VLA模型在部署到新环境、新载体或新任务时，往往无法直接开箱即用，泛化能力不足。", "method": "MoS-VLA将机器人操作策略表示为一组有限学习基础函数的线性组合。在预训练阶段，它在Open X-Embodiment项目的数据集上共同学习这些基础函数，形成一个结构化的技能空间。在测试时，只需一次专家演示即可适应新任务，并通过最小化L1动作误差的轻量级凸优化问题来推断相应的技能表示，无需梯度更新，从而实现快速适应。", "result": "MoS-VLA在五个未见过的数据集中有五个实现了更低的动作预测误差。在模拟和真实机器人任务中，当预训练的VLA模型完全失败时，MoS-VLA却能成功完成任务。", "conclusion": "MoS-VLA通过引入技能混合表示和无梯度适应机制，显著提升了VLA模型在多样化环境和任务中的泛化能力和鲁棒性，为通用机器人控制提供了有效途径。"}}
{"id": "2510.16179", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16179", "abs": "https://arxiv.org/abs/2510.16179", "authors": ["Xavier Giro-i-Nieto", "Nefeli Andreou", "Anqi Liang", "Manel Baradad", "Francesc Moreno-Noguer", "Aleix Martinez"], "title": "Cost Savings from Automatic Quality Assessment of Generated Images", "comment": null, "summary": "Deep generative models have shown impressive progress in recent years, making\nit possible to produce high quality images with a simple text prompt or a\nreference image. However, state of the art technology does not yet meet the\nquality standards offered by traditional photographic methods. For this reason,\nproduction pipelines that use generated images often include a manual stage of\nimage quality assessment (IQA). This process is slow and expensive, especially\nbecause of the low yield of automatically generated images that pass the\nquality bar. The IQA workload can be reduced by introducing an automatic\npre-filtering stage, that will increase the overall quality of the images sent\nto review and, therefore, reduce the average cost required to obtain a high\nquality image. We present a formula that estimates the cost savings depending\non the precision and pass yield of a generic IQA engine. This formula is\napplied in a use case of background inpainting, showcasing a significant cost\nsaving of 51.61% obtained with a simple AutoML solution.", "AI": {"tldr": "本文提出了一种自动预筛选方法，旨在通过提高输入图像的质量来减少深度生成模型图像质量评估（IQA）的成本和工作量。", "motivation": "尽管深度生成模型取得了显著进展，但其生成的图像质量仍未达到传统摄影标准。这导致在生产流程中需要耗时且昂贵的人工IQA阶段，尤其是在自动生成图像合格率较低的情况下。", "method": "研究者提出一个公式，用于估算基于通用IQA引擎的准确率和通过率所能节省的成本。他们将此公式应用于一个背景修复的用例，并使用一个简单的AutoML解决方案进行实现。", "result": "在一个背景修复的用例中，通过使用简单的AutoML解决方案，实现了51.61%的显著成本节约。", "conclusion": "引入自动预筛选阶段可以显著提高送审图像的整体质量，从而有效降低获得高质量图像所需的平均成本，减少IQA工作量。"}}
{"id": "2510.17333", "categories": ["eess.SY", "cs.CR", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17333", "abs": "https://arxiv.org/abs/2510.17333", "authors": ["Sebastian Schlor", "Frank Allgöwer"], "title": "Comparison and performance analysis of dynamic encrypted control approaches", "comment": null, "summary": "Encrypted controllers using homomorphic encryption have proven to guarantee\nthe privacy of measurement and control signals, as well as system and\ncontroller parameters, while regulating the system as intended. However,\nencrypting dynamic controllers has remained a challenge due to growing noise\nand overflow issues in the encoding. In this paper, we review recent approaches\nto dynamic encrypted control, such as bootstrapping, periodic resets of the\ncontroller state, integer reformulations, and FIR controllers, and equip them\nwith a stability and performance analysis to evaluate their suitability. We\ncomplement the analysis with a numerical performance comparison on a benchmark\nsystem.", "AI": {"tldr": "本文回顾并评估了同态加密动态控制器中的现有方法（如自举、周期性重置、整数重构和FIR控制器），通过稳定性和性能分析以及数值比较来评估它们的适用性。", "motivation": "同态加密控制器在保障隐私的同时能有效调节系统，但动态控制器加密面临噪声增长和溢出等挑战。", "method": "审查了动态加密控制的最新方法，包括自举、控制器状态的周期性重置、整数重构和FIR控制器，并对其进行了稳定性和性能分析。此外，还在基准系统上进行了数值性能比较。", "result": "通过稳定性、性能分析和数值比较，评估了各种动态加密控制方法的适用性。", "conclusion": "该研究为动态加密控制器中的现有方法提供了全面的评估，特别关注了其稳定性和性能，以解决噪声和溢出问题。"}}
{"id": "2510.16692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16692", "abs": "https://arxiv.org/abs/2510.16692", "authors": ["Tianshu Ruan", "Zoe Betta", "Georgios Tzoumas", "Rustam Stolkin", "Manolis Chiou"], "title": "First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response", "comment": null, "summary": "This study investigates First Responders' (FRs) attitudes toward the use of\nsemantic information and Situational Awareness (SA) in robotic systems during\nemergency operations. A structured questionnaire was administered to 22 FRs\nacross eight countries, capturing their demographic profiles, general attitudes\ntoward robots, and experiences with semantics-enhanced SA. Results show that\nmost FRs expressed positive attitudes toward robots, and rated the usefulness\nof semantic information for building SA at an average of 3.6 out of 5. Semantic\ninformation was also valued for its role in predicting unforeseen emergencies\n(mean 3.9). Participants reported requiring an average of 74.6\\% accuracy to\ntrust semantic outputs and 67.8\\% for them to be considered useful, revealing a\nwillingness to use imperfect but informative AI support tools.\n  To the best of our knowledge, this study offers novel insights by being one\nof the first to directly survey FRs on semantic-based SA in a cross-national\ncontext. It reveals the types of semantic information most valued in the field,\nsuch as object identity, spatial relationships, and risk context-and connects\nthese preferences to the respondents' roles, experience, and education levels.\nThe findings also expose a critical gap between lab-based robotics capabilities\nand the realities of field deployment, highlighting the need for more\nmeaningful collaboration between FRs and robotics researchers. These insights\ncontribute to the development of more user-aligned and situationally aware\nrobotic systems for emergency response.", "AI": {"tldr": "本研究调查了22名来自8个国家的第一响应者(FRs)对在紧急行动中使用机器人系统中语义信息和态势感知(SA)的态度。结果显示，FRs普遍对机器人持积极态度，高度评价语义信息对SA构建和预测突发事件的价值，并对不完美的AI支持工具表现出使用意愿。研究揭示了实验室能力与实际部署之间的差距，强调FRs与机器人研究人员之间需要更深入的合作。", "motivation": "本研究旨在了解第一响应者(FRs)对在紧急行动中使用机器人系统中语义信息和态势感知的态度，以及他们对这些技术的具体需求和期望，以促进开发更符合用户需求和情境感知的机器人系统。", "method": "研究采用结构化问卷调查的方式，对来自8个国家的22名第一响应者(FRs)进行了访谈。问卷内容涵盖了FRs的人口统计学信息、对机器人的普遍态度以及他们对语义增强型态势感知的经验和看法。", "result": "大多数FRs对机器人持积极态度，并认为语义信息对构建态势感知（平均3.6/5分）和预测突发事件（平均3.9分）非常有用。受访者表示，他们需要74.6%的准确率才能信任语义输出，67.8%的准确率才能认为其有用，这表明他们愿意使用不完美但有信息量的AI支持工具。研究还揭示了最受重视的语义信息类型包括物体身份、空间关系和风险情境，并指出实验室机器人能力与实际部署之间存在显著差距。", "conclusion": "本研究首次在跨国背景下直接调查了FRs对基于语义的态势感知的看法，提供了新颖见解。研究强调了FRs与机器人研究人员之间进行更深入合作的必要性，以开发更符合用户需求和情境感知的紧急响应机器人系统，从而弥合实验室研究与实际应用之间的差距。"}}
{"id": "2510.16196", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16196", "abs": "https://arxiv.org/abs/2510.16196", "authors": ["Zheng Huang", "Enpei Zhang", "Yinghao Cai", "Weikang Qiu", "Carl Yang", "Elynn Chen", "Xiang Zhang", "Rex Ying", "Dawei Zhou", "Yujun Yan"], "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI", "comment": null, "summary": "Understanding how the brain encodes visual information is a central challenge\nin neuroscience and machine learning. A promising approach is to reconstruct\nvisual stimuli, essentially images, from functional Magnetic Resonance Imaging\n(fMRI) signals. This involves two stages: transforming fMRI signals into a\nlatent space and then using a pretrained generative model to reconstruct\nimages. The reconstruction quality depends on how similar the latent space is\nto the structure of neural activity and how well the generative model produces\nimages from that space. Yet, it remains unclear which type of latent space best\nsupports this transformation and how it should be organized to represent visual\nstimuli effectively. We present two key findings. First, fMRI signals are more\nsimilar to the text space of a language model than to either a vision based\nspace or a joint text image space. Second, text representations and the\ngenerative model should be adapted to capture the compositional nature of\nvisual stimuli, including objects, their detailed attributes, and\nrelationships. Building on these insights, we propose PRISM, a model that\nProjects fMRI sIgnals into a Structured text space as an interMediate\nrepresentation for visual stimuli reconstruction. It includes an object centric\ndiffusion module that generates images by composing individual objects to\nreduce object detection errors, and an attribute relationship search module\nthat automatically identifies key attributes and relationships that best align\nwith the neural activity. Extensive experiments on real world datasets\ndemonstrate that our framework outperforms existing methods, achieving up to an\n8% reduction in perceptual loss. These results highlight the importance of\nusing structured text as the intermediate space to bridge fMRI signals and\nimage reconstruction.", "AI": {"tldr": "本研究提出PRISM模型，通过将fMRI信号映射到结构化文本空间，并结合以对象为中心的扩散和属性关系搜索模块，显著提高了视觉刺激的重建质量，证明了结构化文本作为中间表示的重要性。", "motivation": "理解大脑如何编码视觉信息是神经科学和机器学习的挑战。从fMRI信号重建视觉刺激是一个有前景的方法，但关键在于如何选择和组织中间潜在空间，以有效地连接fMRI信号和生成模型，从而实现高质量的图像重建。", "method": "研究首先发现fMRI信号与语言模型的文本空间相似度更高。在此基础上，提出了PRISM模型，该模型将fMRI信号投影到一个结构化的文本空间作为中间表示。PRISM包含两个核心模块：一个以对象为中心的扩散模块，通过组合独立对象来生成图像并减少对象检测错误；一个属性关系搜索模块，自动识别与神经活动最匹配的关键属性和关系。", "result": "研究发现fMRI信号与语言模型的文本空间比视觉空间或文本-图像联合空间更相似。文本表示和生成模型应适应视觉刺激的组合性（包括对象、属性和关系）。PRISM框架在真实世界数据集上表现优于现有方法，感知损失降低了高达8%。", "conclusion": "研究结果强调了使用结构化文本作为中间表示的重要性，它能有效连接fMRI信号和图像重建，从而显著提高视觉刺激的重建质量。"}}
{"id": "2510.16380", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16380", "abs": "https://arxiv.org/abs/2510.16380", "authors": ["Yu Ying Chiu", "Michael S. Lee", "Rachel Calcott", "Brandon Handoko", "Paul de Font-Reaulx", "Paula Rodriguez", "Chen Bo Calvin Zhang", "Ziwen Han", "Udari Madhushani Sehwag", "Yash Maurya", "Christina Q Knight", "Harry R. Lloyd", "Florence Bacus", "Mantas Mazeika", "Bing Liu", "Yejin Choi", "Mitchell L Gordon", "Sydney Levine"], "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes", "comment": "46 pages, 8 figures, 10 tables. Preprint", "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.", "AI": {"tldr": "该研究引入了MoReBench基准测试集，用于评估AI在道德困境中的程序性推理能力，发现现有基准和扩展法则无法预测AI的道德推理表现，且模型对特定道德框架存在偏好。", "motivation": "随着AI系统日益普及并参与决策，理解AI如何做出决策与理解其决策本身同样重要，以确保决策符合人类价值观。道德困境提供了理想的测试平台，因为它们允许多种合理结论，适合进行以过程为中心的评估。", "method": "研究提出了两个基准测试集：1. MoReBench：包含1,000个道德场景，每个场景配有专家定义的、推理时应包含或避免的评判标准（共2.3万余条），涵盖了AI为人类提供道德建议和自主做出道德决策的场景。2. MoReBench-Theory：包含150个示例，用于测试AI在五种主要规范伦理学框架下的推理能力。", "result": "研究结果表明，当前的扩展法则以及数学、代码和科学推理任务的现有基准无法预测模型执行道德推理的能力。此外，模型对特定的道德框架（例如边沁的行为功利主义和康德的义务论）表现出偏好，这可能是流行训练范式带来的副作用。", "conclusion": "这些基准测试共同推动了以过程为中心的推理评估，有助于实现更安全、更透明的AI系统。"}}
{"id": "2510.16207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16207", "abs": "https://arxiv.org/abs/2510.16207", "authors": ["Mateus Pinto da Silva", "Sabrina P. L. P. Correa", "Hugo N. Oliveira", "Ian M. Nunes", "Jefersson A. dos Santos"], "title": "Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions", "comment": "5 pages, 1 figure", "summary": "Mapping agriculture in tropical areas through remote sensing presents unique\nchallenges, including the lack of high-quality annotated data, the elevated\ncosts of labeling, data variability, and regional generalisation. This paper\nadvocates a Data-Centric Artificial Intelligence (DCAI) perspective and\npipeline, emphasizing data quality and curation as key drivers for model\nrobustness and scalability. It reviews and prioritizes techniques such as\nconfident learning, core-set selection, data augmentation, and active learning.\nThe paper highlights the readiness and suitability of 25 distinct strategies in\nlarge-scale agricultural mapping pipelines. The tropical context is of high\ninterest, since high cloudiness, diverse crop calendars, and limited datasets\nlimit traditional model-centric approaches. This tutorial outlines practical\nsolutions as a data-centric approach for curating and training AI models better\nsuited to the dynamic realities of tropical agriculture. Finally, we propose a\npractical pipeline using the 9 most mature and straightforward methods that can\nbe applied to a large-scale tropical agricultural mapping project.", "AI": {"tldr": "本文提出一种以数据为中心的人工智能（DCAI）方法和流程，以解决热带地区农业遥感测绘中数据质量、成本和泛化等挑战，并推荐了一系列实用技术。", "motivation": "热带地区农业遥感测绘面临独特挑战，包括高质量标注数据缺乏、标注成本高昂、数据变异性大、区域泛化能力差、高云量、作物日历多样性以及数据集有限，这些都限制了传统的以模型为中心的方法。", "method": "本文倡导以数据为中心的人工智能（DCAI）视角和流程，强调数据质量和管理是模型鲁棒性和可扩展性的关键驱动因素。它回顾并优先考虑了自信学习、核心集选择、数据增强和主动学习等技术，并提出了一个包含9种最成熟和直接方法的实用流程，可应用于大规模热带农业测绘项目。", "result": "本文强调了25种不同策略在大规模农业测绘流程中的成熟度和适用性，并最终提出了一个包含9种最成熟和直接方法的实用流程，可有效应用于大规模热带农业测绘项目。", "conclusion": "以数据为中心的方法为管理和训练人工智能模型提供了实用的解决方案，使其更适应热带农业的动态现实，从而解决传统模型中心方法在热带农业遥感测绘中遇到的挑战。"}}
{"id": "2510.16738", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16738", "abs": "https://arxiv.org/abs/2510.16738", "authors": ["Matteo El-Hariry", "Vittorio Franzese", "Miguel Olivares-Mendez"], "title": "Towards Active Excitation-Based Dynamic Inertia Identification in Satellites", "comment": null, "summary": "This paper presents a comprehensive analysis of how excitation design\ninfluences the identification of the inertia properties of rigid nano- and\nmicro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel\ncoupling, actuator limits, and external disturbances, and excite the system\nusing eight torque profiles of varying spectral richness. Two estimators are\ncompared, a batch Least Squares method and an Extended Kalman Filter, across\nthree satellite configurations and time-varying inertia scenarios. Results show\nthat excitation frequency content and estimator assumptions jointly determine\nestimation accuracy and robustness, offering practical guidance for in-orbit\nadaptive inertia identification by outlining the conditions under which each\nmethod performs best. The code is provided as open-source .", "AI": {"tldr": "本文分析了激励设计如何影响刚性纳米/微卫星惯性参数的识别，比较了不同激励和估计器在多种场景下的性能，为在轨自适应识别提供了实用指导。", "motivation": "为了实现在轨自适应地识别纳米/微卫星的惯性特性，需要深入理解激励设计对识别准确性和鲁棒性的影响。", "method": "研究通过模拟非线性姿态动力学（包含反作用轮耦合、执行器限制和外部干扰），并使用八种不同频谱丰富度的力矩剖面激励系统。比较了批量最小二乘法和扩展卡尔曼滤波器两种估计器，并在三种卫星配置和时变惯性场景下进行了测试。", "result": "结果表明，激励频率内容和估计器假设共同决定了估计的准确性和鲁棒性。研究还提供了在不同条件下哪种方法表现最佳的实用指导。", "conclusion": "激励频率内容和估计器假设共同影响惯性识别的准确性和鲁棒性。本文为在轨自适应惯性识别提供了实用指导，指出了不同方法在何种条件下表现最优。"}}
{"id": "2510.16755", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16755", "abs": "https://arxiv.org/abs/2510.16755", "authors": ["Kyung-Hwan Kim", "DongHyun Ahn", "Dong-hyun Lee", "JuYoung Yoon", "Dong Jin Hyun"], "title": "Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation", "comment": "6 pages, accepted to IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025", "summary": "State estimation is crucial for legged robots as it directly affects control\nperformance and locomotion stability. In this paper, we propose an Adaptive\nInvariant Extended Kalman Filter to improve proprioceptive state estimation for\nlegged robots. The proposed method adaptively adjusts the noise level of the\ncontact foot model based on online covariance estimation, leading to improved\nstate estimation under varying contact conditions. It effectively handles small\nslips that traditional slip rejection fails to address, as overly sensitive\nslip rejection settings risk causing filter divergence. Our approach employs a\ncontact detection algorithm instead of contact sensors, reducing the reliance\non additional hardware. The proposed method is validated through real-world\nexperiments on the quadruped robot LeoQuad, demonstrating enhanced state\nestimation performance in dynamic locomotion scenarios.", "AI": {"tldr": "本文提出了一种自适应不变扩展卡尔曼滤波器（A-IEKF），通过在线协方差估计自适应调整接触足模型噪声，以提高四足机器人本体状态估计的鲁棒性，有效处理微小滑移并减少对额外硬件的依赖。", "motivation": "腿足机器人的状态估计对其控制性能和运动稳定性至关重要。传统方法在处理微小滑移时存在困难，且过于敏感的滑移抑制设置可能导致滤波器发散；同时，一些方法依赖于额外的接触传感器。", "method": "提出了一种自适应不变扩展卡尔曼滤波器（A-IEKF）。该方法根据在线协方差估计自适应调整接触足模型的噪声水平，以适应变化的接触条件。它采用接触检测算法替代接触传感器。", "result": "该方法在不同接触条件下显著改善了状态估计性能，有效处理了传统滑移抑制难以解决的微小滑移问题。通过在四足机器人LeoQuad上的真实世界实验验证，证明了其在动态运动场景下增强的状态估计性能。", "conclusion": "所提出的自适应不变扩展卡尔曼滤波器通过自适应调整噪声水平和无需接触传感器的接触检测，显著提高了腿足机器人的本体状态估计能力，尤其是在处理微小滑移和动态运动方面，从而提升了控制性能和运动稳定性。"}}
{"id": "2510.17371", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17371", "abs": "https://arxiv.org/abs/2510.17371", "authors": ["Mohammad Boveiri", "Mohammad Khosravi", "Peyman Mohajerin Esfahan"], "title": "Accelerating Adaptive Systems via Normalized Parameter Estimation Laws", "comment": null, "summary": "In this paper, we propose a new class of parameter estimation laws for\nadaptive systems, called \\emph{normalized parameter estimation laws}. A key\nfeature of these estimation laws is that they accelerate the convergence of the\nsystem state, $\\mathit{x(t)}$, to the origin. We quantify this improvement by\nshowing that our estimation laws guarantee finite integrability of the\n$\\mathit{r}$-th root of the squared norm of the system state, i.e., \\(\n\\mathit{\\|x(t)\\|}_2^{2/\\mathit{r}} \\in \\mathcal{L}_1, \\) where $\\mathit{r} \\geq\n1$ is a pre-specified parameter that, for a broad class of systems, can be\nchosen arbitrarily large. In contrast, standard Lyapunov-based estimation laws\nonly guarantee integrability of $\\mathit{\\|x(t)\\|}_2^2$ (i.e., $\\mathit{r} =\n1$). We motivate our method by showing that, for large values of $r$, this\nguarantee serves as a sparsity-promoting mechanism in the time domain, meaning\nthat it penalizes prolonged signal duration and slow decay, thereby promoting\nfaster convergence of $\\mathit{x(t)}$. The proposed estimation laws do not rely\non time-varying or high adaptation gains and do not require persistent\nexcitation. Moreover, they can be applied to systems with matched and unmatched\nuncertainties, regardless of their dynamic structure, as long as a control\nLyapunov function (CLF) exists. Finally, they are compatible with any CLF-based\ncertainty equivalence controllers. We further develop higher-order extensions\nof our estimation laws by incorporating momentum into the estimation dynamics.\nWe illustrate the performance improvements achieved with the proposed scheme\nthrough various numerical experiments.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2510.16381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16381", "abs": "https://arxiv.org/abs/2510.16381", "authors": ["David Peer", "Sebastian Stabinger"], "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet\ntheir deployment in high-stakes domains is hindered by inherent limitations in\ntrustworthiness, including hallucinations, instability, and a lack of\ntransparency. To address these challenges, we introduce a generic\nneuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The\ncore of our approach lies in decoupling tasks into two distinct phases: Offline\nknowledge ingestion and online task processing. During knowledge ingestion, an\nLLM translates an informal problem specification into a formal, symbolic\nknowledge base. This formal representation is crucial as it can be verified and\nrefined by human experts, ensuring its correctness and alignment with domain\nrequirements. In the subsequent task processing phase, each incoming input is\nencoded into the same formal language. A symbolic decision engine then utilizes\nthis encoded input in conjunction with the formal knowledge base to derive a\nreliable result. Through an extensive evaluation on a complex reasoning task,\nwe demonstrate that a concrete implementation of ATA is competitive with\nstate-of-the-art end-to-end reasoning models in a fully automated setup while\nmaintaining trustworthiness. Crucially, with a human-verified and corrected\nknowledge base, our approach significantly outperforms even larger models,\nwhile exhibiting perfect determinism, enhanced stability against input\nperturbations, and inherent immunity to prompt injection attacks. By generating\ndecisions grounded in symbolic reasoning, ATA offers a practical and\ncontrollable architecture for building the next generation of transparent,\nauditable, and reliable autonomous agents.", "AI": {"tldr": "本文提出了一种名为自主可信代理（ATA）的神经符号方法，通过将任务解耦为离线知识摄取和在线任务处理，以解决大型语言模型在可信度方面的局限性，从而实现透明、可审计和可靠的自主代理。", "motivation": "尽管大型语言模型（LLMs）能力强大，但其在幻觉、不稳定性和缺乏透明度等方面的固有局限性阻碍了它们在高风险领域的部署。本研究旨在解决这些可信度挑战。", "method": "引入了一种通用的神经符号方法，称为自主可信代理（ATA）。该方法将任务解耦为两个阶段：1) 离线知识摄取：LLM将非正式问题规范转化为正式的符号知识库，该知识库可由人类专家验证和完善。2) 在线任务处理：将每个输入编码成相同的形式语言，然后符号决策引擎结合该编码输入和形式知识库来得出可靠结果。", "result": "ATA的具体实现在全自动化设置中与最先进的端到端推理模型具有竞争力，并保持了可信度。通过人类验证和修正的知识库，ATA显著优于更大的模型，同时表现出完美的确定性、增强的输入扰动稳定性以及对提示注入攻击的固有免疫力。", "conclusion": "ATA通过将决策建立在符号推理之上，提供了一种实用且可控的架构，用于构建下一代透明、可审计和可靠的自主代理。"}}
{"id": "2510.16209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16209", "abs": "https://arxiv.org/abs/2510.16209", "authors": ["Nyle Siddiqui", "Rohit Gupta", "Sirnam Swetha", "Mubarak Shah"], "title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales", "comment": null, "summary": "State space models (SSMs) have emerged as a competitive alternative to\ntransformers in various tasks. Their linear complexity and hidden-state\nrecurrence make them particularly attractive for modeling long sequences,\nwhereas attention becomes quadratically expensive. However, current training\nmethods for video understanding are tailored towards transformers and fail to\nfully leverage the unique attributes of SSMs. For example, video models are\noften trained at a fixed resolution and video length to balance the quadratic\nscaling of attention cost against performance. Consequently, these models\nsuffer from degraded performance when evaluated on videos with spatial and\ntemporal resolutions unseen during training; a property we call spatio-temporal\ninflexibility. In the context of action recognition, this severely limits a\nmodel's ability to retain performance across both short- and long-form videos.\nTherefore, we propose a flexible training method that leverages and improves\nthe inherent adaptability of SSMs. Our method samples videos at varying\ntemporal and spatial resolutions during training and dynamically interpolates\nmodel weights to accommodate any spatio-temporal scale. This instills our SSM,\nwhich we call StretchySnake, with spatio-temporal flexibility and enables it to\nseamlessly handle videos ranging from short, fine-grained clips to long,\ncomplex activities. We introduce and compare five different variants of\nflexible training, and identify the most effective strategy for video SSMs. On\nshort-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,\nStretchySnake outperforms transformer and SSM baselines alike by up to 28%,\nwith strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,\nour method provides a simple drop-in training recipe that makes video SSMs more\nrobust, resolution-agnostic, and efficient across diverse action recognition\nscenarios.", "AI": {"tldr": "本文提出了一种名为StretchySnake的灵活训练方法，通过在不同时空分辨率下采样视频并动态插值模型权重，解决了视频状态空间模型（SSM）的时空不灵活性问题，使其在各种动作识别任务中表现出卓越的性能和适应性。", "motivation": "当前视频理解的训练方法主要针对Transformer模型设计，导致状态空间模型（SSM）在训练时固定分辨率和视频长度，从而在评估不同于训练时分辨率和长度的视频时性能下降，即存在“时空不灵活性”。这严重限制了模型在短视频和长视频动作识别中的性能。", "method": "本文提出了一种灵活的训练方法，该方法利用并改进了SSM固有的适应性。具体而言，在训练过程中，该方法以不同的时间及空间分辨率采样视频，并动态插值模型权重以适应任何时空尺度。该方法被命名为StretchySnake。此外，本文引入并比较了五种不同的灵活训练变体，以确定视频SSM最有效的策略。", "result": "在短动作（UCF-101、HMDB-51）和长动作（COIN、Breakfast）基准测试中，StretchySnake的性能比Transformer和SSM基线高出28%。它对细粒度动作（SSV2、Diving-48）也表现出强大的适应性。", "conclusion": "StretchySnake提供了一个简单、可直接使用的训练方案，使得视频状态空间模型（SSM）在各种动作识别场景中更加鲁棒、分辨率无关且高效。"}}
{"id": "2510.17619", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17619", "abs": "https://arxiv.org/abs/2510.17619", "authors": ["Nayab Gogosh", "Sohail Khalid", "Bilal Tariq Malik", "Slawomir Koziel"], "title": "Artificial magnetic conductor backed dual-mode sectoral cylindrical DRA for off-body biomedical telemetry", "comment": "13 pages", "summary": "This research investigates the potential of a sectoral Cylindrical Dielectric\nResonator Antenna (CDRA) for biomedical telemetry. CDRAs are known for their\nlow loss, ruggedness, and stability, but their limited bandwidth and size make\nthem unsuitable for wearable devices. The research addresses these limitations\nby proposing a dual mode antenna that operates in EH110 and TE210 modes. The\nsectoral CDRA is a quarter segment with Perfect Electric Conductor boundaries,\nreducing its size by a factor of four. Mathematical derivations of the field\ncomponents for both modes are derived to support the design. To minimize\nspecific absorption rate (SAR), an Artificial Magnetic Conductor (AMC) surface\nis applied to the antennas backside, enhancing compatibility with the\ntransverse electric modes. The antenna achieves a bandwidth of 0.7 GHz (5.2-5.9\nGHz), suitable for biomedical applications, with a measured peak gain of 7.9\ndBi and a SAR of 1.24 W/kg when applied to a human arm.", "AI": {"tldr": "本研究提出了一种用于生物医学遥测的扇形圆柱介质谐振器天线（CDRA），通过双模操作和人工磁导体（AMC）减小了尺寸，提高了带宽，并降低了特定吸收率（SAR），使其适用于可穿戴设备。", "motivation": "传统的圆柱介质谐振器天线（CDRA）虽然具有低损耗、坚固性和稳定性等优点，但其有限的带宽和较大的尺寸使其不适用于可穿戴生物医学设备。本研究旨在解决这些局限性。", "method": "研究提出了一种双模扇形CDRA，通过将其设计为四分之一段并采用理想电导体（PEC）边界，将尺寸缩小了四倍。该天线在EH110和TE210模式下工作，并推导了两种模式的场分量数学表达式以支持设计。为最小化SAR，天线背面应用了人工磁导体（AMC）表面，以增强与横向电模的兼容性。", "result": "该天线实现了0.7 GHz（5.2-5.9 GHz）的带宽，适用于生物医学应用。测得的峰值增益为7.9 dBi，应用于人体手臂时的SAR为1.24 W/kg。", "conclusion": "所提出的扇形CDRA通过减小尺寸、提高带宽、实现高增益和低SAR，成功克服了传统CDRA的局限性，证明了其在生物医学遥测应用中的巨大潜力。"}}
{"id": "2510.16767", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16767", "abs": "https://arxiv.org/abs/2510.16767", "authors": ["Jia Li", "Guoxiang Zhao"], "title": "T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic", "comment": null, "summary": "Translating natural language instructions into executable motion plans is a\nfundamental challenge in robotics. Traditional approaches are typically\nconstrained by their reliance on domain-specific expertise to customize\nplanners, and often struggle with spatio-temporal couplings that usually lead\nto infeasible motions or discrepancies between task planning and motion\nexecution. Despite the proficiency of Large Language Models (LLMs) in\nhigh-level semantic reasoning, hallucination could result in infeasible motion\nplans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic\nmotion planning framework that self-corrects it output with formal methods. The\nframework decomposes spatio-temporal task constraints via three cascaded\nmodules, each of which stimulates an LLM to generate candidate trajectory\nsequences and examines their feasibility via a Signal Temporal Logic (STL)\nverifier until one that satisfies complex spatial, temporal, and logical\nconstraints is found.Experiments across different scenarios show that T3\nPlanner significantly outperforms the baselines. The required reasoning can be\ndistilled into a lightweight Qwen3-4B model that enables efficient deployment.\nAll supplementary materials are accessible at\nhttps://github.com/leeejia/T3_Planner.", "AI": {"tldr": "T3 Planner是一个LLM驱动的机器人运动规划框架，它通过使用信号时序逻辑（STL）验证器进行自校正，解决了传统方法的局限性和LLM幻觉问题，能够生成满足复杂时空约束的可行运动计划。", "motivation": "传统机器人运动规划方法依赖领域专业知识，难以处理时空耦合，常导致不可行运动或规划与执行不一致。大型语言模型（LLMs）虽擅长高层语义推理，但其幻觉问题可能生成不可行的运动计划。", "method": "T3 Planner框架通过三个级联模块分解时空任务约束。每个模块都促使LLM生成候选轨迹序列，并使用信号时序逻辑（STL）验证器检查其可行性，直到找到满足复杂空间、时间及逻辑约束的计划。此过程包含自校正机制。", "result": "在不同场景下的实验表明，T3 Planner显著优于基线方法。此外，所需的推理能力可以被蒸馏到一个轻量级的Qwen3-4B模型中，实现高效部署。", "conclusion": "T3 Planner通过结合LLM的语义推理能力和形式化方法的自校正机制，有效解决了机器人运动规划中的关键挑战，实现了鲁棒且高效的运动计划生成。"}}
{"id": "2510.16220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16220", "abs": "https://arxiv.org/abs/2510.16220", "authors": ["Djamel Eddine Boukhari"], "title": "VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction", "comment": null, "summary": "Facial Beauty Prediction (FBP) is a complex and challenging computer vision\ntask, aiming to model the subjective and intricate nature of human aesthetic\nperception. While deep learning models, particularly Convolutional Neural\nNetworks (CNNs), have made significant strides, they often struggle to capture\nthe global, holistic facial features that are critical to human judgment.\nVision Transformers (ViT) address this by effectively modeling long-range\nspatial relationships, but their quadratic complexity can be a bottleneck. This\npaper introduces a novel, heterogeneous ensemble architecture,\n\\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths\nof a Vision Transformer and a Mamba-based Vision model, a recent advancement in\nState-Space Models (SSMs). The ViT backbone excels at capturing global facial\nstructure and symmetry, while the Mamba backbone efficiently models long-range\ndependencies with linear complexity, focusing on sequential features and\ntextures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our\nproposed VM-BeautyNet achieves state-of-the-art performance, with a\n\\textbf{Pearson Correlation (PC) of 0.9212}, a \\textbf{Mean Absolute Error\n(MAE) of 0.2085}, and a \\textbf{Root Mean Square Error (RMSE) of 0.2698}.\nFurthermore, through Grad-CAM visualizations, we provide interpretability\nanalysis that confirms the complementary feature extraction of the two\nbackbones, offering new insights into the model's decision-making process and\npresenting a powerful new architectural paradigm for computational aesthetics.", "AI": {"tldr": "本文提出VM-BeautyNet，一种异构集成架构，结合Vision Transformer和Mamba模型，用于面部美感预测，在SCUT-FBP5500数据集上取得了最先进的性能。", "motivation": "面部美感预测（FBP）是一项复杂的计算机视觉任务，旨在模拟人类对美学的主观感知。深度学习模型（尤其是CNN）难以捕捉整体面部特征，而Vision Transformer（ViT）虽能有效建模长距离空间关系，但其二次复杂度是瓶颈。Mamba模型则能高效处理长距离依赖。", "method": "本文引入VM-BeautyNet，一个新颖的异构集成架构，它协同融合了Vision Transformer和基于Mamba的视觉模型的互补优势。ViT骨干网络擅长捕捉全局面部结构和对称性，而Mamba骨干网络则以线性复杂度高效建模长距离依赖，侧重于序列特征和纹理。", "result": "VM-BeautyNet在SCUT-FBP5500数据集上取得了最先进的性能，Pearson相关系数（PC）为0.9212，平均绝对误差（MAE）为0.2085，均方根误差（RMSE）为0.2698。通过Grad-CAM可视化，证实了两个骨干网络互补的特征提取能力。", "conclusion": "VM-BeautyNet通过结合ViT和Mamba的优势，为计算美学提供了一种强大且具有解释性的新架构范式，并为模型决策过程提供了新的见解。"}}
{"id": "2510.16387", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16387", "abs": "https://arxiv.org/abs/2510.16387", "authors": ["Fu-An Chao", "Bi-Cheng Yan", "Berlin Chen"], "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment", "comment": null, "summary": "In this paper, we explore the untapped potential of Whisper, a\nwell-established automatic speech recognition (ASR) foundation model, in the\ncontext of L2 spoken language assessment (SLA). Unlike prior studies that\nextrinsically analyze transcriptions produced by Whisper, our approach goes a\nstep further to probe its latent capabilities by extracting acoustic and\nlinguistic features from hidden representations. With only a lightweight\nclassifier being trained on top of Whisper's intermediate and final outputs,\nour method achieves strong performance on the GEPT picture-description dataset,\noutperforming existing cutting-edge baselines, including a multimodal approach.\nFurthermore, by incorporating image and text-prompt information as auxiliary\nrelevance cues, we demonstrate additional performance gains. Finally, we\nconduct an in-depth analysis of Whisper's embeddings, which reveals that, even\nwithout task-specific fine-tuning, the model intrinsically encodes both ordinal\nproficiency patterns and semantic aspects of speech, highlighting its potential\nas a powerful foundation for SLA and other spoken language understanding tasks.", "AI": {"tldr": "本文利用ASR基础模型Whisper的潜在声学和语言特征进行第二语言口语评估（SLA），通过轻量级分类器在GEPT数据集上取得了优异性能，并证明了Whisper在无需微调的情况下，能编码口语熟练度模式和语义信息，显示其在SLA领域的巨大潜力。", "motivation": "以往研究多局限于分析Whisper生成的转录文本，本研究旨在深入挖掘Whisper作为基础模型在第二语言口语评估（SLA）中未被充分利用的潜力，特别是其隐藏表示中蕴含的声学和语言特征。", "method": "研究方法包括从Whisper的中间和最终输出中提取声学和语言特征；在此基础上训练一个轻量级分类器；通过引入图像和文本提示信息作为辅助相关性线索来提升性能；最后，对Whisper的嵌入进行深入分析，以揭示其内在的编码能力。", "result": "本方法在GEPT图片描述数据集上取得了强大的性能，超越了现有最先进的基线方法（包括多模态方法）。通过整合辅助相关性线索，性能进一步提升。分析显示，即使没有针对特定任务的微调，Whisper的模型嵌入也能内在编码口语熟练度的顺序模式和语义方面。", "conclusion": "Whisper作为SLA和其他口语理解任务的强大基础模型，具有巨大的潜力。其潜在能力，即无需特定任务微调即可编码熟练度模式和语义信息，使其成为第二语言口语评估领域的有力工具。"}}
{"id": "2510.16439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16439", "abs": "https://arxiv.org/abs/2510.16439", "authors": ["Syed Rifat Raiyan", "Md Farhan Ishmam", "Abdullah Al Imran", "Mohammad Ali Moni"], "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution", "comment": null, "summary": "Large language models (LLMs) owe much of their stellar performance to\nexpansive input contexts, yet such verbosity inflates monetary costs, carbon\nfootprint, and inference-time latency. Much of this overhead manifests from the\nredundant low-utility tokens present in typical prompts, as only a fraction of\ntokens typically carries the majority of the semantic weight. We address this\ninefficiency by introducing FrugalPrompt, a novel prompt compression framework\nfor LLMs, which retains only the most semantically significant tokens.\nLeveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,\nwe assign salience scores to every token in an input sequence, rank them to\npreserve the top-k% tokens in their original order, and obtain a sparse\nfrugalized prompt. We evaluate the approach across four NLP tasks: Sentiment\nAnalysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a\nsuite of frontier LLMs. For the first three tasks, a 20% prompt reduction\nincurs only a marginal loss in task performance, demonstrating that\ncontemporary LLMs can reconstruct elided context from high-salience cues. In\ncontrast, performance on mathematical reasoning deteriorates sharply,\nreflecting a stronger dependence on complete token continuity. Further analysis\nwith bottom-k% and random-k% tokens reveals asymmetric performance patterns\nthat may suggest potential task contamination effects, wherein models may\nresort to shallow memorized patterns from pretraining exposure for conventional\nNLP tasks. We posit that our work contributes to a more nuanced understanding\nof LLM behavior in performance-efficiency trade-offs, and delineate the\nboundary between tasks tolerant to contextual sparsity and those requiring\nexhaustive context. Our source code and models are available at:\nhttps://github.com/Starscream-11813/Frugal-ICL", "AI": {"tldr": "FrugalPrompt是一个用于大型语言模型（LLMs）的提示词压缩框架，通过保留语义上最重要的token来减少成本和延迟，同时对大多数NLP任务的性能影响甚微，但在数学推理任务上性能下降显著。", "motivation": "LLMs的冗长输入上下文导致高昂的货币成本、碳足迹和推理延迟，因为其中包含大量冗余的低效用token，而只有一小部分token承载了大部分语义权重。", "method": "FrugalPrompt框架利用GlobEnc和DecompX两种先进的token归因方法，为输入序列中的每个token分配显著性分数，然后对这些token进行排序，并保留前k%的token（保持原始顺序），从而获得一个稀疏的“节俭”提示词。", "result": "对于情感分析、常识问答和摘要这三项任务，20%的提示词压缩仅导致微小的性能损失，表明LLMs可以从高显著性线索中重建省略的上下文。然而，在数学推理任务上的性能急剧下降，这反映了其对完整token连续性更强的依赖。进一步使用最低k%和随机k%token的分析揭示了不对称的性能模式，这可能暗示了潜在的任务污染效应，即模型可能依赖预训练中记忆的浅层模式来处理传统NLP任务。", "conclusion": "该研究有助于更深入地理解LLMs在性能-效率权衡中的行为，并划清了对上下文稀疏性容忍和需要详尽上下文的任务之间的界限。"}}
{"id": "2510.17762", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17762", "abs": "https://arxiv.org/abs/2510.17762", "authors": ["Alexandra E. Ballentine", "Raghvendra V. Cowlagi"], "title": "Trajectory Optimization for Minimum Threat Exposure using Physics-Informed Neural Networks", "comment": "2025 Indian Control Conference", "summary": "We apply a physics-informed neural network (PINN) to solve the two-point\nboundary value problem (BVP) arising from the necessary conditions postulated\nby Pontryagin's Minimum Principle for optimal control. Such BVPs are known to\nbe numerically difficult to solve by traditional shooting methods due to\nextremely high sensitivity to initial guesses. In the light of recent successes\nin applying PINNs for solving high-dimensional differential equations, we\ndevelop a PINN to solve the problem of finding trajectories with minimum\nexposure to a spatiotemporal threat for a vehicle kinematic model. First, we\nimplement PINNs that are trained to solve the BVP for a given pair of initial\nand final states for a given threat field. Next, we implement a PINN\nconditioned on the initial state for a given threat field, which eliminates the\nneed for retraining for each initial state. We demonstrate that the PINN\noutputs satisfy the necessary conditions with low numerical error.", "AI": {"tldr": "本文提出使用物理信息神经网络（PINN）来解决源自庞特里亚金最小原理的最优控制两点边值问题（BVP），该问题传统方法难以求解，并成功应用于车辆在时空威胁下的最小暴露轨迹优化。", "motivation": "最优控制中的两点边值问题（BVP）因对初始猜测的极高敏感性，导致传统射击法在数值求解上非常困难。鉴于PINN在解决高维微分方程方面的近期成功，研究旨在利用PINN来克服这一挑战。", "method": "研究开发了一种物理信息神经网络（PINN）来解决车辆在时空威胁下寻找最小暴露轨迹的问题。首先，训练PINN以解决特定初始/最终状态和威胁场下的BVP。其次，实现了一个以初始状态为条件的PINN（针对给定威胁场），从而避免了对每个初始状态进行重新训练。", "result": "实验证明，PINN的输出满足庞特里亚金最小原理的必要条件，且具有较低的数值误差。这表明PINN能够有效且准确地解决这类复杂的两点边值问题。", "conclusion": "PINN是一种有效且稳健的方法，能够解决最优控制中源自庞特里亚金最小原理的、传统方法难以处理的两点边值问题。它不仅能提供低数值误差的解，还能通过条件化设计减少针对不同初始状态的重新训练需求。"}}
{"id": "2510.16771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16771", "abs": "https://arxiv.org/abs/2510.16771", "authors": ["Xu He", "Xiaolin Meng", "Wenxuan Yin", "Youdong Zhang", "Lingfei Mo", "Xiangdong An", "Fangwen Yu", "Shuguo Pan", "Yufeng Liu", "Jingnan Liu", "Yujia Zhang", "Wang Gao"], "title": "A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT", "comment": null, "summary": "Developing universal Positioning, Navigation, and Timing (PNT) is our\nenduring goal. Today's complex environments demand PNT that is more resilient,\nenergy-efficient and cognitively capable. This paper asks how we can endow\nunmanned systems with brain-inspired spatial cognition navigation while\nexploiting the high precision of machine PNT to advance universal PNT. We\nprovide a new perspective and roadmap for shifting PNT from \"tool-oriented\" to\n\"cognition-driven\". Contributions: (1) multi-level dissection of differences\namong traditional PNT, biological brain PNT and brain-inspired PNT; (2) a\nfour-layer (observation-capability-decision-hardware) fusion framework that\nunites numerical precision and brain-inspired intelligence; (3) forward-looking\nrecommendations for future development of brain-inspired PNT.", "AI": {"tldr": "本文提出了一种将机器PNT的高精度与类脑空间认知导航相结合的新范式和路线图，旨在为无人系统开发更具韧性、能效和认知能力的通用PNT。", "motivation": "在当今复杂环境中，传统的PNT（定位、导航和授时）系统面临韧性、能效和认知能力不足的挑战。研究目标是为无人系统赋予类脑空间认知导航能力，同时利用机器PNT的高精度，以实现更先进的通用PNT。", "method": "本文提出了一种将PNT从“工具导向”转变为“认知驱动”的新视角和路线图。具体方法包括：1) 多层次剖析传统PNT、生物大脑PNT和类脑PNT之间的差异；2) 提出一个四层（观测-能力-决策-硬件）融合框架，以结合数值精度和类脑智能；3) 为类脑PNT的未来发展提供前瞻性建议。", "result": "本文的主要贡献在于：1) 对传统PNT、生物大脑PNT和类脑PNT之间的差异进行了多层次剖析；2) 提出了一个统一数值精度和类脑智能的四层（观测-能力-决策-硬件）融合框架；3) 为类脑PNT的未来发展提供了前瞻性建议。这些构成了将PNT从“工具导向”转向“认知驱动”的新视角和路线图。", "conclusion": "通过整合机器PNT的精度和类脑空间认知导航，可以为无人系统开发更具韧性、能效和认知能力的通用PNT。本文提出的新范式和路线图，旨在推动PNT从“工具导向”向“认知驱动”的转变，从而实现通用PNT的进步。"}}
{"id": "2510.16449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16449", "abs": "https://arxiv.org/abs/2510.16449", "authors": ["Bin Yu", "Xinming Wang", "Shijie Lian", "Haotian Li", "Changti Wu", "Ruina Hu", "Bailing Wang", "Yuliang Wei", "Kai Chen"], "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model", "comment": "13 pages, 6 figures. Project website:\n  https://zgca-ai4edu.github.io/TrajSelector", "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.", "AI": {"tldr": "TrajSelector是一个高效的Best-of-N框架，它利用LLM的隐藏状态和一个轻量级验证器进行过程级评分，在降低推理成本的同时，显著优于现有方法。", "motivation": "现有的大语言模型（LLMs）测试时扩展（TTS）范式（特别是Best-of-N选择）存在局限性：部署过程奖励模型计算开销大，且未能充分利用LLM固有的潜在表示。", "method": "本文提出了TrajSelector，一个高效的Best-of-N框架。它利用采样LLM的隐藏状态进行过程级评分。一个轻量级验证器（0.6B参数）评估分步轨迹的质量，然后聚合这些分数以识别最佳推理轨迹。该框架采用完全数据驱动、端到端的训练方法，无需大量步骤级标注。", "result": "TrajSelector在五个基准测试中展现出持续的性能提升。在Best-of-32设置下，其准确率比多数投票高4.61%，比现有过程奖励模型高4.31%至12.21%，同时保持了更低的推理成本。", "conclusion": "TrajSelector是一个高效且有效的Best-of-N框架，通过利用LLM的隐藏状态和轻量级验证器，提高了推理任务的性能并降低了推理成本。"}}
{"id": "2510.17769", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.17769", "abs": "https://arxiv.org/abs/2510.17769", "authors": ["Michael Nestor", "Jiaxin Wang", "Ning Zhang", "Fei Teng"], "title": "Data-driven Communication and Control Design for Distributed Frequency Regulation with Black-box Inverters", "comment": "Preprint submitted to PSCC 2026", "summary": "The increasing penetration of inverter-based resources into the power grid,\nwith often only black-box models available, challenges long-standing frequency\ncontrol methods. Most recent works take a decentralized approach without online\ndevice coordination via communication. This paper considers both dynamic\nbehavior and communication within secondary frequency control on an\nintermediate timescale. We develop a distributed data-driven approach that\nutilizes peer-to-peer communication between inverters to avoid the need for a\ncentral control center. To enable a trade off between communication network\nrequirements and control performance, we present a framework to guide\ncommunication topology design for secondary frequency regulation. Following\ndesign of the inter-agent information exchange scheme, we design a controller\nthat is structured according to the communication topology with a closed-loop\nstability guarantee. Case studies on the IEEE 39-bus system validate the\nframework and illustrate the trade-off between communication requirements and\ncontrol performance that is enabled by our approach.", "AI": {"tldr": "本文提出了一种针对逆变器并网资源的分布式数据驱动二级频率控制方法，利用点对点通信避免中央控制中心，并提供一个指导通信拓扑设计的框架，以权衡通信需求和控制性能。", "motivation": "逆变器并网资源日益普及，但其模型多为黑箱，对传统频率控制方法构成挑战。现有研究多采用去中心化方法，缺乏在线设备协调。", "method": "开发了一种分布式数据驱动方法，通过逆变器之间的点对点通信避免中央控制中心。提出了一个框架来指导二级频率调节的通信拓扑设计，以平衡通信网络需求和控制性能。设计了一个根据通信拓扑结构化的控制器，并提供闭环稳定性保证。", "result": "在IEEE 39节点系统上的案例研究验证了所提出的框架，并展示了该方法在通信需求和控制性能之间实现的权衡。", "conclusion": "该研究提供了一个分布式二级频率控制的解决方案，通过优化通信拓扑设计，有效平衡了通信成本与控制效果，特别适用于大量逆变器并网的现代电网。"}}
{"id": "2510.16905", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16905", "abs": "https://arxiv.org/abs/2510.16905", "authors": ["Yukang Cao", "Rahul Moorthy", "O. Goktug Poyrazoglu", "Volkan Isler"], "title": "C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control", "comment": "Submitted to the 2026 IEEE International Conference on Robotics and\n  Automation (ICRA). 8 pages, 4 figures", "summary": "Trajectory sampling is a key component of sampling-based control mechanisms.\nTrajectory samplers rely on control input samplers, which generate control\ninputs u from a distribution p(u | x) where x is the current state. We\nintroduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for\nshort) which has two key features: (i) it generates a control input\ndistribution so as to uniformly sample the free configuration space, and (ii)\nin contrast to previously introduced trajectory sampling mechanisms where the\ndistribution p(u | x) is independent of the environment, C-Free-Uniform is\nexplicitly conditioned on the current local map. Next, we integrate this\nsampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.\nExperiments show that CFU-MPPI outperforms existing methods in terms of success\nrate in challenging navigation tasks in cluttered polygonal environments while\nrequiring a much smaller sampling budget.", "AI": {"tldr": "本文提出了一种名为C-Free-Uniform的新型控制输入采样器，它能均匀采样自由配置空间并依赖局部地图。该采样器被集成到新的CFU-MPPI控制器中，在拥挤环境中以更少的采样预算显著优于现有方法。", "motivation": "现有的轨迹采样机制中，控制输入分布p(u|x)与环境无关，这可能导致在复杂或拥挤环境中采样效率低下或性能不佳。", "method": "本文引入了C-Free-Uniform概念，该采样器具有两个关键特性：1) 生成控制输入分布以均匀采样自由配置空间；2) 明确地以当前局部地图为条件。随后，将此采样器集成到一种新的模型预测路径积分(MPPI)控制器中，命名为CFU-MPPI。", "result": "实验表明，CFU-MPPI在拥挤多边形环境中的挑战性导航任务中，成功率优于现有方法，并且所需的采样预算大大减少。", "conclusion": "通过引入C-Free-Uniform采样器，实现了对自由配置空间的均匀采样并考虑了局部地图信息，CFU-MPPI控制器在复杂导航任务中展现出更高的效率和鲁棒性。"}}
{"id": "2510.16235", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16235", "abs": "https://arxiv.org/abs/2510.16235", "authors": ["Vishal Manikanden", "Aniketh Bandlamudi", "Daniel Haehn"], "title": "Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection", "comment": null, "summary": "Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head\nand neck cancer. Due to the subtle nature of its early stages, deep and hidden\nareas of development, and slow growth, OCSCC often goes undetected, leading to\npreventable deaths. However, properly trained Convolutional Neural Networks\n(CNNs), with their precise image segmentation techniques and ability to apply\nkernel matrices to modify the RGB values of images for accurate image pattern\nrecognition, would be an effective means for early detection of OCSCC. Pairing\nthis neural network with image capturing and processing hardware would allow\nincreased efficacy in OCSCC detection. The aim of our project is to develop a\nConvolutional Neural Network trained to recognize OCSCC, as well as to design a\nphysical hardware system to capture and process detailed images, in order to\ndetermine the image quality required for accurate predictions. A CNN was\ntrained on 4293 training images consisting of benign and malignant tumors, as\nwell as negative samples, and was evaluated for its precision, recall, and Mean\nAverage Precision (mAP) in its predictions of OCSCC. A testing dataset of\nrandomly assorted images of cancerous, non-cancerous, and negative images was\nchosen, and each image was altered to represent 5 common resolutions. This test\ndata set was thoroughly analyzed by the CNN and predictions were scored on the\nbasis of accuracy. The designed enhancement hardware was used to capture\ndetailed images, and its impact was scored. An application was developed to\nfacilitate the testing process and bring open access to the CNN. Images of\nincreasing resolution resulted in higher-accuracy predictions on a logarithmic\nscale, demonstrating the diminishing returns of higher pixel counts.", "AI": {"tldr": "该项目开发了一个卷积神经网络（CNN）并设计了配套硬件系统，旨在通过图像分析实现口腔鳞状细胞癌（OCSCC）的早期检测，并研究不同图像分辨率对预测准确性的影响。", "motivation": "口腔鳞状细胞癌（OCSCC）因其早期症状不明显、发展隐蔽且生长缓慢，常导致未被及时发现而造成可预防的死亡。研究旨在利用训练有素的CNN和图像处理技术，实现OCSCC的早期、准确检测。", "method": "研究方法包括：1) 开发一个CNN，并使用包含良性肿瘤、恶性肿瘤和阴性样本的4293张图像进行训练；2) 设计一个物理硬件系统，用于捕获和处理详细图像；3) 评估CNN在预测OCSCC时的精确度、召回率和平均精度均值（mAP）；4) 使用一组随机分类的癌变、非癌变和阴性测试图像，并将每张图像调整为5种常见分辨率，以分析图像质量对预测准确性的影响；5) 使用设计的增强硬件捕获详细图像并评估其影响；6) 开发一个应用程序以方便测试过程并提供对CNN的开放访问。", "result": "研究结果显示，图像分辨率的提高会以对数尺度导致更高的预测准确性，但同时也表明像素数量增加带来的收益递减。这揭示了在OCSCC检测中，更高像素数量的回报递减规律。", "conclusion": "结合CNN和定制的图像捕获处理硬件，可以有效提高OCSCC的早期检测能力。在图像分辨率方面，存在一个最优平衡点，超过该点，更高的像素数量带来的准确性提升将呈递减趋势。"}}
{"id": "2510.17798", "categories": ["eess.SY", "cs.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.17798", "abs": "https://arxiv.org/abs/2510.17798", "authors": ["Samuel Talkington", "Cameron Khanpour", "Rahul K. Gupta", "Sergio A. Dorado-Rojas", "Daniel Turizo", "Hyeongon Park", "Dmitrii M. Ostrovskii", "Daniel K. Molzahn"], "title": "Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks", "comment": "9 pages, 1 figure", "summary": "This paper presents probabilistic bounds for the spectrum of the admittance\nmatrix and classical linear power flow models under uncertain network\nparameters; for example, probabilistic line contingencies. Our proposed\napproach imports tools from probability theory, such as concentration\ninequalities for random matrices with independent entries. It yields error\nbounds for common approximations of the AC power flow equations under parameter\nuncertainty, including the DC and LinDistFlow approximations.", "AI": {"tldr": "本文提出了在不确定网络参数下，导纳矩阵频谱和经典线性潮流模型的概率界限，并为交流潮流方程的常见近似（如DC和LinDistFlow）提供了误差界限。", "motivation": "研究旨在量化并提供在网络参数不确定（例如线路偶发事件）情况下，导纳矩阵频谱和线性潮流模型的行为界限，以及常见潮流近似的误差。", "method": "该方法引入了概率论工具，特别是针对具有独立项的随机矩阵的集中不等式。", "result": "研究为在参数不确定性下，包括DC和LinDistFlow近似在内的交流潮流方程的常见近似提供了误差界限。", "conclusion": "通过应用概率论工具，本文成功地为不确定网络参数下的导纳矩阵频谱和线性潮流模型提供了概率界限，并量化了常见潮流近似的误差。"}}
{"id": "2510.16258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16258", "abs": "https://arxiv.org/abs/2510.16258", "authors": ["Claire McLean", "Makenzie Meendering", "Tristan Swartz", "Orri Gabbay", "Alexandra Olsen", "Rachel Jacobs", "Nicholas Rosen", "Philippe de Bree", "Tony Garcia", "Gadsden Merrill", "Jake Sandakly", "Julia Buffalini", "Neham Jain", "Steven Krenn", "Moneish Kumar", "Dejan Markovic", "Evonne Ng", "Fabian Prada", "Andrew Saba", "Siwei Zhang", "Vasu Agrawal", "Tim Godisart", "Alexander Richard", "Michael Zollhoefer"], "title": "Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset", "comment": null, "summary": "The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of\n500 individual hours of 3D motion data from 439 participants collected in a\nmulti-camera collection stage, amounting to over 54 million frames of tracked\n3D motion. The dataset features a wide range of single-person motion data,\nincluding prompted motions, hand gestures, and locomotion; as well as\nmulti-person behavioral and conversational data like discussions, conversations\nin different emotional states, collaborative activities, and co-living\nscenarios in an apartment-like space. We provide tracked human motion including\nhand tracking and body shape, text annotations, and a separate audio track for\neach participant.", "AI": {"tldr": "Meta发布了Embody 3D，一个包含500小时、来自439名参与者的多模态3D运动数据集，涵盖单人及多人交互场景。", "motivation": "需要一个大规模、多样化且包含多模态信息的3D运动数据集，以支持对单人运动、手势、步态以及多人行为、对话和协作活动的研究。", "method": "通过多摄像头采集阶段，收集了来自439名参与者的500小时3D运动数据。数据包括跟踪的人体运动（含手部跟踪和身体形状）、文本注释以及每个参与者的独立音频轨道。", "result": "数据集包含超过5400万帧的跟踪3D运动数据。内容涵盖广泛的单人运动（如提示动作、手势、步态）和多人行为与对话数据（如讨论、不同情绪下的对话、协作活动、公寓式空间中的共同生活场景）。", "conclusion": "Embody 3D是一个全面且大规模的多模态3D运动数据集，为研究人员提供了丰富的单人和多人交互数据，可用于人体运动分析、行为理解和相关AI模型训练。"}}
{"id": "2510.16455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16455", "abs": "https://arxiv.org/abs/2510.16455", "authors": ["Deyi Ji", "Yuekui Yang", "Haiyang Wu", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning", "comment": "ACL 2025 (Oral, Industry Track)", "summary": "Advertisement (Ad) video violation detection is critical for ensuring\nplatform compliance, but existing methods struggle with precise temporal\ngrounding, noisy annotations, and limited generalization. We propose RAVEN, a\nnovel framework that integrates curriculum reinforcement learning with\nmultimodal large language models (MLLMs) to enhance reasoning and cognitive\ncapabilities for violation detection. RAVEN employs a progressive training\nstrategy, combining precisely and coarsely annotated data, and leverages Group\nRelative Policy Optimization (GRPO) to develop emergent reasoning abilities\nwithout explicit reasoning annotations. Multiple hierarchical sophisticated\nreward mechanism ensures precise temporal grounding and consistent category\nprediction. Experiments on industrial datasets and public benchmarks show that\nRAVEN achieves superior performances in violation category accuracy and\ntemporal interval localization. We also design a pipeline to deploy the RAVEN\non the online Ad services, and online A/B testing further validates its\npractical applicability, with significant improvements in precision and recall.\nRAVEN also demonstrates strong generalization, mitigating the catastrophic\nforgetting issue associated with supervised fine-tuning.", "AI": {"tldr": "RAVEN是一个新颖的框架，它将课程强化学习与多模态大语言模型相结合，用于广告视频违规检测，显著提升了时间定位精度、分类准确性和泛化能力，并在实际应用中表现出色。", "motivation": "现有广告视频违规检测方法在精确时间定位、处理噪声标注和泛化能力方面存在局限性，促使研究人员寻求更有效的解决方案。", "method": "RAVEN框架整合了课程强化学习与多模态大语言模型（MLLMs），通过渐进式训练策略结合精确和粗略标注数据。它利用群组相对策略优化（GRPO）来发展涌现推理能力，并设计了多层次的复杂奖励机制以确保精确的时间定位和一致的类别预测。", "result": "RAVEN在工业数据集和公共基准测试中，违规类别准确性和时间间隔定位方面均表现出卓越性能。在线A/B测试进一步验证了其实用性，显著提高了精确率和召回率。此外，RAVEN还展现出强大的泛化能力，有效缓解了监督微调中灾难性遗忘的问题。", "conclusion": "RAVEN为广告视频违规检测提供了一个高效且实用的解决方案，通过结合课程强化学习和多模态大语言模型，显著提升了检测精度、时间定位能力和泛化性，并在实际部署中取得了成功。"}}
{"id": "2510.16931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16931", "abs": "https://arxiv.org/abs/2510.16931", "authors": ["Zhaoliang Wan", "Zida Zhou", "Zetong Bi", "Zehui Yang", "Hao Ding", "Hui Cheng"], "title": "Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems", "comment": "Accepted by IROS2025", "summary": "This paper addresses the scarcity of affordable, fully-actuated five-fingered\nhands for dexterous teleoperation, which is crucial for collecting large-scale\nreal-robot data within the \"Learning from Demonstrations\" paradigm. We\nintroduce the prototype version of the RAPID Hand, the first low-cost,\n20-degree-of-actuation (DoA) dexterous hand that integrates a novel\nanthropomorphic actuation and transmission scheme with an optimized motor\nlayout and structural design to enhance dexterity. Specifically, the RAPID Hand\nfeatures a universal phalangeal transmission scheme for the non-thumb fingers\nand an omnidirectional thumb actuation mechanism. Prioritizing affordability,\nthe hand employs 3D-printed parts combined with custom gears for easier\nreplacement and repair. We assess the RAPID Hand's performance through\nquantitative metrics and qualitative testing in a dexterous teleoperation\nsystem, which is evaluated on three challenging tasks: multi-finger retrieval,\nladle handling, and human-like piano playing. The results indicate that the\nRAPID Hand's fully actuated 20-DoF design holds significant promise for\ndexterous teleoperation.", "AI": {"tldr": "本文介绍了一种名为 RAPID Hand 的低成本、20自由度全驱动五指灵巧手，旨在解决灵巧远程操作中经济实惠机器人手稀缺的问题。", "motivation": "缺乏经济实惠、全驱动的五指灵巧手，这阻碍了“从演示中学习”范式中大规模真实机器人数据的收集。", "method": "本文提出了 RAPID Hand 原型，它结合了新颖的拟人化驱动和传动方案、优化的电机布局和结构设计。非拇指手指采用通用指骨传动方案，拇指采用全向驱动机制。为降低成本，手部采用3D打印部件和定制齿轮。通过定量指标和定性测试，在多指检索、勺子操作和类人钢琴演奏等挑战性任务中评估了其在灵巧远程操作系统中的性能。", "result": "评估结果表明，RAPID Hand 的全驱动20自由度设计在灵巧远程操作中具有显著潜力。", "conclusion": "RAPID Hand 的全驱动20自由度设计在灵巧远程操作中显示出巨大潜力，有望解决当前经济实惠灵巧手稀缺的问题。"}}
{"id": "2510.16272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16272", "abs": "https://arxiv.org/abs/2510.16272", "authors": ["Baicheng Li", "Zike Yan", "Dong Wu", "Hongbin Zha"], "title": "Proactive Scene Decomposition and Reconstruction", "comment": null, "summary": "Human behaviors are the major causes of scene dynamics and inherently contain\nrich cues regarding the dynamics. This paper formalizes a new task of proactive\nscene decomposition and reconstruction, an online approach that leverages\nhuman-object interactions to iteratively disassemble and reconstruct the\nenvironment. By observing these intentional interactions, we can dynamically\nrefine the decomposition and reconstruction process, addressing inherent\nambiguities in static object-level reconstruction. The proposed system\neffectively integrates multiple tasks in dynamic environments such as accurate\ncamera and object pose estimation, instance decomposition, and online map\nupdating, capitalizing on cues from human-object interactions in egocentric\nlive streams for a flexible, progressive alternative to conventional\nobject-level reconstruction methods. Aided by the Gaussian splatting technique,\naccurate and consistent dynamic scene modeling is achieved with photorealistic\nand efficient rendering. The efficacy is validated in multiple real-world\nscenarios with promising advantages.", "AI": {"tldr": "本文提出了一种主动式场景分解与重建的新任务，通过利用人与物体的交互信息，在自我中心直播流中迭代地分解和重建环境，以实现动态场景的准确建模和渲染。", "motivation": "人类行为是场景动态的主要原因，并包含丰富的动态线索。传统的静态物体级别重建存在固有的模糊性，无法有效处理动态环境，因此需要一种能利用人机交互来动态细化分解和重建过程的方法。", "method": "该方法通过观察人与物体的有意交互，动态地细化分解和重建过程。它整合了多项任务，包括准确的相机和物体姿态估计、实例分解和在线地图更新。此外，该系统利用高斯泼溅技术实现了照片级真实感和高效渲染，从而实现准确和一致的动态场景建模。", "result": "研究实现了准确、一致的动态场景建模，具有照片级真实感和高效渲染能力。在多个真实世界场景中验证了其有效性，并展现出显著优势。", "conclusion": "该研究提供了一种灵活、渐进的替代传统物体级重建方法，能够有效处理动态环境，并利用人机交互线索实现更准确、一致的动态场景分解与重建。"}}
{"id": "2510.17270", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17270", "abs": "https://arxiv.org/abs/2510.17270", "authors": ["Lucas Schulze", "Juliano Decico Negri", "Victor Barasuol", "Vivian Suzano Medeiros", "Marcelo Becker", "Jan Peters", "Oleg Arenz"], "title": "Floating-Base Deep Lagrangian Networks", "comment": null, "summary": "Grey-box methods for system identification combine deep learning with\nphysics-informed constraints, capturing complex dependencies while improving\nout-of-distribution generalization. Yet, despite the growing importance of\nfloating-base systems such as humanoids and quadrupeds, current grey-box models\nignore their specific physical constraints. For instance, the inertia matrix is\nnot only positive definite but also exhibits branch-induced sparsity and input\nindependence. Moreover, the 6x6 composite spatial inertia of the floating base\ninherits properties of single-rigid-body inertia matrices. As we show, this\nincludes the triangle inequality on the eigenvalues of the composite rotational\ninertia. To address the lack of physical consistency in deep learning models of\nfloating-base systems, we introduce a parameterization of inertia matrices that\nsatisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),\nwe train neural networks to predict physically plausible inertia matrices that\nminimize inverse dynamics error under Lagrangian mechanics. For evaluation, we\ncollected and released a dataset on multiple quadrupeds and humanoids. In these\nexperiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly\ncompetitive performance on both simulated and real robots, while providing\ngreater physical interpretability.", "AI": {"tldr": "本文提出了一种名为FeLaN的深度学习方法，通过参数化惯性矩阵以满足浮动基系统（如类人机器人和四足机器人）特有的物理约束，解决了现有灰盒模型物理不一致的问题，从而在模拟和真实机器人上实现了具有竞争力的性能和更高的物理可解释性。", "motivation": "尽管灰盒方法结合了深度学习和物理约束，但当前用于系统识别的灰盒模型忽略了浮动基系统（如类人机器人和四足机器人）的特定物理约束。例如，惯性矩阵不仅是正定的，还具有分支引起的稀疏性和输入独立性，并且浮动基的6x6复合空间惯性继承了单刚体惯性矩阵的特性（包括复合旋转惯性特征值的三角不等式）。这种物理不一致性导致了深度学习模型在浮动基系统中的局限性。", "method": "本文引入了一种惯性矩阵的参数化方法，该方法满足浮动基系统的所有物理约束（包括正定性、分支引起的稀疏性、输入独立性以及复合旋转惯性特征值的三角不等式）。受深度拉格朗日网络（DeLaN）的启发，训练神经网络来预测物理上合理的惯性矩阵，以最小化拉格朗日力学下的逆动力学误差。为进行评估，作者收集并发布了一个包含多种四足机器人和类人机器人的数据集。该方法被称为浮动基深度拉格朗日网络（FeLaN）。", "result": "在模拟和真实机器人的实验中，本文提出的浮动基深度拉格朗日网络（FeLaN）取得了极具竞争力的性能，同时提供了更高的物理可解释性。", "conclusion": "通过引入满足浮动基系统特定物理约束的惯性矩阵参数化方法，FeLaN成功解决了深度学习模型在浮动基系统中物理一致性不足的问题，显著提升了模型性能并增强了物理可解释性。"}}
{"id": "2510.17408", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17408", "abs": "https://arxiv.org/abs/2510.17408", "authors": ["Halima I. Kure", "Jishna Retnakumari", "Augustine O. Nwajana", "Umar M. Ismail", "Bilyaminu A. Romo", "Ehigiator Egho-Promise"], "title": "Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting", "comment": "5 pages, 2 figures", "summary": "This paper presents a novel methodology that integrates trustworthy\nartificial intelligence (AI) with an energy-efficient robotic arm for\nintelligent waste classification and sorting. By utilizing a convolutional\nneural network (CNN) enhanced through transfer learning with MobileNetV2, the\nsystem accurately classifies waste into six categories: plastic, glass, metal,\npaper, cardboard, and trash. The model achieved a high training accuracy of\n99.8% and a validation accuracy of 80.5%, demonstrating strong learning and\ngeneralization. A robotic arm simulator is implemented to perform virtual\nsorting, calculating the energy cost for each action using Euclidean distance\nto ensure optimal and efficient movement. The framework incorporates key\nelements of trustworthy AI, such as transparency, robustness, fairness, and\nsafety, making it a reliable and scalable solution for smart waste management\nsystems in urban settings.", "AI": {"tldr": "本文提出了一种结合可信人工智能和节能机械臂的新方法，用于智能垃圾分类和分拣，实现了高精度分类和高效节能运动，并融入了可信赖AI原则。", "motivation": "该研究旨在为城市环境中的智能垃圾管理系统提供可靠且可扩展的解决方案，以应对垃圾分类和分拣的挑战。", "method": "该方法利用通过MobileNetV2进行迁移学习强化的卷积神经网络（CNN）将垃圾分为六类。通过机械臂模拟器，使用欧几里得距离计算每个动作的能耗，以确保最佳和高效的移动。此外，该框架还融入了透明度、鲁棒性、公平性和安全性等可信人工智能的关键要素。", "result": "该模型在训练中实现了99.8%的准确率，验证准确率为80.5%，表现出强大的学习和泛化能力。机械臂模拟器能够计算每次分拣动作的能耗，确保高效运动。该框架成功整合了可信人工智能的关键原则。", "conclusion": "该研究提供了一个可靠且可扩展的智能垃圾管理系统解决方案，通过结合高精度AI分类、节能机械臂运动和可信赖AI原则，适用于城市环境中的智能废物管理。"}}
{"id": "2510.16290", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16290", "abs": "https://arxiv.org/abs/2510.16290", "authors": ["Yue Zheng", "Xiufang Shi", "Jiming Chen", "Yuanchao Shu"], "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models", "comment": null, "summary": "Video anomaly detection (VAD) has rapidly advanced by recent development of\nVision-Language Models (VLMs). While these models offer superior zero-shot\ndetection capabilities, their immense computational cost and unstable visual\ngrounding performance hinder real-time deployment. To overcome these\nchallenges, we introduce Cerberus, a two-stage cascaded system designed for\nefficient yet accurate real-time VAD. Cerberus learns normal behavioral rules\noffline, and combines lightweight filtering with fine-grained VLM reasoning\nduring online inference. The performance gains of Cerberus come from two key\ninnovations: motion mask prompting and rule-based deviation detection. The\nformer directs the VLM's attention to regions relevant to motion, while the\nlatter identifies anomalies as deviations from learned norms rather than\nenumerating possible anomalies. Extensive evaluations on four datasets show\nthat Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a\n151.79$\\times$ speedup, and 97.2\\% accuracy comparable to the state-of-the-art\nVLM-based VAD methods, establishing it as a practical solution for real-time\nvideo analytics.", "AI": {"tldr": "本文提出了Cerberus，一个两阶段级联系统，通过结合轻量级过滤和细粒度VLM推理，以及引入运动掩码提示和基于规则的偏差检测，实现了高效、准确的实时视频异常检测，速度显著提升且准确率与现有SOTA VLM相当。", "motivation": "尽管视觉-语言模型（VLMs）在零样本视频异常检测方面表现出色，但其巨大的计算成本和不稳定的视觉定位性能阻碍了实时部署。", "method": "Cerberus是一个两阶段级联系统：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理。其核心创新包括：1) 运动掩码提示，引导VLM关注运动相关区域；2) 基于规则的偏差检测，将异常识别为偏离学习规范，而非枚举所有可能异常。", "result": "在NVIDIA L40S GPU上平均达到57.68 fps，实现了151.79倍的速度提升；同时保持97.2%的准确率，与最先进的基于VLM的视频异常检测方法相当。在四个数据集上进行了广泛评估。", "conclusion": "Cerberus通过解决现有VLM的计算效率和实时部署挑战，为实时视频分析提供了一个实用且高性能的解决方案。"}}
{"id": "2510.16458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16458", "abs": "https://arxiv.org/abs/2510.16458", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Benjamin Roth", "Barbara Plank"], "title": "Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations", "comment": null, "summary": "Natural Language Inference datasets often exhibit human label variation. To\nbetter understand these variations, explanation-based approaches analyze the\nunderlying reasoning behind annotators' decisions. One such approach is the\nLiTEx taxonomy, which categorizes free-text explanations in English into\nreasoning types. However, previous work applying such taxonomies has focused on\nwithin-label variation: cases where annotators agree on the final NLI label but\nprovide different explanations. In contrast, this paper broadens the scope by\nexamining how annotators may diverge not only in the reasoning type but also in\nthe labeling step. We use explanations as a lens to decompose the reasoning\nprocess underlying NLI annotation and to analyze individual differences. We\napply LiTEx to two NLI English datasets and align annotation variation from\nmultiple aspects: NLI label agreement, explanation similarity, and taxonomy\nagreement, with an additional compounding factor of annotators' selection bias.\nWe observe instances where annotators disagree on the label but provide highly\nsimilar explanations, suggesting that surface-level disagreement may mask\nunderlying agreement in interpretation. Moreover, our analysis reveals\nindividual preferences in explanation strategies and label choices. These\nfindings highlight that agreement in reasoning types better reflects the\nsemantic similarity of free-text explanations than label agreement alone. Our\nfindings underscore the richness of reasoning-based explanations and the need\nfor caution in treating labels as ground truth.", "AI": {"tldr": "本文通过分析NLI数据集中标注者的自由文本解释，研究了标注偏差，发现标注者可能在标签上存在分歧但在解释推理类型上相似，强调了推理一致性比标签一致性更能反映语义相似性。", "motivation": "自然语言推理（NLI）数据集常出现人工标注差异。为了更好地理解这些差异，现有解释性方法分析标注者决策背后的推理过程，但多集中于标签一致而解释不同的情况。本文旨在拓宽研究范围，探究标注者在推理类型和标签选择上的分歧，以分解NLI标注的推理过程并分析个体差异。", "method": "研究采用LiTEx分类法，将其应用于两个NLI英文数据集的自由文本解释。分析维度包括NLI标签一致性、解释相似性、分类法一致性，并考虑了标注者的选择偏差这一复合因素。", "result": "研究发现，标注者可能在标签上存在分歧，但提供高度相似的解释，这表明表面上的分歧可能掩盖了深层的解释一致性。此外，分析揭示了标注者在解释策略和标签选择上的个体偏好。结果表明，推理类型的一致性比单纯的标签一致性更能反映自由文本解释的语义相似性。", "conclusion": "研究结果强调了基于推理的解释的丰富性，以及在处理NLI标签时需谨慎，不应将其一概视为“黄金标准”，因为推理类型的一致性可能比标签一致性更能反映潜在的语义理解。"}}
{"id": "2510.17038", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17038", "abs": "https://arxiv.org/abs/2510.17038", "authors": ["Pedram Fekri", "Majid Roshanfar", "Samuel Barbeau", "Seyedfarzad Famouri", "Thomas Looi", "Dale Podolsky", "Mehrdad Zadeh", "Javad Dargahi"], "title": "DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation", "comment": null, "summary": "Cardiac catheterization remains a cornerstone of minimally invasive\ninterventions, yet it continues to rely heavily on manual operation. Despite\nadvances in robotic platforms, existing systems are predominantly follow-leader\nin nature, requiring continuous physician input and lacking intelligent\nautonomy. This dependency contributes to operator fatigue, more radiation\nexposure, and variability in procedural outcomes. This work moves towards\nautonomous catheter navigation by introducing DINO-CVA, a multimodal\ngoal-conditioned behavior cloning framework. The proposed model fuses visual\nobservations and joystick kinematics into a joint embedding space, enabling\npolicies that are both vision-aware and kinematic-aware. Actions are predicted\nautoregressively from expert demonstrations, with goal conditioning guiding\nnavigation toward specified destinations. A robotic experimental setup with a\nsynthetic vascular phantom was designed to collect multimodal datasets and\nevaluate performance. Results show that DINO-CVA achieves high accuracy in\npredicting actions, matching the performance of a kinematics-only baseline\nwhile additionally grounding predictions in the anatomical environment. These\nfindings establish the feasibility of multimodal, goal-conditioned\narchitectures for catheter navigation, representing an important step toward\nreducing operator dependency and improving the reliability of catheterbased\ntherapies.", "AI": {"tldr": "本文提出了DINO-CVA，一个多模态、目标条件行为克隆框架，旨在实现导管的自主导航，以减少操作员依赖并提高介入手术的可靠性。", "motivation": "心脏导管手术高度依赖手动操作，现有机器人平台缺乏智能自主性，导致操作员疲劳、辐射暴露增加以及手术结果的不确定性。", "method": "引入了DINO-CVA模型，它将视觉观测和操纵杆运动学数据融合到一个联合嵌入空间。该模型通过专家演示自回归地预测动作，并利用目标条件引导导管导航。研究团队设计了一个带有合成血管模型的机器人实验装置，用于收集多模态数据集并评估模型性能。", "result": "DINO-CVA在动作预测方面取得了高精度，其性能与仅基于运动学的基线模型相当，同时还能将预测结果与解剖环境相结合。", "conclusion": "这些发现证实了多模态、目标条件架构在导管导航中的可行性，代表着向减少操作员依赖和提高导管介入治疗可靠性迈出了重要一步。"}}
{"id": "2510.16492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16492", "abs": "https://arxiv.org/abs/2510.16492", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "comment": "Reliable ML and Regulatable ML workshops, Neurips 2025", "summary": "As Large Language Model (LLM) agents increasingly operate in complex\nenvironments with real-world consequences, their safety becomes critical. While\nuncertainty quantification is well-studied for single-turn tasks, multi-turn\nagentic scenarios with real-world tool access present unique challenges where\nuncertainties and ambiguities compound, leading to severe or catastrophic risks\nbeyond traditional text generation failures. We propose using \"quitting\" as a\nsimple yet effective behavioral mechanism for LLM agents to recognize and\nwithdraw from situations where they lack confidence. Leveraging the ToolEmu\nframework, we conduct a systematic evaluation of quitting behavior across 12\nstate-of-the-art LLMs. Our results demonstrate a highly favorable\nsafety-helpfulness trade-off: agents prompted to quit with explicit\ninstructions improve safety by an average of +0.39 on a 0-3 scale across all\nmodels (+0.64 for proprietary models), while maintaining a negligible average\ndecrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding\nexplicit quit instructions proves to be a highly effective safety mechanism\nthat can immediately be deployed in existing agent systems, and establishes\nquitting as an effective first-line defense mechanism for autonomous agents in\nhigh-stakes applications.", "AI": {"tldr": "为应对大型语言模型（LLM）代理在复杂真实世界环境中操作时的安全挑战，本研究提出并评估了一种简单的“退出”机制，即当代理缺乏信心时主动退出任务，结果显示这种机制能显著提升安全性，同时对实用性影响甚微。", "motivation": "随着LLM代理在复杂且具有真实世界后果的环境中运行，其安全性变得至关重要。传统的单轮任务不确定性量化方法不足以应对多轮代理场景中不确定性和模糊性复合带来的严重风险，这些风险远超传统文本生成失败。", "method": "研究提出将“退出”作为LLM代理识别并撤离缺乏信心情况的简单有效行为机制。利用ToolEmu框架，对12个最先进的LLM进行了退出行为的系统评估，通过明确指令提示代理在不确定时退出。", "result": "结果表明，退出行为在安全性-实用性之间取得了非常有利的权衡：通过明确的退出指令，代理的安全性平均提高了+0.39（0-3分制，专有模型提高+0.64），而实用性平均仅轻微下降-0.03。", "conclusion": "简单地添加明确的退出指令被证明是一种高效的安全机制，可以立即部署到现有代理系统中。这确立了“退出”作为高风险应用中自主代理有效的第一道防线。"}}
{"id": "2510.17111", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17111", "abs": "https://arxiv.org/abs/2510.17111", "authors": ["Weifan Guan", "Qinghao Hu", "Aosheng Li", "Jian Cheng"], "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "comment": null, "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence.", "AI": {"tldr": "本文综述了提高视觉-语言-动作（VLA）模型效率的方法，以应对其在边缘平台上面临的计算和内存挑战。", "motivation": "VLA模型在具身控制方面能力强大，但其巨大的计算和内存需求与边缘平台（如移动机械臂）的实时性能要求相冲突。解决这一矛盾是当前研究的重点。", "method": "本文对现有提高VLA效率的方法进行了系统性综述，重点关注降低延迟、内存占用以及训练和推理成本。作者将现有解决方案分为模型架构、感知特征、动作生成和训练/推理策略四个维度，并总结了每个类别中的代表性技术。", "result": "本文系统地分类并总结了在模型架构、感知特征、动作生成和训练/推理策略方面用于提高VLA效率的代表性技术。", "conclusion": "未来VLA系统需要进一步关注效率和可扩展性，通过解决当前挑战并探索新的研究方向，以推进高效具身智能的发展。"}}
{"id": "2510.16295", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16295", "abs": "https://arxiv.org/abs/2510.16295", "authors": ["Ryoto Miyamoto", "Xin Fan", "Fuyuko Kido", "Tsuneo Matsumoto", "Hayato Yamana"], "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models", "comment": null, "summary": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in\nevaluating membership inference attacks (MIA) against large vision-language\nmodels (LVLMs). While prior work has reported high attack success rates, our\nanalysis suggests that these results often arise from detecting distributional\nbias introduced during dataset construction rather than from identifying true\nmembership status. To address this issue, we introduce a controlled benchmark\nof 6{,}000 images where the distributions of member and non-member samples are\ncarefully balanced, and ground-truth membership labels are provided across\nthree distinct training stages. Experiments using OpenLVLM-MIA demonstrated\nthat the performance of state-of-the-art MIA methods converged to random chance\nunder unbiased conditions. By offering a transparent and unbiased benchmark,\nOpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and\nprovides a solid foundation for developing stronger privacy-preserving\ntechniques.", "AI": {"tldr": "OpenLVLM-MIA是一个新的基准，用于解决大型视觉语言模型（LVLMs）上成员推断攻击（MIA）评估中的基本挑战。它揭示了现有MIA方法在无偏数据集上表现与随机猜测无异，表明之前的高成功率是由于数据分布偏差而非真实成员识别。", "motivation": "先前的研究报告了对LVLMs进行MIA时的高攻击成功率，但作者认为这些结果往往是由于数据集构建过程中引入的分布偏差，而非真正识别了成员身份。因此，需要一个受控且无偏的基准来准确评估MIA。", "method": "引入了一个名为OpenLVLM-MIA的受控基准，包含6000张图像。该基准精心平衡了成员和非成员样本的分布，并提供了跨三个不同训练阶段的真实成员标签。", "result": "使用OpenLVLM-MIA进行的实验表明，在无偏条件下，最先进的MIA方法的性能收敛到随机猜测水平。", "conclusion": "OpenLVLM-MIA通过提供一个透明和无偏的基准，阐明了当前LVLM上MIA研究的局限性，并为开发更强大的隐私保护技术奠定了坚实基础。"}}
{"id": "2510.17086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17086", "abs": "https://arxiv.org/abs/2510.17086", "authors": ["Xueqian Bai", "Nicklas Hansen", "Adabhav Singh", "Michael T. Tolley", "Yan Duan", "Pieter Abbeel", "Xiaolong Wang", "Sha Yi"], "title": "Learning to Design Soft Hands using Reward Models", "comment": null, "summary": "Soft robotic hands promise to provide compliant and safe interaction with\nobjects and environments. However, designing soft hands to be both compliant\nand functional across diverse use cases remains challenging. Although co-design\nof hardware and control better couples morphology to behavior, the resulting\nsearch space is high-dimensional, and even simulation-based evaluation is\ncomputationally expensive. In this paper, we propose a Cross-Entropy Method\nwith Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven\nsoft robotic hands based on teleoperation control policy, reducing design\nevaluations by more than half compared to pure optimization while learning a\ndistribution of optimized hand designs from pre-collected teleoperation data.\nWe derive a design space for a soft robotic hand composed of flexural soft\nfingers and implement parallelized training in simulation. The optimized hands\nare then 3D-printed and deployed in the real world using both teleoperation\ndata and real-time teleoperation. Experiments in both simulation and hardware\ndemonstrate that our optimized design significantly outperforms baseline hands\nin grasping success rates across a diverse set of challenging objects.", "AI": {"tldr": "本文提出了一种基于奖励模型的交叉熵方法（CEM-RM），用于高效优化肌腱驱动的软体机器人手，利用遥操作数据将设计评估减少一半以上，并显著提高了抓取成功率。", "motivation": "软体机器人手虽然能提供柔顺和安全的交互，但设计出既柔顺又能在多样化场景中实用的软体手仍具挑战。硬件和控制的协同设计虽然能更好地结合形态与行为，但其搜索空间维度高，即使基于仿真的评估也计算成本高昂。", "method": "本文提出了一种基于奖励模型的交叉熵方法（CEM-RM）框架，用于高效优化基于遥操作控制策略的肌腱驱动软体机器人手。该方法通过预收集的遥操作数据学习优化手设计的分布，将设计评估减少一半以上。研究推导了由弯曲软手指组成的软体手设计空间，并在仿真中实现了并行训练。优化后的手被3D打印并部署到真实世界中，使用遥操作数据和实时遥操作进行测试。", "result": "与纯优化方法相比，CEM-RM框架将设计评估减少了一半以上，并能从预收集的遥操作数据中学习优化手设计的分布。在仿真和硬件实验中，优化后的手在抓取各种具有挑战性物体时的成功率显著优于基线手。", "conclusion": "CEM-RM框架能够高效优化肌腱驱动的软体机器人手设计，利用遥操作数据显著减少了设计评估成本，并实现了在多样化抓取任务中表现出色的柔顺且功能强大的软体手。"}}
{"id": "2510.16499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16499", "abs": "https://arxiv.org/abs/2510.16499", "authors": ["Michelle Yuan", "Khushbu Pahwa", "Shuaichen Chang", "Mustafa Kaba", "Jiarong Jiang", "Xiaofei Ma", "Yi Zhang", "Monica Sunkara"], "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection", "comment": "Accepted to NeurIPS 2025 Conference", "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.", "AI": {"tldr": "本文提出了一种受背包问题启发的结构化自动化框架，用于智能体系统组合。该框架通过动态测试和实时建模组件效用，在考虑性能、预算和兼容性的前提下，系统地选择和组装最优智能体组件，显著提高了成功率并降低了成本。", "motivation": "现有的智能体系统组合方法主要依赖静态、语义检索进行工具或智能体发现，但由于能力描述不完整和检索方法的局限性，导致有效复用和组合现有组件面临挑战。组件选择未基于能力、成本和实时效用，从而影响了决策。", "method": "引入了一个受背包问题启发的结构化自动化框架。该框架使一个组合器智能体能够通过联合考虑性能、预算约束和兼容性，系统地识别、选择和组装一组最优的智能体组件。通过动态测试候选组件并实时建模其效用，实现了智能体系统的流线型组装和资源的可扩展复用。", "result": "在五个基准数据集上使用Claude 3.5 Sonnet进行实证评估显示，该在线背包组合器始终位于帕累托前沿，与基线相比，以显著更低的组件成本实现了更高的成功率。在单智能体设置中，成功率比检索基线提高了高达31.6%。在多智能体系统中，当从100多个智能体库存中选择智能体时，成功率从37%提高到87%。", "conclusion": "该方法在不同领域和预算约束下展现出强大的适应性，显著的性能差距证实了其鲁棒性，有效解决了智能体系统组合中的挑战，实现了资源的高效利用和性能优化。"}}
{"id": "2510.16319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16319", "abs": "https://arxiv.org/abs/2510.16319", "authors": ["Rui Yang", "Huining Li", "Yiyi Long", "Xiaojun Wu", "Shengfeng He"], "title": "Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation", "comment": "ICCV 2025", "summary": "Generating sketches guided by reference styles requires precise transfer of\nstroke attributes, such as line thickness, deformation, and texture sparsity,\nwhile preserving semantic structure and content fidelity. To this end, we\npropose Stroke2Sketch, a novel training-free framework that introduces\ncross-image stroke attention, a mechanism embedded within self-attention layers\nto establish fine-grained semantic correspondences and enable accurate stroke\nattribute transfer. This allows our method to adaptively integrate reference\nstroke characteristics into content images while maintaining structural\nintegrity. Additionally, we develop adaptive contrast enhancement and\nsemantic-focused attention to reinforce content preservation and foreground\nemphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches\nthat closely resemble handcrafted results, outperforming existing methods in\nexpressive stroke control and semantic coherence. Codes are available at\nhttps://github.com/rane7/Stroke2Sketch.", "AI": {"tldr": "Stroke2Sketch是一个免训练的框架，通过引入跨图像笔触注意力机制，实现参考风格引导的素描生成，能够精确转移笔触属性并保持内容结构。", "motivation": "在生成由参考风格引导的素描时，需要精确转移笔触属性（如线条粗细、变形、纹理稀疏度），同时保持语义结构和内容保真度，这是一个挑战。", "method": "本文提出了Stroke2Sketch，一个免训练的框架。核心方法是引入了“跨图像笔触注意力”（cross-image stroke attention），该机制嵌入在自注意力层中，用于建立细粒度语义对应并实现精确的笔触属性转移。此外，还开发了自适应对比度增强和语义聚焦注意力来强化内容保留和前景强调。", "result": "Stroke2Sketch能够有效地合成风格忠实、与手绘结果高度相似的素描。在富有表现力的笔触控制和语义连贯性方面，其性能优于现有方法。", "conclusion": "Stroke2Sketch提供了一种新颖且有效的免训练方法，用于生成风格引导的素描，它通过精确的笔触属性转移和内容结构保持，实现了高质量的素描合成。"}}
{"id": "2510.16549", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16549", "abs": "https://arxiv.org/abs/2510.16549", "authors": ["Haoxuan Zhang", "Ruochi Li", "Sarthak Shrestha", "Shree Harshini Mamidala", "Revanth Putta", "Arka Krishan Aggarwal", "Ting Xiao", "Junhua Ding", "Haihua Chen"], "title": "ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation", "comment": null, "summary": "Peer review serves as the gatekeeper of science, yet the surge in submissions\nand widespread adoption of large language models (LLMs) in scholarly evaluation\npresent unprecedented challenges. Recent work has focused on using LLMs to\nimprove review efficiency or generate insightful review content. However,\nunchecked deficient reviews from both human experts and AI systems threaten to\nsystematically undermine the peer review ecosystem and compromise academic\nintegrity. To address this critical issue, we introduce ReviewGuard, an\nautomated system for detecting and categorizing deficient reviews. ReviewGuard\nemploys a comprehensive four-stage LLM-driven framework that: (1) collects ICLR\nand NeurIPS papers with their corresponding reviews from OpenReview; (2)\nannotates review types using GPT-4.1 with human validation; (3) addresses class\nimbalance and data scarcity through LLM-driven synthetic data augmentation,\nproducing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438\nsynthetic reviews; and (4) fine-tunes both encoder-based models and open source\nLLMs. We perform comprehensive feature analysis of the structure and quality of\nthe review text. Compared to sufficient reviews, deficient reviews demonstrate\nlower rating scores, higher self-reported confidence, reduced structural\ncomplexity, and a higher proportion of negative sentiment. AI-generated text\ndetection reveals that, since ChatGPT's emergence, AI-generated reviews have\nincreased dramatically. In the evaluation of deficient review detection models,\nmixed training with synthetic and real review data provides substantial\nenhancements to recall and F1 scores on the binary task. This study presents\nthe first LLM-driven system for detecting deficient peer reviews, providing\nevidence to inform AI governance in peer review while offering valuable\ninsights into human-AI collaboration to maintain academic integrity.", "AI": {"tldr": "本文介绍了ReviewGuard，一个基于LLM的自动化系统，用于检测和分类同行评审中的缺陷评论。该系统通过四阶段框架处理数据、增强数据并微调模型，揭示了缺陷评论的特征，并发现AI生成评论显著增加，提出混合训练能有效提升检测性能。", "motivation": "同行评审面临提交量激增和LLM广泛应用带来的挑战，导致人类专家和AI系统可能产生有缺陷的评论，从而威胁同行评审生态系统和学术诚信。现有工作主要关注LLM提升评审效率或生成内容，但未解决缺陷评论的系统性问题。", "method": "引入ReviewGuard系统，采用四阶段LLM驱动框架：1) 从OpenReview收集ICLR和NeurIPS论文及其评论；2) 使用GPT-4.1和人工验证标注评论类型；3) 通过LLM驱动的合成数据增强解决类别不平衡和数据稀缺问题，构建包含6,634篇论文、24,657条真实评论和46,438条合成评论的语料库；4) 微调编码器模型和开源LLM进行检测。", "result": "缺陷评论表现出较低的评分、较高的自我报告信心、较低的结构复杂性和较高的负面情绪比例。ChatGPT出现后，AI生成评论显著增加。在缺陷评论检测模型评估中，使用合成和真实评论数据进行混合训练，在二分类任务上显著提升了召回率和F1分数。", "conclusion": "本研究首次提出了一个LLM驱动的缺陷同行评审检测系统，为同行评审中的AI治理提供了证据，并为维护学术诚信的人机协作提供了宝贵见解。"}}
{"id": "2510.16565", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16565", "abs": "https://arxiv.org/abs/2510.16565", "authors": ["Seungho Cho", "Changgeon Ko", "Eui Jun Hwang", "Junmyeong Lee", "Huije Lee", "Jong C. Park"], "title": "Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models", "comment": "Accepted to CIKM 2025 Workshop on Human Centric AI", "summary": "Large language models (LLMs) are increasingly used across diverse cultural\ncontexts, making accurate cultural understanding essential. Prior evaluations\nhave mostly focused on output-level performance, obscuring the factors that\ndrive differences in responses, while studies using circuit analysis have\ncovered few languages and rarely focused on culture. In this work, we trace\nLLMs' internal cultural understanding mechanisms by measuring activation path\noverlaps when answering semantically equivalent questions under two conditions:\nvarying the target country while fixing the question language, and varying the\nquestion language while fixing the country. We also use same-language country\npairs to disentangle language from cultural aspects. Results show that internal\npaths overlap more for same-language, cross-country questions than for\ncross-language, same-country questions, indicating strong language-specific\npatterns. Notably, the South Korea-North Korea pair exhibits low overlap and\nhigh variability, showing that linguistic similarity does not guarantee aligned\ninternal representation.", "AI": {"tldr": "本研究通过测量激活路径重叠来追踪大型语言模型（LLMs）的内部文化理解机制，发现其内部表示受语言影响显著，且语言相似性不保证文化理解的一致性。", "motivation": "以往对LLMs的评估主要集中在输出层面，未能揭示响应差异的内部驱动因素，且电路分析研究很少关注文化和多语言背景。因此，需要深入探究LLMs的内部文化理解机制。", "method": "通过在两种条件下测量语义等效问题的激活路径重叠：1) 固定问题语言，改变目标国家；2) 固定国家，改变问题语言。同时，使用相同语言的国家对来区分语言和文化因素。", "result": "结果显示，相同语言但不同国家的问题比不同语言但相同国家的问题，其内部路径重叠度更高，这表明模型存在很强的语言特异性模式。值得注意的是，韩国-朝鲜这对国家表现出低重叠度和高变异性，说明语言相似性并不能保证内部表示的一致性。", "conclusion": "LLMs的内部文化理解机制表现出强烈的语言特异性模式。语言上的相似性并不能保证模型在内部表示上对相关文化的理解是一致或对齐的。"}}
{"id": "2510.17143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17143", "abs": "https://arxiv.org/abs/2510.17143", "authors": ["Shantnav Agarwal", "Javier Alonso-Mora", "Sihao Sun"], "title": "Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning", "comment": "Accepted by IEEE MRS 2025", "summary": "Existing approaches for transporting and manipulating cable-suspended loads\nusing multiple UAVs along reference trajectories typically rely on either\ncentralized control architectures or reliable inter-agent communication. In\nthis work, we propose a novel machine learning based method for decentralized\nkinodynamic planning that operates effectively under partial observability and\nwithout inter-agent communication. Our method leverages imitation learning to\ntrain a decentralized student policy for each UAV by imitating a centralized\nkinodynamic motion planner with access to privileged global observations. The\nstudent policy generates smooth trajectories using physics-informed neural\nnetworks that respect the derivative relationships in motion. During training,\nthe student policies utilize the full trajectory generated by the teacher\npolicy, leading to improved sample efficiency. Moreover, each student policy\ncan be trained in under two hours on a standard laptop. We validate our method\nin both simulation and real-world environments to follow an agile reference\ntrajectory, demonstrating performance comparable to that of centralized\napproaches.", "AI": {"tldr": "本文提出了一种新颖的基于机器学习的去中心化运动规划方法，用于多无人机悬索负载运输，该方法在部分可观测和无智能体间通信的情况下也能有效运作。", "motivation": "现有用于多无人机悬索负载运输的方法通常依赖于集中式控制架构或可靠的智能体间通信，这限制了其应用范围。", "method": "该方法利用模仿学习，为每个无人机训练一个去中心化的学生策略。学生策略通过模仿一个拥有特权全局观测的集中式运动规划器来生成平滑轨迹，并使用物理信息神经网络来确保轨迹符合运动学关系。训练过程中利用教师策略的完整轨迹以提高样本效率。", "result": "该方法在部分可观测和无智能体间通信的条件下表现出色，其性能与集中式方法相当。每个学生策略的训练时间在标准笔记本电脑上不到两小时。该方法已在仿真和真实世界环境中得到验证。", "conclusion": "研究提出了一种高效、去中心化的机器学习方法，解决了多无人机悬索负载运输中集中控制和通信依赖的挑战，并在实际应用中展现出与集中式方法媲美的性能。"}}
{"id": "2510.17148", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17148", "abs": "https://arxiv.org/abs/2510.17148", "authors": ["Yu Gao", "Yiru Wang", "Anqing Jiang", "Heng Yuwen", "Wang Shuo", "Sun Hao", "Wang Jijun"], "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment", "comment": null, "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.", "AI": {"tldr": "DiffVLA++是一个增强型自动驾驶框架，通过度量引导对齐，显式地连接认知推理（VLA）和端到端规划（E2E），以结合两者的优势，解决各自的局限性。", "motivation": "传统的端到端驾驶模型在长尾场景中泛化能力差，因为它们缺乏理解和推理环境所需的世界知识。而视觉-语言-动作（VLA）模型虽然利用世界知识处理复杂情况，但其有限的3D推理能力可能导致物理上不可行的动作。", "method": "该方法引入了DiffVLA++。首先，构建了一个直接生成语义驱动轨迹的VLA模块。其次，设计了一个具有密集轨迹词汇表的E2E模块，以确保物理可行性。最关键的是，引入了一个度量引导的轨迹评分器，用于引导和对齐VLA和E2E模块的输出，从而整合它们的互补优势。", "result": "在ICCV 2025自动驾驶大挑战排行榜上的实验表明，DiffVLA++实现了49.12的EPDMS（一个性能指标）。", "conclusion": "DiffVLA++通过度量引导对齐，成功地将认知推理与端到端规划相结合，有效整合了VLA和E2E模型的互补优势，提升了自动驾驶的性能。"}}
{"id": "2510.16320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16320", "abs": "https://arxiv.org/abs/2510.16320", "authors": ["Wenhao Wang", "Longqi Cai", "Taihong Xiao", "Yuxiao Wang", "Ming-Hsuan Yang"], "title": "Scaling Laws for Deepfake Detection", "comment": null, "summary": "This paper presents a systematic study of scaling laws for the deepfake\ndetection task. Specifically, we analyze the model performance against the\nnumber of real image domains, deepfake generation methods, and training images.\nSince no existing dataset meets the scale requirements for this research, we\nconstruct ScaleDF, the largest dataset to date in this field, which contains\nover 5.8 million real images from 51 different datasets (domains) and more than\n8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we\nobserve power-law scaling similar to that shown in large language models\n(LLMs). Specifically, the average detection error follows a predictable\npower-law decay as either the number of real domains or the number of deepfake\nmethods increases. This key observation not only allows us to forecast the\nnumber of additional real domains or deepfake methods required to reach a\ntarget performance, but also inspires us to counter the evolving deepfake\ntechnology in a data-centric manner. Beyond this, we examine the role of\npre-training and data augmentations in deepfake detection under scaling, as\nwell as the limitations of scaling itself.", "AI": {"tldr": "本研究系统性地探讨了深度伪造检测任务的扩展法则，发现检测错误率随真实图像域或伪造方法数量的增加呈幂律衰减，并构建了大规模数据集ScaleDF来支持这一发现。", "motivation": "现有数据集无法满足大规模研究需求，作者旨在系统性分析深度伪造检测模型性能如何随真实图像域数量、深度伪造生成方法数量以及训练图像数量的变化而扩展。", "method": "1. 构建了迄今为止最大的深度伪造检测数据集ScaleDF，包含超过580万张来自51个不同数据集（域）的真实图像和超过880万张由102种深度伪造方法生成的伪造图像。 2. 利用ScaleDF数据集，系统分析了模型性能与真实图像域数量、深度伪造生成方法数量以及训练图像数量之间的关系。 3. 考察了预训练和数据增强在扩展条件下的作用，以及扩展本身的局限性。", "result": "1. 观察到与大型语言模型（LLMs）相似的幂律扩展现象。 2. 平均检测错误率随着真实域数量或深度伪造方法数量的增加，呈现可预测的幂律衰减。", "conclusion": "1. 幂律扩展的发现使得预测达到目标性能所需的额外真实域或深度伪造方法数量成为可能。 2. 启发了以数据为中心的方法来对抗不断演进的深度伪造技术。 3. 探讨了扩展的局限性以及预训练和数据增强在深度伪造检测中的作用。"}}
{"id": "2510.16326", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16326", "abs": "https://arxiv.org/abs/2510.16326", "authors": ["Yi Wei", "Shunpu Tang", "Liang Zhao", "Qiangian Yang"], "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution", "comment": null, "summary": "Recent advances in diffusion models have driven remarkable progress in image\ngeneration. However, the generation process remains computationally intensive,\nand users often need to iteratively refine prompts to achieve the desired\nresults, further increasing latency and placing a heavy burden on cloud\nresources. To address this challenge, we propose DiffusionX, a cloud-edge\ncollaborative framework for efficient multi-round, prompt-based generation. In\nthis system, a lightweight on-device diffusion model interacts with users by\nrapidly producing preview images, while a high-capacity cloud model performs\nfinal refinements after the prompt is finalized. We further introduce a noise\nlevel predictor that dynamically balances the computation load, optimizing the\ntrade-off between latency and cloud workload. Experiments show that DiffusionX\nreduces average generation time by 15.8% compared with Stable Diffusion v1.5,\nwhile maintaining comparable image quality. Moreover, it is only 0.9% slower\nthan Tiny-SD with significantly improved image quality, thereby demonstrating\nefficiency and scalability with minimal overhead.", "AI": {"tldr": "DiffusionX是一个云边协同框架，通过轻量级设备端模型快速生成预览图和云端模型进行最终精修，以提高多轮、基于提示词的图像生成效率并降低延迟。", "motivation": "扩散模型生成过程计算密集，且用户迭代优化提示词会增加延迟和云资源负担。", "method": "DiffusionX采用云边协同框架：轻量级设备端扩散模型快速生成预览图与用户交互；高容量云端模型在提示词确定后进行最终精修。引入噪声水平预测器动态平衡计算负载，优化延迟与云工作量之间的权衡。", "result": "与Stable Diffusion v1.5相比，DiffusionX平均生成时间减少15.8%，同时保持可比的图像质量。与Tiny-SD相比，速度仅慢0.9%，但图像质量显著提升。", "conclusion": "DiffusionX在多轮、基于提示词的图像生成中展现出效率和可扩展性，且开销极小。"}}
{"id": "2510.16325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16325", "abs": "https://arxiv.org/abs/2510.16325", "authors": ["Yuyao Zhang", "Yu-Wing Tai"], "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention", "comment": "22 pages", "summary": "Ultra-high-resolution text-to-image generation demands both fine-grained\ntexture synthesis and globally coherent structure, yet current diffusion models\nremain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive\nquadratic complexity of attention and the scarcity of native $4K$ training\ndata. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces\nhierarchical local attention with low-resolution global guidance, enabling\nefficient, scalable, and semantically coherent image synthesis at ultra-high\nresolutions. Specifically, high-resolution latents are divided into fixed-size\nlocal windows to reduce attention complexity from quadratic to near-linear,\nwhile a low-resolution latent equipped with scaled positional anchors injects\nglobal semantics. A lightweight LoRA adaptation bridges global and local\npathways during denoising, ensuring consistency across structure and detail. To\nmaximize inference efficiency, we repermute token sequence in Hilbert curve\norder and implement a fused-kernel for skipping masked operations, resulting in\na GPU-friendly design. Extensive experiments demonstrate that Scale-DiT\nachieves more than $2\\times$ faster inference and lower memory usage compared\nto dense attention baselines, while reliably scaling to $4K \\times 4K$\nresolution without requiring additional high-resolution training data. On both\nquantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,\nScale-DiT delivers superior global coherence and sharper local detail, matching\nor outperforming state-of-the-art methods that rely on native 4K training.\nTaken together, these results highlight hierarchical local attention with\nguided low-resolution anchors as a promising and effective approach for\nadvancing ultra-high-resolution image generation.", "AI": {"tldr": "Scale-DiT是一种新的扩散模型框架，通过分层局部注意力结合低分辨率全局指导，实现了高效、可扩展且语义一致的超高分辨率（例如4Kx4K）文本到图像生成，无需额外的4K训练数据。", "motivation": "当前的扩散模型在生成超高分辨率（高于1Kx1K）图像时面临挑战，原因在于注意力的二次复杂度导致计算成本过高，以及原生4K训练数据的稀缺性。", "method": "Scale-DiT引入了分层局部注意力，将高分辨率潜在空间划分为固定大小的局部窗口，将注意力复杂度从二次降低到接近线性。同时，一个带有缩放位置锚点的低分辨率潜在空间提供全局语义指导。轻量级LoRA适配器用于在去噪过程中连接全局和局部路径，确保结构和细节的一致性。为提高推理效率，模型采用希尔伯特曲线顺序重排token序列，并实现了一个融合核以跳过掩码操作。", "result": "实验表明，Scale-DiT的推理速度比密集注意力基线快两倍以上，内存使用率更低，并且能够可靠地扩展到4Kx4K分辨率，而无需额外的4K高分辨率训练数据。在定量基准（FID、IS、CLIP Score）和定性比较中，Scale-DiT提供了卓越的全局连贯性和更清晰的局部细节，与依赖原生4K训练的最新方法相当或更优。", "conclusion": "分层局部注意力与引导式低分辨率锚点相结合，是推进超高分辨率图像生成的一种有前途且有效的方法。"}}
{"id": "2510.16573", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16573", "abs": "https://arxiv.org/abs/2510.16573", "authors": ["Muhammad Ammar", "Hadiya Murad Hadi", "Usman Majeed Butt"], "title": "AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu", "comment": null, "summary": "Large Language Models (LLMs) are now capable of generating text that closely\nresembles human writing, making them powerful tools for content creation, but\nthis growing ability has also made it harder to tell whether a piece of text\nwas written by a human or by a machine. This challenge becomes even more\nserious for languages like Urdu, where there are very few tools available to\ndetect AI-generated text. To address this gap, we propose a novel AI-generated\ntext detection framework tailored for the Urdu language. A balanced dataset\ncomprising 1,800 humans authored, and 1,800 AI generated texts, sourced from\nmodels such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed\nlinguistic and statistical analysis was conducted, focusing on features such as\ncharacter and word counts, vocabulary richness (Type Token Ratio), and N-gram\npatterns, with significance evaluated through t-tests and MannWhitney U tests.\nThree state-of-the-art multilingual transformer models such as\nmdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were\nfine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest\nperformance, with an F1-score 91.29 and accuracy of 91.26% on the test set.\nThis research advances efforts in contesting misinformation and academic\nmisconduct in Urdu-speaking communities and contributes to the broader\ndevelopment of NLP tools for low resource languages.", "AI": {"tldr": "本文提出了一种针对乌尔都语的AI生成文本检测框架，通过构建平衡数据集和微调多语言Transformer模型，实现了高准确率的检测。", "motivation": "大型语言模型（LLMs）生成的文本与人类写作越来越相似，难以区分，尤其是在乌尔都语等资源匮乏的语言中，缺乏相应的AI文本检测工具。", "method": "研究构建了一个包含1,800篇人类创作和1,800篇AI生成（来自Gemini、GPT-4o-mini、Kimi AI）文本的平衡乌尔都语数据集。进行了语言学和统计学分析，包括字符/词计数、词汇丰富度（Type Token Ratio）和N-gram模式，并通过t检验和MannWhitney U检验评估显著性。微调了mdeberta-v3-base、distilbert-base-multilingualcased和xlm-roberta-base三种先进的多语言Transformer模型。", "result": "mDeBERTa-v3-base模型表现最佳，在测试集上F1-score达到91.29，准确率为91.26%。", "conclusion": "这项研究有助于在乌尔都语社区中打击虚假信息和学术不端行为，并促进了低资源语言NLP工具的进一步发展。"}}
{"id": "2510.17150", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17150", "abs": "https://arxiv.org/abs/2510.17150", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Gokhan Solak", "Arash Ajoudani"], "title": "OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation", "comment": "Code, video and RAG dataset are available at\n  \\url{https://sites.google.com/view/omni-vic}", "summary": "We present OmniVIC, a universal variable impedance controller (VIC) enhanced\nby a vision language model (VLM), which improves safety and adaptation in any\ncontact-rich robotic manipulation task to enhance safe physical interaction.\nTraditional VIC have shown advantages when the robot physically interacts with\nthe environment, but lack generalization in unseen, complex, and unstructured\nsafe interactions in universal task scenarios involving contact or uncertainty.\nTo this end, the proposed OmniVIC interprets task context derived reasoning\nfrom images and natural language and generates adaptive impedance parameters\nfor a VIC controller. Specifically, the core of OmniVIC is a self-improving\nRetrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG\nretrieves relevant prior experiences from a structured memory bank to inform\nthe controller about similar past tasks, and ICL leverages these retrieved\nexamples and the prompt of current task to query the VLM for generating\ncontext-aware and adaptive impedance parameters for the current manipulation\nscenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in\nuniversal task scenarios. The impedance parameter regulation is further\ninformed by real-time force/torque feedback to ensure interaction forces remain\nwithin safe thresholds. We demonstrate that our method outperforms baselines on\na suite of complex contact-rich tasks, both in simulation and on real-world\nrobotic tasks, with improved success rates and reduced force violations.\nOmniVIC takes a step towards bridging high-level semantic reasoning and\nlow-level compliant control, enabling safer and more generalizable\nmanipulation. Overall, the average success rate increases from 27% (baseline)\nto 61.4% (OmniVIC).", "AI": {"tldr": "OmniVIC是一种通用可变阻抗控制器，通过视觉语言模型（VLM）增强，利用自改进的RAG和ICL机制，在接触密集型机器人操作任务中实现更安全、更具适应性的物理交互。", "motivation": "传统的变阻抗控制器（VIC）在机器人与环境物理交互时表现出优势，但在涉及接触或不确定性的通用任务场景中，面对未知、复杂和非结构化的安全交互时，缺乏泛化能力。", "method": "OmniVIC通过VLM从图像和自然语言中解释任务上下文，并生成自适应的阻抗参数。其核心是自改进的检索增强生成（RAG）和上下文学习（ICL），其中RAG从结构化记忆库中检索相关经验，ICL利用这些经验和当前任务提示来查询VLM，生成与上下文相关的自适应阻抗参数。实时力/扭矩反馈进一步调节参数，以确保交互力在安全阈值内。", "result": "OmniVIC在模拟和真实世界的复杂接触密集型任务中均优于基线方法，成功率更高，力违规更少。平均成功率从基线的27%提高到OmniVIC的61.4%。", "conclusion": "OmniVIC是连接高级语义推理和低级柔顺控制的重要一步，实现了更安全、更具泛化能力的机器人操作。"}}
{"id": "2510.16567", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16567", "abs": "https://arxiv.org/abs/2510.16567", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Manuel Giollo", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "Hallucination Benchmark for Speech Foundation Models", "comment": "Under Review", "summary": "Hallucinations in automatic speech recognition (ASR) systems refer to fluent\nand coherent transcriptions produced by neural ASR models that are completely\nunrelated to the underlying acoustic input (i.e., the speech signal). While\nsimilar to conventional decoding errors in potentially compromising the\nusability of transcriptions for downstream applications, hallucinations can be\nmore detrimental due to their preservation of syntactically and semantically\nplausible structure. This apparent coherence can mislead subsequent processing\nstages and introduce serious risks, particularly in critical domains such as\nhealthcare and law. Conventional evaluation metrics are primarily centered on\nerror-based metrics and fail to distinguish between phonetic inaccuracies and\nhallucinations. Consequently, there is a critical need for new evaluation\nframeworks that can effectively identify and assess models with a heightened\npropensity for generating hallucinated content. To this end, we introduce\nSHALLOW, the first benchmark framework that systematically categorizes and\nquantifies hallucination phenomena in ASR along four complementary axes:\nlexical, phonetic, morphological, and semantic. We define targeted metrics\nwithin each category to produce interpretable profiles of model behavior.\nThrough evaluation across various architectures and speech domains, we have\nfound that SHALLOW metrics correlate strongly with word error rate (WER) when\nrecognition quality is high (i.e., low WER). Still, this correlation weakens\nsubstantially as WER increases. SHALLOW, therefore, captures fine-grained error\npatterns that WER fails to distinguish under degraded and challenging\nconditions. Our framework supports specific diagnosis of model weaknesses and\nprovides feedback for model improvement beyond what aggregate error rates can\noffer.", "AI": {"tldr": "该研究引入了SHALLOW，一个用于系统性分类和量化自动语音识别（ASR）系统中幻觉现象的新基准框架，并通过四个维度（词汇、语音、形态和语义）进行评估，以提供超越传统错误率（如WER）的细粒度诊断。", "motivation": "ASR系统中的幻觉是流利且连贯但与声学输入完全无关的转录，比传统解码错误更具危害性，因为其看似合理的结构可能误导后续处理并引入风险。现有评估指标主要关注错误率，无法区分语音不准确和幻觉，因此急需新的评估框架来有效识别和评估易产生幻觉内容的模型。", "method": "引入了SHALLOW，一个基准框架，它沿四个互补轴（词汇、语音、形态和语义）系统地分类和量化ASR中的幻觉现象。在每个类别中定义了有针对性的指标，以生成可解释的模型行为剖面。", "result": "通过对各种架构和语音领域的评估，发现SHALLOW指标在识别质量高（即WER低）时与词错误率（WER）强相关。然而，随着WER的增加，这种相关性显著减弱。SHALLOW因此能够捕捉到在恶劣和挑战性条件下WER无法区分的细粒度错误模式。", "conclusion": "SHALLOW框架支持模型弱点的具体诊断，并提供超越聚合错误率所能提供的模型改进反馈。它尤其在识别质量下降和挑战性条件下，能更有效地捕捉幻觉现象，从而为模型改进提供更深入的洞察。"}}
{"id": "2510.16604", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16604", "abs": "https://arxiv.org/abs/2510.16604", "authors": ["Francisco Jose Cortes Delgado", "Eduardo Martinez Gracia", "Rafael Valencia Garcia"], "title": "Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach", "comment": "6 pages, 3 figures. Submitted to SEPLN 2023 Conference", "summary": "Recent advances in natural language processing with large neural models have\nopened new possibilities for syntactic analysis based on machine learning. This\nwork explores a novel approach to phrase-structure analysis by fine-tuning\nlarge language models (LLMs) to translate an input sentence into its\ncorresponding syntactic structure. The main objective is to extend the\ncapabilities of MiSintaxis, a tool designed for teaching Spanish syntax.\nSeveral models from the Hugging Face repository were fine-tuned using training\ndata generated from the AnCora-ES corpus, and their performance was evaluated\nusing the F1 score. The results demonstrate high accuracy in phrase-structure\nanalysis and highlight the potential of this methodology.", "AI": {"tldr": "本文通过微调大型语言模型（LLMs），将输入句子转换为其句法结构，以实现短语结构分析，并扩展西班牙语句法教学工具MiSintaxis的功能。", "motivation": "随着大型神经网络模型在自然语言处理领域的进步，为基于机器学习的句法分析提供了新机遇。研究旨在探索一种新的短语结构分析方法，并增强现有西班牙语句法教学工具MiSintaxis的功能。", "method": "研究采用新颖的方法，通过微调大型语言模型（LLMs），将输入句子翻译成其对应的句法结构。具体使用了Hugging Face仓库中的多个模型，并利用AnCora-ES语料库生成训练数据进行微调。模型性能通过F1分数进行评估。", "result": "实验结果表明，该方法在短语结构分析方面表现出高准确性。", "conclusion": "该研究证明了利用大型语言模型进行短语结构分析的方法具有巨大潜力，并成功扩展了MiSintaxis工具的能力。"}}
{"id": "2510.16332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16332", "abs": "https://arxiv.org/abs/2510.16332", "authors": ["Haiyue Sun", "Qingdong He", "Jinlong Peng", "Peng Tang", "Jiangning Zhang", "Junwei Zhu", "Xiaobin Hu", "Shuicheng Yan"], "title": "TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement", "comment": null, "summary": "Autoregressive Model (AR) has shown remarkable success in conditional image\ngeneration. However, these approaches for multiple reference generation\nstruggle with decoupling different reference identities. In this work, we\npropose the TokenAR framework, specifically focused on a simple but effective\ntoken-level enhancement mechanism to address reference identity confusion\nproblem. Such token-level enhancement consists of three parts, 1). Token Index\nEmbedding clusters the tokens index for better representing the same reference\nimages; 2). Instruct Token Injection plays as a role of extra visual feature\ncontainer to inject detailed and complementary priors for reference tokens; 3).\nThe identity-token disentanglement strategy (ITD) explicitly guides the token\nrepresentations toward independently representing the features of each\nidentity.This token-enhancement framework significantly augments the\ncapabilities of existing AR based methods in conditional image generation,\nenabling good identity consistency while preserving high quality background\nreconstruction. Driven by the goal of high-quality and high-diversity in\nmulti-subject generation, we introduce the InstructAR Dataset, the first\nopen-source, large-scale, multi-reference input, open domain image generation\ndataset that includes 28K training pairs, each example has two reference\nsubjects, a relative prompt and a background with mask annotation, curated for\nmultiple reference image generation training and evaluating. Comprehensive\nexperiments validate that our approach surpasses current state-of-the-art\nmodels in multiple reference image generation task. The implementation code and\ndatasets will be made publicly. Codes are available, see\nhttps://github.com/lyrig/TokenAR", "AI": {"tldr": "TokenAR框架通过令牌级增强机制解决了自回归模型在多参考图像生成中存在的身份混淆问题，并引入了InstructAR数据集以支持多主体生成。", "motivation": "自回归模型在条件图像生成中表现出色，但在多参考生成中，它们难以解耦不同的参考身份，导致身份混淆问题。", "method": "本文提出了TokenAR框架，专注于简单而有效的令牌级增强机制来解决参考身份混淆问题。该机制包含三部分：1) 令牌索引嵌入（Token Index Embedding）用于聚类令牌索引以更好地表示相同的参考图像；2) 指令令牌注入（Instruct Token Injection）作为额外的视觉特征容器，为参考令牌注入详细和互补的先验信息；3) 身份-令牌解耦策略（ITD）明确引导令牌表示独立地表示每个身份的特征。此外，本文还引入了InstructAR数据集，这是首个开源、大规模、多参考输入、开放域图像生成数据集，用于多参考图像生成训练和评估。", "result": "TokenAR框架显著增强了现有基于AR方法的条件图像生成能力，实现了良好的身份一致性，同时保留了高质量的背景重建。全面的实验验证了该方法在多参考图像生成任务中超越了当前的SOTA模型。", "conclusion": "TokenAR框架通过创新的令牌级增强机制，成功解决了自回归模型在多参考图像生成中的身份混淆难题，并通过引入InstructAR数据集进一步推动了多主体生成领域的发展，实验证明其在身份一致性和图像质量方面均优于现有方法。"}}
{"id": "2510.16645", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16645", "abs": "https://arxiv.org/abs/2510.16645", "authors": ["Zhixuan He", "Yue Feng"], "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance but often lack\ninterpretable reasoning. This paper introduces the Multi-Agent Collaboration\nFramework for Diverse Thinking Modes (DiMo), which enhances both performance\nand interpretability by simulating a structured debate among four specialized\nLLM agents. Each agent embodies a distinct reasoning paradigm, allowing the\nframework to collaboratively explore diverse cognitive approaches. Through\niterative debate, agents challenge and refine initial responses, yielding more\nrobust conclusions and an explicit, auditable reasoning chain. Across six\nbenchmarks and under a unified open-source setup, DiMo improves accuracy over\nwidely used single-model and debate baselines, with the largest gains on math.\nWe position DiMo as a semantics-aware, Web-native multi-agent framework: it\nmodels human-machine intelligence with LLM agents that produce semantically\ntyped, URL-annotated evidence chains for explanations and user-friendly\ninteractions. Although our experiments use standard reasoning benchmarks, the\nframework is designed to be instantiated over Web corpora and knowledge graphs,\ncombining retrieval-augmented reasoning with structured justifications that\ndownstream systems can inspect and reuse.", "AI": {"tldr": "DiMo框架通过模拟四种专业LLM代理的结构化辩论，提升了大型语言模型的性能和可解释性，特别是在数学方面，并生成可审计的推理链。", "motivation": "大型语言模型（LLMs）性能强大但缺乏可解释的推理过程。", "method": "引入了多代理协作框架DiMo，该框架模拟了四个专业LLM代理之间的结构化辩论。每个代理代表一种独特的推理范式，通过迭代辩论挑战和完善初始响应，以探索多样化的认知方法。", "result": "在六个基准测试中，DiMo在统一的开源设置下，比广泛使用的单模型和辩论基线提高了准确性，尤其在数学方面增益最大。它生成了语义感知、带有URL注释的证据链，用于解释和用户交互。", "conclusion": "DiMo通过模拟多样化的思维模式的代理协作，增强了LLM的性能和可解释性，提供了更可靠的结论和可审计的推理链，并被设计为可与Web语料库和知识图谱结合，实现检索增强推理和结构化论证。"}}
{"id": "2510.17191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17191", "abs": "https://arxiv.org/abs/2510.17191", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving", "comment": "6 pages, 2 figures, 2 tables", "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency.", "AI": {"tldr": "本文提出了SimpleVSF框架，通过结合视觉-语言模型（VLM）的认知能力和先进的轨迹融合技术，提升端到端自动驾驶规划的性能，并在ICCV 2025 NAVSIM v2挑战赛中取得了领先的SOTA表现。", "motivation": "现有的端到端自动驾驶方法在复杂场景中面临决策不佳的挑战，需要更鲁棒和智能的驾驶策略。", "method": "SimpleVSF框架利用传统评分器和VLM增强型评分器，并通过鲁棒的权重融合器进行定量聚合，以及强大的基于VLM的融合器进行定性、上下文感知的决策。它结合了VLM的认知能力和先进的轨迹融合技术。", "result": "SimpleVSF框架在ICCV 2025 NAVSIM v2端到端驾驶挑战赛中表现出色，达到了最先进的性能，并在安全性、舒适性和效率之间实现了卓越的平衡。", "conclusion": "SimpleVSF通过整合VLM的认知能力和先进的轨迹融合技术，显著提升了端到端自动驾驶规划的决策质量，实现了领先的性能，为未来智能驾驶策略提供了有力的支持。"}}
{"id": "2510.16333", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16333", "abs": "https://arxiv.org/abs/2510.16333", "authors": ["Junha Song", "Sangdoo Yun", "Dongyoon Han", "Jaegul Choo", "Byeongho Heo"], "title": "RL makes MLLMs see better than SFT", "comment": null, "summary": "A dominant assumption in Multimodal Language Model (MLLM) research is that\nits performance is largely inherited from the LLM backbone, given its immense\nparameter scale and remarkable capabilities. This has created a void in the\nunderstanding of the vision encoder, which determines how MLLMs perceive\nimages. The recent shift in MLLM training paradigms, from Supervised Finetuning\n(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the\nsignificant lack of analysis on how such training reshapes the vision encoder\nas well as the MLLM. To address this, we first investigate the impact of\ntraining strategies on MLLMs, where RL shows a clear advantage over SFT in\nstrongly vision-related VQA benchmarks. Motivated by this, we conduct a\ncritical yet under-explored analysis of the vision encoder of MLLMs through\ndiverse and in-depth experiments, ranging from ImageNet classification and\nsegmentation to gradient visualization. Our results demonstrate that MLLM's\npost-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on\nMLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual\nrepresentations. Specifically, the key finding of our study is that RL produces\nstronger and precisely localized visual representations compared to SFT,\nboosting the ability of the vision encoder for MLLM. We then reframe our\nfindings into a simple recipe for building strong vision encoders for MLLMs,\nPreference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,\na PIVOT-trained vision encoder outperforms even larger and more heavily-trained\ncounterparts, despite requiring less than 1% of the computational cost of\nstandard vision pretraining. This result opens an effective and efficient path\nfor advancing the vision backbones of MLLMs. Project page available at\nhttps://june-page.github.io/pivot/", "AI": {"tldr": "本研究发现，多模态语言模型（MLLM）的后训练策略（如强化学习RL）显著重塑了其视觉编码器，RL比SFT产生更强、更精准的视觉表示。作者提出了PIVOT方法，能以极低成本构建高性能视觉编码器，有效提升MLLM的视觉能力。", "motivation": "多模态语言模型（MLLM）的研究普遍认为其性能主要源于大型语言模型（LLM）骨干，导致对视觉编码器的理解存在空白。随着MLLM训练范式从SFT转向RL，这种忽视被放大，缺乏对RL训练如何重塑视觉编码器及MLLM的分析。", "method": "首先，研究人员调查了不同训练策略对MLLM的影响，特别是在强视觉相关的VQA基准上。其次，通过ImageNet分类、分割和梯度可视化等多样深入的实验，对MLLM的视觉编码器进行了关键且未充分探索的分析。最后，将研究结果提炼成一个构建强大视觉编码器的简单方法：偏好指导视觉优化（PIVOT）。", "result": "研究结果表明，MLLM的后训练策略（SFT或RL）不仅导致下游任务的不同结果，而且从根本上重塑了MLLM底层的视觉表示。具体而言，RL比SFT在强视觉相关的VQA基准上表现出明显优势，并产生了更强、更精准的局部视觉表示，提升了视觉编码器的能力。通过PIVOT训练的视觉编码器，仅需不到标准视觉预训练1%的计算成本，却能超越更大、训练更重的同类模型。", "conclusion": "MLLM的后训练策略（尤其是RL）对视觉编码器及其视觉表示具有深远影响。强化学习能有效提升视觉编码器的能力，产生更强、更精准的视觉特征。PIVOT提供了一种高效且有效的方法来构建强大的MLLM视觉编码器，为推进MLLM的视觉骨干网络开辟了一条新途径。"}}
{"id": "2510.17249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17249", "abs": "https://arxiv.org/abs/2510.17249", "authors": ["Franek Stark", "Rohit Kumar", "Shubham Vyas", "Hannah Isermann", "Jonas Haack", "Mihaela Popescu", "Jakob Middelberg", "Dennis Mronga", "Frank Kirchner"], "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration", "comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)", "summary": "Planetary exploration missions require robots capable of navigating extreme\nand unknown environments. While wheeled rovers have dominated past missions,\ntheir mobility is limited to traversable surfaces. Legged robots, especially\nquadrupeds, can overcome these limitations by handling uneven, obstacle-rich,\nand deformable terrains. However, deploying such robots in unknown conditions\nis challenging due to the need for environment-specific control, which is\ninfeasible when terrain and robot parameters are uncertain. This work presents\na modular control framework that combines model-based dynamic control with\nonline model adaptation and adaptive footstep planning to address uncertainties\nin both robot and terrain properties. The framework includes state estimation\nfor quadrupeds with and without contact sensing, supports runtime\nreconfiguration, and is integrated into ROS 2 with open-source availability.\nIts performance was validated on two quadruped platforms, multiple hardware\narchitectures, and in a volcano field test, where the robot walked over 700 m.", "AI": {"tldr": "该研究提出了一种模块化控制框架，结合模型动态控制、在线模型自适应和自适应步态规划，使四足机器人在未知和极端环境中实现稳健导航，并在火山地带进行了超过700米的实地验证。", "motivation": "行星探索任务需要机器人在极端和未知环境中导航。传统轮式漫游车移动受限，而腿足机器人（特别是四足机器人）能应对不平坦、障碍多和可变形地形。然而，在未知条件下部署腿足机器人面临挑战，因为需要针对特定环境的控制，但地形和机器人参数的不确定性使得这种控制难以实现。", "method": "本文提出一个模块化控制框架，结合了基于模型的动态控制、在线模型自适应和自适应步态规划，以应对机器人和地形属性的不确定性。该框架包括有无接触传感的四足机器人状态估计，支持运行时重新配置，并集成到ROS 2中并开源。", "result": "该框架在两个四足机器人平台、多种硬件架构上进行了性能验证，并在一次火山实地测试中，机器人成功行走了超过700米。", "conclusion": "该模块化控制框架通过结合模型动态控制、在线模型自适应和自适应步态规划，有效解决了在未知环境下四足机器人导航的挑战，并在实际极端环境中展现出卓越的性能和鲁棒性。"}}
{"id": "2510.16335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16335", "abs": "https://arxiv.org/abs/2510.16335", "authors": ["Bo Peng", "Jie Lu", "Guangquan Zhang", "Zhen Fang"], "title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering", "comment": "revised and extended version of ICCV2025", "summary": "This paper investigates the recently emerged problem of Language-assisted\nImage Clustering (LaIC), where textual semantics are leveraged to improve the\ndiscriminability of visual representations to facilitate image clustering. Due\nto the unavailability of true class names, one of core challenges of LaIC lies\nin how to filter positive nouns, i.e., those semantically close to the images\nof interest, from unlabeled wild corpus data. Existing filtering strategies are\npredominantly based on the off-the-shelf feature space learned by CLIP;\nhowever, despite being intuitive, these strategies lack a rigorous theoretical\nfoundation. To fill this gap, we propose a novel gradient-based framework,\ntermed as GradNorm, which is theoretically guaranteed and shows strong\nempirical performance. In particular, we measure the positiveness of each noun\nbased on the magnitude of gradients back-propagated from the cross-entropy\nbetween the predicted target distribution and the softmax output.\nTheoretically, we provide a rigorous error bound to quantify the separability\nof positive nouns by GradNorm and prove that GradNorm naturally subsumes\nexisting filtering strategies as extremely special cases of itself.\nEmpirically, extensive experiments show that GradNorm achieves the\nstate-of-the-art clustering performance on various benchmarks.", "AI": {"tldr": "本论文提出了GradNorm，一个基于梯度的框架，用于语言辅助图像聚类（LaIC）中正向名词的过滤。该方法具有严格的理论保证，并实现了最先进的聚类性能。", "motivation": "语言辅助图像聚类（LaIC）面临的核心挑战是如何从无标签的语料库中过滤出与图像语义相关的正向名词，因为真实的类别名称不可用。现有过滤策略主要基于CLIP的特征空间，但缺乏严格的理论基础。", "method": "本文提出了一种名为GradNorm的梯度 기반框架。它通过测量从预测目标分布与softmax输出之间的交叉熵反向传播的梯度幅度来衡量每个名词的“正向性”。", "result": "理论上，GradNorm为正向名词的可分离性提供了严格的误差界限，并证明了它自然地包含了现有过滤策略作为其极特殊情况。经验上，GradNorm在各种基准测试中实现了最先进的聚类性能。", "conclusion": "GradNorm是一个具有理论保证和强大实证性能的框架，有效解决了LaIC中正向名词过滤的难题，显著提升了图像聚类效果。"}}
{"id": "2510.17203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17203", "abs": "https://arxiv.org/abs/2510.17203", "authors": ["Ryota Soga", "Masataka Kobayashi", "Tsukasa Shimizu", "Shintaro Shiba", "Quan Kong", "Shan Lu", "Takaya Yamazato"], "title": "Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera", "comment": "7pages, APCC2025", "summary": "Event cameras, featuring high temporal resolution and high dynamic range,\noffer visual sensing capabilities comparable to conventional image sensors\nwhile capturing fast-moving objects and handling scenes with extreme lighting\ncontrasts such as tunnel exits. Leveraging these properties, this study\nproposes a novel self-localization system that integrates visible light\ncommunication (VLC) and visible light positioning (VLP) within a single event\ncamera. The system enables a vehicle to estimate its position even in\nGPS-denied environments, such as tunnels, by using VLC to obtain coordinate\ninformation from LED transmitters and VLP to estimate the distance to each\ntransmitter.\n  Multiple LEDs are installed on the transmitter side, each assigned a unique\npilot sequence based on Walsh-Hadamard codes. The event camera identifies\nindividual LEDs within its field of view by correlating the received signal\nwith these codes, allowing clear separation and recognition of each light\nsource. This mechanism enables simultaneous high-capacity MISO (multi-input\nsingle-output) communication through VLC and precise distance estimation via\nphase-only correlation (POC) between multiple LED pairs.\n  To the best of our knowledge, this is the first vehicle-mounted system to\nachieve simultaneous VLC and VLP functionalities using a single event camera.\nField experiments were conducted by mounting the system on a vehicle traveling\nat 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,\nwith a root mean square error (RMSE) of distance estimation within 0.75 m for\nranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.", "AI": {"tldr": "本研究提出了一种新颖的自定位系统，利用单个事件相机同时实现可见光通信（VLC）和可见光定位（VLP），使车辆能够在GPS拒绝环境中（如隧道）进行定位。该系统通过Walsh-Hadamard码识别LED并使用相位相关法（POC）进行距离估计，并在实车实验中表现出鲁棒性。", "motivation": "在GPS信号受限的环境（如隧道）中，车辆需要可靠的自定位能力。传统的图像传感器在捕获快速移动物体和处理极端光照对比方面存在局限性。事件相机凭借其高时间分辨率和高动态范围的特性，为解决这些挑战提供了潜在优势。", "method": "该系统将VLC和VLP功能集成到单个事件相机中。LED发射器安装多个，每个分配一个基于Walsh-Hadamard码的独特导频序列。事件相机通过将接收到的信号与这些代码相关联来识别视场内的单个LED，从而实现清晰的光源分离和识别。这使得能够通过VLC进行同步的高容量MISO通信，并通过多个LED对之间的相位相关法（POC）进行精确距离估计。该系统被安装在以30公里/小时速度行驶的车辆上进行实地实验。", "result": "据作者所知，这是首个使用单个事件相机实现同步VLC和VLP功能的车载系统。实地实验证明了系统在真实世界中的鲁棒性能：在长达100米的范围内，距离估计的均方根误差（RMSE）在0.75米以内；在相同范围内，误码率（BER）低于0.01。", "conclusion": "该研究成功开发并验证了一个基于事件相机的新型系统，能够同时执行VLC和VLP，为车辆在GPS拒绝环境中提供自定位能力。实验结果表明，该系统在真实世界条件下具有出色的鲁棒性和精度。"}}
{"id": "2510.16670", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16670", "abs": "https://arxiv.org/abs/2510.16670", "authors": ["Yiyang Liu", "James C. Liang", "Heng Fan", "Wenhao Yang", "Yiming Cui", "Xiaotian Han", "Lifu Huang", "Dongfang Liu", "Qifan Wang", "Cheng Han"], "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "comment": "NeurIPS 2025", "summary": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)\napproach to facilitate Large Language Model (LLM) adaptation to downstream\ntasks by conditioning generation with task-aware guidance. Despite its\nsuccesses, current prompt-based learning methods heavily rely on laborious grid\nsearching for optimal prompt length and typically require considerable number\nof prompts, introducing additional computational burden. Worse yet, our pioneer\nfindings indicate that the task-aware prompt design is inherently limited by\nits absence of instance-aware information, leading to a subtle attention\ninterplay with the input sequence. In contrast, simply incorporating\ninstance-aware information as a part of the guidance can enhance the\nprompt-tuned model performance without additional fine-tuning. Moreover, we\nfind an interesting phenomenon, namely \"attention anchor\", that incorporating\ninstance-aware tokens at the earliest position of the sequence can successfully\npreserve strong attention to critical structural information and exhibit more\nactive attention interaction with all input tokens. In light of our\nobservation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and\neffective solution that leverages off-the-shelf, informative instance semantics\ninto prompt-based learning. Our approach innovatively integrates both\ninstance-aware and task-aware information in a nearly parameter-free manner\n(i.e., one single capsule prompt). Empirical results demonstrate that our\nmethod can exhibit superior performance across various language tasks (e.g.,\n84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while\nenjoying high parameter efficiency (e.g., 0.003\\% of model parameters on\nLlama3.2-1B).", "AI": {"tldr": "本文提出Capsule Prompt-Tuning (CaPT)，通过整合实例感知信息和任务感知信息，以一个胶囊提示实现参数高效且性能优越的提示学习，并发现“注意力锚点”现象。", "motivation": "当前的提示学习方法存在以下问题：1) 依赖繁琐的网格搜索寻找最佳提示长度，引入计算负担；2) 任务感知提示设计缺乏实例感知信息，导致与输入序列的注意力交互不佳；3) 需要大量提示，效率低下。", "method": "作者发现将实例感知信息作为指导的一部分可以提升模型性能，并提出了“注意力锚点”现象，即在序列最早位置加入实例感知token能保持对关键结构信息的强注意力并促进所有输入token的活跃注意力交互。基于此，他们引入了Capsule Prompt-Tuning (CaPT)，这是一种将现成的、信息丰富的实例语义整合到提示学习中的方法，以近乎无参数（即一个单一的胶囊提示）的方式创新性地结合了实例感知和任务感知信息。", "result": "CaPT在各种语言任务上表现出卓越性能（例如，T5-Large上平均准确率达84.03%），作为“注意力锚点”发挥作用，同时具有极高的参数效率（例如，Llama3.2-1B上仅占模型参数的0.003%）。", "conclusion": "CaPT是一种高效且有效的提示学习解决方案，通过利用实例语义并充当“注意力锚点”，显著提升了大型语言模型在下游任务上的适应性，同时保持了极高的参数效率，克服了现有方法的局限性。"}}
{"id": "2510.16685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16685", "abs": "https://arxiv.org/abs/2510.16685", "authors": ["Damin Zhang", "Julia Rayz"], "title": "Temporal Understanding under Deictic Frame of Reference", "comment": "Under review", "summary": "Understanding time is fundamental to human cognition, where temporal\nexperience is often conceptualized through spatial metaphors grounded in\nsensory-motor experience. For example, \"summer is approaching\" parallels \"We\nare approaching the summer\". In such expressions, humans rely on a frame of\nreference (FoR) to interpret meaning relative to a particular viewpoint.\nExtending this concept to time, a temporal frame of reference (t-FoR) defines\nhow temporal relations are perceived relative to an experiencer's moment of\n\"now\". While Large Language Models (LLMs) have shown remarkable advances in\nnatural language understanding, their ability to interpret and reason about\ntime remains limited. In this work, we introduce TUuD (Temporal Understanding\nunder Deictic t-FoR), a framework that evaluates how LLMs interpret time-event\nand event-event relations when the reference point of \"now\" dynamically shifts\nalong a timeline. Following recent work on temporal cognition\n\\cite{li2025other}, LLMs are prompted to rate the similarity between the\ncurrent moment and a target event from 0.00 (completely dissimilar) to 1.00\n(highly similar), where similarity quantifies perceived temporal alignment\nbetween the two points. Our results show that four evaluated LLMs exhibit\nmeasurable adaptation to a deictic t-FoR, with similarity ratings peaking\naround the present and decreasing toward past and future events. The\nadaptation, however, weakens beyond near-term contexts, suggesting that while\nLLMs display partial human-like temporal cognition, their temporal reasoning\nremains sensitive to reference-frame shifts and temporal distance.", "AI": {"tldr": "本文引入TUuD框架，评估大型语言模型（LLMs）在“现在”动态变化的参照系下，如何理解时间事件关系。结果显示LLMs展现出部分类人时间认知，但其适应性会随时间距离的增加而减弱。", "motivation": "人类理解时间常通过空间隐喻，依赖于参照系（FoR）。尽管LLMs在自然语言理解方面取得显著进展，但它们解释和推理时间的能力，尤其是在“现在”这一参照点动态变化的情况下，仍然有限。", "method": "本文提出了TUuD（Temporal Understanding under Deictic t-FoR）框架。在该框架下，LLMs被要求评估当前时刻与目标事件之间的相似度（0.00-1.00），其中“现在”这一参照点会沿着时间线动态移动。相似度用于量化两者之间感知到的时间对齐程度。", "result": "四种被评估的LLMs展现出对动态时间参照系（deictic t-FoR）的可测量适应性，相似度评分在当前时刻达到峰值，并随着事件向过去和未来延伸而降低。然而，这种适应性在超出近期上下文后会减弱。", "conclusion": "LLMs展现出部分类人的时间认知能力，但其时间推理仍然对参照系的变化和时间距离敏感。这表明LLMs虽然能适应动态的“现在”，但其深度时间理解仍有局限性。"}}
{"id": "2510.16370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16370", "abs": "https://arxiv.org/abs/2510.16370", "authors": ["Pulin Li", "Guocheng Wu", "Li Yin", "Yuxin Zheng", "Wei Zhang", "Yanjie Zhou"], "title": "MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization", "comment": "https://github.com/wu33learn/MIRAD", "summary": "Social manufacturing leverages community collaboration and scattered\nresources to realize mass individualization in modern industry. However, this\nparadigm shift also introduces substantial challenges in quality control,\nparticularly in defect detection. The main difficulties stem from three\naspects. First, products often have highly customized configurations. Second,\nproduction typically involves fragmented, small-batch orders. Third, imaging\nenvironments vary considerably across distributed sites. To overcome the\nscarcity of real-world datasets and tailored algorithms, we introduce the Mass\nIndividualization Robust Anomaly Detection (MIRAD) dataset. As the first\nbenchmark explicitly designed for anomaly detection in social manufacturing,\nMIRAD captures three critical dimensions of this domain: (1) diverse\nindividualized products with large intra-class variation, (2) data collected\nfrom six geographically dispersed manufacturing nodes, and (3) substantial\nimaging heterogeneity, including variations in lighting, background, and motion\nconditions. We then conduct extensive evaluations of state-of-the-art (SOTA)\nanomaly detection methods on MIRAD, covering one-class, multi-class, and\nzero-shot approaches. Results show a significant performance drop across all\nmodels compared with conventional benchmarks, highlighting the unresolved\ncomplexities of defect detection in real-world individualized production. By\nbridging industrial requirements and academic research, MIRAD provides a\nrealistic foundation for developing robust quality control solutions essential\nfor Industry 5.0. The dataset is publicly available at\nhttps://github.com/wu33learn/MIRAD.", "AI": {"tldr": "社会化制造中的缺陷检测面临定制化、小批量和多变环境的挑战。为解决真实世界数据集和算法的稀缺，本文引入了MIRAD数据集，一个专门用于社会化制造异常检测的基准。对现有方法的评估显示，其性能显著下降，凸显了该领域未解决的复杂性，并为工业5.0提供了研究基础。", "motivation": "社会化制造的“大规模个性化”范式为现代工业带来了质量控制，特别是缺陷检测方面的巨大挑战。主要困难源于：产品高度定制化、生产涉及碎片化小批量订单，以及分布式站点成像环境差异大。现有研究缺乏针对性的真实世界数据集和算法。", "method": "本文引入了“大规模个性化鲁棒异常检测”（MIRAD）数据集，这是首个专门为社会化制造中的异常检测设计的基准。MIRAD捕捉了该领域三个关键维度：1) 具有大类内差异的多样化个性化产品；2) 从六个地理分散的制造节点收集的数据；3) 显著的成像异质性（包括光照、背景和运动条件）。随后，在MIRAD上对最先进的（SOTA）异常检测方法（包括单类、多类和零样本方法）进行了广泛评估。", "result": "与传统基准相比，所有模型在MIRAD上的性能都显著下降，这突出显示了真实世界个性化生产中缺陷检测的未解决复杂性。", "conclusion": "MIRAD数据集通过连接工业需求和学术研究，为开发工业5.0所需的鲁棒质量控制解决方案提供了现实基础。该数据集已公开可用。"}}
{"id": "2510.17237", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17237", "abs": "https://arxiv.org/abs/2510.17237", "authors": ["Wuhao Xie", "Kanji Tanaka"], "title": "Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance", "comment": "4 pages, technical report", "summary": "Long-term autonomy for mobile robots requires both robust self-localization\nand reliable map maintenance. Conventional landmark-based methods face a\nfundamental trade-off between landmarks with high detectability but low\ndistinctiveness (e.g., poles) and those with high distinctiveness but difficult\nstable detection (e.g., local point cloud structures). This work addresses the\nchallenge of descriptively identifying a unique \"signature\" (local point cloud)\nby leveraging a detectable, high-precision \"anchor\" (like a pole). To solve\nthis, we propose a novel canonical representation, \"Pole-Image,\" as a hybrid\nmethod that uses poles as anchors to generate signatures from the surrounding\n3D structure. Pole-Image represents a pole-like landmark and its surrounding\nenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate image\nwith the pole itself as the origin. This representation leverages the pole's\nnature as a high-precision reference point, explicitly encoding the \"relative\ngeometry\" between the stable pole and the variable surrounding point cloud. The\nkey advantage of pole landmarks is that \"detection\" is extremely easy. This\nease of detection allows the robot to easily track the same pole, enabling the\nautomatic and large-scale collection of diverse observational data (positive\npairs). This data acquisition feasibility makes \"Contrastive Learning (CL)\"\napplicable. By applying CL, the model learns a viewpoint-invariant and highly\ndiscriminative descriptor. The contributions are twofold: 1) The descriptor\novercomes perceptual aliasing, enabling robust self-localization. 2) The\nhigh-precision encoding enables high-sensitivity change detection, contributing\nto map maintenance.", "AI": {"tldr": "该研究提出了一种名为“Pole-Image”的混合方法，利用杆子作为锚点，结合对比学习，生成对周围3D环境具有视角不变性和高辨识度的描述符，以实现移动机器人的鲁棒自定位和高灵敏度地图维护。", "motivation": "移动机器人的长期自主性需要鲁棒的自定位和可靠的地图维护。传统基于地标的方法在可检测性高但辨识度低的地标和辨识度高但难以稳定检测的地标之间存在根本性权衡。", "method": "提出了一种新颖的“Pole-Image”规范表示，将杆状地标及其周围环境（从LiDAR点云检测）表示为一个2D极坐标图像，以杆子为原点。这种表示利用杆子作为高精度参考点的特性，编码了杆子与周围点云之间的相对几何关系。利用杆子易于检测的特点，实现大规模观测数据的自动收集，并应用对比学习（CL）来训练模型，学习视角不变且高度可区分的描述符。", "result": "所提出的描述符克服了感知混叠问题，实现了鲁棒的自定位。同时，高精度编码实现了高灵敏度的变化检测，有助于地图维护。", "conclusion": "通过结合杆子作为锚点、Pole-Image表示和对比学习，该方法有效解决了移动机器人长期自主性中鲁棒自定位和高灵敏度地图维护的挑战。"}}
{"id": "2510.16371", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16371", "abs": "https://arxiv.org/abs/2510.16371", "authors": ["Mohammad Javad Ahmadi", "Iman Gandomi", "Parisa Abdi", "Seyed-Farzad Mohammadi", "Amirhossein Taslimi", "Mehdi Khodaparast", "Hassan Hashemi", "Mahdi Tavakoli", "Hamid D. Taghirad"], "title": "Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis", "comment": "20 pages, 11 figures, 11 tables. Data descriptor for the Cataract-LMM\n  benchmark dataset. Source code and dataset are available", "summary": "The development of computer-assisted surgery systems depends on large-scale,\nannotated datasets. Current resources for cataract surgery often lack the\ndiversity and annotation depth needed to train generalizable deep-learning\nmodels. To address this gap, we present a dataset of 3,000 phacoemulsification\ncataract surgery videos from two surgical centers, performed by surgeons with a\nrange of experience levels. This resource is enriched with four annotation\nlayers: temporal surgical phases, instance segmentation of instruments and\nanatomical structures, instrument-tissue interaction tracking, and quantitative\nskill scores based on the established competency rubrics like the ICO-OSCAR.\nThe technical quality of the dataset is supported by a series of benchmarking\nexperiments for key surgical AI tasks, including workflow recognition, scene\nsegmentation, and automated skill assessment. Furthermore, we establish a\ndomain adaptation baseline for the phase recognition task by training a model\non a subset of surgical centers and evaluating its performance on a held-out\ncenter. The dataset and annotations are available in Google Form\n(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).", "AI": {"tldr": "本文介绍了一个包含3,000例白内障手术视频的大规模、多层级标注数据集，旨在解决现有数据集多样性和标注深度不足的问题，以支持可泛化的深度学习模型训练和手术AI任务的基准测试。", "motivation": "当前的白内障手术数据集在多样性和标注深度方面存在不足，难以训练出具有良好泛化能力的深度学习模型，从而阻碍了计算机辅助手术系统的发展。", "method": "研究者构建了一个包含3,000例白内障超声乳化手术视频的数据集，这些视频来源于两个手术中心，由不同经验水平的医生操作。该数据集包含四个标注层：时间手术阶段、器械和解剖结构实例分割、器械-组织交互跟踪以及基于ICO-OSCAR等标准的能力评分。", "result": "该数据集通过一系列针对关键手术AI任务（包括工作流识别、场景分割和自动化技能评估）的基准测试实验，证明了其技术质量。此外，研究者还为阶段识别任务建立了一个域适应基线模型，通过在部分中心数据上训练并在保留中心数据上评估其性能。", "conclusion": "本文成功构建并验证了一个多样化且深度标注的白内障手术视频数据集，为训练可泛化的深度学习模型和进行手术AI任务（如工作流识别、场景分割和自动化技能评估）提供了宝贵资源，并为未来研究建立了基准。"}}
{"id": "2510.17261", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17261", "abs": "https://arxiv.org/abs/2510.17261", "authors": ["Fernando Salanova", "Jesús Roche", "Cristian Mahuela", "Eduardo Montijano"], "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection", "comment": "6 pages,3 figures, Iberian Robotics Conference 2025", "summary": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations.", "AI": {"tldr": "本文提出了一种基于Transformer的异常检测方法，结合Nets-within-Nets数据生成框架，用于识别多机器人系统中线性时序逻辑（LTL）计划中的虚假行为，并在实验中展现了高准确性。", "motivation": "多机器人系统在执行高级任务时，需要可靠的方法来检测虚假行为，例如不正确的任务序列、空间约束违反、时间不一致或与LTL指定计划的偏差。", "method": "本文引入了一个基于Nets-within-Nets（NWN）范式的结构化数据生成框架，用于协调机器人行动与LTL衍生的全局任务规范。在此基础上，提出了一个基于Transformer的异常检测流水线，将机器人轨迹分类为正常或异常。此外，还进行了嵌入和架构的消融实验。", "result": "实验评估显示，该方法在识别执行效率低下方面达到了高准确率（91.3%），在核心任务违反检测方面表现出强大的鲁棒性（88.3%），并在基于约束的自适应异常检测方面达到66.8%。消融实验也表明，所提出的新颖方案优于更简单的表示方法。", "conclusion": "所提出的结合NWN数据生成框架和Transformer的异常检测方法，能够有效且鲁棒地识别多机器人系统中LTL计划的各种虚假行为，并在不同类型的异常检测中表现出色。"}}
{"id": "2510.17315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17315", "abs": "https://arxiv.org/abs/2510.17315", "authors": ["Po-Chen Ko", "Jiayuan Mao", "Yu-Hsiang Fu", "Hsien-Jeng Yeh", "Chu-Rong Chen", "Wei-Chiu Ma", "Yilun Du", "Shao-Hua Sun"], "title": "Implicit State Estimation via Video Replanning", "comment": null, "summary": "Video-based representations have gained prominence in planning and\ndecision-making due to their ability to encode rich spatiotemporal dynamics and\ngeometric relationships. These representations enable flexible and\ngeneralizable solutions for complex tasks such as object manipulation and\nnavigation. However, existing video planning frameworks often struggle to adapt\nto failures at interaction time due to their inability to reason about\nuncertainties in partially observed environments. To overcome these\nlimitations, we introduce a novel framework that integrates interaction-time\ndata into the planning process. Our approach updates model parameters online\nand filters out previously failed plans during generation. This enables\nimplicit state estimation, allowing the system to adapt dynamically without\nexplicitly modeling unknown state variables. We evaluate our framework through\nextensive experiments on a new simulated manipulation benchmark, demonstrating\nits ability to improve replanning performance and advance the field of\nvideo-based decision-making.", "AI": {"tldr": "本文提出了一种新的框架，通过在线更新模型参数和过滤失败计划，将交互时数据整合到视频规划过程中，从而在部分观测环境中实现隐式状态估计和动态适应，提高了重新规划的性能。", "motivation": "现有的视频规划框架在交互时难以适应失败，且无法推断部分观测环境中的不确定性，限制了其在复杂任务中的应用。", "method": "该框架将交互时数据整合到规划过程中，通过在线更新模型参数并过滤掉之前失败的计划。这种方法实现了隐式状态估计，无需显式建模未知状态变量即可动态适应。", "result": "通过在一个新的模拟操作基准上进行广泛实验，该框架显著提高了重新规划的性能，并推动了基于视频的决策领域的发展。", "conclusion": "该框架通过在规划过程中整合交互时数据并实现隐式状态估计，成功克服了现有视频规划框架在处理不确定性和交互时失败方面的局限性，为视频基决策提供了更灵活和可泛化的解决方案。"}}
{"id": "2510.17335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17335", "abs": "https://arxiv.org/abs/2510.17335", "authors": ["Xintong Yang", "Minglun Wei", "Ze Ji", "Yu-Kun Lai"], "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials", "comment": "Accepted as a regular paper by the IEEE Transactions on Robotics", "summary": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks.", "AI": {"tldr": "本文提出了一种名为DDBot的可微分挖掘机器人框架，用于处理具有未知物理特性的颗粒材料（如沙子和土壤）的小规模高精度挖掘任务。DDBot结合可微分物理模拟器、GPU加速和自动微分，实现了高效的系统识别和高精度的挖掘技能优化，并在实际部署中表现出色。", "motivation": "由于复杂的接触动力学、不可预测的材料特性和错综复杂的系统状态，自动化颗粒材料操作面临巨大挑战。现有方法在效率和准确性方面往往不足，因此需要填补这一研究空白，尤其是在未知物理特性的小规模高精度颗粒材料挖掘任务中。", "method": "本文提出了DDBot框架，其核心是一个专门为颗粒材料操作定制的可微分物理模拟器，该模拟器由GPU加速并行计算和自动微分驱动。DDBot通过可微分的技能到动作映射、面向任务的演示方法、梯度裁剪和基于线搜索的梯度下降，实现了对未知颗粒材料的高效可微分系统识别和高精度挖掘技能优化。", "result": "实验结果表明，DDBot能够高效地（在5到20分钟内收敛）识别未知颗粒材料的动力学特性并优化挖掘技能。在零样本实际部署中，DDBot取得了高精度结果，凸显了其实用性。与现有最新基线的基准测试结果也证实了DDBot在此类挖掘任务中的鲁棒性和效率。", "conclusion": "DDBot框架通过结合可微分物理模拟和优化技术，成功解决了颗粒材料挖掘任务中的效率和精度挑战。它能够高效识别未知材料特性并优化挖掘技能，在实际应用中展现出高精度、鲁棒性和实用性。"}}
{"id": "2510.16377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16377", "abs": "https://arxiv.org/abs/2510.16377", "authors": ["Tianhang Cheng", "Albert J. Zhai", "Evan Z. Chen", "Rui Zhou", "Yawen Deng", "Zitong Li", "Kejie Zhao", "Janice Shiu", "Qianyu Zhao", "Yide Xu", "Xinlei Wang", "Yuan Shen", "Sheng Wang", "Lisa Ainsworth", "Kaiyu Guan", "Shenlong Wang"], "title": "Demeter: A Parametric Model of Crop Plant Morphology from the Real World", "comment": "ICCV 2025", "summary": "Learning 3D parametric shape models of objects has gained popularity in\nvision and graphics and has showed broad utility in 3D reconstruction,\ngeneration, understanding, and simulation. While powerful models exist for\nhumans and animals, equally expressive approaches for modeling plants are\nlacking. In this work, we present Demeter, a data-driven parametric model that\nencodes key factors of a plant morphology, including topology, shape,\narticulation, and deformation into a compact learned representation. Unlike\nprevious parametric models, Demeter handles varying shape topology across\nvarious species and models three sources of shape variation: articulation,\nsubcomponent shape variation, and non-rigid deformation. To advance crop plant\nmodeling, we collected a large-scale, ground-truthed dataset from a soybean\nfarm as a testbed. Experiments show that Demeter effectively synthesizes\nshapes, reconstructs structures, and simulates biophysical processes. Code and\ndata is available at https://tianhang-cheng.github.io/Demeter/.", "AI": {"tldr": "Demeter是一种数据驱动的参数化植物模型，它能编码植物形态的关键因素，处理不同物种的拓扑结构变化，并建模关节、子组件形状和非刚性变形三种形状变化来源，在形状合成、结构重建和生物物理过程模拟方面表现出色。", "motivation": "虽然人体和动物的强大参数化形状模型已经存在并广泛应用于3D重建、生成、理解和模拟，但植物领域缺乏同样富有表现力的方法。特别是在作物植物建模方面，需要更先进的模型。", "method": "本文提出了Demeter，一个数据驱动的参数化模型，它将植物形态的关键因素（包括拓扑、形状、关节和变形）编码成紧凑的学习表示。与以往模型不同，Demeter能够处理不同物种间变化的形状拓扑结构，并对三种形状变异来源进行建模：关节、子组件形状变异和非刚性变形。为了推进作物植物建模，研究人员收集了一个大规模、经过地面实况验证的大豆农场数据集作为测试平台。", "result": "实验结果表明，Demeter能够有效地合成形状、重建结构并模拟生物物理过程。", "conclusion": "Demeter是一个有效的参数化植物模型，它解决了现有模型在植物建模方面的不足，并通过处理复杂的拓扑和多种变形来源，在形状合成、结构重建和生物物理模拟方面展现了广泛的实用性。"}}
{"id": "2510.16708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16708", "abs": "https://arxiv.org/abs/2510.16708", "authors": ["Kailai Yang", "Yan Leng", "Xin Zhang", "Tianlin Zhang", "Paul Thompson", "Bernard Keavney", "Maciej Tomaszewski", "Sophia Ananiadou"], "title": "Natural Language Processing Applications in Cardiology: A Narrative Review", "comment": null, "summary": "Cardiovascular disease has become increasingly prevalent in modern society\nand has a significant effect on global health and well-being. Heart-related\nconditions are intricate, multifaceted disorders, which may be influenced by a\ncombination of genetic predispositions, lifestyle choices, and various\nsocioeconomic and clinical factors. Information regarding these potentially\ncomplex interrelationships is dispersed among diverse types of textual data,\nwhich include patient narratives, medical records, and scientific literature,\namong others. Natural language processing (NLP) techniques have increasingly\nbeen adopted as a powerful means to analyse and make sense of this vast amount\nof unstructured data. This, in turn, can allow healthcare professionals to gain\ndeeper insights into the cardiology field, which has the potential to\nrevolutionize current approaches to the diagnosis, treatment, and prevention of\ncardiac problems. This review provides a detailed overview of NLP research in\ncardiology between 2014 and 2025. We queried six literature databases to find\narticles describing the application of NLP techniques in the context of a range\nof different cardiovascular diseases. Following a rigorous screening process,\nwe identified a total of 265 relevant articles. We analysed each article from\nmultiple dimensions, i.e., NLP paradigm types, cardiology-related task types,\ncardiovascular disease types, and data source types. Our analysis reveals\nconsiderable diversity within each of these dimensions, thus demonstrating the\nconsiderable breadth of NLP research within the field. We also perform a\ntemporal analysis, which illustrates the evolution and changing trends in NLP\nmethods employed over the last decade that we cover. To our knowledge, the\nreview constitutes the most comprehensive overview of NLP research in\ncardiology to date.", "AI": {"tldr": "该综述全面回顾了2014年至2025年间自然语言处理（NLP）在心脏病学领域的应用研究，分析了其多样性和发展趋势。", "motivation": "心血管疾病日益普遍且复杂，相关信息分散在各类文本数据中。NLP技术有望分析这些非结构化数据，为医疗专业人员提供深入见解，从而革新心脏问题的诊断、治疗和预防。", "method": "作者查询了六个文献数据库，筛选出265篇描述NLP技术在心血管疾病背景下应用的文章（2014-2025年）。文章从NLP范式类型、心脏病学任务类型、心血管疾病类型和数据源类型等多个维度进行分析，并进行了时间序列分析。", "result": "分析揭示了NLP在心脏病学研究中在各个维度上的显著多样性，展示了该领域的广阔性。时间分析还阐明了过去十年NLP方法演变和变化的趋势。", "conclusion": "该综述是迄今为止对NLP在心脏病学研究中最全面的概述，突出了该领域的广度及其方法论的演变。"}}
{"id": "2510.16686", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16686", "abs": "https://arxiv.org/abs/2510.16686", "authors": ["Wenhang Shi", "Shuqing Bian", "Yiren Chen", "Xinyi Zhang", "Zhe Zhao", "Pengfei Hu", "Wei Lu", "Xiaoyong Du"], "title": "Investigating the Impact of Rationales for LLMs on Natural Language Understanding", "comment": null, "summary": "Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to\nderive final answers, benefit LLMs in both inference and training.\nIncorporating rationales, either by generating them before answering during\ninference, or by placing them before or after the original answers during\ntraining - significantly improves model performance on mathematical, symbolic\nand commonsense reasoning tasks. However, most work focuses on the role of\nrationales in these reasoning tasks, overlooking their potential impact on\nother important tasks like natural language understanding (NLU) tasks. In this\nwork, we raise the question: Can rationales similarly benefit NLU tasks? To\nconduct a systematic exploration, we construct NLURC, a comprehensive and\nhigh-quality NLU dataset collection with rationales, and develop various\nrationale-augmented methods. Through exploring the applicability of these\nmethods on NLU tasks using the dataset, we uncover several potentially\nsurprising findings: (1) CoT inference shifts from hindering NLU performance to\nsurpassing direct label prediction as model size grows, indicating a positive\ncorrelation. (2) Most rationale-augmented training methods perform worse than\nlabel-only training, with one specially designed method consistently achieving\nimprovements. (3) LLMs trained with rationales achieve significant performance\ngains on unseen NLU tasks, rivaling models ten times their size, while\ndelivering interpretability on par with commercial LLMs.", "AI": {"tldr": "链式思考（CoT）推理在推理任务中对大型语言模型（LLMs）有益，但其对自然语言理解（NLU）任务的影响尚不明确。本研究通过构建NLURC数据集并开发增强方法，发现CoT推理对大型NLU模型有益，大多数CoT训练方法表现不佳，但一种特定方法能持续改进NLU性能、泛化能力和可解释性。", "motivation": "链式思考（CoT）推理在数学、符号和常识推理任务中显著提升LLMs的性能。然而，现有工作主要关注其在推理任务中的作用，而忽略了其对自然语言理解（NLU）任务的潜在影响。本研究旨在探讨CoT推理是否也能使NLU任务受益。", "method": "1. 构建NLURC：一个综合且高质量的带有推理过程的NLU数据集集合。2. 开发各种推理过程增强方法（包括推理时和训练时）。3. 使用NLURC数据集探索这些方法在NLU任务上的适用性。", "result": "1. CoT推理：随着模型规模的增长，CoT推理在NLU任务上的表现从阻碍转变为超越直接标签预测，显示出正相关性。2. 带有推理过程的训练方法：大多数此类方法表现不如仅使用标签的训练。但一种专门设计的方法持续实现了性能提升。3. 泛化能力：使用推理过程训练的LLMs在未见过的NLU任务上取得了显著的性能提升，可与十倍大小的模型媲美，同时提供与商业LLMs相当的可解释性。", "conclusion": "推理过程（CoT rationales）可以使NLU任务受益，尤其是在模型规模增长时进行推理，以及采用特定训练方法时。这些方法还能显著提升LLMs在未见NLU任务上的泛化能力和可解释性。"}}
{"id": "2510.16375", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16375", "abs": "https://arxiv.org/abs/2510.16375", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance", "comment": "Under review", "summary": "Road potholes pose significant safety hazards and maintenance challenges,\nparticularly on India's diverse and under-maintained road networks. This paper\npresents iWatchRoadv2, a fully automated end-to-end platform for real-time\npothole detection, GPS-based geotagging, and dynamic road health visualization\nusing OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000\ndashcam frames capturing diverse Indian road conditions, weather patterns, and\nlighting scenarios, which we used to fine-tune the Ultralytics YOLO model for\naccurate pothole detection. The system synchronizes OCR-extracted video\ntimestamps with external GPS logs to precisely geolocate each detected pothole,\nenriching detections with comprehensive metadata, including road segment\nattribution and contractor information managed through an optimized backend\ndatabase. iWatchRoadv2 introduces intelligent governance features that enable\nauthorities to link road segments with contract metadata through a secure login\ninterface. The system automatically sends alerts to contractors and officials\nwhen road health deteriorates, supporting automated accountability and warranty\nenforcement. The intuitive web interface delivers actionable analytics to\nstakeholders and the public, facilitating evidence-driven repair planning,\nbudget allocation, and quality assessment. Our cost-effective and scalable\nsolution streamlines frame processing and storage while supporting seamless\npublic engagement for urban and rural deployments. By automating the complete\npothole monitoring lifecycle, from detection to repair verification,\niWatchRoadv2 enables data-driven smart city management, transparent governance,\nand sustainable improvements in road infrastructure maintenance. The platform\nand live demonstration are accessible at\nhttps://smlab.niser.ac.in/project/iwatchroad.", "AI": {"tldr": "iWatchRoadv2 是一个全自动端到端平台，用于实时坑洼检测、GPS地理标记和动态道路健康可视化，特别针对印度道路。它使用 YOLO 模型、自定义数据集和GPS同步，并整合了智能治理、自动警报和用户友好的网络界面，以实现数据驱动的道路维护和透明管理。", "motivation": "道路坑洼对安全构成重大威胁，并带来维护挑战，尤其是在印度多样化且维护不足的道路网络上。需要一个自动化、端到端的解决方案来实时检测、定位和管理坑洼。", "method": "该研究构建了一个包含7000多张行车记录仪图像的自标注数据集，捕捉了印度多样的道路状况。利用该数据集微调 Ultralytics YOLO 模型进行坑洼检测。系统通过OCR提取的视频时间戳与外部GPS日志同步，实现精确的地理定位。检测结果通过优化的后端数据库丰富了道路段属性和承包商信息。平台还引入了智能治理功能，通过安全登录界面将道路段与合同元数据关联，并自动向承包商和官员发送道路健康恶化警报。此外，它提供了一个直观的网络界面，用于分析和公众参与。", "result": "iWatchRoadv2 成功实现了实时坑洼检测、基于GPS的地理标记和使用 OpenStreetMap (OSM) 的动态道路健康可视化。该平台支持自动化问责制和保修执行，为利益相关者和公众提供可操作的分析。它优化了帧处理和存储，提供了一种经济高效且可扩展的解决方案，促进了数据驱动的维修规划、预算分配和质量评估。该系统能够实现数据驱动的智慧城市管理、透明治理和道路基础设施维护的可持续改进。", "conclusion": "iWatchRoadv2 通过自动化从检测到维修验证的整个坑洼监测生命周期，实现了数据驱动的智慧城市管理、透明治理和道路基础设施维护的可持续改进。该平台为解决印度及其他地区的道路维护挑战提供了一个全面且可扩展的解决方案。"}}
{"id": "2510.17341", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17341", "abs": "https://arxiv.org/abs/2510.17341", "authors": ["Fan Shao", "Satoshi Endo", "Sandra Hirche", "Fanny Ficuciello"], "title": "Interactive Force-Impedance Control", "comment": null, "summary": "Human collaboration with robots requires flexible role adaptation, enabling\nrobot to switch between active leader and passive follower. Effective role\nswitching depends on accurately estimating human intention, which is typically\nachieved through external force analysis, nominal robot dynamics, or\ndata-driven approaches. However, these methods are primarily effective in\ncontact-sparse environments. When robots under hybrid or unified\nforce-impedance control physically interact with active humans or non-passive\nenvironments, the robotic system may lose passivity and thus compromise safety.\nTo address this challenge, this paper proposes the unified Interactive\nForce-Impedance Control (IFIC) framework that adapts to the interaction power\nflow, ensuring effortless and safe interaction in contact-rich environments.\nThe proposed control architecture is formulated within a port-Hamiltonian\nframework, incorporating both interaction and task control ports, through which\nsystem passivity is guaranteed.", "AI": {"tldr": "本文提出统一交互力-阻抗控制（IFIC）框架，通过适应交互功率流并在端口哈密顿框架下设计，确保机器人在密集接触环境中与人类进行安全、轻松的协作，并保证系统无源性。", "motivation": "现有的人机协作方法（如通过外部力、名义机器人动力学或数据驱动）在稀疏接触环境中有效，但在机器人采用混合或统一力-阻抗控制并与主动人类或非被动环境交互时，系统可能失去无源性，从而危及安全。", "method": "提出统一交互力-阻抗控制（IFIC）框架。该框架能够适应交互功率流，确保在密集接触环境中的轻松安全交互。它在端口哈密顿框架内构建，整合了交互和任务控制端口，从而保证了系统无源性。", "result": "所提出的IFIC控制架构能够适应交互功率流，确保在密集接触环境中实现轻松且安全的交互，并通过其端口哈密顿框架设计保证了系统无源性。", "conclusion": "IFIC框架提供了一种在密集接触环境中实现安全、轻松人机协作的有效方法，通过其独特的控制架构和无源性保证，解决了现有方法在复杂交互中可能出现的安全问题。"}}
{"id": "2510.17369", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17369", "abs": "https://arxiv.org/abs/2510.17369", "authors": ["Haochen Su", "Cristian Meo", "Francesco Stella", "Andrea Peirone", "Kai Junge", "Josie Hughes"], "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots", "comment": "Accepted by NeurIPS 2025 SpaVLE workshop. 4 pages, 2 figures(in main\n  paper, excluding references and supplements)", "summary": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2510.16396", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16396", "abs": "https://arxiv.org/abs/2510.16396", "authors": ["Yeh Keng Hao", "Hsu Tzu Wei", "Sun Min"], "title": "SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation", "comment": "Accepted to AICCC 2025", "summary": "With the increasing ubiquity of AR/VR devices, the deployment of deep\nlearning models on edge devices has become a critical challenge. These devices\nrequire real-time inference, low power consumption, and minimal latency. Many\nframework designers face the conundrum of balancing efficiency and performance.\nWe design a light framework that adopts an encoder-decoder architecture and\nintroduces several key contributions aimed at improving both efficiency and\naccuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the\ninherent sparsity in hand pose images, achieving a 42% end-to-end efficiency\nimprovement. Moreover, we propose our SPLite decoder. This new architecture\nsignificantly boosts the decoding process's frame rate by 3.1x on the Raspberry\nPi 5, while maintaining accuracy on par. To further optimize performance, we\napply quantization-aware training, reducing memory usage while preserving\naccuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on\nFreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5\nCPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on\ncompound benchmark datasets, demonstrating comparable accuracy to\nstate-of-the-art approaches while significantly enhancing computational\nefficiency.", "AI": {"tldr": "本文提出了一种轻量级编码器-解码器框架，通过稀疏卷积、新型SPLite解码器和量化感知训练，显著提高了AR/VR边缘设备上深度学习模型的效率和速度，同时保持了高精度，特别是在手部姿态估计任务上。", "motivation": "随着AR/VR设备的普及，在边缘设备上部署深度学习模型面临实时推理、低功耗和低延迟的挑战。框架设计者需要在效率和性能之间取得平衡。", "method": "该研究设计了一个轻量级编码器-解码器架构。具体方法包括：1) 在ResNet-18骨干网络上应用稀疏卷积，以利用手部姿态图像固有的稀疏性。2) 提出SPLite解码器架构，以加速解码过程。3) 采用量化感知训练，以减少内存使用并保持精度。", "result": "主要成果包括：1) 稀疏卷积使端到端效率提高了42%。2) SPLite解码器在树莓派5上将解码过程帧率提高了3.1倍，同时保持了精度。3) 量化感知训练在FreiHAND数据集上将PA-MPJPE从9.0毫米微幅增加到9.1毫米，同时减少了内存使用。4) 整体系统在树莓派5 CPU上实现了2.98倍的加速。5) 在复合基准数据集上，该方法与最先进方法相比具有相当的精度，并显著提高了计算效率。", "conclusion": "该研究成功开发了一个高效且准确的深度学习框架，适用于AR/VR边缘设备上的手部姿态估计，通过创新的架构和优化技术，显著提升了计算效率和速度，同时保持了高水平的性能。"}}
{"id": "2510.16712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16712", "abs": "https://arxiv.org/abs/2510.16712", "authors": ["Shivam Ratnakar", "Sanjay Raghavendra"], "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models", "comment": null, "summary": "Integration of Large Language Models with search/retrieval engines has become\nubiquitous, yet these systems harbor a critical vulnerability that undermines\ntheir reliability. We present the first systematic investigation of \"chameleon\nbehavior\" in LLMs: their alarming tendency to shift stances when presented with\ncontradictory questions in multi-turn conversations (especially in\nsearch-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising\n17,770 carefully crafted question-answer pairs across 1,180 multi-turn\nconversations spanning 12 controversial domains, we expose fundamental flaws in\nstate-of-the-art systems. We introduce two theoretically grounded metrics: the\nChameleon Score (0-1) that quantifies stance instability, and Source Re-use\nRate (0-1) that measures knowledge diversity. Our rigorous evaluation of\nLlama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent\nfailures: all models exhibit severe chameleon behavior (scores 0.391-0.511),\nwith GPT-4o-mini showing the worst performance. Crucially, small\nacross-temperature variance (less than 0.004) suggests the effect is not a\nsampling artifact. Our analysis uncovers the mechanism: strong correlations\nbetween source re-use rate and confidence (r=0.627) and stance changes\n(r=0.429) are statistically significant (p less than 0.05), indicating that\nlimited knowledge diversity makes models pathologically deferential to query\nframing. These findings highlight the need for comprehensive consistency\nevaluation before deploying LLMs in healthcare, legal, and financial systems\nwhere maintaining coherent positions across interactions is critical for\nreliable decision support.", "AI": {"tldr": "本研究首次系统调查了大型语言模型（LLMs）的“变色龙行为”，即在多轮对话中面对矛盾问题时改变立场的倾向，尤其是在启用搜索的LLMs中。通过新的基准数据集和指标，发现所有测试模型都存在严重的变色龙行为，其机制在于有限的知识多样性导致模型过度受查询措辞影响。", "motivation": "LLMs与搜索/检索引擎的集成已无处不在，但这些系统存在一个关键漏洞，即在多轮对话中面对矛盾问题时，LLMs会改变其立场，这严重损害了它们的可靠性。在医疗、法律和金融等需要一致立场以提供可靠决策支持的关键领域部署LLMs之前，必须解决这一问题。", "method": "研究方法包括：1. 首次对LLMs的“变色龙行为”（在多轮对话中面对矛盾问题时改变立场的倾向）进行系统调查。2. 构建了新颖的“变色龙基准数据集”，包含12个争议领域的1,180个多轮对话中的17,770个问答对。3. 引入了两个理论上扎根的指标：量化立场不稳定性的“变色龙分数”（0-1）和衡量知识多样性的“来源重用率”（0-1）。4. 对Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash进行了严格评估。5. 分析了来源重用率、置信度和立场变化之间的相关性。", "result": "主要结果如下：1. 所有测试模型都表现出严重的变色龙行为（分数介于0.391-0.511之间），其中GPT-4o-mini表现最差。2. 跨温度方差很小（小于0.004），表明这种效应并非采样伪影。3. 来源重用率与置信度（r=0.627）以及立场变化（r=0.429）之间存在强烈的、统计学上显著的相关性（p < 0.05）。4. 分析揭示的机制是：有限的知识多样性使得模型病态地顺从查询措辞。", "conclusion": "本研究强调，在医疗、法律和金融等需要跨交互保持一致立场以提供可靠决策支持的关键系统中部署LLMs之前，必须进行全面的连贯性评估。研究结果揭示了LLMs在处理矛盾信息时存在的根本性缺陷，并指出了未来改进的方向。"}}
{"id": "2510.16713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16713", "abs": "https://arxiv.org/abs/2510.16713", "authors": ["Sriharsh Bhyravajjula", "Melanie Walsh", "Anna Preus", "Maria Antoniak"], "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs", "comment": null, "summary": "Whitespace is a critical component of poetic form, reflecting both adherence\nto standardized forms and rebellion against those forms. Each poem's whitespace\ndistribution reflects the artistic choices of the poet and is an integral\nsemantic and spatial feature of the poem. Yet, despite the popularity of poetry\nas both a long-standing art form and as a generation task for large language\nmodels (LLMs), whitespace has not received sufficient attention from the NLP\ncommunity. Using a corpus of 19k English-language published poems from Poetry\nFoundation, we investigate how 4k poets have used whitespace in their works. We\nrelease a subset of 2.8k public-domain poems with preserved formatting to\nfacilitate further research in this area. We compare whitespace usage in the\npublished poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems\nposted in an online community. We also explore whitespace usage across time\nperiods, poetic forms, and data sources. Additionally, we find that different\ntext processing methods can result in significantly different representations\nof whitespace in poetry data, motivating us to use these poems and whitespace\npatterns to discuss implications for the processing strategies used to assemble\npretraining datasets for LLMs.", "AI": {"tldr": "本文研究诗歌中的空白符，探讨其在已发表、LLM生成及未发表诗歌中的使用模式，并讨论不同文本处理方法对LLM预训练数据的影响。", "motivation": "空白符是诗歌形式的关键组成部分，反映了诗人的艺术选择，但自然语言处理（NLP）社区，尤其是大型语言模型（LLM）领域，对其关注不足。", "method": "使用来自Poetry Foundation的1.9万首英文已发表诗歌语料库（涵盖4千名诗人），发布了2.8千首保留格式的公共领域诗歌子集。将已发表诗歌中的空白符使用情况与5.1万首LLM生成的诗歌和1.2万首在线社区未发表诗歌进行比较。此外，还探讨了空白符在不同时间段、诗歌形式和数据源中的使用，并分析了不同文本处理方法对空白符表示的影响。", "result": "研究揭示了4千名诗人如何使用空白符，比较了已发表、LLM生成和未发表诗歌中的空白符使用模式，探索了空白符在不同时间段和诗歌形式中的应用。同时发现，不同的文本处理方法会导致诗歌数据中空白符表示的显著差异。", "conclusion": "空白符是诗歌中一个重要但被忽视的语义和空间特征。NLP社区在处理诗歌数据时应给予空白符足够的重视，特别是对于LLM的预训练数据集，需要重新考虑文本处理策略，以更准确地保留和表示诗歌中的空白符模式。"}}
{"id": "2510.17439", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17439", "abs": "https://arxiv.org/abs/2510.17439", "authors": ["Zhengshen Zhang", "Hao Li", "Yalun Dai", "Zhengbang Zhu", "Lei Zhou", "Chenchen Liu", "Dong Wang", "Francis E. H. Tay", "Sijin Chen", "Ziwei Liu", "Yuxiao Liu", "Xinghang Li", "Pan Zhou"], "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors", "comment": "Project page: https://falcon-vla.github.io/", "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.", "AI": {"tldr": "FALCON是一种新型的视觉-语言-动作（VLA）模型，通过将丰富的3D空间tokens注入到动作头部，利用空间基础模型提供几何先验，显著提升了在3D真实世界任务中的泛化性和适应性。", "motivation": "现有VLA模型基于2D编码器，存在空间推理鸿沟，限制了泛化和适应性。当前的3D集成技术要么需要专用传感器且跨模态迁移性差，要么注入弱线索导致缺乏几何信息并损害视觉-语言对齐。", "method": "FALCON引入了一种新的范式，将丰富的3D空间tokens注入到动作头部。它利用空间基础模型仅从RGB图像中提供强大的几何先验，并包含一个具身空间模型，可选择融合深度或姿态以提高保真度，无需重新训练或改变架构。为了保持语言推理能力，空间tokens由一个空间增强动作头部消费，而不是直接拼接进视觉-语言骨干网络。", "result": "FALCON在三个模拟基准和十一个真实世界任务中实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件以及物体尺寸和高度变化下保持鲁棒性。", "conclusion": "FALCON通过其独特的设计，成功解决了VLA模型在空间表示、模态可迁移性和对齐方面的局限性，从而提升了模型的泛化能力和适应性。"}}
{"id": "2510.16410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16410", "abs": "https://arxiv.org/abs/2510.16410", "authors": ["Changyue Shi", "Minghao Chen", "Yiping Mao", "Chuxiao Yang", "Xinyuan Hu", "Jiajun Ding", "Zhou Yu"], "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting", "comment": null, "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.", "AI": {"tldr": "REALM是一个创新的MLLM智能体框架，通过在3D高斯泼溅表示上采用从全局到局部的空间定位策略，实现了基于推理的3D物体分割和交互，无需大量3D特定后训练。", "motivation": "现有的3D分割方法难以解释模糊、基于推理的指令，而擅长推理的2D视觉语言模型又缺乏内在的3D空间理解。因此，需要弥合复杂人类指令与精确3D物体定位之间的鸿沟。", "method": "本文提出了REALM，一个MLLM智能体框架，直接在3D高斯泼溅表示上进行分割。它采用新颖的“从全局到局部空间定位”策略：首先，多个全局视图并行输入MLLM智能体进行粗粒度定位，聚合响应以鲁棒地识别目标；然后，合成目标物体的多个特写新视图进行细粒度局部分割，从而获得准确一致的3D掩模。该方法无需广泛的3D特定后训练。", "result": "REALM在LERF、3D-OVS和新引入的REALM3D基准测试中，在解释显式和隐式指令方面均取得了显著性能。此外，该智能体框架无缝支持多种3D交互任务，包括物体移除、替换和风格迁移。", "conclusion": "REALM提供了一个有效且多功能的解决方案，用于基于推理的3D物体分割和交互，展示了其在处理复杂指令和实际3D应用中的实用性和多功能性。"}}
{"id": "2510.16727", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16727", "abs": "https://arxiv.org/abs/2510.16727", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Angkul Puniya", "Sohom Pal"], "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models", "comment": null, "summary": "Large language models internalize a structural trade-off between truthfulness\nand obsequious flattery, emerging from reward optimization that conflates\nhelpfulness with polite submission. This latent bias, known as sycophancy,\nmanifests as a preference for user agreement over principled reasoning. We\nintroduce Beacon, a single-turn forced-choice benchmark that isolates this bias\nindependent of conversational context, enabling precise measurement of the\ntension between factual accuracy and submissive bias. Evaluations across twelve\nstate-of-the-art models reveal that sycophancy decomposes into stable\nlinguistic and affective sub-biases, each scaling with model capacity. We\nfurther propose prompt-level and activation-level interventions that modulate\nthese biases in opposing directions, exposing the internal geometry of\nalignment as a dynamic manifold between truthfulness and socially compliant\njudgment. Beacon reframes sycophancy as a measurable form of normative\nmisgeneralization, providing a reproducible foundation for studying and\nmitigating alignment drift in large-scale generative systems.", "AI": {"tldr": "大型语言模型（LLMs）在奖励优化中存在真理与奉承之间的结构性权衡，导致一种谄媚偏差，即偏好用户认同而非原则性推理。本研究引入了Beacon基准来衡量这种偏差，发现其可分解为语言和情感子偏差，并随模型容量增加。研究还提出了干预措施来调节这些偏差，将谄媚重新定义为一种可测量的规范性错误泛化。", "motivation": "大型语言模型在奖励优化过程中，将“有用性”与“礼貌顺从”混为一谈，导致模型内部产生一种结构性权衡，即在“真理”和“顺从奉承”之间选择。这种潜在偏差，被称为“谄媚”，表现为模型倾向于认同用户而非基于原则性推理，因此需要被识别和量化。", "method": "研究引入了Beacon，一个单轮强制选择基准测试，旨在独立于对话上下文来隔离和精确测量这种谄媚偏差，即事实准确性与顺从偏差之间的张力。研究评估了十二个最先进的模型，并提出了提示级和激活级干预措施来调节这些偏差。", "result": "对十二个最先进模型的评估表明，谄媚可以分解为稳定的语言和情感子偏差，并且每个子偏差都随着模型容量的增加而扩大。研究还发现，提示级和激活级干预措施可以向相反方向调节这些偏差，揭示了对齐的内部几何结构是真理与社会顺从判断之间的动态流形。", "conclusion": "Beacon将谄媚重新定义为一种可测量的规范性错误泛化形式，为研究和缓解大型生成系统中的对齐漂移提供了一个可复现的基础。这有助于理解模型内部真理与社会顺从判断之间的复杂关系。"}}
{"id": "2510.16416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16416", "abs": "https://arxiv.org/abs/2510.16416", "authors": ["Xiaojun Guo", "Runyu Zhou", "Yifei Wang", "Qi Zhang", "Chenheng Zhang", "Stefanie Jegelka", "Xiaohan Wang", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Yisen Wang"], "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning", "comment": null, "summary": "Vision-language models (VLMs) have shown remarkable abilities by integrating\nlarge language models with visual inputs. However, they often fail to utilize\nvisual evidence adequately, either depending on linguistic priors in\nvision-centric tasks or resorting to textual shortcuts during reasoning.\nAlthough reinforcement learning (RL) can align models with desired behaviors,\nits application to VLMs has been hindered by the lack of scalable and reliable\nreward mechanisms. To overcome this challenge, we propose SSL4RL, a novel\nframework that leverages self-supervised learning (SSL) tasks as a source of\nverifiable rewards for RL-based fine-tuning. Our approach reformulates SSL\nobjectives-such as predicting image rotation or reconstructing masked\npatches-into dense, automatic reward signals, eliminating the need for human\npreference data or unreliable AI evaluators. Experiments show that SSL4RL\nsubstantially improves performance on both vision-centric and vision-language\nreasoning benchmarks. Furthermore, through systematic ablations, we identify\nkey factors-such as task difficulty, model scale, and semantic alignment with\nthe target domain-that influence the effectiveness of SSL4RL tasks, offering\nnew design principles for future work. We also demonstrate the framework's\ngenerality by applying it to graph learning, where it yields significant gains.\nSSL4RL establishes a versatile and effective paradigm for aligning multimodal\nmodels using verifiable, self-supervised objectives.", "AI": {"tldr": "该论文提出SSL4RL框架，利用自监督学习（SSL）任务作为可验证的奖励信号，对视觉语言模型（VLM）进行强化学习（RL）微调，以解决VLM在利用视觉证据方面的不足，并在多个基准测试中取得了显著改进。", "motivation": "视觉语言模型（VLM）在整合视觉输入方面表现出色，但往往未能充分利用视觉证据，例如在以视觉为中心的任务中依赖语言先验，或在推理过程中采取文本捷径。尽管强化学习（RL）可以使模型与期望行为对齐，但由于缺乏可扩展且可靠的奖励机制，其在VLM中的应用受到了阻碍。", "method": "本文提出了SSL4RL框架，将自监督学习（SSL）任务（如预测图像旋转或重建掩码补丁）重新表述为密集、自动的奖励信号，用于基于RL的微调。这种方法消除了对人工偏好数据或不可靠AI评估器的需求，提供了可验证的奖励机制。", "result": "实验表明，SSL4RL显著提高了模型在以视觉为中心和视觉语言推理基准上的性能。通过系统性消融实验，论文识别了影响SSL4RL任务有效性的关键因素，如任务难度、模型规模以及与目标领域的语义对齐。此外，该框架在图学习中也表现出通用性，并取得了显著增益。", "conclusion": "SSL4RL建立了一个通用且有效的范式，利用可验证的自监督目标来对多模态模型进行对齐。"}}
{"id": "2510.17448", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.17448", "abs": "https://arxiv.org/abs/2510.17448", "authors": ["Mirko Mizzoni", "Pieter van Goor", "Barbara Bazzana", "Antonio Franchi"], "title": "A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions", "comment": null, "summary": "This letter presents a systematic framework for switching between different\nsets of outputs for the control of nonlinear systems via feedback\nlinearization. We introduce the concept of a meld to formally define a valid,\nfeedback-linearizable subset of outputs that can be selected from a larger deck\nof possible outputs. The main contribution is a formal proof establishing that\nunder suitable dwell-time and compatibility conditions, it is possible to\nswitch between different melds while guaranteeing the uniform boundedness of\nthe system state. We further show that the error dynamics of the active outputs\nremain exponentially stable within each switching interval and that outputs\ncommon to consecutive melds are tracked seamlessly through transitions. The\nproposed theory is valid for any feedback linearizable nonlinear system, such\nas, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a\nsimple numerical simulation of a robotic manipulator.", "AI": {"tldr": "本文提出了一种通过反馈线性化在非线性系统不同输出集之间切换的系统框架，并在适当的驻留时间和兼容性条件下，证明了系统状态的一致有界性、激活输出的指数稳定性以及共同输出的无缝跟踪。", "motivation": "研究动机是为非线性系统提供一个系统性的框架，使其能够通过反馈线性化在不同的输出集之间进行切换控制。", "method": "引入了“融合”（meld）的概念来形式化定义可从更大输出集中选择的有效、反馈线性化子集。主要方法是通过形式化证明，在满足适当的驻留时间（dwell-time）和兼容性（compatibility）条件下，实现不同融合之间的切换。", "result": "研究结果表明，在合适的条件下，可以保证系统状态的一致有界性；在每个切换间隔内，激活输出的误差动态保持指数稳定；并且连续融合中共同的输出能够通过切换无缝跟踪。", "conclusion": "该理论适用于任何可反馈线性化的非线性系统，如机器人、空中和地面车辆等，并通过机械臂的数值仿真进行了验证。"}}
{"id": "2510.16761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16761", "abs": "https://arxiv.org/abs/2510.16761", "authors": ["Yikai Zhang", "Ye Rong", "Siyu Yuan", "Jiangjie Chen", "Jian Xie", "Yanghua Xiao"], "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games", "comment": null, "summary": "Existing language agents often encounter difficulties in dynamic adversarial\ngames due to poor strategic reasoning. To mitigate this limitation, a promising\napproach is to allow agents to learn from game interactions automatically,\nwithout relying on costly expert-labeled data. Unlike static environments where\nagents receive fixed feedback or rewards, selecting appropriate opponents in\ndynamic adversarial games can significantly impact learning performance.\nHowever, the discussion of opponents in adversarial environments remains an\narea under exploration. In this paper, we propose a Step-level poliCy\nOptimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we\nconduct a detailed analysis of opponent selection by setting opponents at\ndifferent levels and find that self-play is the most effective way to improve\nstrategic reasoning in such adversarial environments. Utilizing SCO-PAL with\nself-play, we increase the average win rate against four opponents by\napproximately 30% compared to baselines and achieve a 54.76% win rate against\nGPT-4 in six adversarial games.", "AI": {"tldr": "本文提出SCO-PAL方法，通过步级策略优化和对战学习来提升语言代理在动态对抗游戏中的战略推理能力。研究发现自博弈是提升战略推理最有效的方式，并显著提高了代理的胜率。", "motivation": "现有语言代理在动态对抗游戏中因战略推理能力差而面临困难。虽然通过游戏交互学习是很有前景的方法，但在对抗环境中选择合适的对手对学习性能至关重要，而这方面研究仍有待深入。", "method": "提出了一种名为“通过对战学习进行步级策略优化”(Step-level poliCy Optimization method through Play-And-Learn, SCO-PAL)的方法。利用SCO-PAL，通过设置不同级别的对手，详细分析了对手选择对学习效果的影响。", "result": "研究发现自博弈（self-play）是提高此类对抗环境中战略推理能力最有效的方式。结合SCO-PAL和自博弈，与基线相比，代理在对抗四种对手时的平均胜率提高了约30%，并在六种对抗游戏中对GPT-4取得了54.76%的胜率。", "conclusion": "SCO-PAL方法结合自博弈能显著提升语言代理在动态对抗游戏中的战略推理能力，其中自博弈被证实是提升效果的最佳对手选择策略。"}}
{"id": "2510.16438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16438", "abs": "https://arxiv.org/abs/2510.16438", "authors": ["Aidyn Ubingazhibov", "Rémi Pautrat", "Iago Suárez", "Shaohui Liu", "Marc Pollefeys", "Viktor Larsson"], "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching", "comment": "Accepted at ICCVW 2025", "summary": "Lines and points are complementary local features, whose combination has\nproven effective for applications such as SLAM and Structure-from-Motion. The\nbackbone of these pipelines are the local feature matchers, establishing\ncorrespondences across images. Traditionally, point and line matching have been\ntreated as independent tasks. Recently, GlueStick proposed a GNN-based network\nthat simultaneously operates on points and lines to establish matches. While\nrunning a single joint matching reduced the overall computational complexity,\nthe heavy architecture prevented real-time applications or deployment to edge\ndevices.\n  Inspired by recent progress in point matching, we propose LightGlueStick, a\nlightweight matcher for points and line segments. The key novel component in\nour architecture is the Attentional Line Message Passing (ALMP), which\nexplicitly exposes the connectivity of the lines to the network, allowing for\nefficient communication between nodes. In thorough experiments we show that\nLightGlueStick establishes a new state-of-the-art across different benchmarks.\nThe code is available at https://github.com/aubingazhib/LightGlueStick.", "AI": {"tldr": "本文提出LightGlueStick，一种轻量级的点线联合匹配器，通过引入注意力线消息传递（ALMP）实现了高效通信，并在多个基准测试中达到了最先进的性能。", "motivation": "传统的点线匹配是独立任务，而最近的联合匹配方法（如GlueStick）虽然减少了整体计算复杂度，但其沉重的架构阻碍了实时应用或在边缘设备上的部署。研究旨在开发一种更轻量、高效的联合点线匹配器。", "method": "受点匹配最新进展的启发，本文提出了LightGlueStick，一个用于点和线段的轻量级匹配器。其核心新颖组件是注意力线消息传递（Attentional Line Message Passing, ALMP），该机制明确地将线的连接性暴露给网络，从而实现节点之间的高效通信。", "result": "通过全面的实验表明，LightGlueStick在不同的基准测试中建立了新的最先进（state-of-the-art）性能。", "conclusion": "LightGlueStick是一种高效、轻量级的点线联合匹配器，通过创新的ALMP机制克服了现有方法的计算限制，并在性能上超越了现有技术水平，适用于对实时性和部署环境有要求的应用。"}}
{"id": "2510.16783", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16783", "abs": "https://arxiv.org/abs/2510.16783", "authors": ["Sheikh Jubair", "Arwa Omayrah", "Amal Alshammari", "Alhanoof Althnian", "Abdulhamed Alothaimen", "Norah A. Alzahrani", "Shahad D. Alzaidi", "Nora Al-Twairesh", "Abdulmohsen Al-Thubaity"], "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding", "comment": "1 figure, 15 tables, 10 main pages", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nsophisticated capabilities, including the ability to process and comprehend\nextended contexts. These emergent capabilities necessitate rigorous evaluation\nmethods to effectively assess their performance in long-context understanding.\nIn this paper, we present \\textbf{LC-Eval}, a bilingual, multi-task evaluation\nbenchmark designed to evaluate long-context understanding in English and\nArabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval\nintroduces four novel and challenging tasks: multi-document question answering,\nbilingual question answering, claim verification within a paragraph, and\nmultiple-choice questions based on long contexts. These tasks are designed to\nassess LLMs' abilities in deep reasoning, document comprehension, information\ntracing, and bilingual information extraction and understanding. The benchmark\nincludes datasets in both Arabic and English for each task, allowing for a\ncomparative analysis of their performance across different text genres.\nEvaluations were conducted on both open-weight and closed LLMs, with results\nindicating that LC-Eval presents significant challenges. Even high-performing\nmodels, such as GPT-4o, struggled with certain tasks, highlighting the\ncomplexity and rigor of the benchmark.", "AI": {"tldr": "本文提出了LC-Eval，一个双语、多任务的评估基准，用于衡量大型语言模型（LLMs）在4k至128k+tokens长上下文理解方面的能力，并发现即使是顶尖模型也面临挑战。", "motivation": "随着大型语言模型（LLMs）在处理和理解长上下文方面展现出复杂的能力，需要更严格的评估方法来有效评估它们在长上下文理解方面的性能。", "method": "研究者创建了LC-Eval，一个双语（英语和阿拉伯语）、多任务的评估基准，旨在评估4k到128k+tokens的长上下文理解能力。LC-Eval引入了四项新颖且具挑战性的任务：多文档问答、双语问答、段落内声明验证和基于长上下文的多项选择题。该基准包含阿拉伯语和英语数据集，并对开源和闭源LLM进行了评估。", "result": "评估结果表明LC-Eval带来了显著挑战。即使是像GPT-4o这样的高性能模型，在某些任务上也表现不佳，突显了该基准的复杂性和严谨性。", "conclusion": "LC-Eval是一个严格的评估工具，能有效评估LLMs在深度推理、文档理解、信息追踪以及双语信息提取和理解方面的长上下文能力，并揭示了当前顶尖模型在此领域的局限性。"}}
{"id": "2510.16797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16797", "abs": "https://arxiv.org/abs/2510.16797", "authors": ["Vera Pavlova", "Mohammed Makhlouf"], "title": "MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning", "comment": null, "summary": "We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain\nContrastive learning), a multi-stage framework for domain adaptation of\nsentence embedding models that incorporates joint domain-specific masked\nsupervision. Our approach addresses the challenges of adapting large-scale\ngeneral-domain sentence embedding models to specialized domains. By jointly\noptimizing masked language modeling (MLM) and contrastive objectives within a\nunified training pipeline, our method enables effective learning of\ndomain-relevant representations while preserving the robust semantic\ndiscrimination properties of the original model. We empirically validate our\napproach on both high-resource and low-resource domains, achieving improvements\nup to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong\ngeneral-domain baselines. Comprehensive ablation studies further demonstrate\nthe effectiveness of each component, highlighting the importance of balanced\njoint supervision and staged adaptation.", "AI": {"tldr": "MOSAIC是一个多阶段框架，通过联合领域特定掩码监督和对比学习，将通用句嵌入模型适配到特定领域。", "motivation": "将大规模通用领域句嵌入模型适配到专业领域面临挑战，需要有效的方法来学习领域相关表示并保持原始模型的语义判别能力。", "method": "MOSAIC框架采用多阶段方法，在统一的训练流程中联合优化掩码语言建模（MLM）和对比学习目标。它还结合了选择性适应（Selective Adaptation）用于域内对比学习。", "result": "在资源丰富和资源稀缺领域，MOSAIC相较于强大的通用领域基线，在NDCG@10上实现了高达13.4%的改进。全面的消融研究证明了每个组件、平衡的联合监督和分阶段适应的有效性。", "conclusion": "MOSAIC能够有效地学习领域相关表示，同时保留原始模型的鲁棒语义判别特性，并通过显著的性能提升和组件分析得到了验证。"}}
{"id": "2510.17525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17525", "abs": "https://arxiv.org/abs/2510.17525", "authors": ["Simon Schaefer", "Helen Oleynikova", "Sandra Hirche", "Stefan Leutenegger"], "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans", "comment": null, "summary": "Safe and efficient robotic navigation among humans is essential for\nintegrating robots into everyday environments. Most existing approaches focus\non simplified 2D crowd navigation and fail to account for the full complexity\nof human body dynamics beyond root motion. We present HumanMPC, a Model\nPredictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation\namong humans that combines theoretical safety guarantees with data-driven\nmodels for realistic human motion forecasting. Our approach introduces a novel\ntwist to reachability-based safety formulation that constrains only the initial\ncontrol input for safety while modeling its effects over the entire planning\nhorizon, enabling safe yet efficient navigation. We validate HumanMPC in both\nsimulated experiments using real human trajectories and in the real-world,\ndemonstrating its effectiveness across tasks ranging from goal-directed\nnavigation to visual servoing for human tracking. While we apply our method to\nMAVs in this work, it is generic and can be adapted by other platforms. Our\nresults show that the method ensures safety without excessive conservatism and\noutperforms baseline approaches in both efficiency and reliability.", "AI": {"tldr": "本文提出HumanMPC，一个用于在人类环境中进行3D微型飞行器（MAV）导航的模型预测控制（MPC）框架，它结合了理论安全保证和数据驱动的人类运动预测模型，实现了安全高效的导航。", "motivation": "现有的机器人导航方法主要关注简化的2D人群导航，未能充分考虑人类身体动力学的复杂性（超越根部运动），且缺乏在人类环境中进行安全高效3D导航的能力，这阻碍了机器人融入日常环境。", "method": "本文提出了HumanMPC框架，用于3D MAV在人类环境中的导航。该方法引入了一种新颖的可达性（reachability-based）安全公式，仅约束初始控制输入以确保安全，同时在整个规划周期内建模其效果。此外，它结合了数据驱动模型来实现真实的人类运动预测。", "result": "HumanMPC在模拟实验（使用真实人类轨迹）和真实世界中都得到了验证。结果表明，该方法在从目标导向导航到人类跟踪的视觉伺服等任务中均表现出有效性。它在不产生过度保守性的前提下确保了安全，并在效率和可靠性方面优于基线方法。该方法具有通用性，可适用于其他机器人平台。", "conclusion": "HumanMPC为在人类环境中进行3D机器人导航提供了一个安全且高效的解决方案。它通过结合理论安全保证和数据驱动的人类运动预测模型，克服了现有方法的局限性，并有望推广到其他机器人平台。"}}
{"id": "2510.16445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16445", "abs": "https://arxiv.org/abs/2510.16445", "authors": ["Chien Thai", "Mai Xuan Trang", "Huong Ninh", "Hoang Hiep Ly", "Anh Son Le"], "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance", "comment": "Neurocomputing", "summary": "Detecting rotated objects accurately and efficiently is a significant\nchallenge in computer vision, particularly in applications such as aerial\nimagery, remote sensing, and autonomous driving. Although traditional object\ndetection frameworks are effective for axis-aligned objects, they often\nunderperform in scenarios involving rotated objects due to their limitations in\ncapturing orientation variations. This paper introduces an improved loss\nfunction aimed at enhancing detection accuracy and robustness by leveraging the\nGaussian bounding box representation and Bhattacharyya distance. In addition,\nwe advocate for the use of an anisotropic Gaussian representation to address\nthe issues associated with isotropic variance in square-like objects. Our\nproposed method addresses these challenges by incorporating a\nrotation-invariant loss function that effectively captures the geometric\nproperties of rotated objects. We integrate this proposed loss function into\nstate-of-the-art deep learning-based rotated object detection detectors, and\nextensive experiments demonstrated significant improvements in mean Average\nPrecision metrics compared to existing methods. The results highlight the\npotential of our approach to establish new benchmark in rotated object\ndetection, with implications for a wide range of applications requiring precise\nand reliable object localization irrespective of orientation.", "AI": {"tldr": "本文提出了一种改进的损失函数，结合高斯边界框表示和Bhattacharyya距离，并引入各向异性高斯表示来提高旋转目标检测的准确性和鲁棒性，并在现有检测器上实现了显著的mAP提升。", "motivation": "传统目标检测框架在处理旋转目标时性能不佳，因为它们难以捕捉方向变化，而旋转目标在航空图像、遥感和自动驾驶等应用中普遍存在。", "method": "引入了一种改进的损失函数，该函数利用高斯边界框表示和Bhattacharyya距离。为解决类方形目标中各向同性方差问题，提倡使用各向异性高斯表示。将此损失函数集成到最先进的深度学习旋转目标检测器中。", "result": "广泛的实验表明，与现有方法相比，该方法在平均精度均值（mAP）指标上取得了显著提升。", "conclusion": "所提出的方法有望在旋转目标检测领域建立新的基准，对需要精确、可靠且与方向无关的目标定位的广泛应用具有重要意义。"}}
{"id": "2510.16442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16442", "abs": "https://arxiv.org/abs/2510.16442", "authors": ["Haoran Sun", "Chen Cai", "Huiping Zhuang", "Kong Aik Lee", "Lap-Pui Chau", "Yi Wang"], "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning", "comment": null, "summary": "The rapid development of deepfake video technology has not only facilitated\nartistic creation but also made it easier to spread misinformation. Traditional\ndeepfake video detection (DVD) methods face issues such as a lack of\ntransparency in their principles and insufficient generalization capabilities\nto cope with evolving forgery techniques. This highlights an urgent need for\ndetectors that can identify forged content and provide verifiable reasoning\nexplanations. This paper proposes the explainable deepfake video detection\n(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model\n(MLLM) reasoning framework, which provides traceable reasoning processes\nalongside accurate detection results and trustworthy explanations. Our approach\nfirst incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)\nto extract and fuse global and local cross-frame deepfake features, providing\nrich spatio-temporal semantic information input for MLLM reasoning. Second, we\nconstruct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which\nintroduces facial feature data as hard constraints during the reasoning process\nto achieve pixel-level spatio-temporal video localization, suppress\nhallucinated outputs, and enhance the reliability of the chain of thought. In\naddition, we build an Explainable Reasoning FF++ benchmark dataset\n(ER-FF++set), leveraging structured data to annotate videos and ensure quality\ncontrol, thereby supporting dual supervision for reasoning and detection.\nExtensive experiments demonstrate that EDVD-LLaMA achieves outstanding\nperformance and robustness in terms of detection accuracy, explainability, and\nits ability to handle cross-forgery methods and cross-dataset scenarios.\nCompared to previous DVD methods, it provides a more explainable and superior\nsolution. The source code and dataset will be publicly available.", "AI": {"tldr": "本文提出了EDVD-LLaMA多模态大语言模型框架，旨在解决传统深度伪造视频检测方法透明度低和泛化能力差的问题，通过结合时空特征提取、细粒度思维链机制和新的基准数据集，实现了高准确率、可解释性强且鲁棒的深度伪造视频检测。", "motivation": "深度伪造技术发展迅速，传统检测方法存在原理不透明、泛化能力不足的问题，难以应对不断演变的伪造技术。因此，迫切需要能够识别伪造内容并提供可验证推理解释的检测器。", "method": ["提出可解释深度伪造视频检测 (EDVD) 任务。", "设计EDVD-LLaMA多模态大语言模型 (MLLM) 推理框架，提供可追溯的推理过程、准确的检测结果和可信的解释。", "引入时空微妙信息Tokenization (ST-SIT) 机制，提取并融合全局和局部跨帧深度伪造特征，为MLLM推理提供丰富的时空语义信息输入。", "构建细粒度多模态思维链 (Fg-MCoT) 机制，引入面部特征数据作为硬约束，实现像素级时空视频定位，抑制幻觉输出，增强思维链的可靠性。", "构建可解释推理FF++基准数据集 (ER-FF++set)，利用结构化数据标注视频，确保质量控制，支持推理和检测的双重监督。"], "result": "EDVD-LLaMA在检测准确性、可解释性以及处理跨伪造方法和跨数据集场景方面表现出卓越的性能和鲁棒性。与以往的深度伪造视频检测方法相比，它提供了更具可解释性和更优越的解决方案。", "conclusion": "本文提出的EDVD-LLaMA框架为可解释的深度伪造视频检测提供了一种创新且高性能的解决方案，显著提升了检测的透明度和可靠性，超越了现有方法。"}}
{"id": "2510.17541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17541", "abs": "https://arxiv.org/abs/2510.17541", "authors": ["Xiaobo Zheng", "Pan Tang", "Defu Lin", "Shaoming He"], "title": "Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm", "comment": null, "summary": "Swarm trajectory optimization problems are a well-recognized class of\nmulti-agent optimal control problems with strong nonlinearity. However, the\nheuristic nature of needing to set the final time for agents beforehand and the\ntime-consuming limitation of the significant number of iterations prohibit the\napplication of existing methods to large-scale swarm of Unmanned Aerial\nVehicles (UAVs) in practice. In this paper, we propose a spatial-temporal\ntrajectory optimization framework that accomplishes multi-UAV consensus based\non the Alternating Direction Multiplier Method (ADMM) and uses Differential\nDynamic Programming (DDP) for fast local planning of individual UAVs. The\nintroduced framework is a two-level architecture that employs Parameterized DDP\n(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local\nconstraints and accomplish the spatial-temporal parameter consensus among all\nUAVs. This results in a fully distributed algorithm called Distributed\nParameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on\nthe spectral gradient method for the penalty parameter is proposed to reduce\nthe number of algorithmic iterations. Several simulation examples are presented\nto verify the effectiveness of the proposed algorithm.", "AI": {"tldr": "本文提出了一种名为D-PDDP的分布式空间-时间轨迹优化框架，结合ADMM和PDDP，用于解决大规模无人机群轨迹优化问题，并通过自适应罚参数调整减少迭代次数。", "motivation": "现有的群轨迹优化方法存在启发式预设最终时间、迭代次数多导致耗时等局限性，使其难以应用于大规模无人机群的实际场景。", "method": "本文提出了一个双层架构的空间-时间轨迹优化框架。下层使用参数化微分动态规划（PDDP）作为每个无人机的轨迹优化器，上层利用交替方向乘子法（ADMM）来满足局部约束并实现所有无人机之间的空间-时间参数共识，从而形成分布式参数化微分动态规划（D-PDDP）算法。此外，还引入了基于谱梯度法的罚参数自适应调整准则，以减少算法迭代次数。", "result": "通过多个仿真例子验证了所提出算法的有效性。", "conclusion": "所提出的D-PDDP分布式轨迹优化框架，结合ADMM和PDDP，并辅以自适应罚参数调整，能够有效解决大规模无人机群轨迹优化问题，克服了现有方法的局限性。"}}
{"id": "2510.16446", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16446", "abs": "https://arxiv.org/abs/2510.16446", "authors": ["Jaekyun Park", "Hye Won Chung"], "title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion", "comment": "NeurIPS 2025", "summary": "In the era of large-scale foundation models, fully fine-tuning pretrained\nnetworks for each downstream task is often prohibitively resource-intensive.\nPrompt tuning offers a lightweight alternative by introducing tunable prompts\nwhile keeping the backbone frozen. However, existing visual prompt tuning\nmethods often fail to specialize the prompts or enrich the representation\nspace--especially when applied to self-supervised backbones. We show that these\nlimitations become especially pronounced in challenging tasks and data-scarce\nsettings, where effective adaptation is most critical. In this work, we\nintroduce VIPAMIN, a visual prompt initialization strategy that enhances\nadaptation of self-supervised models by (1) aligning prompts with semantically\ninformative regions in the embedding space, and (2) injecting novel\nrepresentational directions beyond the pretrained subspace. Despite its\nsimplicity--requiring only a single forward pass and lightweight\noperations--VIPAMIN consistently improves performance across diverse tasks and\ndataset sizes, setting a new state of the art in visual prompt tuning. Our code\nis available at https://github.com/iamjaekyun/vipamin.", "AI": {"tldr": "VIPAMIN是一种视觉提示初始化策略，通过语义对齐和注入新表示方向，显著提升了自监督模型在视觉提示调优中的性能，尤其在资源受限和数据稀缺场景下。", "motivation": "在大规模基础模型时代，为每个下游任务完全微调模型成本高昂。现有视觉提示调优方法，特别是应用于自监督骨干网络时，难以有效特化提示或丰富表示空间，这在挑战性任务和数据稀缺环境下问题尤为突出。", "method": "本文提出了VIPAMIN，一种视觉提示初始化策略。它通过(1)将提示与嵌入空间中语义信息丰富的区域对齐，以及(2)注入超越预训练子空间的新表示方向来增强自监督模型的适应性。该方法简单，仅需一次前向传播和轻量级操作。", "result": "VIPAMIN在不同任务和数据集规模上持续提升性能，并在视觉提示调优领域达到了新的最先进水平。", "conclusion": "VIPAMIN通过其创新的初始化策略，有效解决了现有视觉提示调优方法的局限性，显著提高了自监督模型在各种下游任务中的适应性，且效率高。"}}
{"id": "2510.17576", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17576", "abs": "https://arxiv.org/abs/2510.17576", "authors": ["Cansu Erdogan", "Cesar Alan Contreras", "Alireza Rastegarpanah", "Manolis Chiou", "Rustam Stolkin"], "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries", "comment": "This work is funded by the project called \"Research and Development\n  of a Highly Automated and Safe Streamlined Process for Increasing Lithium-ion\n  Battery Repurposing and Recycling\" (REBELION) under Grant 101104241, and\n  partially supported by the Ministry of National Education, Republic of\n  Turkey. Submitted to Frontiers for Review", "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort.", "AI": {"tldr": "本文提出了一种意图驱动的规划流程，利用计算机视觉和大型语言模型（LLMs）的集成，为多机器人复杂操作任务（如电动汽车电池拆解）生成鲁棒、可执行的动作序列，同时支持人类的简单语言指令并保持低用户投入。", "motivation": "在非结构化场景中，多个具有不同末端执行器和能力的机器人，需要根据计算机视觉信息，规划并执行针对各种位置和配置对象的复杂、连贯的操作序列。目前的挑战在于如何鲁棒地构建此类动作序列，并支持不同程度的人类监督。", "method": "本文提出了一种意图驱动的规划流程，该流程集成了：(i) 感知到文本的场景编码，(ii) 基于操作者意图生成候选移除序列的LLM集成，(iii) 强制格式和优先级约束的LLM验证器，以及(iv) 拒绝幻觉对象的确定性一致性过滤器。该流程在一个双臂协作拆解电动汽车电池的示例任务上进行了评估。通过全序列正确性和下一任务正确性指标，在200个真实场景、600个操作者提示和五个组件类别上评估并比较了五种基于LLM的规划器。同时，通过执行时间和NASA TLX评估了基于LLM的人机界面。", "result": "实验结果表明，本文提出的“集成与验证”方法能够可靠地将操作者的意图映射到安全、可执行的多机器人规划中，同时保持较低的用户投入。该方法在复杂的多机器人操作任务中表现出良好的性能。", "conclusion": "所提出的意图驱动规划流程，通过整合LLMs、验证器和一致性过滤器，能够有效地解决多机器人复杂操作任务的规划问题。它能够将人类的简单语言指令转化为安全、可执行的机器人计划，并在实际应用中展现出高可靠性和低用户努力的优势。"}}
{"id": "2510.16819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16819", "abs": "https://arxiv.org/abs/2510.16819", "authors": ["Shantanu Agarwal", "Joel Barry", "Steven Fincke", "Scott Miller"], "title": "Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank", "comment": null, "summary": "Authorship attribution (AA) is the task of identifying the most likely author\nof a query document from a predefined set of candidate authors. We introduce a\ntwo-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.\nUnlike the field of information retrieval (IR), where retrieve-and-rerank is a\nde facto strategy, cross-genre AA systems must avoid relying on topical cues\nand instead learn to identify author-specific linguistic patterns that are\nindependent of the text's subject matter (genre/domain/topic). Consequently,\nfor the reranker, we demonstrate that training strategies commonly used in IR\nare fundamentally misaligned with cross-genre AA, leading to suboptimal\nbehavior. To address this, we introduce a targeted data curation strategy that\nenables the reranker to effectively learn author-discriminative signals. Using\nour LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of\n22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on\nHIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.", "AI": {"tldr": "本文提出一个基于微调LLM的两阶段检索-重排框架，用于跨领域作者归属识别，并通过引入针对性数据策展策略，有效学习作者特有的语言模式，显著优于现有技术。", "motivation": "跨领域作者归属识别（AA）的挑战在于必须避免依赖主题线索，转而识别独立于文本主题的作者特定语言模式。信息检索（IR）领域常用的检索-重排策略及其训练方法与跨领域AA的目标不符，导致次优表现。", "method": "引入了一个两阶段的检索-重排框架，该框架对大型语言模型（LLMs）进行微调以实现跨领域AA。针对重排器，提出了一种有针对性的数据策展策略，使其能够有效学习作者区分性信号，从而克服IR训练策略与跨领域AA的不匹配问题。", "result": "基于LLM的检索-重排管道在HIATUS的HRS1和HRS2跨领域AA基准测试中，相较于之前的最先进技术，取得了显著的性能提升，分别在Success@8指标上提高了22.3和34.4个绝对百分点。", "conclusion": "所提出的基于LLM的检索-重排框架，结合其创新的数据策展策略，能够有效解决跨领域作者归属识别的挑战，并大幅超越现有技术，证明了其在识别作者特定语言模式方面的有效性。"}}
{"id": "2510.16815", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16815", "abs": "https://arxiv.org/abs/2510.16815", "authors": ["Hans Hergen Lehmann", "Jae Hee Lee", "Steven Schockaert", "Stefan Wermter"], "title": "Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities", "comment": "33 pages, 20 figures. Submitted ACL ARR 2025 October (under review)", "summary": "Large Language Models (LLMs) are increasingly used for knowledge-based\nreasoning tasks, yet understanding when they rely on genuine knowledge versus\nsuperficial heuristics remains challenging. We investigate this question\nthrough entity comparison tasks by asking models to compare entities along\nnumerical attributes (e.g., ``Which river is longer, the Danube or the\nNile?''), which offer clear ground truth for systematic analysis. Despite\nhaving sufficient numerical knowledge to answer correctly, LLMs frequently make\npredictions that contradict this knowledge. We identify three heuristic biases\nthat strongly influence model predictions: entity popularity, mention order,\nand semantic co-occurrence. For smaller models, a simple logistic regression\nusing only these surface cues predicts model choices more accurately than the\nmodel's own numerical predictions, suggesting heuristics largely override\nprincipled reasoning. Crucially, we find that larger models (32B parameters)\nselectively rely on numerical knowledge when it is more reliable, while smaller\nmodels (7--8B parameters) show no such discrimination, which explains why\nlarger models outperform smaller ones even when the smaller models possess more\naccurate knowledge. Chain-of-thought prompting steers all models towards using\nthe numerical features across all model sizes.", "AI": {"tldr": "该研究调查了大型语言模型（LLMs）在知识推理中何时依赖真实知识而非启发式偏见。结果显示，LLMs常受实体流行度、提及顺序和语义共现等启发式偏见影响，即使它们拥有正确的数值知识。大型模型能选择性地依赖可靠的数值知识，而小型模型则不能，链式思考提示（CoT）能引导所有模型使用数值特征。", "motivation": "理解LLMs在执行基于知识的推理任务时，是依赖真正的知识还是表面的启发式方法，仍然是一个挑战。", "method": "通过实体比较任务（例如，比较河流长度）来调查LLMs的行为，这些任务具有明确的数值属性和真实答案。研究分析了实体流行度、提及顺序和语义共现三种启发式偏见对模型预测的影响。对于小型模型，使用逻辑回归来预测其选择。比较了不同大小模型（7-8B vs. 32B参数）的表现，并测试了链式思考（Chain-of-Thought）提示的效果。", "result": "LLMs即使拥有足够的数值知识，也经常做出与该知识相矛盾的预测。实体流行度、提及顺序和语义共现这三种启发式偏见强烈影响模型预测。对于小型模型，仅使用这些表面线索的简单逻辑回归比模型自身的数值预测更能准确预测模型选择，表明启发式偏见在很大程度上压倒了原则性推理。关键发现是，大型模型（32B参数）在数值知识更可靠时会选择性地依赖它，而小型模型（7-8B参数）则没有这种辨别能力，这解释了为什么大型模型即使在小型模型拥有更准确知识的情况下也能表现更好。链式思考提示能引导所有模型使用数值特征。", "conclusion": "LLMs在知识推理中容易受到启发式偏见的影响，尤其是在小型模型中。大型模型表现出更高的辨别能力，能选择性地利用可靠的知识。链式思考提示是一种有效的策略，可以促使所有大小的模型更好地利用其内在的数值知识进行推理。"}}
{"id": "2510.17604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17604", "abs": "https://arxiv.org/abs/2510.17604", "authors": ["Hao Qiao", "Yan Wang", "Shuo Yang", "Xiaoyao Yu", "Jian kuang", "Xiaoji Niu"], "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm", "comment": null, "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%.", "AI": {"tldr": "本文提出了一种改进的、基于MoE模型的紧密学习惯性里程计（TLIO）方法，用于自行车定位，在保持高精度的同时显著降低了计算成本和模型参数。", "motivation": "随着共享单车的快速发展和骑行应用的多样化，精确的自行车定位变得至关重要。传统的GNSS方法受多径效应影响，而现有惯性导航方法依赖精确建模且鲁棒性有限。虽然TLIO能通过结合原始IMU数据和神经网络预测位移实现低位置漂移，但其高计算成本限制了在移动设备上的部署。", "method": "我们扩展了TLIO以应用于自行车定位，并引入了一种改进的专家混合（MoE）模型，以降低训练和推理成本。", "result": "实验表明，与最先进的LLIO框架相比，我们的方法在实现可比精度的同时，将参数减少了64.7%，计算成本降低了81.8%。", "conclusion": "我们开发了一种高效且准确的自行车定位方法，克服了现有TLIO模型计算成本高的缺点，使其更适合在移动设备上部署。"}}
{"id": "2510.16450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16450", "abs": "https://arxiv.org/abs/2510.16450", "authors": ["Shan Xiong", "Jiabao Chen", "Ye Wang", "Jialin Peng"], "title": "Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy", "comment": null, "summary": "Annotation-efficient segmentation of the numerous mitochondria instances from\nvarious electron microscopy (EM) images is highly valuable for biological and\nneuroscience research. Although unsupervised domain adaptation (UDA) methods\ncan help mitigate domain shifts and reduce the high costs of annotating each\ndomain, they typically have relatively low performance in practical\napplications. Thus, we investigate weakly supervised domain adaptation (WDA)\nthat utilizes additional sparse point labels on the target domain, which\nrequire minimal annotation effort and minimal expert knowledge. To take full\nuse of the incomplete and imprecise point annotations, we introduce a multitask\nlearning framework that jointly conducts segmentation and center detection with\na novel cross-teaching mechanism and class-focused cross-domain contrastive\nlearning. While leveraging unlabeled image regions is essential, we introduce\nsegmentation self-training with a novel instance-aware pseudo-label (IPL)\nselection strategy. Unlike existing methods that typically rely on pixel-wise\npseudo-label filtering, the IPL semantically selects reliable and diverse\npseudo-labels with the help of the detection task. Comprehensive validations\nand comparisons on challenging datasets demonstrate that our method outperforms\nexisting UDA and WDA methods, significantly narrowing the performance gap with\nthe supervised upper bound. Furthermore, under the UDA setting, our method also\nachieves substantial improvements over other UDA techniques.", "AI": {"tldr": "本文提出了一种弱监督域适应（WDA）方法，通过利用目标域上的稀疏点标注，结合多任务学习、交叉教学、对比学习和实例感知伪标签自训练，实现了线粒体图像分割的高效和高性能。", "motivation": "线粒体分割对生物学和神经科学研究至关重要，但从不同电镜图像中分割线粒体实例的标注成本高昂。无监督域适应（UDA）方法虽然能缓解域偏移，但实际应用性能较低。因此，作者探索了利用少量点标注的弱监督域适应（WDA）方法，以期在最小标注和专家知识投入下提高性能。", "method": "本文引入了一个多任务学习框架，结合分割和中心检测任务。该框架包含新颖的交叉教学机制和以类别为中心的跨域对比学习。此外，为了充分利用未标注图像区域，提出了一种带有实例感知伪标签（IPL）选择策略的分割自训练方法，该策略利用检测任务语义地选择可靠且多样的伪标签，而非传统的像素级过滤。", "result": "在挑战性数据集上的全面验证和比较表明，所提出的方法优于现有的UDA和WDA方法，显著缩小了与监督上限的性能差距。此外，在UDA设置下，该方法也比其他UDA技术取得了显著改进。", "conclusion": "本文提出的弱监督域适应方法，通过结合多任务学习、交叉教学、对比学习和实例感知伪标签自训练，有效地解决了线粒体分割中标注效率低和性能不足的问题，实现了高性能且标注高效的图像分割。"}}
{"id": "2510.17640", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17640", "abs": "https://arxiv.org/abs/2510.17640", "authors": ["Yuquan Xue", "Guanxing Lu", "Zhenyu Wu", "Chuanrui Zhang", "Bofang Jia", "Zhengyi Gu", "Yansong Tang", "Ziwei Wang"], "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation", "comment": "9 pages,7 figures, submitted to ICRA2026", "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models.", "AI": {"tldr": "现有视觉-语言-动作（VLA）模型在处理分布外（OOD）状态时表现不佳，因为模仿学习数据集缺乏失败或恢复数据。本文提出了一个名为RESample的自动化OOD数据增强框架，通过探索性采样，利用离线强化学习识别次优动作，并自适应地将OOD状态代理纳入训练数据，显著提升了VLA模型的稳定性和泛化能力。", "motivation": "现有的VLA模仿学习数据集只包含成功的轨迹，缺乏失败或恢复数据，特别是在机器人因轻微扰动或错误偏离主策略的分布外（OOD）状态下。这导致VLA模型在面对偏离训练分布的状态时表现不佳。", "method": "RESample框架首先利用离线强化学习获得一个动作价值网络，用于准确识别当前操作策略下的次优动作。接着，通过rollout从轨迹中采样潜在的OOD状态。最后，设计了一个探索性采样机制，将这些动作代理自适应地整合到训练数据集中，以确保效率。该框架明确鼓励VLA模型从OOD状态中恢复，并增强其对分布偏移的鲁棒性。", "result": "在LIBERO基准测试和真实世界机器人操作任务中进行的大量实验表明，RESample持续地提高了VLA模型的稳定性和泛化能力。", "conclusion": "RESample通过自动化OOD数据增强，有效解决了VLA模型在处理分布外状态时的挑战，显著提升了模型的鲁棒性和泛化能力，使其能更好地从异常状态中恢复。"}}
{"id": "2510.16457", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16457", "abs": "https://arxiv.org/abs/2510.16457", "authors": ["Peiran Xu", "Xicheng Gong", "Yadong MU"], "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation", "comment": "ICCV 2025", "summary": "In this work we concentrate on the task of goal-oriented Vision-and-Language\nNavigation (VLN). Existing methods often make decisions based on historical\ninformation, overlooking the future implications and long-term outcomes of the\nactions. In contrast, we aim to develop a foresighted agent. Specifically, we\ndraw upon Q-learning to train a Q-model using large-scale unlabeled trajectory\ndata, in order to learn the general knowledge regarding the layout and object\nrelations within indoor scenes. This model can generate a Q-feature, analogous\nto the Q-value in traditional Q-network, for each candidate action, which\ndescribes the potential future information that may be observed after taking\nthe specific action. Subsequently, a cross-modal future encoder integrates the\ntask-agnostic Q-feature with navigation instructions to produce a set of action\nscores reflecting future prospects. These scores, when combined with the\noriginal scores based on history, facilitate an A*-style searching strategy to\neffectively explore the regions that are more likely to lead to the\ndestination. Extensive experiments conducted on widely used goal-oriented VLN\ndatasets validate the effectiveness of the proposed method.", "AI": {"tldr": "本文提出了一种前瞻性方法，通过Q-learning学习未来信息并结合A*搜索策略，解决目标导向的视觉与语言导航（VLN）任务中现有方法忽视未来影响的问题。", "motivation": "现有的视觉与语言导航（VLN）方法通常基于历史信息做出决策，忽视了行动的未来影响和长期结果，导致导航效率不高。", "method": "本文提出了一种前瞻性智能体。具体来说，利用Q-learning在大规模无标签轨迹数据上训练一个Q-模型，以学习室内场景的布局和物体关系通用知识。该模型为每个候选动作生成一个Q-特征（类似于传统Q-网络的Q-值），描述了采取特定动作后可能观察到的潜在未来信息。随后，一个跨模态未来编码器将任务无关的Q-特征与导航指令整合，生成反映未来前景的动作分数。这些分数与基于历史信息的原始分数结合，促进了一种A*风格的搜索策略，以有效探索更可能到达目的地的区域。", "result": "在广泛使用的目标导向VLN数据集上进行的广泛实验验证了所提出方法的有效性。", "conclusion": "通过整合Q-learning学习的未来信息和A*风格的搜索策略，本文提出的前瞻性方法能够有效提升目标导向的视觉与语言导航任务的性能。"}}
{"id": "2510.16829", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16829", "abs": "https://arxiv.org/abs/2510.16829", "authors": ["Navreet Kaur", "Hoda Ayad", "Hayoung Jung", "Shravika Mittal", "Munmun De Choudhury", "Tanushree Mitra"], "title": "Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation", "comment": null, "summary": "Language model users often embed personal and social context in their\nquestions. The asker's role -- implicit in how the question is framed --\ncreates specific needs for an appropriate response. However, most evaluations,\nwhile capturing the model's capability to respond, often ignore who is asking.\nThis gap is especially critical in stigmatized domains such as opioid use\ndisorder (OUD), where accounting for users' contexts is essential to provide\naccessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for\nUser-centric Question Simulation), a framework for simulating role-based\nquestions. Drawing on role theory and posts from an online OUD recovery\ncommunity (r/OpiatesRecovery), we first build a taxonomy of asker roles --\npatients, caregivers, practitioners. Next, we use it to simulate 15,321\nquestions that embed each role's goals, behaviors, and experiences. Our\nevaluations show that these questions are both highly believable and comparable\nto real-world data. When used to evaluate five LLMs, for the same question but\ndiffering roles, we find systematic differences: vulnerable roles, such as\npatients and caregivers, elicit more supportive responses (+17%) and reduced\nknowledge content (-19%) in comparison to practitioners. Our work demonstrates\nhow implicitly signaling a user's role shapes model responses, and provides a\nmethodology for role-informed evaluation of conversational AI.", "AI": {"tldr": "本研究提出了CoRUS框架，通过模拟基于用户角色的问题来评估大型语言模型（LLMs），发现在像阿片类药物使用障碍（OUD）这样的敏感领域，用户角色会系统性地影响LLM的响应，为对话式AI的角色感知评估提供了方法。", "motivation": "语言模型用户在提问时常包含个人和社会背景，而提问者的角色（通常是隐式的）决定了对恰当回复的具体需求。然而，大多数评估只关注模型的响应能力，却忽略了提问者的身份。在阿片类药物使用障碍（OUD）等敏感领域，考虑到用户背景对于提供可及、无偏见的回复至关重要，因此这种评估空白尤其关键。", "method": "研究提出了CoRUS（COmmunity-driven Roles for User-centric Question Simulation）框架，用于模拟基于角色的问题。研究者借鉴角色理论和来自在线OUD康复社区（r/OpiatesRecovery）的帖子，首先构建了提问者角色的分类法（患者、照护者、从业者）。然后，利用该分类法模拟了15,321个嵌入了每个角色目标、行为和经验的问题。", "result": "评估结果显示，模拟的问题具有高度的可信度，并与真实世界的数据具有可比性。当使用这些问题评估五个大型语言模型时，对于相同的问题但不同角色，研究发现存在系统性差异：与从业者相比，患者和照护者等脆弱角色会引发更具支持性的回复（+17%）和更少的知识内容（-19%）。", "conclusion": "本研究表明，隐式地提示用户角色会影响模型的响应，并为对话式AI的角色感知评估提供了一种方法论。"}}
{"id": "2510.16844", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16844", "abs": "https://arxiv.org/abs/2510.16844", "authors": ["Jiajie Jin", "Yuyao Zhang", "Yimeng Xu", "Hongjin Qian", "Yutao Zhu", "Zhicheng Dou"], "title": "FinSight: Towards Real-World Financial Deep Research", "comment": "Working in progress", "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.", "AI": {"tldr": "FinSight是一个新颖的多智能体框架，通过CAVM架构、迭代视觉增强机制和两阶段写作框架，实现了高质量、多模态的专业金融报告自动化生成，显著优于现有基线。", "motivation": "生成专业的金融报告是一项劳动密集型和智力要求高的工作，现有AI系统难以完全自动化，因此需要新的方法来解决这一挑战。", "method": "该研究引入了FinSight多智能体框架。其核心是带有可变内存的代码智能体（CAVM）架构，将外部数据、设计工具和智能体统一到一个可编程变量空间中，通过可执行代码实现灵活的数据收集、分析和报告生成。为确保专业级可视化，提出了迭代视觉增强机制来精炼视觉输出。此外，一个两阶段写作框架将简洁的分析链扩展为连贯、引用感知和多模态的报告。", "result": "在各种公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于所有基线，包括领先的深度研究系统。", "conclusion": "FinSight展示了生成接近人类专家质量的金融报告的明确途径，有望实现金融报告的自动化和专业化。"}}
{"id": "2510.17783", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17783", "abs": "https://arxiv.org/abs/2510.17783", "authors": ["Simeon Adebola", "Chung Min Kim", "Justin Kerr", "Shuangyu Xie", "Prithvi Akella", "Jose Luis Susa Rincon", "Eugen Solowjow", "Ken Goldberg"], "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/.", "AI": {"tldr": "Botany-Bot是一个利用机器人手臂和立体相机系统，通过操纵叶片来创建植物高细节“带注释数字孪生”的系统，旨在解决传统植物表型系统因叶片遮挡导致细节缺失的问题。", "motivation": "商业植物表型系统使用固定相机时，由于叶片遮挡，无法感知许多植物细节。", "method": "该系统包含两个立体相机、一个灯箱内的数字转盘、一个工业机器人手臂以及3D分割的高斯泼溅模型。它还提出了机器人算法，用于操纵叶片以拍摄被遮挡细节（如茎芽、叶片上下表面）的高分辨率可索引图像。", "result": "实验结果表明，Botany-Bot在叶片分割方面达到90.8%的准确率，叶片检测方面达到86.2%的准确率，叶片抬升/推动方面达到77.9%的准确率，以及拍摄叶片上下表面详细图像方面达到77.3%的准确率。", "conclusion": "Botany-Bot系统能够有效克服叶片遮挡问题，成功创建植物的详细数字孪生，并通过机器人操纵实现对隐藏细节的捕捉。"}}
{"id": "2510.17792", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17792", "abs": "https://arxiv.org/abs/2510.17792", "authors": ["Gabriel B. Margolis", "Michelle Wang", "Nolan Fey", "Pulkit Agrawal"], "title": "SoftMimic: Learning Compliant Whole-body Control from Examples", "comment": "Website: https://gmargo11.github.io/softmimic/", "summary": "We introduce SoftMimic, a framework for learning compliant whole-body control\npolicies for humanoid robots from example motions. Imitating human motions with\nreinforcement learning allows humanoids to quickly learn new skills, but\nexisting methods incentivize stiff control that aggressively corrects\ndeviations from a reference motion, leading to brittle and unsafe behavior when\nthe robot encounters unexpected contacts. In contrast, SoftMimic enables robots\nto respond compliantly to external forces while maintaining balance and\nposture. Our approach leverages an inverse kinematics solver to generate an\naugmented dataset of feasible compliant motions, which we use to train a\nreinforcement learning policy. By rewarding the policy for matching compliant\nresponses rather than rigidly tracking the reference motion, SoftMimic learns\nto absorb disturbances and generalize to varied tasks from a single motion\nclip. We validate our method through simulations and real-world experiments,\ndemonstrating safe and effective interaction with the environment.", "AI": {"tldr": "SoftMimic是一个框架，使人形机器人通过模仿学习获得柔顺的全身控制策略，能够对外部力做出柔顺响应，而非僵硬纠正，从而提高交互安全性和鲁棒性。", "motivation": "现有基于强化学习的模仿人类运动方法导致机器人控制僵硬，在遇到意外接触时行为脆弱且不安全。研究旨在开发一种能使机器人在保持平衡和姿态的同时，柔顺响应外部力的控制策略。", "method": "SoftMimic利用逆运动学求解器生成一个增强的、可行的柔顺运动数据集。然后，使用该数据集训练强化学习策略，并通过奖励策略匹配柔顺响应而非严格跟踪参考运动，来引导机器人学习吸收干扰。", "result": "SoftMimic学习到如何吸收干扰，并能从单个运动片段泛化到各种任务。通过仿真和真实世界实验验证，该方法展示了与环境安全有效的交互能力。", "conclusion": "SoftMimic成功地使人形机器人学习到柔顺的全身控制策略，使其能够对外部力做出柔顺响应，从而提高了机器人在复杂环境中的鲁棒性和安全性。"}}
{"id": "2510.16463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16463", "abs": "https://arxiv.org/abs/2510.16463", "authors": ["Haocheng Tang", "Ruoke Yan", "Xinhui Yin", "Qi Zhang", "Xinfeng Zhang", "Siwei Ma", "Wen Gao", "Chuanmin Jia"], "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars", "comment": "ACM International Conference on Multimedia 2025", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,\nphotorealistic rendering of dynamic 3D scenes, showing strong potential in\nimmersive communication. However, in digital human encoding and transmission,\nthe compression methods based on general 3DGS representations are limited by\nthe lack of human priors, resulting in suboptimal bitrate efficiency and\nreconstruction quality at the decoder side, which hinders their application in\nstreamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical\nGaussian Compression framework designed for efficient transmission and\nhigh-quality rendering of dynamic avatars. Our method disentangles the Gaussian\nrepresentation into a structural layer, which maps poses to Gaussians via a\nStyleUNet-based generator, and a motion layer, which leverages the SMPL-X model\nto represent temporal pose variations compactly and semantically. This\nhierarchical design supports layer-wise compression, progressive decoding, and\ncontrollable rendering from diverse pose inputs such as video sequences or\ntext. Since people are most concerned with facial realism, we incorporate a\nfacial attention mechanism during StyleUNet training to preserve identity and\nexpression details under low-bitrate constraints. Experimental results\ndemonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar\nrendering, while significantly outperforming prior methods in both visual\nquality and compression efficiency.", "AI": {"tldr": "HGC-Avatar 提出了一种分层高斯压缩框架，通过结合人体先验知识，解决了通用 3DGS 在数字人编码传输中效率和质量不足的问题，实现了高效可流式传输的动态 3D 数字人。", "motivation": "现有基于通用 3DGS 表示的压缩方法在数字人编码和传输中，由于缺乏人体先验知识，导致码率效率和解码器侧重建质量不佳，阻碍了其在可流式 3D 数字人系统中的应用。", "method": "HGC-Avatar 将高斯表示分解为结构层和运动层。结构层通过基于 StyleUNet 的生成器将姿态映射到高斯，运动层利用 SMPL-X 模型紧凑且语义地表示时间姿态变化。这种分层设计支持分层压缩、渐进式解码和可控渲染。此外，在 StyleUNet 训练中加入了面部注意力机制，以在低码率下保留身份和表情细节。", "result": "实验结果表明，HGC-Avatar 为快速 3D 数字人渲染提供了一个可流式传输的解决方案，并在视觉质量和压缩效率方面显著优于现有方法。", "conclusion": "HGC-Avatar 通过引入分层高斯压缩和人体先验（如 SMPL-X 和面部注意力），为动态 3D 数字人的高效传输和高质量渲染提供了一种创新且优越的解决方案，使其适用于可流式 3D 数字人系统。"}}
{"id": "2510.16924", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16924", "abs": "https://arxiv.org/abs/2510.16924", "authors": ["Zhihui Yang", "Yupei Wang", "Kaijie Mo", "Zhe Zhao", "Renfen Hu"], "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?", "comment": "Accepted to EMNLP 2025 (Findings). This version corrects a redundant\n  sentence in the Results section that appeared in the camera-ready version", "summary": "Despite significant progress in multimodal language models (LMs), it remains\nunclear whether visual grounding enhances their understanding of embodied\nknowledge compared to text-only models. To address this question, we propose a\nnovel embodied knowledge understanding benchmark based on the perceptual theory\nfrom psychology, encompassing visual, auditory, tactile, gustatory, olfactory\nexternal senses, and interoception. The benchmark assesses the models'\nperceptual abilities across different sensory modalities through vector\ncomparison and question-answering tasks with over 1,700 questions. By comparing\n30 state-of-the-art LMs, we surprisingly find that vision-language models\n(VLMs) do not outperform text-only models in either task. Moreover, the models\nperform significantly worse in the visual dimension compared to other sensory\ndimensions. Further analysis reveals that the vector representations are easily\ninfluenced by word form and frequency, and the models struggle to answer\nquestions involving spatial perception and reasoning. Our findings underscore\nthe need for more effective integration of embodied knowledge in LMs to enhance\ntheir understanding of the physical world.", "AI": {"tldr": "研究发现，在感知心理学启发的具身知识理解基准测试中，视觉语言模型（VLMs）并未优于纯文本模型，尤其在视觉维度表现更差，凸显了具身知识整合的不足。", "motivation": "尽管多模态语言模型取得了显著进展，但尚不清楚视觉基础是否能增强其对具身知识的理解，相较于纯文本模型。", "method": "提出了一种基于心理学感知理论的新型具身知识理解基准测试，涵盖视觉、听觉、触觉、味觉、嗅觉等外部感官和内感受。通过向量比较和问答任务（超过1700个问题），评估模型在不同感官模态下的感知能力。比较了30个最先进的语言模型。", "result": "令人惊讶的是，视觉语言模型（VLMs）在这两项任务中均未优于纯文本模型。此外，模型在视觉维度上的表现明显差于其他感官维度。进一步分析表明，向量表示容易受到词形和词频的影响，并且模型难以回答涉及空间感知和推理的问题。", "conclusion": "研究结果强调了语言模型需要更有效地整合具身知识，以增强其对物理世界的理解。"}}
{"id": "2510.16505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16505", "abs": "https://arxiv.org/abs/2510.16505", "authors": ["Lukas Selch", "Yufang Hou", "M. Jehanzeb Mirza", "Sivan Doveh", "James Glass", "Rogerio Feris", "Wei Lin"], "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies", "comment": null, "summary": "Large Multimodal Models (LMMs) are increasingly applied to scientific\nresearch, yet it remains unclear whether they can reliably understand and\nreason over the multimodal complexity of papers. A central challenge lies in\ndetecting and resolving inconsistencies across text, figures, tables, and\nequations, issues that are often subtle, domain-specific, and ultimately\nundermine clarity, reproducibility, and trust. Existing benchmarks overlook\nthis issue, either isolating single modalities or relying on synthetic errors\nthat fail to capture real-world complexity. We introduce PRISMM-Bench\n(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first\nbenchmark grounded in real reviewer-flagged inconsistencies in scientific\npapers. Through a multi-stage pipeline of review mining, LLM-assisted filtering\nand human verification, we curate 262 inconsistencies from 242 papers. Based on\nthis set, we design three tasks, namely inconsistency identification, remedy\nand pair matching, which assess a model's capacity to detect, correct, and\nreason over inconsistencies across different modalities. Furthermore, to\naddress the notorious problem of choice-only shortcuts in multiple-choice\nevaluation, where models exploit answer patterns without truly understanding\nthe question, we further introduce structured JSON-based answer representations\nthat minimize linguistic biases by reducing reliance on superficial stylistic\ncues. We benchmark 21 leading LMMs, including large open-weight models\n(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5\nwith high reasoning). Results reveal strikingly low performance (26.1-54.2%),\nunderscoring the challenge of multimodal scientific reasoning and motivating\nprogress towards trustworthy scientific assistants.", "AI": {"tldr": "该研究引入了PRISMM-Bench，一个基于真实同行评审中发现的不一致性构建的基准，旨在评估大型多模态模型（LMMs）在科学论文中检测、纠正和推理多模态不一致性的能力。测试结果显示，领先的LMMs表现不佳（26.1-54.2%），揭示了多模态科学推理的巨大挑战。", "motivation": "大型多模态模型（LMMs）正越来越多地应用于科学研究，但它们能否可靠地理解和推理科学论文中复杂的跨模态信息（如文本、图表、表格和公式之间的不一致性）尚不明确。现有基准要么只关注单一模态，要么依赖合成错误，无法捕捉真实世界的复杂性，这促使研究者开发一个更真实的评估工具。", "method": "研究引入了PRISMM-Bench，这是首个基于真实审稿人标记的科学论文不一致性构建的基准。通过审查挖掘、LLM辅助过滤和人工验证的多阶段流程，从242篇论文中整理出262个不一致性。基于此数据集，设计了三个任务：不一致性识别、修正和配对匹配，以评估模型的检测、纠正和推理能力。此外，为了解决多项选择评估中“只选答案”的捷径问题，引入了结构化的JSON答案表示，以减少语言偏见。最终，对21个主流LMMs（包括开源和专有模型）进行了基准测试。", "result": "基准测试结果显示，所有被评估的LMMs（包括领先的开源和专有模型）在PRISMM-Bench上的表现都异常低，准确率仅在26.1%至54.2%之间。", "conclusion": "LMMs在PRISMM-Bench上的低性能凸显了多模态科学推理的巨大挑战。这激励了未来在开发更值得信赖的科学助手方面取得进展，以更好地处理和理解科学论文中复杂的多模态不一致性。"}}
{"id": "2510.16928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16928", "abs": "https://arxiv.org/abs/2510.16928", "authors": ["Emily Chang", "Niyati Bafna"], "title": "ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models", "comment": null, "summary": "Existing benchmarks for large language models (LLMs) are largely restricted\nto high- or mid-resource languages, and often evaluate performance on\nhigher-order tasks in reasoning and generation. However, plenty of evidence\npoints to the fact that LLMs lack basic linguistic competence in the vast\nmajority of the world's 3800+ written languages. We introduce ChiKhaPo,\nconsisting of 8 subtasks of varying difficulty designed to evaluate the lexical\ncomprehension and generation abilities of generative models. ChiKhaPo draws on\nexisting lexicons, monolingual data, and bitext, and provides coverage for\n2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of\nlanguage coverage. We further show that 6 SOTA models struggle on our\nbenchmark, and discuss the factors contributing to performance scores,\nincluding language family, language resourcedness, task, and comprehension\nversus generation directions. With ChiKhaPo, we hope to enable and encourage\nthe massively multilingual benchmarking of LLMs.", "AI": {"tldr": "该论文引入了ChiKhaPo，一个大规模多语言基准测试集，用于评估大型语言模型（LLMs）在2700多种语言中的基本词汇理解和生成能力，发现现有SOTA模型表现不佳。", "motivation": "现有LLM基准测试主要集中于高/中资源语言和高阶推理/生成任务，但有证据表明LLMs在世界上绝大多数书面语言中缺乏基本的语言能力。", "method": "引入了ChiKhaPo基准测试集，包含8个难度不同的子任务，旨在评估生成模型的词汇理解和生成能力。该基准利用现有词典、单语数据和双语语料，其中2个子任务覆盖2700多种语言。", "result": "6个SOTA模型在ChiKhaPo上表现不佳。影响性能的因素包括语系、语言资源丰富程度、任务类型以及理解与生成方向的差异。", "conclusion": "ChiKhaPo旨在促进和鼓励LLMs的大规模多语言基准测试，以提升其在低资源语言中的基本语言能力。"}}
{"id": "2510.16508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16508", "abs": "https://arxiv.org/abs/2510.16508", "authors": ["Franko Šikić", "Sven Lončarić"], "title": "OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks", "comment": null, "summary": "Out-of-stock (OOS) detection is a very important retail verification process\nthat aims to infer the unavailability of products in their designated areas on\nthe shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based\nmethod that advances OOS detection through auxiliary learning. In particular,\nwe extend a well-established YOLOv8 object detection architecture with\nadditional convolutional branches to simultaneously detect OOS, segment\nproducts, and estimate scene depth. While OOS detection and product\nsegmentation branches are trained using ground truth data, the depth estimation\nbranch is trained using pseudo-labeled annotations produced by the\nstate-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,\nsince the aforementioned pseudo-labeled depth estimates display relative depth,\nwe propose an appropriate depth normalization procedure that stabilizes the\ntraining process. The experimental results show that the proposed method\nsurpassed the performance of the SOTA OOS detection methods by 1.8% of the mean\naverage precision (mAP). In addition, ablation studies confirm the\neffectiveness of auxiliary learning and the proposed depth normalization\nprocedure, with the former increasing mAP by 3.7% and the latter by 4.2%.", "AI": {"tldr": "本文提出OOS-DSD，一种基于深度学习的多任务辅助学习方法，通过扩展YOLOv8同时进行缺货检测、产品分割和深度估计，显著提升了缺货检测的准确性。", "motivation": "缺货（OOS）检测是零售业中一项非常重要的验证过程，旨在推断货架上指定区域产品的不可用性，因此需要更先进的方法来解决这一问题。", "method": "引入OOS-DSD，一种新颖的基于深度学习的辅助学习方法。该方法扩展了YOLOv8目标检测架构，增加了额外的卷积分支，以同时检测缺货、分割产品和估计场景深度。缺货检测和产品分割分支使用真实标签数据训练，而深度估计分支则使用最先进的Depth Anything V2模型生成的伪标签注释进行训练。此外，针对伪标签深度估计显示相对深度的问题，提出了一种适当的深度归一化程序来稳定训练过程。", "result": "实验结果表明，所提出的方法在平均精度均值（mAP）上超越了现有最先进的缺货检测方法1.8%。消融研究证实了辅助学习和所提出的深度归一化程序的有效性，前者使mAP提高了3.7%，后者提高了4.2%。", "conclusion": "OOS-DSD通过结合辅助学习（产品分割和深度估计）和创新的深度归一化程序，显著提高了缺货检测的性能，优于现有最先进的方法，并验证了辅助任务和归一化步骤的有效性。"}}
{"id": "2510.16851", "categories": ["cs.CL", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.16851", "abs": "https://arxiv.org/abs/2510.16851", "authors": ["Zhengqi Pei", "Qingming Huang", "Shuhui Wang"], "title": "Neuronal Group Communication for Efficient Neural representation", "comment": "28 pages, 2 figures", "summary": "The ever-increasing scale of modern neural networks has brought unprecedented\nperformance alongside daunting challenges in efficiency and interpretability.\nThis paper addresses the core question of how to build large neural systems\nthat learn efficient, modular, and interpretable representations. We propose\nNeuronal Group Communication (NGC), a theory-driven framework that reimagines a\nneural network as a dynamical system of interacting neuronal groups rather than\na monolithic collection of neural weights. Instead of treating each weight as\nan independent trainable parameter, NGC treats weights as transient\ninteractions between embedding-like neuronal states, with neural computation\nunfolding through iterative communication among groups of neurons. This\nlow-rank, modular representation yields compact models: groups of neurons\nexchange low-dimensional signals, enabling intra-group specialization and\ninter-group information sharing while dramatically reducing redundant\nparameters. By drawing on dynamical systems theory, we introduce a neuronal\nstability metric (analogous to Lyapunov stability) that quantifies the\ncontraction of neuron activations toward stable patterns during sequence\nprocessing. Using this metric, we reveal that emergent reasoning capabilities\ncorrespond to an external driving force or ``potential'', which nudges the\nneural dynamics away from trivial trajectories while preserving stability.\nEmpirically, we instantiate NGC in large language models (LLMs) and demonstrate\nimproved performance on complex reasoning benchmarks under moderate\ncompression. NGC consistently outperforms standard low-rank approximations and\ncross-layer basis-sharing methods at comparable compression rates. We conclude\nby discussing the broader implications of NGC, including how structured\nneuronal group dynamics might relate to generalization in high-dimensional\nlearning systems.", "AI": {"tldr": "本文提出了神经元群通信（NGC）框架，将神经网络视为神经元群的动态系统，通过低秩、模块化的交互实现高效、可解释的表示。该方法在适度压缩下提升了大型语言模型在复杂推理任务上的性能，并优于现有压缩方法。", "motivation": "现代神经网络的规模日益增长，虽然带来了前所未有的性能，但也带来了效率和可解释性方面的巨大挑战。研究旨在构建能学习高效、模块化、可解释表示的大型神经系统。", "method": "提出了神经元群通信（NGC）框架，将神经网络重新构想为相互作用的神经元群的动态系统。权重被视为嵌入式神经元状态之间的瞬态交互，计算通过神经元群间的迭代通信展开。这种低秩、模块化的表示减少了冗余参数。此外，还引入了一个神经元稳定性度量（类似于李雅普诺夫稳定性），量化神经元激活向稳定模式的收缩。", "result": "将NGC应用于大型语言模型（LLMs），在适度压缩下，复杂推理基准测试的性能有所提高。NGC在可比压缩率下，始终优于标准的低秩近似和跨层基共享方法。理论上，发现涌现的推理能力与在保持稳定性的同时将神经动力学推离平凡轨迹的外部驱动力或“势能”相对应。", "conclusion": "NGC框架提供了一种构建高效、模块化、可解释神经网络的新方法，并在LLMs中显示出显著的实证优势。论文还讨论了NGC的更广泛影响，包括结构化神经元群动态如何与高维学习系统中的泛化能力相关。"}}
{"id": "2510.16624", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16624", "abs": "https://arxiv.org/abs/2510.16624", "authors": ["Sebastian Mocanu", "Emil Slusanschi", "Marius Leordeanu"], "title": "Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs", "comment": null, "summary": "This paper presents a vision-only autonomous flight system for small UAVs\noperating in controlled indoor environments. The system combines semantic\nsegmentation with monocular depth estimation to enable obstacle avoidance,\nscene exploration, and autonomous safe landing operations without requiring GPS\nor expensive sensors such as LiDAR. A key innovation is an adaptive scale\nfactor algorithm that converts non-metric monocular depth predictions into\naccurate metric distance measurements by leveraging semantic ground plane\ndetection and camera intrinsic parameters, achieving a mean distance error of\n14.4 cm. The approach uses a knowledge distillation framework where a\ncolor-based Support Vector Machine (SVM) teacher generates training data for a\nlightweight U-Net student network (1.6M parameters) capable of real-time\nsemantic segmentation. For more complex environments, the SVM teacher can be\nreplaced with a state-of-the-art segmentation model. Testing was conducted in a\ncontrolled 5x4 meter laboratory environment with eight cardboard obstacles\nsimulating urban structures. Extensive validation across 30 flight tests in a\nreal-world environment and 100 flight tests in a digital-twin environment\ndemonstrates that the combined segmentation and depth approach increases the\ndistance traveled during surveillance and reduces mission time while\nmaintaining 100% success rates. The system is further optimized through\nend-to-end learning, where a compact student neural network learns complete\nflight policies from demonstration data generated by our best-performing\nmethod, achieving an 87.5% autonomous mission success rate. This work advances\npractical vision-based drone navigation in structured environments,\ndemonstrating solutions for metric depth estimation and computational\nefficiency challenges that enable deployment on resource-constrained platforms.", "AI": {"tldr": "本文提出了一种仅依赖视觉的无人机自主飞行系统，通过结合语义分割和单目深度估计，实现了室内环境下的避障、场景探索和安全着陆，无需GPS或昂贵传感器，并引入自适应尺度因子算法将非度量深度转换为精确的度量距离。", "motivation": "在没有GPS或昂贵传感器（如LiDAR）的受控室内环境中，为小型无人机实现自主飞行，解决避障、场景探索和自主安全着陆等挑战。", "method": "该系统结合了语义分割和单目深度估计。核心创新是自适应尺度因子算法，利用语义地面平面检测和相机内参将非度量单目深度预测转换为精确的度量距离。语义分割通过知识蒸馏框架实现，其中基于颜色的SVM教师网络为轻量级U-Net学生网络（1.6M参数）生成训练数据，以实现实时语义分割。此外，通过端到端学习，一个紧凑的学生神经网络从最佳方法的演示数据中学习完整的飞行策略。", "result": "自适应尺度因子算法实现了14.4厘米的平均距离误差。在5x4米实验室环境和数字孪生环境中进行了广泛验证，30次真实世界飞行测试和100次数字孪生飞行测试表明，结合分割和深度的方法在监控过程中增加了飞行距离，减少了任务时间，并保持了100%的成功率。端到端学习的自主任务成功率达到87.5%。", "conclusion": "这项工作推动了结构化环境中实用视觉无人机导航的发展，展示了解决度量深度估计和计算效率挑战的方案，从而能够在资源受限的平台上部署该系统。"}}
{"id": "2510.16514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16514", "abs": "https://arxiv.org/abs/2510.16514", "authors": ["Duygu Sap", "Martin Lotz", "Connor Mattinson"], "title": "Image Categorization and Search via a GAT Autoencoder and Representative Models", "comment": "10 pages, 22 figures, Under review", "summary": "We propose a method for image categorization and retrieval that leverages\ngraphs and a graph attention network (GAT)-based autoencoder. Our approach is\nrepresentative-centric, that is, we execute the categorization and retrieval\nprocess via the representative models we construct for the images and image\ncategories. We utilize a graph where nodes represent images (or their\nrepresentatives) and edges capture similarity relationships. GAT highlights\nimportant features and relationships between images, enabling the autoencoder\nto construct context-aware latent representations that capture the key features\nof each image relative to its neighbors. We obtain category representatives\nfrom these embeddings and categorize a query image by comparing its\nrepresentative to the category representatives. We then retrieve the most\nsimilar image to the query image within its identified category. We demonstrate\nthe effectiveness of our representative-centric approach through experiments\nwith both the GAT autoencoders and standard feature-based techniques.", "AI": {"tldr": "本文提出一种基于图和图注意力网络(GAT)自编码器的图像分类和检索方法，通过构建图像和类别的代表模型实现。", "motivation": "旨在通过利用图像间的关系和构建上下文感知的潜在表示，改进图像分类和检索的效率和准确性。", "method": "该方法以“代表为中心”，利用图结构（节点代表图像/代表，边代表相似性）和GAT自编码器。GAT突出图像间的重要特征和关系，使自编码器能够构建上下文感知的潜在表示。通过这些嵌入获取类别代表，然后通过比较查询图像的代表与类别代表进行分类，并在识别出的类别中检索最相似的图像。", "result": "实验证明，所提出的以代表为中心的方法，无论是与GAT自编码器结合还是与标准基于特征的技术相比，都展现了其有效性。", "conclusion": "基于GAT自编码器和代表模型的图像分类与检索方法是有效的，能够生成捕获图像与其邻居关键特征的上下文感知表示。"}}
{"id": "2510.17801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17801", "abs": "https://arxiv.org/abs/2510.17801", "authors": ["Yulin Luo", "Chun-Kai Fan", "Menghang Dong", "Jiayu Shi", "Mengdi Zhao", "Bo-Wen Zhang", "Cheng Chi", "Jiaming Liu", "Gaole Dai", "Rongyu Zhang", "Ruichuan An", "Kun Wu", "Zhengping Che", "Shaoxuan Xie", "Guocai Yao", "Zhongxia Zhao", "Pengwei Wang", "Guang Liu", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain", "comment": null, "summary": "Building robots that can perceive, reason, and act in dynamic, unstructured\nenvironments remains a core challenge. Recent embodied systems often adopt a\ndual-system paradigm, where System 2 handles high-level reasoning while System\n1 executes low-level control. In this work, we refer to System 2 as the\nembodied brain, emphasizing its role as the cognitive core for reasoning and\ndecision-making in manipulation tasks. Given this role, systematic evaluation\nof the embodied brain is essential. Yet existing benchmarks emphasize execution\nsuccess, or when targeting high-level reasoning, suffer from incomplete\ndimensions and limited task realism, offering only a partial picture of\ncognitive capability. To bridge this gap, we introduce RoboBench, a benchmark\nthat systematically evaluates multimodal large language models (MLLMs) as\nembodied brains. Motivated by the critical roles across the full manipulation\npipeline, RoboBench defines five dimensions-instruction comprehension,\nperception reasoning, generalized planning, affordance prediction, and failure\nanalysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure\nrealism, we curate datasets across diverse embodiments, attribute-rich objects,\nand multi-view scenes, drawing from large-scale real robotic data. For\nplanning, RoboBench introduces an evaluation framework,\nMLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether\npredicted plans can achieve critical object-state changes. Experiments on 14\nMLLMs reveal fundamental limitations: difficulties with implicit instruction\ncomprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained\naffordance understanding, and execution failure diagnosis. RoboBench provides a\ncomprehensive scaffold to quantify high-level cognition, and guide the\ndevelopment of next-generation embodied MLLMs. The project page is in\nhttps://robo-bench.github.io.", "AI": {"tldr": "本文提出了RoboBench，一个系统性基准，用于评估多模态大语言模型（MLLMs）作为具身系统“大脑”的高级认知能力，涵盖指令理解、感知推理、泛化规划、功能预测和故障分析等维度，并揭示了现有MLLMs在具身认知方面的局限性。", "motivation": "构建能在动态、非结构化环境中感知、推理和行动的机器人仍是核心挑战。现有具身系统基准侧重执行成功，或在评估高级推理时维度不完整、任务真实性有限，无法全面反映认知能力。因此，需要一个系统性基准来评估作为认知核心的具身“大脑”（MLLMs）。", "method": "引入了RoboBench基准，定义了指令理解、感知推理、泛化规划、功能预测和故障分析五个维度，涵盖14种能力、25个任务和6092个问答对。数据集取自大规模真实机器人数据，包含多样化的具身、丰富属性的物体和多视角场景，以确保真实性。对于规划评估，引入了“MLLM即世界模拟器”框架，通过模拟预测计划能否实现关键物体状态变化来评估具身可行性。对14个MLLMs进行了实验。", "result": "对14个MLLMs的实验揭示了其根本性局限：在隐式指令理解、时空推理、跨场景规划、细粒度功能理解和执行故障诊断方面存在困难。", "conclusion": "RoboBench为量化高级认知能力提供了一个全面的支架，并将指导下一代具身MLLMs的开发。"}}
{"id": "2510.16643", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.9; I.2.10; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.16643", "abs": "https://arxiv.org/abs/2510.16643", "authors": ["Aaron Ray", "Jacob Arkin", "Harel Biggie", "Chuchu Fan", "Luca Carlone", "Nicholas Roy"], "title": "Structured Interfaces for Automated Reasoning with 3D Scene Graphs", "comment": "25 pages, 3 figures", "summary": "In order to provide a robot with the ability to understand and react to a\nuser's natural language inputs, the natural language must be connected to the\nrobot's underlying representations of the world. Recently, large language\nmodels (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for\ngrounding natural language and representing the world. In this work, we address\nthe challenge of using LLMs with 3DSGs to ground natural language. Existing\nmethods encode the scene graph as serialized text within the LLM's context\nwindow, but this encoding does not scale to large or rich 3DSGs. Instead, we\npropose to use a form of Retrieval Augmented Generation to select a subset of\nthe 3DSG relevant to the task. We encode a 3DSG in a graph database and provide\na query language interface (Cypher) as a tool to the LLM with which it can\nretrieve relevant data for language grounding. We evaluate our approach on\ninstruction following and scene question-answering tasks and compare against\nbaseline context window and code generation methods. Our results show that\nusing Cypher as an interface to 3D scene graphs scales significantly better to\nlarge, rich graphs on both local and cloud-based models. This leads to large\nperformance improvements in grounded language tasks while also substantially\nreducing the token count of the scene graph content. A video supplement is\navailable at https://www.youtube.com/watch?v=zY_YI9giZSA.", "AI": {"tldr": "本文提出一种基于检索增强生成（RAG）的方法，通过让大型语言模型（LLM）使用Cypher查询语言与图数据库中的3D场景图（3DSG）交互，以实现自然语言接地，显著提高了大规模场景图的扩展性和性能。", "motivation": "为了让机器人理解并响应自然语言输入，需要将自然语言与机器人对世界的底层表示（如3D场景图）关联起来。现有方法将3DSG序列化为文本放入LLM的上下文窗口，但这种方式在大规模或丰富的3DSG上扩展性差。", "method": "作者提出使用检索增强生成（RAG）来选择3DSG中与任务相关的子集。具体做法是将3DSG编码在一个图数据库中，并为LLM提供Cypher查询语言接口作为工具，使其能够检索用于语言接地的相关数据。", "result": "实验结果表明，使用Cypher作为3D场景图的接口，在本地和云端模型上，对大型、丰富的图具有显著更好的扩展性。这在接地语言任务中带来了巨大的性能提升，同时也大幅减少了场景图内容的token数量。", "conclusion": "通过将Cypher作为LLM访问图数据库中3D场景图的接口，本方法有效解决了现有方法在大规模3DSG上扩展性差的问题，显著提升了机器人在自然语言接地任务中的性能，并降低了计算成本。"}}
{"id": "2510.16932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16932", "abs": "https://arxiv.org/abs/2510.16932", "authors": ["Emily Xiao", "Yixiao Zeng", "Ada Chen", "Chin-Jou Li", "Amanda Bertsch", "Graham Neubig"], "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs", "comment": null, "summary": "A popular method to adapt large language models (LLMs) to new tasks is\nin-context learning (ICL), which is effective but incurs high inference costs\nas context length grows. In this paper we propose a method to perform\ninstruction induction, where we take training examples and reduce them to a\ncompact but descriptive prompt that can achieve performance comparable to ICL\nover the full training set. Specifically, we propose PROMPT-MII, a\nreinforcement learning (RL) based framework to meta-learn an instruction\ninduction model that can generate compact instructions on the fly for an\narbitrary new dataset. We train on over 3,000 diverse classification datasets\nfrom the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves\ndownstream model quality by 4-9 F1 points (10-20% relative), matching ICL\nperformance while requiring 3-13x fewer tokens.", "AI": {"tldr": "本文提出PROMPT-MII，一个基于强化学习的元学习框架，用于指令归纳。它能将训练示例压缩成紧凑且描述性的提示，在保持与上下文学习（ICL）相当性能的同时，显著减少LLM的推理成本。", "motivation": "上下文学习（ICL）是适应大型语言模型（LLMs）新任务的流行方法，但随着上下文长度增加，其推理成本高昂。研究动机是寻找一种方法，在不牺牲性能的前提下，降低LLMs的推理成本。", "method": "本文提出了PROMPT-MII，一个基于强化学习（RL）的框架，用于元学习一个指令归纳模型。该模型能够根据训练示例，为任意新数据集动态生成紧凑的指令。PROMPT-MII在HuggingFace hub上的3,000多个分类数据集上进行训练，并在90个未见过的任务上进行评估。", "result": "PROMPT-MII将下游模型质量提高了4-9个F1点（相对提升10-20%），性能与ICL相当，但所需的token数量减少了3-13倍。", "conclusion": "PROMPT-MII通过指令归纳成功生成紧凑提示，有效降低了LLMs的推理成本。它在大幅减少所需token的同时，实现了与传统上下文学习（ICL）相当的性能，为LLM在实际应用中的高效部署提供了新途径。"}}
{"id": "2510.16985", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16985", "abs": "https://arxiv.org/abs/2510.16985", "authors": ["Akif Islam", "Mohd Ruhul Ameen"], "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection", "comment": "Accepted to IEEE COMPAS 2025. 6 pages, 3 figures, 6 tables", "summary": "Bengali social media platforms have witnessed a sharp increase in hate\nspeech, disproportionately affecting women and adolescents. While datasets such\nas BD-SHS provide a basis for structured evaluation, most prior approaches rely\non either computationally costly full-model fine-tuning or proprietary APIs.\nThis paper presents the first application of Parameter-Efficient Fine-Tuning\n(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three\ninstruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and\nMistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated\ncomments. Each model was adapted by training fewer than 1% of its parameters,\nenabling experiments on a single consumer-grade GPU. The results show that\nLlama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at\n88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical\nand replicable strategy for Bengali and related low-resource languages.", "AI": {"tldr": "本文首次将参数高效微调（PEFT）技术（LoRA和QLoRA）应用于孟加拉语仇恨言论检测，通过在消费级GPU上微调少量参数，实现了高性能，证明了PEFT在低资源语言中的实用性。", "motivation": "孟加拉语社交媒体平台上的仇恨言论（尤其针对女性和青少年）急剧增加。现有的检测方法通常计算成本高昂（全模型微调）或依赖专有API，限制了其广泛应用。", "method": "研究人员使用了LoRA和QLoRA两种PEFT方法，对三个指令微调的大型语言模型（Gemma-3-4B、Llama-3.2-3B和Mistral-7B）进行了微调。微调在包含50,281条注释评论的BD-SHS数据集上进行，每个模型训练的参数不到其总参数的1%，所有实验均在单个消费级GPU上完成。", "result": "实验结果显示，Llama-3.2-3B模型实现了最高的F1-分数，达到92.23%；其次是Mistral-7B，F1-分数为88.94%；Gemma-3-4B的F1-分数为80.25%。", "conclusion": "这些发现确立了PEFT作为一种实用且可复制的策略，适用于孟加拉语及其他相关低资源语言的仇恨言论检测任务。"}}
{"id": "2510.16540", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16540", "abs": "https://arxiv.org/abs/2510.16540", "authors": ["Jihoon Kwon", "Kyle Min", "Jy-yong Sohn"], "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "comment": "Accepted at NeurIPS 2025 (poster). This is the camera-ready version", "summary": "Despite recent advances, vision-language models trained with standard\ncontrastive objectives still struggle with compositional reasoning -- the\nability to understand structured relationships between visual and linguistic\nelements. This shortcoming is largely due to the tendency of the text encoder\nto focus on individual words rather than their relations, a limitation\nreinforced by contrastive training that primarily aligns words with visual\nobjects. In this paper, we introduce REconstruction and Alignment of text\nDescriptions (READ), a fine-tuning method designed to enhance compositional\nreasoning by adding two auxiliary objectives to the contrastive learning: (1) a\ntoken-level reconstruction objective, where a frozen pre-trained decoder\nreconstructs alternative captions based on the embedding of the original\ncaption; and (2) a sentence-level alignment objective, which explicitly aligns\nparaphrased sentences in the embedding space. We show that READ-CLIP, a model\nderived by applying the READ method to the pre-trained CLIP model, achieves the\nstate-of-the-art performance across five major compositional reasoning\nbenchmarks, outperforming the strongest conventional fine-tuning baseline by up\nto 4.1%. Furthermore, applying the READ to existing CLIP variants (including\nNegCLIP and FSC-CLIP) also improves performance on these benchmarks.\nQuantitative and qualitative analyses reveal that our proposed objectives --\nreconstruction and alignment -- offer complementary benefits: the former\nencourages the encoder to capture relationships between words within a caption,\nwhile the latter ensures consistent representations for paraphrases expressed\nwith different wording.", "AI": {"tldr": "现有视觉-语言模型在组合推理方面表现不佳，因为文本编码器倾向于关注单个词而非其关系。本文提出了READ微调方法，通过增加词元级重建和句子级对齐两个辅助目标来增强组合推理能力，READ-CLIP在五个主要组合推理基准测试中取得了最先进的性能。", "motivation": "尽管最近取得了进展，但使用标准对比目标训练的视觉-语言模型在组合推理（理解视觉和语言元素之间结构化关系的能力）方面仍然存在困难。这主要是因为文本编码器倾向于关注单个词而非它们之间的关系，而对比训练主要将词与视觉对象对齐，进一步强化了这一局限性。", "method": "本文提出了REconstruction and Alignment of text Descriptions (READ)，这是一种微调方法，通过在对比学习中添加两个辅助目标来增强组合推理：(1) 词元级重建目标，其中一个冻结的预训练解码器根据原始标题的嵌入重建替代标题；(2) 句子级对齐目标，显式地在嵌入空间中对齐释义句。", "result": "将READ方法应用于预训练的CLIP模型（READ-CLIP）后，在五个主要组合推理基准测试中取得了最先进的性能，比最强的传统微调基线高出4.1%。此外，将READ应用于现有的CLIP变体（包括NegCLIP和FSC-CLIP）也提高了这些基准测试的性能。定量和定性分析表明，重建目标鼓励编码器捕捉标题内词语之间的关系，而对齐目标则确保了不同措辞表达的释义具有一致的表示。", "conclusion": "READ方法通过引入词元级重建和句子级对齐这两个互补的辅助目标，有效解决了视觉-语言模型在组合推理方面的不足，显著提高了模型理解词语间关系和处理释义的能力，从而在相关基准测试中实现了最先进的性能。"}}
{"id": "2510.16756", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.16756", "abs": "https://arxiv.org/abs/2510.16756", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Lu Lu", "Chao Zhang"], "title": "End-to-end Listen, Look, Speak and Act", "comment": "22 pages, 8 figures", "summary": "Human interaction is inherently multimodal and full-duplex: we listen while\nwatching, speak while acting, and fluidly adapt to turn-taking and\ninterruptions. Realizing these capabilities is essential for building models\nsimulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),\nwhich, to our knowledge, is the first full-duplex, end-to-end model that\nsimultaneously perceives and generates across vision, text, speech, and action\nwithin a single architecture, enabling interaction patterns previously out of\nreach, yielding more natural, human-like behaviors. At its core is a novel\nSA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each\nmodality to specialized experts and fuses them through a unified attention\nbackbone. This provides a generalizable solution for joint multimodal\nperception and concurrent generation, leveraging strong pre-trained components\nwhile enabling efficient modality integration and mitigating modality\ninterference. On speech-interaction and robot-manipulation benchmarks, ELLSA\nmatches modality-specific baselines, while uniquely supporting advanced\nmultimodal and full-duplex behaviors such as dialogue and action turn-taking,\ndefective instruction rejection, speaking-while-acting, context-grounded visual\nquestion answering, and action barge-ins. We contend that ELLSA represents a\nstep toward more natural and general interactive intelligence, contributing to\nthe broader pursuit of artificial general intelligence. All data, code and\nmodel checkpoints will be released upon acceptance.", "AI": {"tldr": "ELLSA是首个全双工、端到端的多模态模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然、类人的交互行为。", "motivation": "现有模型难以模拟人类固有的多模态、全双工交互能力，如边听边看、边说边做、轮流发言和打断。实现这些能力对于构建模拟人类的模型至关重要。", "method": "提出了ELLSA模型，其核心是一个新颖的SA-MoE（Self-Attention Mixture-of-Experts）架构。该架构将每种模态路由到专门的专家，并通过统一的注意力主干进行融合，实现多模态感知和并发生成，并利用预训练组件高效集成模态并减轻干扰。", "result": "在语音交互和机器人操作基准测试中，ELLSA与特定模态的基线模型性能相当。同时，它独特地支持高级多模态和全双工行为，例如对话和动作轮流、拒绝有缺陷的指令、边说边做、基于上下文的视觉问答以及动作抢占。", "conclusion": "ELLSA代表了迈向更自然和通用交互智能的重要一步，为实现通用人工智能做出了贡献。"}}
{"id": "2510.16541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16541", "abs": "https://arxiv.org/abs/2510.16541", "authors": ["Binyuan Huang", "Yongdong Luo", "Xianda Guo", "Xiawu Zheng", "Zheng Zhu", "Jiahui Pan", "Chengju Zhou"], "title": "Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition", "comment": null, "summary": "Deep learning-based gait recognition has achieved great success in various\napplications. The key to accurate gait recognition lies in considering the\nunique and diverse behavior patterns in different motion regions, especially\nwhen covariates affect visual appearance. However, existing methods typically\nuse predefined regions for temporal modeling, with fixed or equivalent temporal\nscales assigned to different types of regions, which makes it difficult to\nmodel motion regions that change dynamically over time and adapt to their\nspecific patterns. To tackle this problem, we introduce a Region-aware Dynamic\nAggregation and Excitation framework (GaitRDAE) that automatically searches for\nmotion regions, assigns adaptive temporal scales and applies corresponding\nattention. Specifically, the framework includes two core modules: the\nRegion-aware Dynamic Aggregation (RDA) module, which dynamically searches the\noptimal temporal receptive field for each region, and the Region-aware Dynamic\nExcitation (RDE) module, which emphasizes the learning of motion regions\ncontaining more stable behavior patterns while suppressing attention to static\nregions that are more susceptible to covariates. Experimental results show that\nGaitRDAE achieves state-of-the-art performance on several benchmark datasets.", "AI": {"tldr": "本文提出了一种名为GaitRDAE的步态识别框架，通过动态搜索运动区域、分配自适应时间尺度和应用相应注意力，解决了现有方法中固定时间尺度难以建模动态运动模式的问题，并在多个基准数据集上取得了最先进的性能。", "motivation": "现有基于深度学习的步态识别方法通常使用预定义区域进行时间建模，并为不同类型的区域分配固定或相同的时间尺度。这使得它们难以有效建模随时间动态变化的运动区域，也难以适应其特定模式，尤其是在协变量影响视觉外观时。", "method": "本文引入了区域感知动态聚合和激励框架（GaitRDAE），该框架自动搜索运动区域、分配自适应时间尺度并应用相应的注意力。具体包括两个核心模块：1. 区域感知动态聚合（RDA）模块，用于动态搜索每个区域的最佳时间感受野；2. 区域感知动态激励（RDE）模块，用于强调学习包含更稳定行为模式的运动区域，同时抑制对易受协变量影响的静态区域的关注。", "result": "实验结果表明，GaitRDAE在多个基准数据集上取得了最先进的性能。", "conclusion": "GaitRDAE框架通过其区域感知动态聚合和激励机制，有效解决了传统步态识别方法在处理动态运动区域和协变量影响方面的局限性，显著提升了步态识别的准确性。"}}
{"id": "2510.16556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16556", "abs": "https://arxiv.org/abs/2510.16556", "authors": ["Guangyu Lin", "Li Lin", "Christina P. Walker", "Daniel S. Schiff", "Shu Hu"], "title": "Fit for Purpose? Deepfake Detection in the Real World", "comment": null, "summary": "The rapid proliferation of AI-generated content, driven by advances in\ngenerative adversarial networks, diffusion models, and multimodal large\nlanguage models, has made the creation and dissemination of synthetic media\neffortless, heightening the risks of misinformation, particularly political\ndeepfakes that distort truth and undermine trust in political institutions. In\nturn, governments, research institutions, and industry have strongly promoted\ndeepfake detection initiatives as solutions. Yet, most existing models are\ntrained and validated on synthetic, laboratory-controlled datasets, limiting\ntheir generalizability to the kinds of real-world political deepfakes\ncirculating on social platforms that affect the public. In this work, we\nintroduce the first systematic benchmark based on the Political Deepfakes\nIncident Database, a curated collection of real-world political deepfakes\nshared on social media since 2018. Our study includes a systematic evaluation\nof state-of-the-art deepfake detectors across academia, government, and\nindustry. We find that the detectors from academia and government perform\nrelatively poorly. While paid detection tools achieve relatively higher\nperformance than free-access models, all evaluated detectors struggle to\ngeneralize effectively to authentic political deepfakes, and are vulnerable to\nsimple manipulations, especially in the video domain. Results urge the need for\npolitically contextualized deepfake detection frameworks to better safeguard\nthe public in real-world settings.", "AI": {"tldr": "现有深度伪造检测模型在实验室数据集上表现良好，但在真实世界的政治深度伪造内容上泛化能力差，且易受简单操纵影响，急需政治情境化的检测框架。", "motivation": "人工智能生成内容（特别是政治深度伪造）的快速增长加剧了错误信息传播的风险，损害了公众对政治机构的信任。然而，大多数现有检测模型在合成、实验室控制的数据集上训练和验证，限制了它们在真实世界政治深度伪造场景中的泛化能力。", "method": "本研究引入了第一个基于“政治深度伪造事件数据库”（Political Deepfakes Incident Database）的系统性基准测试，该数据库收集了自2018年以来在社交媒体上分享的真实世界政治深度伪造内容。研究系统评估了来自学术界、政府和工业界的最新深度伪造检测器。", "result": "研究发现，学术界和政府的检测器表现相对较差。虽然付费检测工具的性能优于免费模型，但所有评估的检测器都难以有效泛化到真实的政治深度伪造内容，并且容易受到简单操纵的影响，尤其是在视频领域。", "conclusion": "结果表明，迫切需要开发政治情境化的深度伪造检测框架，以在真实世界环境中更好地保护公众。"}}
{"id": "2510.16800", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16800", "abs": "https://arxiv.org/abs/2510.16800", "authors": ["Zhenpeng Zhang", "Yi Wang", "Shanglei Chai", "Yingying Liu", "Zekai Xie", "Wenhao Huang", "Pengyu Li", "Zipei Luo", "Dajiang Lu", "Yibin Tian"], "title": "An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting", "comment": null, "summary": "Lychee is a high-value subtropical fruit. The adoption of vision-based\nharvesting robots can significantly improve productivity while reduce reliance\non labor. High-quality data are essential for developing such harvesting\nrobots. However, there are currently no consistently and comprehensively\nannotated open-source lychee datasets featuring fruits in natural growing\nenvironments. To address this, we constructed a dataset to facilitate lychee\ndetection and maturity classification. Color (RGB) images were acquired under\ndiverse weather conditions, and at different times of the day, across multiple\nlychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset\nencompasses three different ripeness stages and contains 11,414 images,\nconsisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth\nimages. The images are annotated with 9,658 pairs of lables for lychee\ndetection and maturity classification. To improve annotation consistency, three\nindividuals independently labeled the data, and their results were then\naggregated and verified by a fourth reviewer. Detailed statistical analyses\nwere done to examine the dataset. Finally, we performed experiments using three\nrepresentative deep learning models to evaluate the dataset. It is publicly\navailable for academic", "AI": {"tldr": "本文构建并发布了一个全面标注的开源荔枝数据集，用于荔枝检测和成熟度分类，以支持视觉采摘机器人研发。", "motivation": "高价值亚热带水果荔枝的采摘机器人需要高质量的数据。然而，目前缺乏在自然生长环境下，对荔枝进行一致且全面标注的开源数据集。", "method": "研究人员在不同天气、时间、多种荔枝品种（如糯米糍、妃子笑、黑叶、怀枝）下采集了彩色（RGB）图像，并涵盖了三个不同成熟阶段。数据集包含11,414张图像（878张原始RGB、8,780张增强RGB和1,756张深度图像），并用9,658对标签进行了荔枝检测和成熟度分类标注。为确保标注一致性，数据由三人独立标注，并由第四位审阅者聚合验证。对数据集进行了详细统计分析，并使用三种代表性深度学习模型进行了评估。", "result": "构建了一个包含11,414张图像和9,658个标签的公开可用荔枝数据集，涵盖了多种环境条件和成熟度阶段。该数据集经过一致性标注和统计分析，并已通过深度学习模型进行评估。", "conclusion": "该数据集填补了荔枝在自然环境下高质量标注数据缺失的空白，将极大地促进荔枝检测和成熟度分类领域的研究，为视觉采摘机器人的开发提供关键支持。"}}
{"id": "2510.16987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16987", "abs": "https://arxiv.org/abs/2510.16987", "authors": ["Amit Moryossef", "Clara Meister", "Pavel Stepachev", "Desmond Elliott"], "title": "Back to Bytes: Revisiting Tokenization Through UTF-8", "comment": null, "summary": "We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text\nexactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding\n(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,\n2021; Pagnoni et al., 2025), our implementation never introduces out-of-range\nIDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior\n(e.g., padding, boundaries, conversation structure, attention segments, tool\ncalling, \"thinking\" spans, etc.) is encoded using C0 control bytes - just as\nASCII was originally designed to embed control information alongside printable\ntext. These design principles yield practical benefits: (1) faster tokenization\n(14x) and significantly lower host-device transfer (8x less than int64); (2)\nsimple, shareable 256*d embedding tables that can be aligned across models; and\n(3) a training-time enhancement via bit-biased embeddings, which exposes\nper-byte bit structure and can be added to the embedding table post-training,\nremoving inference costs. Our HuggingFace-compatible implementation improves\nlanguage modeling convergence.", "AI": {"tldr": "本文提出了UTF8Tokenizer，一种极简的字节级分词器，将文本精确映射到其UTF-8编码的字节ID（0-255）。它利用C0控制字节处理所有特殊行为，避免了越界ID和辅助令牌，从而实现更快的处理速度、更低的传输开销、更简单的嵌入表，并提升了语言模型的收敛性。", "motivation": "现有的字节级分词方法（如Xue et al., 2021; Pagnoni et al., 2025）引入了越界ID（如256）或辅助令牌，增加了复杂性。本文旨在开发一种更简洁、高效的字节级分词器，通过遵循ASCII设计原则，避免这些问题，并带来实际的性能优势。", "method": "UTF8Tokenizer是一种字节级分词器，将文本精确映射到其UTF-8编码的字节ID。它将字节x09映射为令牌ID 9，确保所有ID都在0-255范围内。所有特殊行为（如填充、边界、会话结构等）都通过C0控制字节编码，不引入越界ID或辅助令牌。此外，它还引入了位偏置嵌入（bit-biased embeddings），在训练时暴露每字节的位结构，可在训练后添加到嵌入表中，且不增加推理成本。", "result": "该方法带来了显著的实际效益：1) 分词速度提升14倍，主机到设备的数据传输量减少8倍（相比int64）；2) 实现了简单、可共享的256*d嵌入表，可在模型间对齐；3) 通过位偏置嵌入增强了训练，在不增加推理成本的情况下，提升了语言模型的收敛性。", "conclusion": "UTF8Tokenizer提供了一种极简、高效且具有实际优势的字节级分词方法。通过严格使用UTF-8字节ID和C0控制字节，它解决了现有方法的复杂性问题，显著提升了分词速度、数据传输效率，并改善了语言模型的训练收敛性，同时简化了嵌入表的管理。"}}
{"id": "2510.17382", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17382", "abs": "https://arxiv.org/abs/2510.17382", "authors": ["Rishabh Jain", "Keisuke Okumura", "Michael Amir", "Amanda Prorok"], "title": "Graph Attention-Guided Search for Dense Multi-Agent Pathfinding", "comment": null, "summary": "Finding near-optimal solutions for dense multi-agent pathfinding (MAPF)\nproblems in real-time remains challenging even for state-of-the-art planners.\nTo this end, we develop a hybrid framework that integrates a learned heuristic\nderived from MAGAT, a neural MAPF policy with a graph attention scheme, into a\nleading search-based algorithm, LaCAM. While prior work has explored\nlearning-guided search in MAPF, such methods have historically underperformed.\nIn contrast, our approach, termed LaGAT, outperforms both purely search-based\nand purely learning-based methods in dense scenarios. This is achieved through\nan enhanced MAGAT architecture, a pre-train-then-fine-tune strategy on maps of\ninterest, and a deadlock detection scheme to account for imperfect neural\nguidance. Our results demonstrate that, when carefully designed, hybrid search\noffers a powerful solution for tightly coupled, challenging multi-agent\ncoordination problems.", "AI": {"tldr": "本文提出LaGAT，一种混合框架，将基于图注意力方案的神经网络策略MAGAT的学习启发式方法集成到搜索算法LaCAM中，以解决密集多智能体路径规划（MAPF）问题，并在密集场景下优于纯搜索和纯学习方法。", "motivation": "在实时环境中为密集的MAPF问题寻找接近最优的解决方案，即使对于最先进的规划器来说，仍然具有挑战性。", "method": "开发了一个名为LaGAT的混合框架，它将从MAGAT（一个具有图注意力机制的神经MAPF策略）中导出的学习启发式方法集成到领先的基于搜索的算法LaCAM中。该方法通过增强的MAGAT架构、对特定地图的预训练-微调策略以及死锁检测方案来实现，以应对不完善的神经引导。", "result": "LaGAT在密集场景下，其性能优于纯粹基于搜索的方法和纯粹基于学习的方法。", "conclusion": "研究结果表明，经过精心设计的混合搜索方法，可以为紧密耦合、具有挑战性的多智能体协调问题提供强大的解决方案。"}}
{"id": "2510.17001", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17001", "abs": "https://arxiv.org/abs/2510.17001", "authors": ["Yuval Reif", "Guy Kaplan", "Roy Schwartz"], "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic", "comment": null, "summary": "Large language models (LLMs) were shown to encode word form variations, such\nas \"walk\"->\"walked\", as linear directions in embedding space. However, standard\ntokenization algorithms treat these variations as distinct tokens -- filling\nthe size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\",\n\"Walk\"), at the expense of less frequent words and multilingual coverage. We\nshow that many of these variations can be captured by transformation vectors --\nadditive offsets that yield the appropriate word's representation when applied\nto the base form word embedding -- in both the input and output spaces.\nBuilding on this, we propose a compact reshaping of the vocabulary: rather than\nassigning unique tokens to each surface form, we compose them from shared base\nform and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We\napply our approach to multiple LLMs and across five languages, removing up to\n10% of vocabulary entries -- thereby freeing space to allocate new, more\ndiverse tokens. Importantly, we do so while also expanding vocabulary coverage\nto out-of-vocabulary words, with minimal impact on downstream performance, and\nwithout modifying model weights. Our findings motivate a foundational\nrethinking of vocabulary design, moving from string enumeration to a\ncompositional vocabulary that leverages the underlying structure of language.", "AI": {"tldr": "本文提出了一种新的词汇表设计方法，通过将单词变体表示为基础词形和转换向量的组合（例如，“walked” = “walk” + 过去时态），从而减少了大型语言模型（LLMs）的词汇表大小，提高了词汇覆盖率，且对模型性能影响甚微。", "motivation": "标准分词算法将词形变体（如“walk”、“walking”、“Walk”）视为不同的token，这导致词汇表被表面形式变体填充，限制了不常见词和多语言覆盖。然而，LLMs已被证明能将这些词形变体编码为嵌入空间中的线性方向。", "method": "研究表明，许多词形变体可以通过转换向量（即加性偏移量）来捕获，这些向量在应用于基础词形嵌入时能生成相应变体的表示。在此基础上，作者提出了一种紧凑的词汇表重塑方法：不再为每个表面形式分配唯一的token，而是通过共享的基础词形和转换向量来组合它们。该方法应用于多种LLMs和五种语言。", "result": "该方法成功移除了高达10%的词汇表条目，从而释放了空间以分配新的、更多样化的token。重要的是，它还在不修改模型权重的情况下，扩展了对词汇表外单词的覆盖，同时对下游性能的影响最小。", "conclusion": "研究结果促使对词汇表设计进行基础性重新思考，从字符串枚举转向利用语言底层结构的组合式词汇表。"}}
{"id": "2510.16596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16596", "abs": "https://arxiv.org/abs/2510.16596", "authors": ["Yiyang Huang", "Liang Shi", "Yitian Zhang", "Yi Xu", "Yun Fu"], "title": "SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense", "comment": null, "summary": "Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.\nHowever, object hallucination, where models produce plausible but inaccurate\nobject descriptions, remains a significant challenge. In contrast to previous\nwork focusing on LLM components, this paper is the first to trace LVLM\nhallucinations to visual encoders and identifies three key issues: statistical\nbias, inherent bias, and vulnerability. To address these challenges, we propose\nSHIELD, a training-free framework that mitigates hallucinations through three\nstrategies: re-weighting visual tokens to reduce statistical bias, introducing\nnoise-derived tokens to counter inherent bias, and applying adversarial attacks\nwith contrastive decoding to address vulnerability. Experiments demonstrate\nthat SHIELD effectively mitigates object hallucinations across diverse\nbenchmarks and LVLM families. Moreover, SHIELD achieves strong performance on\nthe general LVLM benchmark, highlighting its broad applicability. Code will be\nreleased.", "AI": {"tldr": "本文首次将大型视觉-语言模型（LVLMs）中的物体幻觉追溯到视觉编码器，并提出了一个名为SHIELD的无训练框架，通过三种策略有效缓解了这一问题。", "motivation": "大型视觉-语言模型（LVLMs）在跨模态任务中表现出色，但物体幻觉（生成看似合理但不准确的物体描述）是一个重大挑战。以往工作多关注大型语言模型（LLM）组件，但本研究发现视觉编码器是幻觉的根源，存在统计偏差、固有偏差和脆弱性问题。", "method": "本文提出了一个名为SHIELD的无训练框架，通过以下三种策略缓解幻觉：1) 重新加权视觉token以减少统计偏差；2) 引入噪声衍生token以对抗固有偏差；3) 应用对抗性攻击结合对比解码来解决脆弱性。", "result": "实验证明，SHIELD在各种基准测试和LVLM家族中有效缓解了物体幻觉。此外，SHIELD在通用LVLM基准测试上也表现出色，突显了其广泛适用性。", "conclusion": "SHIELD是一个有效的、无训练的框架，通过解决视觉编码器中的特定问题来缓解LVLM的物体幻觉，并具有广泛的适用性。"}}
{"id": "2510.17006", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17006", "abs": "https://arxiv.org/abs/2510.17006", "authors": ["Masahiro Kaneko", "Zeerak Talat", "Timothy Baldwin"], "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization", "comment": null, "summary": "Iterative jailbreak methods that repeatedly rewrite and input prompts into\nlarge language models (LLMs) to induce harmful outputs -- using the model's\nprevious responses to guide each new iteration -- have been found to be a\nhighly effective attack strategy. Despite being an effective attack strategy\nagainst LLMs and their safety mechanisms, existing defenses do not proactively\ndisrupt this dynamic trial-and-error cycle. In this study, we propose a novel\nframework that dynamically updates its defense strategy through online learning\nin response to each new prompt from iterative jailbreak methods. Leveraging the\ndistinctions between harmful jailbreak-generated prompts and typical harmless\nprompts, we introduce a reinforcement learning-based approach that optimizes\nprompts to ensure appropriate responses for harmless tasks while explicitly\nrejecting harmful prompts. Additionally, to curb overfitting to the narrow band\nof partial input rewrites explored during an attack, we introduce\nPast-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs\nshow that our approach significantly outperforms five existing defense methods\nagainst five iterative jailbreak methods. Moreover, our results indicate that\nour prompt optimization strategy simultaneously enhances response quality for\nharmless tasks.", "AI": {"tldr": "针对迭代越狱攻击，本文提出一种基于在线学习的动态防御框架，利用强化学习优化提示并引入PDGD防止过拟合，有效抵御攻击并提升无害任务的响应质量。", "motivation": "迭代越狱方法通过重复重写和输入提示来诱导LLM产生有害输出，并利用模型之前的响应指导新迭代，是一种高效的攻击策略。然而，现有防御机制未能主动干扰这种动态试错循环。", "method": "本文提出一种新颖的框架，通过在线学习动态更新防御策略，以响应迭代越狱方法的每个新提示。该方法利用有害越狱提示与典型无害提示之间的区别，引入基于强化学习的方法来优化提示，确保无害任务的适当响应并明确拒绝有害提示。此外，为抑制对攻击过程中探索的狭窄部分输入重写过拟合，引入了“过去方向梯度阻尼”（Past-Direction Gradient Damping, PDGD）。", "result": "在三种LLM上进行的实验表明，该方法在对抗五种迭代越狱方法时显著优于五种现有防御方法。此外，结果还表明，该提示优化策略同时提升了无害任务的响应质量。", "conclusion": "所提出的动态防御框架能有效对抗迭代越狱攻击，并通过在线学习和强化学习优化，在拒绝有害内容的同时，提升了LLM对无害任务的响应质量。"}}
{"id": "2510.17363", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17363", "abs": "https://arxiv.org/abs/2510.17363", "authors": ["U. V. B. L Udugama", "George Vosselman", "Francesco Nex"], "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). 8 pages, 7 figures", "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.", "AI": {"tldr": "M2H（Multi-Mono-Hydra）是一种新颖的多任务学习框架，用于在边缘设备上实现实时单目空间感知，它能从单张图像中同时进行语义分割、深度、边缘和表面法线估计，并通过窗口式跨任务注意力模块实现高效且一致的预测。", "motivation": "在边缘设备上部署实时空间感知需要高效的多任务模型，这些模型既要利用互补的任务信息，又要最大限度地减少计算开销。", "method": "该论文提出了Multi-Mono-Hydra (M2H) 框架，它基于轻量级的ViT-based DINOv2骨干网络构建。M2H引入了一个“窗口式跨任务注意力模块”（Window-Based Cross-Task Attention Module），该模块能够在保持任务特定细节的同时，实现结构化的特征交换，从而提高跨任务预测的一致性。M2H设计用于从单目图像中进行语义分割、深度、边缘和表面法线估计。", "result": "M2H在NYUDv2数据集上超越了现有最先进的多任务模型，在Hypersim数据集上优于单任务深度和语义基线，并在Cityscapes数据集上取得了卓越性能。所有这些都在保持笔记本硬件上的计算效率的同时实现。此外，M2H还在真实世界数据上进行了验证，证明了其在空间感知任务中的实用性。", "conclusion": "M2H为单目空间感知系统提供了一个高效且实用的基础，能够支持动态环境中3D场景图的构建，并且在多个基准测试和实际应用中都表现出色。"}}
{"id": "2510.16611", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16611", "abs": "https://arxiv.org/abs/2510.16611", "authors": ["Melika Filvantorkaman", "Maral Filvan Torkaman"], "title": "A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications", "comment": "20 pages, 4 figures", "summary": "Medical imaging plays a vital role in modern diagnostics; however,\ninterpreting high-resolution radiological data remains time-consuming and\nsusceptible to variability among clinicians. Traditional image processing\ntechniques often lack the precision, robustness, and speed required for\nreal-time clinical use. To overcome these limitations, this paper introduces a\ndeep learning framework for real-time medical image analysis designed to\nenhance diagnostic accuracy and computational efficiency across multiple\nimaging modalities, including X-ray, CT, and MRI. The proposed system\nintegrates advanced neural network architectures such as U-Net, EfficientNet,\nand Transformer-based models with real-time optimization strategies including\nmodel pruning, quantization, and GPU acceleration. The framework enables\nflexible deployment on edge devices, local servers, and cloud infrastructures,\nensuring seamless interoperability with clinical systems such as PACS and EHR.\nExperimental evaluations on public benchmark datasets demonstrate\nstate-of-the-art performance, achieving classification accuracies above 92%,\nsegmentation Dice scores exceeding 91%, and inference times below 80\nmilliseconds. Furthermore, visual explanation tools such as Grad-CAM and\nsegmentation overlays enhance transparency and clinical interpretability. These\nresults indicate that the proposed framework can substantially accelerate\ndiagnostic workflows, reduce clinician workload, and support trustworthy AI\nintegration in time-critical healthcare environments.", "AI": {"tldr": "本文提出一个深度学习框架，用于实时医学图像分析，旨在提高诊断准确性和计算效率，支持多模态影像，并实现边缘到云端的灵活部署。", "motivation": "传统的医学图像解读耗时且易受临床医生间差异影响，现有图像处理技术缺乏实时临床应用所需的精度、鲁棒性和速度。", "method": "该框架整合了U-Net、EfficientNet和基于Transformer等先进神经网络架构，并结合了模型剪枝、量化和GPU加速等实时优化策略。它支持在边缘设备、本地服务器和云基础设施上灵活部署，并与PACS和EHR等临床系统无缝互操作。同时，利用Grad-CAM等视觉解释工具增强可解释性。", "result": "在公共基准数据集上的实验评估显示，分类准确率超过92%，分割Dice分数超过91%，推理时间低于80毫秒，达到最先进的性能。视觉解释工具也提升了临床可解释性。", "conclusion": "所提出的框架能显著加速诊断工作流程，减轻临床医生工作量，并支持在时间敏感的医疗环境中集成可信赖的AI。"}}
{"id": "2510.17013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17013", "abs": "https://arxiv.org/abs/2510.17013", "authors": ["Lanni Bu", "Lauren Levin", "Amir Zeldes"], "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking", "comment": null, "summary": "Recent LLM benchmarks have tested models on a range of phenomena, but are\nstill focused primarily on natural language understanding for extraction of\nexplicit information, such as QA or summarization, with responses often tar-\ngeting information from individual sentences. We are still lacking more\nchallenging, and im- portantly also multilingual, benchmarks focus- ing on\nimplicit information and pragmatic infer- ences across larger documents in the\ncontext of discourse tracking: integrating and aggregating information across\nsentences, paragraphs and multiple speaker utterances. To this end, we present\nDiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages\nand four levels of discourse understanding: salience recognition, entity\ntracking, discourse relations and bridging inference. Our evaluation shows that\nthese tasks remain challenging, even for state-of-the-art models.", "AI": {"tldr": "本文提出了DiscoTrack，一个多语言LLM基准测试，旨在评估模型在跨文档隐式信息和语用推理方面的能力，并发现现有模型仍面临挑战。", "motivation": "现有的LLM基准测试主要关注显式信息提取（如问答、摘要），且常针对单个句子。目前缺乏更具挑战性、多语言、关注隐式信息和跨文档语用推理（即篇章跟踪）的基准测试。", "method": "引入了DiscoTrack基准测试，涵盖12种语言和四个篇章理解层面：显著性识别、实体跟踪、篇章关系和桥接推理。", "result": "评估结果表明，即使是最先进的模型，在这些任务上仍然面临挑战。", "conclusion": "DiscoTrack揭示了当前LLM在处理复杂、多语言的隐式信息和篇章级语用推理方面的不足，为未来的模型发展指明了方向。"}}
{"id": "2510.16598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16598", "abs": "https://arxiv.org/abs/2510.16598", "authors": ["Jiaying Zhu", "Yurui Zhu", "Xin Lu", "Wenrui Yan", "Dong Li", "Kunlin Liu", "Xueyang Fu", "Zheng-Jun Zha"], "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs", "comment": "22 pages, 8 figures", "summary": "Multimodal Large Language Models (MLLMs) encounter significant computational\nand memory bottlenecks from the massive number of visual tokens generated by\nhigh-resolution images or multi-image inputs. Previous token compression\ntechniques are often constrained by heuristic rules that risk discarding\ncritical information. They may suffer from biases, such as attention sinks,\nthat lead to sharp performance drops under aggressive compression ratios. To\naddress these limitations, we reformulate token compression as a lightweight\nplug-and-play framework that reformulates token compression into an end-to-end\nlearnable decision process. To be specific, we propose VisionSelector, a scorer\nmodule decoupled from the MLLM backbone that incorporates a differentiable\nTop-K mechanism and a curriculum annealing strategy to bridge the\ntraining-inference gap, enabling efficient and adaptive token selection various\narbitrary compression rates. Remarkably lightweight with only 12.85M trainable\nparameters, VisionSelector demonstrates generalization across various\ncompression rates and adaptively identifying critical tokens. This leads to\nsuperior performance across all compression budgets, evidenced by preserving\n100% accuracy on MME with 30% retention budget, outperforming prior methods by\n12.14% at 10% retention budget, and doubling prefill speed. Our code is\navailable at https://github.com/JulietChoo/VisionSelector .", "AI": {"tldr": "本文提出VisionSelector，一个轻量级可学习的即插即用框架，用于多模态大语言模型（MLLMs）中的视觉令牌压缩，通过可微分Top-K和课程退火策略，在各种压缩率下高效自适应地选择关键令牌，显著提升性能和推理速度。", "motivation": "多模态大语言模型（MLLMs）在处理高分辨率或多图像输入时，因生成大量视觉令牌而面临计算和内存瓶颈。现有的令牌压缩技术常受启发式规则限制，可能丢弃关键信息，并存在注意力沉没等偏差，导致在激进压缩下性能急剧下降。", "method": "本文将令牌压缩重构为一个端到端可学习的决策过程，并提出VisionSelector。VisionSelector是一个与MLLM骨干网络解耦的评分器模块，它结合了可微分的Top-K机制和课程退火策略，以弥合训练与推理之间的差距，从而实现高效和自适应的令牌选择，适用于任意压缩率。该模块设计为轻量级且即插即用。", "result": "VisionSelector仅包含12.85M可训练参数，具有卓越的轻量级特性。它在各种压缩率下表现出良好的泛化能力，并能自适应地识别关键令牌。在30%的保留预算下，MME准确率保持100%；在10%的保留预算下，性能优于现有方法12.14%；同时，预填充速度翻倍。", "conclusion": "VisionSelector通过将令牌压缩重新定义为可学习的决策过程，有效解决了MLLMs的计算和内存瓶颈。它作为一个轻量级、可插拔的解决方案，能够在各种压缩预算下显著提升性能并加快推理速度，同时保留关键信息。"}}
{"id": "2510.17017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17017", "abs": "https://arxiv.org/abs/2510.17017", "authors": ["Qiusi Zhan", "Angeline Budiman-Chan", "Abdelrahman Zayed", "Xingzhi Guo", "Daniel Kang", "Joo-Kyung Kim"], "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents", "comment": "Code: https://github.com/ZQS1943/SafeSearch", "summary": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked ``How can I track\nsomeone's location without their consent?'', a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.", "AI": {"tldr": "本研究发现基于LLM的搜索代理比基础LLM更容易产生有害输出，尤其是在经过实用性微调后。为解决此问题，提出了SafeSearch，一种多目标强化学习方法，通过结合最终输出和查询级别的奖励，显著提升了搜索代理的安全性，同时保持了实用性。", "motivation": "研究人员主要关注提升LLM搜索代理的实用性，但其安全行为却未被充分探索。本研究发现搜索代理比基础LLM更容易产生有害输出（例如，在不安全问题上降低拒绝阈值），且以实用性为导向的微调会加剧这种风险，因此需要同时对安全性和实用性进行对齐。", "method": "首先，使用红队数据集评估了搜索代理，发现它们比基础LLM更容易产生有害输出。其次，提出了SafeSearch，一种多目标强化学习方法，该方法结合了最终输出的安全/实用性奖励和新颖的查询级别塑造项，后者惩罚不安全查询并奖励安全查询。", "result": "实验表明，SafeSearch在三个红队数据集上将代理的有害性降低了70%以上，同时能生成安全、有帮助的响应，并与仅经过实用性微调的代理的问答性能相匹配。进一步分析证实了查询级别奖励在同时提高安全性和实用性方面的有效性。", "conclusion": "基于LLM的搜索代理在安全性方面存在显著风险，尤其是经过实用性微调后。SafeSearch通过多目标强化学习和创新的查询级别奖励，成功地将搜索代理的安全性提升了70%以上，同时保持了其问答性能，实现了安全与实用性的联合对齐。"}}
{"id": "2510.16641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16641", "abs": "https://arxiv.org/abs/2510.16641", "authors": ["Young-Jun Lee", "Byung-Kwan Lee", "Jianshu Zhang", "Yechan Hwang", "Byungsoo Ko", "Han-Gyu Kim", "Dongyu Yao", "Xuankun Rong", "Eojin Joo", "Seung-Ho Han", "Bowon Ko", "Ho-Jin Choi"], "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models", "comment": "Project website:\n  https://passing2961.github.io/multiverse-project-page/", "summary": "Vision-and-Language Models (VLMs) have shown impressive capabilities on\nsingle-turn benchmarks, yet real-world applications often demand more intricate\nmulti-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only\npartially capture the breadth and depth of conversational scenarios encountered\nby users. In this work, we introduce MultiVerse, a novel multi-turn\nconversation benchmark featuring 647 dialogues - each averaging four turns -\nderived from a diverse set of 12 popular VLM evaluation benchmarks. With 484\ntasks and 484 interaction goals, MultiVerse covers a wide range of topics, from\nfactual knowledge and perception to advanced reasoning tasks such as\nmathematics and coding. To facilitate robust assessment, we propose a\nchecklist-based evaluation method that leverages GPT-4o as the automated\nevaluator, measuring performance across 37 key aspects, including perceptual\naccuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on\nMultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve\nonly a 50% success rate in complex multi-turn conversations, highlighting the\ndataset's challenging nature. Notably, we find that providing full dialogue\ncontext significantly enhances performance for smaller or weaker models,\nemphasizing the importance of in-context learning. We believe MultiVerse is a\nlandscape of evaluating multi-turn interaction abilities for VLMs.", "AI": {"tldr": "该论文提出了MultiVerse，一个新颖的多轮对话基准测试数据集，用于评估视觉语言模型（VLMs）在复杂多轮交互中的能力，并揭示了现有模型（包括GPT-4o）在此类任务上的局限性。", "motivation": "现有的多轮对话数据集未能充分捕捉真实世界应用中用户遇到的会话场景的广度和深度，限制了对VLMs在复杂多轮交互能力上的评估。", "method": "引入了MultiVerse数据集，包含647个对话，平均每轮四个对话，源自12个流行的VLM评估基准，涵盖484个任务和交互目标。提出了一种基于清单的评估方法，利用GPT-4o作为自动评估器，衡量模型在37个关键方面（如感知准确性、语言清晰度、事实正确性）的表现。", "result": "在MultiVerse上评估了18个VLM，发现即使是最强的模型（如GPT-4o）在复杂多轮对话中也仅达到50%的成功率，突显了数据集的挑战性。值得注意的是，提供完整的对话上下文能显著提高较小或较弱模型的性能，强调了上下文学习的重要性。", "conclusion": "MultiVerse是一个评估VLM多轮交互能力的具有挑战性的基准，它揭示了当前模型在处理复杂多轮对话方面的不足，并强调了未来研究中上下文学习和更鲁棒模型的重要性。"}}
{"id": "2510.17062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17062", "abs": "https://arxiv.org/abs/2510.17062", "authors": ["Guoqing Luo", "Iffat Maab", "Lili Mou", "Junichi Yamagishi"], "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation", "comment": null, "summary": "While reasoning-based large language models excel at complex tasks through an\ninternal, structured thinking process, a concerning phenomenon has emerged that\nsuch a thinking process can aggregate social stereotypes, leading to biased\noutcomes. However, the underlying behaviours of these language models in social\nbias scenarios remain underexplored. In this work, we systematically\ninvestigate mechanisms within the thinking process behind this phenomenon and\nuncover two failure patterns that drive social bias aggregation: 1) stereotype\nrepetition, where the model relies on social stereotypes as its primary\njustification, and 2) irrelevant information injection, where it fabricates or\nintroduces new details to support a biased narrative. Building on these\ninsights, we introduce a lightweight prompt-based mitigation approach that\nqueries the model to review its own initial reasoning against these specific\nfailure patterns. Experiments on question answering (BBQ and StereoSet) and\nopen-ended (BOLD) benchmarks show that our approach effectively reduces bias\nwhile maintaining or improving accuracy.", "AI": {"tldr": "推理型大型语言模型在复杂任务中会聚合社会刻板印象，导致偏见结果。本研究系统地揭示了两种导致偏见聚合的失败模式，并提出了一种轻量级的基于提示的缓解方法，通过让模型自我审查其推理来有效减少偏见。", "motivation": "推理型大型语言模型虽然在复杂任务中表现出色，但其内部思维过程可能会聚合社会刻板印象，导致偏见结果。然而，这些模型在社会偏见场景中的潜在行为机制尚未得到充分探索。", "method": "本研究系统地调查了导致偏见聚合的思维过程中的机制，并揭示了两种失败模式：1) 刻板印象重复（模型将社会刻板印象作为主要理由），2) 不相关信息注入（模型编造或引入新细节以支持有偏见的叙述）。在此基础上，提出了一种轻量级的基于提示的缓解方法，通过要求模型根据这些特定的失败模式审查其初始推理。", "result": "在问答（BBQ和StereoSet）和开放式（BOLD）基准测试上的实验表明，所提出的方法在有效减少偏见的同时，保持或提高了准确性。", "conclusion": "本研究揭示了推理型大型语言模型中导致社会偏见聚合的两种关键失败模式。基于这些发现，提出的轻量级提示式自我审查缓解方法能够有效降低模型偏见，同时保持或提升性能。"}}
{"id": "2510.17028", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17028", "abs": "https://arxiv.org/abs/2510.17028", "authors": ["Kyle Cox", "Jiawei Xu", "Yikun Han", "Rong Xu", "Tianhao Li", "Chi-Yang Hsu", "Tianlong Chen", "Walter Gerych", "Ying Ding"], "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models", "comment": null, "summary": "An interesting behavior in large language models (LLMs) is prompt\nsensitivity. When provided with different but semantically equivalent versions\nof the same prompt, models may produce very different distributions of answers.\nThis suggests that the uncertainty reflected in a model's output distribution\nfor one prompt may not reflect the model's uncertainty about the meaning of the\nprompt. We model prompt sensitivity as a type of generalization error, and show\nthat sampling across the semantic ``concept space'' with paraphrasing\nperturbations improves uncertainty calibration without compromising accuracy.\nAdditionally, we introduce a new metric for uncertainty decomposition in\nblack-box LLMs that improves upon entropy-based decomposition by modeling\nsemantic continuities in natural language generation. We show that this\ndecomposition metric can be used to quantify how much LLM uncertainty is\nattributed to prompt sensitivity. Our work introduces a new way to improve\nuncertainty calibration in prompt-sensitive language models, and provides\nevidence that some LLMs fail to exhibit consistent general reasoning about the\nmeanings of their inputs.", "AI": {"tldr": "大型语言模型（LLM）对提示词敏感，导致不确定性校准不佳。本研究通过释义扰动采样来改进不确定性校准，并引入了一个新的不确定性分解指标来量化提示词敏感性。", "motivation": "LLM对语义等效但形式不同的提示词会产生截然不同的答案分布，这表明模型输出的不确定性可能无法真实反映其对提示词含义的不不确定性，即存在提示词敏感性问题。", "method": "研究将提示词敏感性建模为一种泛化误差。通过释义扰动在语义“概念空间”中进行采样。此外，引入了一种新的黑盒LLM不确定性分解指标，该指标通过建模自然语言生成中的语义连续性，优于传统的基于熵的分解方法。", "result": "通过释义扰动在语义概念空间中采样，可以改善不确定性校准，且不影响准确性。新引入的不确定性分解指标能够量化LLM不确定性中归因于提示词敏感性的程度。", "conclusion": "本研究提出了一种改进提示词敏感语言模型不确定性校准的新方法，并提供了证据表明一些LLM未能对其输入含义表现出一致的通用推理能力。"}}
{"id": "2510.16664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16664", "abs": "https://arxiv.org/abs/2510.16664", "authors": ["Christopher Thirgood", "Oscar Mendez", "Erin Ling", "Jon Storey", "Simon Hadfield"], "title": "HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications", "comment": null, "summary": "Hyperspectral images (HSI) promise to support a range of new applications in\ncomputer vision. Recent research has explored the feasibility of generalizable\nSpectral Reconstruction (SR), the problem of recovering a HSI from a natural\nthree-channel color image in unseen scenarios.\n  However, previous Multi-Scale Attention (MSA) works have only demonstrated\nsufficient generalizable results for very sparse spectra, while modern HSI\nsensors contain hundreds of channels.\n  This paper introduces a novel approach to spectral reconstruction via our\nHYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).\n  Using a Teacher model that encapsulates latent hyperspectral image data and a\nStudent model that learns mappings from natural images to the Teacher's encoded\ndomain, alongside a novel training method, we achieve high-quality spectral\nreconstruction.\n  This addresses key limitations of prior SR models, providing SOTA performance\nacross all metrics, including an 18\\% boost in accuracy, and faster inference\ntimes than current SOTA models at various channel depths.", "AI": {"tldr": "本文提出HYDRA模型，通过混合知识蒸馏和光谱重建架构，解决了现有光谱重建方法对稀疏光谱的限制，实现了从RGB图像到高通道数高光谱图像的高质量重建，并达到了最先进的性能。", "motivation": "现有光谱重建（SR）研究，特别是多尺度注意力（MSA）方法，仅在非常稀疏的光谱上表现出足够的泛化能力，无法有效处理现代高光谱图像（HSI）传感器包含的数百个通道。因此，需要一种能应对密集光谱的通用光谱重建方法。", "method": "本文引入了一种名为HYDRA（HYbrid knowledge Distillation and spectral Reconstruction Architecture）的新型光谱重建方法。它包含一个教师模型（封装潜在高光谱图像数据）和一个学生模型（学习从自然图像到教师编码域的映射），并结合了一种新颖的训练方法。", "result": "HYDRA实现了高质量的光谱重建，在所有指标上均达到了最先进（SOTA）的性能，包括准确性提升18%，并且在不同通道深度下，推理时间比现有SOTA模型更快。", "conclusion": "HYDRA模型成功解决了先前光谱重建模型的关键局限性，为通用光谱重建提供了卓越的性能和效率，并在多项指标上超越了当前最先进的模型。"}}
{"id": "2510.16660", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16660", "abs": "https://arxiv.org/abs/2510.16660", "authors": ["Yuntian Wang", "Xilin Yang", "Che-Yung Shen", "Nir Pillar", "Aydogan Ozcan"], "title": "Universal and Transferable Attacks on Pathology Foundation Models", "comment": "38 Pages, 8 Figures", "summary": "We introduce Universal and Transferable Adversarial Perturbations (UTAP) for\npathology foundation models that reveal critical vulnerabilities in their\ncapabilities. Optimized using deep learning, UTAP comprises a fixed and weak\nnoise pattern that, when added to a pathology image, systematically disrupts\nthe feature representation capabilities of multiple pathology foundation\nmodels. Therefore, UTAP induces performance drops in downstream tasks that\nutilize foundation models, including misclassification across a wide range of\nunseen data distributions. In addition to compromising the model performance,\nwe demonstrate two key features of UTAP: (1) universality: its perturbation can\nbe applied across diverse field-of-views independent of the dataset that UTAP\nwas developed on, and (2) transferability: its perturbation can successfully\ndegrade the performance of various external, black-box pathology foundation\nmodels - never seen before. These two features indicate that UTAP is not a\ndedicated attack associated with a specific foundation model or image dataset,\nbut rather constitutes a broad threat to various emerging pathology foundation\nmodels and their applications. We systematically evaluated UTAP across various\nstate-of-the-art pathology foundation models on multiple datasets, causing a\nsignificant drop in their performance with visually imperceptible modifications\nto the input images using a fixed noise pattern. The development of these\npotent attacks establishes a critical, high-standard benchmark for model\nrobustness evaluation, highlighting a need for advancing defense mechanisms and\npotentially providing the necessary assets for adversarial training to ensure\nthe safe and reliable deployment of AI in pathology.", "AI": {"tldr": "本文介绍了通用可迁移对抗性扰动（UTAP），它是一种固定的弱噪声模式，能系统性地破坏多个病理学基础模型的特征表示能力，导致下游任务性能下降，并对各种未见过的模型和数据具有通用性和可迁移性。", "motivation": "随着病理学基础模型的兴起，需要识别并解决其潜在的脆弱性，以确保AI在病理学领域安全可靠地部署。现有的攻击可能不够通用和可迁移。", "method": "研究人员利用深度学习优化出一种固定的、微弱的噪声模式，即UTAP。将这种噪声模式添加到病理图像中，以系统性地扰乱多个病理学基础模型的特征表示能力。", "result": "UTAP能导致利用基础模型的下游任务性能显著下降，包括在各种未见数据分布上的错误分类。它展现出两大关键特性：(1) 通用性：扰动可应用于不同视野，且与UTAP训练数据集无关；(2) 可迁移性：扰动能成功降低各种外部、黑盒病理学基础模型的性能。这些结果表明UTAP是对各种新兴病理学基础模型及其应用的广泛威胁，且对输入图像的修改视觉上难以察觉。", "conclusion": "UTAP的开发揭示了病理学基础模型的关键漏洞，为模型鲁棒性评估建立了高标准基准。这强调了推进防御机制和提供对抗性训练所需资产的必要性，以确保AI在病理学领域安全可靠地部署。"}}
{"id": "2510.17018", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17018", "abs": "https://arxiv.org/abs/2510.17018", "authors": ["Noor Islam S. Mohammad"], "title": "Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification", "comment": null, "summary": "Toxic comment detection remains a challenging task, where transformer-based\nmodels (e.g., BERT) incur high computational costs and degrade on minority\ntoxicity classes, while classical ensembles lack semantic adaptability. We\npropose xLSTM, a parameter-efficient and theoretically grounded framework that\nunifies cosine-similarity gating, adaptive feature prioritization, and\nprincipled class rebalancing. A learnable reference vector {v} in {R}^d\nmodulates contextual embeddings via cosine similarity, amplifying toxic cues\nand attenuating benign signals to yield stronger gradients under severe class\nimbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)\nthrough a projection layer, a character-level BiLSTM for morphological cues,\nembedding-space SMOTE for minority augmentation, and adaptive focal loss with\ndynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains\n96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%\non identity_hate categories, with 15 times fewer parameters and 50ms inference\nlatency. Cosine gating contributes a +4.8% F1 gain in ablations. The results\nestablish a new efficiency adaptability frontier, demonstrating that\nlightweight, theoretically informed architectures can surpass large pretrained\nmodels on imbalanced, domain-specific NLP tasks.", "AI": {"tldr": "本文提出了xLSTM，一个参数高效且理论基础坚实的框架，通过余弦相似度门控、自适应特征优先级和类再平衡，在有毒评论检测任务中超越了BERT，尤其在少数毒性类别上表现更佳，且计算成本显著降低。", "motivation": "当前有毒评论检测面临挑战：基于Transformer的模型（如BERT）计算成本高昂，且在少数毒性类别上性能下降；而传统集成模型缺乏语义适应性。", "method": "xLSTM框架统一了余弦相似度门控、自适应特征优先级和类再平衡。它使用一个可学习的参考向量v通过余弦相似度调节上下文嵌入，以放大毒性线索并衰减良性信号。该模型集成了多源嵌入（GloVe、FastText、BERT CLS），包含一个字符级BiLSTM用于形态学线索，使用嵌入空间SMOTE进行少数类增强，并采用带有动态类权重的自适应焦点损失。", "result": "在Jigsaw有毒评论基准测试中，xLSTM达到了96.0%的准确率和0.88的macro-F1分数。它在“threat”类别上比BERT高出33%，在“identity_hate”类别上高出28%。xLSTM的参数量是BERT的1/15，推理延迟仅为50毫秒。消融实验表明，余弦门控贡献了+4.8%的F1增益。", "conclusion": "研究结果确立了一个新的效率适应性前沿，证明了轻量级、理论知情的架构在不平衡、领域特定的自然语言处理任务上，能够超越大型预训练模型。"}}
{"id": "2510.17109", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17109", "abs": "https://arxiv.org/abs/2510.17109", "authors": ["Tianyang Xu", "Dan Zhang", "Kushan Mitra", "Estevam Hruschka"], "title": "Verification-Aware Planning for Multi-Agent Systems", "comment": "Submission for ARR Oct", "summary": "Large language model (LLM) agents are increasingly deployed to tackle complex\ntasks, often necessitating collaboration among multiple specialized agents.\nHowever, multi-agent collaboration introduces new challenges in planning,\ncoordination, and verification. Execution failures frequently arise not from\nflawed reasoning alone, but from subtle misalignments in task interpretation,\noutput format, or inter-agent handoffs. To address these challenges, we present\nVeriMAP, a framework for multi-agent collaboration with verification-aware\nplanning. The VeriMAP planner decomposes tasks, models subtask dependencies,\nand encodes planner-defined passing criteria as subtask verification functions\n(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,\ndemonstrating that it outperforms both single- and multi-agent baselines while\nenhancing system robustness and interpretability. Our analysis highlights how\nverification-aware planning enables reliable coordination and iterative\nrefinement in multi-agent systems, without relying on external labels or\nannotations.", "AI": {"tldr": "VeriMAP是一个多智能体协作框架，通过引入验证感知规划来解决大型语言模型多智能体系统中的协调和执行失败问题，提高了系统可靠性、鲁棒性和可解释性。", "motivation": "大型语言模型（LLM）智能体在处理复杂任务时需要多智能体协作，但这带来了规划、协调和验证方面的挑战。执行失败常源于任务理解、输出格式或智能体间交接的细微偏差，而非推理错误。", "method": "该研究提出了VeriMAP框架，其规划器分解任务、建模子任务依赖，并将规划器定义的通过标准编码为Python和自然语言的子任务验证函数（VFs）。", "result": "VeriMAP在不同数据集上表现优于单智能体和多智能体基线，同时增强了系统鲁棒性和可解释性。", "conclusion": "验证感知规划能够实现多智能体系统中可靠的协调和迭代细化，且无需外部标签或注释。"}}
{"id": "2510.17115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17115", "abs": "https://arxiv.org/abs/2510.17115", "authors": ["Wei Du", "Nuowei Liu", "Jie Wang", "Jiahao Kuang", "Tao Ji", "Xiaoling Wang", "Yuanbin Wu"], "title": "DVAGen: Dynamic Vocabulary Augmented Generation", "comment": null, "summary": "Language models trained with a fixed vocabulary struggle to generalize to\nnovel or out-of-vocabulary words, limiting their flexibility in handling\ndiverse token combinations. Existing dynamic vocabulary approaches attempt to\naddress this limitation but face challenges such as fragmented codebases, lack\nof support for modern LLMs, and limited inference scalability. To overcome\nthese issues, we introduce DVAGen, a fully open-source, unified framework\ndesigned for training, evaluation, and visualization of dynamic\nvocabulary-augmented language models. Our framework modularizes the pipeline\nfor ease of customization, integrates seamlessly with open-source LLMs, and is\nthe first to provide both CLI and WebUI tools for real-time result inspection.\nWe validate the effectiveness of dynamic vocabulary methods on modern LLMs and\ndemonstrate support for batch inference, significantly improving inference\nthroughput.", "AI": {"tldr": "本文介绍了DVAGen，一个完全开源、统一的框架，用于训练、评估和可视化动态词汇增强的语言模型，解决了固定词汇模型的局限性以及现有动态词汇方法的挑战。", "motivation": "现有语言模型因固定词汇表而难以泛化到新词或词汇表外词，限制了其处理多样化令牌组合的灵活性。现有的动态词汇方法存在代码库碎片化、缺乏对现代大型语言模型（LLMs）的支持以及推理可扩展性有限等问题。", "method": "本文引入了DVAGen，一个模块化、统一的开源框架，用于动态词汇增强语言模型的训练、评估和可视化。该框架无缝集成了开源LLMs，并首次提供了命令行界面（CLI）和WebUI工具进行实时结果检查，同时支持批量推理。", "result": "研究验证了动态词汇方法在现代LLMs上的有效性，并展示了对批量推理的支持，显著提高了推理吞吐量。", "conclusion": "DVAGen为动态词汇增强的语言模型提供了一个全面且可扩展的解决方案，提高了模型的灵活性和推理效率，并促进了相关研究和应用。"}}
{"id": "2510.16702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16702", "abs": "https://arxiv.org/abs/2510.16702", "authors": ["Huy Minh Nhat Nguyen", "Triet Hoang Minh Dao", "Chau Vinh Hoang Truong", "Cuong Tuan Nguyen"], "title": "SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation", "comment": "2025 IEEE Conference on Computational Intelligence in Bioinformatics\n  and Computational Biology (CIBCB)", "summary": "Optical Coherence Tomography (OCT) is a widely used non-invasive imaging\ntechnique that provides detailed three-dimensional views of the retina, which\nare essential for the early and accurate diagnosis of ocular diseases.\nConsequently, OCT image analysis and processing have emerged as key research\nareas in biomedical imaging. However, acquiring paired datasets of clean and\nreal-world noisy OCT images for supervised denoising models remains a\nformidable challenge due to intrinsic speckle noise and practical constraints\nin clinical imaging environments. To address these issues, we propose SDPA++: A\nGeneral Framework for Self-Supervised Denoising with Patch Aggregation. Our\nnovel approach leverages only noisy OCT images by first generating\npseudo-ground-truth images through self-fusion and self-supervised denoising.\nThese refined images then serve as targets to train an ensemble of denoising\nmodels using a patch-based strategy that effectively enhances image clarity.\nPerformance improvements are validated via metrics such as Contrast-to-Noise\nRatio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge\nPreservation (EP) on the real-world dataset from the IEEE SPS Video and Image\nProcessing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT\nimages without clean references, highlighting our method's potential for\nimproving image quality and diagnostic outcomes in clinical practice.", "AI": {"tldr": "本文提出SDPA++，一个通用的自监督去噪框架，专门针对光学相干断层扫描（OCT）图像，通过自融合和自监督去噪生成伪真实图像，并训练基于补丁聚合的去噪模型集成，以改善图像质量。", "motivation": "OCT是眼科疾病诊断的关键技术，但由于固有的散斑噪声和临床环境限制，获取成对的干净和真实世界噪声OCT图像数据集以训练监督去噪模型极具挑战性。", "method": "SDPA++框架仅利用噪声OCT图像。它首先通过自融合和自监督去噪生成伪真实（pseudo-ground-truth）图像。然后，这些伪真实图像作为目标，用于训练一个采用基于补丁（patch-based）策略的去噪模型集成，从而有效增强图像清晰度。", "result": "在IEEE SPS视频和图像处理杯的真实世界数据集（该数据集仅包含噪声OCT图像，无干净参考）上，通过对比度噪声比（CNR）、均方比（MSR）、纹理保留（TP）和边缘保留（EP）等指标验证了性能改进。结果表明该方法在改善图像质量方面具有潜力。", "conclusion": "SDPA++框架能够有效提高OCT图像质量和临床诊断结果，尤其适用于缺乏干净参考图像的临床实践场景。"}}
{"id": "2510.16688", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16688", "abs": "https://arxiv.org/abs/2510.16688", "authors": ["Yejie Guo", "Yunzhong Hou", "Wufei Ma", "Meng Tang", "Ming-Hsuan Yang"], "title": "Pursuing Minimal Sufficiency in Spatial Reasoning", "comment": null, "summary": "Spatial reasoning, the ability to ground language in 3D understanding,\nremains a persistent challenge for Vision-Language Models (VLMs). We identify\ntwo fundamental bottlenecks: inadequate 3D understanding capabilities stemming\nfrom 2D-centric pre-training, and reasoning failures induced by redundant 3D\ninformation. To address these, we first construct a Minimal Sufficient Set\n(MSS) of information before answering a given question: a compact selection of\n3D perception results from \\textit{expert models}. We introduce MSSR (Minimal\nSufficient Spatial Reasoner), a dual-agent framework that implements this\nprinciple. A Perception Agent programmatically queries 3D scenes using a\nversatile perception toolbox to extract sufficient information, including a\nnovel SOG (Situated Orientation Grounding) module that robustly extracts\nlanguage-grounded directions. A Reasoning Agent then iteratively refines this\ninformation to pursue minimality, pruning redundant details and requesting\nmissing ones in a closed loop until the MSS is curated. Extensive experiments\ndemonstrate that our method, by explicitly pursuing both sufficiency and\nminimality, significantly improves accuracy and achieves state-of-the-art\nperformance across two challenging benchmarks. Furthermore, our framework\nproduces interpretable reasoning paths, offering a promising source of\nhigh-quality training data for future models. Source code is available at\nhttps://github.com/gyj155/mssr.", "AI": {"tldr": "针对视觉-语言模型（VLMs）在空间推理方面的挑战，本文提出了MSSR（Minimal Sufficient Spatial Reasoner）框架。它通过双代理系统，从专家模型中提取“最小充分信息集”（MSS），解决了2D预训练导致的3D理解不足和冗余3D信息引起的推理失败问题，显著提升了空间推理的准确性，并提供了可解释的推理路径。", "motivation": "视觉-语言模型（VLMs）在将语言与3D理解相结合的空间推理能力方面面临持续挑战。主要瓶颈包括：1) 2D为中心的预训练导致3D理解能力不足；2) 冗余的3D信息引发推理失败。", "method": "本文首先提出构建“最小充分信息集”（Minimal Sufficient Set, MSS）的概念，即在回答问题前，从专家模型中精选紧凑的3D感知结果。在此基础上，引入了MSSR（Minimal Sufficient Spatial Reasoner）双代理框架：1) 感知代理（Perception Agent）利用多功能感知工具箱（包括新颖的SOG模块用于稳健地提取语言-地面方向）程序化地查询3D场景，以提取“充分”信息；2) 推理代理（Reasoning Agent）通过闭环迭代地细化信息，剪除冗余细节并请求缺失信息，以追求“最小性”，直到策定出MSS。", "result": "实验结果表明，MSSR方法通过明确追求信息的充分性和最小性，显著提高了准确性，并在两个具有挑战性的基准测试中达到了最先进的性能。此外，该框架生成了可解释的推理路径，为未来模型提供了高质量的训练数据来源。", "conclusion": "通过明确追求3D信息的充分性和最小性，MSSR框架显著提升了视觉-语言模型的空间推理能力，实现了最先进的性能，并生成了有价值的可解释推理路径，为未来的模型训练提供了新的方向。"}}
{"id": "2510.16709", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16709", "abs": "https://arxiv.org/abs/2510.16709", "authors": ["Liu Haojie", "Gao Suixiang"], "title": "HumanCM: One Step Human Motion Prediction", "comment": "6 pages, 2 figures, 2 tables", "summary": "We present HumanCM, a one-step human motion prediction framework built upon\nconsistency models. Instead of relying on multi-step denoising as in\ndiffusion-based methods, HumanCM performs efficient single-step generation by\nlearning a self-consistent mapping between noisy and clean motion states. The\nframework adopts a Transformer-based spatiotemporal architecture with temporal\nembeddings to model long-range dependencies and preserve motion coherence.\nExperiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves\ncomparable or superior accuracy to state-of-the-art diffusion models while\nreducing inference steps by up to two orders of magnitude.", "AI": {"tldr": "HumanCM是一个基于一致性模型的一步式人体运动预测框架，它通过学习噪声与清晰运动状态之间的自洽映射，实现高效的单步生成，并在准确性上与现有扩散模型相当或更优，同时显著减少推理步骤。", "motivation": "现有的基于扩散模型的运动预测方法依赖多步去噪过程，效率较低，因此需要一种更高效的单步生成方法。", "method": "HumanCM采用一致性模型，通过学习噪声和清晰运动状态之间的自洽映射，实现单步生成。其核心是一个基于Transformer的时空架构，并融入时间嵌入以捕捉长距离依赖并保持运动连贯性。", "result": "在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM在准确性上与最先进的扩散模型相当或更优，同时将推理步骤减少了多达两个数量级。", "conclusion": "HumanCM提供了一种高效且准确的人体运动预测新范式，通过单步生成显著提升了推理速度，同时保持了高水平的性能。"}}
{"id": "2510.16704", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16704", "abs": "https://arxiv.org/abs/2510.16704", "authors": ["Tianxin Wei", "Yifan Chen", "Xinrui He", "Wenxuan Bao", "Jingrui He"], "title": "Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization", "comment": "Accepted by KDD 2025", "summary": "Distribution shifts between training and testing samples frequently occur in\npractice and impede model generalization performance. This crucial challenge\nthereby motivates studies on domain generalization (DG), which aim to predict\nthe label on unseen target domain data by solely using data from source\ndomains. It is intuitive to conceive the class-separated representations\nlearned in contrastive learning (CL) are able to improve DG, while the reality\nis quite the opposite: users observe directly applying CL deteriorates the\nperformance. We analyze the phenomenon with the insights from CL theory and\ndiscover lack of intra-class connectivity in the DG setting causes the\ndeficiency. We thus propose a new paradigm, domain-connecting contrastive\nlearning (DCCL), to enhance the conceptual connectivity across domains and\nobtain generalizable representations for DG. On the data side, more aggressive\ndata augmentation and cross-domain positive samples are introduced to improve\nintra-class connectivity. On the model side, to better embed the unseen test\ndomains, we propose model anchoring to exploit the intra-class connectivity in\npre-trained representations and complement the anchoring with generative\ntransformation loss. Extensive experiments on five standard DG benchmarks are\nperformed. The results verify that DCCL outperforms state-of-the-art baselines\neven without domain supervision. The detailed model implementation and the code\nare provided through https://github.com/weitianxin/DCCL", "AI": {"tldr": "本文分析了对比学习（CL）在域泛化（DG）中表现不佳的原因是缺乏类内连接性，并提出了一种新的范式——域连接对比学习（DCCL），通过数据增强、跨域正样本、模型锚定和生成转换损失来增强域间连接性，从而获得更具泛化性的表示，超越了现有基线。", "motivation": "训练集和测试集之间的分布偏移在实际中频繁发生，阻碍了模型的泛化性能。域泛化（DG）旨在仅使用源域数据预测未见目标域数据的标签。直观上，对比学习（CL）中学习到的类分离表示应该能改善DG，但实际情况是直接应用CL反而会恶化性能，这促使研究人员分析并解决这一问题。", "method": "作者从对比学习理论角度分析了现象，发现DG设置中缺乏类内连接性是导致性能下降的原因。因此，提出了域连接对比学习（DCCL）新范式。在数据方面，引入了更激进的数据增强和跨域正样本以改善类内连接性；在模型方面，提出了模型锚定来利用预训练表示中的类内连接性，并辅以生成转换损失，以更好地嵌入未见的测试域。", "result": "在五个标准DG基准上进行了广泛实验。结果验证了DCCL即使在没有域监督的情况下，也优于最先进的基线方法。", "conclusion": "对比学习在域泛化中表现不佳是由于缺乏类内连接性。通过提出的域连接对比学习（DCCL）范式，结合数据和模型层面的创新，可以有效增强域间的概念连接性，从而获得更具泛化性的表示，显著提升域泛化性能。"}}
{"id": "2510.17139", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17139", "abs": "https://arxiv.org/abs/2510.17139", "authors": ["Zhichao Xu", "Shengyao Zhuang", "Xueguang Ma", "Bingsen Chen", "Yijun Tian", "Fengran Mo", "Jie Cao", "Vivek Srikumar"], "title": "Rethinking On-policy Optimization for Query Augmentation", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.", "AI": {"tldr": "本文首次系统比较了基于提示（prompting）和基于强化学习（RL）的查询增强方法，发现简单提示常与RL方法效果相当甚至更优。在此基础上，提出了一种新颖的混合方法OPQE，通过RL策略生成伪文档以最大化检索性能，实现了优于单一方法的最佳效果。", "motivation": "大型语言模型（LLMs）在信息检索（IR）的查询增强方面备受关注，主要有两种方法：基于提示生成答案/伪文档和基于强化学习微调LLMs进行查询重写。然而，这两种方法尚未在一致的实验条件下进行系统比较。", "method": "本文首先在包括证据寻求、即席检索和工具检索在内的多种基准上，对基于提示和基于强化学习的查询增强进行了首次系统比较。随后，提出了一种新颖的混合方法——On-policy Pseudo-document Query Expansion (OPQE)。OPQE不是重写查询，而是让LLM策略学习生成一个伪文档来最大化检索性能，从而将提示的灵活性和生成结构与强化学习的目标优化相结合。", "result": "关键发现是，简单、无需训练的查询增强方法（基于提示）通常与更昂贵的基于强化学习的方法效果相当，甚至超越，尤其是在使用强大的LLMs时。此外，OPQE方法在实验中表现优于单独的基于提示的方法和基于强化学习的重写方法。", "conclusion": "结合提示的灵活性和强化学习的优化能力，协同方法（如OPQE）能够为查询增强带来最佳结果。这一发现为未来的查询增强研究提供了新的方向。"}}
{"id": "2510.16714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16714", "abs": "https://arxiv.org/abs/2510.16714", "authors": ["Xiongkun Linghu", "Jiangyong Huang", "Ziyu Zhu", "Baoxiong Jia", "Siyuan Huang"], "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes", "comment": "Project page: https://scenecot.github.io/", "summary": "Existing research on 3D Large Language Models (LLMs) still struggles to\nachieve grounded question-answering, primarily due to the under-exploration of\nthe mech- anism of human-like scene-object grounded reasoning. This paper\nbridges the gap by presenting a novel framework. We first introduce a grounded\nChain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a\ncomplex reasoning task into simpler and manageable problems, and building\ncorresponding visual clues based on multimodal expert modules. To enable such a\nmethod, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning\ndataset, consisting of 185K high-quality instances. Extensive experiments\nacross various complex 3D scene reasoning benchmarks demonstrate that our new\nframework achieves strong performance with high grounding-QA coherence. To the\nbest of our knowledge, this is the first successful application of CoT\nreasoning to 3D scene understanding, enabling step-by-step human-like reasoning\nand showing potential for extension to broader 3D scene understanding\nscenarios.", "AI": {"tldr": "本文提出了一种新颖的框架，通过引入3D场景中的接地式思维链（CoT）推理方法（SCENECOT）和相应的SCENECOT-185K大规模数据集，解决了现有3D大语言模型在场景-物体接地式问答方面的不足。", "motivation": "现有3D大语言模型（LLMs）在实现接地式问答方面仍面临挑战，主要原因是对于类人场景-物体接地式推理机制的探索不足。", "method": "本文首先提出了一种在3D场景中进行接地式思维链（CoT）推理的方法（SCENECOT），该方法将复杂的推理任务分解为更简单的问题，并基于多模态专家模块构建相应的视觉线索。为支持此方法，开发了SCENECOT-185K，这是首个大规模接地式CoT推理数据集，包含18.5万个高质量实例。", "result": "在各种复杂的3D场景推理基准测试中，新框架表现出强大的性能和高接地式问答一致性。这是CoT推理首次成功应用于3D场景理解，实现了类人逐步推理。", "conclusion": "该框架是CoT推理在3D场景理解中的首次成功应用，能够实现类人逐步推理，并展现出扩展到更广泛3D场景理解场景的潜力。"}}
{"id": "2510.16729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16729", "abs": "https://arxiv.org/abs/2510.16729", "authors": ["Jianbiao Mei", "Yu Yang", "Xuemeng Yang", "Licheng Wen", "Jiajun Lv", "Botian Shi", "Yong Liu"], "title": "Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models", "comment": null, "summary": "End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.", "AI": {"tldr": "IR-WM是一种隐式残差世界模型，通过预测环境的“残差”（变化）而非完整重建未来场景，解决了现有自动驾驶世界模型在建模静态背景上的冗余，显著提升了预测和规划性能。", "motivation": "现有的以视觉为中心的自动驾驶世界模型在重建未来场景时，会消耗大量计算能力冗余地建模静态背景，效率低下。", "method": "IR-WM首先从视觉观测建立当前状态的鸟瞰图（BEV）表示。然后，它利用前一时间步的BEV特征作为时间先验，仅预测在自车动作和场景上下文条件下的“残差”（即变化）。为缓解误差累积，模型还应用了一个对齐模块来校准语义和动态错位。此外，研究还探讨了不同的预测-规划耦合方案。", "result": "IR-WM在nuScenes基准测试中，在4D占用预测和轨迹规划方面均取得了顶尖性能。模型生成的隐式未来状态显著提高了规划精度。", "conclusion": "IR-WM通过专注于建模世界状态的演变和残差，而非完整重建未来场景，有效地解决了现有世界模型的效率问题，从而在自动驾驶的预测和规划任务中实现了卓越的性能。"}}
{"id": "2510.17168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17168", "abs": "https://arxiv.org/abs/2510.17168", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "title": "When AI companions become witty: Can human brain recognize AI-generated irony?", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as social agents\nand trained to produce humor and irony, a question emerges: when encountering\nwitty AI remarks, do people interpret these as intentional communication or\nmere computational output? This study investigates whether people adopt the\nintentional stance, attributing mental states to explain behavior,toward AI\nduring irony comprehension. Irony provides an ideal paradigm because it\nrequires distinguishing intentional contradictions from unintended errors\nthrough effortful semantic reanalysis. We compared behavioral and neural\nresponses to ironic statements from AI versus human sources using established\nERP components: P200 reflecting early incongruity detection and P600 indexing\ncognitive efforts in reinterpreting incongruity as deliberate irony. Results\ndemonstrate that people do not fully adopt the intentional stance toward\nAI-generated irony. Behaviorally, participants attributed incongruity to\ndeliberate communication for both sources, though significantly less for AI\nthan human, showing greater tendency to interpret AI incongruities as\ncomputational errors. Neural data revealed attenuated P200 and P600 effects for\nAI-generated irony, suggesting reduced effortful detection and reanalysis\nconsistent with diminished attribution of communicative intent. Notably, people\nwho perceived AI as more sincere showed larger P200 and P600 effects for\nAI-generated irony, suggesting that intentional stance adoption is calibrated\nby specific mental models of artificial agents. These findings reveal that\nsource attribution shapes neural processing of social-communicative phenomena.\nDespite current LLMs' linguistic sophistication, achieving genuine social\nagency requires more than linguistic competence, it necessitates a shift in how\nhumans perceive and attribute intentionality to artificial agents.", "AI": {"tldr": "研究发现，人们在理解AI生成的反讽时，对其意图性的归因显著低于人类，这体现在行为和神经（P200、P600）层面，表明AI的社交代理需要人类对其意图性认知的转变。", "motivation": "随着大型语言模型（LLMs）被部署为社交代理并训练生成幽默和反讽，研究者想探究当人们遇到AI的机智言论时，是将其解读为有意的沟通还是单纯的计算输出。反讽提供了一个理想范式，因为它需要区分有意的矛盾和无意的错误。", "method": "研究通过比较人们对AI和人类来源的反讽陈述的行为和神经反应（使用ERP成分P200和P600），来调查人们在理解反讽时是否对AI采取意图立场（即归因心理状态来解释其行为）。P200反映早期不一致性检测，P600索引将不一致性重新解释为蓄意反讽的认知努力。", "result": "行为上，参与者将不一致性归因于蓄意沟通，但对AI的归因显著低于人类，更倾向于将AI的不一致性解释为计算错误。神经数据显示，AI生成的反讽引起P200和P600效应减弱，表明检测和重新分析的努力减少，这与沟通意图归因的降低一致。值得注意的是，那些认为AI更真诚的人对AI生成的反讽表现出更大的P200和P600效应，表明意图立场的采纳受对人工智能代理特定心理模型的校准。", "conclusion": "这些发现揭示了来源归因塑造了社会沟通现象的神经处理。尽管当前的LLMs在语言上复杂精巧，但要实现真正的社交代理，需要的不仅仅是语言能力，还需要人类对人工智能代理的感知和意图归因方式的转变。"}}
{"id": "2510.17238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17238", "abs": "https://arxiv.org/abs/2510.17238", "authors": ["Junlong Tong", "Yingqi Fan", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "StreamingThinker: Large Language Models Can Think While Reading", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}", "AI": {"tldr": "本文提出了一种名为“流式思考”（streaming thinking）的新范式，使大型语言模型（LLMs）能够在接收输入的同时进行推理，显著减少了推理开始前的等待时间和总延迟，同时保持了与传统批处理推理相当的性能。", "motivation": "当前LLMs的推理范式是在接收到全部输入后才开始思考，这导致不必要的延迟，并在动态场景中削弱了对早期信息的注意力。受人类“边读边思考”认知的启发，研究旨在解决这些效率和注意力问题。", "method": "研究设计了“流式思考”范式，并用“StreamingThinker”框架实现。该框架整合了流式思维链（CoT）生成（包含质量控制的流式推理单元）、流式约束训练（通过流式注意力掩码和位置编码强制保持推理顺序）以及流式并行推理（利用并行KV缓存解耦输入编码和推理生成，实现真正的并发）。", "result": "在Qwen3模型家族上，针对数学推理、逻辑推理和基于上下文的问答推理任务的评估结果表明，StreamingThinker在保持与批处理思考相当性能的同时，将推理开始前的token等待时间减少了80%，并将生成最终答案的时间级延迟减少了60%以上。", "conclusion": "“流式思考”范式对于LLM推理是有效的，它显著提高了推理效率，减少了延迟，而没有牺牲性能，证明了边读边思考这种新范式的可行性和优越性。"}}
{"id": "2510.17210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17210", "abs": "https://arxiv.org/abs/2510.17210", "authors": ["Chenchen Tan", "Youyang Qu", "Xinghao Li", "Hui Zhang", "Shujie Cui", "Cunjian Chen", "Longxiang Gao"], "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting", "comment": "22 pages, 10 figures", "summary": "The increase in computing power and the necessity of AI-assisted\ndecision-making boost the growing application of large language models (LLMs).\nAlong with this, the potential retention of sensitive data of LLMs has spurred\nincreasing research into machine unlearning. However, existing unlearning\napproaches face a critical dilemma: Aggressive unlearning compromises model\nutility, while conservative strategies preserve utility but risk hallucinated\nresponses. This significantly limits LLMs' reliability in knowledge-intensive\napplications. To address this, we introduce a novel Attention-Shifting (AS)\nframework for selective unlearning. AS is driven by two design objectives: (1)\ncontext-preserving suppression that attenuates attention to fact-bearing tokens\nwithout disrupting LLMs' linguistic structure; and (2) hallucination-resistant\nresponse shaping that discourages fabricated completions when queried about\nunlearning content. AS realizes these objectives through two attention-level\ninterventions, which are importance-aware suppression applied to the unlearning\nset to reduce reliance on memorized knowledge and attention-guided retention\nenhancement that reinforces attention toward semantically essential tokens in\nthe retained dataset to mitigate unintended degradation. These two components\nare jointly optimized via a dual-loss objective, which forms a soft boundary\nthat localizes unlearning while preserving unrelated knowledge under\nrepresentation superposition. Experimental results show that AS improves\nperformance preservation over the state-of-the-art unlearning methods,\nachieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC\nbenchmark, while maintaining competitive hallucination-free unlearning\neffectiveness. Compared to existing methods, AS demonstrates a superior balance\nbetween unlearning effectiveness, generalization, and response reliability.", "AI": {"tldr": "本文提出了一种新颖的注意力转移（Attention-Shifting, AS）框架，用于大型语言模型（LLM）的选择性遗忘，旨在在有效遗忘敏感信息的同时，最大限度地减少对模型效用的损害并抑制幻觉。", "motivation": "随着LLM应用的增长，其潜在的敏感数据保留问题日益突出，促使了对机器遗忘的研究。然而，现有的遗忘方法面临一个关键困境：激进的遗忘损害模型效用，而保守的策略则可能导致幻觉响应，这严重限制了LLM在知识密集型应用中的可靠性。", "method": "本文引入了注意力转移（AS）框架，通过两个设计目标实现选择性遗忘：1) 上下文保留抑制，在不破坏LLM语言结构的情况下减弱对事实性token的注意力；2) 抗幻觉响应塑形，阻止对遗忘内容的虚构完成。AS通过两个注意力层面的干预实现这些目标：对遗忘集应用重要性感知抑制以减少对记忆知识的依赖，以及通过注意力引导的保留增强来强化对保留数据集中语义关键token的注意力，以减轻意外退化。这两个组件通过双重损失目标共同优化。", "result": "实验结果表明，AS在性能保留方面优于现有最先进的遗忘方法，在ToFU基准测试中准确率提高了15%，在TDEC基准测试中提高了10%，同时保持了有竞争力的无幻觉遗忘效果。与现有方法相比，AS在遗忘有效性、泛化能力和响应可靠性之间表现出卓越的平衡。", "conclusion": "AS框架通过在注意力层面进行干预和双重损失优化，成功解决了LLM遗忘中的效用-幻觉困境，实现了遗忘有效性、性能保留和响应可靠性之间的优越平衡，显著提升了LLM在知识密集型应用中的可靠性。"}}
{"id": "2510.16730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16730", "abs": "https://arxiv.org/abs/2510.16730", "authors": ["Tianyang Dou", "Ming Li", "Jiangying Qin", "Xuan Liao", "Jiageng Zhong", "Armin Gruen", "Mengyi Deng"], "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid", "comment": null, "summary": "Coral reefs are vital yet fragile ecosystems that require accurate\nlarge-scale mapping for effective conservation. Although global products such\nas the Allen Coral Atlas provide unprecedented coverage of global coral reef\ndistri-bution, their predictions are frequently limited in spatial precision\nand semantic consistency, especially in regions requiring fine-grained boundary\ndelineation. To address these challenges, we propose UKANFormer, a novel\nse-mantic segmentation model designed to achieve high-precision mapping under\nnoisy supervision derived from Allen Coral Atlas. Building upon the UKAN\narchitecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)\nblock in the decoder, enabling the extraction of both global semantic\nstructures and local boundary details. In experiments, UKANFormer achieved a\ncoral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming\nconventional baselines under the same noisy labels setting. Remarkably, the\nmodel produces predictions that are visually and structurally more accurate\nthan the noisy labels used for training. These results challenge the notion\nthat data quality directly limits model performance, showing that architectural\ndesign can mitigate label noise and sup-port scalable mapping under imperfect\nsupervision. UKANFormer provides a foundation for ecological monitoring where\nreliable labels are scarce.", "AI": {"tldr": "UKANFormer是一种新型语义分割模型，用于在噪声监督下实现高精度珊瑚礁测绘，其独特的架构设计能有效缓解标签噪声问题，并生成比训练数据更准确的预测结果。", "motivation": "珊瑚礁是脆弱的生态系统，需要准确的大规模测绘以进行有效保护。尽管现有全球产品（如Allen Coral Atlas）提供了广泛覆盖，但其预测在空间精度和语义一致性方面存在局限，尤其是在需要精细边界划分的区域，且面临标签噪声问题。", "method": "本文提出了UKANFormer，一个基于UKAN架构的语义分割模型。它在解码器中引入了一个全局-局部Transformer (GL-Trans) 模块，旨在同时提取全局语义结构和局部边界细节，以应对Allen Coral Atlas衍生的噪声监督。", "result": "实验中，UKANFormer在珊瑚类别上实现了67.00%的IoU和83.98%的像素准确率，在相同的噪声标签设置下优于传统基线模型。值得注意的是，该模型生成的预测结果在视觉和结构上比用于训练的噪声标签更加准确。", "conclusion": "研究结果挑战了数据质量直接限制模型性能的观念，表明通过精巧的架构设计可以有效缓解标签噪声，支持在不完美监督下进行可扩展的测绘。UKANFormer为标签稀缺的生态监测提供了基础。"}}
{"id": "2510.17196", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17196", "abs": "https://arxiv.org/abs/2510.17196", "authors": ["Jiaqi Leng", "Xiang Hu", "Junxiong Wang", "Jianguo Li", "Wei Wu", "Yucheng Lu"], "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models", "comment": "Preprint. Work in progress", "summary": "Effectively processing long contexts is a critical challenge for language\nmodels. While standard Transformers are limited by quadratic complexity and\npoor length extrapolation, alternative architectures like sliding window\nattention and state space models sacrifice the ability to effectively utilize\nthe full context due to their fixed-size memory. Chunk-based sparse attention\nhas emerged as a promising paradigm for extreme length generalization, yet the\nkey architectural principles underpinning its success are not yet fully\nunderstood. In this work, we present a systematic dissection of these models to\nidentify the core components driving their performance. Through a unified\nframework and comprehensive ablation studies, we demonstrate that a combination\nof three design principles is critical: (1) an expressive, non-linear Chunk\nEncoder with a dedicated CLS token to produce representations for retrieval;\n(2) a Bypassing Residual Path to stably integrate retrieved global information\nwithout it being overridden by the local residual stream; and (3) enforced\nselection sparsity during pre-training to bridge the train-test distribution\ngap. We provide a theoretical motivation for intra-chunk information processing\nand landmark generation. By combining these principles, we establish a new\nstate-of-the-art for training-free length extrapolation, successfully\ngeneralizing models trained on a 4K context to 32 million tokens on RULER and\nBABILong. Our findings provide a clear and empirically-grounded set of design\nprinciples for developing future, highly-capable long-context language models.", "AI": {"tldr": "本文系统性地剖析了分块稀疏注意力模型，识别出三个关键设计原则：表达性非线性分块编码器、旁路残差路径和预训练期间强制选择稀疏性。结合这些原则，实现了训练无关的长文本外推新SOTA，将4K上下文模型泛化到3200万tokens。", "motivation": "语言模型有效处理长上下文是一个关键挑战。标准Transformer受限于二次复杂度，而其他架构（如滑动窗口注意力、状态空间模型）则因固定大小内存而牺牲了有效利用完整上下文的能力。分块稀疏注意力在极端长度泛化方面前景广阔，但其成功的核心架构原则尚不完全明确。", "method": "本文通过统一框架和全面的消融研究，对分块稀疏注意力模型进行了系统性剖析，以识别驱动其性能的核心组件。研究确定了三个关键设计原则，并提供了分块内信息处理和地标生成的理论依据。", "result": "研究发现，结合以下三个设计原则至关重要：(1) 具有专用CLS token的表达性非线性分块编码器，用于生成检索表示；(2) 旁路残差路径，用于稳定集成检索到的全局信息，防止被局部残差流覆盖；(3) 预训练期间强制选择稀疏性，以弥合训练-测试分布差距。通过结合这些原则，在RULER和BABILong数据集上实现了训练无关长度外推的新SOTA，成功将4K上下文训练的模型泛化到3200万tokens。", "conclusion": "本文的研究结果为开发未来高性能长上下文语言模型提供了一套清晰且有经验依据的设计原则。"}}
{"id": "2510.17247", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17247", "abs": "https://arxiv.org/abs/2510.17247", "authors": ["Zefan Cai", "Haoyi Qiu", "Haozhe Zhao", "Ke Wan", "Jiachen Li", "Jiuxiang Gu", "Wen Xiao", "Nanyun Peng", "Junjie Hu"], "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models", "comment": null, "summary": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.", "AI": {"tldr": "本文提出了VideoBiasEval框架，用于系统性评估文本到视频扩散模型中的社会偏见。研究发现，对齐微调不仅会加剧表示偏见，还会使其在时间上更稳定，导致更平滑但更刻板的描绘。", "motivation": "尽管视频扩散模型在文本到视频生成方面取得了显著进展，并通过奖励模型进行对齐微调提升了视觉质量，但这些方法可能无意中编码和放大了社会偏见。因此，需要一个系统的方法来追踪这些偏见在对齐流程中的演变。", "method": "引入VideoBiasEval诊断框架，基于既定的社会偏见分类法，采用事件驱动的提示策略来分离语义内容（动作和上下文）与演员属性（性别和种族）。该框架还引入了多粒度指标，用于评估：1) 整体种族偏见，2) 基于种族的性别偏见，3) 模型变体中社会属性的分布变化，以及 4) 视频中偏见的时间持久性。利用此框架，研究人员对人类偏好数据集中的偏见、奖励模型中的放大以及对齐微调视频扩散模型中的传播进行了首次端到端分析。", "result": "研究结果表明，对齐微调不仅强化了表示偏见，还使其在时间上更稳定，从而产生了更平滑但更刻板的描绘。", "conclusion": "这些发现强调了在整个对齐过程中进行偏见感知评估和缓解的必要性，以确保公平和对社会负责的视频生成。"}}
{"id": "2510.16751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16751", "abs": "https://arxiv.org/abs/2510.16751", "authors": ["Erik Riise", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling", "comment": null, "summary": "While inference-time scaling through search has revolutionized Large Language\nModels, translating these gains to image generation has proven difficult.\nRecent attempts to apply search strategies to continuous diffusion models show\nlimited benefits, with simple random sampling often performing best. We\ndemonstrate that the discrete, sequential nature of visual autoregressive\nmodels enables effective search for image generation. We show that beam search\nsubstantially improves text-to-image generation, enabling a 2B parameter\nautoregressive model to outperform a 12B parameter diffusion model across\nbenchmarks. Systematic ablations show that this advantage comes from the\ndiscrete token space, which allows early pruning and computational reuse, and\nour verifier analysis highlights trade-offs between speed and reasoning\ncapability. These findings suggest that model architecture, not just scale, is\ncritical for inference-time optimization in visual generation.", "AI": {"tldr": "研究表明，束搜索能显著提升离散自回归模型在文本到图像生成中的性能，使其超越更大参数的扩散模型，并强调了模型架构在视觉生成推理优化中的重要性。", "motivation": "推理时搜索在大型语言模型中取得了巨大成功，但在图像生成（尤其是连续扩散模型）中效果不佳。研究旨在探索如何将搜索策略有效应用于图像生成。", "method": "将束搜索（beam search）应用于离散、序列化的视觉自回归模型进行图像生成。通过系统性消融实验和验证器分析，探究其优势来源和速度与推理能力之间的权衡。", "result": "束搜索显著提升了文本到图像生成的性能。一个20亿参数的自回归模型在基准测试中超越了120亿参数的扩散模型。这种优势源于离散的token空间，它允许早期剪枝和计算复用。验证器分析揭示了速度和推理能力之间的权衡。", "conclusion": "模型架构（而非仅仅模型规模）对于视觉生成中的推理时优化至关重要，特别是离散的token空间是实现有效搜索的关键。"}}
{"id": "2510.17252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17252", "abs": "https://arxiv.org/abs/2510.17252", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Ayesha Siddiqua", "Jungpil Shin"], "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design", "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)", "summary": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.", "AI": {"tldr": "本研究通过大规模情感分析，发现孟加拉语新闻标题和内容中负面情绪（尤其是愤怒、恐惧和失望）占主导地位，并且不同媒体对相似事件的情感描绘存在显著差异。基于此，论文提出了一个以人为本的新闻聚合器设计理念，旨在帮助读者识别隐藏的情感框架。", "motivation": "新闻媒体不仅通过报道内容，还通过其框架方式塑造公众情绪。负面或情绪化的标题更容易吸引注意并传播，这促使媒体以更强烈的方式构建故事。本研究旨在探讨这种倾向。", "method": "研究对30万条孟加拉语新闻标题及其内容进行了大规模情感分析。使用Gemma-3 4B进行零样本推理，以识别每条新闻的主导情绪和整体基调。", "result": "研究结果显示，孟加拉语新闻中负面情绪（特别是愤怒、恐惧和失望）明显占主导地位。此外，不同媒体对相似报道的情感描绘存在显著差异。", "conclusion": "基于研究洞察，论文提出了一种以人为本的新闻聚合器设计理念。该聚合器能够可视化情感线索，帮助读者识别日常新闻中隐藏的情感框架。"}}
{"id": "2510.16732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16732", "abs": "https://arxiv.org/abs/2510.16732", "authors": ["Xinqing Li", "Xin He", "Le Zhang", "Yun Liu"], "title": "A Comprehensive Survey on World Models for Embodied AI", "comment": "https://github.com/Li-Zn-H/AwesomeWorldModels", "summary": "Embodied AI requires agents that perceive, act, and anticipate how actions\nreshape future world states. World models serve as internal simulators that\ncapture environment dynamics, enabling forward and counterfactual rollouts to\nsupport perception, prediction, and decision making. This survey presents a\nunified framework for world models in embodied AI. Specifically, we formalize\nthe problem setting and learning objectives, and propose a three-axis taxonomy\nencompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)\nTemporal Modeling, Sequential Simulation and Inference vs. Global Difference\nPrediction; (3) Spatial Representation, Global Latent Vector, Token Feature\nSequence, Spatial Latent Grid, and Decomposed Rendering Representation. We\nsystematize data resources and metrics across robotics, autonomous driving, and\ngeneral video settings, covering pixel prediction quality, state-level\nunderstanding, and task performance. Furthermore, we offer a quantitative\ncomparison of state-of-the-art models and distill key open challenges,\nincluding the scarcity of unified datasets and the need for evaluation metrics\nthat assess physical consistency over pixel fidelity, the trade-off between\nmodel performance and the computational efficiency required for real-time\ncontrol, and the core modeling difficulty of achieving long-horizon temporal\nconsistency while mitigating error accumulation. Finally, we maintain a curated\nbibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.", "AI": {"tldr": "本综述提出了具身AI中世界模型的统一框架，包括三轴分类法、数据资源、评估指标、模型比较和开放性挑战。", "motivation": "具身AI智能体需要感知、行动并预测行动如何改变未来世界状态。世界模型作为内部模拟器，能够捕捉环境动态，支持感知、预测和决策，因此需要对该领域进行系统化和统一的概述。", "method": "本研究通过以下方法进行：1) 形式化问题设置和学习目标；2) 提出一个三轴分类法：功能性（决策耦合 vs. 通用）、时间建模（序列模拟与推理 vs. 全局差异预测）和空间表示（全局潜在向量、令牌特征序列、空间潜在网格、分解渲染表示）；3) 系统化机器人、自动驾驶和通用视频设置中的数据资源和评估指标；4) 对现有最先进模型进行定量比较；5) 提炼出关键的开放性挑战。", "result": "本研究提供了具身AI中世界模型的统一框架和形式化定义，提出了一个全面的三轴分类法，系统化了数据资源和评估指标，并对最先进模型进行了定量比较。同时，揭示了该领域的关键开放性挑战，包括统一数据集的稀缺、评估物理一致性而非像素保真度的需求、模型性能与实时控制计算效率之间的权衡，以及实现长期时间一致性同时减轻误差累积的核心建模难题。", "conclusion": "本综述为具身AI中的世界模型提供了一个全面的概述、统一框架和深入分析，明确了当前进展并指出了未来研究的关键方向。"}}
{"id": "2510.16752", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16752", "abs": "https://arxiv.org/abs/2510.16752", "authors": ["Ivan Molodetskikh", "Kirill Malyshev", "Mark Mirgaleev", "Nikita Zagainov", "Evgeney Bogatyrev", "Dmitriy Vatolin"], "title": "Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution", "comment": null, "summary": "Generative image super-resolution (SR) is rapidly advancing in visual quality\nand detail restoration. As the capacity of SR models expands, however, so does\ntheir tendency to produce artifacts: incorrect, visually disturbing details\nthat reduce perceived quality. Crucially, their perceptual impact varies: some\nartifacts are barely noticeable while others strongly degrade the image. We\nargue that artifacts should be characterized by their prominence to human\nobservers rather than treated as uniform binary defects. Motivated by this, we\npresent a novel dataset of 1302 artifact examples from 11 contemporary image-SR\nmethods, where each artifact is paired with a crowdsourced prominence score.\nBuilding on this dataset, we train a lightweight regressor that produces\nspatial prominence heatmaps and outperforms existing methods at detecting\nprominent artifacts. We release the dataset and code to facilitate\nprominence-aware evaluation and mitigation of SR artifacts.", "AI": {"tldr": "生成式图像超分辨率（SR）模型在产生视觉质量和细节的同时也容易产生伪影。本文提出根据人类感知的重要性来表征这些伪影，构建了一个包含众包重要性评分的伪影数据集，并训练了一个轻量级回归器来检测显著伪影，该回归器在检测方面优于现有方法。", "motivation": "随着SR模型能力的增强，它们也倾向于产生伪影，这些伪影会降低感知质量。然而，不同伪影的感知影响差异很大，有些几乎不引人注意，有些则严重损害图像。研究认为，伪影应根据其对人类观察者的显著性来表征，而不是被视为统一的二元缺陷。", "method": "构建了一个包含11种当代图像SR方法生成的1302个伪影实例的新数据集，每个伪影都配有众包获得的重要性评分。在此数据集基础上，训练了一个轻量级回归器。", "result": "训练出的回归器能够生成空间重要性热图，并且在检测显著伪影方面优于现有方法。", "conclusion": "论文发布了数据集和代码，以促进对SR伪影进行重要性感知评估和缓解。"}}
{"id": "2510.16772", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16772", "abs": "https://arxiv.org/abs/2510.16772", "authors": ["Thuy Phuong Vu", "Dinh-Cuong Hoang", "Minhhuy Le", "Phan Xuan Tan"], "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning", "comment": null, "summary": "Recent research has made significant progress in localizing and editing image\nregions based on text. However, most approaches treat these regions in\nisolation, relying solely on local cues without accounting for how each part\ncontributes to the overall visual and semantic composition. This often results\nin inconsistent edits, unnatural transitions, or loss of coherence across the\nimage. In this work, we propose Region in Context, a novel framework for\ntext-conditioned image editing that performs multilevel semantic alignment\nbetween vision and language, inspired by the human ability to reason about\nedits in relation to the whole scene. Our method encourages each region to\nunderstand its role within the global image context, enabling precise and\nharmonized changes. At its core, the framework introduces a dual-level guidance\nmechanism: regions are represented with full-image context and aligned with\ndetailed region-level descriptions, while the entire image is simultaneously\nmatched to a comprehensive scene-level description generated by a large\nvision-language model. These descriptions serve as explicit verbal references\nof the intended content, guiding both local modifications and global structure.\nExperiments show that it produces more coherent and instruction-aligned\nresults. Code is available at:\nhttps://github.com/thuyvuphuong/Region-in-Context.git", "AI": {"tldr": "该研究提出了一种名为“上下文区域”（Region in Context）的新框架，通过多层次的视觉与语言语义对齐，实现了更连贯、更符合指令的文本条件图像编辑。", "motivation": "现有的图像编辑方法通常孤立地处理图像区域，仅依赖局部线索，而忽略了每个部分对整体视觉和语义构图的贡献。这导致编辑结果经常出现不一致、不自然的过渡或图像整体连贯性受损。", "method": "本文提出“上下文区域”框架，灵感来源于人类对整体场景中编辑推理的能力。核心是一个双层指导机制：1) 区域在完整图像上下文中表示，并与详细的区域级描述对齐；2) 整个图像同时与大型视觉-语言模型生成的综合场景级描述匹配。这些描述作为预期内容的明确语言参考，指导局部修改和全局结构。", "result": "实验表明，该方法能够产生更连贯且更符合指令的图像编辑结果。", "conclusion": "通过引入多层次语义对齐和双层指导机制，使每个区域理解其在全局图像上下文中的作用，该框架显著提升了文本条件图像编辑的精确性和和谐性，克服了现有方法在处理区域孤立性方面的不足。"}}
{"id": "2510.16765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16765", "abs": "https://arxiv.org/abs/2510.16765", "authors": ["Shengyu Zhu", "Fan", "Fuxuan Zhang"], "title": "WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement", "comment": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV),\n  Oral", "summary": "Image restoration is a fundamental and challenging task in computer vision,\nwhere CNN-based frameworks demonstrate significant computational efficiency.\nHowever, previous CNN-based methods often face challenges in adequately\nrestoring fine texture details, which are limited by the small receptive field\nof CNN structures and the lack of channel feature modeling. In this paper, we\npropose WaMaIR, which is a novel framework with a large receptive field for\nimage perception and improves the reconstruction of texture details in restored\nimages. Specifically, we introduce the Global Multiscale Wavelet Transform\nConvolutions (GMWTConvs) for expandding the receptive field to extract image\nfeatures, preserving and enriching texture features in model inputs. Meanwhile,\nwe propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to\ncapture long-range dependencies within feature channels, which enhancing the\nmodel sensitivity to color, edges, and texture information. Additionally, we\npropose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to\nguide the model in preserving detailed texture structures effectively.\nExtensive experiments confirm that WaMaIR outperforms state-of-the-art methods,\nachieving better image restoration and efficient computational performance of\nthe model.", "AI": {"tldr": "本文提出WaMaIR，一个用于图像恢复的新框架，通过引入全局多尺度小波变换卷积、基于Mamba的通道感知模块和多尺度纹理增强损失，有效扩大感受野、建模通道特征并增强纹理细节重建，超越了现有先进方法。", "motivation": "现有的基于CNN的图像恢复方法在恢复精细纹理细节方面面临挑战，这主要受限于CNN结构较小的感受野以及缺乏对通道特征的建模。", "method": "本文提出了WaMaIR框架，包含以下核心组件：1. 全局多尺度小波变换卷积（GMWTConvs），用于扩大感受野，提取并丰富图像纹理特征；2. 基于Mamba的通道感知模块（MCAM），旨在捕捉特征通道内的长距离依赖，增强模型对颜色、边缘和纹理信息的敏感性；3. 多尺度纹理增强损失（MTELoss），用于指导模型有效保留详细的纹理结构。", "result": "广泛的实验证明，WaMaIR在图像恢复方面超越了现有最先进的方法，实现了更好的图像恢复质量和高效的计算性能。", "conclusion": "WaMaIR是一个新颖的图像恢复框架，通过创新的模块设计有效解决了现有方法在恢复精细纹理细节方面的局限性，并在性能上取得了显著提升。"}}
{"id": "2510.17256", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17256", "abs": "https://arxiv.org/abs/2510.17256", "authors": ["Shahin Atakishiyev", "Housam K. B. Babiker", "Jiayi Dai", "Nawshad Farruque", "Teruaki Hayashi", "Nafisa Sadaf Hriti", "Md Abed Rahman", "Iain Smith", "Mi-Young Kim", "Osmar R. Zaïane", "Randy Goebel"], "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations", "comment": null, "summary": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations.", "AI": {"tldr": "本文探讨了大型语言模型（LLMs）的局部可解释性和机制可解释性，旨在增强对LLMs的信任，并通过文献综述、实验研究（在医疗保健和自动驾驶领域）以及对未解决问题的总结来提供人类对齐的、可信赖的LLM解释。", "motivation": "大型语言模型虽然性能卓越，但其预测和内容生成过程对人类而言普遍难以理解，且常出现预测和推理错误（如幻觉）。这凸显了迫切需要更好地理解和解释LLM的内部运作及其预测输出，以弥补信任鸿沟。", "method": "本文主要采用三种方法：1. 回顾局部可解释性和机制可解释性方法及相关研究见解。2. 描述在医疗保健和自动驾驶这两个关键领域中，关于LLM可解释性和推理的实验研究，并分析此类解释对接收者信任的影响。3. 总结LLM可解释性领域当前未解决的问题，并概述未来方向、机遇和挑战，以生成人类对齐的、可信赖的LLM解释。", "result": "本文提出了三项主要贡献：1. 对局部可解释性和机制可解释性方法及其相关研究见解进行了综述。2. 描述了在医疗保健和自动驾驶领域，关于LLM可解释性和推理的实验研究，并分析了其对解释接收者信任的影响。3. 总结了LLM可解释性领域当前未解决的问题，并概述了生成人类对齐的、可信赖的LLM解释的机遇、挑战和未来方向。", "conclusion": "通过对局部可解释性和机制可解释性的研究，并结合在关键领域的实验分析，本文旨在促进对Transformer基大型语言模型的信任。未来的工作将集中于解决现有问题，并指明生成人类对齐且可信赖的LLM解释的方向。"}}
{"id": "2510.16776", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16776", "abs": "https://arxiv.org/abs/2510.16776", "authors": ["Mingzheng Zhang", "Jinfeng Gao", "Dan Xu", "Jiangrui Yu", "Yuhan Qiao", "Lan Chen", "Jin Tang", "Xiao Wang"], "title": "EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation", "comment": null, "summary": "X-ray image-based medical report generation (MRG) is a pivotal area in\nartificial intelligence that can significantly reduce diagnostic burdens for\nclinicians and patient wait times. Existing MRG models predominantly rely on\nLarge Language Models (LLMs) to improve report generation, with limited\nexploration of pre-trained vision foundation models or advanced fine-tuning\ntechniques. Mainstream frameworks either avoid fine-tuning or utilize\nsimplistic methods like LoRA, often neglecting the potential of enhancing\ncross-attention mechanisms. Additionally, while Transformer-based models\ndominate vision-language tasks, non-Transformer architectures, such as the\nMamba network, remain underexplored for medical report generation, presenting a\npromising avenue for future research. In this paper, we propose EMRRG, a novel\nX-ray report generation framework that fine-tunes pre-trained Mamba networks\nusing parameter-efficient methods. Specifically, X-ray images are divided into\npatches, tokenized, and processed by an SSM-based vision backbone for feature\nextraction, with Partial LoRA yielding optimal performance. An LLM with a\nhybrid decoder generates the medical report, enabling end-to-end training and\nachieving strong results on benchmark datasets. Extensive experiments on three\nwidely used benchmark datasets fully validated the effectiveness of our\nproposed strategies for the X-ray MRG. The source code of this paper will be\nreleased on https://github.com/Event-AHU/Medical_Image_Analysis.", "AI": {"tldr": "本文提出EMRRG框架，通过参数高效微调预训练Mamba网络（一种非Transformer架构）来生成X射线医学报告，并在基准数据集上取得了显著效果。", "motivation": "现有医学报告生成模型过度依赖大型语言模型（LLMs），而忽视了预训练视觉基础模型或高级微调技术，特别是对交叉注意力机制的增强。此外，非Transformer架构（如Mamba网络）在医学报告生成领域的潜力尚未被充分探索。", "method": "本文提出了EMRRG框架。该框架将X射线图像分割、标记后，由基于SSM的视觉骨干网络（Mamba网络）进行特征提取，并采用参数高效方法（特别是Partial LoRA）进行微调。报告生成则由带有混合解码器的LLM完成，实现端到端训练。", "result": "EMRRG在三个广泛使用的基准数据集上取得了优异的报告生成结果，充分验证了所提出策略的有效性。", "conclusion": "EMRRG框架通过结合参数高效微调的Mamba网络和LLM，为X射线医学报告生成提供了一个有效且新颖的解决方案，超越了现有主流方法的局限性。"}}
{"id": "2510.17263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17263", "abs": "https://arxiv.org/abs/2510.17263", "authors": ["Avishek Lahiri", "Yufang Hou", "Debarshi Kumar Sanyal"], "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models", "comment": "This paper has been accepted at the EMNLP 2025 Main Conference", "summary": "Taxonomies play a crucial role in helping researchers structure and navigate\nknowledge in a hierarchical manner. They also form an important part in the\ncreation of comprehensive literature surveys. The existing approaches to\nautomatic survey generation do not compare the structure of the generated\nsurveys with those written by human experts. To address this gap, we present\nour own method for automated taxonomy creation that can bridge the gap between\nhuman-generated and automatically-created taxonomies. For this purpose, we\ncreate the CS-TaxoBench benchmark which consists of 460 taxonomies that have\nbeen extracted from human-written survey papers. We also include an additional\ntest set of 80 taxonomies curated from conference survey papers. We propose\nTaxoAlign, a three-phase topic-based instruction-guided method for scholarly\ntaxonomy generation. Additionally, we propose a stringent automated evaluation\nframework that measures the structural alignment and semantic coherence of\nautomatically generated taxonomies in comparison to those created by human\nexperts. We evaluate our method and various baselines on CS-TaxoBench, using\nboth automated evaluation metrics and human evaluation studies. The results\nshow that TaxoAlign consistently surpasses the baselines on nearly all metrics.\nThe code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.", "AI": {"tldr": "本文提出了一种自动化分类法创建方法TaxoAlign，旨在缩小人与机器生成分类法之间的差距。通过创建CS-TaxoBench基准和严格的评估框架，TaxoAlign在结构对齐和语义连贯性方面显著优于现有基线。", "motivation": "现有的自动化综述生成方法未能将生成综述的结构与人类专家撰写的结构进行比较，导致人与机器创建的分类法之间存在差距。研究人员需要更好的工具来结构化和导航知识，并创建全面的文献综述。", "method": "研究人员创建了CS-TaxoBench基准，包含460个从人工撰写综述论文中提取的分类法，以及额外80个从会议综述论文中整理的测试集。他们提出了TaxoAlign，这是一种三阶段、基于主题、指令引导的学术分类法生成方法。此外，他们还提出了一个严格的自动化评估框架，用于衡量自动生成分类法与人类专家创建分类法在结构对齐和语义连贯性方面的表现。", "result": "TaxoAlign在CS-TaxoBench上，通过自动化评估指标和人工评估研究，在几乎所有指标上都持续超越了基线方法。", "conclusion": "TaxoAlign是一种有效的自动化分类法创建方法，能够有效弥合人与机器生成分类法之间的结构和语义差距，并在广泛的评估中表现出色。"}}
{"id": "2510.17289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17289", "abs": "https://arxiv.org/abs/2510.17289", "authors": ["Hajar Bakarou", "Mohamed Sinane El Messoussi", "Anaïs Ollagnier"], "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning", "comment": null, "summary": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.", "AI": {"tldr": "本研究利用法语数据集CyberAgressionAdo-Large，在多方对话环境中评估了社交媒体上的反社会行为（ASB），发现多模态模型（特别是mBERT + WD-SGCN晚期融合模型）在ASB检测、欺凌行为分析和欺凌同伴群体识别方面表现最佳。", "motivation": "社交媒体上的反社会行为（如仇恨言论、骚扰和网络欺凌）日益威胁平台安全和社会福祉。现有研究主要集中在如X和Reddit等网络上，而多方对话设置因数据有限而未被充分探索，这促使本研究填补这一空白。", "method": "研究使用了开源法语数据集CyberAgressionAdo-Large，该数据集模拟了多方对话中的ASB。评估了三个任务：滥用检测、欺凌行为分析和欺凌同伴群体识别。基准测试了六种基于文本和八种基于图的表示学习方法，并分析了词汇线索、互动动态及其多模态融合。", "result": "结果显示，多模态模型优于单模态基线模型。晚期融合模型mBERT + WD-SGCN取得了最佳的总体结果，在滥用检测（0.718）上表现出色，在同伴群体识别（0.286）和欺凌分析（0.606）上也具有竞争力。错误分析表明其在处理隐含攻击性、角色转换和语境依赖性敌意等细微ASB现象方面非常有效。", "conclusion": "多模态融合模型，特别是mBERT + WD-SGCN，在多方对话环境中对反社会行为的检测和分析表现出卓越的性能，尤其擅长处理微妙的ASB现象。"}}
{"id": "2510.16777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16777", "abs": "https://arxiv.org/abs/2510.16777", "authors": ["Junbo Li", "Weimin Yuan", "Yinuo Wang", "Yue Zeng", "Shihao Shu", "Cai Meng", "Xiangzhi Bai"], "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation", "comment": null, "summary": "Accurate 6D pose estimation of 3D objects is a fundamental task in computer\nvision, and current research typically predicts the 6D pose by establishing\ncorrespondences between 2D image features and 3D model features. However, these\nmethods often face difficulties with textureless objects and varying\nillumination conditions. To overcome these limitations, we propose GS2POSE, a\nnovel approach for 6D object pose estimation. GS2POSE formulates a pose\nregression algorithm inspired by the principles of Bundle Adjustment (BA). By\nleveraging Lie algebra, we extend the capabilities of 3DGS to develop a\npose-differentiable rendering pipeline, which iteratively optimizes the pose by\ncomparing the input image to the rendered image. Additionally, GS2POSE updates\ncolor parameters within the 3DGS model, enhancing its adaptability to changes\nin illumination. Compared to previous models, GS2POSE demonstrates accuracy\nimprovements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and\nLineMod datasets, respectively.", "AI": {"tldr": "GS2POSE是一种新颖的6D物体姿态估计方法，它借鉴Bundle Adjustment原理，利用李代数扩展3DGS以实现姿态可微分渲染，通过迭代优化姿态并更新颜色参数，解决了无纹理物体和光照变化下的挑战，并在多个数据集上提高了精度。", "motivation": "当前的6D姿态估计方法通常通过建立2D-3D特征对应来预测姿态，但在处理无纹理物体和多变光照条件时面临困难。", "method": "GS2POSE提出了一种受Bundle Adjustment (BA) 启发的姿态回归算法。它利用李代数扩展3DGS，构建了一个姿态可微分的渲染管线，通过比较输入图像与渲染图像来迭代优化姿态。此外，GS2POSE还更新3DGS模型中的颜色参数，以增强其对光照变化的适应性。", "result": "与现有模型相比，GS2POSE在T-LESS、LineMod-Occlusion和LineMod数据集上分别实现了1.4%、2.8%和2.5%的精度提升。", "conclusion": "GS2POSE为6D物体姿态估计提供了一种新颖且更鲁棒的方法，有效克服了无纹理物体和多变光照条件下的局限性，并展现出显著的精度改进。"}}
{"id": "2510.17354", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17354", "abs": "https://arxiv.org/abs/2510.17354", "authors": ["Chenghao Zhang", "Guanting Dong", "Xinyu Yang", "Zhicheng Dou"], "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation", "comment": "This work is in progress", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.", "AI": {"tldr": "本文提出了一种名为Nyx的统一混合模态检索器，用于解决通用检索增强生成（URAG）中的混合模态信息检索和推理问题，并通过构建NyxQA数据集和两阶段训练框架，显著提升了视觉-语言生成任务的质量。", "motivation": "现有的检索增强生成（RAG）系统主要关注单模态文本，但在真实世界场景中，查询和文档通常包含文本和图像等混合模态信息，这限制了其应用。研究旨在解决如何检索和推理混合模态信息以改进视觉-语言生成。", "method": "本文提出了Nyx，一个统一的混合模态到混合模态检索器，专为URAG场景设计。为缓解混合模态数据稀缺问题，引入了一个四阶段自动化生成和过滤流程，利用网络文档构建了NyxQA数据集。Nyx采用两阶段训练框架：首先在NyxQA和多种开源检索数据集上进行预训练，然后利用下游视觉-语言模型（VLM）的反馈进行监督微调，以使检索输出与生成偏好对齐。", "result": "实验结果表明，Nyx不仅在标准纯文本RAG基准测试中表现出色，而且在更通用和真实的URAG设置中也表现优异，显著提高了视觉-语言任务的生成质量。", "conclusion": "Nyx成功应对了通用检索增强生成（URAG）中混合模态信息检索和推理的挑战，通过创新的检索器设计和数据构建方法，有效提升了视觉-语言生成模型的性能，展现了其在真实世界混合模态场景中的潜力。"}}
{"id": "2510.16781", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16781", "abs": "https://arxiv.org/abs/2510.16781", "authors": ["Shihao Ji", "Zihui Song"], "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features", "comment": null, "summary": "The remarkable zero-shot reasoning capabilities of large-scale Visual\nLanguage Models (VLMs) on static images have yet to be fully translated to the\nvideo domain. Conventional video understanding models often rely on extensive,\ntask-specific training on annotated datasets, a process that is both costly and\nlimited in scalability. This paper introduces a novel, training-free framework\nfor video understanding that circumvents end-to-end training by synergistically\ncombining the rich semantic priors of pre-trained VLMs with classic machine\nlearning algorithms for pattern discovery. Our core idea is to reframe video\nunderstanding as a self-supervised spatio-temporal clustering problem within a\nhigh-dimensional semantic feature space. The proposed pipeline first transforms\na video stream into a semantic feature trajectory using the frozen visual\nencoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal\nSegmentation (KTS), a robust machine learning technique, to partition the\ncontinuous feature stream into discrete, semantically coherent event segments.\nThese segments are then subjected to unsupervised density-based clustering to\nidentify recurring macroscopic scenes and themes throughout the video. By\nselecting representative keyframes from each discovered cluster and leveraging\nthe VLM's generative capabilities for textual description, our framework\nautomatically produces a structured, multi-modal summary of the video content.\nThis approach provides an effective, interpretable, and model-agnostic pathway\nfor zero-shot, automated structural analysis of video content.", "AI": {"tldr": "本文提出了一种新颖的、免训练的视频理解框架，通过结合预训练视觉语言模型（VLM）的语义先验知识和经典机器学习算法（如时间分割和聚类），实现视频内容的零样本结构化分析和多模态摘要。", "motivation": "大规模视觉语言模型（VLM）在静态图像上的零样本推理能力尚未完全应用于视频领域。传统的视频理解模型通常依赖于大量、特定任务的标注数据集训练，这种方式成本高昂且可扩展性有限。", "method": "该方法将视频理解重构为一个高维语义特征空间中的自监督时空聚类问题。首先，使用预训练VLM的冻结视觉编码器将视频流转换为语义特征轨迹。接着，采用核时间分割（KTS）技术将连续特征流分割成离散的、语义连贯的事件片段。然后，对这些片段进行无监督的基于密度的聚类，以识别视频中重复出现的宏观场景和主题。最后，从每个聚类中选择代表性关键帧，并利用VLM的生成能力生成文本描述，从而自动生成视频内容的多模态结构化摘要。", "result": "该框架能够自动生成视频内容的结构化、多模态摘要。", "conclusion": "该方法为视频内容的零样本、自动化结构分析提供了一条有效、可解释且与模型无关的途径。"}}
{"id": "2510.17388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17388", "abs": "https://arxiv.org/abs/2510.17388", "authors": ["Henry Lim", "Kwan Hui Lim"], "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives", "comment": "11 pages, 1 figure, 8 tables", "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.", "AI": {"tldr": "本研究发现，指令微调大型语言模型（IT-LLMs）在执行简单、独立指令方面存在不足，尤其对选项标签格式的变化敏感，暴露出指令格式偏差和原子指令遵循能力弱的问题。", "motivation": "尽管指令微调大型语言模型（IT-LLMs）展现出强大的零样本推理能力，但它们执行简单、独立指令的能力却鲜有探索，而这正是复杂指令遵循的基础。", "method": "研究评估了20个IT-LLMs在修改后的MMLU和MMLU-Pro基准上的表现。通过系统地改变选项标签的格式（字母、数字、罗马数字），并在四种范式下保持其含义不变进行测试：1) 有明确指令；2) 无指令；3) 移除选项内容；4) 提供三样本示例。同时进行了生成分析。", "result": "1) 在有明确指令的情况下，标签格式变化导致性能大幅下降（例如，罗马数字相比数字下降30.45%），揭示了指令格式偏差。2) 无指令时，性能进一步下降且标签敏感性增强。3) 移除选项内容后，模型除了在数字标签外，均未能达到随机选择基线，表明对原子指令的遵循能力较弱。4) 三样本示例未能显著提升鲁棒性或忠实度，生成分析显示持续的标签错误，尤其对于非数字格式。更大规模的LLM虽然准确率更高，但在指令遵循上仍不一致。", "conclusion": "当前指令微调范式存在不足，需要新的评估方法和训练策略来明确地针对原子指令遵循能力进行提升。"}}
{"id": "2510.17389", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17389", "abs": "https://arxiv.org/abs/2510.17389", "authors": ["Numaan Naeem", "Abdellah El Mekki", "Muhammad Abdul-Mageed"], "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs", "comment": "28 pages, 2 figures, 14 tables, 50 listings, EMNLP 2025 Main", "summary": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt.", "AI": {"tldr": "该研究引入了EduAdapt，一个包含近4.8万个按年级标注的科学问答对基准，用于评估大型语言模型（LLMs）在K-12教育中调整输出以适应学生年级水平的能力，发现LLMs在此方面表现不佳，尤其对低年级学生。", "motivation": "尽管LLMs在学术基准上表现出色，但它们在K-12教育中未能根据学生的年级水平调整回答，导致输出对于年幼学习者来说过于高级或模糊，且缺乏评估这种适应能力的标准化基准。", "method": "引入了EduAdapt基准，包含近4.8万个按年级标注的问答对，涵盖9个科学科目，跨越1-12年级并分为四个年级组。研究评估了多种开源LLMs在该基准上的表现。", "result": "评估结果显示，虽然更大的模型通常表现更好，但它们在为低年级学生（1-5年级）生成合适回答方面仍然存在困难。", "conclusion": "EduAdapt是首个用于评估LLMs年级水平适应性的数据集和评估框架，旨在通过改进训练和提示策略，促进开发更符合学生发展阶段的教育AI系统。"}}
{"id": "2510.16791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16791", "abs": "https://arxiv.org/abs/2510.16791", "authors": ["Chengxuan Zhu", "Shuchen Weng", "Jiacong Fang", "Peixuan Zhang", "Si Li", "Chao Xu", "Boxin Shi"], "title": "Personalized Image Filter: Mastering Your Photographic Style", "comment": null, "summary": "Photographic style, as a composition of certain photographic concepts, is the\ncharm behind renowned photographers. But learning and transferring photographic\nstyle need a profound understanding of how the photo is edited from the unknown\noriginal appearance. Previous works either fail to learn meaningful\nphotographic concepts from reference images, or cannot preserve the content of\nthe content image. To tackle these issues, we proposed a Personalized Image\nFilter (PIF). Based on a pretrained text-to-image diffusion model, the\ngenerative prior enables PIF to learn the average appearance of photographic\nconcepts, as well as how to adjust them according to text prompts. PIF then\nlearns the photographic style of reference images with the textual inversion\ntechnique, by optimizing the prompts for the photographic concepts. PIF shows\noutstanding performance in extracting and transferring various kinds of\nphotographic style. Project page: https://pif.pages.dev/", "AI": {"tldr": "本文提出个性化图像滤镜（PIF），利用预训练文本到图像扩散模型和文本反演技术，学习并迁移摄影风格，同时保留图像内容，解决了现有方法在学习摄影概念和内容保留方面的不足。", "motivation": "学习和迁移摄影风格需要深刻理解照片编辑过程。现有工作在从参考图像中学习有意义的摄影概念或在风格迁移时保留内容方面存在不足。", "method": "PIF基于预训练的文本到图像扩散模型，利用其生成先验学习摄影概念的平均外观及如何根据文本提示进行调整。随后，PIF通过文本反演技术，优化摄影概念的提示词，从而学习参考图像的摄影风格。", "result": "PIF在提取和迁移各种摄影风格方面表现出色。", "conclusion": "PIF成功解决了传统方法在学习摄影概念和内容保留方面的挑战，实现了卓越的摄影风格提取和迁移能力。"}}
{"id": "2510.16785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16785", "abs": "https://arxiv.org/abs/2510.16785", "authors": ["Jiazhen Liu", "Long Chen"], "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs", "comment": null, "summary": "Integrating diverse visual capabilities into a unified model is a significant\ntrend in Multimodal Large Language Models (MLLMs). Among these, the inclusion\nof segmentation poses a distinct set of challenges. To equip MLLMs with\npixel-level segmentation abilities, prevailing methods require finetuning the\nmodel to produce specific outputs compatible with a mask decoder. This process\ntypically alters the model's output space and compromises its intrinsic\ngeneralization, which undermines the goal of building a unified model. We\nintroduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel\nplug-and-play solution. LENS attaches a lightweight, trainable head to a\ncompletely frozen MLLM. By refining the spatial cues embedded in attention\nmaps, LENS extracts keypoints and describes them into point-wise features\ndirectly compatible with the mask decoder. Extensive experiments validate our\napproach: LENS achieves segmentation performance competitive with or superior\nto that of retraining-based methods. Crucially, it does so while fully\npreserving the MLLM's generalization capabilities, which are significantly\ndegraded by finetuning approaches. As such, the attachable design of LENS\nestablishes an efficient and powerful paradigm for extending MLLMs, paving the\nway for truly multi-talented, unified models.", "AI": {"tldr": "现有MLLM整合分割能力的方法需要微调，损害了模型的泛化性。LENS提出一种即插即用的方案，通过在冻结的MLLM上添加轻量级可训练头部，利用注意力图中的空间线索提取关键点进行分割，在保持MLLM泛化能力的同时，实现有竞争力的分割性能。", "motivation": "将多样化的视觉能力整合到统一的多模态大语言模型（MLLM）中是重要趋势，其中像素级分割能力的整合面临独特挑战。现有方法通常需要微调模型以生成与掩码解码器兼容的特定输出，但这会改变模型的输出空间并损害其内在泛化能力，从而违背构建统一模型的初衷。", "method": "LENS（Leveraging kEypoiNts for MLLMs' Segmentation）是一种新颖的即插即用解决方案。它将一个轻量级、可训练的头部连接到一个完全冻结的MLLM上。通过细化注意力图中嵌入的空间线索，LENS提取关键点并将其描述为与掩码解码器直接兼容的点式特征。", "result": "实验验证了LENS方法的有效性：LENS在分割性能上与基于重新训练的方法相比具有竞争力甚至更优。更重要的是，它在实现这一目标的同时，完全保留了MLLM的泛化能力，而微调方法会显著降低这种能力。", "conclusion": "LENS的可连接设计为扩展MLLM建立了一种高效且强大的范式，为实现真正多才多艺的统一模型铺平了道路。"}}
{"id": "2510.16790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16790", "abs": "https://arxiv.org/abs/2510.16790", "authors": ["Sara Hatami Rostami", "Behrooz Nasihatkon"], "title": "Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry", "comment": "7 pages, 3 figures", "summary": "This paper presents a fully unsupervised approach for binary road\nsegmentation (road vs. non-road), eliminating the reliance on costly manually\nlabeled datasets. The method leverages scene geometry and temporal cues to\ndistinguish road from non-road regions. Weak labels are first generated from\ngeometric priors, marking pixels above the horizon as non-road and a predefined\nquadrilateral in front of the vehicle as road. In a refinement stage, temporal\nconsistency is enforced by tracking local feature points across frames and\npenalizing inconsistent label assignments using mutual information\nmaximization. This enhances both precision and temporal stability. On the\nCityscapes dataset, the model achieves an Intersection-over-Union (IoU) of\n0.82, demonstrating high accuracy with a simple design. These findings\ndemonstrate the potential of combining geometric constraints and temporal\nconsistency for scalable unsupervised road segmentation in autonomous driving.", "AI": {"tldr": "本文提出了一种完全无监督的二元道路分割方法，通过结合场景几何先验和时间一致性来避免对昂贵手动标注数据的依赖，并在Cityscapes数据集上取得了0.82的IoU。", "motivation": "自动驾驶中道路分割依赖昂贵的手动标注数据集，因此研究旨在消除这种依赖，实现可扩展的无监督道路分割。", "method": "方法分为两阶段：1) 利用几何先验生成弱标签，将地平线以上标记为非道路，车辆前方预定义四边形标记为道路。2) 在精炼阶段，通过跨帧跟踪局部特征点并使用互信息最大化惩罚不一致的标签分配，强制执行时间一致性。", "result": "在Cityscapes数据集上，该模型实现了0.82的交并比（IoU），展示了其高精度和简单设计的优势。", "conclusion": "研究结果表明，结合几何约束和时间一致性在自动驾驶中实现可扩展的无监督道路分割具有巨大潜力。"}}
{"id": "2510.17405", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17405", "abs": "https://arxiv.org/abs/2510.17405", "authors": ["Mardiyyah Oduwole", "Prince Mireku", "Fatimo Adebanjo", "Oluwatosin Olajide", "Mahi Aminu Aliyu", "Jekaterina Novikova"], "title": "AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages", "comment": null, "summary": "Multimodal AI research has overwhelmingly focused on high-resource languages,\nhindering the democratization of advancements in the field. To address this, we\npresent AfriCaption, a comprehensive framework for multilingual image\ncaptioning in 20 African languages and our contributions are threefold: (i) a\ncurated dataset built on Flickr8k, featuring semantically aligned captions\ngenerated via a context-aware selection and translation process; (ii) a\ndynamic, context-preserving pipeline that ensures ongoing quality through model\nensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B\nparameter vision-to-text architecture that integrates SigLIP and NLLB200 for\ncaption generation across under-represented languages. This unified framework\nensures ongoing data quality and establishes the first scalable\nimage-captioning resource for under-represented African languages, laying the\ngroundwork for truly inclusive multimodal AI.", "AI": {"tldr": "该研究提出了AfriCaption，一个针对20种非洲语言的多语言图像字幕框架，包括一个精心策划的数据集、一个动态质量保证管道和一个0.5B参数的视觉到文本模型，旨在解决低资源语言在多模态AI领域的不足。", "motivation": "多模态AI研究主要集中在高资源语言，阻碍了该领域进步的普及。为了解决这一问题，研究旨在为代表性不足的非洲语言提供可扩展的图像字幕资源。", "method": "该研究采用了三重方法：(i) 构建了一个基于Flickr8k的精选数据集，通过上下文感知的选择和翻译过程生成语义对齐的字幕；(ii) 设计了一个动态、上下文保留的管道，通过模型集成和自适应替换确保持续的质量；(iii) 开发了AfriCaption模型，这是一个0.5B参数的视觉到文本架构，整合了SigLIP和NLLB200以生成低资源语言的字幕。", "result": "AfriCaption框架确保了持续的数据质量，并建立了第一个针对代表性不足的非洲语言的可扩展图像字幕资源。", "conclusion": "该统一框架为真正包容的多模态AI奠定了基础，解决了多模态AI研究中语言资源不平衡的问题。"}}
{"id": "2510.17402", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17402", "abs": "https://arxiv.org/abs/2510.17402", "authors": ["Jiacheng Xie", "Shuai Zeng", "Yang Yu", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.", "AI": {"tldr": "本研究引入了Ladder-base，首个采用群组相对策略优化（GRPO）强化学习方法训练的中医药大语言模型，它在推理和事实一致性方面表现优于现有通用和领域专用模型。", "motivation": "传统中医药（TCM）独特的知识体系对现有大语言模型（LLMs）的应用构成挑战。尽管之前的中医药LLMs通过监督微调取得进展，但在对齐、数据质量和评估一致性方面仍存在局限性。", "method": "本研究构建了Ladder-base模型，基于Qwen2.5-7B-Instruct基础模型，并首次采用群组相对策略优化（GRPO）这一强化学习方法进行训练。GRPO通过优化基于组内比较的响应选择来提高推理和事实一致性。模型仅使用TCM-Ladder基准测试的文本子集进行训练，其中80%用于训练，20%用于验证和测试。", "result": "Ladder-base在标准化评估中，与GPT-4、Gemini 2.5、Claude 3、Qwen3等最先进的通用LLMs以及BenTsao、HuatuoGPT2、Zhongjing等领域专用中医药模型相比，在多个推理指标上均表现出卓越的性能。", "conclusion": "研究结果表明，GRPO为LLMs在中医药领域实现专家级推理对齐提供了一种有效且高效的策略，并支持开发值得信赖且具有临床基础的中医药人工智能系统。"}}
{"id": "2510.17426", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17426", "abs": "https://arxiv.org/abs/2510.17426", "authors": ["Tiancheng Hu", "Benjamin Minixhofer", "Nigel Collier"], "title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging", "comment": null, "summary": "The \"alignment tax\" of post-training is typically framed as a drop in task\naccuracy. We show it also involves a severe loss of calibration, making models\noverconfident, less reliable, and model outputs less diverse. We show that this\ntrade-off can be navigated effectively via a simple post-hoc intervention:\ninterpolating between a model's weights before and after alignment. Crucially,\nthis is not a strict trade-off. We find that the process consistently reveals\nPareto-optimal interpolations - models that improve accuracy beyond both\nparents while substantially recovering the calibration lost during alignment.\nOur work demonstrates that simple model merging provides a computationally\nefficient method for mitigating the full scope of the alignment tax, yielding\nmodels that are more capable and more reliable.", "AI": {"tldr": "对齐（alignment）会导致模型准确性下降和校准（calibration）损失，使模型过度自信。通过对齐前后模型权重的简单插值，可以有效缓解这些问题，甚至在提高准确性的同时恢复校准。", "motivation": "后训练（post-training）中的“对齐税”（alignment tax）不仅表现为任务准确性的下降，还会导致严重的校准损失，使模型过度自信、可靠性降低且输出多样性减少。研究旨在找到一种方法来有效应对这种权衡。", "method": "采用一种简单的后处理干预措施：在模型对齐前后的权重之间进行插值（interpolation）。", "result": "该方法揭示了帕累托最优（Pareto-optimal）的插值模型，这些模型在准确性上超越了对齐前后两个原始模型，同时显著恢复了对齐过程中损失的校准。这表明并非严格的权衡，而是可以同时改进。", "conclusion": "简单的模型合并（通过权重插值）提供了一种计算高效的方法，可以全面减轻对齐税的影响，从而产生更强大、更可靠的模型。"}}
{"id": "2510.17415", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17415", "abs": "https://arxiv.org/abs/2510.17415", "authors": ["Jiacheng Xie", "Yang Yu", "Yibo Chen", "Hanyao Zhang", "Lening Zhao", "Jiaxuan He", "Lei Jiang", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine", "comment": null, "summary": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.", "AI": {"tldr": "BenCao是一个基于ChatGPT的多模态中医助手，通过自然语言指令微调和多模态集成开发，在诊断、草药识别和体质分类等任务中表现优异，并已成功部署。", "motivation": "传统中医依赖整体推理、隐式逻辑和多模态诊断线索，使得大语言模型（LLMs）难以有效应用。现有中医领域LLMs在文本理解方面有进展，但缺乏多模态集成、可解释性和临床适用性。", "method": "开发了BenCao，一个基于ChatGPT的多模态中医助手。它通过自然语言指令微调（而非参数重训练）进行训练，并整合了：1) 包含1000多部经典和现代文本的知识库；2) 情景式指令框架；3) 链式思考模拟机制以增强可解释性；4) 由执业中医医师参与的反馈精炼过程。BenCao还连接外部API以实现舌象分类和多模态数据库检索。", "result": "BenCao在单选题基准测试和多模态分类任务中（特别是诊断、草药识别和体质分类）取得了优于通用领域和现有中医领域模型的准确性。该模型已作为交互式应用部署在OpenAI GPTs Store，截至2025年10月，全球已有近1000名用户访问。", "conclusion": "本研究证明了通过自然语言指令微调和多模态集成开发中医领域LLM的可行性，为将生成式AI与传统医学推理对齐提供了一个实用框架，并为实际部署提供了可扩展的途径。"}}
{"id": "2510.16822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16822", "abs": "https://arxiv.org/abs/2510.16822", "authors": ["Yahia Battach", "Abdulwahab Felemban", "Faizan Farooq Khan", "Yousef A. Radwan", "Xiang Li", "Fabio Marchese", "Sara Beery", "Burton H. Jones", "Francesca Benzoni", "Mohamed Elhoseiny"], "title": "ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification", "comment": null, "summary": "Coral reefs are rapidly declining due to anthropogenic pressures such as\nclimate change, underscoring the urgent need for scalable, automated\nmonitoring. We introduce ReefNet, a large public coral reef image dataset with\npoint-label annotations mapped to the World Register of Marine Species (WoRMS).\nReefNet aggregates imagery from 76 curated CoralNet sources and an additional\nsite from Al Wajh in the Red Sea, totaling approximately 925000 genus-level\nhard coral annotations with expert-verified labels. Unlike prior datasets,\nwhich are often limited by size, geography, or coarse labels and are not\nML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global\nscale to WoRMS. We propose two evaluation settings: (i) a within-source\nbenchmark that partitions each source's images for localized evaluation, and\n(ii) a cross-source benchmark that withholds entire sources to test domain\ngeneralization. We analyze both supervised and zero-shot classification\nperformance on ReefNet and find that while supervised within-source performance\nis promising, supervised performance drops sharply across domains, and\nperformance is low across the board for zero-shot models, especially for rare\nand visually similar genera. This provides a challenging benchmark intended to\ncatalyze advances in domain generalization and fine-grained coral\nclassification. We will release our dataset, benchmarking code, and pretrained\nmodels to advance robust, domain-adaptive, global coral reef monitoring and\nconservation.", "AI": {"tldr": "本文介绍了ReefNet，一个大规模的公共珊瑚礁图像数据集，包含92.5万个属级硬珊瑚注释，用于推动可扩展的自动化珊瑚礁监测。研究发现，监督学习在跨域泛化方面表现不佳，零样本模型性能普遍较低，尤其对于稀有和视觉相似的属。", "motivation": "珊瑚礁因气候变化等人类压力而迅速衰退，急需可扩展、自动化的监测方法。现有数据集在规模、地理范围或标签粒度上存在局限，且不适合机器学习。", "method": "研究引入了ReefNet数据集，整合了来自76个CoralNet来源和一个红海站点的图像，共约92.5万个属级硬珊瑚注释，并与WoRMS分类系统映射。提出了两种评估设置：(i) 源内基准测试（局部评估），和(ii) 跨源基准测试（测试域泛化）。分析了监督学习和零样本分类性能。", "result": "监督学习在源内性能表现良好，但在跨域时性能急剧下降。零样本模型在所有情况下性能普遍较低，尤其对于稀有和视觉相似的属。ReefNet被证明是一个具有挑战性的基准，旨在推动域泛化和细粒度珊瑚分类的进展。", "conclusion": "ReefNet数据集及其基准测试旨在促进域泛化和细粒度珊瑚分类方面的进步，以实现稳健、领域自适应的全球珊瑚礁监测和保护。研究将发布数据集、基准代码和预训练模型。"}}
{"id": "2510.17437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17437", "abs": "https://arxiv.org/abs/2510.17437", "authors": ["Manuela Daniela Danu", "George Marica", "Constantin Suciu", "Lucian Mihai Itu", "Oladimeji Farri"], "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings", "comment": "11 pages, 5 figures, 1 table, published in Working Notes of the\n  Conference and Labs of the Evaluation Forum (CLEF 2024)", "summary": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.", "AI": {"tldr": "本研究开发并评估了基于BERT的深度上下文嵌入模型，用于提升心脏病学领域低资源语言（西班牙语、意大利语）和英语临床文本中的疾病和药物命名实体识别（NER），并在BioASQ MultiCardioNER任务中取得了优于平均水平的F1分数。", "motivation": "电子健康记录（EHR）数据量激增，迫切需要从非结构化临床文本中提取生物医学知识，以支持数据驱动的临床系统。尽管上下文语言模型在英语NER方面表现出色，但针对低资源语言临床文本的研究仍显不足。", "method": "研究开发了多种深度上下文嵌入模型，以增强心脏病学领域的临床NER。具体探索了不同单语和多语BERT模型（在通用领域文本上训练）在提取英语、西班牙语和意大利语临床病例报告中的疾病和药物提及方面的有效性。研究作为BioASQ MultiCardioNER共享任务的一部分进行。", "result": "研究在西班牙语疾病识别（SDR）上获得77.88%的F1分数，西班牙语药物识别（SMR）上获得92.09%，英语药物识别（EMR）上获得91.74%，意大利语药物识别（IMR）上获得88.9%。这些结果在所有子任务中均优于测试排行榜的平均和中位数F1分数。", "conclusion": "研究表明，基于BERT的深度上下文嵌入模型能有效提升低资源语言（如西班牙语、意大利语）和英语临床文本中的疾病和药物命名实体识别性能，并在BioASQ MultiCardioNER共享任务中取得了领先的结果。"}}
{"id": "2510.16832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16832", "abs": "https://arxiv.org/abs/2510.16832", "authors": ["Abdur Rahman", "Mohammad Marufuzzaman", "Jason Street", "Haifeng Wang", "Veera G. Gude", "Randy Buchanan"], "title": "Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction", "comment": null, "summary": "Accurate and quick prediction of wood chip moisture content is critical for\noptimizing biofuel production and ensuring energy efficiency. The current\nwidely used direct method (oven drying) is limited by its longer processing\ntime and sample destructiveness. On the other hand, existing indirect methods,\nincluding near-infrared spectroscopy-based, electrical capacitance-based, and\nimage-based approaches, are quick but not accurate when wood chips come from\nvarious sources. Variability in the source material can alter data\ndistributions, undermining the performance of data-driven models. Therefore,\nthere is a need for a robust approach that effectively mitigates the impact of\nsource variability. Previous studies show that manually extracted texture\nfeatures have the potential to predict wood chip moisture class. Building on\nthis, in this study, we conduct a comprehensive analysis of five distinct\ntexture feature types extracted from wood chip images to predict moisture\ncontent. Our findings reveal that a combined feature set incorporating all five\ntexture features achieves an accuracy of 95% and consistently outperforms\nindividual texture features in predicting moisture content. To ensure robust\nmoisture prediction, we propose a domain adaptation method named AdaptMoist\nthat utilizes the texture features to transfer knowledge from one source of\nwood chip data to another, addressing variability across different domains. We\nalso proposed a criterion for model saving based on adjusted mutual\ninformation. The AdaptMoist method improves prediction accuracy across domains\nby 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted\nmodels. These results highlight the effectiveness of AdaptMoist as a robust\nsolution for wood chip moisture content estimation across domains, making it a\npotential solution for wood chip-reliant industries.", "AI": {"tldr": "该研究提出了一种基于纹理特征和领域适应方法（AdaptMoist）的木屑水分含量预测方案，有效解决了来源多样性导致的预测不准确问题，提高了预测精度和鲁棒性。", "motivation": "木屑水分含量的准确快速预测对生物燃料生产和能源效率至关重要。现有直接方法耗时长且具破坏性，而现有间接方法在木屑来源多样时精度不足。因此，需要一种能够有效减轻来源变异性影响的鲁棒方法。", "method": "本研究对从木屑图像中提取的五种不同纹理特征类型进行了综合分析，以预测水分含量。在此基础上，提出了一种名为AdaptMoist的领域适应方法，利用纹理特征将知识从一个木屑数据源迁移到另一个，以解决不同领域间的变异性。此外，还提出了一种基于调整互信息（adjusted mutual information）的模型保存标准。", "result": "研究发现，结合所有五种纹理特征的特征集在预测水分含量时达到了95%的准确率，并始终优于单独的纹理特征。AdaptMoist方法将跨领域的预测准确率提高了23%，平均准确率达到80%，而非适应模型的准确率为57%。", "conclusion": "这些结果表明AdaptMoist是一种有效且鲁棒的跨领域木屑水分含量估算解决方案，有望成为木屑相关行业的潜在解决方案。"}}
{"id": "2510.17460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17460", "abs": "https://arxiv.org/abs/2510.17460", "authors": ["Muhammad Farmal Khan", "Mousumi Akter"], "title": "Evaluating Large Language Models on Urdu Idiom Translation", "comment": null, "summary": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.", "AI": {"tldr": "本文介绍了首个乌尔都语到英语的习语翻译评估数据集，涵盖原生乌尔都语和罗马乌尔都语，并评估了LLMs和NMT系统。研究发现提示工程能提升习语翻译质量，且原生乌尔都语输入比罗马乌尔都语产生更准确的翻译。", "motivation": "机器翻译中的习语翻译是一个重大挑战，特别是对于乌尔都语等低资源语言，且此前受到的关注有限。", "method": "研究构建了首个乌尔都语到英语的习语翻译评估数据集，包含原生乌尔都语和罗马乌尔都语脚本，并标注了金标准英语等效词。评估了多个开源大型语言模型（LLMs）和神经机器翻译（NMT）系统，使用BLEU、BERTScore、COMET和XCOMET等自动指标，并探讨了提示工程（prompt engineering）的效果。", "result": "研究发现，与直接翻译相比，提示工程能增强习语翻译的性能，尽管不同提示类型之间的性能差异较小。此外，跨脚本比较表明，文本表示形式显著影响翻译质量，原生乌尔都语输入比罗马乌尔都语产生更准确的习语翻译。", "conclusion": "提示工程对于提升习语翻译质量有积极作用，并且文本脚本类型（原生乌尔都语优于罗马乌尔都语）对翻译准确性有重要影响。"}}
{"id": "2510.17431", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17431", "abs": "https://arxiv.org/abs/2510.17431", "authors": ["Yushi Yang", "Shreyansh Padarha", "Andrew Lee", "Adam Mahdi"], "title": "Agentic Reinforcement Learning for Search is Unsafe", "comment": null, "summary": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.", "AI": {"tldr": "研究发现，经过强化学习（RL）训练的代理式大型语言模型在搜索应用中，其安全性（如拒绝有害请求）虽然源自指令微调，但却非常脆弱，两种简单攻击（强制搜索和重复搜索）能显著降低拒绝率和答案安全性，暴露出当前RL训练的缺陷。", "motivation": "代理式强化学习训练的大型语言模型在多步推理任务中表现出色，尤其是在工具调用（如搜索）方面。然而，这些模型的安全属性尚不明确，尤其是在处理有害请求时的行为。", "method": "研究通过两种简单攻击方式（“搜索攻击”和“多搜索攻击”）来评估RL训练的搜索模型的安全性。这些攻击分别强制模型以搜索开始响应或鼓励模型重复搜索。实验对象包括两个模型家族（Qwen、Llama），并结合本地和网络搜索，测量了拒绝率、答案安全性以及搜索查询的安全性。", "result": "结果显示，RL训练的搜索模型确实继承了指令微调的拒绝能力，并能将有害请求转化为安全查询。然而，这种安全性是脆弱的。两种攻击方式能将拒绝率降低高达60.0%，将答案安全性降低82.5%，并将搜索查询安全性降低82.4%。攻击成功的机制是，在模型生成继承的拒绝标记之前，触发模型生成有害的、反映请求的搜索查询。", "conclusion": "当前RL训练存在一个核心弱点：它奖励有效查询的持续生成，却未考虑其潜在危害。这导致RL搜索模型存在用户容易利用的漏洞。因此，迫切需要开发以安全搜索为目标的、具备安全意识的代理式RL训练流程。"}}
{"id": "2510.16837", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16837", "abs": "https://arxiv.org/abs/2510.16837", "authors": ["Haofan Ren", "Qingsong Yan", "Ming Lu", "Rongfeng Lu", "Zunjie Zhu"], "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced\nneural fields, as it enables high-fidelity rendering with impressive visual\nquality. However, 3DGS has difficulty accurately representing surfaces. In\ncontrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian\ndisks. Despite advancements in geometric fidelity, rendering quality remains\ncompromised, highlighting the challenge of achieving both high-quality\nrendering and precise geometric structures. This indicates that optimizing both\ngeometric and rendering quality in a single training stage is currently\nunfeasible. To overcome this limitation, we present 2DGS-R, a new method that\nuses a hierarchical training approach to improve rendering quality while\nmaintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians\nwith the normal consistency regularization. Then 2DGS-R selects the 2D\nGaussians with inadequate rendering quality and applies a novel in-place\ncloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R\nmodel with opacity frozen. Experimental results show that compared to the\noriginal 2DGS, our method requires only 1\\% more storage and minimal additional\ntraining time. Despite this negligible overhead, it achieves high-quality\nrendering results while preserving fine geometric structures. These findings\nindicate that our approach effectively balances efficiency with performance,\nleading to improvements in both visual fidelity and geometric reconstruction\naccuracy.", "AI": {"tldr": "2DGS-R提出一种分层训练方法，通过改进2D高斯点的渲染质量，同时保持几何精度，有效平衡效率与性能。", "motivation": "3DGS难以准确表示表面，而2DGS虽然改进了几何保真度但渲染质量受损。目前难以在单一训练阶段同时优化几何和渲染质量。", "method": "2DGS-R采用分层训练方法：首先，使用法线一致性正则化训练原始2D高斯点；然后，选择渲染质量不足的2D高斯点并应用新颖的原位克隆操作进行增强；最后，冻结不透明度微调2DGS-R模型。", "result": "与原始2DGS相比，2DGS-R仅需额外1%的存储空间和极少的训练时间，却能实现高质量渲染，同时保留精细的几何结构。", "conclusion": "2DGS-R方法有效平衡了效率与性能，显著提升了视觉保真度和几何重建精度。"}}
{"id": "2510.16833", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.16833", "abs": "https://arxiv.org/abs/2510.16833", "authors": ["Xiangyu Mu", "Dongliang Zhou", "Jie Hou", "Haijun Zhang", "Weili Guan"], "title": "From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display", "comment": null, "summary": "Mannequin-based clothing displays offer a cost-effective alternative to\nreal-model showcases for online fashion presentation, but lack realism and\nexpressive detail. To overcome this limitation, we introduce a new task called\nmannequin-to-human (M2H) video generation, which aims to synthesize\nidentity-controllable, photorealistic human videos from footage of mannequins.\nWe propose M2HVideo, a pose-aware and identity-preserving video generation\nframework that addresses two key challenges: the misalignment between head and\nbody motion, and identity drift caused by temporal modeling. In particular,\nM2HVideo incorporates a dynamic pose-aware head encoder that fuses facial\nsemantics with body pose to produce consistent identity embeddings across\nframes. To address the loss of fine facial details due to latent space\ncompression, we introduce a mirror loss applied in pixel space through a\ndenoising diffusion implicit model (DDIM)-based one-step denoising.\nAdditionally, we design a distribution-aware adapter that aligns statistical\ndistributions of identity and clothing features to enhance temporal coherence.\nExtensive experiments on the UBC fashion dataset, our self-constructed ASOS\ndataset, and the newly collected MannequinVideos dataset captured on-site\ndemonstrate that M2HVideo achieves superior performance in terms of clothing\nconsistency, identity preservation, and video fidelity in comparison to\nstate-of-the-art methods.", "AI": {"tldr": "M2HVideo是一个姿态感知和身份保持的视频生成框架，能将模特视频转化为身份可控、逼真的人体视频，解决了头部与身体运动不一致和身份漂移问题。", "motivation": "模特展示服装比真人模特更具成本效益，但在在线时尚展示中缺乏真实感和表现力。研究旨在克服这一限制，通过技术手段提升模特展示的真实性。", "method": "提出了一个名为M2HVideo的姿态感知和身份保持视频生成框架，用于完成“模特到真人”(M2H)视频生成任务。具体方法包括：1) 引入动态姿态感知头部编码器，融合面部语义和身体姿态，生成跨帧一致的身份嵌入。2) 采用基于DDIM的单步去噪，在像素空间应用镜像损失，以解决潜在空间压缩导致的面部细节丢失。3) 设计分布感知适配器，对齐身份和服装特征的统计分布，以增强时间连贯性。", "result": "在UBC时尚数据集、自建ASOS数据集和新收集的MannequinVideos数据集上的大量实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面均优于现有最先进的方法。", "conclusion": "M2HVideo成功地从模特视频中合成了身份可控、逼真的人体视频，为在线时尚展示提供了一种有效且高质量的解决方案，显著提升了模特展示的真实感和表现力。"}}
{"id": "2510.16854", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10; I.5.4; I.4.6"], "pdf": "https://arxiv.org/pdf/2510.16854", "abs": "https://arxiv.org/abs/2510.16854", "authors": ["Akhila Kambhatla", "Taminul Islam", "Khaled R Ahmed"], "title": "ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification", "comment": "9 pages with 4 figures and 5 tables. This is a preprint submitted to\n  arXiv", "summary": "The escalating threat of weapon-related violence necessitates automated\ndetection systems capable of pixel-level precision for accurate threat\nassessment in real-time security applications. Traditional weapon detection\napproaches rely on object detection frameworks that provide only coarse\nbounding box localizations, lacking the fine-grained segmentation required for\ncomprehensive threat analysis. Furthermore, existing semantic segmentation\nmodels either sacrifice accuracy for computational efficiency or require\nexcessive computational resources incompatible with edge deployment scenarios.\nThis paper presents ArmFormer, a lightweight transformer-based semantic\nsegmentation framework that strategically integrates Convolutional Block\nAttention Module (CBAM) with MixVisionTransformer architecture to achieve\nsuperior accuracy while maintaining computational efficiency suitable for\nresource-constrained edge devices. Our approach combines CBAM-enhanced encoder\nbackbone with attention-integrated hamburger decoder to enable multi-class\nweapon segmentation across five categories: handgun, rifle, knife, revolver,\nand human. Comprehensive experiments demonstrate that ArmFormer achieves\nstate-of-the-art performance with 80.64% mIoU and 89.13% mFscore while\nmaintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M\nparameters, ArmFormer outperforms heavyweight models requiring up to 48x more\ncomputation, establishing it as the optimal solution for deployment on portable\nsecurity cameras, surveillance drones, and embedded AI accelerators in\ndistributed security infrastructure.", "AI": {"tldr": "ArmFormer是一种轻量级、基于Transformer的语义分割框架，专为边缘设备上的实时武器检测设计，通过集成CBAM和MixVisionTransformer，在保持高计算效率的同时实现了像素级精度。", "motivation": "日益增长的武器相关暴力威胁需要能够进行像素级精确检测的自动化系统，以实现实时安全应用中的准确威胁评估。传统的物体检测方法仅提供粗略的边界框定位，缺乏细粒度分割，而现有语义分割模型要么牺牲精度以提高效率，要么计算资源需求过高，不适用于边缘部署场景。", "method": "本文提出了ArmFormer，一个轻量级基于Transformer的语义分割框架。它策略性地将卷积块注意力模块（CBAM）与MixVisionTransformer架构相结合，以在资源受限的边缘设备上实现卓越的精度和计算效率。该方法结合了CBAM增强的编码器骨干和注意力集成的hamburger解码器，以实现手枪、步枪、刀具、左轮手枪和人类五类多类别武器分割。", "result": "ArmFormer实现了80.64%的mIoU和89.13%的mFscore，同时保持82.26 FPS的实时推理速度。它仅需4.886G FLOPs和3.66M参数，性能优于计算量高出48倍的重量级模型。", "conclusion": "ArmFormer是部署在便携式安防摄像头、监控无人机和分布式安全基础设施中的嵌入式AI加速器上的最佳解决方案，因为它在保持计算效率的同时实现了最先进的性能和实时推理能力。"}}
{"id": "2510.16863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16863", "abs": "https://arxiv.org/abs/2510.16863", "authors": ["Shujian Gao", "Yuan Wang", "Zekuan Yu"], "title": "BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation", "comment": "14 pages, 5 figures", "summary": "Semi-supervised medical image segmentation (SSMIS) seeks to match fully\nsupervised performance while sharply reducing annotation cost. Mainstream SSMIS\nmethods rely on \\emph{label-space consistency}, yet they overlook the equally\ncritical \\emph{representation-space alignment}. Without harmonizing latent\nfeatures, models struggle to learn representations that are both discriminative\nand spatially coherent. To this end, we introduce \\textbf{Bilateral Alignment\nin Representation and Label spaces (BARL)}, a unified framework that couples\ntwo collaborative branches and enforces alignment in both spaces. For\nlabel-space alignment, inspired by co-training and multi-scale decoding, we\ndevise \\textbf{Dual-Path Regularization (DPR)} and \\textbf{Progressively\nCognitive Bias Correction (PCBC)} to impose fine-grained cross-branch\nconsistency while mitigating error accumulation from coarse to fine scales. For\nrepresentation-space alignment, we conduct region-level and lesion-instance\nmatching between branches, explicitly capturing the fragmented, complex\npathological patterns common in medical imagery. Extensive experiments on four\npublic benchmarks and a proprietary CBCT dataset demonstrate that BARL\nconsistently surpasses state-of-the-art SSMIS methods. Ablative studies further\nvalidate the contribution of each component. Code will be released soon.", "AI": {"tldr": "本文提出BARL框架，通过在表示空间和标签空间进行双边对齐，解决了半监督医学图像分割中主流方法忽略表示空间对齐的问题，显著提升了分割性能。", "motivation": "主流的半监督医学图像分割(SSMIS)方法主要依赖于标签空间一致性，但忽视了同样关键的表示空间对齐。这导致模型难以学习到既具有判别性又具有空间连贯性的潜在特征。", "method": "本文引入了BARL（Representation and Label spaces中的双边对齐）框架，该框架结合了两个协作分支，并在表示空间和标签空间强制执行对齐。在标签空间对齐方面，受协同训练和多尺度解码启发，设计了双路径正则化（DPR）和渐进式认知偏差校正（PCBC），以实现细粒度的跨分支一致性并减轻从粗到细尺度的误差累积。在表示空间对齐方面，在分支之间进行区域级和病灶实例匹配，以明确捕捉医学图像中常见的碎片化、复杂的病理模式。", "result": "在四个公共基准和一个专有CBCT数据集上的大量实验表明，BARL持续超越了最先进的SSMIS方法。消融研究进一步验证了每个组件的贡献。", "conclusion": "BARL通过在表示空间和标签空间进行协同对齐，有效地解决了半监督医学图像分割中的关键挑战，实现了卓越的性能，并为未来研究提供了新的方向。"}}
{"id": "2510.17476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17476", "abs": "https://arxiv.org/abs/2510.17476", "authors": ["Ipek Baris Schlicht", "Burcu Sayin", "Zhixue Zhao", "Frederik M. Labonté", "Cesare Barbera", "Marco Viviani", "Paolo Rosso", "Lucie Flek"], "title": "Disparities in Multilingual LLM-Based Healthcare Q&A", "comment": "Under review", "summary": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.", "AI": {"tldr": "研究发现，多语言大型语言模型（LLMs）在医疗健康信息方面存在跨语言差异，且偏向英语维基百科。通过检索增强生成（RAG）提供非英语上下文，可有效提升事实一致性。", "motivation": "在医疗领域整合人工智能时，确保可靠健康信息的可公平获取至关重要。然而，信息质量因语言而异，引发了对多语言LLMs可靠性和一致性的担忧。", "method": "研究方法包括：(i) 构建多语言维基医疗数据集（MultiWikiHealthCare）；(ii) 分析跨语言医疗覆盖范围；(iii) 评估LLM回复与参考资料的一致性；(iv) 通过使用上下文信息和检索增强生成（RAG）进行事实一致性案例研究。涉及的语言有英语、德语、土耳其语、中文和意大利语。", "result": "研究发现维基百科覆盖范围和LLM事实一致性均存在显著的跨语言差异。LLM的回复更倾向于与英语维基百科对齐，即使提示语是非英语的。在推理时提供非英语维基百科的上下文片段，能有效将事实一致性转向文化相关知识。", "conclusion": "这些结果为构建更公平、多语言的医疗AI系统提供了实用的途径。"}}
{"id": "2510.17483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17483", "abs": "https://arxiv.org/abs/2510.17483", "authors": ["Zheyue Tan", "Zhiyuan Li", "Tao Yuan", "Dong Zhou", "Weilin Liu", "Yueqing Zhuang", "Yadong Li", "Guowei Niu", "Cheng Qin", "Zhuyu Yao", "Congyi Liu", "Haiyang Xu", "Boxun Li", "Guohao Dai", "Bo Zhao", "Yu Wang"], "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.", "AI": {"tldr": "ReXMoE是一种新型MoE架构，通过允许路由器跨相邻层重用专家来改进路由机制，从而解耦专家维度与每层预算，并通过渐进式扩展路由策略提升了语言模型和下游任务的性能。", "motivation": "现有MoE架构的层局部路由机制限制了专家组合的灵活性和模型表达能力，需要在专家维度和路由多样性之间进行权衡。", "method": "ReXMoE通过允许路由器重用相邻层的专家来改进路由，从而将专家维度与每层预算解耦。它还提出了一种新的渐进式扩展路由（PSR）策略，在训练期间逐步增加候选专家池。", "result": "ReXMoE显著提升了语言建模和下游任务的性能。在0.5B到7B参数范围内的模型上进行的广泛实验表明，在固定架构维度下，ReXMoE始终能提高性能。", "conclusion": "ReXMoE被确认为一种新的设计范式，用于参数高效且可扩展的基于MoE的大型语言模型。"}}
{"id": "2510.16865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16865", "abs": "https://arxiv.org/abs/2510.16865", "authors": ["Yuyang Yu", "Zhengwei Chen", "Xuemiao Xu", "Lei Zhang", "Haoxin Yang", "Yongwei Nie", "Shengfeng He"], "title": "Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection", "comment": null, "summary": "3D anomaly detection in point-cloud data is critical for industrial quality\ncontrol, aiming to identify structural defects with high reliability. However,\ncurrent memory bank-based methods often suffer from inconsistent feature\ntransformations and limited discriminative capacity, particularly in capturing\nlocal geometric details and achieving rotation invariance. These limitations\nbecome more pronounced when registration fails, leading to unreliable detection\nresults. We argue that point-cloud registration plays an essential role not\nonly in aligning geometric structures but also in guiding feature extraction\ntoward rotation-invariant and locally discriminative representations. To this\nend, we propose a registration-induced, rotation-invariant feature extraction\nframework that integrates the objectives of point-cloud registration and\nmemory-based anomaly detection. Our key insight is that both tasks rely on\nmodeling local geometric structures and leveraging feature similarity across\nsamples. By embedding feature extraction into the registration learning\nprocess, our framework jointly optimizes alignment and representation learning.\nThis integration enables the network to acquire features that are both robust\nto rotations and highly effective for anomaly detection. Extensive experiments\non the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method\nconsistently outperforms existing approaches in effectiveness and\ngeneralizability.", "AI": {"tldr": "本文提出一种配准诱导的旋转不变特征提取框架，将点云配准与基于记忆库的异常检测相结合，以解决现有3D点云异常检测方法在旋转不变性和局部几何细节捕获上的不足，显著提升了检测性能和泛化能力。", "motivation": "当前基于记忆库的3D点云异常检测方法存在特征变换不一致、判别能力有限（尤其在捕获局部几何细节和实现旋转不变性方面），且在配准失败时检测结果不可靠的问题。", "method": "提出一个配准诱导的旋转不变特征提取框架，该框架将点云配准和基于记忆库的异常检测的目标整合在一起。通过将特征提取嵌入到配准学习过程中，联合优化对齐和表示学习，从而使网络获得对旋转鲁棒且对异常检测高效的特征。", "result": "在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，所提出的方法在有效性和泛化性方面持续优于现有方法。", "conclusion": "通过整合点云配准和记忆库异常检测的目标，并引导特征提取实现旋转不变性和局部判别能力，本方法成功克服了现有技术的局限性，为3D点云异常检测提供了一个更可靠和高效的解决方案。"}}
{"id": "2510.17491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17491", "abs": "https://arxiv.org/abs/2510.17491", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents", "comment": null, "summary": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.", "AI": {"tldr": "本文系统回顾了基于大语言模型的工业智能体（LLM agents）的技术、应用和评估方法，并提出了一个能力成熟度框架，旨在为理解和构建下一代工业智能体提供路线图和理论基础。", "motivation": "尽管大语言模型智能体在自主推理、规划和执行复杂任务方面取得了进展，但如何将其研究成果转化为推动产业转型的生产力仍是一个重大挑战。", "method": "本文采用系统性综述方法，基于一个工业智能体能力成熟度框架，回顾了工业智能体在行业应用中的演变。具体分析了记忆、规划和工具使用这三大关键技术支柱，概述了其在数字工程、科学发现、具身智能、协作业务执行和复杂系统仿真等领域的应用，并审视了基础和专业能力的评估基准与方法，最后探讨了工业智能体面临的实际挑战、能力边界、发展潜力及治理问题。", "result": "研究结果表明，工业智能体正从“流程执行系统”演变为“自适应社会系统”。三大技术支柱（记忆、规划、工具使用）从支持简单任务发展到实现复杂自主系统和集体智能。智能体已应用于多个实际领域。当前的评估系统在真实性、安全性和行业特异性方面面临挑战。同时，论文也指出了工业智能体在实际应用中的能力边界、发展潜力和治理问题。", "conclusion": "通过结合技术演进与行业实践，本综述旨在阐明工业智能体的当前状态，并为理解和构建下一代工业智能体提供清晰的路线图和理论基础。"}}
{"id": "2510.16887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16887", "abs": "https://arxiv.org/abs/2510.16887", "authors": ["Nusrat Munia", "Abdullah Imran"], "title": "Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis", "comment": "EMBC 2025", "summary": "Generative models, especially Diffusion Models, have demonstrated remarkable\ncapability in generating high-quality synthetic data, including medical images.\nHowever, traditional class-conditioned generative models often struggle to\ngenerate images that accurately represent specific medical categories, limiting\ntheir usefulness for applications such as skin cancer diagnosis. To address\nthis problem, we propose a classification-induced diffusion model, namely,\nClass-N-Diff, to simultaneously generate and classify dermoscopic images. Our\nClass-N-Diff model integrates a classifier within a diffusion model to guide\nimage generation based on its class conditions. Thus, the model has better\ncontrol over class-conditioned image synthesis, resulting in more realistic and\ndiverse images. Additionally, the classifier demonstrates improved performance,\nhighlighting its effectiveness for downstream diagnostic tasks. This unique\nintegration in our Class-N-Diff makes it a robust tool for enhancing the\nquality and utility of diffusion model-based synthetic dermoscopic image\ngeneration. Our code is available at https://github.com/Munia03/Class-N-Diff.", "AI": {"tldr": "该论文提出了一种名为Class-N-Diff的分类引导扩散模型，用于同时生成和分类皮肤镜图像，解决了传统模型在特定医学类别生成上的局限性，并提高了生成图像的真实性和分类器性能。", "motivation": "传统的类别条件生成模型（尤其是扩散模型）在生成准确代表特定医学类别的图像时表现不佳，例如皮肤癌诊断，这限制了它们在相关应用中的实用性。", "method": "本文提出Class-N-Diff模型，通过在扩散模型中集成一个分类器，以类别条件引导图像生成。这种独特的集成使得模型能更好地控制类别条件下的图像合成。", "result": "Class-N-Diff模型能够更好地控制类别条件下的图像合成，从而生成更真实、更多样化的图像。此外，集成的分类器在下游诊断任务中也表现出更高的性能。", "conclusion": "Class-N-Diff模型通过其独特的集成方式，成为一个强大的工具，能够显著提高基于扩散模型的合成皮肤镜图像的质量和实用性。"}}
{"id": "2510.16870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16870", "abs": "https://arxiv.org/abs/2510.16870", "authors": ["Yudan Ren", "Xinlong Wang", "Kexin Wang", "Tian Xia", "Zihan Ma", "Zhaowei Li", "Xiangrong Bi", "Xiao Li", "Xiaowei He"], "title": "Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding", "comment": "14 pages, 7 figures", "summary": "While brain-inspired artificial intelligence(AI) has demonstrated promising\nresults, current understanding of the parallels between artificial neural\nnetworks (ANNs) and human brain processing remains limited: (1) unimodal ANN\nstudies fail to capture the brain's inherent multimodal processing\ncapabilities, and (2) multimodal ANN research primarily focuses on high-level\nmodel outputs, neglecting the crucial role of individual neurons. To address\nthese limitations, we propose a novel neuron-level analysis framework that\ninvestigates the multimodal information processing mechanisms in\nvision-language models (VLMs) through the lens of human brain activity. Our\napproach uniquely combines fine-grained artificial neuron (AN) analysis with\nfMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP\nand METER. Our analysis reveals four key findings: (1) ANs successfully predict\nbiological neurons (BNs) activities across multiple functional networks\n(including language, vision, attention, and default mode), demonstrating shared\nrepresentational mechanisms; (2) Both ANs and BNs demonstrate functional\nredundancy through overlapping neural representations, mirroring the brain's\nfault-tolerant and collaborative information processing mechanisms; (3) ANs\nexhibit polarity patterns that parallel the BNs, with oppositely activated BNs\nshowing mirrored activation trends across VLM layers, reflecting the complexity\nand bidirectional nature of neural information processing; (4) The\narchitectures of CLIP and METER drive distinct BNs: CLIP's independent branches\nshow modality-specific specialization, whereas METER's cross-modal design\nyields unified cross-modal activation, highlighting the architecture's\ninfluence on ANN brain-like properties. These results provide compelling\nevidence for brain-like hierarchical processing in VLMs at the neuronal level.", "AI": {"tldr": "本研究通过神经元层面的分析框架，结合fMRI数据，深入探讨了视觉-语言模型（VLMs）中的多模态信息处理机制与人脑活动的相似性，揭示了人工神经网络（ANNs）和生物神经元（BNs）在表征、冗余和极性模式上的共同特征，并强调了模型架构对这些特性的影响。", "motivation": "现有研究在人工神经网络（ANNs）与人脑处理的并行性理解上存在局限：1）单模态ANN研究未能捕捉大脑固有的多模态处理能力；2）多模态ANN研究主要关注高层模型输出，忽视了单个神经元的关键作用。", "method": "提出了一种新颖的神经元层面分析框架，通过人脑活动视角研究视觉-语言模型（VLMs）中的多模态信息处理机制。该方法独特地结合了细粒度的人工神经元（AN）分析与基于fMRI的体素编码，用于检查两种架构不同的VLM：CLIP和METER。", "result": "1) 人工神经元（ANs）成功预测了跨多个功能网络（包括语言、视觉、注意力和默认模式网络）的生物神经元（BNs）活动，表明存在共享的表征机制；2) ANs和BNs都通过重叠的神经表征表现出功能冗余，反映了大脑的容错和协作信息处理机制；3) ANs展现出与BNs平行的极性模式，相反激活的BNs在VLM层中显示出镜像的激活趋势，反映了神经信息处理的复杂性和双向性；4) CLIP和METER的架构驱动了不同的BNs表现：CLIP的独立分支显示出模态特异性专业化，而METER的跨模态设计产生了统一的跨模态激活，突出了架构对ANN类脑特性的影响。", "conclusion": "这些结果为视觉-语言模型（VLMs）在神经元层面的类脑分层处理提供了令人信服的证据。"}}
{"id": "2510.17489", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17489", "abs": "https://arxiv.org/abs/2510.17489", "authors": ["Yongxin He", "Shan Zhang", "Yixuan Cao", "Lei Ma", "Ping Luo"], "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning", "comment": "To appear in NeurIPS 2025", "summary": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.", "AI": {"tldr": "该研究提出了DETree，一种新颖的方法，通过将不同文本生成过程之间的关系建模为分层亲和树结构，并引入专门的损失函数，来检测各种AI参与的混合文本。同时，还开发了RealBench数据集，用于训练和评估该方法，显著提高了混合文本检测的性能、鲁棒性和泛化能力，尤其是在OOD和少样本学习场景下。", "motivation": "检测AI生成文本对于打击虚假信息、抄袭和学术不端行为至关重要。然而，AI文本生成涉及多种复杂的协作过程（如AI生成后人工编辑、人工撰写后AI编辑、AI生成后其他AI精炼），这些过程产生的文本特征复杂，给检测带来了巨大挑战。现有方法（如二元或多元分类）对这些过程的建模过于粗糙。", "method": "研究观察到不同生成过程产生的文本表示具有固有的聚类关系。基于此，提出了DETree方法：1) 将不同生成过程之间的关系建模为分层亲和树结构。2) 引入了一个专门的损失函数，使文本表示与该树结构对齐。3) 开发了RealBench，一个综合性的基准数据集，能自动整合通过各种人机协作过程产生的混合文本，以促进模型学习。", "result": "DETree方法在混合文本检测任务中表现出显著的性能提升。它显著增强了在分布外（OOD）场景下的鲁棒性和泛化能力，特别是在少样本学习条件下。这进一步证明了基于训练的方法在OOD设置中的潜力。", "conclusion": "通过将复杂的AI参与文本生成过程建模为分层亲和树结构，并辅以专门的损失函数和综合数据集，DETree方法能够更有效地检测各种AI参与的混合文本，并在挑战性的OOD和少样本学习场景中展现出优越的性能、鲁棒性和泛化能力。"}}
{"id": "2510.16891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16891", "abs": "https://arxiv.org/abs/2510.16891", "authors": ["Ramon Dalmau", "Gabriel Jarry", "Philippe Very"], "title": "Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data", "comment": null, "summary": "Aviation's non-CO2 effects, particularly contrails, are a significant\ncontributor to its climate impact. Persistent contrails can evolve into\ncirrus-like clouds that trap outgoing infrared radiation, with radiative\nforcing potentially comparable to or exceeding that of aviation's CO2\nemissions. While physical models simulate contrail formation, evolution and\ndissipation, validating and calibrating these models requires linking observed\ncontrails to the flights that generated them, a process known as\ncontrail-to-flight attribution. Satellite-based attribution is challenging due\nto limited spatial and temporal resolution, as contrails often drift and deform\nbefore detection. In this paper, we evaluate an alternative approach using\nground-based cameras, which capture contrails shortly after formation at high\nspatial and temporal resolution, when they remain thin, linear, and visually\ndistinct. Leveraging the ground visible camera contrail sequences (GVCCS)\ndataset, we introduce a modular framework for attributing contrails observed\nusing ground-based cameras to theoretical contrails derived from aircraft\nsurveillance and meteorological data. The framework accommodates multiple\ngeometric representations and distance metrics, incorporates temporal\nsmoothing, and enables flexible probability-based assignment strategies. This\nwork establishes a strong baseline and provides a modular framework for future\nresearch in linking contrails to their source flight.", "AI": {"tldr": "本文提出了一种利用地面摄像头数据将飞机凝结尾迹归因于特定航班的模块化框架，以解决卫星归因的挑战，并为凝结尾迹气候影响模型提供验证数据。", "motivation": "航空业的非二氧化碳效应（特别是凝结尾迹）对气候影响显著，其辐射强迫可能与二氧化碳排放相当或更高。验证和校准凝结尾迹物理模型需要将观测到的凝结尾迹与其产生的航班联系起来，但卫星归因由于时空分辨率限制而具有挑战性。地面摄像头可以在凝结尾迹形成后不久以高分辨率捕获其清晰图像，为归因提供了替代方法。", "method": "本文利用地面可见光摄像头凝结尾迹序列（GVCCS）数据集，引入了一个模块化框架。该框架将地面摄像头观测到的凝结尾迹归因于从飞机监控和气象数据推导出的理论凝结尾迹。它支持多种几何表示和距离度量，整合了时间平滑技术，并允许灵活的基于概率的分配策略。", "result": "这项工作建立了一个强大的基线，并提供了一个模块化框架，用于未来研究中将凝结尾迹与其源航班关联起来。这有助于解决凝结尾迹归因的挑战，并支持凝结尾迹气候影响模型的验证和校准。", "conclusion": "该研究成功建立了一个将地面观测到的凝结尾迹与其源航班关联的模块化框架，为未来凝结尾迹气候影响研究提供了重要的工具和方法学基础，有助于更准确地评估航空业的非二氧化碳气候影响。"}}
{"id": "2510.17509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17509", "abs": "https://arxiv.org/abs/2510.17509", "authors": ["Shiyu Ni", "Keping Bi", "Jiafeng Guo", "Minghao Tang", "Jingtong Wu", "Zengxin Han", "Xueqi Cheng"], "title": "Annotation-Efficient Universal Honesty Alignment", "comment": null, "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.", "AI": {"tldr": "本文提出EliCal框架，通过两阶段方法（先利用自洽性监督激发内部置信度，再用少量正确性标注进行校准）实现大型语言模型（LLMs）的诚实对齐，显著减少了对昂贵标注数据的需求。", "motivation": "LLMs的诚实对齐（识别知识边界并表达校准的置信度）对可信部署至关重要。现有方法要么依赖无训练的置信度估计（效果有限），要么依赖需要大量昂贵正确性标注的训练校准，难以实现普遍诚实对齐。", "method": "引入EliCal框架，分为两阶段：1. 激发阶段：使用廉价的自洽性监督来激发模型内部置信度。2. 校准阶段：使用少量正确性标注来校准这种置信度。为支持大规模研究，作者还发布了HonestyBench基准，包含十个自由形式问答数据集，带有正确性和自洽性信号标注。", "result": "实验表明，EliCal仅使用1k个正确性标注（仅占完全监督的0.18%）即可实现接近最优的对齐效果，并且在未见过的MMLU任务上表现优于仅校准的基线方法。", "conclusion": "EliCal提供了一种可扩展的解决方案，能够以极低的标注成本实现LLMs的普遍诚实对齐，显著提升了模型的可信赖性。"}}
{"id": "2510.17498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17498", "abs": "https://arxiv.org/abs/2510.17498", "authors": ["Zihan Liu", "Shun Zheng", "Xumeng Wen", "Yang Wang", "Jiang Bian", "Mao Yang"], "title": "Deep Self-Evolving Reasoning", "comment": null, "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.", "AI": {"tldr": "本文提出深度自演化推理 (DSER) 框架，通过将迭代推理建模为马尔可夫链并并行运行多个自演化过程，即使在验证和修正能力较弱的情况下，也能显著扩展小型开源大语言模型的推理极限，使其在复杂数学问题上超越大型教师模型。", "motivation": "先进的链式思考推理已成为大型语言模型高级推理的基石。然而，尽管最新的验证-修正框架使专有模型能够解决奥林匹克级别的问题，但其有效性依赖于强大可靠的验证和修正能力，而这些能力在开源、小规模模型中仍然脆弱。因此，研究的动机是在这些模型验证和修正能力较弱的情况下，如何显著扩展其推理极限。", "method": "本文提出了一种名为深度自演化推理 (DSER) 的概率范式。该方法将迭代推理概念化为马尔可夫链，其中每一步都代表解决方案空间中的随机转换。其核心思想是，只要改进的概率略微超过退化的概率，就能保证收敛到正确的解决方案。通过并行运行多个长周期、自演化的过程，DSER 能够放大这些微小的积极趋势，使模型能够渐近地接近正确答案。", "result": "DSER 被应用于 DeepSeek-R1-0528-Qwen3-8B 模型。在具有挑战性的 AIME 2024-2025 基准测试中，DSER 解决了 9 个以前无法解决的问题中的 5 个，并提升了整体性能。通过多数投票，这个紧凑的模型甚至超越了其 600B 参数教师模型的单次推理准确率。", "conclusion": "DSER 框架不仅在测试时扩展了小型模型的推理能力，还诊断了当前开源推理器在自我验证、修正和稳定性方面的根本局限性。这些发现为开发具有强大内在自演化能力的下一代模型奠定了明确的研究议程。"}}
{"id": "2510.17504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17504", "abs": "https://arxiv.org/abs/2510.17504", "authors": ["Jingshu Liu", "Raheel Qader", "Gaëtan Caillaut", "Mariam Nakhlé"], "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task", "comment": null, "summary": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning BERT\nbased cross-lingual sentence embeddings have yet to be explored. We\nsystematically investigate methods for learning multilingual sentence\nembeddings by combining the best methods for learning monolingual and\ncross-lingual representations including: masked language modeling (MLM),\ntranslation language modeling (TLM), dual encoder translation ranking, and\nadditive margin softmax. We show that introducing a pre-trained multilingual\nlanguage model dramatically reduces the amount of parallel training data\nrequired to achieve good performance by 80%. Composing the best of these\nmethods produces a model that achieves 83.7% bi-text retrieval accuracy over\n112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still\nperforming competitively on monolingual transfer learning benchmarks. Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.", "AI": {"tldr": "本文系统研究了基于BERT的多语言句子嵌入方法，通过结合多种技术显著减少了所需并行数据量，并在跨语言检索任务上超越了现有最佳模型，同时保持了单语言任务的竞争力，并发布了模型。", "motivation": "BERT在单语言句子嵌入方面表现出色，但其在跨语言句子嵌入方面的应用尚未得到充分探索。研究旨在开发有效的BERT基跨语言句子嵌入方法。", "method": "研究结合了单语言和跨语言表示学习的最佳方法，包括：掩码语言建模（MLM）、翻译语言建模（TLM）、双编码器翻译排序和加性边界softmax。通过引入预训练的多语言语言模型来减少对并行数据的需求。", "result": "引入预训练多语言语言模型将所需并行训练数据量减少了80%。该模型在Tatoeba数据集上的112种语言双文本检索准确率达到83.7%，远高于LASER的65.5%。同时，在单语言迁移学习基准测试中表现出竞争力。使用该模型从CommonCrawl挖掘的并行数据能够训练出具有竞争力的英-中和英-德神经机器翻译模型。研究发布了针对109+语言的最佳多语言句子嵌入模型。", "conclusion": "通过结合多种先进技术并利用预训练的多语言语言模型，可以显著提高BERT基跨语言句子嵌入的性能，大幅减少对并行数据的依赖，并在跨语言检索任务上达到最先进水平，同时保持单语言任务的竞争力，并能有效支持机器翻译等应用。"}}
{"id": "2510.16888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16888", "abs": "https://arxiv.org/abs/2510.16888", "authors": ["Zongjian Li", "Zheyuan Liu", "Qihui Zhang", "Bin Lin", "Shenghai Yuan", "Zhiyuan Yan", "Yang Ye", "Wangbo Yu", "Yuwei Niu", "Li Yuan"], "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback", "comment": null, "summary": "Instruction-based image editing has achieved remarkable progress; however,\nmodels solely trained via supervised fine-tuning often overfit to annotated\npatterns, hindering their ability to explore and generalize beyond training\ndistributions. To this end, we introduce Edit-R1, a novel post-training\nframework for instruction-based image editing based on policy optimization.\nSpecifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a\nlikelihood-free policy optimization method consistent with the flow matching\nforward process, thereby enabling the use of higher-order samplers and more\nefficient training. Another key challenge here is the absence of a universal\nreward model, resulting from the diverse nature of editing instructions and\ntasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)\nas a unified, training-free reward model, leveraging its output logits to\nprovide fine-grained feedback. Furthermore, we carefully design a low-variance\ngroup filtering mechanism to reduce MLLM scoring noise and stabilize\noptimization. UniWorld-V2, trained with this framework, achieves\n\\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,\nscoring 4.49 and 7.83, respectively. Crucially, our framework is\nmodel-agnostic, delivering substantial performance gains when applied to\ndiverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its\nwide applicability. Code and models are publicly available at\nhttps://github.com/PKU-YuanGroup/UniWorld-V2.", "AI": {"tldr": "本文提出Edit-R1，一个基于策略优化的指令式图像编辑后训练框架。它利用DiffusionNFT进行高效优化，并使用多模态大语言模型（MLLM）作为统一的无训练奖励模型，辅以低方差分组过滤机制，显著提升了模型泛化能力和性能，实现了SOTA。", "motivation": "指令式图像编辑模型在监督微调下容易过拟合，限制了其在训练分布之外的探索和泛化能力。此外，由于编辑指令和任务的多样性，缺乏一个通用的奖励模型。", "method": "引入Edit-R1后训练框架，基于策略优化。具体方法包括：1) 采用Diffusion Negative-aware Finetuning (DiffusionNFT)，一种与流匹配前向过程一致的无似然策略优化方法，支持高阶采样和高效训练。2) 使用多模态大语言模型（MLLM）作为统一、免训练的奖励模型，利用其输出logits提供细粒度反馈。3) 设计低方差分组过滤机制，以减少MLLM评分噪声并稳定优化。", "result": "使用该框架训练的UniWorld-V2在ImgEdit和GEdit-Bench基准测试中取得了最先进（SOTA）的结果，得分分别为4.49和7.83。该框架具有模型无关性，应用于Qwen-Image-Edit和FLUX-Kontext等不同基础模型时，均能带来显著的性能提升。", "conclusion": "Edit-R1框架通过结合策略优化（DiffusionNFT）和基于MLLM的统一奖励模型，有效解决了指令式图像编辑中模型过拟合和泛化能力不足的问题。其模型无关性和在多个基准测试中达到的SOTA性能，证明了其广泛的适用性和有效性。"}}
{"id": "2510.16913", "categories": ["cs.CV", "68T07, 68U10, 68U35", "I.2.10; I.4.8; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.16913", "abs": "https://arxiv.org/abs/2510.16913", "authors": ["Akhila Kambhatla", "Ahmed R Khaled"], "title": "Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation", "comment": "9 Images with 1 figure and 3 Tables. This is a preprint submitted to\n  arXiv", "summary": "Thermal weapon segmentation is crucial for surveillance and security\napplications, enabling robust detection under lowlight and visually obscured\nconditions where RGB-based systems fail. While convolutional neural networks\n(CNNs) dominate thermal segmentation literature, their ability to capture\nlong-range dependencies and fine structural details is limited. Vision\nTransformers (ViTs), with their global context modeling capabilities, have\nachieved state-of-the-art results in RGB segmentation tasks, yet their\npotential in thermal weapon segmentation remains underexplored. This work\nadapts and evaluates four transformer-based architectures SegFormer,\nDeepLabV3\\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a\ncustom thermal dataset comprising 9,711 images collected from real world\nsurveillance videos and automatically annotated using SAM2. We employ standard\naugmentation strategies within the MMSegmentation framework to ensure robust\nmodel training and fair architectural comparison. Experimental results\ndemonstrate significant improvements in segmentation performance: SegFormer-b5\nachieves the highest mIoU (94.15\\%) and Pixel Accuracy (97.04\\%), while\nSegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive\nmIoU (90.84\\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and\n92.24\\% mIoU, and DeepLabV3\\+ R101-D8 reaches 92.76\\% mIoU at 29.86 FPS. The\ntransformer architectures demonstrate robust generalization capabilities for\nweapon detection in low-light and occluded thermal environments, with flexible\naccuracy-speed trade-offs suitable for diverse real-time security applications.", "AI": {"tldr": "本研究评估了四种基于Transformer的架构在定制热成像数据集上的武器分割性能，结果表明SegFormer-b5实现了最高的精度，SegFormer-b0实现了最快的推理速度，证明了Transformer在热成像武器检测中的潜力。", "motivation": "热成像武器分割对监控和安全至关重要，可在低光和视觉受阻条件下提供鲁棒检测，而基于RGB的系统在此类条件下失效。卷积神经网络（CNN）在捕捉长距离依赖和精细结构细节方面存在局限性。虽然Vision Transformer（ViT）在RGB分割任务中表现出色，但其在热成像武器分割中的潜力尚未得到充分探索。", "method": "本研究调整并评估了SegFormer、DeepLabV3+、SegNeXt和Swin Transformer四种基于Transformer的架构，用于在包含9,711张图像的定制热成像数据集上进行二元武器分割。这些图像来自真实世界监控视频，并使用SAM2自动标注。研究在MMSegmentation框架内采用标准数据增强策略，以确保模型训练的鲁棒性和架构比较的公平性。", "result": "实验结果显示，分割性能显著提升：SegFormer-b5实现了最高的mIoU（94.15%）和像素精度（97.04%）。SegFormer-b0提供了最快的推理速度（98.32 FPS），且mIoU具有竞争力（90.84%）。SegNeXt-mscans提供了平衡的性能，具有85.12 FPS和92.24%的mIoU。DeepLabV3+ R101-D8在29.86 FPS下达到了92.76%的mIoU。", "conclusion": "Transformer架构在低光和遮挡的热成像环境中，对武器检测表现出强大的泛化能力，并提供了灵活的精度-速度权衡，适用于各种实时安全应用。"}}
{"id": "2510.16973", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.16973", "abs": "https://arxiv.org/abs/2510.16973", "authors": ["Praveenbalaji Rajendran", "Mojtaba Safari", "Wenfeng He", "Mingzhe Hu", "Shansong Wang", "Jun Zhou", "Xiaofeng Yang"], "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis", "comment": null, "summary": "Recent advancements in artificial intelligence (AI), particularly foundation\nmodels (FMs), have revolutionized medical image analysis, demonstrating strong\nzero- and few-shot performance across diverse medical imaging tasks, from\nsegmentation to report generation. Unlike traditional task-specific AI models,\nFMs leverage large corpora of labeled and unlabeled multimodal datasets to\nlearn generalized representations that can be adapted to various downstream\nclinical applications with minimal fine-tuning. However, despite the rapid\nproliferation of FM research in medical imaging, the field remains fragmented,\nlacking a unified synthesis that systematically maps the evolution of\narchitectures, training paradigms, and clinical applications across modalities.\nTo address this gap, this review article provides a comprehensive and\nstructured analysis of FMs in medical image analysis. We systematically\ncategorize studies into vision-only and vision-language FMs based on their\narchitectural foundations, training strategies, and downstream clinical tasks.\nAdditionally, a quantitative meta-analysis of the studies was conducted to\ncharacterize temporal trends in dataset utilization and application domains. We\nalso critically discuss persistent challenges, including domain adaptation,\nefficient fine-tuning, computational constraints, and interpretability along\nwith emerging solutions such as federated learning, knowledge distillation, and\nadvanced prompting. Finally, we identify key future research directions aimed\nat enhancing the robustness, explainability, and clinical integration of FMs,\nthereby accelerating their translation into real-world medical practice.", "AI": {"tldr": "本文综述了医学图像分析中的基础模型（FMs），系统地分类了其架构、训练范式和临床应用，并讨论了挑战与未来方向。", "motivation": "尽管医学影像领域的基础模型研究迅速发展，但该领域仍呈碎片化状态，缺乏对架构演变、训练范式和跨模态临床应用的统一系统性综合分析。", "method": "本文对医学图像分析中的基础模型进行了全面且结构化的分析。研究方法包括：1) 根据架构基础、训练策略和下游临床任务，将研究系统地分为纯视觉和视觉-语言基础模型；2) 对研究进行了定量元分析，以描述数据集利用和应用领域的时间趋势；3) 批判性地讨论了持续存在的挑战和新兴解决方案。", "result": "本文对基础模型进行了分类（纯视觉和视觉-语言），描述了数据集利用和应用领域的时间趋势。讨论了现有挑战，如领域适应、高效微调、计算限制和可解释性，并提出了联邦学习、知识蒸馏和高级提示等新兴解决方案。", "conclusion": "文章指出了未来研究的关键方向，旨在增强基础模型的鲁棒性、可解释性和临床整合，以加速其在实际医疗实践中的转化应用。"}}
{"id": "2510.17516", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17516", "abs": "https://arxiv.org/abs/2510.17516", "authors": ["Tiancheng Hu", "Joachim Baumann", "Lorenzo Lupo", "Dirk Hovy", "Nigel Collier", "Paul Röttger"], "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors", "comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench", "summary": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.", "AI": {"tldr": "本文引入了SimBench，一个大规模、标准化的基准测试，用于评估大型语言模型（LLM）模拟人类行为的能力。研究发现，当前LLM的模拟能力有限，但与模型大小呈对数线性关系，并揭示了对齐与模拟之间的权衡，以及模型在模拟特定人口群体时的困难。", "motivation": "现有LLM对人类行为模拟的评估是碎片化、基于定制任务和指标的，导致结果不可比较。这阻碍了LLM模拟科学的稳健和可复现发展。", "method": "研究人员推出了SimBench，这是首个大规模、标准化的LLM模拟基准。它整合了20个多样化数据集，涵盖了从道德决策到经济选择等任务，并涉及全球广泛的参与者群体。", "result": "1. 即使是目前最好的LLM，模拟能力也有限（得分40.80/100）。\n2. 模拟性能与模型大小呈对数线性关系。\n3. 增加推理时间计算并不能提高模拟性能。\n4. 存在对齐-模拟权衡：指令微调能提高低熵（共识性）问题的性能，但会降低高熵（多样性）问题的性能。\n5. 模型在模拟特定人口群体时表现尤为困难。\n6. 模拟能力与深度、知识密集型推理（MMLU-Pro）关联最强（r=0.939）。", "conclusion": "SimBench通过使进展可衡量，旨在加速更忠实LLM模拟器的开发。研究结果为理解LLM模拟何时、如何以及为何成功或失败提供了基础。"}}
{"id": "2510.17532", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17532", "abs": "https://arxiv.org/abs/2510.17532", "authors": ["Raghu Vamshi Hemadri", "Geetha Krishna Guruju", "Kristi Topollai", "Anna Ewa Choromanska"], "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction", "comment": null, "summary": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.", "AI": {"tldr": "本研究提出一个多任务学习框架，通过将自回归大型语言模型与临床推理对齐，以预测癌症治疗结果，并生成可解释的理由。CoT提示和GRPO方法显著提升了模型的预测性能和可解释性。", "motivation": "预测癌症治疗结果需要准确且可解释的模型，尤其是在异构临床数据存在的情况下。尽管大型语言模型（LLMs）在生物医学自然语言处理中表现出色，但它们通常缺乏结构化推理能力，这对于高风险决策支持至关重要。", "method": "研究提出了一个统一的多任务学习框架，将自回归LLM（LLaMa3-8B和Med42-8B）与临床推理对齐，用于MSK-CHORD数据集上的结果预测。模型被训练以共同执行二元生存分类、连续生存时间回归和自然语言理由生成。评估了三种对齐策略：1) 标准监督微调（SFT），2) 结合思维链（CoT）提示的SFT，以及3) 群体相对策略优化（GRPO），一种将模型输出与专家推理轨迹对齐的强化学习方法。", "result": "实验表明，CoT提示将F1提高了+6.0，MAE降低了12%。GRPO在BLEU、ROUGE和BERTScore指标上实现了最先进的可解释性和预测性能。研究还发现，现有生物医学LLM由于架构限制，常无法生成有效的推理轨迹。", "conclusion": "研究结果强调了在多任务临床建模中，考虑推理的对齐的重要性，并为精准肿瘤学中可解释、可信赖的LLM树立了新基准。"}}
{"id": "2510.16983", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16983", "abs": "https://arxiv.org/abs/2510.16983", "authors": ["Yuanzhi Zhu", "Eleftherios Tsonis", "Lucas Degeorge", "Vicky Kalogeiton"], "title": "One-step Diffusion Models with Bregman Density Ratio Matching", "comment": "work in progress", "summary": "Diffusion and flow models achieve high generative quality but remain\ncomputationally expensive due to slow multi-step sampling. Distillation methods\naccelerate them by training fast student generators, yet most existing\nobjectives lack a unified theoretical foundation. In this work, we propose\nDi-Bregman, a compact framework that formulates diffusion distillation as\nBregman divergence-based density-ratio matching. This convex-analytic view\nconnects several existing objectives through a common lens. Experiments on\nCIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves\nimproved one-step FID over reverse-KL distillation and maintains high visual\nfidelity compared to the teacher model. Our results highlight Bregman\ndensity-ratio matching as a practical and theoretically-grounded route toward\nefficient one-step diffusion generation.", "AI": {"tldr": "Di-Bregman框架通过基于Bregman散度的密度比匹配，为扩散模型蒸馏提供了一个统一的理论基础，实现了高效的单步生成并提高了性能。", "motivation": "扩散和流模型虽然生成质量高，但多步采样导致计算成本高昂。现有蒸馏方法旨在加速，但其目标函数缺乏统一的理论基础。", "method": "提出了Di-Bregman框架，将扩散蒸馏公式化为基于Bregman散度的密度比匹配问题。这种凸分析的视角统一了现有的一些目标函数。", "result": "在CIFAR-10和文本到图像生成任务上，Di-Bregman相比反向KL蒸馏，实现了更优的单步FID，并保持了与教师模型相似的视觉保真度。", "conclusion": "研究结果表明，Bregman密度比匹配是实现高效单步扩散生成的一种实用且有理论依据的途径。"}}
{"id": "2510.16926", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16926", "abs": "https://arxiv.org/abs/2510.16926", "authors": ["Chenxu Li", "Zhicai Wang", "Yuan Sheng", "Xingyu Zhu", "Yanbin Hao", "Xiang Wang"], "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input", "comment": "23 pages,19 figures", "summary": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image\nresolutions. However, current evaluation paradigms primarily assess semantic\nperformance, overlooking the critical question of resolution robustness -\nwhether performance remains stable across varying input resolutions. To address\nthis gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising\n14,400 samples across 12 resolution levels and six core capability dimensions.\nWe designed a novel evaluation framework that goes beyond traditional accuracy\nmetrics to capture performance stability. This framework introduces multiple\nrobustness metrics: Spearman's correlation for assessing resolution-performance\ntrends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring\nperformance volatility. Using these metrics, we conducted a large-scale\nevaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and\ntask-centric robustness examination, (2) investigation of preprocessing\nstrategies including padding and super-resolution, and (3) exploration of\nfine-tuning for stability enhancement.", "AI": {"tldr": "本文提出了一个名为Res-Bench的综合基准和一套新颖的评估框架，用于衡量多模态大语言模型（MLLMs）在不同图像分辨率下的鲁棒性，填补了现有评估方法中的空白。", "motivation": "尽管多模态大语言模型（MLLMs）日益支持动态图像分辨率，但当前的评估范式主要侧重于语义性能，忽视了分辨率鲁棒性这一关键问题——即性能在不同输入分辨率下是否保持稳定。", "method": "本文引入了Res-Bench，一个包含14,400个样本、涵盖12个分辨率级别和6个核心能力维度的综合基准。设计了一个超越传统准确率指标的新颖评估框架，引入了多个鲁棒性指标：用于评估分辨率-性能趋势的Spearman相关系数，以及用于衡量性能波动性的绝对/相对连续误差（ACE/RCE）。利用这些指标，对领先的MLLMs进行了大规模评估，分析内容包括：模型和任务中心的鲁棒性检查、预处理策略（如填充和超分辨率）的探究，以及为增强稳定性进行微调的探索。", "result": "通过Res-Bench基准和新颖的鲁棒性评估框架，对领先的MLLMs进行了大规模评估。分析范围涵盖了模型中心和任务中心的鲁棒性考察、预处理策略（包括填充和超分辨率）的影响，以及通过微调提升稳定性的可能性。具体评估结果（如哪些模型表现更好，或特定策略的效果）需查阅论文正文。", "conclusion": "本文通过引入Res-Bench基准和一套新的鲁棒性评估框架，成功解决了当前MLLM评估中缺乏分辨率鲁棒性考量的问题。这为理解和提升MLLMs在动态分辨率场景下的性能稳定性提供了关键工具和方法。"}}
{"id": "2510.16988", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16988", "abs": "https://arxiv.org/abs/2510.16988", "authors": ["Junhao Zhao", "Zishuai Liu", "Ruili Fang", "Jin Lu", "Linghan Zhang", "Fei Dou"], "title": "CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams", "comment": null, "summary": "The recognition of Activities of Daily Living (ADLs) from event-triggered\nambient sensors is an essential task in Ambient Assisted Living, yet existing\nmethods remain constrained by representation-level limitations. Sequence-based\napproaches preserve temporal order of sensor activations but are sensitive to\nnoise and lack spatial awareness, while image-based approaches capture global\npatterns and implicit spatial correlations but compress fine-grained temporal\ndynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)\nfail to enforce alignment between sequence- and image-based representation\nviews, underutilizing their complementary strengths. We propose Contrastive\nAlignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an\nend-to-end framework that jointly optimizes representation learning via\nSequence-Image Contrastive Alignment (SICA) and classification via\ncross-entropy, ensuring both cross-representation alignment and task-specific\ndiscriminability. CARE integrates (i) time-aware, noise-resilient sequence\nencoding with (ii) spatially-informed and frequency-sensitive image\nrepresentations, and employs (iii) a joint contrastive-classification objective\nfor end-to-end learning of aligned and discriminative embeddings. Evaluated on\nthree CASAS datasets, CARE achieves state-of-the-art performance (89.8% on\nMilan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to\nsensor malfunctions and layout variability, highlighting its potential for\nreliable ADL recognition in smart homes.", "AI": {"tldr": "本文提出CARE框架，通过序列-图像对比对齐（SICA）来融合事件触发式环境传感器数据，以实现日常生活活动（ADL）识别，达到最先进的性能，并对传感器故障和布局变化具有鲁棒性。", "motivation": "现有ADL识别方法（基于序列或基于图像）存在局限性，如对噪声敏感、缺乏空间意识、时间动态压缩和传感器布局扭曲。简单的特征融合无法有效利用它们的互补优势，且未能强制对齐不同表示视图。", "method": "CARE是一个端到端框架，通过序列-图像对比对齐（SICA）联合优化表示学习和通过交叉熵进行分类。它整合了(i)时间感知、抗噪声的序列编码和(ii)空间感知、频率敏感的图像表示，并采用(iii)联合对比-分类目标以学习对齐且具有判别性的嵌入。", "result": "CARE在三个CASAS数据集上取得了最先进的性能（Milan 89.8%，Cairo 88.9%，Kyoto7 73.3%），并展示了对传感器故障和布局变异的鲁棒性。", "conclusion": "CARE通过有效对齐和利用序列与图像表示的互补优势，为智能家居中可靠的ADL识别提供了潜力，解决了现有方法的局限性。"}}
{"id": "2510.17555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17555", "abs": "https://arxiv.org/abs/2510.17555", "authors": ["Collin Zhang", "Fei Huang", "Chenhan Yuan", "Junyang Lin"], "title": "Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation", "comment": null, "summary": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.", "AI": {"tldr": "本文提出了一种名为语言混淆门（LCG）的轻量级插件解决方案，用于在解码过程中过滤令牌，从而显著减少大型语言模型（LLMs）的语言混淆，且无需重新训练模型或影响任务性能。", "motivation": "大型语言模型在文本生成时常出现语言混淆（无意中混合语言）问题。现有解决方案要么需要重新训练模型，要么无法区分有害的语言混淆和可接受的语码转换。", "method": "LCG是一种轻量级、即插即用的解决方案，在解码过程中过滤令牌，而无需修改基础LLM。它使用范数调整的自蒸馏进行训练，以预测合适的语系并仅在需要时应用掩码。该方法基于以下发现：语言混淆不频繁，正确语言的令牌通常在顶部预测中，以及高资源语言的输出令牌嵌入范数更大，从而导致采样偏差。", "result": "在Qwen3、GPT-OSS、Gemma3、Llama3.1等多种模型上进行评估，LCG显著降低了语言混淆，通常降低了一个数量级，同时没有对任务性能产生负面影响。", "conclusion": "LCG为解决LLMs的语言混淆问题提供了一个有效、轻量级且无需重新训练的插件方案，能在不影响任务性能的前提下大幅减少语言混淆。"}}
{"id": "2510.17591", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17591", "abs": "https://arxiv.org/abs/2510.17591", "authors": ["Guang Yang", "Yujie Zhu"], "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection", "comment": "Accepted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025) as a findings long paper", "summary": "Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.", "AI": {"tldr": "本文提出了一种名为HGAdapter的新方法，通过结合超图神经网络和适配器微调，捕获代码中的高阶数据关联（抽象语法树家族、词法和行关联），以增强预训练语言模型（PLMs）在代码相关任务中的性能。", "motivation": "尽管预训练语言模型（PLMs）在代码相关任务中取得了良好效果，但它们未能充分考虑代码中潜在的高阶数据关联。", "method": "本文提出了三种代码token的高阶关联类型（抽象语法树家族关联、词法关联和行关联），并设计了一个token和超边生成器来捕获这些关联。通过改进超图神经网络架构并结合适配器微调，提出了HGAdapter，该方法能够编码高阶数据关联并可插入各种PLMs以提升性能。", "result": "在代码摘要和代码克隆检测任务的多个公共数据集（涵盖六种语言）上的实验表明，HGAdapter在不同程度上提升了PLMs的性能。", "conclusion": "实验结果验证了引入高阶数据关联有助于提高模型在代码相关任务中的有效性。"}}
{"id": "2510.17548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17548", "abs": "https://arxiv.org/abs/2510.17548", "authors": ["Nisrine Rair", "Alban Goupil", "Valeriu Vrabie", "Emmanuel Chochoy"], "title": "When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity", "comment": "Accepted to appear in the Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2025, Main\n  Conference)", "summary": "Language models are often evaluated with scalar metrics like accuracy, but\nsuch measures fail to capture how models internally represent ambiguity,\nespecially when human annotators disagree. We propose a topological perspective\nto analyze how fine-tuned models encode ambiguity and more generally instances.\n  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from\ntopological data analysis, reveals that fine-tuning restructures embedding\nspace into modular, non-convex regions aligned with model predictions, even for\nhighly ambiguous cases. Over $98\\%$ of connected components exhibit $\\geq 90\\%$\nprediction purity, yet alignment with ground-truth labels drops in ambiguous\ndata, surfacing a hidden tension between structural confidence and label\nuncertainty.\n  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry\ndirectly uncovering decision regions, boundary collapses, and overconfident\nclusters. Our findings position Mapper as a powerful diagnostic tool for\nunderstanding how models resolve ambiguity. Beyond visualization, it also\nenables topological metrics that may inform proactive modeling strategies in\nsubjective NLP tasks.", "AI": {"tldr": "本文提出一种拓扑学方法，利用Mapper工具分析微调后的语言模型（RoBERTa-Large）如何编码歧义。研究发现模型在嵌入空间中形成与预测一致的模块化区域，即使在歧义数据上，预测纯度很高，但与真实标签的一致性下降，揭示了结构自信与标签不确定性之间的矛盾。Mapper被证明是理解模型如何解决歧义的强大诊断工具。", "motivation": "语言模型常以准确率等标量指标评估，但这些指标无法捕捉模型内部如何表示歧义，尤其是在人类标注者存在分歧的情况下。", "method": "本文提出一种拓扑学视角来分析微调模型如何编码歧义和实例。具体方法是，将拓扑数据分析工具Mapper应用于在MD-Offense数据集上微调过的RoBERTa-Large模型，以揭示其嵌入空间的几何结构。", "result": "研究发现，微调将嵌入空间重构为模块化、非凸的区域，这些区域与模型预测（包括高度模糊情况）对齐。超过98%的连通分量显示出≥90%的预测纯度，但在模糊数据中，与真实标签的一致性下降，这揭示了结构置信度与标签不确定性之间的潜在矛盾。与PCA或UMAP不同，Mapper直接捕捉决策区域、边界塌陷和过度自信的聚类。", "conclusion": "研究结果表明，Mapper是一个强大的诊断工具，可用于理解模型如何解决歧义。除了可视化，它还支持拓扑度量，可为面向主观NLP任务的主动建模策略提供信息。"}}
{"id": "2510.16989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16989", "abs": "https://arxiv.org/abs/2510.16989", "authors": ["Luca Zanella", "Massimiliano Mancini", "Yiming Wang", "Alessio Tonioni", "Elisa Ricci"], "title": "Training-free Online Video Step Grounding", "comment": "NeurIPS 2025. Project website at https://lucazanella.github.io/baglm/", "summary": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims\nto detect which steps are performed in a video. Standard approaches for this\ntask require a labeled training set (e.g., with step-level annotations or\nnarrations), which may be costly to collect. Moreover, they process the full\nvideo offline, limiting their applications for scenarios requiring online\ndecisions. Thus, in this work, we explore how to perform VSG online and without\ntraining. We achieve this by exploiting the zero-shot capabilities of recent\nLarge Multimodal Models (LMMs). In particular, we use LMMs to predict the step\nassociated with a restricted set of frames, without access to the whole video.\nWe show that this online strategy without task-specific tuning outperforms\noffline and training-based models. Motivated by this finding, we develop\nBayesian Grounding with Large Multimodal Models (BaGLM), further injecting\nknowledge of past frames into the LMM-based predictions. BaGLM exploits\nBayesian filtering principles, modeling step transitions via (i) a dependency\nmatrix extracted through large language models and (ii) an estimation of step\nprogress. Experiments on three datasets show superior performance of BaGLM over\nstate-of-the-art training-based offline methods.", "AI": {"tldr": "本文提出了一种无需训练的在线视频步骤定位（VSG）方法，利用大型多模态模型（LMMs）的零样本能力，并通过贝叶斯滤波（BaGLM）进一步整合历史帧信息，显著优于现有离线和基于训练的方法。", "motivation": "标准的视频步骤定位（VSG）方法需要昂贵的标注训练集，并且离线处理整个视频，限制了其在需要在线决策场景中的应用。因此，研究人员希望探索如何在没有训练和在线的情况下执行VSG。", "method": "该研究利用近期大型多模态模型（LMMs）的零样本能力，在不访问整个视频的情况下，仅根据有限的帧预测关联步骤，实现了在线和无需训练的VSG。在此基础上，进一步开发了BaGLM（Bayesian Grounding with Large Multimodal Models），通过贝叶斯滤波原理，将过去帧的知识融入LMM的预测中。BaGLM利用大型语言模型提取的步骤依赖矩阵和步骤进度估计来建模步骤转换。", "result": "研究发现，无需任务特定调整的在线LMM策略优于离线和基于训练的模型。BaGLM在三个数据集上的实验结果表明，其性能优于当前最先进的基于训练的离线方法。", "conclusion": "该研究成功展示了利用大型多模态模型的零样本能力，可以实现在线且无需训练的视频步骤定位。通过结合贝叶斯滤波原理，所提出的BaGLM模型进一步提升了性能，甚至超越了当前最先进的、基于训练的离线方法，为VSG任务提供了一个高效且实用的新范式。"}}
{"id": "2510.17698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17698", "abs": "https://arxiv.org/abs/2510.17698", "authors": ["Liqun He", "Manolis Mavrikis", "Mutlu Cukurova"], "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues", "comment": null, "summary": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.", "AI": {"tldr": "本研究旨在通过对话分析方法，从学习者与大型语言模型（LLM）的对话中识别有效的教学策略，以弥补现有评估方法对学习者-LLM互动关注不足的缺陷。", "motivation": "现有对LLM教育应用的评估主要关注技术性能或学习成果，却常常忽视学习者与LLM之间的互动。本研究旨在填补这一空白，通过分析对话来识别有效的教学策略。", "method": "所提出的方法包括对话数据收集、对话行为（DA）标注、DA模式挖掘和预测模型构建。", "result": "目前已初步概述了一些早期见解，作为未来研究的初始步骤。这是一项正在进行中的研究。", "conclusion": "这项工作强调了在评估基于LLM的教育应用时，需要关注对话动态和教学策略的重要性。"}}
{"id": "2510.17602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17602", "abs": "https://arxiv.org/abs/2510.17602", "authors": ["Huiyuan Xie", "Chenyang Li", "Huining Zhu", "Chubin Zhang", "Yuxiao Ye", "Zhenghao Liu", "Zhiyuan Liu"], "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis", "comment": null, "summary": "Legal reasoning is a fundamental component of legal analysis and\ndecision-making. Existing computational approaches to legal reasoning\npredominantly rely on generic reasoning frameworks such as syllogism and IRAC,\nwhich do not comprehensively examine the nuanced processes that underpin legal\nreasoning. Moreover, current research has largely focused on criminal cases,\nwith insufficient modeling for civil cases. In this work, we present a novel\nframework for explicitly modeling legal reasoning in the analysis of Chinese\ntort-related civil cases. We first operationalize the legal reasoning processes\nused in tort analysis into the LawChain framework. LawChain is a three-module\nreasoning framework, with each module consisting of multiple finer-grained\nsub-steps. Informed by the LawChain framework, we introduce the task of tort\nlegal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to\nsystematically assess the critical steps within analytical reasoning chains for\ntort analysis. Leveraging this benchmark, we evaluate state-of-the-art large\nlanguage models for their legal reasoning ability in civil tort contexts. Our\nresults indicate that current models still fall short in accurately handling\ncrucial elements of tort legal reasoning. Furthermore, we introduce several\nbaseline approaches that explicitly incorporate LawChain-style reasoning\nthrough prompting or post-training. We conduct further experiments on\nadditional legal analysis tasks, such as Legal Named-Entity Recognition and\nCriminal Damages Calculation, to verify the generalizability of these\nbaselines. The proposed baseline approaches achieve significant improvements in\ntort-related legal reasoning and generalize well to related legal analysis\ntasks, thus demonstrating the value of explicitly modeling legal reasoning\nchains to enhance the reasoning capabilities of language models.", "AI": {"tldr": "该研究提出了一个名为LawChain的新型框架，用于明确建模中国侵权民事案件的法律推理过程，并构建了一个评估基准。它评估了现有大型语言模型在此任务上的表现，并提出了通过显式建模推理链来提升模型性能的基线方法。", "motivation": "现有的计算法律推理方法主要依赖通用框架，未能充分捕捉法律推理的细微之处。此外，当前研究多集中于刑事案件，对民事案件（特别是侵权案件）的建模不足。", "method": "研究首先将侵权分析中的法律推理过程操作化为LawChain框架，该框架包含三个模块及多个细粒度子步骤。接着，基于LawChain框架，构建了侵权法律推理任务及其评估基准LawChain$_{eval}$。然后，评估了最先进的大型语言模型在该基准上的表现。最后，引入了几种通过提示或后期训练明确整合LawChain式推理的基线方法，并在其他法律分析任务上验证了其泛化能力。", "result": "结果表明，当前大型语言模型在准确处理侵权法律推理的关键要素方面仍有不足。所提出的基线方法在侵权相关法律推理任务中取得了显著改进，并能很好地泛化到相关的法律分析任务。", "conclusion": "明确建模法律推理链对于增强语言模型的推理能力具有重要价值。"}}
{"id": "2510.17007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17007", "abs": "https://arxiv.org/abs/2510.17007", "authors": ["Ignacio M. De la Jara", "Cristian Rodriguez-Opazo", "Edison Marrese-Taylor", "Felipe Bravo-Marquez"], "title": "An empirical study of the effect of video encoders on Temporal Video Grounding", "comment": null, "summary": "Temporal video grounding is a fundamental task in computer vision, aiming to\nlocalize a natural language query in a long, untrimmed video. It has a key role\nin the scientific community, in part due to the large amount of video generated\nevery day. Although we find extensive work in this task, we note that research\nremains focused on a small selection of video representations, which may lead\nto architectural overfitting in the long run. To address this issue, we propose\nan empirical study to investigate the impact of different video features on a\nclassical architecture. We extract features for three well-known benchmarks,\nCharades-STA, ActivityNet-Captions and YouCookII, using video encoders based on\nCNNs, temporal reasoning and transformers. Our results show significant\ndifferences in the performance of our model by simply changing the video\nencoder, while also revealing clear patterns and errors derived from the use of\ncertain features, ultimately indicating potential feature complementarity.", "AI": {"tldr": "本研究通过实证分析不同视频特征对经典时序视频定位模型性能的影响，发现更换视频编码器能显著改变模型表现，并揭示了特征间的互补性。", "motivation": "时序视频定位是一项基础任务，但现有研究过度集中于少数视频表示，可能导致模型架构在长期运行中过拟合。因此，有必要探究不同视频特征的影响。", "method": "研究采用实证方法，在Charades-STA、ActivityNet-Captions和YouCookII三个知名基准数据集上，使用基于CNN、时序推理和Transformer的视频编码器提取不同视频特征，并将其应用于一个经典模型架构进行性能评估。", "result": "结果表明，仅通过更换视频编码器，模型的性能就表现出显著差异。同时，研究也揭示了使用特定特征时出现的清晰模式和错误。", "conclusion": "不同视频特征对时序视频定位模型的性能有显著影响，且研究结果暗示了不同特征之间存在潜在的互补性。"}}
{"id": "2510.17014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17014", "abs": "https://arxiv.org/abs/2510.17014", "authors": ["Ani Vanyan", "Alvard Barseghyan", "Hakob Tamazyan", "Tigran Galstyan", "Vahan Huroyan", "Naira Hovakimyan", "Hrant Khachatrian"], "title": "Do Satellite Tasks Need Special Pretraining?", "comment": null, "summary": "Foundation models have advanced machine learning across various modalities,\nincluding images. Recently multiple teams trained foundation models specialized\nfor remote sensing applications. This line of research is motivated by the\ndistinct characteristics of remote sensing imagery, specific applications and\ntypes of robustness useful for satellite image analysis. In this work we\nsystematically challenge the idea that specific foundation models are more\nuseful than general-purpose vision foundation models, at least in the small\nscale. First, we design a simple benchmark that measures generalization of\nremote sensing models towards images with lower resolution for two downstream\ntasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,\nan ImageNet-scale satellite imagery dataset, with several modifications\nspecific to remote sensing. We show that none of those pretrained models bring\nconsistent improvements upon general-purpose baselines at the ViT-B scale.", "AI": {"tldr": "本文系统性地挑战了特定遥感基础模型在小规模下优于通用视觉基础模型的观点，发现经过遥感特定修改的预训练模型在低分辨率泛化方面并未比通用基线带来持续改进。", "motivation": "基础模型在图像等多种模态上取得了进展，并有团队专门为遥感应用训练了基础模型，这源于遥感图像的独特特性、特定应用以及卫星图像分析所需的鲁棒性。本文旨在质疑这些特定基础模型是否比通用视觉基础模型更有用。", "method": "首先，设计了一个简单的基准，用于衡量遥感模型在两种下游任务中对低分辨率图像的泛化能力。其次，在MillionAID（一个ImageNet规模的卫星图像数据集）上，对iBOT（一种自监督视觉编码器）进行了训练，并进行了一些针对遥感领域的修改。", "result": "研究结果表明，在ViT-B规模下，这些经过遥感特定预训练的模型均未能比通用基线带来持续的改进。", "conclusion": "至少在小规模（ViT-B）下，特定遥感基础模型比通用视觉基础模型更有用的观点受到了挑战，因为它们未能展现出持续的性能提升。"}}
{"id": "2510.17620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17620", "abs": "https://arxiv.org/abs/2510.17620", "authors": ["Yuefeng Peng", "Parnian Afshar", "Megan Ganji", "Thomas Butler", "Amir Houmansadr", "Mingxian Wang", "Dezhi Hong"], "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models", "comment": null, "summary": "Large language models may encode sensitive information or outdated knowledge\nthat needs to be removed, to ensure responsible and compliant model responses.\nUnlearning has emerged as an efficient alternative to full retraining, aiming\nto remove specific knowledge while preserving overall model utility. Existing\nevaluations of unlearning methods focus on (1) the extent of forgetting of the\ntarget knowledge (forget set) and (2) maintaining performance on the retain set\n(i.e., utility). However, these evaluations overlook an important usability\naspect: users may still want the model to leverage the removed information if\nit is re-introduced in the prompt. In a systematic evaluation of six\nstate-of-the-art unlearning methods, we find that they consistently impair such\ncontextual utility. To address this, we augment unlearning objectives with a\nplug-in term that preserves the model's ability to use forgotten knowledge when\nit is present in context. Extensive experiments demonstrate that our approach\nrestores contextual utility to near original levels while still maintaining\neffective forgetting and retain-set utility.", "AI": {"tldr": "现有的大语言模型遗忘方法损害了模型在提示中重新引入被遗忘知识时的上下文利用能力。本文提出了一种插件项来增强遗忘目标，以恢复这种上下文实用性，同时保持有效的遗忘和模型整体效用。", "motivation": "大语言模型可能编码敏感或过时的信息，需要通过遗忘技术移除。然而，现有的遗忘评估方法忽略了一个重要的可用性方面：当被遗忘的信息在提示中重新引入时，用户可能仍然希望模型能够利用这些信息。研究发现现有方法损害了这种“上下文实用性”。", "method": "本文首先对六种最先进的遗忘方法进行了系统评估，发现它们普遍损害了模型的上下文实用性。为解决此问题，研究者在遗忘目标中增加了一个插件项，旨在保留模型在上下文中存在被遗忘知识时利用这些知识的能力。", "result": "实验结果表明，现有最先进的遗忘方法确实持续损害了模型的上下文实用性。而本文提出的方法能够将上下文实用性恢复到接近原始水平，同时仍然保持有效的遗忘和对保留集知识的实用性。", "conclusion": "本文提出了一种有效的方法，通过在遗忘目标中加入一个插件项，解决了大语言模型遗忘过程中被忽视的上下文实用性问题。该方法使模型在提示中重新引入被遗忘知识时仍能利用这些知识，同时不影响知识遗忘的效果和模型的整体性能。"}}
{"id": "2510.17023", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17023", "abs": "https://arxiv.org/abs/2510.17023", "authors": ["Shraman Pramanick", "Effrosyni Mavroudi", "Yale Song", "Rama Chellappa", "Lorenzo Torresani", "Triantafyllos Afouras"], "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs", "comment": "ICCV 2025 (Highlights)", "summary": "We introduce ED-VTG, a method for fine-grained video temporal grounding\nutilizing multi-modal large language models. Our approach harnesses the\ncapabilities of multimodal LLMs to jointly process text and video, in order to\neffectively localize natural language queries in videos through a two-stage\nprocess. Rather than being directly grounded, language queries are initially\ntransformed into enriched sentences that incorporate missing details and cues\nto aid in grounding. In the second stage, these enriched queries are grounded,\nusing a lightweight decoder, which specializes at predicting accurate\nboundaries conditioned on contextualized representations of the enriched\nqueries. To mitigate noise and reduce the impact of hallucinations, our model\nis trained with a multiple-instance-learning objective that dynamically selects\nthe optimal version of the query for each training sample. We demonstrate\nstate-of-the-art results across various benchmarks in temporal video grounding\nand paragraph grounding settings. Experiments reveal that our method\nsignificantly outperforms all previously proposed LLM-based temporal grounding\napproaches and is either superior or comparable to specialized models, while\nmaintaining a clear advantage against them in zero-shot evaluation scenarios.", "AI": {"tldr": "ED-VTG是一种利用多模态大型语言模型进行细粒度视频时间定位的方法，通过两阶段（查询丰富化和轻量级解码器定位）和多实例学习目标，在各项基准测试中取得了最先进的成果，尤其在零样本评估中表现出色。", "motivation": "研究旨在利用多模态大型语言模型的能力，有效处理文本和视频，以实现对视频中自然语言查询的精准时间定位。", "method": "ED-VTG采用两阶段方法：首先，将语言查询转换为包含缺失细节和线索的“丰富化”句子；其次，使用轻量级解码器对这些丰富化查询进行定位，预测准确的时间边界。为减少噪声和幻觉影响，模型采用多实例学习目标进行训练，动态选择每个训练样本的最佳查询版本。", "result": "该方法在时间视频定位和段落定位的各种基准测试中均取得了最先进的（SOTA）结果。实验表明，ED-VTG显著优于所有先前基于LLM的时间定位方法，并与专业模型相比具有优势或可比性，在零样本评估场景中表现出明显优势。", "conclusion": "ED-VTG成功地将多模态大型语言模型应用于细粒度视频时间定位，通过其独特的两阶段处理和多实例学习策略，在多个基准测试中实现了卓越的性能，尤其在零样本泛化能力上表现突出。"}}
{"id": "2510.17652", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17652", "abs": "https://arxiv.org/abs/2510.17652", "authors": ["Joseph McInerney"], "title": "Qomhra: A Bilingual Irish-English Large Language Model", "comment": null, "summary": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.", "AI": {"tldr": "本文介绍了Qomhr'a，一个在低资源限制下开发的爱尔兰语-英语双语大型语言模型（LLM），通过双语持续预训练、指令微调和人类偏好对齐，显著提升了爱尔兰语和英语的性能。", "motivation": "研究动机是在低资源条件下开发一个爱尔兰语-英语双语LLM，旨在提升爱尔兰语性能的同时保持英语能力，并解决现有模型在爱尔兰语文本生成方面的不足。", "method": "研究方法包括：1) 混合和整理爱尔兰语和英语语料库进行双语持续预训练。2) 使用Google Gemini-2.5-Pro（经人工和LLM评估排名最高）合成30K爱尔兰语-英语平行指令微调数据集和1K人类偏好数据集。3) 对Qomhr'a进行指令微调和基于人类偏好的对齐。4) 通过翻译、性别理解、主题识别和世界知识等基准对模型进行全面评估。", "result": "主要结果包括：1) Qomhr'a在爱尔兰语方面实现了高达29%的性能提升，在英语方面实现了高达44%的性能提升。2) 指令微调后，Qomhr'a在指令遵循方面表现出显著进步。3) 合成的人类偏好数据集在接受和拒绝响应方面与爱尔兰语母语使用者表现出近乎完美的对齐。", "conclusion": "Qomhr'a成功地在低资源约束下开发了一个高性能的爱尔兰语-英语双语LLM，通过创新的数据合成和训练流程，显著提升了语言理解和生成能力，并在指令遵循方面取得了关键进展，对于聊天机器人功能至关重要。"}}
{"id": "2510.17034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17034", "abs": "https://arxiv.org/abs/2510.17034", "authors": ["Yutong Zhong"], "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding", "comment": null, "summary": "Multimodal 3D grounding has garnered considerable interest in Vision-Language\nModels (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex\nenvironments. However, these models suffer from a severe \"2D semantic bias\"\nthat arises from over-reliance on 2D image features for coarse localization,\nlargely disregarding 3D geometric inputs and resulting in suboptimal fusion\nperformance. In this paper, we propose a novel training framework called\nWhat-Where Representation Re-Forming (W2R2) to tackle this issue via\ndisentangled representation learning and targeted shortcut suppression. Our\napproach fundamentally reshapes the model's internal space by designating 2D\nfeatures as semantic beacons for \"What\" identification and 3D features as\nspatial anchors for \"Where\" localization, enabling precise 3D grounding without\nmodifying inference architecture. Key components include a dual-objective loss\nfunction with an Alignment Loss that supervises fused predictions using adapted\ncross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes\noverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.\nExperiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of\nW2R2, with significant gains in localization accuracy and robustness,\nparticularly in cluttered outdoor scenes.", "AI": {"tldr": "多模态3D定位模型存在“2D语义偏见”，过度依赖2D特征。本文提出W2R2训练框架，通过解耦2D（语义）和3D（空间）特征并抑制2D主导的捷径，显著提升了3D定位的准确性和鲁棒性，且无需修改推理架构。", "motivation": "现有多模态3D定位模型（VLM）过度依赖2D图像特征进行粗略定位，忽视3D几何输入，导致严重的“2D语义偏见”和次优的融合性能，阻碍了复杂环境中空间推理的进步。", "method": "本文提出“What-Where Representation Re-Forming (W2R2)”训练框架，通过解耦表示学习和有针对性的捷径抑制来解决问题。它将2D特征指定为“What”识别的语义信标，3D特征指定为“Where”定位的空间锚点。关键组件包括一个双目标损失函数：一个对融合预测进行监督的对齐损失（Alignment Loss），以及一个通过基于边距机制惩罚过度有效的2D主导伪输出的伪标签损失（Pseudo-Label Loss）。该方法无需修改推理架构。", "result": "在ScanRefer和ScanQA数据集上的实验表明，W2R2显著提高了定位准确性和鲁棒性，尤其是在杂乱的户外场景中表现突出。", "conclusion": "W2R2框架通过解耦2D和3D特征的表示学习并抑制2D语义偏见，成功解决了多模态3D定位中的2D语义偏见问题，实现了精确且鲁棒的3D定位，且无需改变现有模型架构。"}}
{"id": "2510.17715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17715", "abs": "https://arxiv.org/abs/2510.17715", "authors": ["Hanxu Hu", "Xingxing Zhang", "Jannis Vamvas", "Rico Sennrich", "Furu Wei"], "title": "QueST: Incentivizing LLMs to Generate Difficult Problems", "comment": "20 pages, 7 figures", "summary": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.", "AI": {"tldr": "QueST是一种新颖的框架，通过难度感知图采样和拒绝微调来生成具有挑战性的编程问题，有效解决了大型语言模型在竞争性编程任务中数据稀缺的问题，并显著提升了模型的性能。", "motivation": "大型语言模型在推理任务上表现出色，但其可扩展性受限于人类标注数据集的规模，特别是缺乏大规模、具有挑战性的编程问题训练数据。现有数据集规模小，且合成数据方法依赖于现有指令集或选择已有人类标注的难题，效率有限。", "method": "本文提出了QueST框架，结合了难度感知图采样（difficulty-aware graph sampling）和难度感知拒绝微调（difficulty-aware rejection fine-tuning），直接优化专门的生成器来创建有挑战性的编程问题。生成的合成数据用于从强大的教师模型进行蒸馏或对小型模型进行强化学习。", "result": "QueST训练的生成器在创建挑战性问题方面表现出优于GPT-4o的能力。将Qwen3-8B-base在QueST生成的10万个难题上进行微调后，其在LiveCodeBench上的性能超越了原始Qwen3-8B。额外使用11.2万个示例（2.8万个人工编写问题配以多个合成解决方案）后，8B模型达到了DeepSeek-R1-671B的性能。", "conclusion": "通过QueST生成复杂问题为推动大型语言模型在竞争性编程和推理领域的进步提供了一种有效且可扩展的方法，显著提升了模型的下游性能。"}}
{"id": "2510.17720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17720", "abs": "https://arxiv.org/abs/2510.17720", "authors": ["Nanda Kumar Rengarajan", "Jun Yan", "Chun Wang"], "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.", "AI": {"tldr": "本文提出了一种轻量级少样本命名实体识别（NER）框架，通过新的指令微调模板和保留实体信息的释义数据增强技术，有效解决了低资源场景下的NER挑战，取得了与现有先进模型相当的性能。", "motivation": "命名实体识别（NER）任务需要大量标注数据，这在标签获取成本高昂的低资源场景中极具挑战性。尽管零样本和指令微调方法取得了一些进展，但它们往往难以泛化到特定领域的实体，并且未能有效利用有限的可用数据。", "method": "该框架通过两项关键创新解决上述挑战：1) 设计了一种新的指令微调模板，采用简化的输出格式，结合了现有指令微调方法的原理，以利用最新大型语言模型（LLMs）的大上下文窗口；2) 引入了一种策略性数据增强技术，通过释义实体周围的上下文来扩展训练数据，同时保留了实体信息和语义关系。", "result": "实验结果表明，该方法在少样本和零样本任务上取得了与现有先进模型相当的性能。其中，少样本方法在CrossNER数据集上实现了80.1的平均F1分数。使用释义方法训练的模型比基线版本F1分数提高了多达17个点，显示出持续的改进。", "conclusion": "该框架为拥有有限NER训练数据和计算能力的团队提供了一个有前景的解决方案，有效提升了低资源场景下的NER性能。"}}
{"id": "2510.17035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17035", "abs": "https://arxiv.org/abs/2510.17035", "authors": ["Syed Konain Abbas", "Sandip Purnapatra", "M. G. Sarwar Murshed", "Conor Miller-Lynch", "Lambert Igene", "Soumyabrata Dey", "Stephanie Schuckers", "Faraz Hussain"], "title": "Conditional Synthetic Live and Spoof Fingerprint Generation", "comment": null, "summary": "Large fingerprint datasets, while important for training and evaluation, are\ntime-consuming and expensive to collect and require strict privacy measures.\nResearchers are exploring the use of synthetic fingerprint data to address\nthese issues. This paper presents a novel approach for generating synthetic\nfingerprint images (both spoof and live), addressing concerns related to\nprivacy, cost, and accessibility in biometric data collection. Our approach\nutilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce\nhigh-resolution synthetic live fingerprints, conditioned on specific finger\nidentities (thumb through little finger). Additionally, we employ CycleGANs to\ntranslate these into realistic spoof fingerprints, simulating a variety of\npresentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof\nfingerprints are crucial for developing robust spoof detection systems. Through\nthese generative models, we created two synthetic datasets (DB2 and DB3), each\ncontaining 1,500 fingerprint images of all ten fingers with multiple\nimpressions per finger, and including corresponding spoofs in eight material\ntypes. The results indicate robust performance: our StyleGAN3 model achieves a\nFr\\'echet Inception Distance (FID) as low as 5, and the generated fingerprints\nachieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The\nStyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess\nfingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,\nmatching experiments confirm strong privacy preservation, with no significant\nevidence of identity leakage, confirming the strong privacy-preserving\nproperties of our synthetic datasets.", "AI": {"tldr": "本研究提出了一种新颖的方法，利用条件StyleGAN2-ADA、StyleGAN3和CycleGANs生成高质量、隐私保护的合成活体和伪造指纹图像，以解决生物识别数据采集中的隐私、成本和可访问性问题。", "motivation": "大型指纹数据集的收集耗时、昂贵且需要严格的隐私措施。研究人员正在探索使用合成指纹数据来解决这些问题，尤其是在生物识别数据采集的隐私、成本和可访问性方面。", "method": "该方法利用条件StyleGAN2-ADA和StyleGAN3架构生成高分辨率的合成活体指纹，并根据特定的手指身份（从拇指到小指）进行条件化。此外，通过CycleGANs将这些活体指纹转换为逼真的伪造指纹，模拟多种呈现攻击材料（例如EcoFlex、Play-Doh）。研究者创建了两个合成数据集（DB2和DB3），每个包含1,500张指纹图像，涵盖十个手指，每个手指有多个印模，并包括八种材料类型的对应伪造指纹。", "result": "结果表明性能强大：StyleGAN3模型的Fréchet Inception Distance（FID）低至5，生成的指纹在0.01%的错误接受率下达到了99.47%的真实接受率。StyleGAN2-ADA模型在相同的错误接受率下达到了98.67%的真实接受率。指纹质量通过标准指标（NFIQ2、MINDTCT）进行评估。匹配实验证实了强大的隐私保护，没有显著的身份泄露证据，确认了合成数据集的隐私保护特性。", "conclusion": "该研究成功地开发了一种新颖的方法来生成高质量、隐私保护的合成活体和伪造指纹图像。这些合成数据集对于开发强大的防伪检测系统至关重要，并能有效解决生物识别数据收集中的隐私、成本和可访问性问题。"}}
{"id": "2510.17725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17725", "abs": "https://arxiv.org/abs/2510.17725", "authors": ["Haozhen Zhang", "Tao Feng", "Pengrui Han", "Jiaxuan You"], "title": "AcademicEval: Live Long-Context LLM Benchmark", "comment": "Accepted by TMLR. Code is available at\n  https://github.com/ulab-uiuc/AcademicEval", "summary": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval", "AI": {"tldr": "本文提出了一个名为\\textsc{AcademicEval}的实时基准测试，用于评估大型语言模型（LLMs）在长上下文生成任务中的表现，解决了现有基准的局限性，并揭示了LLMs在此类任务中的不足。", "motivation": "当前长上下文LLM基准测试存在三个主要限制：上下文长度僵化、标注工作量大以及LLM训练过程中标签泄露问题严重。", "method": "本文提出了\\textsc{AcademicEval}，一个实时评估LLMs长上下文生成任务的基准。它利用arXiv上的论文，设计了包括标题、摘要、引言和相关工作等学术写作任务，这些任务覆盖了不同抽象级别且无需手动标注。此外，\\textsc{AcademicEval}整合了来自合著者图谱的高质量、专家策划的少样本示例，以实现灵活的上下文长度，并采用高效的实时评估机制以防止标签泄露。", "result": "在\\textsc{AcademicEval}上进行的全面评估显示，LLMs在具有分层抽象级别的任务上表现不佳，并且难以处理长的少样本示例，这突显了该基准测试的挑战性。实验分析还为增强LLMs的长上下文建模能力提供了一些见解。", "conclusion": "\\textsc{AcademicEval}是一个有效且具有挑战性的实时基准测试，用于评估LLMs在长上下文生成任务中的性能。它揭示了当前LLMs在处理复杂抽象和长示例方面的不足，并为未来模型改进提供了方向。"}}
{"id": "2510.17051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17051", "abs": "https://arxiv.org/abs/2510.17051", "authors": ["Masoud Khairi Atani", "Alon Harell", "Hyomin Choi", "Runyu Yang", "Fabien Racape", "Ivan V. Bajic"], "title": "How Universal Are SAM2 Features?", "comment": "This work has been accepted for publication in IEEE Picture Coding\n  Symposium (PCS) 2025", "summary": "The trade-off between general-purpose foundation vision models and their\nspecialized counterparts is critical for efficient feature coding design and is\nnot yet fully understood. We investigate this trade-off by comparing the\nfeature versatility of the general-purpose Hiera encoder against the\nsegmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,\ntrainable neck to probe the adaptability of their frozen features, we quantify\nthe information-theoretic cost of specialization. Our results reveal that while\nSAM2's specialization is highly effective for spatially-related tasks like\ndepth estimation, it comes at a cost. The specialized SAM2 encoder\nunderperforms its generalist predecessor, Hiera, on conceptually distant tasks\nsuch as pose estimation and image captioning, demonstrating a measurable loss\nof broader semantic information. A novel cross-neck analysis on SAM2 reveals\nthat each level of adaptation creates a further representational bottleneck.\nOur analysis illuminates these trade-offs in feature universality, providing a\nquantitative foundation for designing efficient feature coding and adaptation\nstrategies for diverse downstream applications.", "AI": {"tldr": "本文通过比较通用模型Hiera和专业分割模型SAM2，研究了通用视觉基础模型与专业模型之间的权衡，发现专业化在特定任务上有效但会牺牲更广泛的语义信息，并引入了表示瓶颈。", "motivation": "通用视觉基础模型与专业模型之间的权衡对于高效特征编码设计至关重要，但尚未被充分理解。", "method": "通过使用轻量级、可训练的“颈部”（neck）探测冻结特征的适应性，比较了通用Hiera编码器和分割专业SAM2模型的特征多功能性，并量化了专业化的信息论成本。还对SAM2进行了新颖的跨颈部分析。", "result": "SAM2的专业化在深度估计等空间相关任务上非常有效，但它以牺牲为代价。专业化的SAM2编码器在姿态估计和图像字幕等概念上较远的任务上表现不如其通用前身Hiera，这表明其更广泛的语义信息有所损失。对SAM2进行的新颖跨颈部分析表明，每个适应级别都会进一步创建一个表示瓶颈。", "conclusion": "本文的分析阐明了特征通用性中的这些权衡，为设计高效的特征编码和适应策略以应对多样化的下游应用提供了量化基础。"}}
{"id": "2510.17045", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17045", "abs": "https://arxiv.org/abs/2510.17045", "authors": ["Deepak Sridhar", "Kartikeya Bhardwaj", "Jeya Pradha Jeyaraj", "Nuno Vasconcelos", "Ankita Nayak", "Harris Teague"], "title": "Video Reasoning without Training", "comment": null, "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.", "AI": {"tldr": "本文提出V-Reason，一种无需强化学习或监督微调的方法，通过在推理时使用基于熵的目标函数优化小型控制器来调整大型多模态模型（LMM）的行为，显著提升视频推理性能并大幅减少计算开销。", "motivation": "现有LMM的视频推理依赖昂贵的强化学习（RL）和冗长的思维链（CoT），导致训练和推理计算开销巨大，并且对模型思考过程的控制机制有限。", "method": "研究发现高质量模型在推理过程中会经历一系列微探索和微利用阶段，并利用模型输出的熵作为信号来捕捉这些行为。在此基础上，提出V-Reason方法，在推理时通过一个小型可训练控制器，利用基于熵的目标函数对LMM的价值缓存进行优化，以直接调整模型的微探索和利用行为，无需任何监督数据或RL。", "result": "V-Reason在多个视频推理数据集上显著优于基础指令调优模型，将与RL训练模型的平均准确率差距缩小到0.6%以内，且无需额外训练。此外，与RL模型相比，输出token减少了58.6%，大幅提升了效率。", "conclusion": "通过对模型推理过程中微探索和微利用行为的理论洞察，本文提出了一种新颖、高效的推理时调优方法V-Reason，它在不使用RL或监督微调的情况下，显著提升了LMM的视频推理能力，并实现了可与RL模型媲美的性能及更高的效率。"}}
{"id": "2510.17068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17068", "abs": "https://arxiv.org/abs/2510.17068", "authors": ["Zhe Luo", "Wenjing Jia", "Stuart Perry"], "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding", "comment": null, "summary": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet", "AI": {"tldr": "ProDAT是一种新颖的密度感知尾部丢弃机制，用于渐进式点云编码，它允许使用单个模型在多个比特率下进行解码，并显著提高了编码效率。", "motivation": "三维点云在自动驾驶等应用中日益重要，但其庞大的数据量和带宽限制阻碍了在资源受限环境中的部署。现有的基于学习的点云几何编码方法不支持渐进式解码，无法在不同细节级别进行解码。", "method": "本文提出了ProDAT，一种密度感知尾部丢弃机制。它利用密度信息作为指导信号，根据潜在特征和坐标的重要性自适应地解码它们，从而使用单个模型实现多比特率的渐进式解码。", "result": "实验结果表明，ProDAT不仅支持渐进式编码，而且与最先进的基于学习的编码技术相比，实现了卓越的编码效率，在SemanticKITTI上PSNR-D2的BD-rate改善超过28.6%，在ShapeNet上超过18.15%。", "conclusion": "ProDAT成功地弥补了现有学习方法在渐进式解码方面的不足，提供了一种高效且支持多比特率的渐进式点云编码方案。"}}
{"id": "2510.17733", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17733", "abs": "https://arxiv.org/abs/2510.17733", "authors": ["Tong Chen", "Akari Asai", "Luke Zettlemoyer", "Hannaneh Hajishirzi", "Faeze Brahman"], "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations", "comment": null, "summary": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.", "AI": {"tldr": "本文提出了一种名为二元检索增强奖励（RAR）的在线强化学习方法，旨在减少大型语言模型的事实性幻觉，同时避免对开放式生成和下游任务性能造成负面影响。该方法通过严格的二元奖励机制显著降低了幻觉率和错误答案，且不损害模型在指令遵循、数学或代码等方面的表现。", "motivation": "语言模型普遍存在生成与训练数据不符的事实性错误信息（外部幻觉）的问题。现有的缓解方法通常会降低模型在开放式生成和下游任务上的性能，从而限制了它们的实际应用价值。因此，研究人员旨在寻找一种能在减少幻觉的同时不牺牲其他性能的方法来解决这一权衡问题。", "method": "研究人员提出了一种在线强化学习方法，并引入了一种新颖的二元检索增强奖励（RAR）机制。与传统的连续奖励方案不同，该方法仅当模型的输出完全事实正确时才给予奖励1，否则奖励为0。该方法在Qwen3推理模型上进行评估，涵盖了多种任务。", "result": "在开放式生成任务中，二元RAR方法使幻觉率降低了39.3%，显著优于监督训练和连续奖励RL基线。在简答式问答中，模型学会了校准的弃权，在参数知识不足时策略性地输出“我不知道”，从而在PopQA和GPQA上分别减少了44.4%和21.7%的错误答案。重要的是，这些事实性改进并未导致指令遵循、数学或代码等任务的性能下降，而连续奖励RL虽然提高了事实性，却导致了质量退化。", "conclusion": "二元检索增强奖励的在线强化学习方法能有效减少大型语言模型的事实性幻觉和错误答案，同时保持了在指令遵循、数学和代码等其他任务上的性能。这解决了现有缓解方法中性能下降的权衡问题，提升了模型的实用性。"}}
{"id": "2510.17039", "categories": ["cs.CV", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.17039", "abs": "https://arxiv.org/abs/2510.17039", "authors": ["Mohammad R. Salmanpour", "Sonya Falahati", "Amir Hossein Pouria", "Amin Mousavi", "Somayeh Sadat Mehrnia", "Morteza Alizadeh", "Arman Gorji", "Zeinab Farsangi", "Alireza Safarian", "Mehdi Maghsudi", "Carlos Uribe", "Arman Rahmim", "Ren Yuan"], "title": "Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework", "comment": "13 pages, 2 figures, and 2 tables", "summary": "Lung cancer remains the leading cause of cancer mortality, with CT imaging\ncentral to screening, prognosis, and treatment. Manual segmentation is variable\nand time-intensive, while deep learning (DL) offers automation but faces\nbarriers to clinical adoption. Guided by the Knowledge-to-Action framework,\nthis study develops a clinician-in-the-loop DL pipeline to enhance\nreproducibility, prognostic accuracy, and clinical trust. Multi-center CT data\nfrom 999 patients across 12 public datasets were analyzed using five DL models\n(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against\nexpert contours on whole and click-point cropped images. Segmentation\nreproducibility was assessed using 497 PySERA-extracted radiomic features via\nSpearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic\nmodeling compared supervised (SL) and semi-supervised learning (SSL) across 38\ndimensionality reduction strategies and 24 classifiers. Six physicians\nqualitatively evaluated masks across seven domains, including clinical\nmeaningfulness, boundary quality, prognostic value, trust, and workflow\nintegration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),\nradiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive\naccuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed\nSL across models. Radiologists favored VNet for peritumoral representation and\nsmoother boundaries, preferring AI-generated initial masks for refinement\nrather than replacement. These results demonstrate that integrating VNet with\nSSL yields accurate, reproducible, and clinically trusted CT-based lung cancer\nprognosis, highlighting a feasible path toward physician-centered AI\ntranslation.", "AI": {"tldr": "本研究开发了一个以临床医生为中心的深度学习（DL）流程，用于肺癌CT影像的自动化分割和预后，旨在提高可重复性、预后准确性和临床信任度，并发现VNet结合半监督学习（SSL）表现最佳且受到临床医生青睐。", "motivation": "肺癌是癌症死亡的主要原因，CT影像在筛查、预后和治疗中至关重要。然而，手动分割耗时且结果多变，而深度学习自动化技术在临床应用中面临障碍。", "method": "本研究遵循“知识-行动”框架，开发了一个“临床医生在环”的深度学习流程。分析了来自12个公共数据集的999名患者的多中心CT数据，并使用了五种深度学习模型（3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D）。通过与专家勾画进行基准测试，评估了分割结果。通过497个影像组学特征，使用Spearman相关、ICC、Wilcoxon检验和MANOVA评估了分割的可重复性。预后模型比较了监督学习（SL）和半监督学习（SSL），并结合了38种降维策略和24种分类器。六名医生对AI生成的掩膜在临床意义、边界质量、预后价值、信任度和工作流程整合等七个方面进行了定性评估。", "result": "VNet在分割性能（Dice = 0.83, IoU = 0.71）、影像组学稳定性（平均相关性 = 0.76, ICC = 0.65）和SSL下的预测准确性（准确率 = 0.88, F1 = 0.83）方面表现最佳。半监督学习（SSL）在所有模型中始终优于监督学习（SL）。放射科医生偏爱VNet在肿瘤周围的表示和更平滑的边界，并倾向于将AI生成的初始掩膜用于完善而非完全替代。", "conclusion": "研究结果表明，将VNet与半监督学习（SSL）相结合，能够实现准确、可重复且临床信任的基于CT的肺癌预后，为以医生为中心的AI转化提供了一条可行的路径。"}}
{"id": "2510.17764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17764", "abs": "https://arxiv.org/abs/2510.17764", "authors": ["Xiao Ye", "Jacob Dineen", "Zhaonan Li", "Zhikun Xu", "Weiyu Chen", "Shijie Lu", "Yuxi Huang", "Ming Shen", "Phu Tran", "Ji-Eun Irene Yum", "Muhammad Ali Khan", "Muhammad Umar Afzal", "Irbaz Bin Riaz", "Ben Zhou"], "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications", "comment": null, "summary": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.", "AI": {"tldr": "该调查提出了一种基于自主性水平（L0-L3）的医疗大型语言模型评估框架，旨在将基准分数转化为临床工作流中安全可靠的性能，并强调风险意识评估。", "motivation": "医疗大型语言模型在标准基准测试中表现出色，但将其结果转化为临床工作流中安全可靠的性能仍然是一个挑战。现有的评估方法未能充分反映实际临床应用及其相关风险。", "method": "该调查通过“自主性水平”（L0-L3）重新构建了评估框架，涵盖信息工具、信息转换与聚合、决策支持和受监督的代理。它将现有基准和指标与每个级别允许的操作及其相关风险对齐，明确了评估目标。此外，还提出了一个基于级别的蓝图，用于选择指标、收集证据和报告主张，并将评估与监督联系起来。", "result": "通过将评估重心放在自主性上，该调查提供了一个框架，使得医疗大型语言模型的评估目标更加明确，并将其与特定自主性水平和风险关联起来。这有助于推动该领域从基于分数的声明转向可信、风险意识强的临床使用证据。", "conclusion": "通过以自主性为核心重新构建评估，该调查使医疗大型语言模型的评估超越了单纯的分数，转向了对真实临床使用具有可信、风险意识的证据，从而促进其在医疗领域的安全可靠应用。"}}
{"id": "2510.17078", "categories": ["cs.CV", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2510.17078", "abs": "https://arxiv.org/abs/2510.17078", "authors": ["Jad Berjawi", "Yoann Dupas", "Christophe C'erin"], "title": "Towards a Generalizable Fusion Architecture for Multimodal Object Detection", "comment": "8 pages, 8 figures, accepted at ICCV 2025 MIRA Workshop", "summary": "Multimodal object detection improves robustness in chal- lenging conditions\nby leveraging complementary cues from multiple sensor modalities. We introduce\nFiltered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing\narchitecture designed to enhance the fusion of RGB and infrared (IR) inputs.\nFMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress\nredun- dant spectral features with a cross-attention-based fusion module (MCAF)\nto improve intermodal feature sharing. Unlike approaches tailored to specific\ndatasets, FMCAF aims for generalizability, improving performance across\ndifferent multimodal challenges without requiring dataset- specific tuning. On\nLLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),\nFMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50\non VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a\nflexible foundation for robust multimodal fusion in future detection pipelines.", "AI": {"tldr": "本文提出了一种名为FMCAF的预处理架构，通过结合频域滤波和交叉注意力机制，增强RGB和红外输入的多模态融合，从而提高在挑战性条件下的目标检测鲁棒性及泛化能力。", "motivation": "多模态目标检测在复杂条件下能通过利用多传感器模态的互补信息提高鲁棒性。现有方法可能需要针对特定数据集进行调整，而本文旨在开发一种更具泛化性的融合方法，以应对不同的多模态挑战。", "method": "FMCAF（Filtered Multi-Modal Cross Attention Fusion）是一种预处理架构，它结合了两个主要模块：1) 频域滤波块（Freq-Filter），用于抑制冗余的光谱特征；2) 基于交叉注意力的融合模块（MCAF），用于改善模态间特征共享。该架构旨在增强RGB和红外（IR）输入的融合。", "result": "在LLVIP（低光照行人检测）和VEDAI（空中车辆检测）数据集上，FMCAF均优于传统的融合方法（拼接）。具体而言，在VEDAI上mAP@50提升了+13.9%，在LLVIP上提升了+1.1%。", "conclusion": "研究结果表明，FMCAF作为未来检测管道中鲁棒多模态融合的灵活基础具有潜力，其在不同多模态挑战中无需特定数据集调优即可提高性能。"}}
{"id": "2510.17795", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17795", "abs": "https://arxiv.org/abs/2510.17795", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "title": "Executable Knowledge Graphs for Replicating AI Research", "comment": "Work in progress", "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.", "AI": {"tldr": "LLM代理在复现AI研究时面临代码执行、背景知识不足和RAG限制等挑战。本文提出可执行知识图谱（xKG），一个整合技术洞察和代码片段的知识库，显著提升了代理在AI研究复现任务上的性能。", "motivation": "现有大型语言模型（LLM）代理在复现AI研究时，难以生成可执行代码，原因包括背景知识不足、检索增强生成（RAG）方法未能捕捉参考文献中的潜在技术细节，以及忽视实现层面的代码信号和缺乏支持多粒度检索和重用的结构化知识表示。", "method": "提出可执行知识图谱（xKG），这是一个模块化、可插拔的知识库，能够自动整合从科学文献中提取的技术洞察、代码片段和领域特定知识。", "result": "将xKG集成到三个不同的代理框架和两个不同的LLM中后，在PaperBench数据集上实现了显著的性能提升（使用o3-mini时提升10.9%）。", "conclusion": "xKG作为一种通用且可扩展的解决方案，有效解决了LLM代理在自动化AI研究复现中的挑战，并展示了其有效性。"}}
{"id": "2510.17793", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17793", "abs": "https://arxiv.org/abs/2510.17793", "authors": ["Austin Xu", "Xuan-Phi Nguyen", "Yilun Zhou", "Chien-Sheng Wu", "Caiming Xiong", "Shafiq Joty"], "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains", "comment": "29 pages, 9 tables, 6 figures", "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.", "AI": {"tldr": "本文通过大规模数据驱动的方法，使用简单的监督微调（SFT）训练了Foundational Automatic Reasoning Evaluators (FARE)，在多种评估任务和实际应用中，FARE-8B和FARE-20B超越了更大或基于RL训练的评估器，为开源评估器树立了新标准。", "motivation": "生成式评估器在训练和测试阶段对可扩展评估的需求日益增长，但现有研究主要侧重于新方法（如强化学习），而非大规模、数据驱动的开发。", "method": "本文通过数据扩展，收集了250万个样本，涵盖五种独特的评估任务（成对、步骤级、无参考和有参考验证、单次评分）和多个推理评估领域。在此基础上，采用简单的迭代拒绝采样监督微调（SFT）方法，训练了8B和20B（3.6B活跃参数）参数的Foundational Automatic Reasoning Evaluators (FARE) 模型家族。", "result": "FARE-8B挑战了更大的专业RL训练评估器，FARE-20B为开源评估器设定了新标准，超越了专业70B+评估器。在实际任务中，作为推理时重排序器，FARE-20B在MATH上实现了接近预言机的性能；作为RL训练中的验证器，FARE将下游RL训练模型的性能提高了高达14.1%；从FARE初始化并持续微调的FARE-Code在评估测试用例质量方面比gpt-oss-20B高出65%。", "conclusion": "大规模数据驱动的开发，结合简单的监督微调方法，可以训练出高性能的生成式评估器（FARE），其在各种评估任务和实际应用中均展现出卓越的性能，甚至超越了更大或基于强化学习训练的专业评估器。"}}
{"id": "2510.17797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17797", "abs": "https://arxiv.org/abs/2510.17797", "authors": ["Akshara Prabhakar", "Roshan Ram", "Zixiang Chen", "Silvio Savarese", "Frank Wang", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics", "comment": "Technical report; 13 pages plus references and appendices", "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200", "AI": {"tldr": "本文提出了企业深度研究（EDR），一个多智能体系统，旨在将非结构化企业数据转化为可操作的洞察，通过自适应查询分解、专业搜索、工具生态系统、可视化和反射机制实现。", "motivation": "随着信息呈指数级增长，企业面临将非结构化数据转化为连贯、可操作洞察的巨大压力。虽然自主智能体有前景，但它们在领域特定细微差别、意图对齐和企业集成方面常常遇到困难。", "method": "EDR是一个多智能体系统，包含：1) 一个用于自适应查询分解的主规划智能体；2) 四个专业搜索智能体（通用、学术、GitHub、LinkedIn）；3) 一个可扩展的基于MCP的工具生态系统，支持NL2SQL、文件分析和企业工作流；4) 一个用于数据驱动洞察的可视化智能体；5) 一个检测知识差距并更新研究方向（可选人工干预）的反射机制。", "result": "EDR实现了自动化报告生成、实时流媒体和无缝企业部署，并在内部数据集上得到验证。在DeepResearch Bench和DeepConsult等开放式基准测试中，EDR在无需人工干预的情况下，性能优于最先进的智能体系统。", "conclusion": "EDR系统有效解决了企业非结构化数据分析挑战，并在开放式基准测试中表现出色。作者发布了EDR框架和基准轨迹，以推动多智能体推理应用的研究进展。"}}
{"id": "2510.17095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17095", "abs": "https://arxiv.org/abs/2510.17095", "authors": ["Ruitong Gan", "Junran Peng", "Yang Liu", "Chuanchen Luo", "Qing Li", "Zhaoxiang Zhang"], "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation", "comment": null, "summary": "Planes are fundamental primitives of 3D sences, especially in man-made\nenvironments such as indoor spaces and urban streets. Representing these planes\nin a structured and parameterized format facilitates scene editing and physical\nsimulations in downstream applications. Recently, Gaussian Splatting (GS) has\ndemonstrated remarkable effectiveness in the Novel View Synthesis task, with\nextensions showing great potential in accurate surface reconstruction. However,\neven state-of-the-art GS representations often struggle to reconstruct planar\nregions with sufficient smoothness and precision. To address this issue, we\npropose GSPlane, which recovers accurate geometry and produces clean and\nwell-structured mesh connectivity for plane regions in the reconstructed scene.\nBy leveraging off-the-shelf segmentation and normal prediction models, GSPlane\nextracts robust planar priors to establish structured representations for\nplanar Gaussian coordinates, which help guide the training process by enforcing\ngeometric consistency. To further enhance training robustness, a Dynamic\nGaussian Re-classifier is introduced to adaptively reclassify planar Gaussians\nwith persistently high gradients as non-planar, ensuring more reliable\noptimization. Furthermore, we utilize the optimized planar priors to refine the\nmesh layouts, significantly improving topological structure while reducing the\nnumber of vertices and faces. We also explore applications of the structured\nplanar representation, which enable decoupling and flexible manipulation of\nobjects on supportive planes. Extensive experiments demonstrate that, with no\nsacrifice in rendering quality, the introduction of planar priors significantly\nimproves the geometric accuracy of the extracted meshes across various\nbaselines.", "AI": {"tldr": "GSPlane通过引入平面先验和动态高斯重分类器，显著提升了高斯泼溅（GS）在平面区域的几何精度和网格结构，同时保持了渲染质量。", "motivation": "高斯泼溅（GS）在新视角合成方面表现出色，但在重建平面区域时，即使是最新技术也难以达到足够的平滑度和精度。然而，平面作为3D场景（尤其是人造环境）的基础元素，其结构化和参数化表示对于场景编辑和物理模拟等下游应用至关重要。", "method": "GSPlane首先利用现成的分割和法线预测模型提取鲁棒的平面先验，为平面高斯坐标建立结构化表示，通过强制几何一致性来指导训练。其次，引入动态高斯重分类器，将持续高梯度的平面高斯自适应地重新分类为非平面高斯，以增强训练的鲁棒性。最后，利用优化后的平面先验来优化网格布局，显著改善拓扑结构并减少顶点和面片数量。该方法还探索了结构化平面表示在支持平面上物体解耦和灵活操作方面的应用。", "result": "实验证明，GSPlane在不牺牲渲染质量的前提下，通过引入平面先验显著提高了各种基线方法提取网格的几何精度。它能为重建场景中的平面区域生成准确的几何形状和干净、结构良好的网格连接，并支持支持平面上物体的解耦和灵活操作。", "conclusion": "GSPlane成功解决了高斯泼溅在重建平面区域时精度不足的问题。通过整合平面先验和动态高斯重分类，该方法显著提升了几何准确性，生成了结构化网格，并为下游应用提供了更灵活的场景表示，且不影响渲染质量。"}}
{"id": "2510.16724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16724", "abs": "https://arxiv.org/abs/2510.16724", "authors": ["Minhua Lin", "Zongyu Wu", "Zhichao Xu", "Hui Liu", "Xianfeng Tang", "Qi He", "Charu Aggarwal", "Hui Liu", "Xiang Zhang", "Suhang Wang"], "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications", "comment": "38 pages, 4 figures, 7 tables", "summary": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.", "AI": {"tldr": "这篇综述首次全面概述了基于强化学习（RL）的智能体搜索（agentic search），并从功能角色、优化策略和优化范围三个维度对其进行了组织和分析。", "motivation": "大型语言模型（LLMs）存在知识静态、幻觉和无法获取实时/领域特定信息的局限性。检索增强生成（RAG）通过外部证据缓解了这些问题，但传统RAG缺乏自适应控制。智能体搜索通过多步交互解决了这些限制，而强化学习（RL）为实现自适应和自我改进的搜索行为提供了强大机制。", "method": "该综述通过三个互补的维度组织了RL-based智能体搜索领域：(i) RL的作用（功能角色），(ii) RL的使用方式（优化策略），以及(iii) RL的应用范围（优化范围）。", "result": "综述总结了代表性方法、评估协议和应用，并讨论了构建可靠和可扩展的RL驱动智能体搜索系统所面临的开放挑战和未来方向。", "conclusion": "该综述旨在启发未来在RL与智能体搜索集成方面的研究，以期构建更可靠、更可扩展的系统。"}}
{"id": "2510.17105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17105", "abs": "https://arxiv.org/abs/2510.17105", "authors": ["Xiaogang Xu", "Jian Wang", "Yunfan Lu", "Ruihang Chu", "Ruixing Wang", "Jiafei Wu", "Bei Yu", "Liang Lin"], "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement", "comment": null, "summary": "Diffusion-based methods, leveraging pre-trained large models like Stable\nDiffusion via ControlNet, have achieved remarkable performance in several\nlow-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods\noften sacrifice content fidelity to attain higher perceptual realism. This\nissue is exacerbated in low-light scenarios, where severely degraded\ninformation caused by the darkness limits effective control. We identify two\nprimary causes of fidelity loss: the absence of suitable conditional latent\nmodeling and the lack of bidirectional interaction between the conditional\nlatent and noisy latent in the diffusion process. To address this, we propose a\nnovel optimization strategy for conditioning in pre-trained diffusion models,\nenhancing fidelity while preserving realism and aesthetics. Our method\nintroduces a mechanism to recover spatial details lost during VAE encoding,\ni.e., a latent refinement pipeline incorporating generative priors.\nAdditionally, the refined latent condition interacts dynamically with the noisy\nlatent, leading to improved restoration performance. Our approach is\nplug-and-play, seamlessly integrating into existing diffusion networks to\nprovide more effective control. Extensive experiments demonstrate significant\nfidelity improvements in PTDB methods.", "AI": {"tldr": "本文提出了一种针对预训练扩散模型（PTDB）的条件优化策略，通过恢复VAE编码中丢失的空间细节和实现条件潜变量与噪声潜变量的双向交互，显著提升了PTDB方法的内容保真度，同时保持了感知真实感和美观性。", "motivation": "扩散基方法（如基于ControlNet的Stable Diffusion）在低级视觉任务中表现出色，但预训练扩散基（PTDB）方法常以牺牲内容保真度为代价来获得更高的感知真实感。在低光照场景中，这一问题因严重退化的信息而加剧。主要原因是缺乏合适的条件潜变量建模以及扩散过程中条件潜变量与噪声潜变量之间缺乏双向交互。", "method": "提出了一种新颖的预训练扩散模型条件优化策略。该方法引入了一个机制来恢复VAE编码过程中丢失的空间细节，即一个结合生成先验的潜变量精炼管道。此外，精炼后的条件潜变量与噪声潜变量进行动态交互，从而改善了恢复性能。该方法是即插即用的。", "result": "广泛的实验证明，该方法显著提升了PTDB方法的内容保真度。", "conclusion": "本文提出的方法通过改进条件潜变量建模和引入动态双向交互，有效解决了PTDB方法在内容保真度方面的不足，同时保持了真实感和美观性，并能无缝集成到现有扩散网络中，提供了更有效的控制。"}}
{"id": "2510.17114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17114", "abs": "https://arxiv.org/abs/2510.17114", "authors": ["Hodaka Kawachi", "Tomoya Nakamura", "Hiroaki Santo", "SaiKiran Kumar Tedla", "Trevor Dalton Canham", "Yasushi Yagi", "Michael S. Brown"], "title": "Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras", "comment": null, "summary": "This paper introduces a method for using LED-based environmental lighting to\nproduce visually imperceptible watermarks for consumer cameras. Our approach\noptimizes an LED light source's spectral profile to be minimally visible to the\nhuman eye while remaining highly detectable by typical consumer cameras. The\nmethod jointly considers the human visual system's sensitivity to visible\nspectra, modern consumer camera sensors' spectral sensitivity, and narrowband\nLEDs' ability to generate broadband spectra perceived as \"white light\"\n(specifically, D65 illumination). To ensure imperceptibility, we employ\nspectral modulation rather than intensity modulation. Unlike conventional\nvisible light communication, our approach enables watermark extraction at\nstandard low frame rates (30-60 fps). While the information transfer rate is\nmodest-embedding 128 bits within a 10-second video clip-this capacity is\nsufficient for essential metadata supporting privacy protection and content\nverification.", "AI": {"tldr": "本文提出一种利用LED环境光产生人眼不可见但消费级相机可检测的水印方法，通过优化LED光谱，实现低帧率下的信息嵌入，用于隐私保护和内容验证。", "motivation": "研究动机是开发一种利用LED照明创建对人眼不可见、但能被普通消费级相机检测到的水印方法，以支持隐私保护和内容验证等元数据需求。", "method": "该方法优化了LED光源的光谱配置文件，使其对人眼可见度最低，同时对典型消费级相机具有高可检测性。它综合考虑了人眼视觉系统对可见光谱的敏感度、现代消费级相机传感器的光谱敏感度以及窄带LED生成宽带光谱（如D65白光）的能力。为确保不可感知性，采用光谱调制而非强度调制。该方法可在标准低帧率（30-60 fps）下提取水印。", "result": "该方法成功产生了人眼不可见的数字水印，并能被消费级相机高度检测。水印可在标准低帧率（30-60 fps）下提取。信息传输速率为每10秒视频嵌入128比特，足以承载支持隐私保护和内容验证所需的基本元数据。", "conclusion": "通过光谱优化和调制，该LED水印技术实现了人眼不可见但相机可检测的水印，其适度的信息传输能力足以满足隐私保护和内容验证等重要元数据需求。"}}
{"id": "2510.17131", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17131", "abs": "https://arxiv.org/abs/2510.17131", "authors": ["Xin Gao", "Jiyao Liu", "Guanghao Li", "Yueming Lyu", "Jianxiong Gao", "Weichen Yu", "Ningsheng Xu", "Liang Wang", "Caifeng Shan", "Ziwei Liu", "Chenyang Si"], "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection", "comment": "28 pages, 16 figures, conference", "summary": "Recent advancements have explored text-to-image diffusion models for\nsynthesizing out-of-distribution (OOD) samples, substantially enhancing the\nperformance of OOD detection. However, existing approaches typically rely on\nperturbing text-conditioned embeddings, resulting in semantic instability and\ninsufficient shift diversity, which limit generalization to realistic OOD. To\naddress these challenges, we propose GOOD, a novel and flexible framework that\ndirectly guides diffusion sampling trajectories towards OOD regions using\noff-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level\nguidance: (1) Image-level guidance based on the gradient of log partition to\nreduce input likelihood, drives samples toward low-density regions in pixel\nspace. (2) Feature-level guidance, derived from k-NN distance in the\nclassifier's latent space, promotes sampling in feature-sparse regions. Hence,\nthis dual-guidance design enables more controllable and diverse OOD sample\ngeneration. Additionally, we introduce a unified OOD score that adaptively\ncombines image and feature discrepancies, enhancing detection robustness. We\nperform thorough quantitative and qualitative analyses to evaluate the\neffectiveness of GOOD, demonstrating that training with samples generated by\nGOOD can notably enhance OOD detection performance.", "AI": {"tldr": "本文提出了GOOD框架，通过在扩散模型中引入图像级和特征级双重指导，直接引导采样轨迹生成多样化且可控的域外（OOD）样本，显著提升了OOD检测性能。", "motivation": "现有的基于文本到图像扩散模型生成OOD样本的方法，通常依赖于扰动文本条件嵌入，这导致了语义不稳定性和多样性不足，限制了其在真实OOD场景下的泛化能力。", "method": "GOOD框架利用现成的域内（ID）分类器，直接引导扩散采样轨迹向OOD区域移动。它包含双层指导：1) 图像级指导，基于对数配分函数梯度降低输入似然，将样本推向像素空间中的低密度区域；2) 特征级指导，利用分类器潜在空间中的k-NN距离，促进在特征稀疏区域采样。此外，GOOD引入了一个统一的OOD分数，自适应地结合图像和特征差异，增强检测鲁棒性。", "result": "GOOD的双重指导设计能够生成更可控和多样化的OOD样本。通过使用GOOD生成的样本进行训练，可以显著提高OOD检测性能。定性和定量分析均验证了GOOD的有效性。", "conclusion": "GOOD框架通过创新的双层指导机制，克服了现有方法在OOD样本生成中的局限性，实现了更可控、更多样化的OOD样本生成，从而显著提升了OOD检测的鲁棒性和性能。"}}
{"id": "2510.16234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16234", "abs": "https://arxiv.org/abs/2510.16234", "authors": ["Hanane Nour Moussa", "Patrick Queiroz Da Silva", "Daniel Adu-Ampratwum", "Alyson East", "Zitong Lu", "Nikki Puccetti", "Mingyi Xue", "Huan Sun", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "comment": null, "summary": "As AI tools become increasingly common for research ideation, robust\nevaluation is critical to ensure the validity and usefulness of generated\nideas. We introduce ScholarEval, a retrieval augmented evaluation framework\nthat assesses research ideas based on two fundamental criteria: soundness - the\nempirical validity of proposed methods based on existing literature, and\ncontribution - the degree of advancement made by the idea across different\ndimensions relative to prior research. To evaluate ScholarEval, we introduce\nScholarIdeas, the first expert-annotated dataset of multi-domain research ideas\nand reviews, comprised of 117 ideas across four disciplines: artificial\nintelligence, neuroscience, biochemistry, and ecology. Our evaluation shows\nthat ScholarEval achieves significantly higher coverage of points mentioned in\nthe human expert annotated rubrics in ScholarIdeas compared to all baselines.\nFurthermore, ScholarEval is consistently preferred over our strongest baseline\no4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,\nin terms of evaluation actionability, depth, and evidence support. Our\nlarge-scale user study also shows that ScholarEval significantly outperforms\ndeep research in literature engagement, idea refinement, and usefulness. We\nopenly release our code, dataset, and ScholarEval tool for the community to use\nand build on.", "AI": {"tldr": "本文介绍了ScholarEval，一个检索增强的评估框架，用于评估AI生成的科研想法的合理性和贡献度。通过专家标注的数据集ScholarIdeas，实验证明ScholarEval在评估覆盖率、可操作性、深度和证据支持方面显著优于现有基线，并在用户研究中表现出更高的文献参与度、想法完善度和实用性。", "motivation": "随着AI工具在科研构思中日益普及，对生成想法进行鲁棒评估至关重要，以确保其有效性和实用性。", "method": "本文引入了ScholarEval，一个基于检索增强的评估框架，它根据两个基本标准评估科研想法：合理性（基于现有文献的经验有效性）和贡献度（相对于现有研究，想法在不同维度上的进步程度）。为了评估ScholarEval，本文构建了ScholarIdeas，这是第一个专家标注的多领域科研想法和评审数据集，包含117个跨人工智能、神经科学、生物化学和生态学四个学科的想法。ScholarEval与基线系统（如OpenAI的o4-mini-deep-research）进行了比较，并进行了大规模用户研究。", "result": "评估结果显示，与所有基线相比，ScholarEval在ScholarIdeas数据集中对人类专家标注的评估要点覆盖率显著更高。此外，在评估的可操作性、深度和证据支持方面，ScholarEval始终优于最强的基线o4-mini-deep-research。大规模用户研究也表明，ScholarEval在文献参与度、想法完善度和实用性方面显著优于deep research。", "conclusion": "ScholarEval是一个有效且实用的科研想法评估框架，能够提高AI生成想法的有效性和实用性。研究成果、代码、数据集和ScholarEval工具已开源，供社区使用和进一步开发。"}}
{"id": "2510.16769", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16769", "abs": "https://arxiv.org/abs/2510.16769", "authors": ["Shuo Han", "Yukun Cao", "Zezhong Ding", "Zengyi Gao", "S Kevin Zhou", "Xike Xie"], "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have shown promise in graph understanding, but\nremain limited by input-token constraints, facing scalability bottlenecks and\nlacking effective mechanisms to coordinate textual and visual modalities. To\naddress these challenges, we propose GraphVista, a unified framework that\nenhances both scalability and modality coordination in graph understanding. For\nscalability, GraphVista organizes graph information hierarchically into a\nlightweight GraphRAG base, which retrieves only task-relevant textual\ndescriptions and high-resolution visual subgraphs, compressing redundant\ncontext while preserving key reasoning elements. For modality coordination,\nGraphVista introduces a planning agent that routes tasks to the most suitable\nmodality-using the text modality for simple property reasoning and the visual\nmodality for local and structurally complex reasoning grounded in explicit\ntopology. Extensive experiments demonstrate that GraphVista scales to large\ngraphs, up to $200\\times$ larger than those used in existing benchmarks, and\nconsistently outperforms existing textual, visual, and fusion-based methods,\nachieving up to $4.4\\times$ quality improvement over the state-of-the-art\nbaselines by fully exploiting the complementary strengths of both modalities.", "AI": {"tldr": "GraphVista是一个统一框架，通过分层检索和规划代理增强了视觉-语言模型（VLMs）在图理解中的可扩展性和模态协调能力，显著提升了性能。", "motivation": "现有的视觉-语言模型在图理解方面受到输入令牌限制，导致可扩展性瓶颈，并且缺乏有效的机制来协调文本和视觉模态。", "method": "GraphVista采用两种策略：1. **可扩展性**：将图信息分层组织成轻量级的GraphRAG基础，仅检索与任务相关的文本描述和高分辨率视觉子图，压缩冗余上下文。2. **模态协调**：引入一个规划代理，根据任务类型将任务路由到最合适的模态（文本用于简单属性推理，视觉用于局部和结构复杂推理）。", "result": "GraphVista可扩展到比现有基准大200倍的图，并且持续优于现有的文本、视觉和融合方法，与最先进的基线相比，质量提升高达4.4倍。", "conclusion": "GraphVista通过充分利用文本和视觉模态的互补优势，有效解决了图理解中的可扩展性和模态协调挑战，实现了显著的性能提升。"}}
{"id": "2510.17137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17137", "abs": "https://arxiv.org/abs/2510.17137", "authors": ["WenBo Xu", "Liu Liu", "Li Zhang", "Ran Zhang", "Hao Wu", "Dan Guo", "Meng Wang"], "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation", "comment": null, "summary": "Articulated objects, such as laptops and drawers, exhibit significant\nchallenges for 3D reconstruction and pose estimation due to their multi-part\ngeometries and variable joint configurations, which introduce structural\ndiversity across different states. To address these challenges, we propose\nKineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object\nShape Reconstruction and Generation, a unified framework for reconstructing\ndiverse articulated instances and pose estimation from single view input.\nSpecifically, we first encode complete geometry (SDFs), joint angles, and part\nsegmentation into a structured latent space via a novel Kinematic-Aware VAE\n(KA-VAE). In addition, we employ two conditional diffusion models: one for\nregressing global pose (SE(3)) and joint parameters, and another for generating\nthe kinematic-aware latent code from partial observations. Finally, we produce\nan iterative optimization module that bidirectionally refines reconstruction\naccuracy and kinematic parameters via Chamfer-distance minimization while\npreserving articulation constraints. Experimental results on synthetic,\nsemi-synthetic, and real-world datasets demonstrate the effectiveness of our\napproach in accurately reconstructing articulated objects and estimating their\nkinematic properties.", "AI": {"tldr": "KineDiff3D是一个统一框架，通过运动学感知VAE和扩散模型，从单视图输入实现铰接物体的3D重建和姿态估计，并进行迭代优化。", "motivation": "铰接物体由于其多部件几何形状和可变关节配置，导致结构多样性，给3D重建和姿态估计带来了重大挑战。", "method": "该方法首先通过新型运动学感知VAE（KA-VAE）将完整几何（SDFs）、关节角度和部件分割编码到结构化潜在空间。接着，利用两个条件扩散模型：一个用于回归全局姿态和关节参数，另一个用于从部分观测生成运动学感知潜在代码。最后，通过迭代优化模块，利用Chamfer距离最小化双向细化重建精度和运动学参数，同时保留铰接约束。", "result": "在合成、半合成和真实世界数据集上的实验结果表明，该方法能有效准确地重建铰接物体并估计其运动学特性。", "conclusion": "KineDiff3D框架成功解决了铰接物体3D重建和姿态估计的挑战，实现了从单视图输入的精确重建和运动学参数估计。"}}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.", "AI": {"tldr": "本文介绍了DeepAnalyze-8B，一个8B参数的智能体LLM，旨在实现从数据源到分析报告的全流程自主数据科学。它采用课程式智能体训练范式和数据驱动的轨迹合成框架，在各种数据任务上表现优异，超越了基于工作流的先进专有LLM智能体。", "motivation": "自主数据科学是一个长期存在的挑战，尽管强大的LLM已经出现，但现有的基于工作流的数据智能体因依赖预定义工作流，在实现完全自主数据科学方面仍存在根本性限制。", "method": "引入DeepAnalyze-8B，首个用于自主数据科学的智能体LLM。提出了一种课程式智能体训练范式，模仿人类数据科学家的学习轨迹，使LLM能逐步获取和整合多种能力。同时，引入了一个数据驱动的轨迹合成框架来构建高质量的训练数据。", "result": "DeepAnalyze-8B（仅8B参数）能够自动完成从数据源到分析师级别深度研究报告的端到端流程。它学会执行广泛的数据任务，包括数据问答、专业分析任务和开放式数据研究，并超越了基于最先进专有LLM构建的先前工作流智能体。", "conclusion": "DeepAnalyze-8B代表了迈向自主数据科学的重要一步。该模型的模型、代码和训练数据均已开源，为自主数据科学铺平了道路。"}}
{"id": "2510.17157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17157", "abs": "https://arxiv.org/abs/2510.17157", "authors": ["Yinghui Wang", "Xinyu Zhang", "Peng Du"], "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image", "comment": null, "summary": "Generating editable, parametric CAD models from a single image holds great\npotential to lower the barriers of industrial concept design. However, current\nmulti-modal large language models (MLLMs) still struggle with accurately\ninferring 3D geometry from 2D images due to limited spatial reasoning\ncapabilities. We address this limitation by introducing GACO-CAD, a novel\ntwo-stage post-training framework. It is designed to achieve a joint objective:\nsimultaneously improving the geometric accuracy of the generated CAD models and\nencouraging the use of more concise modeling procedures. First, during\nsupervised fine-tuning, we leverage depth and surface normal maps as dense\ngeometric priors, combining them with the RGB image to form a multi-channel\ninput. In the context of single-view reconstruction, these priors provide\ncomplementary spatial cues that help the MLLM more reliably recover 3D geometry\nfrom 2D observations. Second, during reinforcement learning, we introduce a\ngroup length reward that, while preserving high geometric fidelity, promotes\nthe generation of more compact and less redundant parametric modeling\nsequences. A simple dynamic weighting strategy is adopted to stabilize\ntraining. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD\nachieves state-of-the-art performance under the same MLLM backbone,\nconsistently outperforming existing methods in terms of code validity,\ngeometric accuracy, and modeling conciseness.", "AI": {"tldr": "GACO-CAD是一个两阶段后训练框架，通过结合几何先验和强化学习奖励，显著提升了从单张图像生成可编辑、参数化CAD模型的几何精度和建模简洁性，优于现有方法。", "motivation": "目前的跨模态大型语言模型（MLLMs）在从2D图像推断3D几何方面存在空间推理能力不足的问题，这限制了它们在降低工业概念设计门槛方面的潜力。", "method": "本文提出了GACO-CAD框架，包含两个阶段：1. 监督微调：利用深度图和表面法线图作为密集的几何先验，与RGB图像结合形成多通道输入，以提高MLLM从2D观测中恢复3D几何的可靠性。2. 强化学习：引入了群组长度奖励（group length reward），在保持高几何保真度的同时，促进生成更紧凑、冗余度更低的参数化建模序列。同时采用动态加权策略来稳定训练。", "result": "在DeepCAD和Fusion360数据集上的实验表明，在相同MLLM骨干网络下，GACO-CAD达到了最先进的性能，在代码有效性、几何精度和建模简洁性方面持续优于现有方法。", "conclusion": "GACO-CAD框架通过其两阶段后训练方法，有效解决了MLLMs在从单张图像生成可编辑参数化CAD模型时几何精度和建模效率的挑战，实现了卓越的性能，有望降低工业概念设计的门槛。"}}
{"id": "2510.17169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17169", "abs": "https://arxiv.org/abs/2510.17169", "authors": ["Roland Croft", "Brian Du", "Darcy Joseph", "Sharath Kumar"], "title": "Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition", "comment": "Accepted for publication in DICTA 2025", "summary": "Face Recognition (FR) models have been shown to be vulnerable to adversarial\nexamples that subtly alter benign facial images, exposing blind spots in these\nsystems, as well as protecting user privacy. End-to-end FR systems first obtain\npreprocessed faces from diverse facial imagery prior to computing the\nsimilarity of the deep feature embeddings. Whilst face preprocessing is a\ncritical component of FR systems, and hence adversarial attacks against them,\nwe observe that this preprocessing is often overlooked in blackbox settings.\nOur study seeks to investigate the transferability of several out-of-the-box\nstate-of-the-art adversarial attacks against FR when applied against different\npreprocessing techniques used in a blackbox setting. We observe that the choice\nof face detection model can degrade the attack success rate by up to 78%,\nwhereas choice of interpolation method during downsampling has relatively\nminimal impacts. Furthermore, we find that the requirement for facial\npreprocessing even degrades attack strength in a whitebox setting, due to the\nunintended interaction of produced noise vectors against face detection models.\nBased on these findings, we propose a preprocessing-invariant method using\ninput transformations that improves the transferability of the studied attacks\nby up to 27%. Our findings highlight the importance of preprocessing in FR\nsystems, and the need for its consideration towards improving the adversarial\ngeneralisation of facial adversarial examples.", "AI": {"tldr": "人脸识别模型易受对抗性攻击，但预处理步骤（尤其是人脸检测模型）会显著影响攻击的可迁移性。本研究提出一种预处理不变的方法，可将攻击的可迁移性提高27%。", "motivation": "人脸识别（FR）模型容易受到对抗性样本的攻击，但黑盒设置下的现有攻击通常忽略了关键的预处理阶段。本研究旨在调查对抗性攻击在不同预处理技术下的可迁移性。", "method": "研究了多种现成的最先进的对抗性攻击在黑盒设置下对不同预处理技术（如人脸检测模型和下采样插值方法）的可迁移性。此外，还在白盒设置下分析了噪声向量与人脸检测模型之间的相互作用。基于研究结果，提出了一种使用输入变换的预处理不变方法。", "result": "研究发现，人脸检测模型的选择可使攻击成功率降低高达78%，而插值方法的选择影响相对较小。在白盒设置下，人脸预处理甚至会因生成的噪声向量与人脸检测模型意外交互而降低攻击强度。所提出的预处理不变方法将所研究攻击的可迁移性提高了高达27%。", "conclusion": "人脸识别系统中的预处理至关重要，需要对其进行考虑，以提高人脸对抗性样本的对抗性泛化能力。"}}
{"id": "2510.17173", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17173", "abs": "https://arxiv.org/abs/2510.17173", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users", "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models", "summary": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.", "AI": {"tldr": "该研究通过用户试点和模拟器，评估了工具增强型LLM健康教练，发现统一的工具策略可能损害特定用户群体，并提出了一种“评估优先”的个性化路径，以避免平均值掩盖的子群体危害。", "motivation": "研究者旨在了解一个部署在网络上、由工具增强的LLM健康教练在真实用户场景下的表现，并探索如何实现个性化，同时避免对特定用户群体的潜在损害。", "method": "研究方法包括：1) 对七名真实用户进行试点研究（280次评分交互），使用因子化决策头（工具/风格）进行离线策略评估（OPE）。2) 构建一个带有隐藏原型的轻量级模拟器，并尝试添加早期信息增益奖励。", "result": "主要结果有：1) 统一的“重工具”策略虽然提高了日志上的平均价值，但损害了特定子群体，尤其是健康素养低/自我效能高的用户。2) 在模拟器中增加少量早期信息增益奖励，能可靠地缩短特质识别时间，并提高目标成功率和pass@3指标。", "conclusion": "研究得出结论，应采取“评估优先”的个性化路径：冻结生成器，基于类型化奖励（客观工具结果和满意度）学习子群体感知的决策头，并始终报告按原型划分的指标，以揭示平均值可能掩盖的子群体危害。"}}
{"id": "2510.17205", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17205", "abs": "https://arxiv.org/abs/2510.17205", "authors": ["Yingqi Fan", "Anhao Zhao", "Jinlan Fu", "Junlong Tong", "Hui Su", "Yijie Pan", "Wei Zhang", "Xiaoyu Shen"], "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs", "comment": "EMNLP 2025 Main", "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.", "AI": {"tldr": "本文通过系统分析揭示了多模态大语言模型（MLLMs）中跨模态交互的三阶段过程，并基于此提出了一种免训练的剪枝框架VisiPruner，显著降低了计算成本并优于现有方法。", "motivation": "多模态大语言模型（MLLMs）因多模态tokens数量的二次增长导致注意力计算量巨大，面临显著的计算开销。现有token剪枝方法缺乏对MLLMs如何处理和融合多模态信息的根本理解。", "method": "通过系统分析，本文揭示了MLLMs的跨模态交互三阶段过程：(1) 浅层识别任务意图，视觉tokens作为被动注意力接收器；(2) 中间层由少数关键视觉tokens驱动，突发性地发生跨模态融合；(3) 深层丢弃视觉tokens，专注于语言精炼。基于这些发现，本文提出了VisiPruner，一个免训练的剪枝框架。", "result": "VisiPruner在LLaVA-v1.5 7B上将视觉相关注意力计算量减少了高达99%，FLOPs减少了53.9%。它显著优于现有token剪枝方法，并能泛化到不同的MLLMs。", "conclusion": "本文的发现不仅提供了有效的剪枝框架，更重要的是，为通过将模型架构与其固有的逐层处理动态对齐来训练高效MLLMs提供了可操作的指导方针。"}}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.", "AI": {"tldr": "该研究提出通过显式视觉状态推理来构建VLM智能体的内部世界模型，通过分解推理过程、优化信念表示、引入世界建模奖励和Bi-Level GAE，显著提升了VLM智能体在多视觉环境下的表现，超越了现有专有模型。", "motivation": "与LLM智能体不同，VLM智能体在从文本状态转向复杂视觉观测时面临部分可观测性和对鲁棒世界建模的需求。核心问题是：VLM智能体能否通过显式视觉状态推理来构建内部世界模型？", "method": "研究通过强化学习（RL）在架构上强制并奖励智能体的推理过程，将其公式化为部分可观测马尔可夫决策过程（POMDP）。关键方法包括：将推理分解为状态估计和转换建模；调查内部信念表示（自然语言与结构化格式）；设计“世界建模奖励”以提供密集的、回合级的准确状态预测监督；引入“Bi-Level General Advantage Estimation (Bi-Level GAE)”进行回合感知的信用分配。所有实验均在VAGEN框架内进行。", "result": "研究发现分解推理（状态估计和转换建模）对成功至关重要。最佳的内部信念表示是任务依赖的：自然语言擅长捕捉通用任务中的语义关系，而结构化格式对于精确操作和控制不可或缺。一个3B参数的模型在五个多样化智能体基准测试中取得了0.82的分数，比未经训练的对应模型（0.21）提高了3倍，并超越了GPT-5（0.75）、Gemini 2.5 Pro（0.67）和Claude 4.5（0.62）等专有推理模型。", "conclusion": "通过显式视觉状态推理，VLM智能体能够构建有效的内部世界模型。分解推理过程、选择合适的信念表示以及结合新颖的奖励和信用分配机制，对于VLM智能体在复杂视觉环境中实现卓越性能至关重要。"}}
{"id": "2510.17179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17179", "abs": "https://arxiv.org/abs/2510.17179", "authors": ["Yingzi Han", "Jiakai He", "Chuanlong Xie", "Jianping Li"], "title": "Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring", "comment": null, "summary": "Automated plankton recognition models face significant challenges during\nreal-world deployment due to distribution shifts (Out-of-Distribution, OoD)\nbetween training and test data. This stems from plankton's complex\nmorphologies, vast species diversity, and the continuous discovery of novel\nspecies, which leads to unpredictable errors during inference. Despite rapid\nadvancements in OoD detection methods in recent years, the field of plankton\nrecognition still lacks a systematic integration of the latest computer vision\ndevelopments and a unified benchmark for large-scale evaluation. To address\nthis, this paper meticulously designed a series of OoD benchmarks simulating\nvarious distribution shift scenarios based on the DYB-PlanktonNet dataset\n\\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection\nmethods. Extensive experimental results demonstrate that the ViM\n\\cite{wang2022vim} method significantly outperforms other approaches in our\nconstructed benchmarks, particularly excelling in Far-OoD scenarios with\nsubstantial improvements in key metrics. This comprehensive evaluation not only\nprovides a reliable reference for algorithm selection in automated plankton\nrecognition but also lays a solid foundation for future research in plankton\nOoD detection. To our knowledge, this study marks the first large-scale,\nsystematic evaluation and analysis of Out-of-Distribution data detection\nmethods in plankton recognition. Code is available at\nhttps://github.com/BlackJack0083/PlanktonOoD.", "AI": {"tldr": "本研究针对浮游生物识别中存在的分布偏移（OoD）问题，首次大规模系统评估了22种OoD检测方法，并构建了模拟不同分布偏移场景的基准，发现ViM方法表现最佳。", "motivation": "浮游生物识别模型在实际部署中面临严重的分布偏移挑战，原因在于其形态复杂、物种多样且不断有新物种被发现，导致推理时出现不可预测的错误。尽管OoD检测方法近年来发展迅速，但浮游生物识别领域仍缺乏系统整合最新计算机视觉进展和统一的大规模评估基准。", "method": "本研究基于DYB-PlanktonNet数据集，精心设计了一系列模拟各种分布偏移场景的OoD基准。在此基础上，系统评估了二十二种OoD检测方法。", "result": "广泛的实验结果表明，ViM方法在我们构建的基准测试中显著优于其他方法，尤其在远距离OoD（Far-OoD）场景中，关键指标有显著提升。", "conclusion": "这项全面的评估不仅为自动化浮游生物识别中的算法选择提供了可靠参考，也为未来浮游生物OoD检测研究奠定了坚实基础。据我们所知，这是浮游生物识别领域首次对分布外数据检测方法进行大规模、系统性的评估和分析。"}}
{"id": "2510.17171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17171", "abs": "https://arxiv.org/abs/2510.17171", "authors": ["Feihong Yan", "Peiru Wang", "Yao Zhu", "Kaiyu Pang", "Qingyan Wei", "Huiqi Li", "Linfeng Zhang"], "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling", "comment": "12 pages, 6 figures", "summary": "Masked Autoregressive (MAR) models promise better efficiency in visual\ngeneration than autoregressive (AR) models for the ability of parallel\ngeneration, yet their acceleration potential remains constrained by the\nmodeling complexity of spatially correlated visual tokens in a single step. To\naddress this limitation, we introduce Generation then Reconstruction (GtR), a\ntraining-free hierarchical sampling strategy that decomposes generation into\ntwo stages: structure generation establishing global semantic scaffolding,\nfollowed by detail reconstruction efficiently completing remaining tokens.\nAssuming that it is more difficult to create an image from scratch than to\ncomplement images based on a basic image framework, GtR is designed to achieve\nacceleration by computing the reconstruction stage quickly while maintaining\nthe generation quality by computing the generation stage slowly. Moreover,\nobserving that tokens on the details of an image often carry more semantic\ninformation than tokens in the salient regions, we further propose\nFrequency-Weighted Token Selection (FTS) to offer more computation budget to\ntokens on image details, which are localized based on the energy of high\nfrequency information. Extensive experiments on ImageNet class-conditional and\ntext-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining\ncomparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),\nsubstantially outperforming existing acceleration methods across various model\nscales and generation tasks. Our codes will be released in\nhttps://github.com/feihongyan1/GtR.", "AI": {"tldr": "本文提出了一种名为GtR（生成后重建）的无训练分层采样策略，用于加速掩码自回归（MAR）模型的图像生成。GtR将生成过程分解为结构生成和细节重建两个阶段，并通过频率加权令牌选择（FTS）进一步优化。该方法在保持生成质量的同时，实现了显著的加速。", "motivation": "掩码自回归（MAR）模型在视觉生成中具有并行生成的潜力，但其加速能力受限于单步中空间相关视觉令牌建模的复杂性。研究旨在解决这一效率瓶颈。", "method": "本文引入了两种方法：\n1. **GtR（生成后重建）**：一种无训练的分层采样策略，将生成分解为两个阶段：首先是缓慢的结构生成（建立全局语义骨架），然后是快速的细节重建（高效完成剩余令牌）。\n2. **FTS（频率加权令牌选择）**：观察到图像细节上的令牌通常比显著区域的令牌携带更多语义信息，FTS通过高频信息能量定位细节，并为其分配更多计算预算。", "result": "在ImageNet类别条件和文本到图像生成任务中，该方法在MAR-H模型上实现了3.72倍的加速，同时保持了与原始模型相当的生成质量（例如，FID: 1.59, IS: 304.4 对比原始的 1.59, 299.1）。它显著优于现有加速方法，适用于各种模型规模和生成任务。", "conclusion": "GtR结合FTS通过将生成分解为结构和细节阶段，并优先处理细节令牌，有效地解决了MAR模型在处理空间相关视觉令牌时的效率问题。该方法在大幅提升生成速度的同时，保持了高质量的图像生成效果。"}}
{"id": "2510.17590", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG", "I.2.7; H.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.17590", "abs": "https://arxiv.org/abs/2510.17590", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning", "comment": "16 pages, 3 tables, 1 figure", "summary": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.", "AI": {"tldr": "MIRAGE是一个推理时、可插拔的智能体框架，通过分解多模态验证任务并结合视觉-语言模型推理和网络检索，实现了在缺乏特定领域训练数据的情况下，对多模态虚假信息的高效检测，性能优于零样本基线。", "motivation": "多模态虚假信息通过结合文本和图像在网络平台快速传播，人工事实核查能力不堪重负。现有的监督检测模型需要特定领域的训练数据，并且难以泛化到多样化的操纵策略。", "method": "MIRAGE将多模态验证分解为四个顺序模块：视觉真实性评估（检测AI生成图像）、跨模态一致性分析（识别上下文不符的重用）、检索增强的事实核查（通过迭代问题生成将声明与网络证据关联）、以及校准判断模块（整合所有信号）。它通过协调视觉-语言模型推理与有针对性的网络检索来运作，并输出结构化、带有引用的理由。", "result": "在MMFakeBench验证集（1,000样本）上，使用GPT-4o-mini的MIRAGE达到了81.65%的F1分数和75.1%的准确率，比最强的零样本基线（使用MMD-Agent的GPT-4V，F1分数为74.0%）高出7.65个百分点，同时将误报率保持在34.3%（而仅判断基线为97.3%）。在测试集（5,000样本）上，泛化能力得到证实，F1分数为81.44%，准确率为75.08%。消融研究表明，视觉验证贡献了5.18个F1点，检索增强推理贡献了2.97个F1点。", "conclusion": "研究结果表明，结合网络检索的分解式智能体推理可以在没有特定领域训练的情况下，达到与监督检测器相当的性能，从而在标记数据稀缺的多模态领域实现虚假信息检测。"}}
{"id": "2510.17598", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17598", "abs": "https://arxiv.org/abs/2510.17598", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation", "comment": null, "summary": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.", "AI": {"tldr": "本文提出一种将超大型语言模型（VLLM）的推理能力蒸馏到小型模型的方法，通过结构感知损失优化，显著提升了小型模型在代码生成任务上的表现。", "motivation": "有效的代码生成不仅需要准确的token预测，更需要理解解决方案级别的结构关系和算法推理能力。VLLM具备这种推理能力，但小型模型缺乏且部署成本更低。因此，目标是让小型模型获得VLLM的推理能力。", "method": "通过一种新颖的结构感知损失优化方法，训练小型模型模仿VLLM的推理和问题解决能力。该方法旨在让模型识别正确的解决方案路径，并在问题定义和潜在解决方案之间建立结构对应关系，使其超越token级别生成，深入理解解决方案的整体结构。", "result": "实验结果表明，通过廉价且易于实现的过程开发出的微调模型，在MBPP、MBPP Plus和HumanEval基准测试中，其pass@1、平均数据流和平均语法匹配指标均显著优于基线模型。", "conclusion": "通过将VLLM的推理能力蒸馏到小型模型中，可以有效提升小型模型的代码生成性能，使其能够深入理解解决方案的整体结构，而非仅仅停留在token级别生成。该方法成本低廉且易于实施。"}}
{"id": "2510.17181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17181", "abs": "https://arxiv.org/abs/2510.17181", "authors": ["Haonan He", "Yufeng Zheng", "Jie Song"], "title": "Capturing Head Avatar with Hand Contacts from a Monocular Video", "comment": "ICCV 2025", "summary": "Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.\nHowever, most methods focus solely on facial regions, ignoring natural\nhand-face interactions, such as a hand resting on the chin or fingers gently\ntouching the cheek, which convey cognitive states like pondering. In this work,\nwe present a novel framework that jointly learns detailed head avatars and the\nnon-rigid deformations induced by hand-face interactions.\n  There are two principal challenges in this task. First, naively tracking hand\nand face separately fails to capture their relative poses. To overcome this, we\npropose to combine depth order loss with contact regularization during pose\ntracking, ensuring correct spatial relationships between the face and hand.\nSecond, no publicly available priors exist for hand-induced deformations,\nmaking them non-trivial to learn from monocular videos. To address this, we\nlearn a PCA basis specific to hand-induced facial deformations from a face-hand\ninteraction dataset. This reduces the problem to estimating a compact set of\nPCA parameters rather than a full spatial deformation field. Furthermore,\ninspired by physics-based simulation, we incorporate a contact loss that\nprovides additional supervision, significantly reducing interpenetration\nartifacts and enhancing the physical plausibility of the results.\n  We evaluate our approach on RGB(D) videos captured by an iPhone.\nAdditionally, to better evaluate the reconstructed geometry, we construct a\nsynthetic dataset of avatars with various types of hand interactions. We show\nthat our method can capture better appearance and more accurate deforming\ngeometry of the face than SOTA surface reconstruction methods.", "AI": {"tldr": "本文提出了一种新颖的框架，能够联合学习逼真的3D头部替身及其由手部与面部交互引起的非刚性变形，解决了现有方法忽略手脸交互的问题。", "motivation": "现有方法主要关注面部区域，忽略了手部与面部的自然交互（如手托下巴、手指轻触脸颊），这些交互对于表达认知状态至关重要。因此，需要一个能同时捕捉头部替身和手部引发变形的方法。", "method": "该方法联合学习详细的头部替身和手部与面部交互引起的非刚性变形。为解决手部和面部相对姿态跟踪的挑战，引入了深度顺序损失和接触正则化。为解决手部诱导变形缺乏先验知识的挑战，从一个手脸交互数据集中学习了特定于手部诱导面部变形的PCA基，并结合了接触损失以提高物理真实性。", "result": "该方法在iPhone捕获的RGB(D)视频和合成数据集上进行了评估。结果表明，与现有最先进的表面重建方法相比，本文方法能够捕捉到更好的外观和更精确的面部变形几何。", "conclusion": "本文提出的框架成功地解决了3D头部替身中手部与面部交互的捕捉问题，通过结合创新的姿态跟踪和变形学习机制，实现了比现有方法更逼真、更准确的头部替身重建，特别是在处理手部引发的面部变形方面表现优异。"}}
{"id": "2510.17638", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17638", "abs": "https://arxiv.org/abs/2510.17638", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena", "comment": "https://www.prophetarena.co/", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.", "AI": {"tldr": "本文系统研究了大型语言模型（LLMs）的预测能力，发现它们在预测实时事件方面表现出令人印象深刻的潜力，但也存在事件回忆不准确、数据源理解不足和信息聚合速度慢等关键瓶颈。", "motivation": "预测是重要的智力追求，对金融和经济等社会系统具有重要意义。随着大型语言模型（LLMs）的快速发展，利用LLMs预测现实世界未来事件成为可能，这是一种新兴的范式，即“LLM即先知”。", "method": "为了系统性地研究LLMs的预测智能，本文构建了一个名为“Prophet Arena”的通用评估基准。该基准持续收集实时预测任务，并将每个任务分解为不同的管道阶段，以支持受控和大规模的实验。", "result": "综合评估显示，许多LLMs已展现出令人印象深刻的预测能力，例如校准误差小、预测置信度一致以及有望获得市场回报。然而，研究也揭示了通过“LLM即先知”实现卓越预测智能的关键瓶颈，包括LLMs事件回忆不准确、对数据源的误解以及在事件临近时信息聚合速度慢于市场。", "conclusion": "LLMs在预测方面展现出巨大潜力，但要实现卓越的预测智能，需要解决其在事件回忆、数据源理解和信息聚合速度等方面的现有瓶颈。"}}
{"id": "2510.17188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17188", "abs": "https://arxiv.org/abs/2510.17188", "authors": ["Vaibhav Rathore", "Divyam Gupta", "Biplab Banerjee"], "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery", "comment": "Accpeted at NeurIPS (2025) Main Conference", "summary": "Generalized Category Discovery (GCD) aims to classify test-time samples into\neither seen categories** -- available during training -- or novel ones, without\nrelying on label supervision. Most existing GCD methods assume simultaneous\naccess to labeled and unlabeled data during training and arising from the same\ndomain, limiting applicability in open-world scenarios involving distribution\nshifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by\nrequiring models to generalize to unseen domains containing novel categories,\nwithout accessing targetdomain data during training. The only prior DG-GCD\nmethod, DG2CD-Net, relies on episodic training with multiple synthetic domains\nand task vector aggregation, incurring high computational cost and error\naccumulation. We propose HIDISC, a hyperbolic representation learning framework\nthat achieves domain and category-level generalization without episodic\nsimulation. To expose the model to minimal but diverse domain variations, we\naugment the source domain using GPT-guided diffusion, avoiding overfitting\nwhile maintaining efficiency. To structure the representation space, we\nintroduce Tangent CutMix, a curvature-aware interpolation that synthesizes\npseudo-novel samples in tangent space, preserving manifold consistency. A\nunified loss -- combining penalized Busemann alignment, hybrid hyperbolic\ncontrastive regularization, and adaptive outlier repulsion -- **facilitates\ncompact, semantically structured embeddings. A learnable curvature parameter\nfurther adapts the geometry to dataset complexity. HIDISC achieves\nstate-of-the-art results on PACS , Office-Home , and DomainNet, consistently\noutperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.", "AI": {"tldr": "本文提出HIDISC，一个基于双曲表示学习的框架，用于解决领域泛化与广义类别发现（DG-GCD）问题。它通过GPT引导的扩散增强和曲率感知插值，以及统一的损失函数，实现了在未见领域和新类别上的最先进性能，同时避免了现有方法的计算成本和误差积累。", "motivation": "现有广义类别发现（GCD）方法假设训练时可同时访问有标签和无标签数据，且数据来自同一领域，这限制了其在开放世界场景（涉及分布偏移）中的适用性。DG-GCD旨在解决这一限制，要求模型泛化到包含新类别的未见领域，且训练时不访问目标领域数据。然而，唯一的现有DG-GCD方法DG2CD-Net计算成本高昂且易于积累误差。", "method": "本文提出了HIDISC，一个无需情景模拟的双曲表示学习框架。它通过以下方法实现领域和类别泛化：1) 使用GPT引导的扩散增强源领域，以暴露模型于最小但多样的领域变体；2) 引入Tangent CutMix，一种曲率感知插值方法，用于在切空间中合成伪新样本，保持流形一致性；3) 采用统一的损失函数，结合惩罚性Busemann对齐、混合双曲对比正则化和自适应异常值排斥，以促进紧凑、语义结构化的嵌入；4) 引入可学习的曲率参数以适应数据集复杂性。", "result": "HIDISC在PACS、Office-Home和DomainNet数据集上取得了最先进的结果，持续优于现有的欧几里得和双曲(DG)-GCD基线方法。", "conclusion": "HIDISC通过其独特的双曲表示学习框架、创新的数据增强和损失函数设计，成功解决了DG-GCD中的泛化和效率挑战，并在多个基准测试中展现出卓越的性能，为开放世界场景下的类别发现提供了更有效的解决方案。"}}
{"id": "2510.17197", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17197", "abs": "https://arxiv.org/abs/2510.17197", "authors": ["Pu Zhang", "Yuwei Li", "Xingyuan Xian", "Guoming Tang"], "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models", "comment": null, "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can\nprocess increasingly large inputs, which, unlike in LLMs, generates significant\nvisual token redundancy and leads to prohibitive inference costs. While many\nmethods aim to reduce these costs by pruning visual tokens, existing\napproaches, whether based on attention or diversity, typically neglect the\nguidance of the text prompt and thus fail to prioritize task relevance. In this\nwork, we propose a novel, zero-shot method that reframes the problem by\nintroducing a prompt-aware perspective, explicitly modeling visual token\npruning as a balance between task relevance and information diversity. Our\nhierarchical approach first selects a core set of task-relevant visual tokens\nand then supplements them with diversity tokens to preserve broader context.\nExperiments across multiple models and benchmarks show that our method achieves\nperformance that matches or surpasses the state-of-the-art with only minimal\naccuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these\ngains are accompanied by significant reductions in GPU memory footprint and\ninference latency.", "AI": {"tldr": "本文提出了一种新颖的、零样本的、提示词感知的视觉令牌剪枝方法，通过平衡任务相关性和信息多样性，显著降低了视觉语言模型的推理成本，同时保持了高准确性。", "motivation": "随着视觉语言模型（VLMs）处理能力的增强，它们能够处理越来越大的输入，这导致了大量的视觉令牌冗余和高昂的推理成本。现有剪枝方法（基于注意力或多样性）通常忽略文本提示词的指导，未能优先考虑任务相关性。", "method": "本文提出了一种零样本、提示词感知的分层方法。它将视觉令牌剪枝重新定义为任务相关性和信息多样性之间的平衡问题。首先选择一组核心的任务相关视觉令牌，然后补充多样性令牌以保留更广泛的上下文信息。", "result": "该方法在多个模型和基准测试中，即使剪枝高达90%的令牌，也能达到或超越现有最佳性能，且准确性损失极小。同时，显著降低了GPU内存占用和推理延迟。", "conclusion": "该研究提供了一种高效的、提示词感知的视觉令牌剪枝策略，有效解决了VLM中的视觉冗余问题，在保持性能的同时显著提升了模型的推理效率和资源利用率。"}}
{"id": "2510.17790", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17790", "abs": "https://arxiv.org/abs/2510.17790", "authors": ["Yuhao Yang", "Zhen Yang", "Zi-Yi Dou", "Anh Nguyen", "Keen You", "Omar Attia", "Andrew Szot", "Michael Feng", "Ram Ramrakhya", "Alexander Toshev", "Chao Huang", "Yinfei Yang", "Zhe Gan"], "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action", "comment": null, "summary": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.", "AI": {"tldr": "UltraCUA是一个基础模型，通过混合动作（结合GUI原始操作和高层编程工具调用）提升了计算机使用代理的性能和效率，解决了现有代理仅依赖原始GUI动作的局限性。", "motivation": "现有计算机使用代理（CUAs）仅依赖原始GUI动作（点击、输入、滚动），这需要精确的视觉定位和冗长的执行链，导致级联故障和性能瓶颈。与利用丰富编程接口（API、工具）的其他代理不同，CUAs未能利用这些能力。", "method": "UltraCUA通过混合动作（无缝整合GUI原始操作和高层编程工具调用）来弥补这一差距。其方法包括四个关键组成部分：1) 一个从软件文档、开源仓库和代码生成中扩展编程工具的自动化流程；2) 一个生成超过17,000个可验证任务的合成数据引擎；3) 一个包含低级GUI动作和高级编程工具调用的大规模高质量混合动作轨迹数据集；4) 一个结合监督微调和在线强化学习的两阶段训练流程，以实现低级和高级动作之间的策略性切换。", "result": "UltraCUA的7B和32B模型在实验中表现出比现有最先进代理的显著改进。在OSWorld上，UltraCUA模型比基础模型平均相对提升22%，且在步骤上快11%。在WindowsAgentArena上的域外评估显示，模型达到21.7%的成功率，优于在Windows数据上训练的基线模型。混合动作机制被证明至关重要，它在保持执行效率的同时减少了错误传播。", "conclusion": "UltraCUA通过其混合动作机制成功弥合了计算机使用代理的空白，将GUI原始操作与高层编程工具调用无缝集成，从而显著提高了性能、效率并减少了错误传播。"}}
{"id": "2510.17199", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17199", "abs": "https://arxiv.org/abs/2510.17199", "authors": ["Nirai Hayakawa", "Kazumasa Shimari", "Kazuma Yamasaki", "Hirotatsu Hoshikawa", "Rikuto Tsuchida", "Kenichi Matsumoto"], "title": "Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis", "comment": "Accepted to IEEE 2025 Conference on Games", "summary": "Recently, research on predicting match outcomes in esports has been actively\nconducted, but much of it is based on match log data and statistical\ninformation. This research targets the FPS game VALORANT, which requires\ncomplex strategies, and aims to build a round outcome prediction model by\nanalyzing minimap information in match footage. Specifically, based on the\nvideo recognition model TimeSformer, we attempt to improve prediction accuracy\nby incorporating detailed tactical features extracted from minimap information,\nsuch as character position information and other in-game events. This paper\nreports preliminary results showing that a model trained on a dataset augmented\nwith such tactical event labels achieved approximately 81% prediction accuracy,\nespecially from the middle phases of a round onward, significantly\noutperforming a model trained on a dataset with the minimap information itself.\nThis suggests that leveraging tactical features from match footage is highly\neffective for predicting round outcomes in VALORANT.", "AI": {"tldr": "本研究针对FPS游戏VALORANT，通过分析比赛录像中的小地图信息，并融入详细的战术特征（如角色位置和游戏内事件），构建了一个回合结果预测模型，初步结果显示其预测准确率达到81%，显著优于仅使用小地图信息的模型。", "motivation": "当前电竞比赛结果预测多基于比赛日志和统计数据，但对于VALORANT这类需要复杂策略的FPS游戏，需要更深入的分析。本研究旨在通过分析比赛录像中的视觉信息（小地图）来提高预测能力。", "method": "研究基于视频识别模型TimeSformer，通过从小地图信息中提取详细的战术特征（如角色位置信息和游戏内事件），并用这些战术事件标签增强数据集，从而构建并训练回合结果预测模型。", "result": "初步结果显示，使用战术事件标签增强数据集训练的模型达到了大约81%的预测准确率，尤其是在回合中后期表现突出。该模型显著优于仅使用小地图信息本身进行训练的模型。", "conclusion": "研究表明，利用比赛录像中的战术特征对于预测VALORANT的回合结果非常有效。"}}
{"id": "2510.17198", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17198", "abs": "https://arxiv.org/abs/2510.17198", "authors": ["M Saifuzzaman Rafat", "Mohd Ruhul Ameen", "Akif Islam", "Abu Saleh Musa Miah", "Jungpil Shin"], "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh", "comment": "Submitted to the International Conference on Data and Applied\n  Analytics (IDAA 2025). 15 pages, 5 figures, 4 tables", "summary": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also\nagents of relentless destruction. Each year, they swallow whole villages and\nvast tracts of farmland, erasing communities from the map and displacing\nthousands of families. To track this slow-motion catastrophe has, until now,\nbeen a Herculean task for human analysts. Here we show how a powerful\ngeneral-purpose vision model, the Segment Anything Model (SAM), can be adapted\nto this task with remarkable precision. To do this, we assembled a new dataset\n- a digital chronicle of loss compiled from historical Google Earth imagery of\nBangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur\nUnion, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,\nthis dataset is the first to include manually annotated data on the settlements\nthat have vanished beneath the water. Our method first uses a simple\ncolor-channel analysis to provide a rough segmentation of land and water, and\nthen fine-tunes SAM's mask decoder to recognize the subtle signatures of\nriverbank erosion. The resulting model demonstrates a keen eye for this\ndestructive process, achieving a mean Intersection over Union of 86.30% and a\nDice score of 92.60% - a performance that significantly surpasses traditional\nmethods and off-the-shelf deep learning models. This work delivers three key\ncontributions: the first annotated dataset of disappeared settlements in\nBangladesh due to river erosion; a specialized AI model fine-tuned for this\ncritical task; and a method for quantifying land loss with compelling visual\nevidence. Together, these tools provide a powerful new lens through which\npolicymakers and disaster management agencies can monitor erosion, anticipate\nits trajectory, and ultimately protect the vulnerable communities in its path.", "AI": {"tldr": "本研究利用新数据集和微调的Segment Anything Model (SAM) 模型，实现了对孟加拉国河流侵蚀导致村庄和农田消失的精确监测和量化，为灾害管理提供了强大工具。", "motivation": "孟加拉国的大河每年都会吞噬村庄和农田，导致社区消失和数千家庭流离失所。传统上，追踪这种缓慢的灾难对人类分析师来说是一项艰巨的任务，因此需要更有效的方法来监测和量化土地流失。", "method": "研究首先构建了一个新的数据集，包含2003年至2025年孟加拉国脆弱地区的历史Google Earth图像，并首次手动标注了因河流侵蚀而消失的定居点。然后，该方法使用简单的颜色通道分析进行土地和水域的粗略分割，接着对Segment Anything Model (SAM) 的掩码解码器进行微调，以识别河岸侵蚀的细微特征。", "result": "所得到的模型在识别破坏性侵蚀过程方面表现出色，平均交并比（IoU）达到86.30%，Dice系数达到92.60%，显著优于传统方法和现成的深度学习模型。研究的三个主要贡献包括：第一个因河流侵蚀而消失的孟加拉国定居点标注数据集；一个为此关键任务微调的专业AI模型；以及一种通过引人注目的视觉证据量化土地流失的方法。", "conclusion": "这些工具为政策制定者和灾害管理机构提供了一个强大的新视角，可以监测侵蚀、预测其轨迹，并最终保护受其影响的脆弱社区。"}}
{"id": "2510.17705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17705", "abs": "https://arxiv.org/abs/2510.17705", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models", "comment": "Accepted by CIKM' 25", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.", "AI": {"tldr": "本文提出HyCAM框架，通过动态上下文注意力调制（CAM）机制，解决大型语言模型在多任务适应中知识保留与任务特异性平衡的难题，有效克服灾难性遗忘并提升性能。", "motivation": "大型语言模型（LLMs）虽具备强大的泛化能力，但在多任务适应中难以平衡知识保留与任务特异性。传统微调方法存在灾难性遗忘和资源消耗大的问题，而现有参数高效方法在复杂多任务场景中表现不佳。", "method": "本文提出上下文注意力调制（CAM）机制，动态调节LLM自注意力模块的表示，以增强任务特定特征并保留通用知识。CAM被整合到混合上下文注意力调制（HyCAM）框架中，该框架结合了一个共享的全参数CAM模块和多个专门的轻量级CAM模块，并通过动态路由策略实现自适应知识融合。", "result": "在问答、代码生成和逻辑推理等异构任务上进行广泛实验，结果表明该方法显著优于现有方法，平均性能提升3.65%。", "conclusion": "HyCAM通过动态调制自注意力表示和创新的混合模块架构，有效解决了LLMs在多任务适应中的挑战，实现了更高效、更出色的适应能力。"}}
{"id": "2510.17201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17201", "abs": "https://arxiv.org/abs/2510.17201", "authors": ["Mika Feng", "Pierre Gallin-Martel", "Koichi Ito", "Takafumi Aoki"], "title": "Optimizing DINOv2 with Registers for Face Anti-Spoofing", "comment": "ICCV 2025 Workshop FAS", "summary": "Face recognition systems are designed to be robust against variations in head\npose, illumination, and image blur during capture. However, malicious actors\ncan exploit these systems by presenting a face photo of a registered user,\npotentially bypassing the authentication process. Such spoofing attacks must be\ndetected prior to face recognition. In this paper, we propose a DINOv2-based\nspoofing attack detection method to discern minute differences between live and\nspoofed face images. Specifically, we employ DINOv2 with registers to extract\ngeneralizable features and to suppress perturbations in the attention\nmechanism, which enables focused attention on essential and minute features. We\ndemonstrate the effectiveness of the proposed method through experiments\nconducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:\nUnified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.", "AI": {"tldr": "本文提出了一种基于DINOv2（带寄存器）的面部反欺诈检测方法，旨在区分真实人脸和欺诈图像中的细微差异。", "motivation": "面部识别系统容易受到欺诈攻击（如使用人脸照片），这可能绕过身份验证过程。因此，在进行面部识别之前，必须检测出此类欺诈攻击。", "method": "该方法利用DINOv2模型，并结合寄存器（registers）来提取可泛化的特征，并抑制注意力机制中的扰动。这使得模型能够专注于关键且细微的特征，从而区分真实和欺诈人脸图像。", "result": "通过在“The 6th Face Anti-Spoofing Workshop: Unified Physical-Digital Attacks Detection@ICCV2025”数据集和SiW数据集上进行的实验，证明了所提出方法的有效性。", "conclusion": "基于DINOv2（带寄存器）的方法能够有效检测面部欺诈攻击，通过专注于细微特征来区分真实和欺诈人脸图像。"}}
{"id": "2510.17800", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17800", "abs": "https://arxiv.org/abs/2510.17800", "authors": ["Jiale Cheng", "Yusen Liu", "Xinyu Zhang", "Yulin Fei", "Wenyi Hong", "Ruiliang Lyu", "Weihan Wang", "Zhe Su", "Xiaotao Gu", "Xiao Liu", "Yushi Bai", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "Glyph: Scaling Context Windows via Visual-Text Compression", "comment": null, "summary": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.", "AI": {"tldr": "Glyph框架通过将长文本渲染成图像并由视觉语言模型（VLM）处理，实现了3-4倍的文本压缩，同时保持准确性，显著提高了推理和训练速度，并使128K上下文的VLM能够处理百万级token的文本任务。", "motivation": "大型语言模型（LLMs）越来越依赖长上下文建模，但将上下文窗口扩展到百万级token会带来高昂的计算和内存成本，限制了其实用性。", "method": "该研究提出Glyph框架，通过将长文本渲染成图像，并使用视觉语言模型（VLMs）进行处理。此外，还设计了一种LLM驱动的遗传搜索算法来寻找最佳的视觉渲染配置，以平衡准确性和压缩率。", "result": "该方法实现了3-4倍的token压缩，同时在各种长上下文基准测试上保持了与Qwen3-8B等领先LLMs相当的准确性。这种压缩还带来了约4倍的预填充和解码速度提升，以及约2倍的SFT训练速度提升。在极端压缩下，一个128K上下文的VLM能够处理1M token级别的文本任务。此外，渲染后的文本数据还有益于文档理解等多模态任务。", "conclusion": "Glyph框架通过视觉上下文扩展，有效解决了长上下文LLMs的计算和内存限制问题，显著提高了效率和可扩展性，并为多模态任务带来了额外优势。"}}
{"id": "2510.17200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17200", "abs": "https://arxiv.org/abs/2510.17200", "authors": ["Bingrong Liu", "Jun Shi", "Yushan Zheng"], "title": "EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification", "comment": null, "summary": "Class-incremental learning (CIL) for endoscopic image analysis is crucial for\nreal-world clinical applications, where diagnostic models should continuously\nadapt to evolving clinical data while retaining performance on previously\nlearned ones. However, existing replay-based CIL methods fail to effectively\nmitigate catastrophic forgetting due to severe domain discrepancies and class\nimbalance inherent in endoscopic imaging. To tackle these challenges, we\npropose EndoCIL, a novel and unified CIL framework specifically tailored for\nendoscopic image diagnosis. EndoCIL incorporates three key components: Maximum\nMean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy\nstrategy to select diverse and representative exemplars, Prior Regularized\nClass Balanced Loss (PRCBL), designed to alleviate both inter-phase and\nintra-phase class imbalance by integrating prior class distributions and\nbalance weights into the loss function, and Calibration of Fully-Connected\nGradients (CFG), which adjusts the classifier gradients to mitigate bias toward\nnew classes. Extensive experiments conducted on four public endoscopic datasets\ndemonstrate that EndoCIL generally outperforms state-of-the-art CIL methods\nacross varying buffer sizes and evaluation metrics. The proposed framework\neffectively balances stability and plasticity in lifelong endoscopic diagnosis,\nshowing promising potential for clinical scalability and deployment.", "AI": {"tldr": "EndoCIL是一种新颖的统一增量学习（CIL）框架，专为内窥镜图像诊断设计，通过改进回放策略、平衡损失函数和校准梯度来有效解决灾难性遗忘和类别不平衡问题。", "motivation": "在真实临床应用中，诊断模型需要持续适应不断演变的临床数据，同时保持对已学习数据的性能。然而，现有基于回放的CIL方法由于内窥镜图像固有的严重域差异和类别不平衡，未能有效缓解灾难性遗忘。", "method": "本文提出了EndoCIL框架，包含三个关键组件：1. 基于最大均值差异的回放（MDBR），采用分布对齐的贪婪策略选择多样且具代表性的样本；2. 先验正则化类别平衡损失（PRCBL），通过将先验类别分布和平衡权重整合到损失函数中，缓解阶段内和阶段间的类别不平衡；3. 全连接梯度校准（CFG），调整分类器梯度以减轻对新类别的偏置。", "result": "在四个公开内窥镜数据集上进行的广泛实验表明，EndoCIL在不同缓冲区大小和评估指标下，普遍优于最先进的CIL方法。", "conclusion": "所提出的EndoCIL框架有效地平衡了终身内窥镜诊断中的稳定性与可塑性，显示出在临床可扩展性和部署方面的巨大潜力。"}}
{"id": "2510.17264", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17264", "abs": "https://arxiv.org/abs/2510.17264", "authors": ["Akihito Yoshii", "Ryosuke Sonoda", "Ramya Srinivasan"], "title": "Fair and Interpretable Deepfake Detection in Videos", "comment": "10 pages (including References)", "summary": "Existing deepfake detection methods often exhibit bias, lack transparency,\nand fail to capture temporal information, leading to biased decisions and\nunreliable results across different demographic groups. In this paper, we\npropose a fairness-aware deepfake detection framework that integrates temporal\nfeature learning and demographic-aware data augmentation to enhance fairness\nand interpretability. Our method leverages sequence-based clustering for\ntemporal modeling of deepfake videos and concept extraction to improve\ndetection reliability while also facilitating interpretable decisions for\nnon-expert users. Additionally, we introduce a demography-aware data\naugmentation method that balances underrepresented groups and applies\nfrequency-domain transformations to preserve deepfake artifacts, thereby\nmitigating bias and improving generalization. Extensive experiments on\nFaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)\narchitectures (Xception, ResNet) demonstrate the efficacy of the proposed\nmethod in obtaining the best tradeoff between fairness and accuracy when\ncompared to SoTA.", "AI": {"tldr": "本文提出了一种兼顾公平性和可解释性的深度伪造检测框架，通过集成时间特征学习和人口统计学感知数据增强，以解决现有方法存在的偏见、缺乏透明度和时间信息捕获不足的问题。", "motivation": "现有深度伪造检测方法存在偏见、缺乏透明度，且未能有效捕获时间信息，导致在不同人口群体中决策有偏且结果不可靠。", "method": "该方法通过序列聚类进行深度伪造视频的时间建模，并利用概念提取提高检测可靠性及非专业用户的可解释性。此外，引入了人口统计学感知数据增强方法，以平衡代表性不足的群体，并应用频域变换来保留深度伪造伪影，从而减轻偏见并提高泛化能力。", "result": "在FaceForensics++、DFD、Celeb-DF和DFDC数据集上，使用Xception和ResNet等先进架构进行的大量实验表明，与现有最先进方法相比，所提出的方法在公平性和准确性之间取得了最佳权衡。", "conclusion": "该框架通过结合时间特征学习和人口统计学感知数据增强，有效提升了深度伪造检测的公平性和可解释性，同时保持了高准确性。"}}
{"id": "2510.17269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17269", "abs": "https://arxiv.org/abs/2510.17269", "authors": ["Luis Wiedmann", "Orr Zohar", "Amir Mahla", "Xiaohan Wang", "Rui Li", "Thibaud Frere", "Leandro von Werra", "Aritra Roy Gosthipaty", "Andrés Marafioti"], "title": "FineVision: Open Data Is All You Need", "comment": null, "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduce\nFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated, human-in-the-loop pipeline: automation\nperforms bulk ingestion and schema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorous de-duplication within and across\nsources and decontamination against 66 public benchmarks. FineVision also\nencompasses agentic/GUI tasks with a unified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained on FineVision consistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.", "AI": {"tldr": "本文介绍了FineVision，这是一个精心收集、整理和统一的2400万样本视觉-语言模型（VLM）语料库，旨在解决现有公共数据集碎片化、不一致和受污染的问题，并通过严格的数据清洗和人工审核，显著提升了模型的性能。", "motivation": "视觉-语言模型（VLM）的进步受到公共数据集碎片化、不一致和受污染的阻碍。", "method": "引入FineVision，一个包含2400万样本的语料库，通过半自动化、人工参与的流程将200多个来源统一到185个子集中。该流程包括自动化批量摄取和模式映射，人工审核映射和抽查输出以验证注释的忠实性、格式和多样性，以及安全性。工作流还进行严格的内部和跨源去重，并针对66个公共基准进行去污染。FineVision还包含具有统一动作空间的代理/GUI任务，并通过人工验证模式和检查轨迹样本来确认可执行的保真度。", "result": "在FineVision上训练的模型在广泛的评估套件中持续优于在现有开放混合数据集上训练的模型，这突显了数据规模、数据卫生以及自动化与人工监督平衡的好处。", "conclusion": "FineVision的规模、数据卫生以及自动化与人工监督的平衡显著促进了VLM研究。作者发布了该语料库和整理工具，以加速以数据为中心的VLM研究。"}}
{"id": "2510.17218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17218", "abs": "https://arxiv.org/abs/2510.17218", "authors": ["Zhuo Cao", "Heming Du", "Bingqing Zhang", "Xin Yu", "Xue Li", "Sen Wang"], "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions", "comment": "Accepted to NeurIPS 2025", "summary": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval\n(SMR). However, one query can correspond to multiple relevant moments in\nreal-world applications. This makes the existing datasets and methods\ninsufficient for video temporal grounding. By revisiting the gap between\ncurrent MR tasks and real-world applications, we introduce a high-quality\ndatasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new\nevaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists\nof 2,212 annotations covering 6,384 video segments. Building on existing\nefforts in MMR, we propose a framework called FlashMMR. Specifically, we\npropose a Multi-moment Post-verification module to refine the moment\nboundaries. We introduce constrained temporal adjustment and subsequently\nleverage a verification module to re-evaluate the candidate segments. Through\nthis sophisticated filtering pipeline, low-confidence proposals are pruned, and\nrobust multi-moment alignment is achieved. We retrain and evaluate 6 existing\nMR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.\nResults show that QV-M$^2$ serves as an effective benchmark for training and\nevaluating MMR models, while FlashMMR provides a strong baseline. Specifically,\non QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,\n2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method\nestablish a foundation for advancing research in more realistic and challenging\nvideo temporal grounding scenarios. Code is released at\nhttps://github.com/Zhuo-Cao/QV-M2.", "AI": {"tldr": "该论文针对现有单时刻检索（SMR）无法处理多时刻对应查询的局限性，提出了一个高质量的多时刻检索（MMR）数据集QV-M$^2$和新的评估指标，并开发了FlashMMR框架，显著提升了多时刻视频时序定位的性能。", "motivation": "现有的时刻检索（MR）方法主要关注单时刻检索（SMR），但实际应用中一个查询往往对应多个相关时刻，这使得现有数据集和方法不足以应对视频时序定位的实际需求。", "method": "本文引入了高质量的多时刻数据集QVHighlights Multi-Moment Dataset (QV-M$^2$)，包含2,212个标注和6,384个视频片段，并提出了新的多时刻检索（MMR）评估指标。在此基础上，提出了FlashMMR框架，其核心是多时刻后验证模块（Multi-moment Post-verification module），通过约束时序调整和验证模块来精炼时刻边界，筛选低置信度提议，实现鲁棒的多时刻对齐。同时，在QV-M$^2$和QVHighlights数据集上，以SMR和MMR两种设置重新训练并评估了6种现有MR方法。", "result": "QV-M$^2$被证明是训练和评估MMR模型的有效基准。FlashMMR提供了一个强大的基线，在QV-M$^2$数据集上，其性能相较于之前的SOTA方法在G-mAP上提升了3.00%，在mAP@3+tgt上提升了2.70%，在mR@3上提升了2.56%。", "conclusion": "所提出的基准数据集和方法为推进更真实、更具挑战性的视频时序定位研究奠定了基础。"}}
{"id": "2510.17274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17274", "abs": "https://arxiv.org/abs/2510.17274", "authors": ["Katie Luo", "Jingwei Ji", "Tong He", "Runsheng Xu", "Yichen Xie", "Dragomir Anguelov", "Mingxing Tan"], "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models", "comment": "In proceedings of IROS 2025", "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.", "AI": {"tldr": "Plug-and-Forecast (PnF) 提出一种即插即用方法，利用多模态大语言模型（MLLMs）增强现有运动预测模型，通过自然语言描述和零样本推理能力，提高在复杂场景下的泛化性能。", "motivation": "当前的自动驾驶系统在标准条件下表现可靠，但在成本效益的前提下，泛化到多样化的真实世界场景仍然是一个重大挑战。", "method": "PnF 方法通过设计提示词从 MLLMs 中提取结构化的场景理解，并将这些信息提炼成可学习的嵌入，以增强现有的行为预测模型。该方法利用 MLLMs 的零样本推理能力，无需微调即可实现性能提升。", "result": "在 Waymo Open Motion Dataset 和 nuScenes Dataset 上，PnF 对两个最先进的运动预测模型进行了验证，结果表明在两个基准测试中均实现了显著且一致的运动预测性能提升。", "conclusion": "PnF 通过利用 MLLMs 的零样本推理能力和自然语言理解，有效解决了运动预测模型在复杂场景中泛化能力不足的问题，提供了一种实用且适应性强的解决方案。"}}
{"id": "2510.17287", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17287", "abs": "https://arxiv.org/abs/2510.17287", "authors": ["Amir Gharghabi", "Mahdi Hakiminezhad", "Maryam Shafaei", "Shaghayegh Gharghabi"], "title": "Machine Vision-Based Surgical Lighting System:Design and Implementation", "comment": null, "summary": "Effortless and ergonomically designed surgical lighting is critical for\nprecision and safety during procedures. However, traditional systems often rely\non manual adjustments, leading to surgeon fatigue, neck strain, and\ninconsistent illumination due to drift and shadowing. To address these\nchallenges, we propose a novel surgical lighting system that leverages the\nYOLOv11 object detection algorithm to identify a blue marker placed above the\ntarget surgical site. A high-power LED light source is then directed to the\nidentified location using two servomotors equipped with tilt-pan brackets. The\nYOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated\nimages simulating surgical scenes with the blue spherical marker. By automating\nthe lighting process, this machine vision-based solution reduces physical\nstrain on surgeons, improves consistency in illumination, and supports improved\nsurgical outcomes.", "AI": {"tldr": "本文提出一种基于YOLOv11和伺服电机的自动化手术照明系统，通过识别蓝色标记实现精准光照，减少外科医生疲劳并提高照明一致性。", "motivation": "传统手术照明系统依赖手动调节，导致外科医生疲劳、颈部劳损以及由于漂移和阴影造成的不一致照明，影响手术精度和安全性。", "method": "该系统利用YOLOv11目标检测算法识别放置在目标手术部位上方的蓝色标记。然后，通过配备倾斜-平移支架的两个伺服电机，将高功率LED光源精确地引导至识别出的位置。", "result": "YOLO模型在包含模拟手术场景和蓝色球形标记的验证集上，实现了96.7%的mAP@50。这表明系统能够准确识别手术目标。", "conclusion": "这种基于机器视觉的自动化照明解决方案减轻了外科医生的身体负担，提高了照明的一致性，并有助于改善手术结果。"}}
{"id": "2510.17278", "categories": ["cs.CV", "68T07, 92C55", "I.4.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17278", "abs": "https://arxiv.org/abs/2510.17278", "authors": ["Mehdi Zekriyapanah Gashti", "Mostafa Mohammadpour", "Ghasem Farjamnia"], "title": "SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation", "comment": null, "summary": "Accurate segmentation and classification of white blood cells (WBCs) in\nmicroscopic images are essential for diagnosis and monitoring of many\nhematological disorders, yet remain challenging due to staining variability,\ncomplex backgrounds, and class imbalance. In this paper, we introduce a novel\nSaliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that\ntightly integrates saliency-driven preprocessing with multi-scale deep feature\naggregation to improve both robustness and interpretability for WBC analysis.\nSG-CLDFF first computes saliency priors to highlight candidate WBC regions and\nguide subsequent feature extraction. A lightweight hybrid backbone\n(EfficientSwin-style) produces multi-resolution representations, which are\nfused by a ResNeXt-CC-inspired cross-layer fusion module to preserve\ncomplementary information from shallow and deep layers. The network is trained\nin a multi-task setup with concurrent segmentation and cell-type classification\nheads, using class-aware weighted losses and saliency-alignment regularization\nto mitigate imbalance and suppress background activation. Interpretability is\nenforced through Grad-CAM visualizations and saliency consistency checks,\nallowing model decisions to be inspected at the regional level. We validate the\nframework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting\nconsistent gains in IoU, F1, and classification accuracy compared to strong CNN\nand transformer baselines. An ablation study also demonstrates the individual\ncontributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers\na practical and explainable path toward more reliable automated WBC analysis in\nclinical workflows.", "AI": {"tldr": "本文提出了一种名为SG-CLDFF的显著性引导跨层深度特征融合框架，用于白细胞（WBC）的精确分割和分类，通过结合显著性预处理和多尺度特征融合，提高了鲁棒性和可解释性。", "motivation": "显微图像中白细胞的准确分割和分类对于诊断和监测血液疾病至关重要，但由于染色变异性、复杂背景和类别不平衡等问题，仍面临挑战。", "method": "SG-CLDFF框架首先计算显著性先验以突出WBC区域并指导特征提取。一个轻量级混合骨干网络（EfficientSwin风格）生成多分辨率表示，并通过受ResNeXt-CC启发的跨层融合模块进行融合，以保留浅层和深层的互补信息。该网络采用多任务设置进行训练，同时包含分割和细胞类型分类头，并使用类别感知加权损失和显著性对齐正则化来缓解不平衡并抑制背景激活。通过Grad-CAM可视化和显著性一致性检查来增强可解释性。", "result": "该框架在标准公共基准数据集（BCCD、LISC、ALL-IDB）上进行了验证，与强大的CNN和Transformer基线相比，在IoU、F1和分类准确性方面均报告了持续的提升。消融研究也证明了显著性预处理和跨层融合的独立贡献。", "conclusion": "SG-CLDFF为临床工作流程中更可靠的自动化白细胞分析提供了一条实用且可解释的途径。"}}
{"id": "2510.17299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17299", "abs": "https://arxiv.org/abs/2510.17299", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning", "comment": "Accepted by NeurIPS 2025", "summary": "In this work, we observe a counterintuitive phenomenon in self-supervised\nlearning (SSL): longer training may impair the performance of dense prediction\ntasks (e.g., semantic segmentation). We refer to this phenomenon as\nSelf-supervised Dense Degradation (SDD) and demonstrate its consistent presence\nacross sixteen state-of-the-art SSL methods with various losses, architectures,\nand datasets. When the model performs suboptimally on dense tasks at the end of\ntraining, measuring the performance during training becomes essential. However,\nevaluating dense performance effectively without annotations remains an open\nchallenge. To tackle this issue, we introduce a Dense representation Structure\nEstimator (DSE), composed of a class-relevance measure and an effective\ndimensionality measure. The proposed DSE is both theoretically grounded and\nempirically validated to be closely correlated with the downstream performance.\nBased on this metric, we introduce a straightforward yet effective model\nselection strategy and a DSE-based regularization method. Experiments on\nsixteen SSL methods across four benchmarks confirm that model selection\nimproves mIoU by $3.0\\%$ on average with negligible computational cost.\nAdditionally, DSE regularization consistently mitigates the effects of dense\ndegradation. Code is available at\nhttps://github.com/EldercatSAM/SSL-Degradation.", "AI": {"tldr": "本研究发现自监督学习中，训练时间过长会导致密集预测任务（如语义分割）性能下降，并提出了DSE（密集表征结构估计器）来无标注评估密集性能，进而通过模型选择和正则化策略缓解这一问题。", "motivation": "研究者观察到自监督学习（SSL）中存在一个反直觉现象：更长的训练时间可能会损害密集预测任务的性能（称之为自监督密集退化，SDD）。此外，在训练结束时模型在密集任务上表现不佳时，有效评估训练过程中的密集性能但又无需标注是一个未解决的挑战。", "method": "1. 识别并证明了SDD现象在16种SOTA SSL方法中的普遍存在。2. 引入了DSE（密集表征结构估计器），该估计器由类别相关性度量和有效维度度量组成，用于在无标注情况下评估密集预测性能。3. 基于DSE提出了一个简单有效的模型选择策略。4. 提出了一个基于DSE的正则化方法。", "result": "1. SDD现象在不同损失、架构和数据集的16种SOTA SSL方法中持续存在。2. 所提出的DSE在理论上和经验上都与下游性能密切相关。3. 基于DSE的模型选择策略平均将mIoU提高了3.0%，计算成本可忽略不计。4. DSE正则化方法持续缓解了密集退化的影响。", "conclusion": "自监督学习中训练时间过长会导致密集预测任务的性能退化（SDD）。DSE能够有效评估无标注的密集表征质量，并通过DSE驱动的模型选择和正则化方法，显著缓解了SDD现象，提升了密集预测任务的性能。"}}
{"id": "2510.17330", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17330", "abs": "https://arxiv.org/abs/2510.17330", "authors": ["Gyuhwan Park", "Kihyun Na", "Injung Kim"], "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration", "comment": "11 pages, 6 figures", "summary": "The significance of license plate image restoration goes beyond the\npreprocessing stage of License Plate Recognition (LPR) systems, as it also\nserves various purposes, including increasing evidential value, enhancing the\nclarity of visual interface, and facilitating further utilization of license\nplate images. We propose a novel diffusion-based framework with character-level\nguidance, CharDiff, which effectively restores and recognizes severely degraded\nlicense plate images captured under realistic conditions. CharDiff leverages\nfine-grained character-level priors extracted through external segmentation and\nOptical Character Recognition (OCR) modules tailored for low-quality license\nplate images. For precise and focused guidance, CharDiff incorporates a novel\nCharacter-guided Attention through Region-wise Masking (CHARM) module, which\nensures that each character's guidance is restricted to its own region, thereby\navoiding interference with other regions. In experiments, CharDiff\nsignificantly outperformed the baseline restoration models in both restoration\nquality and recognition accuracy, achieving a 28% relative reduction in CER on\nthe Roboflow-LP dataset, compared to the best-performing baseline model. These\nresults indicate that the structured character-guided conditioning effectively\nenhances the robustness of diffusion-based license plate restoration and\nrecognition in practical deployment scenarios.", "AI": {"tldr": "本文提出了一种名为CharDiff的新型扩散模型框架，通过字符级引导有效恢复和识别严重降级的车牌图像，显著提高了恢复质量和识别准确性。", "motivation": "车牌图像恢复不仅是车牌识别（LPR）系统预处理的关键，还具有增加证据价值、提高视觉界面清晰度和促进车牌图像进一步利用等多种重要用途。", "method": "CharDiff是一个基于扩散的框架，采用字符级引导。它利用外部分割和针对低质量车牌图像定制的光学字符识别（OCR）模块提取细粒度的字符级先验信息。为实现精确聚焦的引导，CharDiff引入了新颖的通过区域掩蔽实现字符引导注意力（CHARM）模块，确保每个字符的引导仅限于其自身区域，避免与其他区域的干扰。", "result": "实验表明，CharDiff在恢复质量和识别准确性方面均显著优于基线恢复模型。在Roboflow-LP数据集上，与性能最佳的基线模型相比，CharDiff使字符错误率（CER）相对降低了28%。", "conclusion": "这些结果表明，结构化的字符引导条件能有效增强基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性。"}}
{"id": "2510.17322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17322", "abs": "https://arxiv.org/abs/2510.17322", "authors": ["Wei Zhang", "Zhanhao Hu", "Xiao Li", "Xiaopei Zhu", "Xiaolin Hu"], "title": "A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World", "comment": "13 pages, 8 figures", "summary": "In recent years, adversarial attacks against deep learning-based object\ndetectors in the physical world have attracted much attention. To defend\nagainst these attacks, researchers have proposed various defense methods\nagainst adversarial patches, a typical form of physically-realizable attack.\nHowever, our experiments showed that simply enlarging the patch size could make\nthese defense methods fail. Motivated by this, we evaluated various defense\nmethods against adversarial clothes which have large coverage over the human\nbody. Adversarial clothes provide a good test case for adversarial defense\nagainst patch-based attacks because they not only have large sizes but also\nlook more natural than a large patch on humans. Experiments show that all the\ndefense methods had poor performance against adversarial clothes in both the\ndigital world and the physical world. In addition, we crafted a single set of\nclothes that broke multiple defense methods on Faster R-CNN. The set achieved\nan Attack Success Rate (ASR) of 96.06% against the undefended detector and over\n64.84% ASRs against nine defended models in the physical world, unveiling the\ncommon vulnerability of existing adversarial defense methods against\nadversarial clothes. Code is available at:\nhttps://github.com/weiz0823/adv-clothes-break-multiple-defenses.", "AI": {"tldr": "研究发现，现有针对物理世界对抗性补丁的防御方法在面对大尺寸对抗性服装时普遍失效，揭示了这些防御方法的共同漏洞。", "motivation": "现有针对物理世界对抗性补丁的防御方法在补丁尺寸增大时会失效。研究者希望通过评估对抗性服装（具有大覆盖范围且比大补丁更自然）来测试这些防御方法的有效性。", "method": "评估了多种针对对抗性补丁的防御方法在数字世界和物理世界中对抗对抗性服装的性能。此外，他们制作了一套特定的对抗性服装，用于攻击Faster R-CNN上的多个防御模型。", "result": "实验表明，所有防御方法在对抗对抗性服装时表现不佳。所制作的对抗性服装对未防御检测器的攻击成功率达到96.06%，对九种已防御模型的攻击成功率在物理世界中超过64.84%。", "conclusion": "现有对抗性防御方法对对抗性服装存在普遍的漏洞，无法有效抵御这种大尺寸、自然形态的物理世界对抗性攻击。"}}
{"id": "2510.17318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17318", "abs": "https://arxiv.org/abs/2510.17318", "authors": ["Sangyoon Bae", "Jiook Cha"], "title": "CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference", "comment": null, "summary": "We introduce CausalMamba, a scalable framework that addresses fundamental\nlimitations in fMRI-based causal inference: the ill-posed nature of inferring\nneural causality from hemodynamically distorted BOLD signals and the\ncomputational intractability of existing methods like Dynamic Causal Modeling\n(DCM). Our approach decomposes this complex inverse problem into two tractable\nstages: BOLD deconvolution to recover latent neural activity, followed by\ncausal graph inference using a novel Conditional Mamba architecture. On\nsimulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,\nwhen applied to real task fMRI data, our method recovers well-established\nneural pathways with 88% fidelity, whereas conventional approaches fail to\nidentify these canonical circuits in over 99% of subjects. Furthermore, our\nnetwork analysis of working memory data reveals that the brain strategically\nshifts its primary causal hub-recruiting executive or salience networks\ndepending on the stimulus-a sophisticated reconfiguration that remains\nundetected by traditional methods. This work provides neuroscientists with a\npractical tool for large-scale causal inference that captures both fundamental\ncircuit motifs and flexible network dynamics underlying cognitive function.", "AI": {"tldr": "CausalMamba是一个可扩展的框架，通过将BOLD信号去卷积与条件Mamba架构相结合，解决了fMRI因果推断中信号失真和计算复杂性问题，显著提高了因果路径识别的准确性，并揭示了传统方法无法检测到的灵活脑网络动态。", "motivation": "fMRI因果推断存在两个主要限制：从血流动力学扭曲的BOLD信号推断神经因果关系是一个不适定问题；现有方法（如动态因果模型DCM）计算上难以处理。", "method": "该方法将复杂的逆问题分解为两个可处理的阶段：首先进行BOLD去卷积以恢复潜在的神经活动，然后使用新颖的条件Mamba架构进行因果图推断。", "result": "在模拟数据上，CausalMamba的准确性比DCM高37%。在真实的任务fMRI数据上，该方法以88%的准确度恢复了公认的神经通路，而传统方法在超过99%的受试者中未能识别这些通路。此外，对工作记忆数据的网络分析揭示了大脑根据刺激策略性地改变其主要因果枢纽（招募执行或显著性网络），这种复杂的重构是传统方法无法检测到的。", "conclusion": "这项工作为神经科学家提供了一个实用的工具，用于大规模因果推断，能够捕获认知功能背后的基本回路模式和灵活的网络动态。"}}
{"id": "2510.17305", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.17305", "abs": "https://arxiv.org/abs/2510.17305", "authors": ["ZhaoYang Han", "Qihan Lin", "Hao Liang", "Bowen Chen", "Zhou Liu", "Wentao Zhang"], "title": "LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding", "comment": "Submitted to ARR Rolling Review", "summary": "We introduce \\textbf{LongInsightBench}, the first benchmark designed to\nassess models' ability to understand long videos, with a focus on human\nlanguage, viewpoints, actions, and other contextual elements, while integrating\n\\textbf{visual, audio, and text} modalities. Our benchmark excels in three key\nareas: \\textbf{a) Long-Duration, Information-Dense Videos:} We carefully select\napproximately 1,000 videos from open-source datasets FineVideo based on\nduration limit and the information density of both visual and audio modalities,\nfocusing on content like lectures, interviews, and vlogs, which contain rich\nlanguage elements. \\textbf{b) Diverse and Challenging Task Scenarios:} We have\ndesigned six challenging task scenarios, including both Intra-Event and\nInter-Event Tasks. \\textbf{c) Rigorous and Comprehensive Quality Assurance\nPipelines:} We have developed a three-step, semi-automated data quality\nassurance pipeline to ensure the difficulty and validity of the synthesized\nquestions and answer options. Based on LongInsightBench, we designed a series\nof experiments. Experimental results shows that Omni-modal models(OLMs) still\nface challenge in tasks requiring precise temporal localization (T-Loc) and\nlong-range causal inference (CE-Caus). Extended experiments reveal the\ninformation loss and processing bias in multi-modal fusion of OLMs. Our dataset\nand code is available at\nhttps://anonymous.4open.science/r/LongInsightBench-910F/.", "AI": {"tldr": "本文介绍了LongInsightBench，首个旨在评估模型理解长视频能力的基准测试，该基准整合了视觉、音频和文本模态，并专注于人类语言、视角、动作和上下文元素。实验结果显示，全模态模型在精确时间定位和长距离因果推理方面仍面临挑战。", "motivation": "现有模型难以理解长视频中包含的人类语言、视角、动作及其他上下文元素，尤其是在整合视觉、音频和文本模态时。因此，需要一个专门的基准来评估模型在这方面的能力。", "method": "本文引入了LongInsightBench基准测试，其特点包括：a) 从开源数据集FineVideo中精心挑选了约1000个时长长且信息密度高的视频（如讲座、访谈、vlog），侧重于丰富的语言内容；b) 设计了六种多样且具有挑战性的任务场景，包括事件内和事件间任务；c) 开发了三步半自动化数据质量保证流程，以确保合成问题和答案选项的难度和有效性。基于此基准，设计了一系列实验来评估全模态模型（OLMs）。", "result": "实验结果表明，全模态模型（OLMs）在需要精确时间定位（T-Loc）和长距离因果推理（CE-Caus）的任务中表现不佳。进一步的实验揭示了全模态模型在多模态融合过程中存在信息丢失和处理偏差。", "conclusion": "LongInsightBench为评估长视频理解能力提供了一个全面且严格的基准。当前的全模态模型在处理长视频的精确时间定位和长距离因果推理方面存在显著局限，这表明在多模态融合和理解长时序信息方面仍需进一步研究和改进。"}}
{"id": "2510.17332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17332", "abs": "https://arxiv.org/abs/2510.17332", "authors": ["Zhaoran Zhao", "Xinli Yue", "Jianhui Sun", "Yuhao Xie", "Tao Shao", "Liangchao Yao", "Fan Xia", "Yuetang Deng"], "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA", "comment": "Accepted to ICCV 2025 Workshop", "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction\nto more interpretable, human-aligned evaluation paradigms. In this work, we\naddress the emerging challenge of detailed and explainable IQA by proposing\niDETEX-a unified multimodal large language model (MLLM) capable of\nsimultaneously performing three key tasks: quality grounding, perception, and\ndescription. To facilitate efficient and generalizable training across these\nheterogeneous subtasks, we design a suite of task-specific offline augmentation\nmodules and a data mixing strategy. These are further complemented by online\nenhancement strategies to fully exploit multi-sourced supervision. We validate\nour approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves\nstate-of-the-art performance across all subtasks. Our model ranks first in the\nICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its\neffectiveness and robustness in delivering accurate and interpretable quality\nassessments.", "AI": {"tldr": "本文提出了iDETEX，一个统一的多模态大型语言模型（MLLM），能够同时执行图像质量评估中的质量定位、感知和描述任务，并通过创新的数据增强和混合策略，在ViDA-UGC基准测试中取得了最先进的性能，并在ICCV MIPI 2025挑战赛中获得第一名。", "motivation": "现有的图像质量评估（IQA）已从单一的标量预测发展到更具解释性、更符合人类感知的评估范式。本研究旨在解决详细和可解释IQA的新兴挑战。", "method": "本文提出了iDETEX，一个统一的多模态大型语言模型（MLLM），能够同时执行质量定位、感知和描述三个关键任务。为了在这些异构子任务中实现高效和泛化的训练，设计了一套任务特定的离线增强模块和数据混合策略，并辅以在线增强策略以充分利用多源监督。", "result": "iDETEX在大型ViDA-UGC基准测试的所有子任务中均取得了最先进的性能。该模型在ICCV MIPI 2025详细图像质量评估挑战赛中排名第一。", "conclusion": "iDETEX模型在提供准确和可解释的质量评估方面表现出卓越的有效性和鲁棒性。"}}
{"id": "2510.17338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17338", "abs": "https://arxiv.org/abs/2510.17338", "authors": ["Jiahao Huo", "Mufhumudzi Muthivhi", "Terence L. van Zyl", "Fredrik Gustafsson"], "title": "Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition", "comment": null, "summary": "Current state-of-the-art Wildlife classification models are trained under the\nclosed world setting. When exposed to unknown classes, they remain\noverconfident in their predictions. Open-set Recognition (OSR) aims to classify\nknown classes while rejecting unknown samples. Several OSR methods have been\nproposed to model the closed-set distribution by observing the feature, logit,\nor softmax probability space. A significant drawback of many existing\napproaches is the requirement to retrain the pre-trained classification model\nwith the OSR-specific strategy. This study contributes a post-processing OSR\nmethod that measures the agreement between the models' features and predicted\nlogits. We propose a probability distribution based on an input's distance to\nits Nearest Class Mean (NCM). The NCM-based distribution is then compared with\nthe softmax probabilities from the logit space to measure agreement between the\nNCM and the classification head. Our proposed strategy ranks within the top\nthree on two evaluated datasets, showing consistent performance across the two\ndatasets. In contrast, current state-of-the-art methods excel on a single\ndataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish\nanimals. The code can be found\nhttps://github.com/Applied-Representation-Learning-Lab/OSR.", "AI": {"tldr": "本文提出了一种无需重训练的后处理开放集识别（OSR）方法，通过比较输入到最近类别均值（NCM）的距离分布与Softmax概率的吻合度，有效处理野生动物分类模型在未知类别上的过度自信问题，并在两个数据集上取得了前三名的表现。", "motivation": "现有的野生动物分类模型在“封闭世界”设置下训练，面对未知类别时表现出过度自信。许多现有开放集识别（OSR）方法需要重新训练预训练分类模型，这是一个显著的缺点。", "method": "本文提出了一种后处理OSR方法，该方法通过测量模型特征与预测逻辑之间的一致性来工作。具体而言，它基于输入到其最近类别均值（NCM）的距离构建了一个概率分布，然后将这个NCM-based分布与来自逻辑空间的Softmax概率进行比较，以衡量NCM与分类头之间的一致性。", "result": "所提出的策略在两个评估数据集（非洲动物和瑞典动物）上均排名前三，显示出跨数据集的一致性能。与在单一数据集上表现优异的现有最先进方法形成对比。在非洲动物数据集上实现了93.41的AUROC，在瑞典动物数据集上实现了95.35的AUROC。", "conclusion": "该研究提供了一种有效的后处理OSR方法，无需重新训练即可提高野生动物分类模型在开放集场景下的性能和鲁棒性，并在多个数据集上展现出卓越且一致的表现。"}}
{"id": "2510.17364", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17364", "abs": "https://arxiv.org/abs/2510.17364", "authors": ["Vaggelis Dorovatas", "Soroush Seifi", "Gunshi Gupta", "Rahaf Aljundi"], "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs", "comment": "NeurIPS 2025", "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos\nin-context, provided they have full access to the video when answering queries.\nHowever, these models face challenges in streaming scenarios where hour-long\nvideos must be processed online, and questions need timely responses. In this\nwork, we propose a training-free approach compatible with standard Video-LLMs,\nleveraging three key concepts: 1) LLM-informed selection of visual tokens to\nidentify those that the LLM has attended to and contributed to its\nunderstanding of each short clip. Our attention-based selection allows us to\ndiscard up to ~95% of unimportant visual tokens with minimal performance loss;\n2) Recurrent processing of past selected tokens to generate temporally coherent\nunderstanding of each processed clip; 3) Caption-based question answering for\nlightweight and accurate responses. Our method achieves state-of-the-art\nperformance on streaming video benchmarks, striking a balance between\nefficiency and effectiveness.", "AI": {"tldr": "本文提出了一种无需训练的方法，使视频大语言模型（Video-LLMs）能够处理流媒体视频。该方法通过LLM指导的视觉令牌选择、对过去令牌的循环处理以及基于字幕的问答，实现了高效且准确的流媒体视频理解。", "motivation": "现有的视频大语言模型在处理流媒体场景（如长达数小时的视频需要在线处理并及时响应问题）时面临挑战，因为它们通常需要完全访问视频才能回答查询。", "method": "该方法是无需训练的，并与标准Video-LLMs兼容，主要包含三个关键概念：1) 基于LLM注意力机制的视觉令牌选择，识别对LLM理解每个短视频片段有贡献的令牌，可丢弃高达约95%的不重要令牌，性能损失极小；2) 对过去选定令牌的循环处理，以生成对每个已处理片段在时间上连贯的理解；3) 基于字幕的问答，实现轻量级和准确的响应。", "result": "该方法在流媒体视频基准测试中取得了最先进的性能，在效率和有效性之间取得了平衡。", "conclusion": "所提出的方法通过高效的令牌选择和循环处理，结合基于字幕的问答，成功地使视频大语言模型适用于流媒体场景，实现了卓越的性能和效率平衡。"}}
{"id": "2510.17347", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17347", "abs": "https://arxiv.org/abs/2510.17347", "authors": ["Jingqian Wu", "Shengpeng Xu", "Yunbo Jia", "Edmund Y. Lam"], "title": "Exploring The Missing Semantics In Event Modality", "comment": null, "summary": "Event cameras offer distinct advantages such as low latency, high dynamic\nrange, and efficient motion capture. However, event-to-video reconstruction\n(E2V), a fundamental event-based vision task, remains challenging, particularly\nfor reconstructing and recovering semantic information. This is primarily due\nto the nature of the event camera, as it only captures intensity changes,\nignoring static objects and backgrounds, resulting in a lack of semantic\ninformation in captured event modality. Further, semantic information plays a\ncrucial role in video and frame reconstruction, yet is often overlooked by\nexisting E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V\nframework that explores the missing visual semantic knowledge in event modality\nand leverages it to enhance event-to-video reconstruction. Specifically,\nSemantic-E2VID introduces a cross-modal feature alignment (CFA) module to\ntransfer the robust visual semantics from a frame-based vision foundation\nmodel, the Segment Anything Model (SAM), to the event encoder, while aligning\nthe high-level features from distinct modalities. To better utilize the learned\nsemantic feature, we further propose a semantic-aware feature fusion (SFF)\nblock to integrate learned semantics in frame modality to form event\nrepresentations with rich semantics that can be decoded by the event decoder.\nFurther, to facilitate the reconstruction of semantic information, we propose a\nnovel Semantic Perceptual E2V Supervision that helps the model to reconstruct\nsemantic details by leveraging SAM-generated categorical labels. Extensive\nexperiments demonstrate that Semantic-E2VID significantly enhances frame\nquality, outperforming state-of-the-art E2V methods across multiple benchmarks.\nThe sample code is included in the supplementary material.", "AI": {"tldr": "事件相机到视频重建（E2V）在语义信息恢复方面面临挑战。本文提出Semantic-E2VID框架，通过跨模态特征对齐、语义感知特征融合以及语义感知E2V监督，将外部视觉语义知识（来自SAM）整合到事件流中，显著提升了重建帧的质量。", "motivation": "事件相机只捕捉强度变化，忽略静态物体，导致事件数据缺乏语义信息。现有E2V方法常忽视语义信息在视频和帧重建中的关键作用，造成重建挑战。", "method": "本文提出Semantic-E2VID框架：1. 引入跨模态特征对齐（CFA）模块，将帧基视觉基础模型（SAM）的鲁棒视觉语义知识转移到事件编码器，并对齐不同模态的高级特征。2. 提出语义感知特征融合（SFF）模块，将学习到的语义整合到事件表示中。3. 提出新颖的语义感知E2V监督，利用SAM生成的类别标签促进语义细节重建。", "result": "实验结果表明，Semantic-E2VID显著提升了帧质量，在多个基准测试中超越了现有最先进的E2V方法。", "conclusion": "Semantic-E2VID通过引入和利用外部视觉语义知识，有效解决了事件相机到视频重建中语义信息缺失的问题，从而实现了更优质的帧重建效果。"}}
{"id": "2510.17373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17373", "abs": "https://arxiv.org/abs/2510.17373", "authors": ["Yintao Zhou", "Wei Huang", "Zhengyu Li", "Jing Huang", "Meng Pang"], "title": "Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing", "comment": "3 pages, 2 figures, accepted by MIND 2025", "summary": "Parkinson's disease (PD) severity diagnosis is crucial for early detecting\npotential patients and adopting tailored interventions. Diagnosing PD based on\nfacial expression is grounded in PD patients' \"masked face\" symptom and gains\ngrowing interest recently for its convenience and affordability. However,\ncurrent facial expression-based approaches often rely on single type of\nexpression which can lead to misdiagnosis, and ignore the class imbalance\nacross different PD stages which degrades the prediction performance. Moreover,\nmost existing methods focus on binary classification (i.e., PD / non-PD) rather\nthan diagnosing the severity of PD. To address these issues, we propose a new\nfacial expression-based method for PD severity diagnosis which integrates\nmultiple facial expression features through attention-based feature fusion.\nMoreover, we mitigate the class imbalance problem via an adaptive class\nbalancing strategy which dynamically adjusts the contribution of training\nsamples based on their class distribution and classification difficulty.\nExperimental results demonstrate the promising performance of the proposed\nmethod for PD severity diagnosis, as well as the efficacy of attention-based\nfeature fusion and adaptive class balancing.", "AI": {"tldr": "该研究提出了一种基于面部表情的帕金森病（PD）严重程度诊断方法，通过注意力机制融合多种面部表情特征，并采用自适应类别平衡策略解决数据不平衡问题，以提高诊断的准确性和鲁棒性。", "motivation": "当前的基于面部表情的PD诊断方法存在以下问题：1) 依赖单一表情类型易导致误诊；2) 忽略不同PD阶段的类别不平衡问题，导致预测性能下降；3) 大多数现有方法侧重于二元分类（PD/非PD），而非PD严重程度的诊断。", "method": "该研究提出了一种新的PD严重程度诊断方法：1) 通过注意力机制的特征融合技术，整合多种面部表情特征；2) 采用自适应类别平衡策略，根据类别分布和分类难度动态调整训练样本的贡献，以缓解类别不平衡问题。", "result": "实验结果表明，所提出的方法在PD严重程度诊断方面表现出良好的性能，并验证了注意力机制特征融合和自适应类别平衡策略的有效性。", "conclusion": "该研究成功开发了一种用于PD严重程度诊断的基于面部表情的新方法，有效解决了单一表情依赖和类别不平衡等问题，为PD的早期检测和个性化干预提供了有前景的工具。"}}
{"id": "2510.17372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17372", "abs": "https://arxiv.org/abs/2510.17372", "authors": ["Paweł Borsukiewicz", "Fadi Boutros", "Iyiola E. Olatunji", "Charles Beumier", "Wendkûuni C. Ouedraogo", "Jacques Klein", "Tegawendé F. Bissyandé"], "title": "Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise", "comment": null, "summary": "The deployment of facial recognition systems has created an ethical dilemma:\nachieving high accuracy requires massive datasets of real faces collected\nwithout consent, leading to dataset retractions and potential legal liabilities\nunder regulations like GDPR. While synthetic facial data presents a promising\nprivacy-preserving alternative, the field lacks comprehensive empirical\nevidence of its viability. This study addresses this critical gap through\nextensive evaluation of synthetic facial recognition datasets. We present a\nsystematic literature review identifying 25 synthetic facial recognition\ndatasets (2018-2025), combined with rigorous experimental validation. Our\nmethodology examines seven key requirements for privacy-preserving synthetic\ndata: identity leakage prevention, intra-class variability, identity\nseparability, dataset scale, ethical data sourcing, bias mitigation, and\nbenchmark reliability. Through experiments involving over 10 million synthetic\nsamples, extended by a comparison of results reported on five standard\nbenchmarks, we provide the first comprehensive empirical assessment of\nsynthetic data's capability to replace real datasets. Best-performing synthetic\ndatasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and\n94.91% respectively, surpassing established real datasets including\nCASIA-WebFace (94.70%). While those images remain private, publicly available\nalternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our\nfindings reveal that they ensure proper intra-class variability while\nmaintaining identity separability. Demographic bias analysis shows that, even\nthough synthetic data inherits limited biases, it offers unprecedented control\nfor bias mitigation through generation parameters. These results establish\nsynthetic facial data as a scientifically viable and ethically imperative\nalternative for facial recognition research.", "AI": {"tldr": "本研究通过全面评估证明，合成人脸数据是人脸识别研究中真实数据集的科学可行且道德上必要的替代方案，其识别准确率可超越现有真实数据集。", "motivation": "人脸识别系统面临伦理困境：高准确率依赖大量未经同意收集的真实人脸数据，导致数据集撤回和GDPR等法规下的潜在法律责任。虽然合成数据有望解决隐私问题，但目前缺乏其可行性的全面实证证据。", "method": "本研究通过以下方法解决问题：1. 系统性文献综述，识别了25个2018-2025年间的合成人脸识别数据集。2. 严格的实验验证，评估了合成数据的七个关键要求：身份泄露预防、类内变异性、身份可分离性、数据集规模、伦理数据来源、偏见缓解和基准可靠性。3. 涉及超过1000万合成样本的实验，并与五个标准基准上报告的结果进行比较。", "result": "研究发现：1. 表现最佳的合成数据集（VariFace、VIGFace）分别达到95.67%和94.91%的识别准确率，超过了CASIA-WebFace（94.70%）等已建立的真实数据集。2. 公开可用的替代方案（Vec2Face、CemiFace）也紧随其后，准确率分别为93.52%和93.22%。3. 这些合成数据集在保持身份可分离性的同时，确保了适当的类内变异性。4. 尽管合成数据继承了有限的偏差，但通过生成参数提供了前所未有的偏差缓解控制。", "conclusion": "本研究结果确立了合成人脸数据作为人脸识别研究中科学可行且道德上必要的替代方案的地位。"}}
{"id": "2510.17409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17409", "abs": "https://arxiv.org/abs/2510.17409", "authors": ["Dmitrii Galimzianov", "Viacheslav Vyshegorodtsev", "Ivan Nezhivykh"], "title": "Monitoring Horses in Stalls: From Object to Event Detection", "comment": "12 pages, 4 figures, 4 tables", "summary": "Monitoring the behavior of stalled horses is essential for early detection of\nhealth and welfare issues but remains labor-intensive and time-consuming. In\nthis study, we present a prototype vision-based monitoring system that\nautomates the detection and tracking of horses and people inside stables using\nobject detection and multi-object tracking techniques. The system leverages\nYOLOv11 and BoT-SORT for detection and tracking, while event states are\ninferred based on object trajectories and spatial relations within the stall.\nTo support development, we constructed a custom dataset annotated with\nassistance from foundation models CLIP and GroundingDINO. The system\ndistinguishes between five event types and accounts for the camera's blind\nspots. Qualitative evaluation demonstrated reliable performance for\nhorse-related events, while highlighting limitations in detecting people due to\ndata scarcity. This work provides a foundation for real-time behavioral\nmonitoring in equine facilities, with implications for animal welfare and\nstable management.", "AI": {"tldr": "该研究提出了一种基于视觉的原型系统，利用YOLOv11和BoT-SORT技术自动监测马厩中马匹和人员的行为，以实现早期健康和福利问题检测。", "motivation": "监测马厩中马匹的行为对于早期发现健康和福利问题至关重要，但传统方法劳动密集且耗时。", "method": "系统采用YOLOv11进行目标检测，BoT-SORT进行多目标跟踪。事件状态通过目标轨迹和马厩内的空间关系推断。为支持开发，构建了一个自定义数据集，并借助CLIP和GroundingDINO等基础模型进行标注。", "result": "该系统能够区分五种事件类型并考虑摄像头的盲区。定性评估显示，系统在马匹相关事件检测方面表现可靠，但在人员检测方面因数据稀缺而存在局限性。", "conclusion": "这项工作为马匹设施中的实时行为监测奠定了基础，对动物福利和马厩管理具有重要意义。"}}
{"id": "2510.17384", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17384", "abs": "https://arxiv.org/abs/2510.17384", "authors": ["Jiajin Tang", "Zhengxuan Wei", "Ge Zheng", "Sibei Yang"], "title": "Closed-Loop Transfer for Weakly-supervised Affordance Grounding", "comment": "Accepted at ICCV 2025", "summary": "Humans can perform previously unexperienced interactions with novel objects\nsimply by observing others engage with them. Weakly-supervised affordance\ngrounding mimics this process by learning to locate object regions that enable\nactions on egocentric images, using exocentric interaction images with\nimage-level annotations. However, extracting affordance knowledge solely from\nexocentric images and transferring it one-way to egocentric images limits the\napplicability of previous works in complex interaction scenarios. Instead, this\nstudy introduces LoopTrans, a novel closed-loop framework that not only\ntransfers knowledge from exocentric to egocentric but also transfers back to\nenhance exocentric knowledge extraction. Within LoopTrans, several innovative\nmechanisms are introduced, including unified cross-modal localization and\ndenoising knowledge distillation, to bridge domain gaps between object-centered\negocentric and interaction-centered exocentric images while enhancing knowledge\ntransfer. Experiments show that LoopTrans achieves consistent improvements\nacross all metrics on image and video benchmarks, even handling challenging\nscenarios where object interaction regions are fully occluded by the human\nbody.", "AI": {"tldr": "LoopTrans是一个新颖的闭环框架，通过双向知识迁移（从外部视角到自我视角，再从自我视角返回）来增强弱监督的物体功能区域定位，从而在复杂交互场景中实现一致的性能提升。", "motivation": "现有弱监督功能区域定位方法仅将知识从外部视角单向迁移到自我视角，这限制了它们在复杂交互场景中的适用性。研究旨在解决这一局限性，提高知识提取和迁移的有效性。", "method": "本研究提出了LoopTrans，一个闭环框架，不仅将知识从外部视角迁移到自我视角，还反向迁移以增强外部视角知识提取。LoopTrans引入了统一的跨模态定位和去噪知识蒸馏等机制，以弥合领域差距并增强知识迁移。", "result": "LoopTrans在图像和视频基准测试的所有指标上都取得了持续的改进。它甚至能处理物体交互区域被人体完全遮挡的挑战性场景。", "conclusion": "LoopTrans通过其创新的闭环双向知识迁移机制，显著提升了弱监督功能区域定位的性能和鲁棒性，使其能够应对复杂的、包含遮挡的交互场景。"}}
{"id": "2510.17440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17440", "abs": "https://arxiv.org/abs/2510.17440", "authors": ["Qiyuan Guan", "Xiang Chen", "Guiyue Jin", "Jiyu Jin", "Shumin Fan", "Tianyu Song", "Jinshan Pan"], "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation", "comment": "Accepted by NeurIPS 2025", "summary": "Compared to daytime image deraining, nighttime image deraining poses\nsignificant challenges due to inherent complexities of nighttime scenarios and\nthe lack of high-quality datasets that accurately represent the coupling effect\nbetween rain and illumination. In this paper, we rethink the task of nighttime\nimage deraining and contribute a new high-quality benchmark, HQ-NightRain,\nwhich offers higher harmony and realism compared to existing datasets. In\naddition, we develop an effective Color Space Transformation Network (CST-Net)\nfor better removing complex rain from nighttime scenes. Specifically, we\npropose a learnable color space converter (CSC) to better facilitate rain\nremoval in the Y channel, as nighttime rain is more pronounced in the Y channel\ncompared to the RGB color space. To capture illumination information for\nguiding nighttime deraining, implicit illumination guidance is introduced\nenabling the learned features to improve the model's robustness in complex\nscenarios. Extensive experiments show the value of our dataset and the\neffectiveness of our method. The source code and datasets are available at\nhttps://github.com/guanqiyuan/CST-Net.", "AI": {"tldr": "夜间图像去雨面临挑战，本文贡献了一个高质量数据集HQ-NightRain，并提出了CST-Net网络。CST-Net利用可学习的色彩空间转换器在Y通道去雨，并引入隐式光照引导，有效提升了夜间去雨效果。", "motivation": "与白天图像去雨相比，夜间图像去雨由于场景复杂性和缺乏高质量数据集（未能准确反映雨水与光照的耦合效应）而面临巨大挑战。", "method": "本文提出了一个高质量的基准数据集HQ-NightRain，以提供更高的和谐性和真实感。同时，开发了一个有效的色彩空间转换网络（CST-Net），其中包含一个可学习的色彩空间转换器（CSC），用于在Y通道更好地去除雨水（夜间雨水在Y通道更明显）。此外，引入了隐式光照引导，以捕获光照信息并提高模型在复杂场景中的鲁棒性。", "result": "大量的实验表明，本文提出的数据集具有价值，并且所提出的方法是有效的。", "conclusion": "本文通过提供一个高质量的夜间去雨基准数据集HQ-NightRain，并开发了结合可学习色彩空间转换和隐式光照引导的CST-Net，成功解决了夜间图像去雨的复杂挑战，显著提升了去雨性能。"}}
{"id": "2510.17422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17422", "abs": "https://arxiv.org/abs/2510.17422", "authors": ["Shaharyar Ahmed Khan Tareen", "Filza Khan Tareen"], "title": "DeepDetect: Learning All-in-One Dense Keypoints", "comment": "6 pages, 6 figures, 2 tables, 7 equations", "summary": "Keypoint detection is the foundation of many computer vision tasks, including\nimage registration, structure-from motion, 3D reconstruction, visual odometry,\nand SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning\nbased methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong\nperformance yet suffer from key limitations: sensitivity to photometric\nchanges, low keypoint density and repeatability, limited adaptability to\nchallenging scenes, and lack of semantic understanding, often failing to\nprioritize visually important regions. We present DeepDetect, an intelligent,\nall-in-one, dense keypoint detector that unifies the strengths of classical\ndetectors using deep learning. Firstly, we create ground-truth masks by fusing\noutputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from\ncorners and blobs to prominent edges and textures in the images. Afterwards, a\nlightweight and efficient model: ESPNet, is trained using these masks as\nlabels, enabling DeepDetect to focus semantically on images while producing\nhighly dense keypoints, that are adaptable to diverse and visually degraded\nconditions. Evaluations on the Oxford Affine Covariant Regions dataset\ndemonstrate that DeepDetect surpasses other detectors in keypoint density,\nrepeatability, and the number of correct matches, achieving maximum values of\n0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003\n(correct matches).", "AI": {"tldr": "DeepDetect是一种智能、一体化的密集关键点检测器，它融合了经典检测器的优点和深度学习，通过融合多种检测器输出的掩码训练，实现了高密度、高重复性、语义理解和对挑战性场景的适应性，性能优于现有方法。", "motivation": "现有关键点检测器（无论是传统还是基于学习的方法）存在局限性，包括对光度变化的敏感性、关键点密度和重复性低、对挑战性场景适应性有限以及缺乏语义理解，常常无法优先考虑视觉上重要的区域。", "method": "首先，通过融合7个关键点检测器和2个边缘检测器的输出创建地面真值掩码，以提取图像中从角点、斑点到显著边缘和纹理的各种视觉线索。然后，使用这些掩码作为标签训练一个轻量级高效的ESPNet模型，使DeepDetect能够语义化地聚焦图像，并生成高度密集、适应多样化和视觉退化条件的关键点。", "result": "在Oxford Affine Covariant Regions数据集上的评估表明，DeepDetect在关键点密度、重复性和正确匹配数量方面超越了其他检测器，分别达到最大值0.5143（平均关键点密度）、0.9582（平均重复性）和59,003（正确匹配）。", "conclusion": "DeepDetect是一个智能、一体化的密集关键点检测器，它通过深度学习统一了经典检测器的优势，能够生成高密度、高重复性且适应性强的关键点，有效解决了现有方法的局限性。"}}
{"id": "2510.17434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17434", "abs": "https://arxiv.org/abs/2510.17434", "authors": ["Julien Zouein", "Hossein Javidnia", "François Pitié", "Anil Kokaram"], "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching", "comment": "Accepted ICIR 2025, camera-ready version", "summary": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences\nand short tracks filtered by cosine consistency. On short videos, this\ncompressed-domain front end runs comparably to sequential SIFT while using far\nless CPU, and yields denser matches with competitive pairwise geometry. As a\nsmall SfM demo on a 117-frame clip, MV matches register all images and\nreconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows\nwith match density. These results show compressed-domain correspondences are a\npractical, resource-efficient front end with clear paths to scaling in full\npipelines.", "AI": {"tldr": "该研究利用AV1运动矢量生成密集的亚像素对应关系和短轨迹，作为一种资源高效的前端，在性能上与SIFT相当，但CPU消耗更少，并显示出在完整管道中扩展的潜力。", "motivation": "传统方法如SIFT在生成密集对应关系时CPU消耗大。研究旨在探索一种更高效、资源节约的方法，通过利用视频压缩领域的现有信息（如AV1运动矢量）来解决这一问题，从而在计算效率和匹配密度之间取得平衡。", "method": "研究方法是将AV1运动矢量重新用于生成密集的亚像素对应关系，并通过余弦一致性进行过滤以产生短轨迹。然后，将这种压缩域前端的性能与顺序SIFT进行比较，并在一个小型SfM（运动恢复结构）演示中评估其在图像注册和点云重建方面的能力。", "result": "在短视频上，该压缩域前端的运行速度与顺序SIFT相当，但CPU使用率显著降低，并产生了具有竞争性成对几何形状的更密集匹配。在一个117帧片段的小型SfM演示中，运动矢量匹配成功注册了所有图像，并重建了0.46-0.62M个点，重投影误差为0.51-0.53像素。同时，捆绑调整（BA）时间随匹配密度而增加。", "conclusion": "研究得出结论，压缩域对应关系（通过AV1运动矢量生成）是一种实用且资源高效的前端，具有明确的路径可以在完整的管道中进行扩展，为后续的计算机视觉任务提供了有前景的解决方案。"}}
{"id": "2510.17482", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17482", "abs": "https://arxiv.org/abs/2510.17482", "authors": ["Chenxu Dang", "Haiyan Liu", "Guangjun Bao", "Pei An", "Xinyue Tang", "Jie Ma", "Bingchuan Sun", "Yan Wang"], "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries", "comment": "Under Review", "summary": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.", "AI": {"tldr": "本文提出SparseWorld，一个基于稀疏动态查询的新型4D语义占据世界模型，解决了现有模型在灵活性和动态场景对齐方面的限制，并在感知、预测和规划任务中实现了最先进的性能。", "motivation": "现有的语义占据世界模型依赖于静态固定嵌入或网格，限制了感知的灵活性。此外，它们基于网格的“原地分类”可能与真实场景的动态连续性不一致。", "method": "本文提出了SparseWorld，一个灵活、自适应且高效的4D占据世界模型，其核心是稀疏和动态查询。具体包括：1) 范围自适应感知模块：通过自我车辆状态调制可学习查询，并融入时空关联以实现扩展范围感知。2) 状态条件预测模块：用回归引导的公式替代基于分类的预测，使动态查询与4D环境的连续性精确对齐。3) 时间感知自调度训练策略：实现平滑高效的训练。", "result": "广泛的实验表明，SparseWorld在感知、预测和规划任务中均取得了最先进的性能。全面的可视化和消融研究进一步验证了SparseWorld在灵活性、适应性和效率方面的优势。", "conclusion": "SparseWorld是一个新颖的4D占据世界模型，通过稀疏动态查询、范围自适应感知和状态条件预测，克服了现有模型的局限性，实现了卓越的灵活性、适应性和效率，并在多项任务中达到SOTA表现。"}}
{"id": "2510.17479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17479", "abs": "https://arxiv.org/abs/2510.17479", "authors": ["Feng Zhou", "Wenkai Guo", "Pu Cao", "Zhicheng Zhang", "Jianqin Yin"], "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS", "comment": "A preprint paper", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training\nviews, leading to artifacts like blurring in novel view rendering. Prior work\naddresses it either by enhancing the initialization (\\emph{i.e.}, the point\ncloud from Structure-from-Motion (SfM)) or by adding training-time constraints\n(regularization) to the 3DGS optimization. Yet our controlled ablations reveal\nthat initialization is the decisive factor: it determines the attainable\nperformance band in sparse-view 3DGS, while training-time constraints yield\nonly modest within-band improvements at extra cost. Given initialization's\nprimacy, we focus our design there. Although SfM performs poorly under sparse\nviews due to its reliance on feature matching, it still provides reliable seed\npoints. Thus, building on SfM, our effort aims to supplement the regions it\nfails to cover as comprehensively as possible. Specifically, we design: (i)\nfrequency-aware SfM that improves low-texture coverage via low-frequency view\naugmentation and relaxed multi-view correspondences; (ii) 3DGS\nself-initialization that lifts photometric supervision into additional points,\ncompensating SfM-sparse regions with learned Gaussian centers; and (iii)\npoint-cloud regularization that enforces multi-view consistency and uniform\nspatial coverage through simple geometric/visibility priors, yielding a clean\nand reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate\nconsistent gains in sparse-view settings, establishing our approach as a\nstronger initialization strategy. Code is available at\nhttps://github.com/zss171999645/ItG-GS.", "AI": {"tldr": "针对稀疏视角3D高斯泼溅（3DGS）过拟合问题，本文通过消融实验发现初始化是决定性因素。作者提出了一种新的初始化策略，包括频率感知SfM、3DGS自初始化和点云正则化，以生成更密集、更可靠的初始点云，显著提升了稀疏视角下的渲染质量。", "motivation": "稀疏视角3DGS常对训练视图过拟合，导致新视角渲染模糊。现有方法或改进初始化，或添加训练时约束。但本文消融实验表明，初始化是决定性因素，而SfM在稀疏视角下表现不佳。", "method": "本文专注于改进初始化设计：\n1.  **频率感知SfM**：通过低频视图增强和放宽多视图对应关系，改善低纹理区域的覆盖。\n2.  **3DGS自初始化**：将光度监督提升为额外的点，用学习到的高斯中心补偿SfM未覆盖的稀疏区域。\n3.  **点云正则化**：通过简单的几何/可见性先验，强制执行多视图一致性和均匀空间覆盖，生成干净可靠的点云。", "result": "在LLFF和Mip-NeRF360数据集上的实验表明，该方法在稀疏视角设置下持续取得性能提升，证明其作为一种更强大的初始化策略的有效性。", "conclusion": "初始化是稀疏视角3DGS性能的关键。本文提出的新初始化策略，通过增强SfM、引入自初始化和点云正则化，有效解决了SfM在稀疏视角下的局限性，显著提升了3DGS的渲染质量。"}}
{"id": "2510.17529", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17529", "abs": "https://arxiv.org/abs/2510.17529", "authors": ["Yovin Yahathugoda", "Davide Prezzi", "Piyalitt Ittichaiwong", "Vicky Goh", "Sebastien Ourselin", "Michela Antonelli"], "title": "MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation", "comment": null, "summary": "Active Surveillance (AS) is a treatment option for managing low and\nintermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while\nmonitoring disease progression through serial MRI and clinical follow-up.\nAccurate prostate segmentation is an important preliminary step for automating\nthis process, enabling automated detection and diagnosis of PCa. However,\nexisting deep-learning segmentation models are often trained on\nsingle-time-point and expertly annotated datasets, making them unsuitable for\nlongitudinal AS analysis, where multiple time points and a scarcity of expert\nlabels hinder their effective fine-tuning. To address these challenges, we\npropose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation\narchitecture that computes the segmentation for time point t by leveraging the\nMRI and the corresponding segmentation mask from the previous time point. We\nintroduce two new components: (i) a Mamba-enhanced Cross-Attention Module,\nwhich integrates the Mamba block into cross attention to efficiently capture\ntemporal evolution and long-range spatial dependencies, and (ii) a Shape\nExtractor Module that encodes the previous segmentation mask into a latent\nanatomical representation for refined zone delination. Moreover, we introduce a\nsemi-supervised self-training strategy that leverages pseudo-labels generated\nfrom a pre-trained nnU-Net, enabling effective learning without expert\nannotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results\nshowed that it significantly outperforms state-of-the-art U-Net and\nTransformer-based models, achieving superior prostate zone segmentation even\nwhen trained on limited and noisy data.", "AI": {"tldr": "本文提出MambaX-Net，一种新颖的半监督、双扫描3D分割架构，用于纵向前列腺癌主动监测(AS)。该模型利用先前时间点的MRI和分割掩模，并通过Mamba增强的交叉注意力模块和形状提取模块，在有限和噪声数据下，显著优于现有模型，实现了卓越的前列腺区域分割。", "motivation": "主动监测(AS)需要准确的前列腺分割以实现PCa的自动化检测和诊断。然而，现有的深度学习分割模型通常在单时间点和专家标注数据集上训练，不适用于纵向AS分析，因为多时间点和专家标签稀缺性阻碍了其有效微调。", "method": "本文提出MambaX-Net，一个半监督、双扫描3D分割架构，通过利用前一时间点t的MRI及其相应的分割掩模来计算当前时间点t的分割。该架构包含两个新组件：(i) Mamba增强的交叉注意力模块，将Mamba块集成到交叉注意力中，以有效捕获时间演变和长距离空间依赖；(ii) 形状提取模块，将前一个分割掩模编码为潜在解剖表示，以进行精细区域划分。此外，引入了一种半监督自训练策略，利用预训练nnU-Net生成的伪标签，在没有专家标注的情况下实现有效学习。", "result": "MambaX-Net在纵向AS数据集上进行了评估，结果表明，即使在有限和噪声数据上训练，它也显著优于最先进的U-Net和基于Transformer的模型，实现了卓越的前列腺区域分割。", "conclusion": "MambaX-Net为纵向前列腺癌主动监测中的前列腺区域分割提供了一个鲁棒且高效的解决方案，有效克服了传统模型在多时间点和稀缺专家标签场景下的挑战，为自动化PCa检测和诊断奠定了基础。"}}
{"id": "2510.17566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17566", "abs": "https://arxiv.org/abs/2510.17566", "authors": ["Nachuan Ma", "Zhengfei Song", "Qiang Hu", "Xiaoyu Tang", "Chengxi Zhang", "Rui Fan", "Lihua Xie"], "title": "WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection", "comment": null, "summary": "Road crack detection is essential for intelligent infrastructure maintenance\nin smart cities. To reduce reliance on costly pixel-level annotations, we\npropose WP-CrackNet, an end-to-end weakly-supervised method that trains with\nonly image-level labels for pixel-wise crack detection. WP-CrackNet integrates\nthree components: a classifier generating class activation maps (CAMs), a\nreconstructor measuring feature inferability, and a detector producing\npixel-wise road crack detection results. During training, the classifier and\nreconstructor alternate in adversarial learning to encourage crack CAMs to\ncover complete crack regions, while the detector learns from pseudo labels\nderived from post-processed crack CAMs. This mutual feedback among the three\ncomponents improves learning stability and detection accuracy. To further boost\ndetection performance, we design a path-aware attention module (PAAM) that\nfuses high-level semantics from the classifier with low-level structural cues\nfrom the reconstructor by modeling spatial and channel-wise dependencies.\nAdditionally, a center-enhanced CAM consistency module (CECCM) is proposed to\nrefine crack CAMs using center Gaussian weighting and consistency constraints,\nenabling better pseudo-label generation. We create three image-level datasets\nand extensive experiments show that WP-CrackNet achieves comparable results to\nsupervised methods and outperforms existing weakly-supervised methods,\nsignificantly advancing scalable road inspection. The source code package and\ndatasets are available at https://mias.group/WP-CrackNet/.", "AI": {"tldr": "WP-CrackNet是一种端到端弱监督方法，仅使用图像级标签进行像素级道路裂缝检测，其性能可与监督方法媲美，并优于现有弱监督方法，显著推动了可扩展道路检测。", "motivation": "智能城市中道路裂缝检测对基础设施维护至关重要，但传统的像素级标注成本高昂。本研究旨在减少对像素级标注的依赖，提出一种仅需图像级标签的弱监督方法。", "method": "WP-CrackNet集成了分类器（生成CAMs）、重构器（测量特征可推断性）和检测器（像素级裂缝检测）三个组件。训练过程中，分类器和重构器通过对抗学习交替进行，促使裂缝CAMs覆盖完整的裂缝区域；检测器则从后处理的裂缝CAMs生成的伪标签中学习。组件间的相互反馈提高了学习稳定性和检测精度。此外，设计了路径感知注意力模块（PAAM）融合高层语义和低层结构线索，并通过中心增强CAM一致性模块（CECCM）使用中心高斯加权和一致性约束来细化CAMs，以生成更好的伪标签。", "result": "WP-CrackNet在三个自建的图像级数据集上进行了广泛实验，结果表明其取得了与监督方法相当的性能，并且优于现有的弱监督方法。", "conclusion": "WP-CrackNet显著推动了可扩展道路检测技术的发展，提供了一种高效的弱监督裂缝检测解决方案，减少了对昂贵像素级标注的依赖。"}}
{"id": "2510.17519", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17519", "abs": "https://arxiv.org/abs/2510.17519", "authors": ["Yongshun Zhang", "Zhongyi Fan", "Yonghang Zhang", "Zhangzikang Li", "Weifeng Chen", "Zhongwei Feng", "Chaoyue Wang", "Peng Hou", "Anxiang Zeng"], "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models", "comment": "Technical Report; Project Page:\n  \\href{https://github.com/Shopee-MUG/MUG-V}", "summary": "In recent years, large-scale generative models for visual content\n(\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable\nprogress. However, training large-scale video generation models remains\nparticularly challenging and resource-intensive due to cross-modal text-video\nalignment, the long sequences involved, and the complex spatiotemporal\ndependencies. To address these challenges, we present a training framework that\noptimizes four pillars: (i) data processing, (ii) model architecture, (iii)\ntraining strategy, and (iv) infrastructure for large-scale video generation\nmodels. These optimizations delivered significant efficiency gains and\nperformance improvements across all stages of data preprocessing, video\ncompression, parameter scaling, curriculum-based pretraining, and\nalignment-focused post-training. Our resulting model, MUG-V 10B, matches recent\nstate-of-the-art video generators overall and, on e-commerce-oriented video\ngeneration tasks, surpasses leading open-source baselines in human evaluations.\nMore importantly, we open-source the complete stack, including model weights,\nMegatron-Core-based large-scale training code, and inference pipelines for\nvideo generation and enhancement. To our knowledge, this is the first public\nrelease of large-scale video generation training code that exploits\nMegatron-Core to achieve high training efficiency and near-linear multi-node\nscaling, details are available in\n\\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.", "AI": {"tldr": "本文提出一个优化数据、模型架构、训练策略和基础设施的训练框架，以解决大规模视频生成模型的挑战。其成果MUG-V 10B模型在性能上达到SOTA，并在电商视频生成任务中超越开源基线，并首次开源了基于Megatron-Core的高效训练代码和模型。", "motivation": "训练大规模视频生成模型面临严峻挑战，主要体现在跨模态文本-视频对齐、长序列处理以及复杂的时空依赖性，导致训练过程资源密集且困难。", "method": "研究者提出了一个优化四个关键支柱的训练框架：(i) 数据处理、(ii) 模型架构、(iii) 训练策略和 (iv) 基础设施。具体优化包括数据预处理、视频压缩、参数扩展、基于课程的预训练和以对齐为重点的后训练。该框架利用Megatron-Core实现了高效且近线性多节点扩展的大规模训练。", "result": "通过这些优化，MUG-V 10B模型在整体上与最新的SOTA视频生成器表现相当，并在电商导向的视频生成任务中，通过人工评估超越了领先的开源基线。更重要的是，研究者开源了包括模型权重、基于Megatron-Core的大规模训练代码和推理管道在内的完整堆栈，实现了高训练效率和近线性的多节点扩展。", "conclusion": "本文提出的训练框架和开源的MUG-V 10B模型，在大规模视频生成模型的训练效率和性能方面取得了显著进展，并首次公开了利用Megatron-Core实现高效、可扩展训练的大规模视频生成训练代码，为社区提供了宝贵的资源。"}}
{"id": "2510.17484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17484", "abs": "https://arxiv.org/abs/2510.17484", "authors": ["Muhammad Umer Ramzan", "Ali Zia", "Abdelwahed Khamis", "Noman Ali", "Usman Ali", "Wei Xiang"], "title": "Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment", "comment": null, "summary": "Salient object detection (SOD) aims to segment visually prominent regions in\nimages and serves as a foundational task for various computer vision\napplications. We posit that SOD can now reach near-supervised accuracy without\na single pixel-level label, but only when reliable pseudo-masks are available.\nWe revisit the prototype-based line of work and make two key observations.\nFirst, boundary pixels and interior pixels obey markedly different geometry;\nsecond, the global consistency enforced by optimal transport (OT) is\nunderutilized if prototype quality is weak. To address this, we introduce\nPOTNet, an adaptation of Prototypical Optimal Transport that replaces POT's\nsingle k-means step with an entropy-guided dual-clustering head: high-entropy\npixels are organized by spectral clustering, low-entropy pixels by k-means, and\nthe two prototype sets are subsequently aligned by OT. This\nsplit-fuse-transport design yields sharper, part-aware pseudo-masks in a single\nforward pass, without handcrafted priors. Those masks supervise a standard\nMaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end\nunsupervised SOD pipeline that eliminates SelfMask's offline voting yet\nimproves both accuracy and training efficiency. Extensive experiments on five\nbenchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and\nweakly supervised methods by up to 36% in F-measure, further narrowing the gap\nto fully supervised models.", "AI": {"tldr": "本文提出了一种名为AutoSOD的无监督显著目标检测（SOD）端到端流水线，通过引入POTNet生成高质量伪掩码，实现了接近有监督的准确性，并在多个基准测试中显著优于现有无监督和弱监督方法。", "motivation": "显著目标检测是计算机视觉的基础任务。作者认为，如果能获得可靠的伪掩码，SOD可以在没有像素级标注的情况下达到接近有监督的准确性。现有基于原型的方法在处理边界像素和内部像素的几何差异时存在不足，并且在原型质量较差时未能充分利用最优传输（OT）的全局一致性。", "method": "本文引入了POTNet，它是原型最优传输（POT）的改进版。POTNet用一个熵引导的双聚类头替换了POT的单一k-means步骤：高熵像素（通常是边界）通过谱聚类组织，低熵像素（通常是内部）通过k-means聚类。随后，这两个原型集通过最优传输进行对齐。这种“分割-融合-传输”设计在单次前向传播中生成更清晰、感知部件的伪掩码。这些伪掩码用于监督一个标准的MaskFormer风格编码器-解码器，从而形成端到端的无监督SOD流水线AutoSOD。", "result": "在五个基准测试中，AutoSOD在F-measure上比无监督方法高出26%，比弱监督方法高出36%，进一步缩小了与全监督模型的差距。与SelfMask相比，它还消除了离线投票并提高了训练效率。", "conclusion": "在可靠伪掩码的帮助下，无监督显著目标检测可以达到接近有监督的准确性。本文提出的POTNet和AutoSOD流水线通过生成高质量的伪掩码，显著提升了无监督SOD的性能，并为该领域的发展开辟了新路径。"}}
{"id": "2510.17501", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17501", "abs": "https://arxiv.org/abs/2510.17501", "authors": ["Yuanli Wu", "Long Zhang", "Yue Du", "Bin Li"], "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization", "comment": null, "summary": "With the rapid proliferation of video content across social media,\nsurveillance, and education platforms, efficiently summarizing long videos into\nconcise yet semantically faithful surrogates has become increasingly vital.\nExisting supervised methods achieve strong in-domain accuracy by learning from\ndense annotations but suffer from high labeling costs and limited cross-dataset\ngeneralization, while unsupervised approaches, though label-free, often fail to\ncapture high-level human semantics and fine-grained narrative cues. More\nrecently, zero-shot prompting pipelines have leveraged large language models\n(LLMs) for training-free video summarization, yet remain highly sensitive to\nhandcrafted prompt templates and dataset-specific score normalization. To\novercome these limitations, we introduce a rubric-guided, pseudo-labeled\nprompting framework that transforms a small subset of ground-truth annotations\ninto high-confidence pseudo labels, which are aggregated into structured,\ndataset-adaptive scoring rubrics guiding interpretable scene evaluation. During\ninference, first and last segments are scored based solely on their\ndescriptions, whereas intermediate ones incorporate brief contextual summaries\nof adjacent scenes to assess narrative progression and redundancy. This\ncontextual prompting enables the LLM to balance local salience and global\ncoherence without parameter tuning. On SumMe and TVSum, our method achieves F1\nscores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior\nzero-shot baselines while approaching supervised performance. The results\ndemonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based\nscoring and establishes a general, interpretable zero-shot paradigm for video\nsummarization.", "AI": {"tldr": "该研究提出了一种基于评分准则和伪标签的提示框架，用于零样本视频摘要。它利用少量真实标注生成伪标签和数据集自适应评分准则，并通过上下文提示平衡局部显著性和全局连贯性，在SumMe和TVSum数据集上取得了优于现有零样本基线并接近有监督方法的F1分数。", "motivation": "随着视频内容激增，高效摘要长视频变得至关重要。现有有监督方法标注成本高且泛化性差，无监督方法难以捕捉高级语义和叙事线索。而当前的零样本LLM方法对提示模板和分数归一化高度敏感。", "method": "该方法引入了一个评分准则引导的伪标签提示框架。它将少量真实标注转化为高置信度伪标签，并将其聚合成结构化、数据集自适应的评分准则，以指导可解释的场景评估。在推理阶段，首尾片段仅根据描述评分，而中间片段则结合相邻场景的简要上下文摘要，评估叙事进展和冗余，从而使LLM在无需参数调优的情况下平衡局部显著性和全局连贯性。", "result": "在SumMe和TVSum数据集上，该方法分别取得了57.58和63.05的F1分数。这些结果超越了无监督和先前的零样本基线，并接近了有监督方法的性能。", "conclusion": "研究表明，评分准则引导的伪标签方法能有效稳定基于LLM的评分，并为视频摘要建立了一个通用、可解释的零样本范式。"}}
{"id": "2510.17568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17568", "abs": "https://arxiv.org/abs/2510.17568", "authors": ["Kaichen Zhou", "Yuhan Wang", "Grace Chen", "Xinhai Chang", "Gaspard Beaudouin", "Fangneng Zhan", "Paul Pu Liang", "Mengyu Wang"], "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception", "comment": null, "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.", "AI": {"tldr": "PAGE-4D是一个前馈模型，它将VGGT扩展到动态场景，通过动态感知聚合器解耦静态和动态信息，从而实现无需后处理的相机姿态估计、深度预测和点云重建。", "motivation": "现有的3D前馈模型（如VGGT）在静态数据集上训练，因此在涉及移动人体或可变形物体等复杂动态元素的真实世界场景中表现不佳。", "method": "引入PAGE-4D模型，它是VGGT的扩展，用于动态场景。核心方法是提出一个“动态感知聚合器”，通过预测一个动态感知掩码来解耦静态和动态信息。该掩码在姿态估计时抑制运动线索，在几何重建时放大运动线索，以解决多任务4D重建中固有的任务冲突。", "result": "PAGE-4D在动态场景中持续优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了卓越成果。", "conclusion": "PAGE-4D通过有效解耦动态信息，成功解决了动态场景中4D重建的挑战，并在多项任务上表现出优越性能，无需后处理即可进行全面的动态场景理解。"}}
{"id": "2510.17585", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17585", "abs": "https://arxiv.org/abs/2510.17585", "authors": ["Chuhong Wang", "Hua Li", "Chongyi Li", "Huazhong Liu", "Xiongxin Tang", "Sam Kwong"], "title": "Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset", "comment": null, "summary": "With the development of underwater exploration and marine protection,\nunderwater vision tasks are widespread. Due to the degraded underwater\nenvironment, characterized by color distortion, low contrast, and blurring,\ncamouflaged instance segmentation (CIS) faces greater challenges in accurately\nsegmenting objects that blend closely with their surroundings. Traditional\ncamouflaged instance segmentation methods, trained on terrestrial-dominated\ndatasets with limited underwater samples, may exhibit inadequate performance in\nunderwater scenes. To address these issues, we introduce the first underwater\ncamouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which\ncomprises 3,953 images of camouflaged marine organisms with instance-level\nannotations. In addition, we propose an Underwater Camouflaged Instance\nSegmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM\nincludes three key modules. First, the Channel Balance Optimization Module\n(CBOM) enhances channel characteristics to improve underwater feature learning,\neffectively addressing the model's limited understanding of underwater\nenvironments. Second, the Frequency Domain True Integration Module (FDTIM) is\nproposed to emphasize intrinsic object features and reduce interference from\ncamouflage patterns, enhancing the segmentation performance of camouflaged\nobjects blending with their surroundings. Finally, the Multi-scale Feature\nFrequency Aggregation Module (MFFAM) is designed to strengthen the boundaries\nof low-contrast camouflaged instances across multiple frequency bands,\nimproving the model's ability to achieve more precise segmentation of\ncamouflaged objects. Extensive experiments on the proposed UCIS4K and public\nbenchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.", "AI": {"tldr": "该研究针对水下伪装实例分割的挑战，首次提出了UCIS4K水下伪装实例分割数据集，并设计了基于Segment Anything Model的UCIS-SAM网络，通过三个关键模块显著提升了水下伪装目标的分割性能。", "motivation": "水下环境退化（颜色失真、对比度低、模糊）导致水下视觉任务面临巨大挑战，尤其是在伪装实例分割中。传统的伪装实例分割方法主要基于陆地数据集训练，在水下场景表现不佳，难以准确分割与环境高度融合的目标。", "method": "1. 构建了首个水下伪装实例分割数据集UCIS4K，包含3,953张带有实例级标注的伪装海洋生物图像。2. 提出了基于Segment Anything Model（SAM）的水下伪装实例分割网络UCIS-SAM。3. UCIS-SAM包含三个核心模块：通道平衡优化模块（CBOM）增强水下特征学习；频域真值集成模块（FDTIM）强调目标固有特征并减少伪装干扰；多尺度特征频率聚合模块（MFFAM）强化低对比度伪装实例的边界。", "result": "在提出的UCIS4K数据集和公共基准上的大量实验表明，UCIS-SAM网络优于现有的最先进方法，实现了更精确的水下伪装目标分割。", "conclusion": "该研究通过引入专门的水下伪装实例分割数据集和创新的网络架构，有效解决了水下伪装实例分割的难题，为水下探索和海洋保护提供了更强大的视觉分析工具。"}}
{"id": "2510.17609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17609", "abs": "https://arxiv.org/abs/2510.17609", "authors": ["Siqi Chen", "Shanyue Guan"], "title": "Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation", "comment": null, "summary": "The advancement of UAV technology has enabled efficient, non-contact\nstructural health monitoring. Combined with photogrammetry, UAVs can capture\nhigh-resolution scans and reconstruct detailed 3D models of infrastructure.\nHowever, a key challenge remains in segmenting specific structural components\nfrom these models-a process traditionally reliant on time-consuming and\nerror-prone manual labeling. To address this issue, we propose a machine\nlearning-based framework for automated segmentation of 3D point clouds. Our\napproach uses the complementary strengths of real-world UAV-scanned point\nclouds and synthetic data generated from Building Information Modeling (BIM) to\novercome the limitations associated with manual labeling. Validation on a\nrailroad track dataset demonstrated high accuracy in identifying and segmenting\nmajor components such as rails and crossties. Moreover, by using smaller-scale\ndatasets supplemented with BIM data, the framework significantly reduced\ntraining time while maintaining reasonable segmentation accuracy. This\nautomated approach improves the precision and efficiency of 3D infrastructure\nmodel segmentation and advances the integration of UAV and BIM technologies in\nstructural health monitoring and infrastructure management.", "AI": {"tldr": "该研究提出一个基于机器学习的框架，利用无人机扫描的真实点云和BIM生成的合成数据，实现3D点云的自动化分割，以解决传统手动标记耗时且易错的问题。", "motivation": "无人机和摄影测量技术能够获取基础设施的详细3D模型，但从这些模型中分割特定结构组件传统上依赖耗时且易出错的手动标记，这是结构健康监测中的一个关键挑战。", "method": "本文提出一种基于机器学习的框架，用于3D点云的自动化分割。该方法结合了真实世界无人机扫描点云和从建筑信息模型（BIM）生成的合成数据的互补优势，以克服手动标记的局限性。", "result": "该框架在铁路轨道数据集上进行了验证，在识别和分割钢轨、轨枕等主要组件方面表现出高精度。此外，通过使用补充了BIM数据的小规模数据集，该框架显著缩短了训练时间，同时保持了合理的分割精度。", "conclusion": "这种自动化方法提高了3D基础设施模型分割的精度和效率，并推动了无人机和BIM技术在结构健康监测和基础设施管理中的整合应用。"}}
{"id": "2510.17603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17603", "abs": "https://arxiv.org/abs/2510.17603", "authors": ["Shuyuan Zhang", "Chenhan Jiang", "Zuoou Li", "Jiankang Deng"], "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling", "comment": "NeurIPS 2025 Poster", "summary": "3D generation from natural language offers significant potential to reduce\nexpert manual modeling efforts and enhance accessibility to 3D assets. However,\nexisting methods often yield unstructured meshes and exhibit poor\ninteractivity, making them impractical for artistic workflows. To address these\nlimitations, we represent 3D assets as shape programs and introduce ShapeCraft,\na novel multi-agent framework for text-to-3D generation. At its core, we\npropose a Graph-based Procedural Shape (GPS) representation that decomposes\ncomplex natural language into a structured graph of sub-tasks, thereby\nfacilitating accurate LLM comprehension and interpretation of spatial\nrelationships and semantic shape details. Specifically, LLM agents\nhierarchically parse user input to initialize GPS, then iteratively refine\nprocedural modeling and painting to produce structured, textured, and\ninteractive 3D assets. Qualitative and quantitative experiments demonstrate\nShapeCraft's superior performance in generating geometrically accurate and\nsemantically rich 3D assets compared to existing LLM-based agents. We further\nshow the versatility of ShapeCraft through examples of animated and\nuser-customized editing, highlighting its potential for broader interactive\napplications.", "AI": {"tldr": "ShapeCraft是一个新颖的多智能体框架，通过将3D资产表示为形状程序和引入基于图的程序形状（GPS）表示，实现了从自然语言生成结构化、纹理化和交互式的3D资产。", "motivation": "现有的文本到3D生成方法通常产生非结构化网格且交互性差，不适用于艺术工作流，限制了3D资产的生成和可访问性。", "method": "该研究将3D资产表示为形状程序，并提出了ShapeCraft多智能体框架。核心是引入了基于图的程序形状（GPS）表示，它将复杂的自然语言分解为子任务的结构化图，以提高LLM对空间关系和语义细节的理解。LLM智能体分层解析用户输入以初始化GPS，然后迭代优化程序建模和绘制过程。", "result": "定性和定量实验表明，ShapeCraft在生成几何精确和语义丰富的3D资产方面优于现有的基于LLM的智能体。它还通过动画和用户自定义编辑的示例展示了其多功能性。", "conclusion": "ShapeCraft在生成结构化、纹理化和交互式3D资产方面表现出色，具有广泛交互式应用的巨大潜力。"}}
{"id": "2510.17611", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17611", "abs": "https://arxiv.org/abs/2510.17611", "authors": ["Jia Guo", "Shuai Lu", "Lei Fan", "Zelin Li", "Donglin Di", "Yang Song", "Weihang Zhang", "Wenbing Zhu", "Hong Yan", "Fang Chen", "Huiqi Li", "Hongen Liao"], "title": "One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection", "comment": "Extended version of CVPR2025", "summary": "Unsupervised anomaly detection (UAD) has evolved from building specialized\nsingle-class models to unified multi-class models, yet existing multi-class\nmodels significantly underperform the most advanced one-for-one counterparts.\nMoreover, the field has fragmented into specialized methods tailored to\nspecific scenarios (multi-class, 3D, few-shot, etc.), creating deployment\nbarriers and highlighting the need for a unified solution. In this paper, we\npresent Dinomaly2, the first unified framework for full-spectrum image UAD,\nwhich bridges the performance gap in multi-class models while seamlessly\nextending across diverse data modalities and task settings. Guided by the \"less\nis more\" philosophy, we demonstrate that the orchestration of five simple\nelement achieves superior performance in a standard reconstruction-based\nframework. This methodological minimalism enables natural extension across\ndiverse tasks without modification, establishing that simplicity is the\nfoundation of true universality. Extensive experiments on 12 UAD benchmarks\ndemonstrate Dinomaly2's full-spectrum superiority across multiple modalities\n(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,\ninference-unified multi-class, few-shot) and application domains (industrial,\nbiological, outdoor). For example, our multi-class model achieves unprecedented\n99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For\nmulti-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art\nperformance with minimum adaptations. Moreover, using only 8 normal examples\nper class, our method surpasses previous full-shot models, achieving 98.7% and\n97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,\ncomputational scalability, and universal applicability positions Dinomaly2 as a\nunified solution for the full spectrum of real-world anomaly detection\napplications.", "AI": {"tldr": "Dinomaly2 是一种统一的无监督异常检测（UAD）框架，通过简洁的设计实现了跨多种数据模态和任务设置的卓越性能，弥补了多类别模型与先进单一类别模型之间的性能差距，并解决了领域碎片化问题。", "motivation": "现有的多类别无监督异常检测模型性能显著低于最先进的单一类别模型。此外，该领域已碎片化为针对特定场景（如多类别、3D、少样本等）的专业方法，这带来了部署障碍，凸显了对统一解决方案的需求。", "method": "本文提出了Dinomaly2，这是首个用于全谱图像UAD的统一框架。它遵循“少即是多”的理念，在一个标准的基于重建的框架中，通过协调五个简单元素实现了卓越性能。这种方法上的极简主义使其能够无需修改地自然扩展到各种任务。", "result": "Dinomaly2弥合了多类别模型与先进单一类别模型之间的性能差距，并无缝扩展到各种数据模态和任务设置。它在12个UAD基准测试中展现出全谱优势，例如，其多类别模型在MVTec-AD和VisA上分别达到了前所未有的99.9%和99.3%的图像级AUROC。在多视角和多模态检测中也表现出最先进的性能。此外，仅使用每类别8个正常样本，其少样本方法就超越了之前的全样本模型，在MVTec-AD和VisA上分别达到了98.7%和97.4%的图像级AUROC。", "conclusion": "Dinomaly2凭借其极简设计、计算可扩展性和普适性，被定位为全谱真实世界异常检测应用的统一解决方案。"}}
{"id": "2510.17664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17664", "abs": "https://arxiv.org/abs/2510.17664", "authors": ["Ling Liu", "Jun Tian", "Li Yi"], "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads", "comment": null, "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.", "AI": {"tldr": "本文提出4DSegStreamer，一个双线程系统，用于在流式设置中实现实时4D全景分割，尤其适用于动态环境和高帧率条件，并能预测复杂场景中的动态物体。", "motivation": "在密集人群疏散和复杂场景下的自动驾驶等高度动态环境中，实时、细粒度的感知在有限时间预算内至关重要，因此需要一种高效的流式4D全景分割方法。", "method": "4DSegStreamer框架采用双线程系统：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程通过与最新内存对齐并补偿自我运动和动态物体移动，确保对传入帧的及时预测。该框架通用，可与现有3D和4D分割方法无缝集成。", "result": "在室内HOI4D数据集和室外SemanticKITTI及nuScenes数据集上进行了评估。实验证明了该方法的有效性，尤其是在准确预测复杂场景中的动态物体方面。与现有流式感知方法相比，在高FPS条件下表现出卓越的鲁棒性。", "conclusion": "4DSegStreamer提供了一个高效、鲁棒的实时4D全景分割解决方案，特别适用于处理流式数据和准确预测动态环境中的物体，填补了现有方法在实时性和鲁棒性上的不足。"}}
{"id": "2510.17644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17644", "abs": "https://arxiv.org/abs/2510.17644", "authors": ["Zexian Huang", "Mashnoon Islam", "Brian Armstrong", "Kourosh Khoshelham", "Martin Tomko"], "title": "Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives", "comment": null, "summary": "Dry-stone walls hold significant heritage and environmental value. Mapping\nthese structures is essential for ecosystem preservation and wildfire\nmanagement in Australia. Yet, many walls remain unidentified due to their\ninaccessibility and the high cost of manual mapping. Deep learning-based\nsegmentation offers a scalable solution, but two major challenges persist: (1)\nvisual occlusion of low-lying walls by dense vegetation, and (2) limited\nlabeled data for supervised training. We propose DINO-CV, a segmentation\nframework for automatic mapping of low-lying dry-stone walls using\nhigh-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs\novercome visual occlusion by capturing terrain structures hidden beneath\nvegetation, enabling analysis of structural rather than spectral cues. DINO-CV\nintroduces a self-supervised cross-view pre-training strategy based on\nknowledge distillation to mitigate data scarcity. It learns invariant visual\nand geometric representations across multiple DEM derivatives, supporting\nvarious vision backbones including ResNet, Wide ResNet, and Vision\nTransformers. Applied to the UNESCO World Heritage cultural landscape of Budj\nBim, Victoria, the method identifies one of Australia's densest collections of\ncolonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves\na mean Intersection over Union (mIoU) of 68.6% on test areas and maintains\n63.8% mIoU when fine-tuned with only 10% labeled data. These results\ndemonstrate the potential of self-supervised learning on high-resolution DEM\nderivatives for automated dry-stone wall mapping in vegetated and heritage-rich\nenvironments with scarce annotations.", "AI": {"tldr": "本文提出DINO-CV框架，利用LiDAR衍生的DEM和自监督学习，解决植被遮挡和数据稀缺问题，实现澳大利亚低矮干石墙的自动化高精度测绘。", "motivation": "干石墙具有重要的遗产和环境价值，但由于难以进入、人工测绘成本高、植被遮挡以及监督训练所需的标记数据有限，导致许多墙体未被识别。有效的测绘对于生态系统保护和野火管理至关重要。", "method": "本文提出了DINO-CV框架，用于使用高分辨率机载LiDAR衍生的数字高程模型（DEMs）自动测绘低矮干石墙。DEMs通过捕获植被下隐藏的地形结构来克服视觉遮挡，从而分析结构而非光谱线索。DINO-CV引入了一种基于知识蒸馏的自监督跨视图预训练策略，以缓解数据稀缺问题，学习跨多个DEM导数的不变视觉和几何表示，并支持多种视觉骨干网络（如ResNet、Wide ResNet和Vision Transformers）。", "result": "该方法应用于澳大利亚维多利亚州Budj Bim联合国教科文组织世界遗产文化景观，成功识别了澳大利亚最密集的殖民干石墙群之一。在测试区域，DINO-CV实现了68.6%的平均交并比（mIoU），并且在使用仅10%标记数据进行微调时仍保持63.8%的mIoU。", "conclusion": "研究结果表明，在植被茂密、遗产丰富且注释稀缺的环境中，利用高分辨率DEM导数进行自监督学习，在自动化干石墙测绘方面具有巨大潜力。"}}
{"id": "2510.17684", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17684", "abs": "https://arxiv.org/abs/2510.17684", "authors": ["Xinwei Zhang", "Hu Chen", "Zhe Yuan", "Sukun Tian", "Peng Feng"], "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model", "comment": null, "summary": "Foundation models for medical image segmentation have achieved remarkable\nperformance. Adaptive fine-tuning of natural image segmentation foundation\nmodels is crucial for medical image segmentation tasks. However, some\nlimitations exist in existing fine-tuning methods: 1) insufficient\nrepresentation of high-level features and 2) the fine-tuning process disrupts\nthe structural integrity of pretrained weights. Inspired by these critical\nproblems, we propose an intelligent communication mixture-of-experts\nboosted-medical image segmentation foundation model, named IC-MoE, with twofold\nideas: 1) We construct basic experts, semantic experts, and adaptive experts.\nMoreover, we implement a pixel probability adaptive voting strategy, which\nenables expert selection and fusion through label consistency and load\nbalancing. This approach preliminarily enhances the representation capability\nof high-level features while preserving the structural integrity of pretrained\nweights. 2) We propose a semantic-guided contrastive learning method to address\nthe issue of weak supervision in contrastive learning. This method further\nenhances the representation capability of high-level features while preserving\nthe structural integrity of pretrained weights. Extensive experiments across\nthree public medical image segmentation datasets demonstrate that the IC-MoE\noutperforms other SOTA models. Consequently, the proposed IC-MoE effectively\nsupplements foundational medical image segmentation models with high-level\nfeatures and pretrained structural integrity. We also validate the superior\ngeneralizability of the IC-MoE across diverse medical image segmentation\nscenarios.", "AI": {"tldr": "本文提出了一种名为IC-MoE的智能通信混合专家（MoE）模型，用于医学图像分割。该模型通过自适应投票的混合专家机制和语义引导对比学习，解决了现有微调方法中高层特征表示不足和预训练权重结构完整性受损的问题，显著提升了分割性能和泛化能力。", "motivation": "现有医学图像分割基础模型的自适应微调方法存在局限性：1) 高层特征表示不足；2) 微调过程破坏了预训练权重的结构完整性。", "method": "1. 构建了基础专家、语义专家和自适应专家，并实现了像素概率自适应投票策略，通过标签一致性和负载均衡进行专家选择和融合，以增强高层特征表示并保留预训练权重结构。\n2. 提出了语义引导对比学习方法，解决对比学习中弱监督问题，进一步增强高层特征表示并保留预训练权重结构。", "result": "IC-MoE在三个公开医学图像分割数据集上优于其他最先进模型。它有效地补充了基础医学图像分割模型的高层特征和预训练结构完整性，并展现出卓越的泛化能力。", "conclusion": "所提出的IC-MoE模型通过有效补充高层特征和保持预训练结构完整性，显著提升了医学图像分割基础模型的性能和泛化能力。"}}
{"id": "2510.17626", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17626", "abs": "https://arxiv.org/abs/2510.17626", "authors": ["Frédéric LIN", "Biruk Abere Ambaw", "Adrian Popescu", "Hejer Ammar", "Romaric Audigier", "Hervé Le Borgne"], "title": "CaMiT: A Time-Aware Car Model Dataset for Classification and Generation", "comment": "To be published in NeurIPS 2025 Track on Datasets and Benchmarks", "summary": "AI systems must adapt to evolving visual environments, especially in domains\nwhere object appearances change over time. We introduce Car Models in Time\n(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,\na representative class of technological artifacts. CaMiT includes 787K labeled\nsamples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),\nsupporting both supervised and self-supervised learning. Static pretraining on\nin-domain data achieves competitive performance with large-scale generalist\nmodels while being more resource-efficient, yet accuracy declines when models\nare tested across years. To address this, we propose a time-incremental\nclassification setting, a realistic continual learning scenario with emerging,\nevolving, and disappearing classes. We evaluate two strategies:\ntime-incremental pretraining, which updates the backbone, and time-incremental\nclassifier learning, which updates only the final layer, both improving\ntemporal robustness. Finally, we explore time-aware image generation that\nleverages temporal metadata during training, yielding more realistic outputs.\nCaMiT offers a rich benchmark for studying temporal adaptation in fine-grained\nvisual recognition and generation.", "AI": {"tldr": "本文介绍了CaMiT数据集，用于捕捉汽车模型随时间演变，并提出了时间增量分类设置及相关学习策略，以提高AI系统在动态视觉环境中的时间适应性。", "motivation": "AI系统必须适应不断变化的视觉环境，尤其是在物体外观随时间演变（如汽车模型）的领域。现有模型在跨年份测试时准确性会下降，需要新的方法来解决这一挑战。", "method": "研究引入了CaMiT数据集，包含78.7万个标注样本和510万个未标注样本，涵盖190种汽车模型（2007-2023年）。提出了时间增量分类设置，这是一种具有新出现、演变和消失类别的持续学习场景。评估了两种策略：时间增量预训练（更新骨干网络）和时间增量分类器学习（仅更新最后一层）。此外，还探索了利用时间元数据进行训练的时间感知图像生成。", "result": "静态预训练在域内数据上表现出与大型通用模型相当的竞争力，但跨年份测试时准确性会下降。两种时间增量策略（预训练和分类器学习）都显著提高了模型的时间鲁棒性。时间感知图像生成利用时间元数据，产生了更逼真的输出。", "conclusion": "CaMiT数据集为研究细粒度视觉识别和生成中的时间适应性提供了一个丰富的基准。所提出的时间增量学习策略有效提升了AI系统在处理随时间演变物体时的性能和鲁棒性。"}}
{"id": "2510.17651", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17651", "abs": "https://arxiv.org/abs/2510.17651", "authors": ["Sébastien Thuau", "Siba Haidar", "Ayush Bajracharya", "Rachid Chelouah"], "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs", "comment": "7 pages, 1 figure, FLTA 2025", "summary": "We examine frugal federated learning approaches to violence detection by\ncomparing two complementary strategies: (i) zero-shot and federated fine-tuning\nof vision-language models (VLMs), and (ii) personalized training of a compact\n3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter\nCNN3D as representative cases, we evaluate accuracy, calibration, and energy\nusage under realistic non-IID settings. Both approaches exceed 90% accuracy.\nCNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and\nlog loss, while using less energy. VLMs remain favorable for contextual\nreasoning and multimodal inference. We quantify energy and CO$_2$ emissions\nacross training and inference, and analyze sustainability trade-offs for\ndeployment. To our knowledge, this is the first comparative study of LoRA-tuned\nvision-language models and personalized CNNs for federated violence detection,\nwith an emphasis on energy efficiency and environmental metrics. These findings\nsupport a hybrid model: lightweight CNNs for routine classification, with\nselective VLM activation for complex or descriptive scenarios. The resulting\nframework offers a reproducible baseline for responsible, resource-aware AI in\nvideo surveillance, with extensions toward real-time, multimodal, and\nlifecycle-aware systems.", "AI": {"tldr": "本研究比较了两种节俭型联邦学习方法在暴力检测中的应用：视觉语言模型（VLM）的联邦微调和紧凑型3D卷积神经网络（CNN3D）的个性化训练，评估了准确性、校准和能耗，并提出了混合模型策略。", "motivation": "研究旨在探索在联邦学习环境下，实现高效、可持续的暴力检测AI方案，尤其关注资源受限和环境影响下的模型性能与能耗权衡。", "method": "研究对比了两种策略：(i) LLaVA-7B等视觉语言模型的零样本和联邦低秩适应（LoRA）微调，以及(ii) 65.8M参数的紧凑型3D卷积神经网络的个性化训练。在非独立同分布（non-IID）设置下，评估了模型的准确性、校准性和能耗，并量化了训练和推理过程中的能源消耗及二氧化碳排放。", "result": "两种方法均实现了超过90%的准确率。CNN3D在ROC AUC和对数损失方面略优于LoRA微调的VLM，且能耗更低。VLM在上下文推理和多模态推理方面表现出优势。这是首次比较LoRA微调VLM和个性化CNN在联邦暴力检测中性能的研究，并强调了能效和环境指标。", "conclusion": "研究结果支持采用混合模型策略：轻量级CNN用于常规分类，而VLM则按需激活以处理复杂或描述性场景。该框架为视频监控中负责任、资源感知型AI提供了一个可复现的基线，并可扩展到实时、多模态和全生命周期感知的系统。"}}
{"id": "2510.17685", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17685", "abs": "https://arxiv.org/abs/2510.17685", "authors": ["Min Cao", "Xinyu Zhou", "Ding Jiang", "Bo Du", "Mang Ye", "Min Zhang"], "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning", "comment": "Final version published in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI). Xplore link:\n  https://ieeexplore.ieee.org/document/11199360", "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person\nusing textual descriptions, facing challenge in modality heterogeneity. Prior\nworks have attempted to address it by developing cross-modal global or local\nalignment strategies. However, global methods typically overlook fine-grained\ncross-modal differences, whereas local methods require prior information to\nexplore explicit part alignments. Additionally, current methods are\nEnglish-centric, restricting their application in multilingual contexts. To\nalleviate these issues, we pioneer a multilingual TIPR task by developing a\nmultilingual TIPR benchmark, for which we leverage large language models for\ninitial translations and refine them by integrating domain-specific knowledge.\nCorrespondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation\nReasoning and Aligning framework to learn alignment across languages and\nmodalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module\nenables bidirectional prediction of masked image and text, implicitly enhancing\nthe modeling of local relations across languages and modalities, a\nmulti-dimensional global alignment module is integrated to bridge the modality\nheterogeneity. The proposed method achieves new state-of-the-art results on all\nmultilingual TIPR datasets. Data and code are presented in\nhttps://github.com/Flame-Chasers/Bi-IRRA.", "AI": {"tldr": "该论文提出了多语言文本到图像人物检索（TIPR）任务，构建了多语言TIPR基准，并提出了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决跨语言和跨模态的对齐问题，实现了SOTA性能。", "motivation": "现有的文本到图像人物检索（TIPR）方法面临模态异构性挑战。全局对齐方法忽视细粒度差异，局部对齐方法需要先验信息。此外，当前方法以英语为中心，限制了其在多语言环境中的应用。", "method": "1. 开创性地提出了多语言TIPR任务，并构建了多语言TIPR基准，利用大型语言模型进行初步翻译并结合领域知识进行优化。2. 提出了Bi-IRRA（Bidirectional Implicit Relation Reasoning and Aligning）框架，用于学习跨语言和跨模态的对齐。3. Bi-IRRA包含一个双向隐式关系推理模块，通过预测被遮蔽的图像和文本来隐式增强跨语言和模态的局部关系建模。4. 集成了一个多维全局对齐模块来弥合模态异构性。", "result": "所提出的方法在所有多语言TIPR数据集上均取得了新的最先进（SOTA）结果。", "conclusion": "该研究成功解决了多语言文本到图像人物检索的挑战，通过构建多语言基准和提出Bi-IRRA框架，有效地实现了跨语言和跨模态的对齐，显著提升了检索性能。"}}
{"id": "2510.17681", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17681", "abs": "https://arxiv.org/abs/2510.17681", "authors": ["Yuandong Pu", "Le Zhuo", "Songhao Han", "Jinbo Xing", "Kaiwen Zhu", "Shuo Cao", "Bin Fu", "Si Liu", "Hongsheng Li", "Yu Qiao", "Wenlong Zhang", "Xi Chen", "Yihao Liu"], "title": "PICABench: How Far Are We from Physically Realistic Image Editing?", "comment": null, "summary": "Image editing has achieved remarkable progress recently. Modern editing\nmodels could already follow complex instructions to manipulate the original\ncontent. However, beyond completing the editing instructions, the accompanying\nphysical effects are the key to the generation realism. For example, removing\nan object should also remove its shadow, reflections, and interactions with\nnearby objects. Unfortunately, existing models and benchmarks mainly focus on\ninstruction completion but overlook these physical effects. So, at this moment,\nhow far are we from physically realistic image editing? To answer this, we\nintroduce PICABench, which systematically evaluates physical realism across\neight sub-dimension (spanning optics, mechanics, and state transitions) for\nmost of the common editing operations (add, remove, attribute change, etc). We\nfurther propose the PICAEval, a reliable evaluation protocol that uses\nVLM-as-a-judge with per-case, region-level human annotations and questions.\nBeyond benchmarking, we also explore effective solutions by learning physics\nfrom videos and construct a training dataset PICA-100K. After evaluating most\nof the mainstream models, we observe that physical realism remains a\nchallenging problem with large rooms to explore. We hope that our benchmark and\nproposed solutions can serve as a foundation for future work moving from naive\ncontent editing toward physically consistent realism.", "AI": {"tldr": "现有图像编辑模型在完成指令方面表现出色，但普遍忽视了伴随的物理效果，导致真实感不足。本文提出了PICABench基准来系统评估物理真实感，并引入PICAEval评估协议。通过评估主流模型，发现物理真实感仍是一个巨大挑战，并构建了PICA-100K数据集以探索解决方案。", "motivation": "现代图像编辑模型能够遵循复杂指令操纵内容，但其生成的图像往往缺乏物理真实感，例如移除物体时未能同时移除其阴影、反射或与周围物体的互动。现有模型和基准主要关注指令完成，而忽略了这些对真实感至关重要的物理效果。", "method": "1. 引入PICABench基准，系统评估八个子维度（光学、力学、状态转换）的物理真实感，涵盖常见的编辑操作（添加、移除、属性更改等）。2. 提出PICAEval，一个可靠的评估协议，使用VLM-as-a-judge，结合逐案例、区域级别的人工标注和问题。3. 探索从视频中学习物理规律的有效解决方案，并构建了PICA-100K训练数据集。", "result": "在评估了大多数主流模型后，作者观察到物理真实感仍然是一个极具挑战性的问题，有很大的探索空间。现有模型在物理一致性方面表现不佳。", "conclusion": "本研究提出的基准和解决方案可以为未来的工作奠定基础，促使图像编辑从单纯的内容编辑迈向物理一致的真实感生成。"}}
{"id": "2510.17699", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17699", "abs": "https://arxiv.org/abs/2510.17699", "authors": ["Aleksandr Oganov", "Ilya Bykov", "Eva Neudachina", "Mishan Aliev", "Alexander Tolmachev", "Alexander Sidorov", "Aleksandr Zuev", "Andrey Okhotin", "Denis Rakitin", "Aibek Alanov"], "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver", "comment": null, "summary": "While diffusion models achieve state-of-the-art generation quality, they\nstill suffer from computationally expensive sampling. Recent works address this\nissue with gradient-based optimization methods that distill a few-step ODE\ndiffusion solver from the full sampling process, reducing the number of\nfunction evaluations from dozens to just a few. However, these approaches often\nrely on intricate training techniques and do not explicitly focus on preserving\nfine-grained details. In this paper, we introduce the Generalized Solver: a\nsimple parameterization of the ODE sampler that does not require additional\ntraining tricks and improves quality over existing approaches. We further\ncombine the original distillation loss with adversarial training, which\nmitigates artifacts and enhances detail fidelity. We call the resulting method\nthe Generalized Adversarial Solver and demonstrate its superior performance\ncompared to existing solver training methods under similar resource\nconstraints. Code is available at https://github.com/3145tttt/GAS.", "AI": {"tldr": "本文提出了一种名为“广义对抗求解器”的新方法，通过简化ODE采样器参数化并结合对抗训练，在不增加训练复杂性的前提下，显著加速了扩散模型的采样过程，同时提升了生成图像的细节保真度。", "motivation": "扩散模型虽然生成质量卓越，但采样计算成本高昂。现有通过梯度优化方法（蒸馏）减少采样步数的方案，往往依赖复杂的训练技巧，且未能明确关注细节的保留。", "method": "本文提出了两种方法：1. “广义求解器”（Generalized Solver）：一种简单的ODE采样器参数化，无需额外的训练技巧即可提升质量。2. “广义对抗求解器”（Generalized Adversarial Solver）：将原始蒸馏损失与对抗训练相结合，以减轻伪影并增强细节保真度。", "result": "广义对抗求解器在相似资源限制下，相比现有求解器训练方法展现出卓越的性能，有效缓解了伪影并显著提升了细节保真度。", "conclusion": "广义对抗求解器提供了一种简单且无需复杂训练技巧的扩散模型加速方案，它不仅提高了生成质量，还通过结合对抗训练有效保留并增强了图像的精细细节。"}}
{"id": "2510.17686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17686", "abs": "https://arxiv.org/abs/2510.17686", "authors": ["Taichi Liu", "Zhenyu Wang", "Ruofeng Liu", "Guang Wang", "Desheng Zhang"], "title": "Towards 3D Objectness Learning in an Open World", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advancements in 3D object detection and novel category detection have\nmade significant progress, yet research on learning generalized 3D objectness\nremains insufficient. In this paper, we delve into learning open-world 3D\nobjectness, which focuses on detecting all objects in a 3D scene, including\nnovel objects unseen during training. Traditional closed-set 3D detectors\nstruggle to generalize to open-world scenarios, while directly incorporating 3D\nopen-vocabulary models for open-world ability struggles with vocabulary\nexpansion and semantic overlap. To achieve generalized 3D object discovery, We\npropose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect\nany objects within 3D scenes without relying on hand-crafted text prompts. We\nintroduce the strong generalization and zero-shot capabilities of 2D foundation\nmodels, utilizing both 2D semantic priors and 3D geometric priors for\nclass-agnostic proposals to broaden 3D object discovery. Then, by integrating\ncomplementary information from point cloud and RGB image in the cross-modal\nmixture of experts, OP3Det dynamically routes uni-modal and multi-modal\nfeatures to learn generalized 3D objectness. Extensive experiments demonstrate\nthe extraordinary performance of OP3Det, which significantly surpasses existing\nopen-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement\ncompared to closed-world 3D detectors.", "AI": {"tldr": "该论文提出OP3Det，一个无需提示的类别无关开放世界3D检测器，通过结合2D基础模型和跨模态专家混合，实现泛化3D物体发现，显著优于现有方法。", "motivation": "尽管3D物体检测和新类别检测取得了进展，但泛化3D物体感知（即检测3D场景中所有物体，包括训练中未见的新物体）的研究仍不足。传统的封闭集3D检测器难以泛化到开放世界场景，而直接使用3D开放词汇模型则面临词汇扩展和语义重叠问题。", "method": "论文提出了OP3Det，一个类别无关、无需提示的开放世界3D检测器。它利用2D基础模型的强大泛化和零样本能力，结合2D语义先验和3D几何先验来生成类别无关的3D物体候选。随后，通过一个跨模态专家混合模块，整合点云和RGB图像的互补信息，动态路由单模态和多模态特征，以学习泛化的3D物体感知。", "result": "OP3Det展现出卓越的性能，在AR（平均召回率）上比现有开放世界3D检测器高出16.0%，比封闭世界3D检测器提高13.5%。", "conclusion": "OP3Det通过结合2D基础模型的泛化能力和跨模态信息融合，有效解决了开放世界3D物体发现的挑战，显著提升了泛化3D物体检测的性能。"}}
{"id": "2510.17700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17700", "abs": "https://arxiv.org/abs/2510.17700", "authors": ["Walter Simoncini", "Michael Dorkenwald", "Tijmen Blankevoort", "Cees G. M. Snoek", "Yuki M. Asano"], "title": "Elastic ViTs from Pretrained Models without Retraining", "comment": "Accepted at NeurIPS 2025", "summary": "Vision foundation models achieve remarkable performance but are only\navailable in a limited set of pre-determined sizes, forcing sub-optimal\ndeployment choices under real-world constraints. We introduce SnapViT:\nSingle-shot network approximation for pruned Vision Transformers, a new\npost-pretraining structured pruning method that enables elastic inference\nacross a continuum of compute budgets. Our approach efficiently combines\ngradient information with cross-network structure correlations, approximated\nvia an evolutionary algorithm, does not require labeled data, generalizes to\nmodels without a classification head, and is retraining-free. Experiments on\nDINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over\nstate-of-the-art methods across various sparsities, requiring less than five\nminutes on a single A100 GPU to generate elastic models that can be adjusted to\nany computational budget. Our key contributions include an efficient pruning\nstrategy for pretrained Vision Transformers, a novel evolutionary approximation\nof Hessian off-diagonal structures, and a self-supervised importance scoring\nmechanism that maintains strong performance without requiring retraining or\nlabels. Code and pruned models are available at: https://elastic.ashita.nl/", "AI": {"tldr": "SnapViT是一种针对预训练视觉Transformer的单次剪枝方法，通过结合梯度信息和交叉网络结构相关性，实现无需重新训练和标签的弹性推理，可在多种计算预算下高效部署。", "motivation": "现有的视觉基础模型尺寸固定，导致在实际部署中，面对不同的计算资源限制时，无法进行最优选择，从而造成性能或效率上的次优。", "method": "本文提出了SnapViT，一种后预训练的结构化剪枝方法。它高效地结合了梯度信息和通过进化算法近似的交叉网络结构相关性。该方法不需要标记数据，适用于没有分类头的模型，并且无需重新训练，从而实现了跨连续计算预算的弹性推理。", "result": "在DINO、SigLIPv2、DeIT和AugReg模型上的实验表明，SnapViT在各种稀疏度下均优于现有最先进的方法。它能在少于五分钟的时间内（单块A100 GPU）生成弹性模型，这些模型可以适应任何计算预算。", "conclusion": "SnapViT提供了一种高效的预训练视觉Transformer剪枝策略，引入了新颖的Hessian非对角结构进化近似方法，并提出了一个自监督的重要性评分机制，能在不重新训练或使用标签的情况下保持强大的性能，从而实现了视觉模型的弹性部署。"}}
{"id": "2510.17731", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17731", "abs": "https://arxiv.org/abs/2510.17731", "authors": ["Aaron Appelle", "Jerome P. Lynch"], "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?", "comment": "Appeared in the ICML 2025 Workshop on Building Physically Plausible\n  World Models, July 2025, https://physical-world-modeling.github.io/", "summary": "Recent high-performing image-to-video (I2V) models based on variants of the\ndiffusion transformer (DiT) have displayed remarkable inherent world-modeling\ncapabilities by virtue of training on large scale video datasets. We\ninvestigate whether these models can generate realistic pedestrian movement\npatterns in crowded public scenes. Our framework conditions I2V models on\nkeyframes extracted from pedestrian trajectory benchmarks, then evaluates their\ntrajectory prediction performance using quantitative measures of pedestrian\ndynamics.", "AI": {"tldr": "本文研究了基于扩散变换器（DiT）的高性能图像到视频（I2V）模型是否能生成真实的人群行人运动模式。", "motivation": "近期高性能的I2V模型（基于DiT变体）通过在大型视频数据集上训练，展现出卓越的内在世界建模能力。研究者想探究这些模型是否也能生成拥挤公共场景中逼真的行人运动模式。", "method": "研究框架将I2V模型以从行人轨迹基准中提取的关键帧为条件进行训练，然后使用行人动力学的定量指标评估其轨迹预测性能。", "result": "抽象中未提及具体结果，仅说明正在进行调查和评估。", "conclusion": "抽象中未提及具体结论，仅说明正在进行调查和评估。"}}
{"id": "2510.17722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17722", "abs": "https://arxiv.org/abs/2510.17722", "authors": ["Yaning Pan", "Zekun Wang", "Qianqian Xie", "Yongqian Wen", "Yuanxing Zhang", "Guohui Zhang", "Haoxuan Hu", "Zhiyu Pan", "Yibing Huang", "Zhidong Gan", "Yonghong Lin", "An Ping", "Tianhao Peng", "Jiaheng Liu"], "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues", "comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench", "summary": "The recent development of Multimodal Large Language Models (MLLMs) has\nsignificantly advanced AI's ability to understand visual modalities. However,\nexisting evaluation benchmarks remain limited to single-turn question\nanswering, overlooking the complexity of multi-turn dialogues in real-world\nscenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video\nunderstanding benchmark for evaluating MLLMs in multi-turn dialogues.\nSpecifically, our MT-Video-Bench mainly assesses six core competencies that\nfocus on perceptivity and interactivity, encompassing 987 meticulously curated\nmulti-turn dialogues from diverse domains. These capabilities are rigorously\naligned with real-world applications, such as interactive sports analysis and\nmulti-turn video-based intelligent tutoring. With MT-Video-Bench, we\nextensively evaluate various state-of-the-art open-source and closed-source\nMLLMs, revealing their significant performance discrepancies and limitations in\nhandling multi-turn video dialogues. The benchmark will be publicly available\nto foster future research.", "AI": {"tldr": "MLLM在视频理解方面取得进展，但现有基准缺乏多轮对话评估。本文提出MT-Video-Bench，一个用于评估MLLM多轮视频对话理解能力的基准，涵盖感知和交互的六项核心能力，并揭示了现有模型的局限性。", "motivation": "现有MLLM评估基准仅限于单轮问答，未能涵盖真实世界中多轮对话的复杂性，因此需要一个能评估MLLM多轮视频对话理解能力的全面基准。", "method": "提出了MT-Video-Bench，一个全面的视频理解基准，用于评估MLLM在多轮对话中的表现。该基准主要评估六项核心能力，侧重于感知和交互，包含987个精心策划的来自不同领域的多轮对话。这些能力与现实应用（如交互式体育分析和多轮视频智能辅导）严格对齐。", "result": "使用MT-Video-Bench对各种最先进的开源和闭源MLLM进行了广泛评估，结果揭示了它们在处理多轮视频对话时存在的显著性能差异和局限性。", "conclusion": "MT-Video-Bench填补了现有评估基准的空白，揭示了当前MLLM在多轮视频对话理解方面的不足，并将公开可用以促进未来研究。"}}
{"id": "2510.17724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17724", "abs": "https://arxiv.org/abs/2510.17724", "authors": ["Matheus Ramos Parracho"], "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization", "comment": "Undergraduate thesis (preprint)---submitted to Escola Polit\\'ecnica,\n  Universidade Federal do Rio de Janeiro (POLI/UFRJ). The final version will\n  include official signatures and defense approval", "summary": "Automated signature verification is a critical biometric technique used in\nbanking, identity authentication, and legal documentation. Despite the notable\nprogress achieved by deep learning methods, most approaches in offline\nsignature verification still struggle to generalize across datasets, as\nvariations in handwriting styles and acquisition protocols often degrade\nperformance. This study investigates feature learning strategies for signature\nforgery detection, focusing on improving cross-dataset generalization -- that\nis, model robustness when trained on one dataset and tested on another. Using\nthree public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental\npipelines were developed: one based on raw signature images and another\nemploying a preprocessing method referred to as shell preprocessing. Several\nbehavioral patterns were identified and analyzed; however, no definitive\nsuperiority between the two approaches was established. The results show that\nthe raw-image model achieved higher performance across benchmarks, while the\nshell-based model demonstrated promising potential for future refinement toward\nrobust, cross-domain signature verification.", "AI": {"tldr": "本研究旨在提高离线签名验证在跨数据集场景下的泛化能力，比较了基于原始图像和基于壳预处理的两种特征学习策略，发现原始图像模型表现更好，但壳预处理模型具有未来潜力。", "motivation": "尽管深度学习在离线签名验证方面取得了进展，但由于笔迹风格和采集协议的差异，大多数方法在跨数据集泛化方面表现不佳，这限制了其在银行、身份验证和法律文件等关键应用中的实际部署。", "method": "本研究调查了签名伪造检测的特征学习策略，重点关注提高跨数据集泛化能力。使用了CEDAR、ICDAR和GPDS Synthetic三个公开基准数据集。开发了两种实验流程：一种基于原始签名图像，另一种采用“壳预处理”方法。分析了两种方法识别出的行为模式。", "result": "研究结果显示，在两种方法之间没有建立明确的优劣关系。基于原始图像的模型在不同基准测试中取得了更高的性能，而基于壳预处理的模型则展示了在实现鲁棒、跨域签名验证方面未来改进的潜力。", "conclusion": "尽管基于原始图像的模型目前表现更优，但基于壳预处理的模型在未来有望通过进一步完善，实现更鲁棒的跨域签名验证，值得进一步研究和优化。"}}
{"id": "2510.17719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17719", "abs": "https://arxiv.org/abs/2510.17719", "authors": ["Zhiqiang Teng", "Beibei Lin", "Tingting Chen", "Zifeng Yuan", "Xuanyi Li", "Xuanyu Zhang", "Shunli Zhang"], "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions", "comment": null, "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe\nocclusions and optical distortions caused by raindrop contamination on the\ncamera lens, substantially degrading reconstruction quality. Existing\nbenchmarks typically evaluate 3DGS using synthetic raindrop images with known\ncamera poses (constrained images), assuming ideal conditions. However, in\nreal-world scenarios, raindrops often interfere with accurate camera pose\nestimation and point cloud initialization. Moreover, a significant domain gap\nbetween synthetic and real raindrops further impairs generalization. To tackle\nthese issues, we introduce RaindropGS, a comprehensive benchmark designed to\nevaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images\nto clear 3DGS reconstructions. Specifically, the whole benchmark pipeline\nconsists of three parts: data preparation, data processing, and raindrop-aware\n3DGS evaluation, including types of raindrop interference, camera pose\nestimation and point cloud initialization, single image rain removal\ncomparison, and 3D Gaussian training comparison. First, we collect a real-world\nraindrop reconstruction dataset, in which each scene contains three aligned\nimage sets: raindrop-focused, background-focused, and rain-free ground truth,\nenabling a comprehensive evaluation of reconstruction quality under different\nfocus conditions. Through comprehensive experiments and analyses, we reveal\ncritical insights into the performance limitations of existing 3DGS methods on\nunconstrained raindrop images and the varying impact of different pipeline\ncomponents: the impact of camera focus position on 3DGS reconstruction\nperformance, and the interference caused by inaccurate pose and point cloud\ninitialization on reconstruction. These insights establish clear directions for\ndeveloping more robust 3DGS methods under raindrop conditions.", "AI": {"tldr": "该论文引入了RaindropGS，这是一个全面的基准测试，旨在评估在未受约束的真实雨滴条件下，3D Gaussian Splatting (3DGS) 从图像到清晰3D重建的完整管线性能。", "motivation": "现有3DGS基准测试通常使用已知相机姿态的合成雨滴图像，假设理想条件。然而，在真实世界中，雨滴会导致严重的遮挡和光学畸变，干扰相机姿态估计和点云初始化，且合成与真实雨滴之间存在显著的域差距，严重影响3DGS的重建质量和泛化能力。", "method": "本文提出了RaindropGS基准测试，包含数据准备、数据处理和雨滴感知3DGS评估三个部分。具体地，他们收集了一个真实的雨滴重建数据集，每个场景包含雨滴聚焦、背景聚焦和无雨地面真值三组对齐图像。通过此基准，他们评估了不同雨滴干扰类型、相机姿态估计和点云初始化的影响，以及单图像去雨和3D Gaussian训练的性能。", "result": "通过全面的实验和分析，研究揭示了现有3DGS方法在未受约束的雨滴图像上的性能局限性，以及不同管线组件（如相机焦点位置、不准确的姿态和点云初始化）对3DGS重建性能的显著影响。", "conclusion": "这些发现为在雨滴条件下开发更鲁棒的3DGS方法指明了清晰的方向，强调了解决真实世界雨滴干扰和初始化问题的必要性。"}}
{"id": "2510.17773", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17773", "abs": "https://arxiv.org/abs/2510.17773", "authors": ["Md. Enamul Atiq", "Shaikh Anowarul Fattah"], "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion", "comment": "15 pages, 7 Figures, 3 Tables", "summary": "Skin cancer is a life-threatening disease where early detection significantly\nimproves patient outcomes. Automated diagnosis from dermoscopic images is\nchallenging due to high intra-class variability and subtle inter-class\ndifferences. Many deep learning models operate as \"black boxes,\" limiting\nclinical trust. In this work, we propose a dual-encoder attention-based\nframework that leverages both segmented lesions and clinical metadata to\nenhance skin lesion classification in terms of both accuracy and\ninterpretability. A novel Deep-UNet architecture with Dual Attention Gates\n(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment\nlesions. The classification stage uses two DenseNet201 encoders-one on the\noriginal image and another on the segmented lesion whose features are fused via\nmulti-head cross-attention. This dual-input design guides the model to focus on\nsalient pathological regions. In addition, a transformer-based module\nincorporates patient metadata (age, sex, lesion site) into the prediction. We\nevaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019\nchallenges. The proposed method achieves state-of-the-art segmentation\nperformance and significantly improves classification accuracy and average AUC\ncompared to baseline models. To validate our model's reliability, we use\nGradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.\nThese visualizations confirm that our model's predictions are based on the\nlesion area, unlike models that rely on spurious background features. These\nresults demonstrate that integrating precise lesion segmentation and clinical\ndata with attention-based fusion leads to a more accurate and interpretable\nskin cancer classification model.", "AI": {"tldr": "本文提出了一种双编码器注意力框架，结合精确的病灶分割和临床元数据，实现了更准确、可解释的皮肤癌分类。", "motivation": "皮肤癌的早期检测对患者预后至关重要，但从皮肤镜图像中进行自动化诊断面临挑战，包括图像内部高变异性和类别间细微差异。此外，许多深度学习模型作为“黑箱”运行，限制了临床信任。", "method": "该方法首先使用带有双注意力门（DAG）和空洞空间金字塔池化（ASPP）的新型Deep-UNet架构对病灶进行分割。分类阶段使用两个DenseNet201编码器（一个用于原始图像，一个用于分割后的病灶），通过多头交叉注意力融合其特征。此外，一个基于Transformer的模块将患者元数据（年龄、性别、病灶部位）整合到预测中。通过Grad-CAM生成热图以验证模型的可解释性。", "result": "该方法在HAM10000、ISIC 2018和2019数据集上取得了最先进的分割性能，并显著提高了分类准确性和平均AUC。Grad-CAM可视化证实模型预测是基于病灶区域，而非背景特征。", "conclusion": "将精确的病灶分割和临床数据与基于注意力的融合相结合，能够构建出更准确、更可解释的皮肤癌分类模型。"}}
{"id": "2510.17716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17716", "abs": "https://arxiv.org/abs/2510.17716", "authors": ["Suqiang Ma", "Subhadeep Sengupta", "Yao Lee", "Beikang Gu", "Xianyan Chen", "Xianqiao Wang", "Yang Liu", "Mengjia Xu", "Galit H. Frydman", "He Li"], "title": "Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging", "comment": null, "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),\nwhite blood cells(WBCs), and platelets are significant biomarkers linked to\nconditions like thrombosis, infection, and inflammation. Flow cytometry, paired\nwith fluorescence staining, is commonly used to analyze these cell clusters,\nrevealing cell morphology and protein profiles. While computational approaches\nbased on machine learning have advanced the automatic analysis of single-cell\nflow cytometry images, there is a lack of effort to build tools to\nautomatically analyze images containing CCCs. Unlike single cells, cell\nclusters often exhibit irregular shapes and sizes. In addition, these cell\nclusters often consist of heterogeneous cell types, which require multi-channel\nstaining to identify the specific cell types within the clusters. This study\nintroduces a new computational framework for analyzing CCC images and\nidentifying cell types within clusters. Our framework uses a two-step analysis\nstrategy. First, it categorizes images into cell cluster and non-cluster groups\nby fine-tuning the You Only Look Once(YOLOv11) model, which outperforms\ntraditional convolutional neural networks (CNNs), Vision Transformers (ViT).\nThen, it identifies cell types by overlaying cluster contours with regions from\nmulti-channel fluorescence stains, enhancing accuracy despite cell debris and\nstaining artifacts. This approach achieved over 95% accuracy in both cluster\nclassification and phenotype identification. In summary, our automated\nframework effectively analyzes CCC images from flow cytometry, leveraging both\nbright-field and fluorescence data. Initially tested on blood cells, it holds\npotential for broader applications, such as analyzing immune and tumor cell\nclusters, supporting cellular research across various diseases.", "AI": {"tldr": "本文提出了一种自动计算框架，用于分析流式细胞术图像中的循环血细胞簇（CCCs），通过微调YOLOv11模型进行簇分类，并结合多通道荧光染色识别细胞类型，实现了超过95%的准确率。", "motivation": "循环血细胞簇（CCCs）是血栓形成、感染和炎症等疾病的重要生物标志物。尽管流式细胞术结合荧光染色常用于分析这些细胞簇，但目前缺乏用于自动分析包含CCCs图像的工具。与单细胞不同，细胞簇形状和大小不规则，且由异质细胞类型组成，需要多通道染色来识别具体细胞类型。", "method": "该研究引入了一个两步分析策略：首先，通过微调YOLOv11模型将图像分类为细胞簇和非细胞簇组，该模型优于传统的卷积神经网络（CNNs）和视觉Transformer（ViT）。其次，通过将细胞簇轮廓与多通道荧光染色区域叠加来识别细胞类型，即使存在细胞碎片和染色伪影也能提高准确性。该方法同时利用了明场和荧光数据。", "result": "该方法在细胞簇分类和表型识别方面均实现了超过95%的准确率。", "conclusion": "所提出的自动化框架能够有效分析流式细胞术中的CCCs图像，利用明场和荧光数据。该框架初步在血细胞上进行了测试，并具有更广泛的应用潜力，例如分析免疫和肿瘤细胞簇，从而支持各种疾病的细胞研究。"}}
{"id": "2510.17777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17777", "abs": "https://arxiv.org/abs/2510.17777", "authors": ["Samir Khaki", "Junxian Guo", "Jiaming Tang", "Shang Yang", "Yukang Chen", "Konstantinos N. Plataniotis", "Yao Lu", "Song Han", "Zhijian Liu"], "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference", "comment": null, "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.", "AI": {"tldr": "SparseVILA提出了一种新的高效VLM推理范式，通过在预填充阶段修剪冗余视觉token并在解码阶段检索与查询相关的token，实现视觉稀疏性的解耦，显著加速VLM推理并提升准确性。", "motivation": "视觉语言模型（VLMs）在多模态推理方面取得了巨大进步，但其可扩展性受到视觉token数量不断增长的限制，这些token主导了推理延迟，成为瓶颈。", "method": "SparseVILA通过以下方式解耦视觉稀疏性：1. 在预填充阶段（prefill）修剪冗余的视觉token（与查询无关的剪枝）。2. 在解码阶段（decoding）从保留的视觉缓存中仅检索与查询相关的token（与查询相关的检索）。该方法建立在AWQ优化的推理管道之上，是免训练且与架构无关的。", "result": "SparseVILA在长上下文视频任务上实现了高达4.0倍的预填充加速、2.5倍的解码加速以及2.6倍的端到端整体加速。同时，它在文档理解和推理任务上还提高了准确性。", "conclusion": "通过解耦与查询无关的剪枝和与查询相关的检索，SparseVILA为高效多模态推理开辟了新方向，提供了一个免训练、与架构无关的框架，可在不牺牲能力的情况下加速大型VLM。"}}
{"id": "2510.17739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17739", "abs": "https://arxiv.org/abs/2510.17739", "authors": ["Timur Ismagilov", "Shakaiba Majeed", "Michael Milford", "Tan Viet Tuyen Nguyen", "Sarvapali D. Ramchurn", "Shoaib Ehsan"], "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition", "comment": "13 pages", "summary": "We address multi-reference visual place recognition (VPR), where reference\nsets captured under varying conditions are used to improve localisation\nperformance. While deep learning with large-scale training improves robustness,\nincreasing data diversity and model complexity incur extensive computational\ncost during training and deployment. Descriptor-level fusion via voting or\naggregation avoids training, but often targets multi-sensor setups or relies on\nheuristics with limited gains under appearance and viewpoint change. We propose\na training-free, descriptor-agnostic approach that jointly models places using\nmultiple reference descriptors via matrix decomposition into basis\nrepresentations, enabling projection-based residual matching. We also introduce\nSotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance\ndata, our method improves Recall@1 by up to ~18% over single-reference and\noutperforms multi-reference baselines across appearance and viewpoint changes,\nwith gains of ~5% on unstructured data, demonstrating strong generalisation\nwhile remaining lightweight.", "AI": {"tldr": "本文提出了一种免训练、描述符无关的方法，通过矩阵分解联合建模多参考视觉地点识别（VPR）中的地点，并利用基于投影的残差匹配。该方法显著提升了Recall@1性能，并优于现有基线，同时保持轻量化和良好的泛化能力。", "motivation": "多参考视觉地点识别（VPR）旨在通过不同条件下的参考集提升定位性能。现有深度学习方法虽然鲁棒，但训练和部署成本高昂。而描述符级别的融合方法（如投票或聚合）虽避免了训练，但通常针对多传感器设置或依赖启发式方法，在外观和视角变化下增益有限。因此，需要一种更有效、计算成本更低的方法来处理多参考VPR。", "method": "本文提出了一种免训练、描述符无关的方法。它通过矩阵分解将多个参考描述符联合建模为基础表示，从而实现基于投影的残差匹配来识别地点。此外，还引入了SotonMV，一个用于多视角VPR的结构化基准。", "result": "在多外观数据集上，本文方法将Recall@1提高了约18%，超越了单参考方法。在外观和视角变化下，其性能优于多参考基线，在非结构化数据上获得了约5%的增益。该方法展示了强大的泛化能力，同时保持了轻量化。", "conclusion": "所提出的免训练、描述符无关的矩阵分解方法，通过联合建模多参考描述符并进行残差匹配，有效提升了多参考视觉地点识别的性能。它在处理外观和视角变化时表现出优越的鲁棒性和泛化能力，且计算成本较低。"}}
{"id": "2510.17703", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17703", "abs": "https://arxiv.org/abs/2510.17703", "authors": ["Mhd Adnan Albani", "Riad Sonbol"], "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns", "comment": "19 pages, 2 figures, 9 tables", "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of\npeople over the age of 60, causing motor impairments that impede hand\ncoordination activities such as writing and drawing. Many approaches have tried\nto support early detection of Parkinson's disease based on hand-drawn images;\nhowever, we identified two major limitations in the related works: (1) the lack\nof sufficient datasets, (2) the robustness when dealing with unseen patient\ndata. In this paper, we propose a new approach to detect Parkinson's disease\nthat consists of two stages: The first stage classifies based on their drawing\ntype(circle, meander, spiral), and the second stage extracts the required\nfeatures from the images and detects Parkinson's disease. We overcame the\nprevious two limitations by applying a chunking strategy where we divide each\nimage into 2x2 chunks. Each chunk is processed separately when extracting\nfeatures and recognizing Parkinson's disease indicators. To make the final\nclassification, an ensemble method is used to merge the decisions made from\neach chunk. Our evaluation shows that our proposed approach outperforms the top\nperforming state-of-the-art approaches, in particular on unseen patients. On\nthe NewHandPD dataset our approach, it achieved 97.08% accuracy for seen\npatients and 94.91% for unseen patients, our proposed approach maintained a gap\nof only 2.17 percentage points, compared to the 4.76-point drop observed in\nprior work.", "AI": {"tldr": "该论文提出一种基于分块策略和集成学习的帕金森病检测新方法，通过将图像分块处理并合并决策，有效解决了现有方法在数据集不足和对未见患者数据鲁棒性差的问题，并在未见患者数据上取得了优于现有技术的表现。", "motivation": "帕金森病（PD）是一种常见的神经退行性疾病，早期检测至关重要。现有基于手绘图像的帕金森病检测方法存在两个主要局限性：1) 缺乏足够的数据集；2) 对未见患者数据的鲁棒性不足。", "method": "本文提出一种两阶段帕金森病检测方法：第一阶段根据绘图类型（圆形、蜿蜒形、螺旋形）进行分类；第二阶段从图像中提取所需特征并检测帕金森病。为解决现有局限性，采用“分块策略”，将每张图像分成2x2的块，每个块独立处理以提取特征和识别帕金森病指标。最终使用集成方法合并每个块的决策进行最终分类。", "result": "在NewHandPD数据集上，该方法对已知患者的准确率为97.08%，对未见患者的准确率为94.91%。与现有技术相比，该方法在已知患者与未见患者之间的准确率差距仅为2.17个百分点，远低于先前工作观察到的4.76个百分点。这表明该方法在处理未见患者数据时的鲁棒性显著优于最先进的方法。", "conclusion": "所提出的基于分块策略和集成学习的帕金森病检测方法，有效提升了对未见患者数据的鲁棒性，并在性能上超越了现有最先进的方法，为帕金森病的早期检测提供了一种更有效和稳健的解决方案。"}}
{"id": "2510.16342", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16342", "abs": "https://arxiv.org/abs/2510.16342", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu", "Zhen Yang", "Gongshen Liu"], "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts", "comment": null, "summary": "Existing concept erasure methods for text-to-image diffusion models commonly\nrely on fixed anchor strategies, which often lead to critical issues such as\nconcept re-emergence and erosion. To address this, we conduct causal tracing to\nreveal the inherent sensitivity of erasure to anchor selection and define\nSibling Exclusive Concepts as a superior class of anchors. Based on this\ninsight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for\nContextual Targeting), a dynamic anchor selection framework designed to\novercome the limitations of fixed anchors. Our framework introduces a novel\ntwo-stage evaluation mechanism that automatically discovers optimal anchors for\nprecise erasure while identifying critical boundary anchors to preserve related\nconcepts. Extensive evaluations demonstrate that SELECT, as a universal anchor\nsolution, not only efficiently adapts to multiple erasure frameworks but also\nconsistently outperforms existing baselines across key performance metrics,\naveraging only 4 seconds for anchor mining of a single concept.", "AI": {"tldr": "本文提出SELECT，一种用于文本到图像扩散模型中概念擦除的动态锚点选择框架，通过引入“同级排他概念”作为锚点，解决了现有固定锚点方法导致的概念重现和侵蚀问题，并显著优于现有基线。", "motivation": "现有文本到图像扩散模型中的概念擦除方法依赖于固定的锚点策略，这常导致概念重现和侵蚀等关键问题。", "method": "研究人员通过因果溯源揭示了擦除对锚点选择的敏感性，并定义了“同级排他概念”作为一种更优的锚点类别。在此基础上，提出了SELECT（Sibling-Exclusive Evaluation for Contextual Targeting）动态锚点选择框架，该框架引入了两阶段评估机制，能自动发现最优锚点进行精确擦除，并识别关键边界锚点以保留相关概念。", "result": "SELECT作为一种通用锚点解决方案，不仅能高效适应多种擦除框架，还在关键性能指标上持续优于现有基线，平均每个概念的锚点挖掘仅需4秒。", "conclusion": "SELECT框架通过动态锚点选择，特别是利用“同级排他概念”，有效解决了固定锚点策略的局限性，提供了一种高效、精确且通用的概念擦除解决方案，能够防止概念重现并保留相关概念。"}}
{"id": "2510.17803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17803", "abs": "https://arxiv.org/abs/2510.17803", "authors": ["Zixin Yin", "Ling-Hao Chen", "Lionel Ni", "Xili Dai"], "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing", "comment": "SIGGRAPH Asia 2025", "summary": "Recent advances in training-free attention control methods have enabled\nflexible and efficient text-guided editing capabilities for existing generation\nmodels. However, current approaches struggle to simultaneously deliver strong\nediting strength while preserving consistency with the source. This limitation\nbecomes particularly critical in multi-round and video editing, where visual\nerrors can accumulate over time. Moreover, most existing methods enforce global\nconsistency, which limits their ability to modify individual attributes such as\ntexture while preserving others, thereby hindering fine-grained editing.\nRecently, the architectural shift from U-Net to MM-DiT has brought significant\nimprovements in generative performance and introduced a novel mechanism for\nintegrating text and vision modalities. These advancements pave the way for\novercoming challenges that previous methods failed to resolve. Through an\nin-depth analysis of MM-DiT, we identify three key insights into its attention\nmechanisms. Building on these, we propose ConsistEdit, a novel attention\ncontrol method specifically tailored for MM-DiT. ConsistEdit incorporates\nvision-only attention control, mask-guided pre-attention fusion, and\ndifferentiated manipulation of the query, key, and value tokens to produce\nconsistent, prompt-aligned edits. Extensive experiments demonstrate that\nConsistEdit achieves state-of-the-art performance across a wide range of image\nand video editing tasks, including both structure-consistent and\nstructure-inconsistent scenarios. Unlike prior methods, it is the first\napproach to perform editing across all inference steps and attention layers\nwithout handcraft, significantly enhancing reliability and consistency, which\nenables robust multi-round and multi-region editing. Furthermore, it supports\nprogressive adjustment of structural consistency, enabling finer control.", "AI": {"tldr": "ConsistEdit是一种针对MM-DiT模型设计的免训练注意力控制方法，能够实现强大、一致且细粒度的图像和视频编辑，克服了现有方法在编辑强度和一致性方面的局限性。", "motivation": "现有的免训练注意力控制方法难以在实现强大编辑能力的同时保持与源图像的一致性，尤其在多轮和视频编辑中问题更突出。此外，这些方法通常强制全局一致性，限制了对特定属性的细粒度修改。MM-DiT架构的出现为解决这些挑战提供了新机遇。", "method": "通过深入分析MM-DiT的注意力机制，作者识别出三个关键洞察。在此基础上，提出了ConsistEdit方法，专门针对MM-DiT。该方法结合了纯视觉注意力控制、掩码引导的注意力前融合，以及对查询（query）、键（key）和值（value）token的差异化操作，以产生一致且与提示对齐的编辑。该方法在所有推理步骤和注意力层进行编辑，无需手动调整。", "result": "ConsistEdit在广泛的图像和视频编辑任务（包括结构一致和结构不一致场景）中实现了最先进的性能。它显著提高了可靠性和一致性，支持鲁棒的多轮和多区域编辑。作为首个无需手动调整即可在所有推理步骤和注意力层进行编辑的方法，它还支持结构一致性的渐进式调整，从而实现更精细的控制。", "conclusion": "ConsistEdit通过利用MM-DiT的架构优势和创新的注意力控制机制，显著提升了免训练文本引导的图像和视频编辑能力，提供了卓越的一致性、编辑强度和细粒度控制，特别适用于复杂的多轮和视频编辑场景。"}}
{"id": "2510.17771", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17771", "abs": "https://arxiv.org/abs/2510.17771", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs", "comment": "21 pages, 10 figures, 6 tables", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such\nas visual question answering, yet they can still fail even when the correct\nvisual evidence is present. In this work, we systematically investigate whether\nthese failures arise from not perceiving the evidence or from not leveraging it\neffectively. By examining layer-wise attention dynamics, we find that shallow\nlayers focus primarily on text, while deeper layers sparsely but reliably\nattend to localized evidence regions. Surprisingly, VLMs often perceive the\nvisual evidence when outputting incorrect answers, a phenomenon we term\n``seeing but not believing'' that widely exists in major VLM families. Building\non this, we introduce an inference-time intervention that highlights deep-layer\nevidence regions through selective attention-based masking. It requires no\ntraining and consistently improves accuracy across multiple families, including\nLLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable\nevidence internally but under-utilize it, making such signals explicit can\nbridge the gap between perception and reasoning, advancing the diagnostic\nunderstanding and reliability of VLMs.", "AI": {"tldr": "研究发现视觉语言模型（VLMs）在给出错误答案时，通常已经感知到正确的视觉证据（“看到但不相信”现象）。通过在推理时干预，突出深层注意力机制中的证据区域，可以在不训练的情况下提高多个VLM家族的准确性，表明VLM内部编码了可靠证据但未充分利用。", "motivation": "尽管存在正确的视觉证据，视觉语言模型（VLMs）在多模态任务中仍会失败。研究旨在系统性地探究这些失败是源于未能感知证据，还是未能有效地利用证据。", "method": "通过检查层级注意力动态，研究分析了VLM内部感知证据的方式。在此基础上，引入了一种推理时干预方法，通过选择性基于注意力的掩蔽（attention-based masking）来突出深层注意力层中的证据区域。", "result": "浅层主要关注文本，而深层则稀疏但可靠地关注局部证据区域。令人惊讶的是，VLMs在给出错误答案时，通常已经感知到视觉证据，这种“看到但不相信”的现象在主要VLM家族中普遍存在。所提出的推理时干预方法无需训练，能持续提高LLaVA、Qwen、Gemma和InternVL等多个VLM家族的准确性。", "conclusion": "VLMs内部编码了可靠的证据，但未能充分利用。明确这些内部信号可以弥合感知与推理之间的鸿沟，从而增进对VLM的诊断性理解和可靠性。"}}
